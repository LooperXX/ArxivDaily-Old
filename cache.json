{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2104.05218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "We propose Future Discriminators for Generation (FUDGE), a flexible and\nmodular method for controlled text generation. Given a pre-existing model G for\ngenerating text from a distribution of interest, FUDGE enables conditioning on\na desired attribute a (for example, formality) while requiring access only to\nG's output logits. FUDGE learns an attribute predictor operating on a partial\nsequence, and uses this predictor's outputs to adjust G's original\nprobabilities. We show that FUDGE models terms corresponding to a Bayesian\ndecomposition of the conditional distribution of G given attribute a. Moreover,\nFUDGE can easily compose predictors for multiple desired attributes. We\nevaluate FUDGE on three tasks -- couplet completion in poetry, topic control in\nlanguage generation, and formality change in machine translation -- and observe\ngains in all three tasks.",
          "link": "http://arxiv.org/abs/2104.05218",
          "publishedOn": "2021-08-17T01:54:46.340Z",
          "wordCount": 590,
          "title": "FUDGE: Controlled Text Generation With Future Discriminators. (arXiv:2104.05218v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We introduce a noisy channel approach for language model prompting in\nfew-shot text classification. Instead of computing the likelihood of the label\ngiven the input (referred as direct models), channel models compute the\nconditional probability of the input given the label, and are thereby required\nto explain every word in the input. We use channel models for recently proposed\nfew-shot learning methods with no or very limited updates to the language model\nparameters, via either in-context demonstration or prompt tuning. Our\nexperiments show that, for both methods, channel models significantly\noutperform their direct counterparts, which we attribute to their stability,\ni.e., lower variance and higher worst-case accuracy. We also present extensive\nablations that provide recommendations for when to use channel prompt tuning\ninstead of other competitive models (e.g., direct head tuning): channel prompt\ntuning is preferred when the number of training examples is small, labels in\nthe training data are imbalanced, or generalization to unseen labels is\nrequired.",
          "link": "http://arxiv.org/abs/2108.04106",
          "publishedOn": "2021-08-17T01:54:46.279Z",
          "wordCount": 617,
          "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification. (arXiv:2108.04106v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "The newly emerged transformer technology has a tremendous impact on NLP\nresearch. In the general English domain, transformer-based models have achieved\nstate-of-the-art performances on various NLP benchmarks. In the clinical\ndomain, researchers also have investigated transformer models for clinical\napplications. The goal of this study is to systematically explore three widely\nused transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical\nrelation extraction and develop an open-source package with clinical\npre-trained transformer-based models to facilitate information extraction in\nthe clinical domain. We developed a series of clinical RE models based on three\ntransformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these\nmodels using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2\nchallenges. We compared two classification strategies (binary vs. multi-class\nclassification) and investigated two approaches to generate candidate relations\nin different experimental settings. In this study, we compared three\ntransformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We\ndemonstrated that the RoBERTa-clinical RE model achieved the best performance\non the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2\ndataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our\nresults indicated that the binary classification strategy consistently\noutperformed the multi-class classification strategy for clinical relation\nextraction. Our methods and models are publicly available at\nhttps://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction.\nWe believe this work will improve current practice on clinical relation\nextraction and other related NLP tasks in the biomedical domain.",
          "link": "http://arxiv.org/abs/2107.08957",
          "publishedOn": "2021-08-17T01:54:46.271Z",
          "wordCount": 708,
          "title": "Clinical Relation Extraction Using Transformer-based Models. (arXiv:2107.08957v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>",
          "description": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.",
          "link": "http://arxiv.org/abs/2108.03857",
          "publishedOn": "2021-08-17T01:54:46.244Z",
          "wordCount": 613,
          "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>",
          "description": "In this paper, we describe our end-to-end multilingual speech translation\nsystem submitted to the IWSLT 2021 evaluation campaign on the Multilingual\nSpeech Translation shared task. Our system is built by leveraging transfer\nlearning across modalities, tasks and languages. First, we leverage\ngeneral-purpose multilingual modules pretrained with large amounts of\nunlabelled and labelled data. We further enable knowledge transfer from the\ntext task to the speech task by training two tasks jointly. Finally, our\nmultilingual model is finetuned on speech translation task-specific data to\nachieve the best translation results. Experimental results show our system\noutperforms the reported systems, including both end-to-end and cascaded based\napproaches, by a large margin.\n\nIn some translation directions, our speech translation results evaluated on\nthe public Multilingual TEDx test set are even comparable with the ones from a\nstrong text-to-text translation system, which uses the oracle speech\ntranscripts as input.",
          "link": "http://arxiv.org/abs/2107.06959",
          "publishedOn": "2021-08-17T01:54:46.238Z",
          "wordCount": 641,
          "title": "FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task. (arXiv:2107.06959v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zuoyun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "Subword units are commonly used for end-to-end automatic speech recognition\n(ASR), while a fully acoustic-oriented subword modeling approach is somewhat\nmissing. We propose an acoustic data-driven subword modeling (ADSM) approach\nthat adapts the advantages of several text-based and acoustic-based subword\nmethods into one pipeline. With a fully acoustic-oriented label design and\nlearning process, ADSM produces acoustic-structured subword units and\nacoustic-matched target sequence for further ASR training. The obtained ADSM\nlabels are evaluated with different end-to-end ASR approaches including CTC,\nRNN-Transducer and attention models. Experiments on the LibriSpeech corpus show\nthat ADSM clearly outperforms both byte pair encoding (BPE) and\npronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis\nshows that ADSM achieves acoustically more logical word segmentation and more\nbalanced sequence length, and thus, is suitable for both time-synchronous and\nlabel-synchronous models. We also briefly describe how to apply acoustic-based\nsubword regularization and unseen text segmentation using ADSM.",
          "link": "http://arxiv.org/abs/2104.09106",
          "publishedOn": "2021-08-17T01:54:46.213Z",
          "wordCount": 624,
          "title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1\">Deepak Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1\">Jared Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGresley_P/0/1/0/all/0/1\">Patrick LeGresley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Anand Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainbrand_D/0/1/0/all/0/1\">Dmitri Vainbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashinkunti_P/0/1/0/all/0/1\">Prethvi Kashinkunti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1\">Julie Bernauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1\">Amar Phanishayee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Large language models have led to state-of-the-art accuracies across a range\nof tasks. However, training these models efficiently is challenging for two\nreasons: a) GPU memory capacity is limited, making it impossible to fit large\nmodels on even a multi-GPU server, and b) the number of compute operations\nrequired to train these models can result in unrealistically long training\ntimes. Consequently, new methods of model parallelism such as tensor and\npipeline parallelism have been proposed. Unfortunately, naive usage of these\nmethods leads to fundamental scaling issues at thousands of GPUs, e.g., due to\nexpensive cross-node communication or devices spending significant time waiting\non other devices to make progress.\n\nIn this paper, we show how different types of parallelism methods (tensor,\npipeline, and data parallelism) can be composed to scale to thousands of GPUs\nand models with trillions of parameters. We survey techniques for pipeline\nparallelism and propose a novel interleaved pipeline parallelism schedule that\ncan improve throughput by 10+% with memory footprint comparable to existing\napproaches. We quantitatively study the trade-offs between tensor, pipeline,\nand data parallelism, and provide intuition as to how to configure distributed\ntraining of a large model. Our approach allows us to perform training\niterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs\nwith achieved per-GPU throughput of 52% of theoretical peak. Our code is open\nsourced at https://github.com/nvidia/megatron-lm.",
          "link": "http://arxiv.org/abs/2104.04473",
          "publishedOn": "2021-08-17T01:54:46.207Z",
          "wordCount": 744,
          "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. (arXiv:2104.04473v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "The existence of multiple datasets for sarcasm detection prompts us to apply\ntransfer learning to exploit their commonality. The adversarial neural transfer\n(ANT) framework utilizes multiple loss terms that encourage the source-domain\nand the target-domain feature distributions to be similar while optimizing for\ndomain-specific performance. However, these objectives may be in conflict,\nwhich can lead to optimization difficulties and sometimes diminished transfer.\nWe propose a generalized latent optimization strategy that allows different\nlosses to accommodate each other and improves training dynamics. The proposed\nmethod outperforms transfer learning and meta-learning baselines. In\nparticular, we achieve 10.02% absolute performance gain over the previous state\nof the art on the iSarcasm dataset.",
          "link": "http://arxiv.org/abs/2104.09261",
          "publishedOn": "2021-08-17T01:54:46.188Z",
          "wordCount": 594,
          "title": "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection. (arXiv:2104.09261v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jia Tracy Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_M/0/1/0/all/0/1\">Michiharu Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prihar_E/0/1/0/all/0/1\">Ethan Prihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_B/0/1/0/all/0/1\">Ben Graff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>",
          "description": "Since the introduction of the original BERT (i.e., BASE BERT), researchers\nhave developed various customized BERT models with improved performance for\nspecific domains and tasks by exploiting the benefits of {\\em transfer\nlearning}. Due to the nature of mathematical texts, which often use domain\nspecific vocabulary along with equations and math symbols, we posit that the\ndevelopment of a new BERT model for mathematics would be useful for many\nmathematical downstream tasks. In this resource paper, we introduce our\nmulti-institutional effort (i.e., two learning platforms and three academic\ninstitutions in the US) toward this need: MathBERT, a model created by\npre-training the BASE BERT model on a large mathematical corpus ranging from\npre-kindergarten (pre-k), to high-school, to college graduate level\nmathematical content. In addition, we select three general NLP tasks that are\noften used in mathematics education: prediction of knowledge component,\nauto-grading open-ended Q\\&A, and knowledge tracing, to demonstrate the\nsuperiority of MathBERT over BASE BERT. Our experiments show that MathBERT\noutperforms prior best methods by 1.2-22\\% and BASE BERT by 2-8\\% on these\ntasks. In addition, we build a mathematics specific vocabulary `mathVocab' to\ntrain with MathBERT. We discover that MathBERT pre-trained with `mathVocab'\noutperforms MathBERT trained with the BASE BERT vocabulary (i.e., `origVocab').\nMathBERT is currently being adopted at the participated leaning platforms:\nStride, Inc, a commercial educational resource provider, and ASSISTments.org, a\nfree online educational platform. We release MathBERT for public usage at:\nhttps://github.com/tbs17/MathBERT.",
          "link": "http://arxiv.org/abs/2106.07340",
          "publishedOn": "2021-08-17T01:54:46.170Z",
          "wordCount": 718,
          "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kahyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas J. Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McInnes_B/0/1/0/all/0/1\">Bridget McInnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">&#xd6;zlem Uzuner</a>",
          "description": "Methods and Materials: We investigated transferability of neural\nnetwork-based de-identification sys-tems with and without domain\ngeneralization. We used two domain generalization approaches: a novel approach\nJoint-Domain Learning (JDL) as developed in this paper, and a state-of-the-art\ndomain general-ization approach Common-Specific Decomposition (CSD) from the\nliterature. First, we measured trans-ferability from a single external source.\nSecond, we used two external sources and evaluated whether domain\ngeneralization can improve transferability of de-identification models across\ndomains which rep-resent different note types from the same institution. Third,\nusing two external sources with in-domain training data, we studied whether\nexternal source data are useful even in cases where sufficient in-domain\ntraining data are available. Finally, we investigated transferability of the\nde-identification mod-els across institutions. Results and Conclusions: We\nfound transferability from a single external source gave inconsistent re-sults.\nUsing additional external sources consistently yielded an F1-score of\napproximately 80%, but domain generalization was not always helpful to improve\ntransferability. We also found that external sources were useful even in cases\nwhere in-domain training data were available by reducing the amount of needed\nin-domain training data or by improving performance. Transferability across\ninstitutions was differed by note type and annotation label. External sources\nfrom a different institution were also useful to further improve performance.",
          "link": "http://arxiv.org/abs/2102.08517",
          "publishedOn": "2021-08-17T01:54:46.163Z",
          "wordCount": 684,
          "title": "Transferability of Neural Network-based De-identification Systems. (arXiv:2102.08517v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David I. Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adebonojo_D/0/1/0/all/0/1\">Damilola Adebonojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayeni_A/0/1/0/all/0/1\">Adesina Ayeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofe Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1\">Ayodele Awokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>",
          "description": "Massively multilingual machine translation (MT) has shown impressive\ncapabilities, including zero and few-shot translation between low-resource\nlanguage pairs. However, these models are often evaluated on high-resource\nlanguages with the assumption that they generalize to low-resource ones. The\ndifficulty of evaluating MT models on low-resource pairs is often due to lack\nof standardized evaluation datasets. In this paper, we present MENYO-20k, the\nfirst multi-domain parallel corpus with a special focus on clean orthography\nfor Yor\\`ub\\'a--English with standardized train-test splits for benchmarking.\nWe provide several neural MT benchmarks and compare them to the performance of\npopular pre-trained (massively multilingual) MT models both for the\nheterogeneous test set and its subdomains. Since these pre-trained models use\nhuge amounts of data with uncertain quality, we also analyze the effect of\ndiacritics, a major characteristic of Yor\\`ub\\'a, in the training data. We\ninvestigate how and when this training condition affects the final quality and\nintelligibility of a translation. Our models outperform massively multilingual\nmodels such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$ BLEU) when\ntranslating to Yor\\`ub\\'a, setting a high quality benchmark for future\nresearch.",
          "link": "http://arxiv.org/abs/2103.08647",
          "publishedOn": "2021-08-17T01:54:46.140Z",
          "wordCount": 673,
          "title": "The Effect of Domain and Diacritics in Yor\\`ub\\'a-English Neural Machine Translation. (arXiv:2103.08647v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong Q. Nguyen</a>",
          "description": "The mainstream image captioning models rely on Convolutional Neural Network\n(CNN) image features to generate captions via recurrent models. Recently, image\nscene graphs have been used to augment captioning models so as to leverage\ntheir structural semantics, such as object entities, relationships and\nattributes. Several studies have noted that the naive use of scene graphs from\na black-box scene graph generator harms image captioning performance and that\nscene graph-based captioning models have to incur the overhead of explicit use\nof image features to generate decent captions. Addressing these challenges, we\npropose \\textbf{SG2Caps}, a framework that utilizes only the scene graph labels\nfor competitive image captioning performance. The basic idea is to close the\nsemantic gap between the two scene graphs - one derived from the input image\nand the other from its caption. In order to achieve this, we leverage the\nspatial location of objects and the Human-Object-Interaction (HOI) labels as an\nadditional HOI graph. SG2Caps outperforms existing scene graph-only captioning\nmodels by a large margin, indicating scene graphs as a promising representation\nfor image captioning. Direct utilization of scene graph labels avoids expensive\ngraph convolutions over high-dimensional CNN features resulting in 49% fewer\ntrainable parameters. Our code is available at:\nhttps://github.com/Kien085/SG2Caps",
          "link": "http://arxiv.org/abs/2102.04990",
          "publishedOn": "2021-08-17T01:54:46.113Z",
          "wordCount": 683,
          "title": "In Defense of Scene Graphs for Image Captioning. (arXiv:2102.04990v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soldan_M/0/1/0/all/0/1\">Mattia Soldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Sisi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1\">Jesper Tegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Grounding language queries in videos aims at identifying the time interval\n(or moment) semantically relevant to a language query. The solution to this\nchallenging task demands understanding videos' and queries' semantic content\nand the fine-grained reasoning about their multi-modal interactions. Our key\nidea is to recast this challenge into an algorithmic graph matching problem.\nFueled by recent advances in Graph Neural Networks, we propose to leverage\nGraph Convolutional Networks to model video and textual information as well as\ntheir semantic alignment. To enable the mutual exchange of information across\nthe modalities, we design a novel Video-Language Graph Matching Network\n(VLG-Net) to match video and query graphs. Core ingredients include\nrepresentation graphs built atop video snippets and query tokens separately and\nused to model intra-modality relationships. A Graph Matching layer is adopted\nfor cross-modal context modeling and multi-modal fusion. Finally, moment\ncandidates are created using masked moment attention pooling by fusing the\nmoment's enriched snippet features. We demonstrate superior performance over\nstate-of-the-art grounding methods on three widely used datasets for temporal\nlocalization of moments in videos with language queries: ActivityNet-Captions,\nTACoS, and DiDeMo.",
          "link": "http://arxiv.org/abs/2011.10132",
          "publishedOn": "2021-08-17T01:54:46.097Z",
          "wordCount": 672,
          "title": "VLG-Net: Video-Language Graph Matching Network for Video Grounding. (arXiv:2011.10132v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chun Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zaixiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "The choice of token vocabulary affects the performance of machine\ntranslation. This paper aims to figure out what is a good vocabulary and\nwhether one can find the optimal vocabulary without trial training. To answer\nthese questions, we first provide an alternative understanding of the role of\nvocabulary from the perspective of information theory. Motivated by this, we\nformulate the quest of vocabularization -- finding the best token dictionary\nwith a proper size -- as an optimal transport (OT) problem. We propose VOLT, a\nsimple and efficient solution without trial training. Empirical results show\nthat VOLT outperforms widely-used vocabularies in diverse scenarios, including\nWMT-14 English-German and TED's 52 translation directions. For example, VOLT\nachieves almost 70% vocabulary size reduction and 0.5 BLEU gain on\nEnglish-German translation. Also, compared to BPE-search, VOLT reduces the\nsearch time from 384 GPU hours to 30 GPU hours on English-German translation.\nCodes are available at https://github.com/Jingjing-NLP/VOLT .",
          "link": "http://arxiv.org/abs/2012.15671",
          "publishedOn": "2021-08-17T01:54:46.090Z",
          "wordCount": 642,
          "title": "Vocabulary Learning via Optimal Transport for Neural Machine Translation. (arXiv:2012.15671v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>",
          "description": "Referring expression generation (REG) algorithms offer computational models\nof the production of referring expressions. In earlier work, a corpus of\nreferring expressions (REs) in Mandarin was introduced. In the present paper,\nwe annotate this corpus, evaluate classic REG algorithms on it, and compare the\nresults with earlier results on the evaluation of REG for English referring\nexpressions. Next, we offer an in-depth analysis of the corpus, focusing on\nissues that arise from the grammar of Mandarin. We discuss shortcomings of\nprevious REG evaluations that came to light during our investigation and we\nhighlight some surprising results. Perhaps most strikingly, we found a much\nhigher proportion of under-specified expressions than previous studies had\nsuggested, not just in Mandarin but in English as well.",
          "link": "http://arxiv.org/abs/2011.07398",
          "publishedOn": "2021-08-17T01:54:46.023Z",
          "wordCount": 591,
          "title": "Lessons from Computational Modelling of Reference Production in Mandarin and English. (arXiv:2011.07398v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Soumyadeep Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sural_S/0/1/0/all/0/1\">Shamik Sural</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1\">Niyati Chhaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_A/0/1/0/all/0/1\">Anandhavelu Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>",
          "description": "A consumer-dependent (business-to-consumer) organization tends to present\nitself as possessing a set of human qualities, which is termed as the brand\npersonality of the company. The perception is impressed upon the consumer\nthrough the content, be it in the form of advertisement, blogs or magazines,\nproduced by the organization. A consistent brand will generate trust and retain\ncustomers over time as they develop an affinity towards regularity and common\npatterns. However, maintaining a consistent messaging tone for a brand has\nbecome more challenging with the virtual explosion in the amount of content\nwhich needs to be authored and pushed to the Internet to maintain an edge in\nthe era of digital marketing. To understand the depth of the problem, we\ncollect around 300K web page content from around 650 companies. We develop\ntrait-specific classification models by considering the linguistic features of\nthe content. The classifier automatically identifies the web articles which are\nnot consistent with the mission and vision of a company and further helps us to\ndiscover the conditions under which the consistency cannot be maintained. To\naddress the brand inconsistency issue, we then develop a sentence ranking\nsystem that outputs the top three sentences that need to be changed for making\na web article more consistent with the company's brand personality.",
          "link": "http://arxiv.org/abs/2011.09754",
          "publishedOn": "2021-08-17T01:54:46.012Z",
          "wordCount": 718,
          "title": "An Integrated Approach for Improving Brand Consistency of Web Content: Modeling, Analysis and Recommendation. (arXiv:2011.09754v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_V/0/1/0/all/0/1\">Violet Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeNero_J/0/1/0/all/0/1\">John DeNero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "We propose an efficient batching strategy for variable-length decoding on GPU\narchitectures. During decoding, when candidates terminate or are pruned\naccording to heuristics, our streaming approach periodically \"refills\" the\nbatch before proceeding with a selected subset of candidates. We apply our\nmethod to variable-width beam search on a state-of-the-art machine translation\nmodel. Our method decreases runtime by up to 71% compared to a fixed-width beam\nsearch baseline and 17% compared to a variable-width baseline, while matching\nbaselines' BLEU. Finally, experiments show that our method can speed up\ndecoding in other domains, such as semantic and syntactic parsing.",
          "link": "http://arxiv.org/abs/2010.02164",
          "publishedOn": "2021-08-17T01:54:45.988Z",
          "wordCount": 589,
          "title": "A Streaming Approach For Efficient Batched Beam Search. (arXiv:2010.02164v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>",
          "description": "Vision-and-language pretraining (VLP) aims to learn generic multimodal\nrepresentations from massive image-text pairs. While various successful\nattempts have been proposed, learning fine-grained semantic alignments between\nimage-text pairs plays a key role in their approaches. Nevertheless, most\nexisting VLP approaches have not fully utilized the intrinsic knowledge within\nthe image-text pairs, which limits the effectiveness of the learned alignments\nand further restricts the performance of their models. To this end, we\nintroduce a new VLP method called ROSITA, which integrates the cross- and\nintra-modal knowledge in a unified scene graph to enhance the semantic\nalignments. Specifically, we introduce a novel structural knowledge masking\n(SKM) strategy to use the scene graph structure as a priori to perform masked\nlanguage (region) modeling, which enhances the semantic alignments by\neliminating the interference information within and across modalities.\nExtensive ablation studies and comprehensive analysis verifies the\neffectiveness of ROSITA in semantic alignments. Pretrained with both in-domain\nand out-of-domain datasets, ROSITA significantly outperforms existing\nstate-of-the-art VLP methods on three typical vision-and-language tasks over\nsix benchmark datasets.",
          "link": "http://arxiv.org/abs/2108.07073",
          "publishedOn": "2021-08-17T01:54:45.979Z",
          "wordCount": 634,
          "title": "ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration. (arXiv:2108.07073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1\">Houcemeddine Turki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taieb_M/0/1/0/all/0/1\">Mohamed Ali Hadj Taieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouicha_M/0/1/0/all/0/1\">Mohamed Ben Aouicha</a>",
          "description": "So far, multi-label classification algorithms have been evaluated using\nstatistical methods that do not consider the semantics of the considered\nclasses and that fully depend on abstract computations such as Bayesian\nReasoning. Currently, there are several attempts to develop ontology-based\nmethods for a better assessment of supervised classification algorithms. In\nthis research paper, we define a novel approach that aligns expected labels\nwith predicted labels in multi-label classification using ontology-driven\nfeature-based semantic similarity measures and we use it to develop a method\nfor creating precise confusion matrices for a more effective evaluation of\nmulti-label classification algorithms.",
          "link": "http://arxiv.org/abs/2011.00109",
          "publishedOn": "2021-08-17T01:54:45.972Z",
          "wordCount": 579,
          "title": "Knowledge-Based Construction of Confusion Matrices for Multi-Label Classification Algorithms using Semantic Similarity Measures. (arXiv:2011.00109v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velankar_M/0/1/0/all/0/1\">Makarand Velankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotian_R/0/1/0/all/0/1\">Rachita Kotian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Parag Kulkarni</a>",
          "description": "Lyrics play a significant role in conveying the song's mood and are\ninformation to understand and interpret music communication. Conventional\nnatural language processing approaches use translation of the Hindi text into\nEnglish for analysis. This approach is not suitable for lyrics as it is likely\nto lose the inherent intended contextual meaning. Thus, the need was identified\nto develop a system for Devanagari text analysis. The data set of 300 song\nlyrics with equal distribution in five different moods is used for the\nexperimentation. The proposed system performs contextual mood analysis of Hindi\nsong lyrics in Devanagari text format. The contextual analysis is stored as a\nknowledge base, updated using an incremental learning approach with new data.\nContextual knowledge graph with moods and associated important contextual terms\nprovides the graphical representation of the lyric data set used. The testing\nresults show 64% accuracy for the mood prediction. This work can be easily\nextended to applications related to Hindi literary work such as summarization,\nindexing, contextual retrieval, context-based classification and grouping of\ndocuments.",
          "link": "http://arxiv.org/abs/2108.06947",
          "publishedOn": "2021-08-17T01:54:45.954Z",
          "wordCount": 616,
          "title": "Contextual Mood Analysis with Knowledge Graph Representation for Hindi Song Lyrics in Devanagari Script. (arXiv:2108.06947v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burnyshev_P/0/1/0/all/0/1\">Pavel Burnyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malykh_V/0/1/0/all/0/1\">Valentin Malykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bout_A/0/1/0/all/0/1\">Andrey Bout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>",
          "description": "Sub-tasks of intent classification, such as robustness to distribution shift,\nadaptation to specific user groups and personalization, out-of-domain\ndetection, require extensive and flexible datasets for experiments and\nevaluation. As collecting such datasets is time- and labor-consuming, we\npropose to use text generation methods to gather datasets. The generator should\nbe trained to generate utterances that belong to the given intent. We explore\ntwo approaches to generating task-oriented utterances. In the zero-shot\napproach, the model is trained to generate utterances from seen intents and is\nfurther used to generate utterances for intents unseen during training. In the\none-shot approach, the model is presented with a single utterance from a test\nintent. We perform a thorough automatic, and human evaluation of the dataset\ngenerated utilizing two proposed approaches. Our results reveal that the\nattributes of the generated data are close to original test sets, collected via\ncrowd-sourcing.",
          "link": "http://arxiv.org/abs/2108.06991",
          "publishedOn": "2021-08-17T01:54:45.932Z",
          "wordCount": 584,
          "title": "A Single Example Can Improve Zero-Shot Data Generation. (arXiv:2108.06991v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1906.04225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>",
          "description": "Retrieve-and-edit based approaches to structured prediction, where structures\nassociated with retrieved neighbors are edited to form new structures, have\nrecently attracted increased interest. However, much recent work merely\nconditions on retrieved structures (e.g., in a sequence-to-sequence framework),\nrather than explicitly manipulating them. We show we can perform accurate\nsequence labeling by explicitly (and only) copying labels from retrieved\nneighbors. Moreover, because this copying is label-agnostic, we can achieve\nimpressive performance in zero-shot sequence-labeling tasks. We additionally\nconsider a dynamic programming approach to sequence labeling in the presence of\nretrieved neighbors, which allows for controlling the number of distinct\n(copied) segments used to form a prediction, and leads to both more\ninterpretable and accurate predictions.",
          "link": "http://arxiv.org/abs/1906.04225",
          "publishedOn": "2021-08-17T01:54:45.922Z",
          "wordCount": 577,
          "title": "Label-Agnostic Sequence Labeling by Copying Nearest Neighbors. (arXiv:1906.04225v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dat Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Stephen S. Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>",
          "description": "We propose a novel interpretable deep neural network for text classification,\ncalled ProtoryNet, based on a new concept of prototype trajectories. Motivated\nby the prototype theory in modern linguistics, ProtoryNet makes a prediction by\nfinding the most similar prototype for each sentence in a text sequence and\nfeeding an RNN backbone with the proximity of each sentence to the\ncorresponding active prototype. The RNN backbone then captures the temporal\npattern of the prototypes, which we refer to as prototype trajectories.\nPrototype trajectories enable intuitive and fine-grained interpretation of the\nreasoning process of the RNN model, in resemblance to how humans analyze texts.\nWe also design a prototype pruning procedure to reduce the total number of\nprototypes used by the model for better interpretability. Experiments on\nmultiple public data sets show that ProtoryNet is more accurate than the\nbaseline prototype-based deep neural net and reduces the performance gap\ncompared to state-of-the-art black-box models. In addition, after prototype\npruning, the resulting ProtoryNet models only need less than or around 20\nprototypes for all datasets, which significantly benefits interpretability.\nFurthermore, we report a survey result indicating that human users find\nProtoryNet more intuitive and easier to understand than other prototype-based\nmethods.",
          "link": "http://arxiv.org/abs/2007.01777",
          "publishedOn": "2021-08-17T01:54:45.913Z",
          "wordCount": 658,
          "title": "Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>",
          "description": "We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.",
          "link": "http://arxiv.org/abs/2108.07127",
          "publishedOn": "2021-08-17T01:54:45.905Z",
          "wordCount": 638,
          "title": "Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages. (arXiv:2108.07127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lizhi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Weijia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenmian Yang</a>",
          "description": "Spoken Language Understanding (SLU), a core component of the task-oriented\ndialogue system, expects a shorter inference latency due to the impatience of\nhumans. Non-autoregressive SLU models clearly increase the inference speed but\nsuffer uncoordinated-slot problems caused by the lack of sequential dependency\ninformation among each slot chunk. To gap this shortcoming, in this paper, we\npropose a novel non-autoregressive SLU model named Layered-Refine Transformer,\nwhich contains a Slot Label Generation (SLG) task and a Layered Refine\nMechanism (LRM). SLG is defined as generating the next slot label with the\ntoken sequence and generated slot labels. With SLG, the non-autoregressive\nmodel can efficiently obtain dependency information during training and spend\nno extra time in inference. LRM predicts the preliminary SLU results from\nTransformer's middle states and utilizes them to guide the final prediction.\nExperiments on two public datasets indicate that our model significantly\nimproves SLU performance (1.5\\% on Overall accuracy) while substantially speed\nup (more than 10 times) the inference process over the state-of-the-art\nbaseline.",
          "link": "http://arxiv.org/abs/2108.07005",
          "publishedOn": "2021-08-17T01:54:45.898Z",
          "wordCount": 599,
          "title": "An Effective Non-Autoregressive Model for Spoken Language Understanding. (arXiv:2108.07005v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaduo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shujuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>",
          "description": "The multi-format information extraction task in the 2021 Language and\nIntelligence Challenge is designed to comprehensively evaluate information\nextraction from different dimensions. It consists of an multiple slots relation\nextraction subtask and two event extraction subtasks that extract events from\nboth sentence-level and document-level. Here we describe our system for this\nmulti-format information extraction competition task. Specifically, for the\nrelation extraction subtask, we convert it to a traditional triple extraction\ntask and design a voting based method that makes full use of existing models.\nFor the sentence-level event extraction subtask, we convert it to a NER task\nand use a pointer labeling based method for extraction. Furthermore,\nconsidering the annotated trigger information may be helpful for event\nextraction, we design an auxiliary trigger recognition model and use the\nmulti-task learning mechanism to integrate the trigger features into the event\nextraction model. For the document-level event extraction subtask, we design an\nEncoder-Decoder based method and propose a Transformer-alike decoder.\nFinally,our system ranks No.4 on the test set leader-board of this multi-format\ninformation extraction task, and its F1 scores for the subtasks of relation\nextraction, event extractions of sentence-level and document-level are 79.887%,\n85.179%, and 70.828% respectively. The codes of our model are available at\n{https://github.com/neukg/MultiIE}.",
          "link": "http://arxiv.org/abs/2108.06957",
          "publishedOn": "2021-08-17T01:54:45.871Z",
          "wordCount": 644,
          "title": "An Effective System for Multi-format Information Extraction. (arXiv:2108.06957v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiawen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_J/0/1/0/all/0/1\">Jinye Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1\">Kenny Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>",
          "description": "The analytical description of charts is an exciting and important research\narea with many applications in academia and industry. Yet, this challenging\ntask has received limited attention from the computational linguistics research\ncommunity. This paper proposes \\textsf{AutoChart}, a large dataset for the\nanalytical description of charts, which aims to encourage more research into\nthis important area. Specifically, we offer a novel framework that generates\nthe charts and their analytical description automatically. We conducted\nextensive human and machine evaluations on the generated charts and\ndescriptions and demonstrate that the generated texts are informative,\ncoherent, and relevant to the corresponding charts.",
          "link": "http://arxiv.org/abs/2108.06897",
          "publishedOn": "2021-08-17T01:54:45.838Z",
          "wordCount": 543,
          "title": "AutoChart: A Dataset for Chart-to-Text Generation Task. (arXiv:2108.06897v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1\">Phuc Tran Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabryszak_A/0/1/0/all/0/1\">Aleksandra Gabryszak</a>",
          "description": "We present MobIE, a German-language dataset, which is human-annotated with 20\ncoarse- and fine-grained entity types and entity linking information for\ngeographically linkable entities. The dataset consists of 3,232 social media\ntexts and traffic reports with 91K tokens, and contains 20.5K annotated\nentities, 13.1K of which are linked to a knowledge base. A subset of the\ndataset is human-annotated with seven mobility-related, n-ary relation types,\nwhile the remaining documents are annotated using a weakly-supervised labeling\napproach implemented with the Snorkel framework. To the best of our knowledge,\nthis is the first German-language dataset that combines annotations for NER, EL\nand RE, and thus can be used for joint and multi-task learning of these\nfundamental information extraction tasks. We make MobIE public at\nhttps://github.com/dfki-nlp/mobie.",
          "link": "http://arxiv.org/abs/2108.06955",
          "publishedOn": "2021-08-17T01:54:45.829Z",
          "wordCount": 585,
          "title": "MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain. (arXiv:2108.06955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barba_O/0/1/0/all/0/1\">Ocean M. Barba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calbay_F/0/1/0/all/0/1\">Franz Arvin T. Calbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francisco_A/0/1/0/all/0/1\">Angelica Jane S. Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1\">Angel Luis D. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponay_C/0/1/0/all/0/1\">Charmaine S. Ponay</a>",
          "description": "Social media has played a huge part on how people get informed and\ncommunicate with one another. It has helped people express their needs due to\ndistress especially during disasters. Because posts made through it are\npublicly accessible by default, Twitter is among the most helpful social media\nsites in times of disaster. With this, the study aims to assess the needs\nexpressed during calamities by Filipinos on Twitter. Data were gathered and\nclassified as either disaster-related or unrelated with the use of Na\\\"ive\nBayes classifier. After this, the disaster-related tweets were clustered per\ndisaster type using Incremental Clustering Algorithm, and then sub-clustered\nbased on the location and time of the tweet using Density-based Spatiotemporal\nClustering Algorithm. Lastly, using Support Vector Machines, the tweets were\nclassified according to the expressed need, such as shelter, rescue, relief,\ncash, prayer, and others. After conducting the study, results showed that the\nIncremental Clustering Algorithm and Density-Based Spatiotemporal Clustering\nAlgorithm were able to cluster the tweets with f-measure scores of 47.20% and\n82.28% respectively. Also, the Na\\\"ive Bayes and Support Vector Machines were\nable to classify with an average f-measure score of 97% and an average accuracy\nof 77.57% respectively.",
          "link": "http://arxiv.org/abs/2108.06853",
          "publishedOn": "2021-08-17T01:54:45.753Z",
          "wordCount": 661,
          "title": "Clustering Filipino Disaster-Related Tweets Using Incremental and Density-Based Spatiotemporal Algorithm with Support Vector Machines for Needs Assessment 2. (arXiv:2108.06853v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Tables are often created with hierarchies, but existing works on table\nreasoning mainly focus on flat tables and neglect hierarchical tables.\nHierarchical tables challenge existing methods by hierarchical indexing, as\nwell as implicit relationships of calculation and semantics. This work presents\nHiTab, a free and open dataset for the research community to study question\nanswering (QA) and natural language generation (NLG) over hierarchical tables.\nHiTab is a cross-domain dataset constructed from a wealth of statistical\nreports and Wikipedia pages, and has unique characteristics: (1) nearly all\ntables are hierarchical, and (2) both target sentences for NLG and questions\nfor QA are revised from high-quality descriptions in statistical reports that\nare meaningful and diverse. (3) HiTab provides fine-grained annotations on both\nentity and quantity alignment. Targeting hierarchical structure, we devise a\nnovel hierarchy-aware logical form for symbolic reasoning over tables, which\nshows high effectiveness. Then given annotations of entity and quantity\nalignment, we propose partially supervised training, which helps models to\nlargely reduce spurious predictions in the QA task. In the NLG task, we find\nthat entity and quantity alignment also helps NLG models to generate better\nresults in a conditional generation setting. Experiment results of\nstate-of-the-art baselines suggest that this dataset presents a strong\nchallenge and a valuable benchmark for future research.",
          "link": "http://arxiv.org/abs/2108.06712",
          "publishedOn": "2021-08-17T01:54:45.681Z",
          "wordCount": 668,
          "title": "HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. (arXiv:2108.06712v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yuchen Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>",
          "description": "To quantitatively and intuitively explore the generalization ability of\npre-trained language models (PLMs), we have designed several tasks of\narithmetic and logical reasoning. We both analyse how well PLMs generalize when\nthe test data is in the same distribution as the train data and when it is\ndifferent, for the latter analysis, we have also designed a cross-distribution\ntest set other than the in-distribution test set. We conduct experiments on one\nof the most advanced and publicly released generative PLM - BART. Our research\nfinds that the PLMs can easily generalize when the distribution is the same,\nhowever, it is still difficult for them to generalize out of the distribution.",
          "link": "http://arxiv.org/abs/2108.06743",
          "publishedOn": "2021-08-17T01:54:45.655Z",
          "wordCount": 557,
          "title": "Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning. (arXiv:2108.06743v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Same_F/0/1/0/all/0/1\">Fahime Same</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>",
          "description": "Despite achieving encouraging results, neural Referring Expression Generation\nmodels are often thought to lack transparency. We probed neural Referential\nForm Selection (RFS) models to find out to what extent the linguistic features\ninfluencing the RE form are learnt and captured by state-of-the-art RFS models.\nThe results of 8 probing tasks show that all the defined features were learnt\nto some extent. The probing tasks pertaining to referential status and\nsyntactic position exhibited the highest performance. The lowest performance\nwas achieved by the probing models designed to predict discourse structure\nproperties beyond the sentence level.",
          "link": "http://arxiv.org/abs/2108.06806",
          "publishedOn": "2021-08-17T01:54:45.642Z",
          "wordCount": 529,
          "title": "What can Neural Referential Form Selectors Learn?. (arXiv:2108.06806v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yutong Li</a>",
          "description": "Building an independent misspelling detector and serve it before correction\ncan bring multiple benefits to speller and other search components, which is\nparticularly true for the most commonly deployed noisy-channel based speller\nsystems. With rapid development of deep learning and substantial advancement in\ncontextual representation learning such as BERTology, building a decent\nmisspelling detector without having to rely on hand-crafted features associated\nwith noisy-channel architecture becomes more-than-ever accessible. However\nBERTolgy models are trained with natural language corpus but Maps Search is\nhighly domain specific, would BERTology continue its success. In this paper we\ndesign 4 stages of models for misspeling detection ranging from the most basic\nLSTM to single-domain augmented fine-tuned BERT. We found for Maps Search in\nour case, other advanced BERTology family model such as RoBERTa does not\nnecessarily outperform BERT, and a classic cross-domain fine-tuned full BERT\neven underperforms a smaller single-domain fine-tuned BERT. We share more\nfindings through comprehensive modeling experiments and analysis, we also\nbriefly cover the data generation algorithm breakthrough.",
          "link": "http://arxiv.org/abs/2108.06842",
          "publishedOn": "2021-08-17T01:54:45.630Z",
          "wordCount": 596,
          "title": "Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations. (arXiv:2108.06842v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_M/0/1/0/all/0/1\">Megan Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jingjing Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acero_A/0/1/0/all/0/1\">Alex Acero</a>",
          "description": "Named entity recognition (NER) is usually developed and tested on text from\nwell-written sources. However, in intelligent voice assistants, where NER is an\nimportant component, input to NER may be noisy because of user or speech\nrecognition error. In applications, entity labels may change frequently, and\nnon-textual properties like topicality or popularity may be needed to choose\namong alternatives.\n\nWe describe a NER system intended to address these problems. We test and\ntrain this system on a proprietary user-derived dataset. We compare with a\nbaseline text-only NER system; the baseline enhanced with external gazetteers;\nand the baseline enhanced with the search and indirect labelling techniques we\ndescribe below. The final configuration gives around 6% reduction in NER error\nrate. We also show that this technique improves related tasks, such as semantic\nparsing, with an improvement of up to 5% in error rate.",
          "link": "http://arxiv.org/abs/2108.06633",
          "publishedOn": "2021-08-17T01:54:45.617Z",
          "wordCount": 609,
          "title": "DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants. (arXiv:2108.06633v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Gaole He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Knowledge base question answering (KBQA) aims to answer a question over a\nknowledge base (KB). Early studies mainly focused on answering simple questions\nover KBs and achieved great success. However, their performance on complex\nquestions is still far from satisfactory. Therefore, in recent years,\nresearchers propose a large number of novel methods, which looked into the\nchallenges of answering complex questions. In this survey, we review recent\nadvances on KBQA with the focus on solving complex questions, which usually\ncontain multiple subjects, express compound relations, or involve numerical\noperations. In detail, we begin with introducing the complex KBQA task and\nrelevant background. Then, we describe benchmark datasets for complex KBQA task\nand introduce the construction process of these datasets. Next, we present two\nmainstream categories of methods for complex KBQA, namely semantic\nparsing-based (SP-based) methods and information retrieval-based (IR-based)\nmethods. Specifically, we illustrate their procedures with flow designs and\ndiscuss their major differences and similarities. After that, we summarize the\nchallenges that these two categories of methods encounter when answering\ncomplex questions, and explicate advanced solutions and techniques used in\nexisting work. Finally, we conclude and discuss several promising directions\nrelated to complex KBQA for future research.",
          "link": "http://arxiv.org/abs/2108.06688",
          "publishedOn": "2021-08-17T01:54:45.603Z",
          "wordCount": 639,
          "title": "Complex Knowledge Base Question Answering: A Survey. (arXiv:2108.06688v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_J/0/1/0/all/0/1\">Jessica Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narisetty_C/0/1/0/all/0/1\">Chaitanya Narisetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>",
          "description": "We motivate and propose a suite of simple but effective improvements for\nconcept-to-text generation called SAPPHIRE: Set Augmentation and Post-hoc\nPHrase Infilling and REcombination. We demonstrate their effectiveness on\ngenerative commonsense reasoning, a.k.a. the CommonGen task, through\nexperiments using both BART and T5 models. Through extensive automatic and\nhuman evaluation, we show that SAPPHIRE noticeably improves model performance.\nAn in-depth qualitative analysis illustrates that SAPPHIRE effectively\naddresses many issues of the baseline model generations, including lack of\ncommonsense, insufficient specificity, and poor fluency.",
          "link": "http://arxiv.org/abs/2108.06643",
          "publishedOn": "2021-08-17T01:54:45.566Z",
          "wordCount": 532,
          "title": "SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation. (arXiv:2108.06643v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanqun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dineen_S/0/1/0/all/0/1\">Shay Dineen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhipeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueqing Liu</a>",
          "description": "Public security vulnerability reports (e.g., CVE reports) play an important\nrole in the maintenance of computer and network systems. Security companies and\nadministrators rely on information from these reports to prioritize tasks on\ndeveloping and deploying patches to their customers. Since these reports are\nunstructured texts, automatic information extraction (IE) can help scale up the\nprocessing by converting the unstructured reports to structured forms, e.g.,\nsoftware names and versions and vulnerability types. Existing works on\nautomated IE for security vulnerability reports often rely on a large number of\nlabeled training samples. However, creating massive labeled training set is\nboth expensive and time consuming. In this work, for the first time, we propose\nto investigate this problem where only a small number of labeled training\nsamples are available. In particular, we investigate the performance of\nfine-tuning several state-of-the-art pre-trained language models on our small\ntraining dataset. The results show that with pre-trained language models and\ncarefully tuned hyperparameters, we have reached or slightly outperformed the\nstate-of-the-art system on this task. Consistent with previous two-step process\nof first fine-tuning on main category and then transfer learning to others as\nin [7], if otherwise following our proposed approach, the number of required\nlabeled samples substantially decrease in both stages: 90% reduction in\nfine-tuning from 5758 to 576,and 88.8% reduction in transfer learning with 64\nlabeled samples per category. Our experiments thus demonstrate the\neffectiveness of few-sample learning on NER for security vulnerability report.\nThis result opens up multiple research opportunities for few-sample learning\nfor security vulnerability reports, which is discussed in the paper. Code:\nhttps://github.com/guanqun-yang/FewVulnerability.",
          "link": "http://arxiv.org/abs/2108.06590",
          "publishedOn": "2021-08-17T01:54:45.498Z",
          "wordCount": 721,
          "title": "Few-Sample Named Entity Recognition for Security Vulnerability Reports by Fine-Tuning Pre-Trained Language Models. (arXiv:2108.06590v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ernie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_A/0/1/0/all/0/1\">Alex Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>",
          "description": "We propose a shared task on training instance selection for few-shot neural\ntext generation. Large-scale pretrained language models have led to dramatic\nimprovements in few-shot text generation. Nonetheless, almost all previous work\nsimply applies random sampling to select the few-shot training instances.\nLittle to no attention has been paid to the selection strategies and how they\nwould affect model performance. The study of the selection strategy can help us\nto (1) make the most use of our annotation budget in downstream tasks and (2)\nbetter benchmark few-shot text generative models. We welcome submissions that\npresent their selection strategies and the effects on the generation quality.",
          "link": "http://arxiv.org/abs/2108.06614",
          "publishedOn": "2021-08-17T01:54:45.387Z",
          "wordCount": 553,
          "title": "The SelectGen Challenge: Finding the Best Training Samples for Few-Shot Neural Text Generation. (arXiv:2108.06614v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1\">Deuk Sin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Consistency, which refers to the capability of generating the same\npredictions for semantically similar contexts, is a highly desirable property\nfor a sound language understanding model. Although recent pretrained language\nmodels (PLMs) deliver outstanding performance in various downstream tasks, they\nshould exhibit consistent behaviour provided the models truly understand\nlanguage. In this paper, we propose a simple framework named consistency\nanalysis on language understanding models (CALUM)} to evaluate the model's\nlower-bound consistency ability. Through experiments, we confirmed that current\nPLMs are prone to generate inconsistent predictions even for semantically\nidentical inputs. We also observed that multi-task training with paraphrase\nidentification tasks is of benefit to improve consistency, increasing the\nconsistency by 13% on average.",
          "link": "http://arxiv.org/abs/2108.06665",
          "publishedOn": "2021-08-17T01:54:45.340Z",
          "wordCount": 546,
          "title": "Accurate, yet inconsistent? Consistency Analysis on Language Understanding Models. (arXiv:2108.06665v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Atul Kr. Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao-Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shatam_S/0/1/0/all/0/1\">Sheetal Shatam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fransen_T/0/1/0/all/0/1\">Theodorus Fransen</a>",
          "description": "We present the findings of the LoResMT 2021 shared task which focuses on\nmachine translation (MT) of COVID-19 data for both low-resource spoken and sign\nlanguages. The organization of this task was conducted as part of the fourth\nworkshop on technologies for machine translation of low resource languages\n(LoResMT). Parallel corpora is presented and publicly available which includes\nthe following directions: English$\\leftrightarrow$Irish,\nEnglish$\\leftrightarrow$Marathi, and Taiwanese Sign\nlanguage$\\leftrightarrow$Traditional Chinese. Training data consists of 8112,\n20933 and 128608 segments, respectively. There are additional monolingual data\nsets for Marathi and English that consist of 21901 segments. The results\npresented here are based on entries from a total of eight teams. Three teams\nsubmitted systems for English$\\leftrightarrow$Irish while five teams submitted\nsystems for English$\\leftrightarrow$Marathi. Unfortunately, there were no\nsystems submissions for the Taiwanese Sign language$\\leftrightarrow$Traditional\nChinese task. Maximum system performance was computed using BLEU and follow as\n36.0 for English--Irish, 34.6 for Irish--English, 24.2 for English--Marathi,\nand 31.3 for Marathi--English.",
          "link": "http://arxiv.org/abs/2108.06598",
          "publishedOn": "2021-08-17T01:54:45.014Z",
          "wordCount": 656,
          "title": "Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-resource Languages. (arXiv:2108.06598v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiajun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guangyu Yan</a>",
          "description": "Entity extraction is a key technology for obtaining information from massive\ntexts in natural language processing. The further interaction between them does\nnot meet the standards of human reading comprehension, thus limiting the\nunderstanding of the model, and also the omission or misjudgment of the answer\n(ie the target entity) due to the reasoning question. An effective MRC-based\nentity extraction model-MRC-I2DP, which uses the proposed gated\nattention-attracting mechanism to adjust the restoration of each part of the\ntext pair, creating problems and thinking for multi-level interactive attention\ncalculations to increase the target entity It also uses the proposed 2D\nprobability coding module, TALU function and mask mechanism to strengthen the\ndetection of all possible targets of the target, thereby improving the\nprobability and accuracy of prediction. Experiments have proved that MRC-I2DP\nrepresents an overall state-of-the-art model in 7 from the scientific and\npublic domains, achieving a performance improvement of 2.1% ~ 10.4% compared to\nthe model model in F1.",
          "link": "http://arxiv.org/abs/2108.06444",
          "publishedOn": "2021-08-17T01:54:44.992Z",
          "wordCount": 598,
          "title": "A New Entity Extraction Method Based on Machine Reading Comprehension. (arXiv:2108.06444v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratik Kumar</a>",
          "description": "With surge in online platforms, there has been an upsurge in the user\nengagement on these platforms via comments and reactions. A large portion of\nsuch textual comments are abusive, rude and offensive to the audience. With\nmachine learning systems in-place to check such comments coming onto platform,\nbiases present in the training data gets passed onto the classifier leading to\ndiscrimination against a set of classes, religion and gender. In this work, we\nevaluate different classifiers and feature to estimate the bias in these\nclassifiers along with their performance on downstream task of toxicity\nclassification. Results show that improvement in performance of automatic toxic\ncomment detection models is positively correlated to mitigating biases in these\nmodels. In our work, LSTM with attention mechanism proved to be a better\nmodelling strategy than a CNN model. Further analysis shows that fasttext\nembeddings is marginally preferable than glove embeddings on training models\nfor toxicity comment detection. Deeper analysis reveals the findings that such\nautomatic models are particularly biased to specific identity groups even\nthough the model has a high AUC score. Finally, in effort to mitigate bias in\ntoxicity detection models, a multi-task setup trained with auxiliary task of\ntoxicity sub-types proved to be useful leading to upto 0.26% (6% relative) gain\nin AUC scores.",
          "link": "http://arxiv.org/abs/2108.06487",
          "publishedOn": "2021-08-17T01:54:44.936Z",
          "wordCount": 659,
          "title": "Investigating Bias In Automatic Toxic Comment Detection: An Empirical Study. (arXiv:2108.06487v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walter_T/0/1/0/all/0/1\">Tobias Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirschner_C/0/1/0/all/0/1\">Celina Kirschner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>",
          "description": "We analyze bias in historical corpora as encoded in diachronic distributional\nsemantic models by focusing on two specific forms of bias, namely a political\n(i.e., anti-communism) and racist (i.e., antisemitism) one. For this, we use a\nnew corpus of German parliamentary proceedings, DeuPARL, spanning the period\n1867--2020. We complement this analysis of historical biases in diachronic word\nembeddings with a novel measure of bias on the basis of term co-occurrences and\ngraph-based label propagation. The results of our bias measurements align with\ncommonly perceived historical trends of antisemitic and anti-communist biases\nin German politics in different time periods, thus indicating the viability of\nanalyzing historical bias trends using semantic spaces induced from historical\ncorpora.",
          "link": "http://arxiv.org/abs/2108.06295",
          "publishedOn": "2021-08-16T00:47:33.181Z",
          "wordCount": 570,
          "title": "Diachronic Analysis of German Parliamentary Proceedings: Ideological Shifts through the Lens of Political Biases. (arXiv:2108.06295v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.01107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1\">Luis Enrico Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1\">Diane Kathryn Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Question generation (QG) is a natural language generation task where a model\nis trained to ask questions corresponding to some input text. Most recent\napproaches frame QG as a sequence-to-sequence problem and rely on additional\nfeatures and mechanisms to increase performance; however, these often increase\nmodel complexity, and can rely on auxiliary data unavailable in practical use.\nA single Transformer-based unidirectional language model leveraging transfer\nlearning can be used to produce high quality questions while disposing of\nadditional task-specific complexity. Our QG model, finetuned from GPT-2 Small,\noutperforms several paragraph-level QG baselines on the SQuAD dataset by 0.95\nMETEOR points. Human evaluators rated questions as easy to answer, relevant to\ntheir context paragraph, and corresponding well to natural human speech. Also\nintroduced is a new set of baseline scores on the RACE dataset, which has not\npreviously been used for QG tasks. Further experimentation with varying model\ncapacities and datasets with non-identification type questions is recommended\nin order to further verify the robustness of pretrained Transformer-based LMs\nas question generators.",
          "link": "http://arxiv.org/abs/2005.01107",
          "publishedOn": "2021-08-16T00:47:32.091Z",
          "wordCount": 666,
          "title": "Simplifying Paragraph-level Question Generation via Transformer Language Models. (arXiv:2005.01107v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Kelvin Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cachola_I/0/1/0/all/0/1\">Isabel Cachola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "We address the task of explaining relationships between two scientific\ndocuments using natural language text. This task requires modeling the complex\ncontent of long technical documents, deducing a relationship between these\ndocuments, and expressing the details of that relationship in text. In addition\nto the theoretical interest of this task, successful solutions can help improve\nresearcher efficiency in search and review. In this paper we establish a\ndataset of 622K examples from 154K documents. We pretrain a large language\nmodel to serve as the foundation for autoregressive approaches to the task. We\nexplore the impact of taking different views on the two documents, including\nthe use of dense representations extracted with scientific IE systems. We\nprovide extensive automatic and human evaluations which show the promise of\nsuch models, but make clear challenges for future work.",
          "link": "http://arxiv.org/abs/2002.00317",
          "publishedOn": "2021-08-16T00:47:32.080Z",
          "wordCount": 612,
          "title": "Explaining Relationships Between Scientific Documents. (arXiv:2002.00317v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>",
          "description": "Most previous methods for text data augmentation are limited to simple tasks\nand weak baselines. We explore data augmentation on hard tasks (i.e., few-shot\nnatural language understanding) and strong baselines (i.e., pretrained models\nwith over one billion parameters). Under this setting, we reproduced a large\nnumber of previous augmentation methods and found that these methods bring\nmarginal gains at best and sometimes degrade the performance much. To address\nthis challenge, we propose a novel data augmentation method FlipDA that jointly\nuses a generative model and a classifier to generate label-flipped data.\nCentral to the idea of FlipDA is the discovery that generating label-flipped\ndata is more crucial to the performance than generating label-preserved data.\nExperiments show that FlipDA achieves a good tradeoff between effectiveness and\nrobustness---it substantially improves many tasks while not negatively\naffecting the others.",
          "link": "http://arxiv.org/abs/2108.06332",
          "publishedOn": "2021-08-16T00:47:32.049Z",
          "wordCount": 571,
          "title": "FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning. (arXiv:2108.06332v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1\">Joaqu&#xed;n Silveira-Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>",
          "description": "This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as\nwell as the corresponding performance evaluations. Both models were pre-trained\nusing the largest Spanish corpus known to date, with a total of 570GB of clean\nand deduplicated text processed for this work, compiled from the web crawlings\nperformed by the National Library of Spain from 2009 to 2019. We extended the\ncurrent evaluation datasets with an extractive Question Answering dataset and\nour models outperform the existing Spanish models across tasks and settings.",
          "link": "http://arxiv.org/abs/2107.07253",
          "publishedOn": "2021-08-16T00:47:32.041Z",
          "wordCount": 547,
          "title": "Spanish Language Models. (arXiv:2107.07253v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Benefiting from large-scale pre-training, we have witnessed significant\nperformance boost on the popular Visual Question Answering (VQA) task. Despite\nrapid progress, it remains unclear whether these state-of-the-art (SOTA) models\nare robust when encountering examples in the wild. To study this, we introduce\nAdversarial VQA, a new large-scale VQA benchmark, collected iteratively via an\nadversarial human-and-model-in-the-loop procedure. Through this new benchmark,\nwe discover several interesting findings. (i) Surprisingly, we find that during\ndataset collection, non-expert annotators can easily attack SOTA VQA models\nsuccessfully. (ii) Both large-scale pre-trained models and adversarial training\nmethods achieve far worse performance on the new benchmark than over standard\nVQA v2 dataset, revealing the fragility of these models while demonstrating the\neffectiveness of our adversarial dataset. (iii) When used for data\naugmentation, our dataset can effectively boost model performance on other\nrobust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light\non robustness study in the community and serve as a valuable benchmark for\nfuture work.",
          "link": "http://arxiv.org/abs/2106.00245",
          "publishedOn": "2021-08-16T00:47:32.034Z",
          "wordCount": 649,
          "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models. (arXiv:2106.00245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resabal_J/0/1/0/all/0/1\">Jose Kristian Resabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">James Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1\">Dan John Velasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>",
          "description": "Transformers represent the state-of-the-art in Natural Language Processing\n(NLP) in recent years, proving effective even in tasks done in low-resource\nlanguages. While pretrained transformers for these languages can be made, it is\nchallenging to measure their true performance and capacity due to the lack of\nhard benchmark datasets, as well as the difficulty and cost of producing them.\nIn this paper, we present three contributions: First, we propose a methodology\nfor automatically producing Natural Language Inference (NLI) benchmark datasets\nfor low-resource languages using published news articles. Through this, we\ncreate and release NewsPH-NLI, the first sentence entailment benchmark dataset\nin the low-resource Filipino language. Second, we produce new pretrained\ntransformers based on the ELECTRA technique to further alleviate the resource\nscarcity in Filipino, benchmarking them on our dataset against other\ncommonly-used transfer learning techniques. Lastly, we perform analyses on\ntransfer learning techniques to shed light on their true performance when\noperating in low-data domains through the use of degradation tests.",
          "link": "http://arxiv.org/abs/2010.11574",
          "publishedOn": "2021-08-16T00:47:31.996Z",
          "wordCount": 675,
          "title": "Exploiting News Article Structure for Automatic Corpus Generation of Entailment Datasets. (arXiv:2010.11574v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Hannah Lei</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiqi Lu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ji_A/0/1/0/all/0/1\">Alan Ji</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bertram_E/0/1/0/all/0/1\">Emmett Bertram</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Paul Gao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Barman_A/0/1/0/all/0/1\">Arko Barman</a> (1) ((1) Rice University, Houston, United States, (2) The University of Texas Health Science Center at Houston, United States)",
          "description": "Many COVID-19 patients developed prolonged symptoms after the infection,\nincluding fatigue, delirium, and headache. The long-term health impact of these\nconditions is still not clear. It is necessary to develop a way to follow up\nwith these patients for monitoring their health status to support timely\nintervention and treatment. In the lack of sufficient human resources to follow\nup with patients, we propose a novel smart chatbot solution backed with machine\nlearning to collect information (i.e., generating digital diary) in a\npersonalized manner. In this article, we describe the design framework and\ncomponents of our prototype.",
          "link": "http://arxiv.org/abs/2103.06816",
          "publishedOn": "2021-08-16T00:47:31.979Z",
          "wordCount": 636,
          "title": "COVID-19 Smart Chatbot Prototype for Patient Monitoring. (arXiv:2103.06816v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>",
          "description": "Time is an important dimension in our physical world. Lots of facts can\nevolve with respect to time. For example, the U.S. President might change every\nfour years. Therefore, it is important to consider the time dimension and\nempower the existing QA models to reason over time. However, the existing QA\ndatasets contain rather few time-sensitive questions, hence not suitable for\ndiagnosing or benchmarking the model's temporal reasoning capability. In order\nto promote research in this direction, we propose to construct a time-sensitive\nQA dataset. The dataset is constructed by 1) mining time-evolving facts from\nWikiData and align them to their corresponding Wikipedia page, 2) employing\ncrowd workers to verify and calibrate these noisy facts, 3) generating\nquestion-answer pairs based on the annotated time-sensitive facts. Our dataset\nposes two novel challenges: 1) the model needs to understand both explicit and\nimplicit mention of time information in the long document, 2) the model needs\nto perform temporal reasoning like comparison, addition, subtraction. We\nevaluate different SoTA long-document QA systems like BigBird and FiD on our\ndataset. The best-performing model FiD can only achieve 46\\% accuracy, still\nfar behind the human performance of 87\\%. We demonstrate that these models are\nstill lacking the ability to perform robust temporal understanding and\nreasoning. Therefore, we believe that our dataset could serve as a benchmark to\nempower future studies in temporal reasoning. The dataset and code are released\nin~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.",
          "link": "http://arxiv.org/abs/2108.06314",
          "publishedOn": "2021-08-16T00:47:31.968Z",
          "wordCount": 673,
          "title": "A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadri_N/0/1/0/all/0/1\">Nima Sadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bihan Liu</a>",
          "description": "Creating abstractive summaries from meeting transcripts has proven to be\nchallenging due to the limited amount of labeled data available for training\nneural network models. Moreover, Transformer-based architectures have proven to\nbeat state-of-the-art models in summarizing news data. In this paper, we\nutilize a Transformer-based Pointer Generator Network to generate abstract\nsummaries for meeting transcripts. This model uses 2 LSTMs as an encoder and a\ndecoder, a Pointer network which copies words from the inputted text, and a\nGenerator network to produce out-of-vocabulary words (hence making the summary\nabstractive). Moreover, a coverage mechanism is used to avoid repetition of\nwords in the generated summary. First, we show that training the model on a\nnews summary dataset and using zero-shot learning to test it on the meeting\ndataset proves to produce better results than training it on the AMI meeting\ndataset. Second, we show that training this model first on out-of-domain data,\nsuch as the CNN-Dailymail dataset, followed by a fine-tuning stage on the AMI\nmeeting dataset is able to improve the performance of the model significantly.\nWe test our model on a testing set from the AMI dataset and report the ROUGE-2\nscore of the generated summary to compare with previous literature. We also\nreport the Factual score of our summaries since it is a better benchmark for\nabstractive summaries since the ROUGE-2 score is limited to measuring\nword-overlaps. We show that our improved model is able to improve on previous\nmodels by at least 5 ROUGE-2 scores, which is a substantial improvement. Also,\na qualitative analysis of the summaries generated by our model shows that these\nsummaries and human-readable and indeed capture most of the important\ninformation from the transcripts.",
          "link": "http://arxiv.org/abs/2108.06310",
          "publishedOn": "2021-08-16T00:47:31.945Z",
          "wordCount": 712,
          "title": "MeetSum: Transforming Meeting Transcript Summarization using Transformers!. (arXiv:2108.06310v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dietrich_A/0/1/0/all/0/1\">Anastasia Dietrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gressmann_F/0/1/0/all/0/1\">Frithjof Gressmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_D/0/1/0/all/0/1\">Douglas Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelombiev_I/0/1/0/all/0/1\">Ivan Chelombiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justus_D/0/1/0/all/0/1\">Daniel Justus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>",
          "description": "Identifying algorithms for computational efficient unsupervised training of\nlarge language models is an important and active area of research. In this\nwork, we develop and study a straightforward, dynamic always-sparse\npre-training approach for BERT language modeling task, which leverages periodic\ncompression steps based on magnitude pruning followed by random parameter\nre-allocation. This approach enables us to achieve Pareto improvements in terms\nof the number of floating-point operations (FLOPs) over statically sparse and\ndense models across a broad spectrum of network sizes. Furthermore, we\ndemonstrate that training remains FLOP-efficient when using coarse-grained\nblock sparsity, making it particularly promising for efficient execution on\nmodern hardware accelerators.",
          "link": "http://arxiv.org/abs/2108.06277",
          "publishedOn": "2021-08-16T00:47:31.934Z",
          "wordCount": 541,
          "title": "Towards Structured Dynamic Sparse Pre-Training of BERT. (arXiv:2108.06277v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gizinski_S/0/1/0/all/0/1\">Stanis&#x142;aw Gizinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzba_M/0/1/0/all/0/1\">Micha&#x142; Kuzba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielinski_B/0/1/0/all/0/1\">Bartosz Pielinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sienkiewicz_J/0/1/0/all/0/1\">Julian Sienkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laniewski_S/0/1/0/all/0/1\">Stanis&#x142;aw &#x141;aniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1\">Przemys&#x142;aw Biecek</a>",
          "description": "The growing number of AI applications, also for high-stake decisions,\nincreases the interest in Explainable and Interpretable Machine Learning\n(XI-ML). This trend can be seen both in the increasing number of regulations\nand strategies for developing trustworthy AI and the growing number of\nscientific papers dedicated to this topic. To ensure the sustainable\ndevelopment of AI, it is essential to understand the dynamics of the impact of\nregulation on research papers as well as the impact of scientific discourse on\nAI-related policies. This paper introduces a novel framework for joint analysis\nof AI-related policy documents and eXplainable Artificial Intelligence (XAI)\nresearch papers. The collected documents are enriched with metadata and\ninterconnections, using various NLP methods combined with a methodology\ninspired by Institutional Grammar. Based on the information extracted from\ncollected documents, we showcase a series of analyses that help understand\ninteractions, similarities, and differences between documents at different\nstages of institutionalization. To the best of our knowledge, this is the first\nwork to use automatic language analysis tools to understand the dynamics\nbetween XI-ML methods and regulations. We believe that such a system\ncontributes to better cooperation between XAI researchers and AI policymakers.",
          "link": "http://arxiv.org/abs/2108.06216",
          "publishedOn": "2021-08-16T00:47:31.920Z",
          "wordCount": 662,
          "title": "MAIR: Framework for mining relationships between research articles, strategies, and regulations in the field of explainable artificial intelligence. (arXiv:2108.06216v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gerhard_Young_G/0/1/0/all/0/1\">Greyson Gerhard-Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantha_R/0/1/0/all/0/1\">Raviteja Anantha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chappidi_S/0/1/0/all/0/1\">Srinivas Chappidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmeister_B/0/1/0/all/0/1\">Bj&#xf6;rn Hoffmeister</a>",
          "description": "Recent work building open-domain chatbots has demonstrated that increasing\nmodel size improves performance. On the other hand, latency and connectivity\nconsiderations dictate the move of digital assistants on the device. Giving a\ndigital assistant like Siri, Alexa, or Google Assistant the ability to discuss\njust about anything leads to the need for reducing the chatbot model size such\nthat it fits on the user's device. We demonstrate that low parameter models can\nsimultaneously retain their general knowledge conversational abilities while\nimproving in a specific domain. Additionally, we propose a generic framework\nthat accounts for variety in question types, tracks reference throughout\nmulti-turn conversations, and removes inconsistent and potentially toxic\nresponses. Our framework seamlessly transitions between chatting and performing\ntransactional tasks, which will ultimately make interactions with digital\nassistants more human-like. We evaluate our framework on 1 internal and 4\npublic benchmark datasets using both automatic (Perplexity) and human (SSA -\nSensibleness and Specificity Average) evaluation metrics and establish\ncomparable performance while reducing model parameters by 90%.",
          "link": "http://arxiv.org/abs/2108.06329",
          "publishedOn": "2021-08-16T00:47:31.901Z",
          "wordCount": 601,
          "title": "Low-Resource Adaptation of Open-Domain Generative Chatbots. (arXiv:2108.06329v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>",
          "description": "The advent of contextualised language models has brought gains in search\neffectiveness, not just when applied for re-ranking the output of classical\nweighting models such as BM25, but also when used directly for passage indexing\nand retrieval, a technique which is called dense retrieval. In the existing\nliterature in neural ranking, two dense retrieval families have become\napparent: single representation, where entire passages are represented by a\nsingle embedding (usually BERT's [CLS] token, as exemplified by the recent ANCE\napproach), or multiple representations, where each token in a passage is\nrepresented by its own embedding (as exemplified by the recent ColBERT\napproach). These two families have not been directly compared. However, because\nof the likely importance of dense retrieval moving forward, a clear\nunderstanding of their advantages and disadvantages is paramount. To this end,\nthis paper contributes a direct study on their comparative effectiveness,\nnoting situations where each method under/over performs w.r.t. each other, and\nw.r.t. a BM25 baseline. We observe that, while ANCE is more efficient than\nColBERT in terms of response time and memory usage, multiple representations\nare statistically more effective than the single representations for MAP and\nMRR@10. We also show that multiple representations obtain better improvements\nthan single representations for queries that are the hardest for BM25, as well\nas for definitional queries, and those with complex information needs.",
          "link": "http://arxiv.org/abs/2108.06279",
          "publishedOn": "2021-08-16T00:47:31.842Z",
          "wordCount": 668,
          "title": "On Single and Multiple Representations in Dense Passage Retrieval. (arXiv:2108.06279v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolova_M/0/1/0/all/0/1\">Marina Sokolova</a>",
          "description": "Reddit.com is a popular social media platform among young people. Reddit\nusers share their stories to seek support from other users, especially during\nthe Covid-19 pandemic. Messages posted on Reddit and their content have\nprovided researchers with opportunity to analyze public concerns. In this\nstudy, we analyzed sentiments of COVID-related messages posted on r/Depression.\nOur study poses the following questions: a) What are the common topics that the\nReddit users discuss? b) Can we use these topics to classify sentiments of the\nposts? c) What matters concern people more during the pandemic?\n\nKey Words: Sentiment Classification, Depression, COVID-19, Reddit, LDA, BERT",
          "link": "http://arxiv.org/abs/2108.06215",
          "publishedOn": "2021-08-16T00:47:31.787Z",
          "wordCount": 598,
          "title": "Sentiment Analysis of the COVID-related r/Depression Posts. (arXiv:2108.06215v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_P/0/1/0/all/0/1\">Prithiviraj Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhkaran Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achankuju_J/0/1/0/all/0/1\">Josemon Achankuju</a>",
          "description": "We present VESPA, an intentionally simple yet novel zero-shot system for\nlayout, locale, and domain agnostic document extraction. In spite of the\navailability of large corpora of documents, the lack of labeled and validated\ndatasets makes it a challenge to discriminatively train document extraction\nmodels for enterprises. We show that this problem can be addressed by simply\ntransferring the information extraction (IE) task to a natural language\nQuestion-Answering (QA) task without engineering task-specific architectures.\nWe demonstrate the effectiveness of our system by evaluating on a closed corpus\nof real-world retail and tax invoices with multiple complex layouts, domains,\nand geographies. The empirical evaluation shows that our system outperforms 4\nprominent commercial invoice solutions that use discriminatively trained models\nwith architectures specifically crafted for invoice extraction. We extracted 6\nfields with zero upfront human annotation or training with an Avg. F1 of 87.50.",
          "link": "http://arxiv.org/abs/2108.06069",
          "publishedOn": "2021-08-16T00:47:31.761Z",
          "wordCount": 585,
          "title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA Ensemble. (arXiv:2108.06069v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1\">Tapas Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting triplets\nof aspect terms, their associated sentiments, and the opinion terms that\nprovide evidence for the expressed sentiments. Previous approaches to ASTE\nusually simultaneously extract all three components or first identify the\naspect and opinion terms, then pair them up to predict their sentiment\npolarities. In this work, we present a novel paradigm, ASTE-RL, by regarding\nthe aspect and opinion terms as arguments of the expressed sentiment in a\nhierarchical reinforcement learning (RL) framework. We first focus on\nsentiments expressed in a sentence, then identify the target aspect and opinion\nterms for that sentiment. This takes into account the mutual interactions among\nthe triplet's components while improving exploration and sample efficiency.\nFurthermore, this hierarchical RLsetup enables us to deal with multiple and\noverlapping triplets. In our experiments, we evaluate our model on existing\ndatasets from laptop and restaurant domains and show that it achieves\nstate-of-the-art performance. The implementation of this work is publicly\navailable at https://github.com/declare-lab/ASTE-RL.",
          "link": "http://arxiv.org/abs/2108.06107",
          "publishedOn": "2021-08-16T00:47:31.734Z",
          "wordCount": 606,
          "title": "Aspect Sentiment Triplet Extraction Using Reinforcement Learning. (arXiv:2108.06107v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>",
          "description": "Both latent semantic analysis (LSA) and correspondence analysis (CA) use a\nsingular value decomposition (SVD) for dimensionality reduction. In this\narticle, LSA and CA are compared from a theoretical point of view and applied\nin both a toy example and an authorship attribution example. In text mining\ninterest goes out to the relationships among documents and terms: for example,\nwhat terms are more often used in what documents. However, the LSA solution\ndisplays a mix of marginal effects and these relationships. It appears that CA\nhas more attractive properties than LSA. One such property is that, in CA, the\neffect of the margins is effectively eliminated, so that the CA solution is\noptimally suited to focus on the relationships among documents and terms. Three\nmechanisms are distinguished to weight documents and terms, and a unifying\nframework is proposed that includes these three mechanisms and includes both CA\nand LSA as special cases. In the authorship attribution example, the national\nanthem of the Netherlands, the application of the discussed methods is\nillustrated.",
          "link": "http://arxiv.org/abs/2108.06197",
          "publishedOn": "2021-08-16T00:47:31.709Z",
          "wordCount": 621,
          "title": "A Comparison of Latent Semantic Analysis and Correspondence Analysis for Text Mining. (arXiv:2108.06197v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutsch_J/0/1/0/all/0/1\">Julian Gutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietsch_M/0/1/0/all/0/1\">Malte Pietsch</a>",
          "description": "The evaluation of question answering models compares ground-truth annotations\nwith model predictions. However, as of today, this comparison is mostly\nlexical-based and therefore misses out on answers that have no lexical overlap\nbut are still semantically similar, thus treating correct answers as false.\nThis underestimation of the true performance of models hinders user acceptance\nin applications and complicates a fair comparison of different models.\nTherefore, there is a need for an evaluation metric that is based on semantics\ninstead of pure string similarity. In this short paper, we present SAS, a\ncross-encoder-based metric for the estimation of semantic answer similarity,\nand compare it to seven existing metrics. To this end, we create an English and\na German three-way annotated evaluation dataset containing pairs of answers\nalong with human judgment of their semantic similarity, which we release along\nwith an implementation of the SAS metric and the experiments. We find that\nsemantic similarity metrics based on recent transformer models correlate much\nbetter with human judgment than traditional lexical similarity metrics on our\ntwo newly created datasets and one dataset from related work.",
          "link": "http://arxiv.org/abs/2108.06130",
          "publishedOn": "2021-08-16T00:47:31.701Z",
          "wordCount": 622,
          "title": "Semantic Answer Similarity for Evaluating Question Answering Models. (arXiv:2108.06130v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>",
          "description": "Hateful and offensive content detection has been extensively explored in a\nsingle modality such as text. However, such toxic information could also be\ncommunicated via multimodal content such as online memes. Therefore, detecting\nmultimodal hateful content has recently garnered much attention in academic and\nindustry research communities. This paper aims to contribute to this emerging\nresearch topic by proposing DisMultiHate, which is a novel framework that\nperformed the classification of multimodal hateful content. Specifically,\nDisMultiHate is designed to disentangle target entities in multimodal memes to\nimprove hateful content classification and explainability. We conduct extensive\nexperiments on two publicly available hateful and offensive memes datasets. Our\nexperiment results show that DisMultiHate is able to outperform\nstate-of-the-art unimodal and multimodal baselines in the hateful meme\nclassification task. Empirical case studies were also conducted to demonstrate\nDisMultiHate's ability to disentangle target entities in memes and ultimately\nshowcase DisMultiHate's explainability of the multimodal hateful content\nclassification task.",
          "link": "http://arxiv.org/abs/2108.06207",
          "publishedOn": "2021-08-16T00:47:31.670Z",
          "wordCount": 597,
          "title": "Disentangling Hate in Online Memes. (arXiv:2108.06207v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shangwen Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">QiaoQiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Recently, dense passage retrieval has become a mainstream approach to finding\nrelevant information in various natural language processing tasks. A number of\nstudies have been devoted to improving the widely adopted dual-encoder\narchitecture. However, most of the previous studies only consider query-centric\nsimilarity relation when learning the dual-encoder retriever. In order to\ncapture more comprehensive similarity relations, we propose a novel approach\nthat leverages both query-centric and PAssage-centric sImilarity Relations\n(called PAIR) for dense passage retrieval. To implement our approach, we make\nthree major technical contributions by introducing formal formulations of the\ntwo kinds of similarity relations, generating high-quality pseudo labeled data\nvia knowledge distillation, and designing an effective two-stage training\nprocedure that incorporates passage-centric similarity relation constraint.\nExtensive experiments show that our approach significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions datasets.",
          "link": "http://arxiv.org/abs/2108.06027",
          "publishedOn": "2021-08-16T00:47:31.565Z",
          "wordCount": 595,
          "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>",
          "description": "Reasoning on the knowledge graph (KG) aims to infer new facts from existing\nones. Methods based on the relational path in the literature have shown strong,\ninterpretable, and inductive reasoning ability. However, the paths are\nnaturally limited in capturing complex topology in KG. In this paper, we\nintroduce a novel relational structure, i.e., relational directed graph\n(r-digraph), which is composed of overlapped relational paths, to capture the\nKG's structural information. Since the digraph exhibits more complex structure\nthan paths, constructing and learning on the r-digraph are challenging. Here,\nwe propose a variant of graph neural network, i.e., RED-GNN, to address the\nabove challenges by learning the RElational Digraph with a variant of GNN.\nSpecifically, RED-GNN recursively encodes multiple r-digraphs with shared edges\nand selects the strongly correlated edges through query-dependent attention\nweights. We demonstrate the significant gains on reasoning both KG with unseen\nentities and incompletion KG benchmarks by the r-digraph, the efficiency of\nRED-GNN, and the interpretable dependencies learned on the r-digraph.",
          "link": "http://arxiv.org/abs/2108.06040",
          "publishedOn": "2021-08-16T00:47:31.552Z",
          "wordCount": 591,
          "title": "Knowledge Graph Reasoning with Relational Directed Graph. (arXiv:2108.06040v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>",
          "description": "Ranking models have achieved promising results, but it remains challenging to\ndesign personalized ranking systems to leverage user profiles and semantic\nrepresentations between queries and documents. In this paper, we propose a\ntopic-based personalized ranking model (TPRM) that integrates user topical\nprofile with pretrained contextualized term representations to tailor the\ngeneral document ranking list. Experiments on the real-world dataset\ndemonstrate that TPRM outperforms state-of-the-art ad-hoc ranking models and\npersonalized ranking models significantly.",
          "link": "http://arxiv.org/abs/2108.06014",
          "publishedOn": "2021-08-16T00:47:31.535Z",
          "wordCount": 511,
          "title": "TPRM: A Topic-based Personalized Ranking Model for Web Search. (arXiv:2108.06014v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandla_T/0/1/0/all/0/1\">Thomas Mandla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modha_S/0/1/0/all/0/1\">Sandip Modha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Amit Kumar Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandini_D/0/1/0/all/0/1\">Durgesh Nandini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Daksh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schafer_J/0/1/0/all/0/1\">Johannes Sch&#xe4;fer</a>",
          "description": "With the growth of social media, the spread of hate speech is also increasing\nrapidly. Social media are widely used in many countries. Also Hate Speech is\nspreading in these countries. This brings a need for multilingual Hate Speech\ndetection algorithms. Much research in this area is dedicated to English at the\nmoment. The HASOC track intends to provide a platform to develop and optimize\nHate Speech detection algorithms for Hindi, German and English. The dataset is\ncollected from a Twitter archive and pre-classified by a machine learning\nsystem. HASOC has two sub-task for all three languages: task A is a binary\nclassification problem (Hate and Not Offensive) while task B is a fine-grained\nclassification problem for three classes (HATE) Hate speech, OFFENSIVE and\nPROFANITY. Overall, 252 runs were submitted by 40 teams. The performance of the\nbest classification algorithms for task A are F1 measures of 0.51, 0.53 and\n0.52 for English, Hindi, and German, respectively. For task B, the best\nclassification algorithms achieved F1 measures of 0.26, 0.33 and 0.29 for\nEnglish, Hindi, and German, respectively. This article presents the tasks and\nthe data development as well as the results. The best performing algorithms\nwere mainly variants of the transformer architecture BERT. However, also other\nsystems were applied with good success",
          "link": "http://arxiv.org/abs/2108.05927",
          "publishedOn": "2021-08-16T00:47:31.505Z",
          "wordCount": 680,
          "title": "Overview of the HASOC track at FIRE 2020: Hate Speech and Offensive Content Identification in Indo-European Languages. (arXiv:2108.05927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Meizhen Ding</a>",
          "description": "Query expansion with pseudo-relevance feedback (PRF) is a powerful approach\nto enhance the effectiveness in information retrieval. Recently, with the rapid\nadvance of deep learning techniques, neural text generation has achieved\npromising success in many natural language tasks. To leverage the strength of\ntext generation for information retrieval, in this article, we propose a novel\napproach which effectively integrates text generation models into PRF-based\nquery expansion. In particular, our approach generates augmented query terms\nvia neural text generation models conditioned on both the initial query and\npseudo-relevance feedback. Moreover, in order to train the generative model, we\nadopt the conditional generative adversarial nets (CGANs) and propose the\nPRF-CGAN method in which both the generator and the discriminator are\nconditioned on the pseudo-relevance feedback. We evaluate the performance of\nour approach on information retrieval tasks using two benchmark datasets. The\nexperimental results show that our approach achieves comparable performance or\noutperforms traditional query expansion methods on both the retrieval and\nreranking tasks.",
          "link": "http://arxiv.org/abs/2108.06010",
          "publishedOn": "2021-08-16T00:47:31.397Z",
          "wordCount": 598,
          "title": "GQE-PRF: Generative Query Expansion with Pseudo-Relevance Feedback. (arXiv:2108.06010v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertram Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>",
          "description": "Detecting online hate is a complex task, and low-performing detection models\nhave harmful consequences when used for sensitive applications such as content\nmoderation. Emoji-based hate is a key emerging challenge for online hate\ndetection. We present HatemojiCheck, a test suite of 3,930 short-form\nstatements that allows us to evaluate how detection models perform on hateful\nlanguage expressed with emoji. Using the test suite, we expose weaknesses in\nexisting hate detection models. To address these weaknesses, we create the\nHatemojiTrain dataset using an innovative human-and-model-in-the-loop approach.\nModels trained on these 5,912 adversarial examples perform substantially better\nat detecting emoji-based hate, while retaining strong performance on text-only\nhate. Both HatemojiCheck and HatemojiTrain are made publicly available.",
          "link": "http://arxiv.org/abs/2108.05921",
          "publishedOn": "2021-08-16T00:47:31.347Z",
          "wordCount": 564,
          "title": "Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (arXiv:2108.05921v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and show excellent\nresults, in particular for rare answers. Furthermore, we demonstrate our method\nto significantly outperform the state of the art on MSRVTT-QA, MSVD-QA,\nActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce\niVQA, a new VideoQA dataset with reduced language biases and high-quality\nredundant manual annotations. Our code, datasets and trained models are\navailable at https://antoyang.github.io/just-ask.html.",
          "link": "http://arxiv.org/abs/2012.00451",
          "publishedOn": "2021-08-13T01:56:55.083Z",
          "wordCount": 684,
          "title": "Just Ask: Learning to Answer Questions from Millions of Narrated Videos. (arXiv:2012.00451v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiatong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chunyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1\">Christian Fuegen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>",
          "description": "As speech-enabled devices such as smartphones and smart speakers become\nincreasingly ubiquitous, there is growing interest in building automatic speech\nrecognition (ASR) systems that can run directly on-device; end-to-end (E2E)\nspeech recognition models such as recurrent neural network transducers and\ntheir variants have recently emerged as prime candidates for this task. Apart\nfrom being accurate and compact, such systems need to decode speech with low\nuser-perceived latency (UPL), producing words as soon as they are spoken. This\nwork examines the impact of various techniques - model architectures, training\ncriteria, decoding hyperparameters, and endpointer parameters - on UPL. Our\nanalyses suggest that measures of model size (parameters, input chunk sizes),\nor measures of computation (e.g., FLOPS, RTF) that reflect the model's ability\nto process input frames are not always strongly correlated with observed UPL.\nThus, conventional algorithmic latency measurements might be inadequate in\naccurately capturing latency observed when models are deployed on embedded\ndevices. Instead, we find that factors affecting token emission latency, and\nendpointing behavior have a larger impact on UPL. We achieve the best trade-off\nbetween latency and word error rate when performing ASR jointly with\nendpointing, while utilizing the recently proposed alignment regularization\nmechanism.",
          "link": "http://arxiv.org/abs/2104.02207",
          "publishedOn": "2021-08-13T01:56:54.790Z",
          "wordCount": 696,
          "title": "Dissecting User-Perceived Latency of On-Device E2E Speech Recognition. (arXiv:2104.02207v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Word alignment over parallel corpora has a wide variety of applications,\nincluding learning translation lexicons, cross-lingual transfer of language\nprocessing tools, and automatic evaluation or analysis of translation outputs.\nThe great majority of past work on word alignment has worked by performing\nunsupervised learning on parallel texts. Recently, however, other work has\ndemonstrated that pre-trained contextualized word embeddings derived from\nmultilingually trained language models (LMs) prove an attractive alternative,\nachieving competitive results on the word alignment task even in the absence of\nexplicit training on parallel data. In this paper, we examine methods to marry\nthe two approaches: leveraging pre-trained LMs but fine-tuning them on parallel\ntext with objectives designed to improve alignment quality, and proposing\nmethods to effectively extract alignments from these fine-tuned models. We\nperform experiments on five language pairs and demonstrate that our model can\nconsistently outperform previous state-of-the-art models of all varieties. In\naddition, we demonstrate that we are able to train multilingual word aligners\nthat can obtain robust performance on different language pairs. Our aligner,\nAWESOME (Aligning Word Embedding Spaces of Multilingual Encoders), with\npre-trained models is available at https://github.com/neulab/awesome-align",
          "link": "http://arxiv.org/abs/2101.08231",
          "publishedOn": "2021-08-13T01:56:54.776Z",
          "wordCount": 662,
          "title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora. (arXiv:2101.08231v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyubok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>",
          "description": "Accurate terminology translation is crucial for ensuring the practicality and\nreliability of neural machine translation (NMT) systems. To address this,\nlexically constrained NMT explores various methods to ensure pre-specified\nwords and phrases appear in the translation output. However, in many cases,\nthose methods are studied on general domain corpora, where the terms are mostly\nuni- and bi-grams (>98%). In this paper, we instead tackle a more challenging\nsetup consisting of domain-specific corpora with much longer n-gram and highly\nspecialized terms. Inspired by the recent success of masked span prediction\nmodels, we propose a simple and effective training strategy that achieves\nconsistent improvements on both terminology and sentence-level translation for\nthree domain-specific corpora in two language pairs.",
          "link": "http://arxiv.org/abs/2105.05498",
          "publishedOn": "2021-08-13T01:56:54.769Z",
          "wordCount": 607,
          "title": "Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction. (arXiv:2105.05498v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_A/0/1/0/all/0/1\">Alexander Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiferet_D/0/1/0/all/0/1\">Doron Tiferet</a>",
          "description": "An automaton is unambiguous if for every input it has at most one accepting\ncomputation. An automaton is k-ambiguous (for k > 0) if for every input it has\nat most k accepting computations. An automaton is boundedly ambiguous if it is\nk-ambiguous for some $k \\in \\mathbb{N}$. An automaton is finitely\n(respectively, countably) ambiguous if for every input it has at most finitely\n(respectively, countably) many accepting computations.\n\nThe degree of ambiguity of a regular language is defined in a natural way. A\nlanguage is k-ambiguous (respectively, boundedly, finitely, countably\nambiguous) if it is accepted by a k-ambiguous (respectively, boundedly,\nfinitely, countably ambiguous) automaton. Over finite words every regular\nlanguage is accepted by a deterministic automaton. Over finite trees every\nregular language is accepted by an unambiguous automaton. Over $\\omega$-words\nevery regular language is accepted by an unambiguous B\\\"uchi automaton and by a\ndeterministic parity automaton. Over infinite trees Carayol et al. showed that\nthere are ambiguous languages.\n\nWe show that over infinite trees there is a hierarchy of degrees of\nambiguity: For every k > 1 there are k-ambiguous languages that are not k - 1\nambiguous; and there are finitely (respectively countably, uncountably)\nambiguous languages that are not boundedly (respectively finitely, countably)\nambiguous.",
          "link": "http://arxiv.org/abs/2009.02985",
          "publishedOn": "2021-08-13T01:56:54.704Z",
          "wordCount": 688,
          "title": "Ambiguity Hierarchy of Regular Infinite Tree Languages. (arXiv:2009.02985v3 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu C. Aggarwal</a>",
          "description": "Existing text style transfer (TST) methods rely on style classifiers to\ndisentangle the text's content and style attributes for text style transfer.\nWhile the style classifier plays a critical role in existing TST methods, there\nis no known investigation on its effect on the TST methods. In this paper, we\nconduct an empirical study on the limitations of the style classifiers used in\nexisting TST methods. We demonstrate that the existing style classifiers cannot\nlearn sentence syntax effectively and ultimately worsen existing TST models'\nperformance. To address this issue, we propose a novel Syntax-Aware\nControllable Generation (SACG) model, which includes a syntax-aware style\nclassifier that ensures learned style latent representations effectively\ncapture the syntax information for TST. Through extensive experiments on two\npopular TST tasks, we show that our proposed method significantly outperforms\nthe state-of-the-art methods. Our case studies have also demonstrated SACG's\nability to generate fluent target-style sentences that preserved the original\ncontent.",
          "link": "http://arxiv.org/abs/2108.05869",
          "publishedOn": "2021-08-13T01:56:54.689Z",
          "wordCount": 585,
          "title": "Syntax Matters! Syntax-Controlled in Text Style Transfer. (arXiv:2108.05869v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.14891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kori_M/0/1/0/all/0/1\">Mayuko Kori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_T/0/1/0/all/0/1\">Takeshi Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_N/0/1/0/all/0/1\">Naoki Kobayashi</a>",
          "description": "A cyclic proof system allows us to perform inductive reasoning without\nexplicit inductions. We propose a cyclic proof system for HFLN, which is a\nhigher-order predicate logic with natural numbers and alternating fixed-points.\nOurs is the first cyclic proof system for a higher-order logic, to our\nknowledge. Due to the presence of higher-order predicates and alternating\nfixed-points, our cyclic proof system requires a more delicate global condition\non cyclic proofs than the original system of Brotherston and Simpson. We prove\nthe decidability of checking the global condition and soundness of this system,\nand also prove a restricted form of standard completeness for an infinitary\nvariant of our cyclic proof system. A potential application of our cyclic proof\nsystem is semi-automated verification of higher-order programs, based on\nKobayashi et al.'s recent work on reductions from program verification to HFLN\nvalidity checking.",
          "link": "http://arxiv.org/abs/2010.14891",
          "publishedOn": "2021-08-13T01:56:54.684Z",
          "wordCount": 617,
          "title": "A Cyclic Proof System for HFLN. (arXiv:2010.14891v3 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.11057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qinliang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijia Zhang</a>",
          "description": "Textual network embeddings aim to learn a low-dimensional representation for\nevery node in the network so that both the structural and textual information\nfrom the networks can be well preserved in the representations. Traditionally,\nthe structural and textual embeddings were learned by models that rarely take\nthe mutual influences between them into account. In this paper, a deep neural\narchitecture is proposed to effectively fuse the two kinds of informations into\none representation. The novelties of the proposed architecture are manifested\nin the aspects of a newly defined objective function, the complementary\ninformation fusion method for structural and textual features, and the mutual\ngate mechanism for textual feature extraction. Experimental results show that\nthe proposed model outperforms the comparing methods on all three datasets.",
          "link": "http://arxiv.org/abs/1908.11057",
          "publishedOn": "2021-08-13T01:56:54.678Z",
          "wordCount": 622,
          "title": "A Deep Neural Information Fusion Architecture for Textual Network Embeddings. (arXiv:1908.11057v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.05759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockett_C/0/1/0/all/0/1\">Chris Brockett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>",
          "description": "Generating responses that are consistent with the dialogue context is one of\nthe central challenges in building engaging conversational agents. We\ndemonstrate that neural conversation models can be geared towards generating\nconsistent responses by maintaining certain features related to topics and\npersonas throughout the conversation. Past work has required external\nsupervision that exploits features such as user identities that are often\nunavailable. In our approach, topic and persona feature extractors are trained\nusing a contrastive training scheme that utilizes the natural structure of\ndialogue data. We further adopt a feature disentangling loss which, paired with\ncontrollable response generation techniques, allows us to promote or demote\ncertain learned topics and persona features. Evaluation results demonstrate the\nmodel's ability to capture meaningful topics and persona features. The\nincorporation of the learned features brings significant improvement in terms\nof the quality of generated responses on two dialogue datasets.",
          "link": "http://arxiv.org/abs/1903.05759",
          "publishedOn": "2021-08-13T01:56:54.660Z",
          "wordCount": 641,
          "title": "Consistent Dialogue Generation with Self-supervised Feature Learning. (arXiv:1903.05759v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>",
          "description": "Graph-based methods are popular in dependency parsing for decades. Recently,\n\\citet{yang2021headed} propose a headed span-based method. Both of them score\nall possible trees and globally find the highest-scoring tree. In this paper,\nwe combine these two kinds of methods, designing several dynamic programming\nalgorithms for joint inference. Experiments show the effectiveness of our\nproposed methods\\footnote{Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.}.",
          "link": "http://arxiv.org/abs/2108.05838",
          "publishedOn": "2021-08-13T01:56:54.653Z",
          "wordCount": 495,
          "title": "Combining (second-order) graph-based and headed span-based projective dependency parsing. (arXiv:2108.05838v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jennifer Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaslavsky_N/0/1/0/all/0/1\">Noga Zaslavsky</a>",
          "description": "Models of context-sensitive communication often use the Rational Speech Act\nframework (RSA; Frank & Goodman, 2012), which formulates listeners and speakers\nin a cooperative reasoning process. However, the standard RSA formulation can\nonly be applied to small domains, and large-scale applications have relied on\nimitating human behavior. Here, we propose a new approach to scalable\npragmatics, building upon recent theoretical results (Zaslavsky et al., 2020)\nthat characterize pragmatic reasoning in terms of general information-theoretic\nprinciples. Specifically, we propose an architecture and learning process in\nwhich agents acquire pragmatic policies via self-supervision instead of\nimitating human data. This work suggests a new principled approach for\nequipping artificial agents with pragmatic skills via self-supervision, which\nis grounded both in pragmatic theory and in information theory.",
          "link": "http://arxiv.org/abs/2108.05799",
          "publishedOn": "2021-08-13T01:56:54.646Z",
          "wordCount": 558,
          "title": "Scalable pragmatic communication via self-supervision. (arXiv:2108.05799v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dixit_S/0/1/0/all/0/1\">Sharad Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulwad_V/0/1/0/all/0/1\">Varish Mulwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1\">Abhinav Saxena</a>",
          "description": "Rapid progress in natural language processing has led to its utilization in a\nvariety of industrial and enterprise settings, including in its use for\ninformation extraction, specifically named entity recognition and relation\nextraction, from documents such as engineering manuals and field maintenance\nreports. While named entity recognition is a well-studied problem, existing\nstate-of-the-art approaches require large labelled datasets which are hard to\nacquire for sensitive data such as maintenance records. Further, industrial\ndomain experts tend to distrust results from black box machine learning models,\nespecially when the extracted information is used in downstream predictive\nmaintenance analytics. We overcome these challenges by developing three\napproaches built on the foundation of domain expert knowledge captured in\ndictionaries and ontologies. We develop a syntactic and semantic rules-based\napproach and an approach leveraging a pre-trained language model, fine-tuned\nfor a question-answering task on top of our base dictionary lookup to extract\nentities of interest from maintenance records. We also develop a preliminary\nontology to represent and capture the semantics of maintenance records. Our\nevaluations on a real-world aviation maintenance records dataset show promising\nresults and help identify challenges specific to named entity recognition in\nthe context of noisy industrial data.",
          "link": "http://arxiv.org/abs/2108.05454",
          "publishedOn": "2021-08-13T01:56:54.638Z",
          "wordCount": 643,
          "title": "Extracting Semantics from Maintenance Records. (arXiv:2108.05454v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1\">Verna Dankers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1\">Elia Bruni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>",
          "description": "Moving towards human-like linguistic performance is often argued to require\ncompositional generalisation. Whether neural networks exhibit this ability is\ntypically studied using artificial languages, for which the compositionality of\ninput fragments can be guaranteed and their meanings algebraically composed.\nHowever, compositionality in natural language is vastly more complex than this\nrigid, arithmetics-like version of compositionality, and as such artificial\ncompositionality tests do not allow us to draw conclusions about how neural\nmodels deal with compositionality in more realistic scenarios. In this work, we\nre-instantiate three compositionality tests from the literature and reformulate\nthem for neural machine translation (NMT). The results highlight two main\nissues: the inconsistent behaviour of NMT models and their inability to\n(correctly) modulate between local and global processing. Aside from an\nempirical study, our work is a call to action: we should rethink the evaluation\nof compositionality in neural networks of natural language, where composing\nmeaning is not as straightforward as doing the math.",
          "link": "http://arxiv.org/abs/2108.05885",
          "publishedOn": "2021-08-13T01:56:54.622Z",
          "wordCount": 606,
          "title": "The paradox of the compositionality of natural language: a neural machine translation case study. (arXiv:2108.05885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guriel_D/0/1/0/all/0/1\">David Guriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>",
          "description": "In the domain of Morphology, Inflection is a fundamental and important task\nthat gained a lot of traction in recent years, mostly via SIGMORPHON's\nshared-tasks. With average accuracy above 0.9 over the scores of all languages,\nthe task is considered mostly solved using relatively generic neural\nsequence-to-sequence models, even with little data provided. In this work, we\npropose to re-evaluate morphological inflection models by employing harder\ntrain-test splits that will challenge the generalization capacity of the\nmodels. In particular, as opposed to the na\\\"ive split-by-form, we propose a\nsplit-by-lemma method to challenge the performance on existing benchmarks. Our\nexperiments with the three top-ranked systems on the SIGMORPHON's 2020\nshared-task show that the lemma-split presents an average drop of 30 percentage\npoints in macro-average for the 90 languages included. The effect is most\nsignificant for low-resourced languages with a drop as high as 95 points, but\neven high-resourced languages lose about 10 points on average. Our results\nclearly show that generalizing inflection to unseen lemmas is far from being\nsolved, presenting a simple yet effective means to promote more sophisticated\nmodels.",
          "link": "http://arxiv.org/abs/2108.05682",
          "publishedOn": "2021-08-13T01:56:54.616Z",
          "wordCount": 611,
          "title": "(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance. (arXiv:2108.05682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumari_S/0/1/0/all/0/1\">Sushma Kumari</a>",
          "description": "Much research has been done for debunking and analysing fake news. Many\nresearchers study fake news detection in the last year, but many are limited to\nsocial media data. Currently, multiples fact-checkers are publishing their\nresults in various formats. Also, multiple fact-checkers use different labels\nfor the fake news, making it difficult to make a generalisable classifier. With\nthe merge classes, the performance of the machine model can be enhanced. This\ndomain categorisation will help group the article, which will help save the\nmanual effort in assigning the claim verification. In this paper, we have\npresented BERT based classification model to predict the domain and\nclassification. We have also used additional data from fact-checked articles.\nWe have achieved a macro F1 score of 83.76 % for Task 3Aand 85.55 % for Task 3B\nusing the additional training data.",
          "link": "http://arxiv.org/abs/2108.05419",
          "publishedOn": "2021-08-13T01:56:54.570Z",
          "wordCount": 574,
          "title": "NoFake at CheckThat! 2021: Fake News Detection Using BERT. (arXiv:2108.05419v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1\">Anson Bastos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadgeri_A/0/1/0/all/0/1\">Abhishek Nadgeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarpour_S/0/1/0/all/0/1\">Saeedeh Shekarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulang_I/0/1/0/all/0/1\">Isaiah Onando Mulang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffart_J/0/1/0/all/0/1\">Johannes Hoffart</a>",
          "description": "Recently, several Knowledge Graph Embedding (KGE) approaches have been\ndevised to represent entities and relations in dense vector space and employed\nin downstream tasks such as link prediction. A few KGE techniques address\ninterpretability, i.e., mapping the connectivity patterns of the relations\n(i.e., symmetric/asymmetric, inverse, and composition) to a geometric\ninterpretation such as rotations. Other approaches model the representations in\nhigher dimensional space such as four-dimensional space (4D) to enhance the\nability to infer the connectivity patterns (i.e., expressiveness). However,\nmodeling relation and entity in a 4D space often comes at the cost of\ninterpretability. This paper proposes HopfE, a novel KGE approach aiming to\nachieve the interpretability of inferred relations in the four-dimensional\nspace. We first model the structural embeddings in 3D Euclidean space and view\nthe relation operator as an SO(3) rotation. Next, we map the entity embedding\nvector from a 3D space to a 4D hypersphere using the inverse Hopf Fibration, in\nwhich we embed the semantic information from the KG ontology. Thus, HopfE\nconsiders the structural and semantic properties of the entities without losing\nexpressivity and interpretability. Our empirical results on four well-known\nbenchmarks achieve state-of-the-art performance for the KG completion task.",
          "link": "http://arxiv.org/abs/2108.05774",
          "publishedOn": "2021-08-13T01:56:54.561Z",
          "wordCount": 656,
          "title": "HopfE: Knowledge Graph Representation Learning using Inverse Hopf Fibrations. (arXiv:2108.05774v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castel_O/0/1/0/all/0/1\">Or Castel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>",
          "description": "Fine-tuned language models use greedy decoding to answer reading\ncomprehension questions with relative success. However, this approach does not\nensure that the answer is a span in the given passage, nor does it guarantee\nthat it is the most probable one. Does greedy decoding actually perform worse\nthan an algorithm that does adhere to these properties? To study the\nperformance and optimality of greedy decoding, we present exact-extract, a\ndecoding algorithm that efficiently finds the most probable answer span in the\ncontext. We compare the performance of T5 with both decoding algorithms on\nzero-shot and few-shot extractive question answering. When no training examples\nare available, exact-extract significantly outperforms greedy decoding.\nHowever, greedy decoding quickly converges towards the performance of\nexact-extract with the introduction of a few training examples, becoming more\nextractive and increasingly likelier to generate the most probable span as the\ntraining set grows. We also show that self-supervised training can bias the\nmodel towards extractive behavior, increasing performance in the zero-shot\nsetting without resorting to annotated examples. Overall, our results suggest\nthat pretrained language models are so good at adapting to extractive question\nanswering, that it is often enough to fine-tune on a small training set for the\ngreedy algorithm to emulate the optimal decoding strategy.",
          "link": "http://arxiv.org/abs/2108.05857",
          "publishedOn": "2021-08-13T01:56:54.546Z",
          "wordCount": 646,
          "title": "How Optimal is Greedy Decoding for Extractive Question Answering?. (arXiv:2108.05857v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Portenoy_J/0/1/0/all/0/1\">Jason Portenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radensky_M/0/1/0/all/0/1\">Marissa Radensky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_J/0/1/0/all/0/1\">Jevin West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>",
          "description": "Scientific silos can hinder innovation. These information \"filter bubbles\"\nand the growing challenge of information overload limit awareness across the\nliterature, making it difficult to keep track of even narrow areas of interest,\nlet alone discover new ones. Algorithmic curation and recommendation, which\noften prioritize relevance, can further reinforce these bubbles. In response,\nwe describe Bridger, a system for facilitating discovery of scholars and their\nwork, to explore design tradeoffs among relevant and novel recommendations. We\nconstruct a faceted representation of authors using information extracted from\ntheir papers and inferred personas. We explore approaches both for recommending\nnew content and for displaying it in a manner that helps researchers to\nunderstand the work of authors who they are unfamiliar with. In studies with\ncomputer science researchers, our approach substantially improves users'\nabilities to do so. We develop an approach that locates commonalities and\ncontrasts between scientists---retrieving partially similar authors, rather\nthan aiming for strict similarity. We find this approach helps users discover\nauthors useful for generating novel research ideas of relevance to their work,\nat a higher rate than a state-of-art neural model. Our analysis reveals that\nBridger connects authors who have different citation profiles, publish in\ndifferent venues, and are more distant in social co-authorship networks,\nraising the prospect of bridging diverse communities and facilitating\ndiscovery.",
          "link": "http://arxiv.org/abs/2108.05669",
          "publishedOn": "2021-08-13T01:56:54.462Z",
          "wordCount": 674,
          "title": "Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Lin Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">XiuQiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Recently, pre-trained language models such as BERT have been applied to\ndocument ranking for information retrieval, which first pre-train a general\nlanguage model on an unlabeled large corpus and then conduct ranking-specific\nfine-tuning on expert-labeled relevance datasets. Ideally, an IR system would\nmodel relevance from a user-system dualism: the user's view and the system's\nview. User's view judges the relevance based on the activities of \"real users\"\nwhile the system's view focuses on the relevance signals from the system side,\ne.g., from the experts or algorithms, etc. Inspired by the user-system\nrelevance views and the success of pre-trained language models, in this paper\nwe propose a novel ranking framework called Pre-Rank that takes both user's\nview and system's view into consideration, under the pre-training and\nfine-tuning paradigm. Specifically, to model the user's view of relevance,\nPre-Rank pre-trains the initial query-document representations based on\nlarge-scale user activities data such as the click log. To model the system's\nview of relevance, Pre-Rank further fine-tunes the model on expert-labeled\nrelevance data. More importantly, the pre-trained representations, are\nfine-tuned together with handcrafted learning-to-rank features under a wide and\ndeep network architecture. In this way, Pre-Rank can model the relevance by\nincorporating the relevant knowledge and signals from both real search users\nand the IR experts. To verify the effectiveness of Pre-Rank, we showed two\nimplementations by using BERT and SetRank as the underlying ranking model,\nrespectively. Experimental results base on three publicly available benchmarks\nshowed that in both of the implementations, Pre-Rank can respectively\noutperform the underlying ranking models and achieved state-of-the-art\nperformances.",
          "link": "http://arxiv.org/abs/2108.05652",
          "publishedOn": "2021-08-13T01:56:54.330Z",
          "wordCount": 699,
          "title": "Modeling Relevance Ranking under the Pre-training and Fine-tuning Paradigm. (arXiv:2108.05652v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiuzhou Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_D/0/1/0/all/0/1\">Daniel Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>",
          "description": "Text generation from semantic graphs is traditionally performed with\ndeterministic methods, which generate a unique description given an input\ngraph. However, the generation problem admits a range of acceptable textual\noutputs, exhibiting lexical, syntactic and semantic variation. To address this\ndisconnect, we present two main contributions. First, we propose a stochastic\ngraph-to-text model, incorporating a latent variable in an encoder-decoder\nmodel, and its use in an ensemble. Second, to assess the diversity of the\ngenerated sentences, we propose a new automatic evaluation metric which jointly\nevaluates output diversity and quality in a multi-reference setting. We\nevaluate the models on WebNLG datasets in English and Russian, and show an\nensemble of stochastic models produces diverse sets of generated sentences,\nwhile retaining similar quality to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.05659",
          "publishedOn": "2021-08-13T01:56:54.255Z",
          "wordCount": 551,
          "title": "Generating Diverse Descriptions from Semantic Graphs. (arXiv:2108.05659v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>",
          "description": "Recent research demonstrates the effectiveness of using fine-tuned language\nmodels~(LM) for dense retrieval. However, dense retrievers are hard to train,\ntypically requiring heavily engineered fine-tuning pipelines to realize their\nfull potential. In this paper, we identify and address two underlying problems\nof dense retrievers: i)~fragility to training data noise and ii)~requiring\nlarge batches to robustly learn the embedding space. We use the recently\nproposed Condenser pre-training architecture, which learns to condense\ninformation into the dense vector through LM pre-training. On top of it, we\npropose coCondenser, which adds an unsupervised corpus-level contrastive loss\nto warm up the passage embedding space. Retrieval experiments on MS-MARCO,\nNatural Question, and Trivia QA datasets show that coCondenser removes the need\nfor heavy data engineering such as augmentation, synthesis, or filtering, as\nwell as the need for large batch training. It shows comparable performance to\nRocketQA, a state-of-the-art, heavily engineered system, using simple small\nbatch fine-tuning.",
          "link": "http://arxiv.org/abs/2108.05540",
          "publishedOn": "2021-08-13T01:56:54.225Z",
          "wordCount": 586,
          "title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. (arXiv:2108.05540v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_K/0/1/0/all/0/1\">Katikapalli Subramanyam Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1\">Ajit Rajasekharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangeetha_S/0/1/0/all/0/1\">Sivanesan Sangeetha</a>",
          "description": "Transformer-based pretrained language models (T-PTLMs) have achieved great\nsuccess in almost every NLP task. The evolution of these models started with\nGPT and BERT. These models are built on the top of transformers,\nself-supervised learning and transfer learning. Transformed-based PTLMs learn\nuniversal language representations from large volumes of text data using\nself-supervised learning and transfer this knowledge to downstream tasks. These\nmodels provide good background knowledge to downstream tasks which avoids\ntraining of downstream models from scratch. In this comprehensive survey paper,\nwe initially give a brief overview of self-supervised learning. Next, we\nexplain various core concepts like pretraining, pretraining methods,\npretraining tasks, embeddings and downstream adaptation methods. Next, we\npresent a new taxonomy of T-PTLMs and then give brief overview of various\nbenchmarks including both intrinsic and extrinsic. We present a summary of\nvarious useful libraries to work with T-PTLMs. Finally, we highlight some of\nthe future research directions which will further improve these models. We\nstrongly believe that this comprehensive survey paper will serve as a good\nreference to learn the core concepts as well as to stay updated with the recent\nhappenings in T-PTLMs.",
          "link": "http://arxiv.org/abs/2108.05542",
          "publishedOn": "2021-08-13T01:56:54.189Z",
          "wordCount": 631,
          "title": "AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing. (arXiv:2108.05542v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Minnema_G/0/1/0/all/0/1\">Gosse Minnema</a>",
          "description": "This technical report introduces an adapted version of the LOME frame\nsemantic parsing model (Xia et al., EACL 2021) which is capable of\nautomatically annotating texts according to the \"Kicktionary\" domain-specific\nframenet resource. Several methods for training a model even with limited\navailable training data are proposed. While there are some challenges for\nevaluation related to the nature of the available annotations, preliminary\nresults are very promising, with the best model reaching F1-scores of 0.83\n(frame prediction) and 0.81 (semantic role prediction).",
          "link": "http://arxiv.org/abs/2108.05575",
          "publishedOn": "2021-08-13T01:56:54.175Z",
          "wordCount": 513,
          "title": "Kicktionary-LOME: A Domain-Specific Multilingual Frame Semantic Parsing Model for Football Language. (arXiv:2108.05575v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomson_C/0/1/0/all/0/1\">Craig Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1\">Ehud Reiter</a>",
          "description": "The Shared Task on Evaluating Accuracy focused on techniques (both manual and\nautomatic) for evaluating the factual accuracy of texts produced by neural NLG\nsystems, in a sports-reporting domain. Four teams submitted evaluation\ntechniques for this task, using very different approaches and techniques. The\nbest-performing submissions did encouragingly well at this difficult task.\nHowever, all automatic submissions struggled to detect factual errors which are\nsemantically or pragmatically complex (for example, based on incorrect\ncomputation or inference).",
          "link": "http://arxiv.org/abs/2108.05644",
          "publishedOn": "2021-08-13T01:56:54.168Z",
          "wordCount": 513,
          "title": "Generation Challenges: Results of the Accuracy Evaluation Shared Task. (arXiv:2108.05644v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jezek_K/0/1/0/all/0/1\">Kamil Jezek</a>",
          "description": "Ethereum platform operates with rich spectrum of data structures and hashing\nand coding functions. The main source describing them is the Yellow paper,\ncomplemented by a lot of informal blogs. These sources are somehow limited. In\nparticular, the Yellow paper does not ideally balance brevity and detail, in\nsome parts it is very detail, while too shallow elsewhere. The blogs on the\nother hand are often too vague and in certain cases contain incorrect\ninformation. As a solution, we provide this document, which summarises data\nstructures used in Ethereum. The goal is to provide sufficient detail while\nkeeping brevity. Sufficiently detailed formal view is enriched with examples to\nextend on clarity.",
          "link": "http://arxiv.org/abs/2108.05513",
          "publishedOn": "2021-08-13T01:56:54.134Z",
          "wordCount": 526,
          "title": "Ethereum Data Structures. (arXiv:2108.05513v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samadi_A/0/1/0/all/0/1\">Anahita Samadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debapriya Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1\">Shirin Nilizadeh</a>",
          "description": "Recently, some studies have shown that text classification tasks are\nvulnerable to poisoning and evasion attacks. However, little work has\ninvestigated attacks against decision making algorithms that use text\nembeddings, and their output is a ranking. In this paper, we focus on ranking\nalgorithms for recruitment process, that employ text embeddings for ranking\napplicants resumes when compared to a job description. We demonstrate both\nwhite box and black box attacks that identify text items, that based on their\nlocation in embedding space, have significant contribution in increasing the\nsimilarity score between a resume and a job description. The adversary then\nuses these text items to improve the ranking of their resume among others. We\ntested recruitment algorithms that use the similarity scores obtained from\nUniversal Sentence Encoder (USE) and Term Frequency Inverse Document Frequency\n(TF IDF) vectors. Our results show that in both adversarial settings, on\naverage the attacker is successful. We also found that attacks against TF IDF\nis more successful compared to USE.",
          "link": "http://arxiv.org/abs/2108.05490",
          "publishedOn": "2021-08-13T01:56:54.115Z",
          "wordCount": 609,
          "title": "Attacks against Ranking Algorithms with Text Embeddings: a Case Study on Recruitment Algorithms. (arXiv:2108.05490v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.",
          "link": "http://arxiv.org/abs/2107.13189",
          "publishedOn": "2021-08-12T01:56:21.388Z",
          "wordCount": 586,
          "title": "Goal-Oriented Script Construction. (arXiv:2107.13189v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karadzhov_G/0/1/0/all/0/1\">Georgi Karadzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stafford_T/0/1/0/all/0/1\">Tom Stafford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>",
          "description": "Dialogue systems research is traditionally focused on dialogues between two\ninterlocutors, largely ignoring group conversations. Moreover, most previous\nresearch is focused either on task-oriented dialogue (e.g.\\ restaurant\nbookings) or user engagement (chatbots), while research on systems for\ncollaborative dialogues is an under-explored area. To this end, we introduce\nthe first publicly available dataset containing collaborative conversations on\nsolving a cognitive task, consisting of 500 group dialogues and 14k utterances.\nFurthermore, we propose a novel annotation schema that captures deliberation\ncues and release 50 dialogues annotated with it. Finally, we demonstrate the\nusefulness of the annotated data in training classifiers to predict the\nconstructiveness of a conversation. The data collection platform, dataset and\nannotated corpus are publicly available at https://delibot.xyz",
          "link": "http://arxiv.org/abs/2108.05271",
          "publishedOn": "2021-08-12T01:56:21.249Z",
          "wordCount": 559,
          "title": "DeliData: A dataset for deliberation in multi-party problem solving. (arXiv:2108.05271v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.07086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rotman_G/0/1/0/all/0/1\">Guy Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>",
          "description": "Recent improvements in the predictive quality of natural language processing\nsystems are often dependent on a substantial increase in the number of model\nparameters. This has led to various attempts of compressing such models, but\nexisting methods have not considered the differences in the predictive power of\nvarious model components or in the generalizability of the compressed models.\nTo understand the connection between model compression and out-of-distribution\ngeneralization, we define the task of compressing language representation\nmodels such that they perform best in a domain adaptation setting. We choose to\naddress this problem from a causal perspective, attempting to estimate the\naverage treatment effect (ATE) of a model component, such as a single layer, on\nthe model's predictions. Our proposed ATE-guided Model Compression scheme\n(AMoC), generates many model candidates, differing by the model components that\nwere removed. Then, we select the best candidate through a stepwise regression\nmodel that utilizes the ATE to predict the expected performance on the target\ndomain. AMoC outperforms strong baselines on dozens of domain pairs across\nthree text classification and sequence tagging tasks.",
          "link": "http://arxiv.org/abs/2101.07086",
          "publishedOn": "2021-08-12T01:56:21.244Z",
          "wordCount": 647,
          "title": "Model Compression for Domain Adaptation through Causal Effect Estimation. (arXiv:2101.07086v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wentao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jinhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Large-scale pre-trained models (PTMs) such as BERT and GPT have recently\nachieved great success and become a milestone in the field of artificial\nintelligence (AI). Owing to sophisticated pre-training objectives and huge\nmodel parameters, large-scale PTMs can effectively capture knowledge from\nmassive labeled and unlabeled data. By storing knowledge into huge parameters\nand fine-tuning on specific tasks, the rich knowledge implicitly encoded in\nhuge parameters can benefit a variety of downstream tasks, which has been\nextensively demonstrated via experimental verification and empirical analysis.\nIt is now the consensus of the AI community to adopt PTMs as backbone for\ndownstream tasks rather than learning models from scratch. In this paper, we\ntake a deep look into the history of pre-training, especially its special\nrelation with transfer learning and self-supervised learning, to reveal the\ncrucial position of PTMs in the AI development spectrum. Further, we\ncomprehensively review the latest breakthroughs of PTMs. These breakthroughs\nare driven by the surge of computational power and the increasing availability\nof data, towards four important directions: designing effective architectures,\nutilizing rich contexts, improving computational efficiency, and conducting\ninterpretation and theoretical analysis. Finally, we discuss a series of open\nproblems and research directions of PTMs, and hope our view can inspire and\nadvance the future study of PTMs.",
          "link": "http://arxiv.org/abs/2106.07139",
          "publishedOn": "2021-08-12T01:56:21.204Z",
          "wordCount": 723,
          "title": "Pre-Trained Models: Past, Present and Future. (arXiv:2106.07139v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Medical imaging technologies, including computed tomography (CT) or chest\nX-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.\nSince manual report writing is usually too time-consuming, a more intelligent\nauxiliary medical system that could generate medical reports automatically and\nimmediately is urgently needed. In this article, we propose to use the medical\nvisual language BERT (Medical-VLBERT) model to identify the abnormality on the\nCOVID-19 scans and generate the medical report automatically based on the\ndetected lesion regions. To produce more accurate medical reports and minimize\nthe visual-and-linguistic differences, this model adopts an alternate learning\nstrategy with two procedures that are knowledge pretraining and transferring.\nTo be more precise, the knowledge pretraining procedure is to memorize the\nknowledge from medical texts, while the transferring procedure is to utilize\nthe acquired knowledge for professional medical sentences generations through\nobservations of medical images. In practice, for automatic medical report\ngeneration on the COVID-19 cases, we constructed a dataset of 368 medical\nfindings in Chinese and 1104 chest CT scans from The First Affiliated Hospital\nof Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun\nYat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of\nthe COVID-19 training samples, our model was first trained on the large-scale\nChinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for\nfurther fine-tuning. The experimental results showed that Medical-VLBERT\nachieved state-of-the-art performances on terminology prediction and report\ngeneration with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The\nChinese COVID-19 CT dataset is available at https://covid19ct.github.io/.",
          "link": "http://arxiv.org/abs/2108.05067",
          "publishedOn": "2021-08-12T01:56:21.142Z",
          "wordCount": 782,
          "title": "Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simonarson_H/0/1/0/all/0/1\">Haukur Barri S&#xed;monarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1\">V&#xe9;steinn Sn&#xe6;bjarnarson</a>",
          "description": "We present a new Icelandic-English parallel corpus, the Icelandic Parallel\nAbstracts Corpus (IPAC), composed of abstracts from student theses and\ndissertations. The texts were collected from the Skemman repository which keeps\nrecords of all theses, dissertations and final projects from students at\nIcelandic universities. The corpus was aligned based on sentence-level BLEU\nscores, in both translation directions, from NMT models using Bleualign. The\nresult is a corpus of 64k sentence pairs from over 6 thousand parallel\nabstracts.",
          "link": "http://arxiv.org/abs/2108.05289",
          "publishedOn": "2021-08-12T01:56:21.135Z",
          "wordCount": 498,
          "title": "Icelandic Parallel Abstracts Corpus. (arXiv:2108.05289v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_C/0/1/0/all/0/1\">Chong Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Songzi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adekkanattu_P/0/1/0/all/0/1\">Prakash Adekkanattu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_J/0/1/0/all/0/1\">Jyotishman Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thomas J. George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William R. Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "Social and behavioral determinants of health (SBDoH) have important roles in\nshaping people's health. In clinical research studies, especially comparative\neffectiveness studies, failure to adjust for SBDoH factors will potentially\ncause confounding issues and misclassification errors in either statistical\nanalyses and machine learning-based models. However, there are limited studies\nto examine SBDoH factors in clinical outcomes due to the lack of structured\nSBDoH information in current electronic health record (EHR) systems, while much\nof the SBDoH information is documented in clinical narratives. Natural language\nprocessing (NLP) is thus the key technology to extract such information from\nunstructured clinical text. However, there is not a mature clinical NLP system\nfocusing on SBDoH. In this study, we examined two state-of-the-art\ntransformer-based NLP models, including BERT and RoBERTa, to extract SBDoH\nconcepts from clinical narratives, applied the best performing model to extract\nSBDoH concepts on a lung cancer screening patient cohort, and examined the\ndifference of SBDoH information between NLP extracted results and structured\nEHRs (SBDoH information captured in standard vocabularies such as the\nInternational Classification of Diseases codes). The experimental results show\nthat the BERT-based NLP model achieved the best strict/lenient F1-score of\n0.8791 and 0.8999, respectively. The comparison between NLP extracted SBDoH\ninformation and structured EHRs in the lung cancer patient cohort of 864\npatients with 161,933 various types of clinical notes showed that much more\ndetailed information about smoking, education, and employment were only\ncaptured in clinical narratives and that it is necessary to use both clinical\nnarratives and structured EHRs to construct a more complete picture of\npatients' SBDoH factors.",
          "link": "http://arxiv.org/abs/2108.04949",
          "publishedOn": "2021-08-12T01:56:21.121Z",
          "wordCount": 743,
          "title": "A Study of Social and Behavioral Determinants of Health in Lung Cancer Patients Using Transformers-based Natural Language Processing Models. (arXiv:2108.04949v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangfeng Li</a>",
          "description": "Aspect based sentiment analysis (ABSA), exploring sentim- ent polarity of\naspect-given sentence, has drawn widespread applications in social media and\npublic opinion. Previously researches typically derive aspect-independent\nrepresentation by sentence feature generation only depending on text data. In\nthis paper, we propose a Position-Guided Contributive Distribution (PGCD) unit.\nIt achieves a position-dependent contributive pattern and generates\naspect-related statement feature for ABSA task. Quoted from Shapley Value, PGCD\ncan gain position-guided contextual contribution and enhance the aspect-based\nrepresentation. Furthermore, the unit can be used for improving effects on\nmultimodal ABSA task, whose datasets restructured by ourselves. Extensive\nexperiments on both text and text-audio level using dataset (SemEval) show that\nby applying the proposed unit, the mainstream models advance performance in\naccuracy and F1 score.",
          "link": "http://arxiv.org/abs/2108.05098",
          "publishedOn": "2021-08-12T01:56:21.100Z",
          "wordCount": 566,
          "title": "PGCD: a position-guied contributive distribution unit for aspect based sentiment analysis. (arXiv:2108.05098v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Sanchit Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1\">Arshdeep Sekhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>",
          "description": "Interpretability methods like Integrated Gradient and LIME are popular\nchoices for explaining natural language model predictions with relative word\nimportance scores. These interpretations need to be robust for trustworthy NLP\napplications in high-stake areas like medicine or finance. Our paper\ndemonstrates how interpretations can be manipulated by making simple word\nperturbations on an input text. Via a small portion of word-level swaps, these\nadversarial perturbations aim to make the resulting text semantically and\nspatially similar to its seed input (therefore sharing similar\ninterpretations). Simultaneously, the generated examples achieve the same\nprediction label as the seed yet are given a substantially different\nexplanation by the interpretation methods. Our experiments generate fragile\ninterpretations to attack two SOTA interpretation methods, across three popular\nTransformer models and on two different NLP datasets. We observe that the rank\norder correlation drops by over 20% when less than 10% of words are perturbed\non average. Further, rank-order correlation keeps decreasing as more words get\nperturbed. Furthermore, we demonstrate that candidates generated from our\nmethod have good quality metrics.",
          "link": "http://arxiv.org/abs/2108.04990",
          "publishedOn": "2021-08-12T01:56:21.053Z",
          "wordCount": 615,
          "title": "Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing. (arXiv:2108.04990v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1\">Huy Quang Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Nghia Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>",
          "description": "Handwritten mathematical expressions (HMEs) contain ambiguities in their\ninterpretations, even for humans sometimes. Several math symbols are very\nsimilar in the writing style, such as dot and comma or 0, O, and o, which is a\nchallenge for HME recognition systems to handle without using contextual\ninformation. To address this problem, this paper presents a Transformer-based\nMath Language Model (TMLM). Based on the self-attention mechanism, the\nhigh-level representation of an input token in a sequence of tokens is computed\nby how it is related to the previous tokens. Thus, TMLM can capture long\ndependencies and correlations among symbols and relations in a mathematical\nexpression (ME). We trained the proposed language model using a corpus of\napproximately 70,000 LaTeX sequences provided in CROHME 2016. TMLM achieved the\nperplexity of 4.42, which outperformed the previous math language models, i.e.,\nthe N-gram and recurrent neural network-based language models. In addition, we\ncombine TMLM into a stochastic context-free grammar-based HME recognition\nsystem using a weighting parameter to re-rank the top-10 best candidates. The\nexpression rates on the testing sets of CROHME 2016 and CROHME 2019 were\nimproved by 2.97 and 0.83 percentage points, respectively.",
          "link": "http://arxiv.org/abs/2108.05002",
          "publishedOn": "2021-08-12T01:56:21.023Z",
          "wordCount": 645,
          "title": "A Transformer-based Math Language Model for Handwritten Math Expression Recognition. (arXiv:2108.05002v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We introduce a new domain expert mixture (DEMix) layer that enables\nconditioning a language model (LM) on the domain of the input text. A DEMix\nlayer is a collection of expert feedforward networks, each specialized to a\ndomain, that makes the LM modular: experts can be mixed, added or removed after\ninitial training. Extensive experiments with autoregressive transformer LMs (up\nto 1.3B parameters) show that DEMix layers reduce test-time perplexity,\nincrease training efficiency, and enable rapid adaptation with little overhead.\nWe show that mixing experts during inference, using a parameter-free weighted\nensemble, allows the model to better generalize to heterogeneous or unseen\ndomains. We also show that experts can be added to iteratively incorporate new\ndomains without forgetting older ones, and that experts can be removed to\nrestrict access to unwanted domains, without additional training. Overall,\nthese results demonstrate benefits of explicitly conditioning on textual\ndomains during language modeling.",
          "link": "http://arxiv.org/abs/2108.05036",
          "publishedOn": "2021-08-12T01:56:20.862Z",
          "wordCount": 587,
          "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling. (arXiv:2108.05036v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1\">Andreas Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>",
          "description": "Natural Language Processing (NLP) models have become increasingly more\ncomplex and widespread. With recent developments in neural networks, a growing\nconcern is whether it is responsible to use these models. Concerns such as\nsafety and ethics can be partially addressed by providing explanations.\nFurthermore, when models do fail, providing explanations is paramount for\naccountability purposes. To this end, interpretability serves to provide these\nexplanations in terms that are understandable to humans. Central to what is\nunderstandable is how explanations are communicated. Therefore, this survey\nprovides a categorization of how recent interpretability methods communicate\nexplanations and discusses the methods in depth. Furthermore, the survey\nfocuses on post-hoc methods, which provide explanations after a model is\nlearned and generally model-agnostic. A common concern for this class of\nmethods is whether they accurately reflect the model. Hence, how these post-hoc\nmethods are evaluated is discussed throughout the paper.",
          "link": "http://arxiv.org/abs/2108.04840",
          "publishedOn": "2021-08-12T01:56:20.741Z",
          "wordCount": 582,
          "title": "Post-hoc Interpretability for Neural NLP: A Survey. (arXiv:2108.04840v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1\">Gaurav Sukhatme</a>",
          "description": "Language-guided robots performing home and office tasks must navigate in and\ninteract with the world. Grounding language instructions against visual\nobservations and actions to take in an environment is an open challenge. We\npresent Embodied BERT (EmBERT), a transformer-based model which can attend to\nhigh-dimensional, multi-modal inputs across long temporal horizons for\nlanguage-conditioned task completion. Additionally, we bridge the gap between\nsuccessful object-centric navigation models used for non-interactive agents and\nthe language-guided visual task completion benchmark, ALFRED, by introducing\nobject navigation targets for EmBERT training. We achieve competitive\nperformance on the ALFRED benchmark, and EmBERT marks the first\ntransformer-based model to successfully handle the long-horizon, dense,\nmulti-modal histories of ALFRED, and the first ALFRED model to utilize\nobject-centric navigation targets.",
          "link": "http://arxiv.org/abs/2108.04927",
          "publishedOn": "2021-08-12T01:56:20.701Z",
          "wordCount": 581,
          "title": "Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion. (arXiv:2108.04927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhsedaghat_M/0/1/0/all/0/1\">Mozhdeh Rouhsedaghat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_A/0/1/0/all/0/1\">Aichi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scalzo_F/0/1/0/all/0/1\">Fabien Scalzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>",
          "description": "Vision-and-language(V&L) models take image and text as input and learn to\ncapture the associations between them. Prior studies show that pre-trained V&L\nmodels can significantly improve the model performance for downstream tasks\nsuch as Visual Question Answering (VQA). However, V&L models are less effective\nwhen applied in the medical domain (e.g., on X-ray images and clinical notes)\ndue to the domain gap. In this paper, we investigate the challenges of applying\npre-trained V&L models in medical applications. In particular, we identify that\nthe visual representation in general V&L models is not suitable for processing\nmedical data. To overcome this limitation, we propose BERTHop, a\ntransformer-based model based on PixelHop++ and VisualBERT, for better\ncapturing the associations between the two modalities. Experiments on the OpenI\ndataset, a commonly used thoracic disease diagnosis benchmark, show that\nBERTHop achieves an average Area Under the Curve (AUC) of 98.12% which is 1.62%\nhigher than state-of-the-art (SOTA) while it is trained on a 9 times smaller\ndataset.",
          "link": "http://arxiv.org/abs/2108.04938",
          "publishedOn": "2021-08-12T01:56:20.638Z",
          "wordCount": 628,
          "title": "BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease Diagnosis. (arXiv:2108.04938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.14408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Sida Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Justine Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Atish Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1\">Ray Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yinying Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1\">Vivek Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1\">Mubarak Seyed Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1\">Gang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1\">Nan Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yidan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1\">Andy O</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1\">Pooja Chitkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Alkesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1\">Kushal Tayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Roger Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1\">Peter Grasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason D. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>",
          "description": "Named Entity Recognition (NER) and Entity Linking (EL) play an essential role\nin voice assistant interaction, but are challenging due to the special\ndifficulties associated with spoken user queries. In this paper, we propose a\nnovel architecture that jointly solves the NER and EL tasks by combining them\nin a joint reranking module. We show that our proposed framework improves NER\naccuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features\nused also lead to better accuracies in other natural language understanding\ntasks, such as domain classification and semantic parsing.",
          "link": "http://arxiv.org/abs/2005.14408",
          "publishedOn": "2021-08-11T01:55:21.360Z",
          "wordCount": 638,
          "title": "Noise Robust Named Entity Understanding for Voice Assistants. (arXiv:2005.14408v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1\">Amir Karami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Michael Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1\">Bailey Goldschmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1\">Hannah R. Boyajieff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafabadi_M/0/1/0/all/0/1\">Mahdi M. Najafabadi</a>",
          "description": "Public response to COVID-19 vaccines is the key success factor to control the\nCOVID-19 pandemic. To understand the public response, there is a need to\nexplore public opinion. Traditional surveys are expensive and time-consuming,\naddress limited health topics, and obtain small-scale data. Twitter can provide\na great opportunity to understand public opinion regarding COVID-19 vaccines.\nThe current study proposes an approach using computational and human coding\nmethods to collect and analyze a large number of tweets to provide a wider\nperspective on the COVID-19 vaccine. This study identifies the sentiment of\ntweets and their temporal trend, discovers major topics, compares topics of\nnegative and non-negative tweets, and discloses top topics of negative and\nnon-negative tweets. Our findings show that the negative sentiment regarding\nthe COVID-19 vaccine had a decreasing trend between November 2020 and February\n2021. We found Twitter users have discussed a wide range of topics from\nvaccination sites to the 2020 U.S. election between November 2020 and February\n2021. The findings show that there was a significant difference between\nnegative and non-negative tweets regarding the weight of most topics. Our\nresults also indicate that the negative and non-negative tweets had different\ntopic priorities and focuses.",
          "link": "http://arxiv.org/abs/2108.04816",
          "publishedOn": "2021-08-11T01:55:21.091Z",
          "wordCount": 689,
          "title": "COVID-19 Vaccine and Social Media: Exploring Emotions and Discussions on Twitter. (arXiv:2108.04816v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_M/0/1/0/all/0/1\">Mathieu Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilakis_N/0/1/0/all/0/1\">Nicolas Hamilakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyssel_M/0/1/0/all/0/1\">Maureen de Seyssel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roze_P/0/1/0/all/0/1\">Patricia Roz&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>",
          "description": "We present the Zero Resource Speech Challenge 2021, which asks participants\nto learn a language model directly from audio, without any text or labels. The\nchallenge is based on the Libri-light dataset, which provides up to 60k hours\nof audio from English audio books without any associated text. We provide a\npipeline baseline system consisting on an encoder based on contrastive\npredictive coding (CPC), a quantizer ($k$-means) and a standard language model\n(BERT or LSTM). The metrics evaluate the learned representations at the\nacoustic (ABX discrimination), lexical (spot-the-word), syntactic\n(acceptability judgment) and semantic levels (similarity judgment). We present\nan overview of the eight submitted systems from four groups and discuss the\nmain results.",
          "link": "http://arxiv.org/abs/2104.14700",
          "publishedOn": "2021-08-11T01:55:21.085Z",
          "wordCount": 607,
          "title": "The Zero Resource Speech Challenge 2021: Spoken language modelling. (arXiv:2104.14700v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>",
          "description": "We propose a headed span-based method for projective dependency parsing. In a\nprojective tree, the subtree rooted at each word occurs in a contiguous\nsequence (i.e., span) in the surface order, we call the span-headword pair\n\\textit{headed span}. In this view, a projective tree can be regarded as a\ncollection of headed spans. It is similar to the case in constituency parsing\nsince a constituency tree can be regarded as a collection of constituent spans.\nSpan-based methods decompose the score of a constituency tree sorely into the\nscore of constituent spans and use the CYK algorithm for global training and\nexact inference, obtaining state-of-the-art results in constituency parsing.\nInspired by them, we decompose the score of a dependency tree into the score of\nheaded spans. We use neural networks to score headed spans and design a novel\n$O(n^3)$ dynamic programming algorithm to enable global training and exact\ninference. We evaluate our method on PTB, CTB, and UD, achieving\nstate-of-the-art or comparable results.",
          "link": "http://arxiv.org/abs/2108.04750",
          "publishedOn": "2021-08-11T01:55:21.072Z",
          "wordCount": 583,
          "title": "Headed Span-Based Projective Dependency Parsing. (arXiv:2108.04750v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yubo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>",
          "description": "Humor recognition has been widely studied as a text classification problem\nusing data-driven approaches. However, most existing work does not examine the\nactual joke mechanism to understand humor. We break down any joke into two\ndistinct components: the set-up and the punchline, and further explore the\nspecial relationship between them. Inspired by the incongruity theory of humor,\nwe model the set-up as the part developing semantic uncertainty, and the\npunchline disrupting audience expectations. With increasingly powerful language\nmodels, we were able to feed the set-up along with the punchline into the GPT-2\nlanguage model, and calculate the uncertainty and surprisal values of the\njokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found\nthat these two features have better capabilities of telling jokes from\nnon-jokes, compared with existing baselines.",
          "link": "http://arxiv.org/abs/2012.12007",
          "publishedOn": "2021-08-11T01:55:21.049Z",
          "wordCount": 607,
          "title": "Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition. (arXiv:2012.12007v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1\">Noriyuki Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>",
          "description": "We study continual learning for natural language instruction generation, by\nobserving human users' instruction execution. We focus on a collaborative\nscenario, where the system both acts and delegates tasks to human users using\nnatural language. We compare user execution of generated instructions to the\noriginal system intent as an indication to the system's success communicating\nits intent. We show how to use this signal to improve the system's ability to\ngenerate instructions via contextual bandit learning. In interaction with real\nusers, our system demonstrates dramatic improvements in its ability to generate\nlanguage over time.",
          "link": "http://arxiv.org/abs/2108.04812",
          "publishedOn": "2021-08-11T01:55:21.042Z",
          "wordCount": 553,
          "title": "Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior. (arXiv:2108.04812v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onta%7Bn%7D%7Bo%7Dn_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cvicek_V/0/1/0/all/0/1\">Vaclav Cvicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_Z/0/1/0/all/0/1\">Zachary Fisher</a>",
          "description": "Several studies have reported the inability of Transformer models to\ngeneralize compositionally, a key type of generalization in many NLP tasks such\nas semantic parsing. In this paper we explore the design space of Transformer\nmodels showing that the inductive biases given to the model by several design\ndecisions significantly impact compositional generalization. Through this\nexploration, we identified Transformer configurations that generalize\ncompositionally significantly better than previously reported in the literature\nin a diverse set of compositional tasks, and that achieve state-of-the-art\nresults in a semantic parsing compositional generalization benchmark (COGS),\nand a string edit operation composition benchmark (PCFG).",
          "link": "http://arxiv.org/abs/2108.04378",
          "publishedOn": "2021-08-11T01:55:21.037Z",
          "wordCount": 528,
          "title": "Making Transformers Solve Compositional Tasks. (arXiv:2108.04378v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Anbukkarasi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1\">Kingston Pal Thamburaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1\">Prabakaran Chandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>",
          "description": "Numerous methods have been developed to monitor the spread of negativity in\nmodern years by eliminating vulgar, offensive, and fierce comments from social\nmedia platforms. However, there are relatively lesser amounts of study that\nconverges on embracing positivity, reinforcing supportive and reassuring\ncontent in online forums. Consequently, we propose creating an English-Kannada\nHope speech dataset, KanHope and comparing several experiments to benchmark the\ndataset. The dataset consists of 6,176 user-generated comments in code mixed\nKannada scraped from YouTube and manually annotated as bearing hope speech or\nNot-hope speech. In addition, we introduce DC-BERT4HOPE, a dual-channel model\nthat uses the English translation of KanHope for additional training to promote\nhope speech detection. The approach achieves a weighted F1-score of 0.756,\nbettering other models. Henceforth, KanHope aims to instigate research in\nKannada while broadly promoting researchers to take a pragmatic approach\ntowards online content that encourages, positive, and supportive.",
          "link": "http://arxiv.org/abs/2108.04616",
          "publishedOn": "2021-08-11T01:55:21.030Z",
          "wordCount": 581,
          "title": "Hope Speech detection in under-resourced Kannada language. (arXiv:2108.04616v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>",
          "description": "Understanding documents from their visual snapshots is an emerging problem\nthat requires both advanced computer vision and NLP methods. The recent advance\nin OCR enables the accurate recognition of text blocks, yet it is still\nchallenging to extract key information from documents due to the diversity of\ntheir layouts. Although recent studies on pre-trained language models show the\nimportance of incorporating layout information on this task, the conjugation of\ntexts and their layouts still follows the style of BERT optimized for\nunderstanding the 1D text. This implies there is room for further improvement\nconsidering the 2D nature of text layouts. This paper introduces a pre-trained\nlanguage model, BERT Relying On Spatiality (BROS), which effectively utilizes\nthe information included in individual text blocks and their layouts.\nSpecifically, BROS encodes spatial information by utilizing relative positions\nand learns spatial dependencies between OCR blocks with a novel area-masking\nstrategy. These two novel approaches lead to an efficient encoding of spatial\nlayout information highlighted by the robust performance of BROS under\nlow-resource environments. We also introduce a general-purpose parser that can\nbe combined with BROS to extract key information even when there is no order\ninformation between text blocks. BROS shows its superiority on four public\nbenchmarks---FUNSD, SROIE*, CORD, and SciTSR---and its robustness in practical\ncases where order information of text blocks is not available. Further\nexperiments with a varying number of training examples demonstrate the high\ntraining efficiency of our approach. Our code will be open to the public.",
          "link": "http://arxiv.org/abs/2108.04539",
          "publishedOn": "2021-08-11T01:55:21.011Z",
          "wordCount": 689,
          "title": "BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wallbridge_S/0/1/0/all/0/1\">Sarenne Wallbridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>",
          "description": "People convey information extremely effectively through spoken interaction\nusing multiple channels of information transmission: the lexical channel of\nwhat is said, and the non-lexical channel of how it is said. We propose\nstudying human perception of spoken communication as a means to better\nunderstand how information is encoded across these channels, focusing on the\nquestion 'What characteristics of communicative context affect listener's\nexpectations of speech?'. To investigate this, we present a novel behavioural\ntask testing whether listeners can discriminate between the true utterance in a\ndialogue and utterances sampled from other contexts with the same lexical\ncontent. We characterize how perception - and subsequent discriminative\ncapability - is affected by different degrees of additional contextual\ninformation across both the lexical and non-lexical channel of speech. Results\ndemonstrate that people can effectively discriminate between different prosodic\nrealisations, that non-lexical context is informative, and that this channel\nprovides more salient information than the lexical channel, highlighting the\nimportance of the non-lexical channel in spoken interaction.",
          "link": "http://arxiv.org/abs/2105.00260",
          "publishedOn": "2021-08-11T01:55:20.998Z",
          "wordCount": 648,
          "title": "It's not what you said, it's how you said it: discriminative perception of speech as a multichannel communication system. (arXiv:2105.00260v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yubo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>",
          "description": "In this paper, we give an overview of commonsense reasoning in natural\nlanguage processing, which requires a deeper understanding of the contexts and\nusually involves inference over implicit external knowledge. We first review\nsome popular commonsense knowledge bases and commonsense reasoning benchmarks,\nbut give more emphasis on the methodologies, including recent approaches that\naim at solving some general natural language problems that take advantage of\nexternal knowledge bases. Finally, we discuss some future directions in pushing\nthe boundary of commonsense reasoning in natural language processing.",
          "link": "http://arxiv.org/abs/2108.04674",
          "publishedOn": "2021-08-11T01:55:20.988Z",
          "wordCount": 527,
          "title": "How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of Recent Resources and Methodologies. (arXiv:2108.04674v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufis</a>",
          "description": "EuroVoc is a multilingual thesaurus that was built for organizing the\nlegislative documentary of the European Union institutions. It contains\nthousands of categories at different levels of specificity and its descriptors\nare targeted by legal texts in almost thirty languages. In this work we propose\na unified framework for EuroVoc classification on 22 languages by fine-tuning\nmodern Transformer-based pretrained language models. We study extensively the\nperformance of our trained models and show that they significantly improve the\nresults obtained by a similar tool - JEX - on the same dataset. The code and\nthe fine-tuned models were open sourced, together with a programmatic interface\nthat eases the process of loading the weights of a trained model and of\nclassifying a new document.",
          "link": "http://arxiv.org/abs/2108.01139",
          "publishedOn": "2021-08-11T01:55:20.980Z",
          "wordCount": 594,
          "title": "PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shruti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>",
          "description": "Comparing research papers is a conventional method to demonstrate progress in\nexperimental research. We present COMPARE, a taxonomy and a dataset of\ncomparison discussions in peer reviews of research papers in the domain of\nexperimental deep learning. From a thorough observation of a large set of\nreview sentences, we build a taxonomy of categories in comparison discussions\nand present a detailed annotation scheme to analyze this. Overall, we annotate\n117 reviews covering 1,800 sentences. We experiment with various methods to\nidentify comparison sentences in peer reviews and report a maximum F1 Score of\n0.49. We also pretrain two language models specifically on ML, NLP, and CV\npaper abstracts and reviews to learn informative representations of peer\nreviews. The annotated dataset and the pretrained models are available at\nhttps://github.com/shruti-singh/COMPARE .",
          "link": "http://arxiv.org/abs/2108.04366",
          "publishedOn": "2021-08-11T01:55:20.962Z",
          "wordCount": 571,
          "title": "COMPARE: A Taxonomy and Dataset of Comparison Discussions in Peer Reviews. (arXiv:2108.04366v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1\">Eden Bensaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1\">Mauro Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>",
          "description": "Storytelling is an open-ended task that entails creative thinking and\nrequires a constant flow of ideas. Natural language generation (NLG) for\nstorytelling is especially challenging because it requires the generated text\nto follow an overall theme while remaining creative and diverse to engage the\nreader. In this work, we introduce a system and a web-based demo, FairyTailor,\nfor human-in-the-loop visual story co-creation. Users can create a cohesive\nchildren's fairytale by weaving generated texts and retrieved images with their\ninput. FairyTailor adds another modality and modifies the text generation\nprocess to produce a coherent and creative sequence of text and images. To our\nknowledge, this is the first dynamic tool for multimodal story generation that\nallows interactive co-formation of both texts and images. It allows users to\ngive feedback on co-created stories and share their results.",
          "link": "http://arxiv.org/abs/2108.04324",
          "publishedOn": "2021-08-11T01:55:20.955Z",
          "wordCount": 592,
          "title": "FairyTailor: A Multimodal Generative Framework for Storytelling. (arXiv:2108.04324v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>",
          "description": "Conventional Intent Detection (ID) models are usually trained offline, which\nrelies on a fixed dataset and a predefined set of intent classes. However, in\nreal-world applications, online systems usually involve continually emerging\nnew user intents, which pose a great challenge to the offline training\nparadigm. Recently, lifelong learning has received increasing attention and is\nconsidered to be the most promising solution to this challenge. In this paper,\nwe propose Lifelong Intent Detection (LID), which continually trains an ID\nmodel on new data to learn newly emerging intents while avoiding\ncatastrophically forgetting old data. Nevertheless, we find that existing\nlifelong learning methods usually suffer from a serious imbalance between old\nand new data in the LID task. Therefore, we propose a novel lifelong learning\nmethod, Multi-Strategy Rebalancing (MSR), which consists of cosine\nnormalization, hierarchical knowledge distillation, and inter-class margin loss\nto alleviate the multiple negative effects of the imbalance problem.\nExperimental results demonstrate the effectiveness of our method, which\nsignificantly outperforms previous state-of-the-art lifelong learning methods\non the ATIS, SNIPS, HWU64, and CLINC150 benchmarks.",
          "link": "http://arxiv.org/abs/2108.04445",
          "publishedOn": "2021-08-11T01:55:20.941Z",
          "wordCount": 603,
          "title": "Lifelong Intent Detection via Multi-Strategy Rebalancing. (arXiv:2108.04445v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koh_A/0/1/0/all/0/1\">Andrew Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>",
          "description": "In this paper, we examine the use of Transfer Learning using Pretrained Audio\nNeural Networks (PANNs), and propose an architecture that is able to better\nleverage the acoustic features provided by PANNs for the Automated Audio\nCaptioning Task. We also introduce a novel self-supervised objective,\nReconstruction Latent Space Similarity Regularization (RLSSR). The RLSSR module\nsupplements the training of the model by minimizing the similarity between the\nencoder and decoder embedding. The combination of both methods allows us to\nsurpass state of the art results by a significant margin on the Clotho dataset\nacross several metrics and benchmarks.",
          "link": "http://arxiv.org/abs/2108.04692",
          "publishedOn": "2021-08-11T01:55:20.930Z",
          "wordCount": 551,
          "title": "Automated Audio Captioning using Transfer Learning and Reconstruction Latent Space Similarity Regularization. (arXiv:2108.04692v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>",
          "description": "In this paper, we present a semi-automated framework called AMUSED for\ngathering multi-modal annotated data from the multiple social media platforms.\nThe framework is designed to mitigate the issues of collecting and annotating\nsocial media data by cohesively combining machine and human in the data\ncollection process. From a given list of the articles from professional news\nmedia or blog, AMUSED detects links to the social media posts from news\narticles and then downloads contents of the same post from the respective\nsocial media platform to gather details about that specific post. The framework\nis capable of fetching the annotated data from multiple platforms like Twitter,\nYouTube, Reddit. The framework aims to reduce the workload and problems behind\nthe data annotation from the social media platforms. AMUSED can be applied in\nmultiple application domains, as a use case, we have implemented the framework\nfor collecting COVID-19 misinformation data from different social media\nplatforms.",
          "link": "http://arxiv.org/abs/2010.00502",
          "publishedOn": "2021-08-11T01:55:20.915Z",
          "wordCount": 670,
          "title": "AMUSED: An Annotation Framework of Multi-modal Social Media Data. (arXiv:2010.00502v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yadao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>",
          "description": "Pre-trained models for programming languages have proven their significant\nvalues in various code-related tasks, such as code search, code clone\ndetection, and code translation. Currently, most pre-trained models treat a\ncode snippet as a sequence of tokens or only focus on the data flow between\ncode identifiers. However, rich code syntax and hierarchy are ignored which can\nprovide important structure information and semantic rules of codes to help\nenhance code representations. In addition, although the BERT-based code\npre-trained models achieve high performance on many downstream tasks, the\nnative derived sequence representations of BERT are proven to be of\nlow-quality, it performs poorly on code matching and similarity tasks. To\naddress these problems, we propose CLSEBERT, a Constrastive Learning Framework\nfor Syntax Enhanced Code Pre-Trained Model, to deal with various code\nintelligence tasks. In the pre-training stage, we consider the code syntax and\nhierarchy contained in the Abstract Syntax Tree (AST) and leverage the\nconstrastive learning to learn noise-invariant code representations. Besides\nthe masked language modeling (MLM), we also introduce two novel pre-training\nobjectives. One is to predict the edges between nodes in the abstract syntax\ntree, and the other is to predict the types of code tokens. Through extensive\nexperiments on four code intelligence tasks, we successfully show the\neffectiveness of our proposed model.",
          "link": "http://arxiv.org/abs/2108.04556",
          "publishedOn": "2021-08-11T01:55:20.907Z",
          "wordCount": 673,
          "title": "CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model. (arXiv:2108.04556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eikema_B/0/1/0/all/0/1\">Bryan Eikema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>",
          "description": "In neural machine translation (NMT), we search for the mode of the model\ndistribution to form predictions. The mode as well as other high probability\ntranslations found by beam search have been shown to often be inadequate in a\nnumber of ways. This prevents practitioners from improving translation quality\nthrough better search, as these idiosyncratic translations end up being\nselected by the decoding algorithm, a problem known as the beam search curse.\nRecently, a sampling-based approximation to minimum Bayes risk (MBR) decoding\nhas been proposed as an alternative decision rule for NMT that would likely not\nsuffer from the same problems. We analyse this approximation and establish that\nit has no equivalent to the beam search curse, i.e. better search always leads\nto better translations. We also design different approximations aimed at\ndecoupling the cost of exploration from the cost of robust estimation of\nexpected utility. This allows for exploration of much larger hypothesis spaces,\nwhich we show to be beneficial. We also show that it can be beneficial to make\nuse of strategies like beam search and nucleus sampling to construct hypothesis\nspaces efficiently. We show on three language pairs (English into and from\nGerman, Romanian, and Nepali) that MBR can improve upon beam search with\nmoderate computation.",
          "link": "http://arxiv.org/abs/2108.04718",
          "publishedOn": "2021-08-11T01:55:20.898Z",
          "wordCount": 638,
          "title": "Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation. (arXiv:2108.04718v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>",
          "description": "Multi-head attention, a collection of several attention mechanisms that\nindependently attend to different parts of the input, is the key ingredient in\nthe Transformer (Vaswaniet al., 2017). Recent work has shown, however, that a\nlarge proportion of the heads in a Transformer's multi-head attention mechanism\ncan be safely pruned away without significantly harming the performance of the\nmodel; such pruning leads to models that are noticeably smaller and faster in\npractice. Our work introduces a new head pruning technique that we term\ndifferentiable subset pruning. Intuitively, our method learns per-head\nimportance variables and then enforces a user-specified hard constraint on the\nnumber of unpruned heads. The importance variables are learned via stochastic\ngradient descent. We conduct experiments on natural language inference and\nmachine translation; we show that differentiable subset pruning performs\ncomparably or better than Voita et al. (2019) while offering the same exact\ncontrol over the number of heads as Michel et al. (2019).",
          "link": "http://arxiv.org/abs/2108.04657",
          "publishedOn": "2021-08-11T01:55:20.866Z",
          "wordCount": 581,
          "title": "Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.06973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lesota_O/0/1/0/all/0/1\">Oleg Lesota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melchiorre_A/0/1/0/all/0/1\">Alessandro B. Melchiorre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stefan Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowald_D/0/1/0/all/0/1\">Dominik Kowald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1\">Elisabeth Lex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>",
          "description": "Several studies have identified discrepancies between the popularity of items\nin user profiles and the corresponding recommendation lists. Such behavior,\nwhich concerns a variety of recommendation algorithms, is referred to as\npopularity bias. Existing work predominantly adopts simple statistical\nmeasures, such as the difference of mean or median popularity, to quantify\npopularity bias. Moreover, it does so irrespective of user characteristics\nother than the inclination to popular content. In this work, in contrast, we\npropose to investigate popularity differences (between the user profile and\nrecommendation list) in terms of median, a variety of statistical moments, as\nwell as similarity measures that consider the entire popularity distributions\n(Kullback-Leibler divergence and Kendall's tau rank-order correlation). This\nresults in a more detailed picture of the characteristics of popularity bias.\nFurthermore, we investigate whether such algorithmic popularity bias affects\nusers of different genders in the same way. We focus on music recommendation\nand conduct experiments on the recently released standardized LFM-2b dataset,\ncontaining listening profiles of Last.fm users. We investigate the algorithmic\npopularity bias of seven common recommendation algorithms (five collaborative\nfiltering and two baselines). Our experiments show that (1) the studied metrics\nprovide novel insights into popularity bias in comparison with only using\naverage differences, (2) algorithms less inclined towards popularity bias\namplification do not necessarily perform worse in terms of utility (NDCG), (3)\nthe majority of the investigated recommenders intensify the popularity bias of\nthe female users.",
          "link": "http://arxiv.org/abs/2108.06973",
          "publishedOn": "2021-08-17T01:54:46.579Z",
          "wordCount": 691,
          "title": "Analyzing Item Popularity Bias of Music Recommender Systems: Are Different Genders Equally Affected?. (arXiv:2108.06973v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinghong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shangxuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yudong Chen</a>",
          "description": "Hashing technology has been widely used in image retrieval due to its\ncomputational and storage efficiency. Recently, deep unsupervised hashing\nmethods have attracted increasing attention due to the high cost of human\nannotations in the real world and the superiority of deep learning technology.\nHowever, most deep unsupervised hashing methods usually pre-compute a\nsimilarity matrix to model the pairwise relationship in the pre-trained feature\nspace. Then this similarity matrix would be used to guide hash learning, in\nwhich most of the data pairs are treated equivalently. The above process is\nconfronted with the following defects: 1) The pre-computed similarity matrix is\ninalterable and disconnected from the hash learning process, which cannot\nexplore the underlying semantic information. 2) The informative data pairs may\nbe buried by the large number of less-informative data pairs. To solve the\naforementioned problems, we propose a \\textbf{Deep Self-Adaptive\nHashing~(DSAH)} model to adaptively capture the semantic information with two\nspecial designs: \\textbf{Adaptive Neighbor Discovery~(AND)} and\n\\textbf{Pairwise Information Content~(PIC)}. Firstly, we adopt the AND to\ninitially construct a neighborhood-based similarity matrix, and then refine\nthis initial similarity matrix with a novel update strategy to further\ninvestigate the semantic structure behind the learned representation. Secondly,\nwe measure the priorities of data pairs with PIC and assign adaptive weights to\nthem, which is relies on the assumption that more dissimilar data pairs contain\nmore discriminative information for hash learning. Extensive experiments on\nseveral benchmark datasets demonstrate that the above two technologies\nfacilitate the deep hashing model to achieve superior performance in a\nself-adaptive manner.",
          "link": "http://arxiv.org/abs/2108.07094",
          "publishedOn": "2021-08-17T01:54:46.573Z",
          "wordCount": 698,
          "title": "Deep Self-Adaptive Hashing for Image Retrieval. (arXiv:2108.07094v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Depeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "These years much effort has been devoted to improving the accuracy or\nrelevance of the recommendation system. Diversity, a crucial factor which\nmeasures the dissimilarity among the recommended items, received rather little\nscrutiny. Directly related to user satisfaction, diversification is usually\ntaken into consideration after generating the candidate items. However, this\ndecoupled design of diversification and candidate generation makes the whole\nsystem suboptimal. In this paper, we aim at pushing the diversification to the\nupstream candidate generation stage, with the help of Graph Convolutional\nNetworks (GCN). Although GCN based recommendation algorithms have shown great\npower in modeling complex collaborative filtering effect to improve the\naccuracy of recommendation, how diversity changes is ignored in those advanced\nworks. We propose to perform rebalanced neighbor discovering, category-boosted\nnegative sampling and adversarial learning on top of GCN. We conduct extensive\nexperiments on real-world datasets. Experimental results verify the\neffectiveness of our proposed method on diversification. Further ablation\nstudies validate that our proposed method significantly alleviates the\naccuracy-diversity dilemma.",
          "link": "http://arxiv.org/abs/2108.06952",
          "publishedOn": "2021-08-17T01:54:46.550Z",
          "wordCount": 598,
          "title": "DGCN: Diversified Recommendation with Graph Convolutional Networks. (arXiv:2108.06952v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noor_K/0/1/0/all/0/1\">Kawsar Noor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roguski_L/0/1/0/all/0/1\">Lukasz Roguski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handy_A/0/1/0/all/0/1\">Alex Handy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klapaukh_R/0/1/0/all/0/1\">Roman Klapaukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folarin_A/0/1/0/all/0/1\">Amos Folarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romao_L/0/1/0/all/0/1\">Luis Romao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteson_J/0/1/0/all/0/1\">Joshua Matteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lea_N/0/1/0/all/0/1\">Nathan Lea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Leilei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Wai Keong Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Anoop Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1\">Richard J Dobson</a>",
          "description": "As more healthcare organisations transition to using electronic health record\n(EHR) systems it is important for these organisations to maximise the secondary\nuse of their data to support service improvement and clinical research. These\norganisations will find it challenging to have systems which can mine\ninformation from the unstructured data fields in the record (clinical notes,\nletters etc) and more practically have such systems interact with all of the\nhospitals data systems (legacy and current). To tackle this problem at\nUniversity College London Hospitals, we have deployed an enhanced version of\nthe CogStack platform; an information retrieval platform with natural language\nprocessing capabilities which we have configured to process the hospital's\nexisting and legacy records. The platform has improved data ingestion\ncapabilities as well as better tools for natural language processing. To date\nwe have processed over 18 million records and the insights produced from\nCogStack have informed a number of clinical research use cases at the\nhospitals.",
          "link": "http://arxiv.org/abs/2108.06835",
          "publishedOn": "2021-08-17T01:54:46.542Z",
          "wordCount": 628,
          "title": "Deployment of a Free-Text Analytics Platform at a UK National Health Service Research Hospital: CogStack at University College London Hospitals. (arXiv:2108.06835v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lijuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Semantic text matching is a critical problem in information retrieval.\nRecently, deep learning techniques have been widely used in this area and\nobtained significant performance improvements. However, most models are black\nboxes and it is hard to understand what happened in the matching process, due\nto the poor interpretability of deep learning. This paper aims at tackling this\nproblem. The key idea is to test whether existing deep text matching methods\nsatisfy some fundamental heuristics in information retrieval. Specifically,\nfour heuristics are used in our study, i.e., term frequency constraint, term\ndiscrimination constraint, length normalization constraints, and TF-length\nconstraint. Since deep matching models usually contain many parameters, it is\ndifficult to conduct a theoretical study for these complicated functions. In\nthis paper, We propose an empirical testing method. Specifically, We first\nconstruct some queries and documents to make them satisfy the assumption in a\nconstraint, and then test to which extend a deep text matching model trained on\nthe original dataset satisfies the corresponding constraint. Besides, a famous\nattribution based interpretation method, namely integrated gradient, is adopted\nto conduct detailed analysis and guide for feasible improvement. Experimental\nresults on LETOR 4.0 and MS Marco show that all the investigated deep text\nmatching methods, both representation and interaction based methods, satisfy\nthe above constraints with high probabilities in statistics. We further extend\nthese constraints to the semantic settings, which are shown to be better\nsatisfied for all the deep text matching models. These empirical findings give\nclear understandings on why deep text matching models usually perform well in\ninformation retrieval. We believe the proposed evaluation methodology will be\nuseful for testing future deep text matching models.",
          "link": "http://arxiv.org/abs/2108.07081",
          "publishedOn": "2021-08-17T01:54:46.459Z",
          "wordCount": 716,
          "title": "Toward the Understanding of Deep Text Matching Models for Information Retrieval. (arXiv:2108.07081v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1\">Mengting Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yusan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>",
          "description": "Modeling inter-dependencies between time-series is the key to achieve high\nperformance in anomaly detection for multivariate time-series data. The\nde-facto solution to model the dependencies is to feed the data into a\nrecurrent neural network (RNN). However, the fully connected network structure\nunderneath the RNN (either GRU or LSTM) assumes a static and complete\ndependency graph between time-series, which may not hold in many real-world\napplications. To alleviate this assumption, we propose a dynamic bipartite\ngraph structure to encode the inter-dependencies between time-series. More\nconcretely, we model time series as one type of nodes, and the time series\nsegments (regarded as event) as another type of nodes, where the edge between\ntwo types of nodes describe a temporal pattern occurred on a specific time\nseries at a certain time. Based on this design, relations between time series\ncan be explicitly modelled via dynamic connections to event nodes, and the\nmultivariate time-series anomaly detection problem can be formulated as a\nself-supervised, edge stream prediction problem in dynamic graphs. We conducted\nextensive experiments to demonstrate the effectiveness of the design.",
          "link": "http://arxiv.org/abs/2108.06783",
          "publishedOn": "2021-08-17T01:54:46.433Z",
          "wordCount": 629,
          "title": "Event2Graph: Event-driven Bipartite Graph for Multivariate Time-series Anomaly Detection. (arXiv:2108.06783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "The newly emerged transformer technology has a tremendous impact on NLP\nresearch. In the general English domain, transformer-based models have achieved\nstate-of-the-art performances on various NLP benchmarks. In the clinical\ndomain, researchers also have investigated transformer models for clinical\napplications. The goal of this study is to systematically explore three widely\nused transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical\nrelation extraction and develop an open-source package with clinical\npre-trained transformer-based models to facilitate information extraction in\nthe clinical domain. We developed a series of clinical RE models based on three\ntransformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these\nmodels using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2\nchallenges. We compared two classification strategies (binary vs. multi-class\nclassification) and investigated two approaches to generate candidate relations\nin different experimental settings. In this study, we compared three\ntransformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We\ndemonstrated that the RoBERTa-clinical RE model achieved the best performance\non the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2\ndataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our\nresults indicated that the binary classification strategy consistently\noutperformed the multi-class classification strategy for clinical relation\nextraction. Our methods and models are publicly available at\nhttps://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction.\nWe believe this work will improve current practice on clinical relation\nextraction and other related NLP tasks in the biomedical domain.",
          "link": "http://arxiv.org/abs/2107.08957",
          "publishedOn": "2021-08-17T01:54:46.403Z",
          "wordCount": 708,
          "title": "Clinical Relation Extraction Using Transformer-based Models. (arXiv:2107.08957v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>",
          "description": "Session-based recommendation systems suggest relevant items to users by\nmodeling user behavior and preferences using short-term anonymous sessions.\nExisting methods leverage Graph Neural Networks (GNNs) that propagate and\naggregate information from neighboring nodes i.e., local message passing. Such\ngraph-based architectures have representational limits, as a single sub-graph\nis susceptible to overfit the sequential dependencies instead of accounting for\ncomplex transitions between items in different sessions. We propose using a\nTransformer in combination with a target attentive GNN, which allows richer\nRepresentation Learning. Our experimental results and ablation show that our\nproposed method is competitive with the existing methods on real-world\nbenchmark datasets, improving on graph-based hypotheses.",
          "link": "http://arxiv.org/abs/2107.01516",
          "publishedOn": "2021-08-17T01:54:46.393Z",
          "wordCount": 570,
          "title": "Improved Representation Learning for Session-based Recommendation. (arXiv:2107.01516v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziwei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "In order to model the evolution of user preference, we should learn user/item\nembeddings based on time-ordered item purchasing sequences, which is defined as\nSequential Recommendation (SR) problem. Existing methods leverage sequential\npatterns to model item transitions. However, most of them ignore crucial\ntemporal collaborative signals, which are latent in evolving user-item\ninteractions and coexist with sequential patterns. Therefore, we propose to\nunify sequential patterns and temporal collaborative signals to improve the\nquality of recommendation, which is rather challenging. Firstly, it is hard to\nsimultaneously encode sequential patterns and collaborative signals. Secondly,\nit is non-trivial to express the temporal effects of collaborative signals.\n\nHence, we design a new framework Temporal Graph Sequential Recommender\n(TGSRec) upon our defined continuous-time bi-partite graph. We propose a novel\nTemporal Collaborative Trans-former (TCT) layer in TGSRec, which advances the\nself-attention mechanism by adopting a novel collaborative attention. TCT layer\ncan simultaneously capture collaborative signals from both users and items, as\nwell as considering temporal dynamics inside sequential patterns. We propagate\nthe information learned fromTCTlayerover the temporal graph to unify sequential\npatterns and temporal collaborative signals. Empirical results on five datasets\nshow that TGSRec significantly outperforms other baselines, in average up to\n22.5% and 22.1%absolute improvements in Recall@10and MRR, respectively.",
          "link": "http://arxiv.org/abs/2108.06625",
          "publishedOn": "2021-08-17T01:54:46.387Z",
          "wordCount": 650,
          "title": "Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer. (arXiv:2108.06625v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yankai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Menglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Ziqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jian Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "Aiming to alleviate data sparsity and cold-start problems of traditional\nrecommender systems, incorporating knowledge graphs (KGs) to supplement\nauxiliary information has recently gained considerable attention. Via unifying\nthe KG with user-item interactions into a tripartite graph, recent works\nexplore the graph topologies to learn the low-dimensional representations of\nusers and items with rich semantics. However, these real-world tripartite\ngraphs are usually scale-free, the intrinsic hierarchical graph structures of\nwhich are underemphasized in existing works, consequently, leading to\nsuboptimal recommendation performance.\n\nTo address this issue and provide more accurate recommendation, we propose a\nknowledge-aware recommendation method with the hyperbolic geometry, namely\nLorentzian Knowledge-enhanced Graph convolutional networks for Recommendation\n(LKGR). LKGR facilitates better modeling of scale-free tripartite graphs after\nthe data unification. Specifically, we employ different information propagation\nstrategies in the hyperbolic space to explicitly encode heterogeneous\ninformation from historical interactions and KGs. Our proposed knowledge-aware\nattention mechanism enables the model to automatically measure the information\ncontribution, producing the coherent information aggregation in the hyperbolic\nspace. Extensive experiments on three real-world benchmarks demonstrate that\nLKGR outperforms state-of-the-art methods by 2.2-29.9% of Recall@20 on Top-K\nrecommendation.",
          "link": "http://arxiv.org/abs/2108.06468",
          "publishedOn": "2021-08-17T01:54:46.379Z",
          "wordCount": 616,
          "title": "Modeling Scale-free Graphs for Knowledge-aware Recommendation. (arXiv:2108.06468v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Tables are often created with hierarchies, but existing works on table\nreasoning mainly focus on flat tables and neglect hierarchical tables.\nHierarchical tables challenge existing methods by hierarchical indexing, as\nwell as implicit relationships of calculation and semantics. This work presents\nHiTab, a free and open dataset for the research community to study question\nanswering (QA) and natural language generation (NLG) over hierarchical tables.\nHiTab is a cross-domain dataset constructed from a wealth of statistical\nreports and Wikipedia pages, and has unique characteristics: (1) nearly all\ntables are hierarchical, and (2) both target sentences for NLG and questions\nfor QA are revised from high-quality descriptions in statistical reports that\nare meaningful and diverse. (3) HiTab provides fine-grained annotations on both\nentity and quantity alignment. Targeting hierarchical structure, we devise a\nnovel hierarchy-aware logical form for symbolic reasoning over tables, which\nshows high effectiveness. Then given annotations of entity and quantity\nalignment, we propose partially supervised training, which helps models to\nlargely reduce spurious predictions in the QA task. In the NLG task, we find\nthat entity and quantity alignment also helps NLG models to generate better\nresults in a conditional generation setting. Experiment results of\nstate-of-the-art baselines suggest that this dataset presents a strong\nchallenge and a valuable benchmark for future research.",
          "link": "http://arxiv.org/abs/2108.06712",
          "publishedOn": "2021-08-17T01:54:46.355Z",
          "wordCount": 668,
          "title": "HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. (arXiv:2108.06712v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongjun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>",
          "description": "Sequential Recommendationdescribes a set of techniques to model dynamic user\nbehavior in order to predict future interactions in sequential user data. At\ntheir core, such approaches model transition probabilities between items in a\nsequence, whether through Markov chains, recurrent networks, or more recently,\nTransformers. However both old and new issues remain, including data-sparsity\nand noisy data; such issues can impair the performance, especially in complex,\nparameter-hungry models. In this paper, we investigate the application of\ncontrastive Self-Supervised Learning (SSL) to the sequential recommendation, as\na way to alleviate some of these issues. Contrastive SSL constructs\naugmentations from unlabelled instances, where agreements among positive pairs\nare maximized. It is challenging to devise a contrastive SSL framework for a\nsequential recommendation, due to its discrete nature, correlations among\nitems, and skewness of length distributions. To this end, we propose a novel\nframework, Contrastive Self-supervised Learning for sequential Recommendation\n(CoSeRec). We introduce two informative augmentation operators leveraging item\ncorrelations to create high-quality views for contrastive learning.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof the proposed method on improving model performance and the robustness\nagainst sparse and noisy data. Our implementation is available online at\n\\url{https://github.com/YChen1993/CoSeRec}",
          "link": "http://arxiv.org/abs/2108.06479",
          "publishedOn": "2021-08-17T01:54:46.349Z",
          "wordCount": 643,
          "title": "Contrastive Self-supervised Sequential Recommendation with Robust Augmentation. (arXiv:2108.06479v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hillmann_P/0/1/0/all/0/1\">Peter Hillmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heiland_E/0/1/0/all/0/1\">Erik Heiland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karcher_A/0/1/0/all/0/1\">Andreas Karcher</a>",
          "description": "Metadata are like the steam engine of the 21st century, driving businesses\nand offer multiple enhancements. Nevertheless, many companies are unaware that\nthese data can be used efficiently to improve their own operation. This is\nwhere the Enterprise Architecture Framework comes in. It empowers an\norganisation to get a clear view of their business, application, technical and\nphysical layer. This modelling approach is an established method for\norganizations to take a deeper look into their structure and processes. The\ndevelopment of such models requires a great deal of effort, is carried out\nmanually by interviewing stakeholders and requires continuous maintenance. Our\nnew approach enables the automated mining of Enterprise Architecture models.\nThe system uses common technologies to collect the metadata based on network\ntraffic, log files and other information in an organisation. Based on this, the\nnew approach generates EA models with the desired views points. Furthermore, a\nrule and knowledge-based reasoning is used to obtain a holistic overview. This\noffers a strategic decision support from business structure over process design\nup to planning the appropriate support technology. Therefore, it forms the base\nfor organisations to act in an agile way. The modelling can be performed in\ndifferent modelling languages, including ArchiMate and the Nato Architecture\nFramework (NAF). The designed approach is already evaluated on a small company\nwith multiple services and an infrastructure with several nodes.",
          "link": "http://arxiv.org/abs/2108.06696",
          "publishedOn": "2021-08-17T01:54:46.333Z",
          "wordCount": 670,
          "title": "Automated Enterprise Architecture Model Mining. (arXiv:2108.06696v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David/0/1/0/all/0/1\">David</a> (Xuejun) <a href=\"http://arxiv.org/find/cs/1/au:+Wang/0/1/0/all/0/1\">Wang</a>",
          "description": "Recommender systems (RecSys) have been well developed to assist user decision\nmaking. Traditional RecSys usually optimize a single objective (e.g., rating\nprediction errors or ranking quality) in the model. There is an emerging demand\nin multi-objective optimization recently in RecSys, especially in the area of\nmulti-stakeholder and multi-task recommender systems. This article provides an\noverview of multi-objective recommendations, followed by the discussions with\ncase studies. The document is considered as a supplementary material for our\ntutorial on multi-objective recommendations at ACM SIGKDD 2021.",
          "link": "http://arxiv.org/abs/2108.06367",
          "publishedOn": "2021-08-17T01:54:46.298Z",
          "wordCount": 499,
          "title": "Multi-Objective Recommendations: A Tutorial. (arXiv:2108.06367v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_R/0/1/0/all/0/1\">Rohan Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Maheep Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurya_C/0/1/0/all/0/1\">Chandresh Kumar Maurya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1\">Shitala Prasad</a>",
          "description": "Intelligent recommendation and reminder systems are the need of the\nfast-pacing life. Current intelligent systems such as Siri, Google Assistant,\nMicrosoft Cortona, etc., have limited capability. For example, if you want to\nwake up at 6 am because you have an upcoming trip, you have to set the alarm\nmanually. Besides, these systems do not recommend or remind what else to carry,\nsuch as carrying an umbrella during a likely rain. The present work proposes a\nsystem that takes an email as input and returns a recommendation-cumreminder\nlist. As a first step, we parse the emails, recognize the entities using named\nentity recognition (NER). In the second step, information retrieval over the\nweb is done to identify nearby places, climatic conditions, etc. Imperative\nsentences from the reviews of all places are extracted and passed to the object\nextraction module. The main challenge lies in extracting the objects (items) of\ninterest from the review. To solve it, a modified Machine Reading\nComprehension-NER (MRC-NER) model is trained to tag objects of interest by\nformulating annotation rules as a query. The objects so found are recommended\nto the user one day in advance. The final reminder list of objects is pruned by\nour proposed model for tracking objects kept during the \"packing activity.\"\nEventually, when the user leaves for the event/trip, an alert is sent\ncontaining the reminding list items. Our approach achieves superior performance\ncompared to several baselines by as much as 30% on recall and 10% on precision.",
          "link": "http://arxiv.org/abs/2108.06206",
          "publishedOn": "2021-08-16T00:47:33.990Z",
          "wordCount": 676,
          "title": "An Intelligent Recommendation-cum-Reminder System. (arXiv:2108.06206v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shangwen Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">QiaoQiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Recently, dense passage retrieval has become a mainstream approach to finding\nrelevant information in various natural language processing tasks. A number of\nstudies have been devoted to improving the widely adopted dual-encoder\narchitecture. However, most of the previous studies only consider query-centric\nsimilarity relation when learning the dual-encoder retriever. In order to\ncapture more comprehensive similarity relations, we propose a novel approach\nthat leverages both query-centric and PAssage-centric sImilarity Relations\n(called PAIR) for dense passage retrieval. To implement our approach, we make\nthree major technical contributions by introducing formal formulations of the\ntwo kinds of similarity relations, generating high-quality pseudo labeled data\nvia knowledge distillation, and designing an effective two-stage training\nprocedure that incorporates passage-centric similarity relation constraint.\nExtensive experiments show that our approach significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions datasets.",
          "link": "http://arxiv.org/abs/2108.06027",
          "publishedOn": "2021-08-16T00:47:33.945Z",
          "wordCount": 595,
          "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolova_M/0/1/0/all/0/1\">Marina Sokolova</a>",
          "description": "Reddit.com is a popular social media platform among young people. Reddit\nusers share their stories to seek support from other users, especially during\nthe Covid-19 pandemic. Messages posted on Reddit and their content have\nprovided researchers with opportunity to analyze public concerns. In this\nstudy, we analyzed sentiments of COVID-related messages posted on r/Depression.\nOur study poses the following questions: a) What are the common topics that the\nReddit users discuss? b) Can we use these topics to classify sentiments of the\nposts? c) What matters concern people more during the pandemic?\n\nKey Words: Sentiment Classification, Depression, COVID-19, Reddit, LDA, BERT",
          "link": "http://arxiv.org/abs/2108.06215",
          "publishedOn": "2021-08-16T00:47:33.249Z",
          "wordCount": 598,
          "title": "Sentiment Analysis of the COVID-related r/Depression Posts. (arXiv:2108.06215v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1\">Pavan Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1\">Alexander Lerch</a>",
          "description": "Several automatic approaches for objective music performance assessment (MPA)\nhave been proposed in the past, however, existing systems are not yet capable\nof reliably predicting ratings with the same accuracy as professional judges.\nThis study investigates contrastive learning as a potential method to improve\nexisting MPA systems. Contrastive learning is a widely used technique in\nrepresentation learning to learn a structured latent space capable of\nseparately clustering multiple classes. It has been shown to produce state of\nthe art results for image-based classification problems. We introduce a\nweighted contrastive loss suitable for regression tasks applied to a\nconvolutional neural network and show that contrastive loss results in\nperformance gains in regression tasks for MPA. Our results show that\ncontrastive-based methods are able to match and exceed SoTA performance for MPA\nregression tasks by creating better class clusters within the latent space of\nthe neural networks.",
          "link": "http://arxiv.org/abs/2108.01711",
          "publishedOn": "2021-08-16T00:47:32.398Z",
          "wordCount": 595,
          "title": "Improving Music Performance Assessment with Contrastive Learning. (arXiv:2108.01711v1 [cs.SD] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jeongwhan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jinsung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>",
          "description": "Collaborative filtering (CF) is a long-standing problem of recommender\nsystems. Many novel methods have been proposed, ranging from classical matrix\nfactorization to recent graph convolutional network-based approaches. After\nrecent fierce debates, researchers started to focus on linear graph\nconvolutional networks (GCNs) with a layer combination, which show\nstate-of-the-art accuracy in many datasets. In this work, we extend them based\non neural ordinary differential equations (NODEs), because the linear GCN\nconcept can be interpreted as a differential equation, and present the method\nof Learnable-Time ODE-based Collaborative Filtering (LT-OCF). The main novelty\nin our method is that after redesigning linear GCNs on top of the NODE regime,\ni) we learn the optimal architecture rather than relying on manually designed\nones, ii) we learn smooth ODE solutions that are considered suitable for CF,\nand iii) we test with various ODE solvers that internally build a diverse set\nof neural network connections. We also present a novel training method\nspecialized to our method. In our experiments with three benchmark datasets,\nGowalla, Yelp2018, and Amazon-Book, our method consistently shows better\naccuracy than existing methods, e.g., a recall of 0.0411 by LightGCN vs. 0.0442\nby LT-OCF and an NDCG of 0.0315 by LightGCN vs. 0.0341 by LT-OCF in\nAmazon-Book. One more important discovery in our experiments that is worth\nmentioning is that our best accuracy was achieved by dense connections rather\nthan linear connections.",
          "link": "http://arxiv.org/abs/2108.06208",
          "publishedOn": "2021-08-16T00:47:32.305Z",
          "wordCount": 673,
          "title": "LT-OCF: Learnable-Time ODE-based Collaborative Filtering. (arXiv:2108.06208v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>",
          "description": "The advent of contextualised language models has brought gains in search\neffectiveness, not just when applied for re-ranking the output of classical\nweighting models such as BM25, but also when used directly for passage indexing\nand retrieval, a technique which is called dense retrieval. In the existing\nliterature in neural ranking, two dense retrieval families have become\napparent: single representation, where entire passages are represented by a\nsingle embedding (usually BERT's [CLS] token, as exemplified by the recent ANCE\napproach), or multiple representations, where each token in a passage is\nrepresented by its own embedding (as exemplified by the recent ColBERT\napproach). These two families have not been directly compared. However, because\nof the likely importance of dense retrieval moving forward, a clear\nunderstanding of their advantages and disadvantages is paramount. To this end,\nthis paper contributes a direct study on their comparative effectiveness,\nnoting situations where each method under/over performs w.r.t. each other, and\nw.r.t. a BM25 baseline. We observe that, while ANCE is more efficient than\nColBERT in terms of response time and memory usage, multiple representations\nare statistically more effective than the single representations for MAP and\nMRR@10. We also show that multiple representations obtain better improvements\nthan single representations for queries that are the hardest for BM25, as well\nas for definitional queries, and those with complex information needs.",
          "link": "http://arxiv.org/abs/2108.06279",
          "publishedOn": "2021-08-16T00:47:32.256Z",
          "wordCount": 668,
          "title": "On Single and Multiple Representations in Dense Passage Retrieval. (arXiv:2108.06279v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hansi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "Modern E-commerce websites contain heterogeneous sources of information, such\nas numerical ratings, textual reviews and images. These information can be\nutilized to assist recommendation. Through textual reviews, a user explicitly\nexpress her affinity towards the item. Previous researchers found that by using\nthe information extracted from these reviews, we can better profile the users'\nexplicit preferences as well as the item features, leading to the improvement\nof recommendation performance. However, most of the previous algorithms were\nonly utilizing the review information for explicit-feedback problem i.e. rating\nprediction, and when it comes to implicit-feedback ranking problem such as\ntop-N recommendation, the usage of review information has not been fully\nexplored. Seeing this gap, in this work, we investigate the effectiveness of\ntextual review information for top-N recommendation under E-commerce settings.\nWe adapt several SOTA review-based rating prediction models for top-N\nrecommendation tasks and compare them to existing top-N recommendation models\nfrom both performance and efficiency. We find that models utilizing only review\ninformation can not achieve better performances than vanilla implicit-feedback\nmatrix factorization method. When utilizing review information as a regularizer\nor auxiliary information, the performance of implicit-feedback matrix\nfactorization method can be further improved. However, the optimal model\nstructure to utilize textual reviews for E-commerce top-N recommendation is yet\nto be determined.",
          "link": "http://arxiv.org/abs/2106.09665",
          "publishedOn": "2021-08-16T00:47:32.209Z",
          "wordCount": 695,
          "title": "Understanding the Effectiveness of Reviews in E-commerce Top-N Recommendation. (arXiv:2106.09665v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parasrampuria_R/0/1/0/all/0/1\">Rohan Parasrampuria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Suchandra Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Dhrubasish Sarkar</a>",
          "description": "In today's tech-savvy world every industry is trying to formulate methods for\nrecommending products by combining several techniques and algorithms to form a\npool that would bring forward the most enhanced models for making the\npredictions. Building on these lines is our paper focused on the application of\nsentiment analysis for recommendation in the insurance domain. We tried\nbuilding the following Machine Learning models namely, Logistic Regression,\nMultinomial Naive Bayes, and the mighty Random Forest for analyzing the\npolarity of a given feedback line given by a customer. Then we used this\npolarity along with other attributes like Age, Gender, Locality, Income, and\nthe list of other products already purchased by our existing customers as input\nfor our recommendation model. Then we matched the polarity score along with the\nuser's profiles and generated the list of insurance products to be recommended\nin descending order. Despite our model's simplicity and the lack of the key\ndata sets, the results seemed very logical and realistic. So, by developing the\nmodel with more enhanced methods and with access to better and true data\ngathered from an insurance industry may be the sector could be very well\nbenefitted from the amalgamation of sentiment analysis with a recommendation.",
          "link": "http://arxiv.org/abs/2108.06210",
          "publishedOn": "2021-08-16T00:47:32.176Z",
          "wordCount": 642,
          "title": "Recommending Insurance products by using Users' Sentiments. (arXiv:2108.06210v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gizinski_S/0/1/0/all/0/1\">Stanis&#x142;aw Gizinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzba_M/0/1/0/all/0/1\">Micha&#x142; Kuzba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielinski_B/0/1/0/all/0/1\">Bartosz Pielinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sienkiewicz_J/0/1/0/all/0/1\">Julian Sienkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laniewski_S/0/1/0/all/0/1\">Stanis&#x142;aw &#x141;aniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1\">Przemys&#x142;aw Biecek</a>",
          "description": "The growing number of AI applications, also for high-stake decisions,\nincreases the interest in Explainable and Interpretable Machine Learning\n(XI-ML). This trend can be seen both in the increasing number of regulations\nand strategies for developing trustworthy AI and the growing number of\nscientific papers dedicated to this topic. To ensure the sustainable\ndevelopment of AI, it is essential to understand the dynamics of the impact of\nregulation on research papers as well as the impact of scientific discourse on\nAI-related policies. This paper introduces a novel framework for joint analysis\nof AI-related policy documents and eXplainable Artificial Intelligence (XAI)\nresearch papers. The collected documents are enriched with metadata and\ninterconnections, using various NLP methods combined with a methodology\ninspired by Institutional Grammar. Based on the information extracted from\ncollected documents, we showcase a series of analyses that help understand\ninteractions, similarities, and differences between documents at different\nstages of institutionalization. To the best of our knowledge, this is the first\nwork to use automatic language analysis tools to understand the dynamics\nbetween XI-ML methods and regulations. We believe that such a system\ncontributes to better cooperation between XAI researchers and AI policymakers.",
          "link": "http://arxiv.org/abs/2108.06216",
          "publishedOn": "2021-08-16T00:47:32.150Z",
          "wordCount": 662,
          "title": "MAIR: Framework for mining relationships between research articles, strategies, and regulations in the field of explainable artificial intelligence. (arXiv:2108.06216v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhuoyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xingzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1\">Rui Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee Keong Kwoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chi Xu</a>",
          "description": "Latent factor models play a dominant role among recommendation techniques.\nHowever, most of the existing latent factor models assume both historical\ninteractions and embedding dimensions are independent of each other, and thus\nregrettably ignore the high-order interaction information among historical\ninteractions and embedding dimensions. In this paper, we propose a novel latent\nfactor model called COMET (COnvolutional diMEnsion inTeraction), which\nsimultaneously model the high-order interaction patterns among historical\ninteractions and embedding dimensions. To be specific, COMET stacks the\nembeddings of historical interactions horizontally at first, which results in\ntwo \"embedding maps\". In this way, internal interactions and dimensional\ninteractions can be exploited by convolutional neural networks with kernels of\ndifferent sizes simultaneously. A fully-connected multi-layer perceptron is\nthen applied to obtain two interaction vectors. Lastly, the representations of\nusers and items are enriched by the learnt interaction vectors, which can\nfurther be used to produce the final prediction. Extensive experiments and\nablation studies on various public implicit feedback datasets clearly\ndemonstrate the effectiveness and the rationality of our proposed method.",
          "link": "http://arxiv.org/abs/2007.14129",
          "publishedOn": "2021-08-16T00:47:32.116Z",
          "wordCount": 646,
          "title": "COMET: Convolutional Dimension Interaction for Collaborative Filtering. (arXiv:2007.14129v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>",
          "description": "Hateful and offensive content detection has been extensively explored in a\nsingle modality such as text. However, such toxic information could also be\ncommunicated via multimodal content such as online memes. Therefore, detecting\nmultimodal hateful content has recently garnered much attention in academic and\nindustry research communities. This paper aims to contribute to this emerging\nresearch topic by proposing DisMultiHate, which is a novel framework that\nperformed the classification of multimodal hateful content. Specifically,\nDisMultiHate is designed to disentangle target entities in multimodal memes to\nimprove hateful content classification and explainability. We conduct extensive\nexperiments on two publicly available hateful and offensive memes datasets. Our\nexperiment results show that DisMultiHate is able to outperform\nstate-of-the-art unimodal and multimodal baselines in the hateful meme\nclassification task. Empirical case studies were also conducted to demonstrate\nDisMultiHate's ability to disentangle target entities in memes and ultimately\nshowcase DisMultiHate's explainability of the multimodal hateful content\nclassification task.",
          "link": "http://arxiv.org/abs/2108.06207",
          "publishedOn": "2021-08-16T00:47:32.109Z",
          "wordCount": 597,
          "title": "Disentangling Hate in Online Memes. (arXiv:2108.06207v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>",
          "description": "Both latent semantic analysis (LSA) and correspondence analysis (CA) use a\nsingular value decomposition (SVD) for dimensionality reduction. In this\narticle, LSA and CA are compared from a theoretical point of view and applied\nin both a toy example and an authorship attribution example. In text mining\ninterest goes out to the relationships among documents and terms: for example,\nwhat terms are more often used in what documents. However, the LSA solution\ndisplays a mix of marginal effects and these relationships. It appears that CA\nhas more attractive properties than LSA. One such property is that, in CA, the\neffect of the margins is effectively eliminated, so that the CA solution is\noptimally suited to focus on the relationships among documents and terms. Three\nmechanisms are distinguished to weight documents and terms, and a unifying\nframework is proposed that includes these three mechanisms and includes both CA\nand LSA as special cases. In the authorship attribution example, the national\nanthem of the Netherlands, the application of the discussed methods is\nillustrated.",
          "link": "http://arxiv.org/abs/2108.06197",
          "publishedOn": "2021-08-16T00:47:32.058Z",
          "wordCount": 621,
          "title": "A Comparison of Latent Semantic Analysis and Correspondence Analysis for Text Mining. (arXiv:2108.06197v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>",
          "description": "Ranking models have achieved promising results, but it remains challenging to\ndesign personalized ranking systems to leverage user profiles and semantic\nrepresentations between queries and documents. In this paper, we propose a\ntopic-based personalized ranking model (TPRM) that integrates user topical\nprofile with pretrained contextualized term representations to tailor the\ngeneral document ranking list. Experiments on the real-world dataset\ndemonstrate that TPRM outperforms state-of-the-art ad-hoc ranking models and\npersonalized ranking models significantly.",
          "link": "http://arxiv.org/abs/2108.06014",
          "publishedOn": "2021-08-16T00:47:32.006Z",
          "wordCount": 511,
          "title": "TPRM: A Topic-based Personalized Ranking Model for Web Search. (arXiv:2108.06014v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutsch_J/0/1/0/all/0/1\">Julian Gutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietsch_M/0/1/0/all/0/1\">Malte Pietsch</a>",
          "description": "The evaluation of question answering models compares ground-truth annotations\nwith model predictions. However, as of today, this comparison is mostly\nlexical-based and therefore misses out on answers that have no lexical overlap\nbut are still semantically similar, thus treating correct answers as false.\nThis underestimation of the true performance of models hinders user acceptance\nin applications and complicates a fair comparison of different models.\nTherefore, there is a need for an evaluation metric that is based on semantics\ninstead of pure string similarity. In this short paper, we present SAS, a\ncross-encoder-based metric for the estimation of semantic answer similarity,\nand compare it to seven existing metrics. To this end, we create an English and\na German three-way annotated evaluation dataset containing pairs of answers\nalong with human judgment of their semantic similarity, which we release along\nwith an implementation of the SAS metric and the experiments. We find that\nsemantic similarity metrics based on recent transformer models correlate much\nbetter with human judgment than traditional lexical similarity metrics on our\ntwo newly created datasets and one dataset from related work.",
          "link": "http://arxiv.org/abs/2108.06130",
          "publishedOn": "2021-08-16T00:47:31.878Z",
          "wordCount": 622,
          "title": "Semantic Answer Similarity for Evaluating Question Answering Models. (arXiv:2108.06130v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Meizhen Ding</a>",
          "description": "Query expansion with pseudo-relevance feedback (PRF) is a powerful approach\nto enhance the effectiveness in information retrieval. Recently, with the rapid\nadvance of deep learning techniques, neural text generation has achieved\npromising success in many natural language tasks. To leverage the strength of\ntext generation for information retrieval, in this article, we propose a novel\napproach which effectively integrates text generation models into PRF-based\nquery expansion. In particular, our approach generates augmented query terms\nvia neural text generation models conditioned on both the initial query and\npseudo-relevance feedback. Moreover, in order to train the generative model, we\nadopt the conditional generative adversarial nets (CGANs) and propose the\nPRF-CGAN method in which both the generator and the discriminator are\nconditioned on the pseudo-relevance feedback. We evaluate the performance of\nour approach on information retrieval tasks using two benchmark datasets. The\nexperimental results show that our approach achieves comparable performance or\noutperforms traditional query expansion methods on both the retrieval and\nreranking tasks.",
          "link": "http://arxiv.org/abs/2108.06010",
          "publishedOn": "2021-08-16T00:47:31.869Z",
          "wordCount": 598,
          "title": "GQE-PRF: Generative Query Expansion with Pseudo-Relevance Feedback. (arXiv:2108.06010v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_P/0/1/0/all/0/1\">Prithiviraj Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhkaran Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achankuju_J/0/1/0/all/0/1\">Josemon Achankuju</a>",
          "description": "We present VESPA, an intentionally simple yet novel zero-shot system for\nlayout, locale, and domain agnostic document extraction. In spite of the\navailability of large corpora of documents, the lack of labeled and validated\ndatasets makes it a challenge to discriminatively train document extraction\nmodels for enterprises. We show that this problem can be addressed by simply\ntransferring the information extraction (IE) task to a natural language\nQuestion-Answering (QA) task without engineering task-specific architectures.\nWe demonstrate the effectiveness of our system by evaluating on a closed corpus\nof real-world retail and tax invoices with multiple complex layouts, domains,\nand geographies. The empirical evaluation shows that our system outperforms 4\nprominent commercial invoice solutions that use discriminatively trained models\nwith architectures specifically crafted for invoice extraction. We extracted 6\nfields with zero upfront human annotation or training with an Avg. F1 of 87.50.",
          "link": "http://arxiv.org/abs/2108.06069",
          "publishedOn": "2021-08-16T00:47:31.816Z",
          "wordCount": 585,
          "title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA Ensemble. (arXiv:2108.06069v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>",
          "description": "Recommender systems aim to provide personalized services to users and are\nplaying an increasingly important role in our daily lives. The key of\nrecommender systems is to predict how likely users will interact with items\nbased on their historical online behaviors, e.g., clicks, add-to-cart,\npurchases, etc. To exploit these user-item interactions, there are increasing\nefforts on considering the user-item interactions as a user-item bipartite\ngraph and then performing information propagation in the graph via Graph Neural\nNetworks (GNNs). Given the power of GNNs in graph representation learning,\nthese GNN-based recommendation methods have remarkably boosted the\nrecommendation performance. Despite their success, most existing GNN-based\nrecommender systems overlook the existence of interactions caused by unreliable\nbehaviors (e.g., random/bait clicks) and uniformly treat all the interactions,\nwhich can lead to sub-optimal and unstable performance. In this paper, we\ninvestigate the drawbacks (e.g., non-adaptive propagation and non-robustness)\nof existing GNN-based recommendation methods. To address these drawbacks, we\npropose the Graph Trend Networks for recommendations (GTN) with principled\ndesigns that can capture the adaptive reliability of the interactions.\nComprehensive experiments and ablation studies are presented to verify and\nunderstand the effectiveness of the proposed framework. Our implementation and\ndatasets can be released after publication.",
          "link": "http://arxiv.org/abs/2108.05552",
          "publishedOn": "2021-08-13T01:56:54.203Z",
          "wordCount": 641,
          "title": "Graph Trend Networks for Recommendations. (arXiv:2108.05552v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1\">Chieh Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_K/0/1/0/all/0/1\">Krutika Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Changchen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kathy Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platz_J/0/1/0/all/0/1\">Justin Platz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilardi_A/0/1/0/all/0/1\">Adam Ilardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhvanath_S/0/1/0/all/0/1\">Sriganesh Madhvanath</a>",
          "description": "The item details page (IDP) is a web page on an e-commerce website that\nprovides information on a specific product or item listing. Just below the\ndetails of the item on this page, the buyer can usually find recommendations\nfor other relevant items. These are typically in the form of a series of\nmodules or carousels, with each module containing a set of recommended items.\nThe selection and ordering of these item recommendation modules are intended to\nincrease discover-ability of relevant items and encourage greater user\nengagement, while simultaneously showcasing diversity of inventory and\nsatisfying other business objectives. Item recommendation modules on the IDP\nare often curated and statically configured for all customers, ignoring\nopportunities for personalization. In this paper, we present a scalable\nend-to-end production system to optimize the personalized selection and\nordering of item recommendation modules on the IDP in real-time by utilizing\ndeep neural networks. Through extensive offline experimentation and online A/B\ntesting, we show that our proposed system achieves significantly higher\nclick-through and conversion rates compared to other existing methods. In our\nonline A/B test, our framework improved click-through rate by 2.48% and\npurchase-through rate by 7.34% over a static configuration.",
          "link": "http://arxiv.org/abs/2108.05891",
          "publishedOn": "2021-08-13T01:56:54.182Z",
          "wordCount": 639,
          "title": "Page-level Optimization of e-Commerce Item Recommendations. (arXiv:2108.05891v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>",
          "description": "Recent research demonstrates the effectiveness of using fine-tuned language\nmodels~(LM) for dense retrieval. However, dense retrievers are hard to train,\ntypically requiring heavily engineered fine-tuning pipelines to realize their\nfull potential. In this paper, we identify and address two underlying problems\nof dense retrievers: i)~fragility to training data noise and ii)~requiring\nlarge batches to robustly learn the embedding space. We use the recently\nproposed Condenser pre-training architecture, which learns to condense\ninformation into the dense vector through LM pre-training. On top of it, we\npropose coCondenser, which adds an unsupervised corpus-level contrastive loss\nto warm up the passage embedding space. Retrieval experiments on MS-MARCO,\nNatural Question, and Trivia QA datasets show that coCondenser removes the need\nfor heavy data engineering such as augmentation, synthesis, or filtering, as\nwell as the need for large batch training. It shows comparable performance to\nRocketQA, a state-of-the-art, heavily engineered system, using simple small\nbatch fine-tuning.",
          "link": "http://arxiv.org/abs/2108.05540",
          "publishedOn": "2021-08-13T01:56:54.162Z",
          "wordCount": 586,
          "title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. (arXiv:2108.05540v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Mingjian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indrakanti_S/0/1/0/all/0/1\">Saratchandra Indrakanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannadasan_M/0/1/0/all/0/1\">Manojkumar Rangasamy Kannadasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagherjeiran_A/0/1/0/all/0/1\">Abraham Bagherjeiran</a>",
          "description": "The top search results matching a user query that are displayed on the first\npage are critical to the effectiveness and perception of a search system. A\nsearch ranking system typically orders the results by independent\nquery-document scores to produce a slate of search results. However, such\nunilateral scoring methods may fail to capture inter-document dependencies that\nusers are sensitive to, thus producing a sub-optimal slate. Further, in\npractice, many real-world applications such as e-commerce search require\nenforcing certain distributional criteria at the slate-level, due to business\nobjectives or long term user retention goals. Unilateral scoring of results\ndoes not explicitly support optimizing for such objectives with respect to a\nslate. Hence, solutions to the slate optimization problem must consider the\noptimal selection and order of the documents, along with adherence to\nslate-level distributional criteria. To that end, we propose a hybrid framework\nextended from traditional slate optimization to solve the conditional slate\noptimization problem. We introduce conditional sequential slate optimization\n(CSSO), which jointly learns to optimize for traditional ranking metrics as\nwell as prescribed distribution criteria of documents within the slate. The\nproposed method can be applied to practical real world problems such as\nenforcing diversity in e-commerce search results, mitigating bias in top\nresults and personalization of results. Experiments on public datasets and\nreal-world data from e-commerce datasets show that CSSO outperforms popular\ncomparable ranking methods in terms of adherence to distributional criteria\nwhile producing comparable or better relevance metrics.",
          "link": "http://arxiv.org/abs/2108.05618",
          "publishedOn": "2021-08-13T01:56:54.142Z",
          "wordCount": 678,
          "title": "Conditional Sequential Slate Optimization. (arXiv:2108.05618v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Portenoy_J/0/1/0/all/0/1\">Jason Portenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radensky_M/0/1/0/all/0/1\">Marissa Radensky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_J/0/1/0/all/0/1\">Jevin West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>",
          "description": "Scientific silos can hinder innovation. These information \"filter bubbles\"\nand the growing challenge of information overload limit awareness across the\nliterature, making it difficult to keep track of even narrow areas of interest,\nlet alone discover new ones. Algorithmic curation and recommendation, which\noften prioritize relevance, can further reinforce these bubbles. In response,\nwe describe Bridger, a system for facilitating discovery of scholars and their\nwork, to explore design tradeoffs among relevant and novel recommendations. We\nconstruct a faceted representation of authors using information extracted from\ntheir papers and inferred personas. We explore approaches both for recommending\nnew content and for displaying it in a manner that helps researchers to\nunderstand the work of authors who they are unfamiliar with. In studies with\ncomputer science researchers, our approach substantially improves users'\nabilities to do so. We develop an approach that locates commonalities and\ncontrasts between scientists---retrieving partially similar authors, rather\nthan aiming for strict similarity. We find this approach helps users discover\nauthors useful for generating novel research ideas of relevance to their work,\nat a higher rate than a state-of-art neural model. Our analysis reveals that\nBridger connects authors who have different citation profiles, publish in\ndifferent venues, and are more distant in social co-authorship networks,\nraising the prospect of bridging diverse communities and facilitating\ndiscovery.",
          "link": "http://arxiv.org/abs/2108.05669",
          "publishedOn": "2021-08-13T01:56:54.124Z",
          "wordCount": 674,
          "title": "Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Proper initialization is crucial to the optimization and the generalization\nof neural networks. However, most existing neural recommendation systems\ninitialize the user and item embeddings randomly. In this work, we propose a\nnew initialization scheme for user and item embeddings called Laplacian\nEigenmaps with Popularity-based Regularization for Isolated Data (LEPORID).\nLEPORID endows the embeddings with information regarding multi-scale\nneighborhood structures on the data manifold and performs adaptive\nregularization to compensate for high embedding variance on the tail of the\ndata distribution. Exploiting matrix sparsity, LEPORID embeddings can be\ncomputed efficiently. We evaluate LEPORID in a wide range of neural\nrecommendation models. In contrast to the recent surprising finding that the\nsimple K-nearest-neighbor (KNN) method often outperforms neural recommendation\nsystems, we show that existing neural systems initialized with LEPORID often\nperform on par or better than KNN. To maximize the effects of the\ninitialization, we propose the Dual-Loss Residual Recommendation (DLR2)\nnetwork, which, when initialized with LEPORID, substantially outperforms both\ntraditional and state-of-the-art neural recommender systems.",
          "link": "http://arxiv.org/abs/2106.04993",
          "publishedOn": "2021-08-13T01:56:54.087Z",
          "wordCount": 635,
          "title": "Initialization Matters: Regularizing Manifold-informed Initialization for Neural Recommendation Systems. (arXiv:2106.04993v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_N/0/1/0/all/0/1\">Natasha Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamoto_Y/0/1/0/all/0/1\">Yusuke Kawamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1\">Takao Murakami</a>",
          "description": "Extended differential privacy, a generalization of standard differential\nprivacy (DP) using a general metric, has been widely studied to provide\nrigorous privacy guarantees while keeping high utility. However, existing works\non extended DP are limited to few metrics, such as the Euclidean metric.\nConsequently, they have only a small number of applications, such as\nlocation-based services and document processing. In this paper, we propose a\ncouple of mechanisms providing extended DP with a different metric: angular\ndistance (or cosine distance). Our mechanisms are based on locality sensitive\nhashing (LSH), which can be applied to the angular distance and work well for\npersonal data in a high-dimensional space. We theoretically analyze the privacy\nproperties of our mechanisms, and prove extended DP for input data by taking\ninto account that LSH preserves the original metric only approximately. We\napply our mechanisms to friend matching based on high-dimensional personal data\nwith angular distance in the local model, and evaluate our mechanisms using two\nreal datasets. We show that LDP requires a very large privacy budget and that\nRAPPOR does not work in this application. Then we show that our mechanisms\nenable friend matching with high utility and rigorous privacy guarantees based\non extended DP.",
          "link": "http://arxiv.org/abs/2010.09393",
          "publishedOn": "2021-08-13T01:56:54.077Z",
          "wordCount": 713,
          "title": "Locality Sensitive Hashing with Extended Differential Privacy. (arXiv:2010.09393v5 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Senzhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kaimin Wei</a>",
          "description": "The purpose of the Session-Based Recommendation System is to predict the\nuser's next click according to the previous session sequence. The current\nstudies generally learn user preferences according to the transitions of items\nin the user's session sequence. However, other effective information in the\nsession sequence, such as user profiles, are largely ignored which may lead to\nthe model unable to learn the user's specific preferences. In this paper, we\npropose a heterogeneous graph neural network-based session recommendation\nmethod, named SR-HetGNN, which can learn session embeddings by heterogeneous\ngraph neural network (HetGNN), and capture the specific preferences of\nanonymous users. Specifically, SR-HetGNN first constructs heterogeneous graphs\ncontaining various types of nodes according to the session sequence, which can\ncapture the dependencies among items, users, and sessions. Second, HetGNN\ncaptures the complex transitions between items and learns the item embeddings\ncontaining user information. Finally, to consider the influence of users' long\nand short-term preferences, local and global session embeddings are combined\nwith the attentional network to obtain the final session embedding. SR-HetGNN\nis shown to be superior to the existing state-of-the-art session-based\nrecommendation methods through extensive experiments over two real large\ndatasets Diginetica and Tmall.",
          "link": "http://arxiv.org/abs/2108.05641",
          "publishedOn": "2021-08-13T01:56:54.065Z",
          "wordCount": 628,
          "title": "Session-based Recommendation with Heterogeneous Graph Neural Network. (arXiv:2108.05641v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihwan Kim</a>",
          "description": "All learning algorithms for recommendations face inevitable and critical\ntrade-off between exploiting partial knowledge of a user's preferences for\nshort-term satisfaction and exploring additional user preferences for long-term\ncoverage. Although exploration is indispensable for long success of a\nrecommender system, the exploration has been considered as the risk to decrease\nuser satisfaction. The reason for the risk is that items chosen for exploration\nfrequently mismatch with the user's interests. To mitigate this risk,\nrecommender systems have mixed items chosen for exploration into a\nrecommendation list, disguising the items as recommendations to elicit feedback\non the items to discover the user's additional tastes. This mix-in approach has\nbeen widely used in many recommenders, but there is rare research, evaluating\nthe effectiveness of the mix-in approach or proposing a new approach for\neliciting user feedback without deceiving users. In this work, we aim to\npropose a new approach for feedback elicitation without any deception and\ncompare our approach to the conventional mix-in approach for evaluation. To\nthis end, we designed a recommender interface that reveals which items are for\nexploration and conducted a within-subject study with 94 MTurk workers. Our\nresults indicated that users left significantly more feedback on items chosen\nfor exploration with our interface. Besides, users evaluated that our new\ninterface is better than the conventional mix-in interface in terms of novelty,\ndiversity, transparency, trust, and satisfaction. Finally, path analysis show\nthat, in only our new interface, exploration caused to increase user-centric\nevaluation metrics. Our work paves the way for how to design an interface,\nwhich utilizes learning algorithm based on users' feedback signals, giving\nbetter user experience and gathering more feedback data.",
          "link": "http://arxiv.org/abs/2108.00151",
          "publishedOn": "2021-08-13T01:56:54.050Z",
          "wordCount": 725,
          "title": "An Empirical Analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Lin Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">XiuQiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Recently, pre-trained language models such as BERT have been applied to\ndocument ranking for information retrieval, which first pre-train a general\nlanguage model on an unlabeled large corpus and then conduct ranking-specific\nfine-tuning on expert-labeled relevance datasets. Ideally, an IR system would\nmodel relevance from a user-system dualism: the user's view and the system's\nview. User's view judges the relevance based on the activities of \"real users\"\nwhile the system's view focuses on the relevance signals from the system side,\ne.g., from the experts or algorithms, etc. Inspired by the user-system\nrelevance views and the success of pre-trained language models, in this paper\nwe propose a novel ranking framework called Pre-Rank that takes both user's\nview and system's view into consideration, under the pre-training and\nfine-tuning paradigm. Specifically, to model the user's view of relevance,\nPre-Rank pre-trains the initial query-document representations based on\nlarge-scale user activities data such as the click log. To model the system's\nview of relevance, Pre-Rank further fine-tunes the model on expert-labeled\nrelevance data. More importantly, the pre-trained representations, are\nfine-tuned together with handcrafted learning-to-rank features under a wide and\ndeep network architecture. In this way, Pre-Rank can model the relevance by\nincorporating the relevant knowledge and signals from both real search users\nand the IR experts. To verify the effectiveness of Pre-Rank, we showed two\nimplementations by using BERT and SetRank as the underlying ranking model,\nrespectively. Experimental results base on three publicly available benchmarks\nshowed that in both of the implementations, Pre-Rank can respectively\noutperform the underlying ranking models and achieved state-of-the-art\nperformances.",
          "link": "http://arxiv.org/abs/2108.05652",
          "publishedOn": "2021-08-13T01:56:54.031Z",
          "wordCount": 699,
          "title": "Modeling Relevance Ranking under the Pre-training and Fine-tuning Paradigm. (arXiv:2108.05652v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>",
          "description": "We introduce the needs for explainable AI that arise from Standard No. 239\nfrom the Basel Committee on Banking Standards (BCBS 239), which outlines 11\nprinciples for effective risk data aggregation and risk reporting for financial\ninstitutions. Of these, explainableAI is necessary for compliance in two key\naspects: data quality, and appropriate reporting for multiple stakeholders. We\ndescribe the implementation challenges for one specific regulatory\nrequirement:that of having a complete data taxonomy that is appropriate for\nfirmwide use. The constantly evolving nature of financial ontologies\nnecessitate a continuous updating process to ensure ongoing compliance.",
          "link": "http://arxiv.org/abs/2108.05401",
          "publishedOn": "2021-08-13T01:56:54.013Z",
          "wordCount": 555,
          "title": "Ontology drift is a challenge for explainable data governance. (arXiv:2108.05401v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1\">Anson Bastos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadgeri_A/0/1/0/all/0/1\">Abhishek Nadgeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarpour_S/0/1/0/all/0/1\">Saeedeh Shekarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulang_I/0/1/0/all/0/1\">Isaiah Onando Mulang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffart_J/0/1/0/all/0/1\">Johannes Hoffart</a>",
          "description": "Recently, several Knowledge Graph Embedding (KGE) approaches have been\ndevised to represent entities and relations in dense vector space and employed\nin downstream tasks such as link prediction. A few KGE techniques address\ninterpretability, i.e., mapping the connectivity patterns of the relations\n(i.e., symmetric/asymmetric, inverse, and composition) to a geometric\ninterpretation such as rotations. Other approaches model the representations in\nhigher dimensional space such as four-dimensional space (4D) to enhance the\nability to infer the connectivity patterns (i.e., expressiveness). However,\nmodeling relation and entity in a 4D space often comes at the cost of\ninterpretability. This paper proposes HopfE, a novel KGE approach aiming to\nachieve the interpretability of inferred relations in the four-dimensional\nspace. We first model the structural embeddings in 3D Euclidean space and view\nthe relation operator as an SO(3) rotation. Next, we map the entity embedding\nvector from a 3D space to a 4D hypersphere using the inverse Hopf Fibration, in\nwhich we embed the semantic information from the KG ontology. Thus, HopfE\nconsiders the structural and semantic properties of the entities without losing\nexpressivity and interpretability. Our empirical results on four well-known\nbenchmarks achieve state-of-the-art performance for the KG completion task.",
          "link": "http://arxiv.org/abs/2108.05774",
          "publishedOn": "2021-08-13T01:56:53.995Z",
          "wordCount": 656,
          "title": "HopfE: Knowledge Graph Representation Learning using Inverse Hopf Fibrations. (arXiv:2108.05774v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasamy_L/0/1/0/all/0/1\">Lakshmi Narayanan Ramasamy</a>",
          "description": "Product retrieval systems have served as the main entry for customers to\ndiscover and purchase products online. With increasing concerns on the\ntransparency and accountability of AI systems, studies on explainable\ninformation retrieval has received more and more attention in the research\ncommunity. Interestingly, in the domain of e-commerce, despite the extensive\nstudies on explainable product recommendation, the studies of explainable\nproduct search is still in an early stage. In this paper, we study how to\nconstruct effective explainable product search by comparing model-agnostic\nexplanation paradigms with model-intrinsic paradigms and analyzing the\nimportant factors that determine the performance of product search\nexplanations. We propose an explainable product search model with\nmodel-intrinsic interpretability and conduct crowdsourcing to compare it with\nthe state-of-the-art explainable product search model with model-agnostic\ninterpretability. We observe that both paradigms have their own advantages and\nthe effectiveness of search explanations on different properties are affected\nby different factors. For example, explanation fidelity is more important for\nuser's overall satisfaction on the system while explanation novelty may be more\nuseful in attracting user purchases. These findings could have important\nimplications for the future studies and design of explainable product search\nengines.",
          "link": "http://arxiv.org/abs/2108.05317",
          "publishedOn": "2021-08-12T01:56:21.092Z",
          "wordCount": 620,
          "title": "Model-agnostic vs. Model-intrinsic Interpretability for Explainable Product Search. (arXiv:2108.05317v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammad Masudur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeasmin_S/0/1/0/all/0/1\">Shamima Yeasmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_C/0/1/0/all/0/1\">Chanchal K. Roy</a>",
          "description": "Being light-weight and cost-effective, IR-based approaches for bug\nlocalization have shown promise in finding software bugs. However, the accuracy\nof these approaches heavily depends on their used bug reports. A significant\nnumber of bug reports contain only plain natural language texts. According to\nexisting studies, IR-based approaches cannot perform well when they use these\nbug reports as search queries. On the other hand, there is a piece of recent\nevidence that suggests that even these natural language-only reports contain\nenough good keywords that could help localize the bugs successfully. On one\nhand, these findings suggest that natural language-only bug reports might be a\nsufficient source for good query keywords. On the other hand, they cast serious\ndoubt on the query selection practices in the IR-based bug localization. In\nthis article, we attempted to clear the sky on this aspect by conducting an\nin-depth empirical study that critically examines the state-of-the-art query\nselection practices in IR-based bug localization. In particular, we use a\ndataset of 2,320 bug reports, employ ten existing approaches from the\nliterature, exploit the Genetic Algorithm-based approach to construct optimal,\nnear-optimal search queries from these bug reports, and then answer three\nresearch questions. We confirmed that the state-of-the-art query construction\napproaches are indeed not sufficient for constructing appropriate queries (for\nbug localization) from certain natural language-only bug reports although they\ncontain such queries. We also demonstrate that optimal queries and non-optimal\nqueries chosen from bug report texts are significantly different in terms of\nseveral keyword characteristics, which has led us to actionable insights.\nFurthermore, we demonstrate 27%--34% improvement in the performance of\nnon-optimal queries through the application of our actionable insights to them.",
          "link": "http://arxiv.org/abs/2108.05341",
          "publishedOn": "2021-08-12T01:56:20.854Z",
          "wordCount": 766,
          "title": "The Forgotten Role of Search Queries in IR-based Bug Localization: An Empirical Study. (arXiv:2108.05341v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Man Lan</a>",
          "description": "Entity alignment (EA) aims to find the equivalent entities in different KGs,\nwhich is a crucial step in integrating multiple KGs. However, most existing EA\nmethods have poor scalability and are unable to cope with large-scale datasets.\nWe summarize three issues leading to such high time-space complexity in\nexisting EA methods: (1) Inefficient graph encoders, (2) Dilemma of negative\nsampling, and (3) \"Catastrophic forgetting\" in semi-supervised learning. To\naddress these challenges, we propose a novel EA method with three new\ncomponents to enable high Performance, high Scalability, and high Robustness\n(PSR): (1) Simplified graph encoder with relational graph sampling, (2)\nSymmetric negative-free alignment loss, and (3) Incremental semi-supervised\nlearning. Furthermore, we conduct detailed experiments on several public\ndatasets to examine the effectiveness and efficiency of our proposed method.\nThe experimental results show that PSR not only surpasses the previous SOTA in\nperformance but also has impressive scalability and robustness.",
          "link": "http://arxiv.org/abs/2108.05278",
          "publishedOn": "2021-08-12T01:56:20.798Z",
          "wordCount": 605,
          "title": "Are Negative Samples Necessary in Entity Alignment? An Approach with High Performance, Scalability and Robustness. (arXiv:2108.05278v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhuoyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1\">Rui Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee-Keong Kwoh</a>",
          "description": "Graph-based recommendation models work well for top-N recommender systems due\nto their capability to capture the potential relationships between entities.\nHowever, most of the existing methods only construct a single global item graph\nshared by all the users and regrettably ignore the diverse tastes between\ndifferent user groups. Inspired by the success of local models for\nrecommendation, this paper provides the first attempt to investigate multiple\nlocal item graphs along with a global item graph for graph-based recommendation\nmodels. We argue that recommendation on global and local graphs outperforms\nthat on a single global graph or multiple local graphs. Specifically, we\npropose a novel graph-based recommendation model named GLIMG (Global and Local\nIteM Graphs), which simultaneously captures both the global and local user\ntastes. By integrating the global and local graphs into an adapted\nsemi-supervised learning model, users' preferences on items are propagated\nglobally and locally. Extensive experimental results on real-world datasets\nshow that our proposed method consistently outperforms the state-of-the art\ncounterparts on the top-N recommendation task.",
          "link": "http://arxiv.org/abs/2007.14018",
          "publishedOn": "2021-08-12T01:56:20.731Z",
          "wordCount": 647,
          "title": "GLIMG: Global and Local Item Graphs for Top-N Recommender Systems. (arXiv:2007.14018v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1\">Xiang-Rong Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liqin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guorui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinyao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Binding Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Siran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jingshan Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hongbo Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>",
          "description": "Traditional industrial recommenders are usually trained on a single business\ndomain and then serve for this domain. However, in large commercial platforms,\nit is often the case that the recommenders need to make click-through rate\n(CTR) predictions for multiple business domains. Different domains have\noverlapping user groups and items. Thus, there exist commonalities. Since the\nspecific user groups have disparity and the user behaviors may change in\nvarious business domains, there also have distinctions. The distinctions result\nin domain-specific data distributions, making it hard for a single shared model\nto work well on all domains. To learn an effective and efficient CTR model to\nhandle multiple domains simultaneously, we present Star Topology Adaptive\nRecommender (STAR). Concretely, STAR has the star topology, which consists of\nthe shared centered parameters and domain-specific parameters. The shared\nparameters are applied to learn commonalities of all domains, and the\ndomain-specific parameters capture domain distinction for more refined\nprediction. Given requests from different business domains, STAR can adapt its\nparameters conditioned on the domain characteristics. The experimental result\nfrom production data validates the superiority of the proposed STAR model.\nSince 2020, STAR has been deployed in the display advertising system of\nAlibaba, obtaining averaging 8.0% improvement on CTR and 6.0% on RPM (Revenue\nPer Mille).",
          "link": "http://arxiv.org/abs/2101.11427",
          "publishedOn": "2021-08-12T01:56:20.717Z",
          "wordCount": 704,
          "title": "One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. (arXiv:2101.11427v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirnap_O/0/1/0/all/0/1\">&#xd6;mer K&#x131;rnap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia Biega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekstrand_M/0/1/0/all/0/1\">Michael Ekstrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carterette_B/0/1/0/all/0/1\">Ben Carterette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Y&#x131;lmaz</a>",
          "description": "There is increasing attention to evaluating the fairness of search system\nranking decisions. These metrics often consider the membership of items to\nparticular groups, often identified using protected attributes such as gender\nor ethnicity. To date, these metrics typically assume the availability and\ncompleteness of protected attribute labels of items. However, the protected\nattributes of individuals are rarely present, limiting the application of fair\nranking metrics in large scale systems. In order to address this problem, we\npropose a sampling strategy and estimation technique for four fair ranking\nmetrics. We formulate a robust and unbiased estimator which can operate even\nwith very limited number of labeled items. We evaluate our approach using both\nsimulated and real world data. Our experimental results demonstrate that our\nmethod can estimate this family of fair ranking metrics and provides a robust,\nreliable alternative to exhaustive or random data annotation.",
          "link": "http://arxiv.org/abs/2108.05152",
          "publishedOn": "2021-08-12T01:56:20.685Z",
          "wordCount": 599,
          "title": "Estimation of Fair Ranking Metrics with Incomplete Judgments. (arXiv:2108.05152v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "Learning to rank systems has become an important aspect of our daily life.\nHowever, the implicit user feedback that is used to train many learning to rank\nmodels is usually noisy and suffered from user bias (i.e., position bias).\nThus, obtaining an unbiased model using biased feedback has become an important\nresearch field for IR. Existing studies on unbiased learning to rank (ULTR) can\nbe generalized into two families-algorithms that attain unbiasedness with\nlogged data, offline learning, and algorithms that achieve unbiasedness by\nestimating unbiased parameters with real-time user interactions, namely online\nlearning. While there exist many algorithms from both families, there lacks a\nunified way to compare and benchmark them. As a result, it can be challenging\nfor researchers to choose the right technique for their problems or for people\nwho are new to the field to learn and understand existing algorithms. To solve\nthis problem, we introduced ULTRA, which is a flexible, extensible, and easily\nconfigure ULTR toolbox. Its key features include support for multiple ULTR\nalgorithms with configurable hyperparameters, a variety of built-in click\nmodels that can be used separately to simulate clicks, different ranking model\narchitecture and evaluation metrics, and simple learning to rank pipeline\ncreation. In this paper, we discuss the general framework of ULTR, briefly\ndescribe the algorithms in ULTRA, detailed the structure, and pipeline of the\ntoolbox. We experimented on all the algorithms supported by ultra and showed\nthat the toolbox performance is reasonable. Our toolbox is an important\nresource for researchers to conduct experiments on ULTR algorithms with\ndifferent configurations as well as testing their own algorithms with the\nsupported features.",
          "link": "http://arxiv.org/abs/2108.05073",
          "publishedOn": "2021-08-12T01:56:20.630Z",
          "wordCount": 708,
          "title": "ULTRA: An Unbiased Learning To Rank Algorithm Toolbox. (arXiv:2108.05073v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jiarui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Rong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhirong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiuqiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>",
          "description": "Prediction over tabular data is an essential task in many data science\napplications such as recommender systems, online advertising, medical\ntreatment, etc. Tabular data is structured into rows and columns, with each row\nas a data sample and each column as a feature attribute. Both the columns and\nrows of the tabular data carry useful patterns that could improve the model\nprediction performance. However, most existing models focus on the cross-column\npatterns yet overlook the cross-row patterns as they deal with single samples\nindependently. In this work, we propose a general learning framework named\nRetrieval & Interaction Machine (RIM) that fully exploits both cross-row and\ncross-column patterns among tabular data. Specifically, RIM first leverages\nsearch engine techniques to efficiently retrieve useful rows of the table to\nassist the label prediction of the target row, then uses feature interaction\nnetworks to capture the cross-column patterns among the target row and the\nretrieved rows so as to make the final label prediction. We conduct extensive\nexperiments on 11 datasets of three important tasks, i.e., CTR prediction\n(classification), top-n recommendation (ranking) and rating prediction\n(regression). Experimental results show that RIM achieves significant\nimprovements over the state-of-the-art and various baselines, demonstrating the\nsuperiority and efficacy of RIM.",
          "link": "http://arxiv.org/abs/2108.05252",
          "publishedOn": "2021-08-12T01:56:20.579Z",
          "wordCount": 643,
          "title": "Retrieval & Interaction Machine for Tabular Data Prediction. (arXiv:2108.05252v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia J. Biega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekstrand_M/0/1/0/all/0/1\">Michael D. Ekstrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sergey Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohlmeier_S/0/1/0/all/0/1\">Sebastian Kohlmeier</a>",
          "description": "This paper provides an overview of the NIST TREC 2020 Fair Ranking track. For\n2020, we again adopted an academic search task, where we have a corpus of\nacademic article abstracts and queries submitted to a production academic\nsearch engine. The central goal of the Fair Ranking track is to provide fair\nexposure to different groups of authors (a group fairness framing). We\nrecognize that there may be multiple group definitions (e.g. based on\ndemographics, stature, topic) and hoped for the systems to be robust to these.\nWe expected participants to develop systems that optimize for fairness and\nrelevance for arbitrary group definitions, and did not reveal the exact group\ndefinitions until after the evaluation runs were submitted.The track contains\ntwo tasks,reranking and retrieval, with a shared evaluation.",
          "link": "http://arxiv.org/abs/2108.05135",
          "publishedOn": "2021-08-12T01:56:20.522Z",
          "wordCount": 582,
          "title": "Overview of the TREC 2020 Fair Ranking Track. (arXiv:2108.05135v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Recently, we have witnessed the bloom of neural ranking models in the\ninformation retrieval (IR) field. So far, much effort has been devoted to\ndeveloping effective neural ranking models that can generalize well on new\ndata. There has been less attention paid to the robustness perspective. Unlike\nthe effectiveness which is about the average performance of a system under\nnormal purpose, robustness cares more about the system performance in the worst\ncase or under malicious operations instead. When a new technique enters into\nthe real-world application, it is critical to know not only how it works in\naverage, but also how would it behave in abnormal situations. So we raise the\nquestion in this work: Are neural ranking models robust? To answer this\nquestion, firstly, we need to clarify what we refer to when we talk about the\nrobustness of ranking models in IR. We show that robustness is actually a\nmulti-dimensional concept and there are three ways to define it in IR: 1) The\nperformance variance under the independent and identically distributed (I.I.D.)\nsetting; 2) The out-of-distribution (OOD) generalizability; and 3) The\ndefensive ability against adversarial operations. The latter two definitions\ncan be further specified into two different perspectives respectively, leading\nto 5 robustness tasks in total. Based on this taxonomy, we build corresponding\nbenchmark datasets, design empirical experiments, and systematically analyze\nthe robustness of several representative neural ranking models against\ntraditional probabilistic ranking models and learning-to-rank (LTR) models. The\nempirical results show that there is no simple answer to our question. While\nneural ranking models are less robust against other IR models in most cases,\nsome of them can still win 1 out of 5 tasks. This is the first comprehensive\nstudy on the robustness of neural ranking models.",
          "link": "http://arxiv.org/abs/2108.05018",
          "publishedOn": "2021-08-12T01:56:20.342Z",
          "wordCount": 717,
          "title": "Are Neural Ranking Models Robust?. (arXiv:2108.05018v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Question Answering (QA), a popular and promising technique for intelligent\ninformation access, faces a dilemma about data as most other AI techniques. On\none hand, modern QA methods rely on deep learning models which are typically\ndata-hungry. Therefore, it is expected to collect and fuse all the available QA\ndatasets together in a common site for developing a powerful QA model. On the\nother hand, real-world QA datasets are typically distributed in the form of\nisolated islands belonging to different parties. Due to the increasing\nawareness of privacy security, it is almost impossible to integrate the data\nscattered around, or the cost is prohibited. A possible solution to this\ndilemma is a new approach known as federated learning, which is a\nprivacy-preserving machine learning technique over distributed datasets. In\nthis work, we propose to adopt federated learning for QA with the special\nconcern on the statistical heterogeneity of the QA data. Here the heterogeneity\nrefers to the fact that annotated QA data are typically with non-identical and\nindependent distribution (non-IID) and unbalanced sizes in practice.\nTraditional federated learning methods may sacrifice the accuracy of individual\nmodels under the heterogeneous situation. To tackle this problem, we propose a\nnovel Federated Matching framework for QA, named FedMatch, with a\nbackbone-patch architecture. The shared backbone is to distill the common\nknowledge of all the participants while the private patch is a compact and\nefficient module to retain the domain information for each participant. To\nfacilitate the evaluation, we build a benchmark collection based on several QA\ndatasets from different domains to simulate the heterogeneous situation in\npractice. Empirical studies demonstrate that our model can achieve significant\nimprovements against the baselines over all the datasets.",
          "link": "http://arxiv.org/abs/2108.05069",
          "publishedOn": "2021-08-12T01:56:20.279Z",
          "wordCount": 717,
          "title": "FedMatch: Federated Learning Over Heterogeneous Question Answering Data. (arXiv:2108.05069v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kai Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_D/0/1/0/all/0/1\">Da Kuang</a>",
          "description": "Autocomplete (a.k.a \"Query Auto-Completion\", \"AC\") suggests full queries\nbased on a prefix typed by customer. Autocomplete has been a core feature of\ncommercial search engine. In this paper, we propose a novel context-aware\nneural network based pairwise ranker (DeepPLTR) to improve AC ranking, DeepPLTR\nleverages contextual and behavioral features to rank queries by minimizing a\npairwise loss, based on a fully-connected neural network structure. Compared to\nLambdaMART ranker, DeepPLTR shows +3.90% MeanReciprocalRank (MRR) lift in\noffline evaluation, and yielded +0.06% (p < 0.1) Gross Merchandise Value (GMV)\nlift in an Amazon's online A/B experiment.",
          "link": "http://arxiv.org/abs/2108.04976",
          "publishedOn": "2021-08-12T01:56:20.094Z",
          "wordCount": 536,
          "title": "Deep Pairwise Learning To Rank For Search Autocomplete. (arXiv:2108.04976v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>",
          "description": "In this paper, we present a semi-automated framework called AMUSED for\ngathering multi-modal annotated data from the multiple social media platforms.\nThe framework is designed to mitigate the issues of collecting and annotating\nsocial media data by cohesively combining machine and human in the data\ncollection process. From a given list of the articles from professional news\nmedia or blog, AMUSED detects links to the social media posts from news\narticles and then downloads contents of the same post from the respective\nsocial media platform to gather details about that specific post. The framework\nis capable of fetching the annotated data from multiple platforms like Twitter,\nYouTube, Reddit. The framework aims to reduce the workload and problems behind\nthe data annotation from the social media platforms. AMUSED can be applied in\nmultiple application domains, as a use case, we have implemented the framework\nfor collecting COVID-19 misinformation data from different social media\nplatforms.",
          "link": "http://arxiv.org/abs/2010.00502",
          "publishedOn": "2021-08-11T01:55:21.055Z",
          "wordCount": 670,
          "title": "AMUSED: An Annotation Framework of Multi-modal Social Media Data. (arXiv:2010.00502v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shangfeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haobin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PinghuaGong/0/1/0/all/0/1\">PinghuaGong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>",
          "description": "Recommendation for new users, also called user cold start, has been a\nwell-recognized challenge for online recommender systems. Most existing methods\nview the crux as the lack of initial data. However, in this paper, we argue\nthat there are neglected problems: 1) New users' behaviour follows much\ndifferent distributions from regular users. 2) Although personalized features\nare involved, heavily imbalanced samples prevent the model from balancing\nnew/regular user distributions, as if the personalized features are\noverwhelmed. We name the problem as the ``submergence\" of personalization. To\ntackle this problem, we propose a novel module: Personalized COld Start MOdules\n(POSO). Considering from a model architecture perspective, POSO personalizes\nexisting modules by introducing multiple user-group-specialized sub-modules.\nThen, it fuses their outputs by personalized gates, resulting in comprehensive\nrepresentations. In such way, POSO projects imbalanced features to even\nmodules. POSO can be flexibly integrated into many existing modules and\neffectively improves their performance with negligible computational overheads.\nThe proposed method shows remarkable advantage in industrial scenario. It has\nbeen deployed on the large-scale recommender system of Kwai, and improves new\nuser Watch Time by a large margin (+7.75%). Moreover, POSO can be further\ngeneralized to regular users, inactive users and returning users (+2%-3% on\nWatch Time), as well as item cold start (+3.8% on Watch Time). Its\neffectiveness has also been verified on public dataset (MovieLens 20M). We\nbelieve such practical experience can be well generalized to other scenarios.",
          "link": "http://arxiv.org/abs/2108.04690",
          "publishedOn": "2021-08-11T01:55:20.567Z",
          "wordCount": 677,
          "title": "POSO: Personalized Cold Start Modules for Large-scale Recommender Systems. (arXiv:2108.04690v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1\">Praveen Kumar Bodigutla</a>",
          "description": "\"High Quality Related Search Query Suggestions\" task aims at recommending\nsearch queries which are real, accurate, diverse, relevant and engaging.\nObtaining large amounts of query-quality human annotations is expensive. Prior\nwork on supervised query suggestion models suffered from selection and exposure\nbias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),\nleading to low quality suggestions. Reinforcement Learning techniques employed\nto reformulate a query using terms from search results, have limited\nscalability to large-scale industry applications. To recommend high quality\nrelated search queries, we train a Deep Reinforcement Learning model to predict\nthe query a user would enter next. The reward signal is composed of long-term\nsession-based user feedback, syntactic relatedness and estimated naturalness of\ngenerated query. Over the baseline supervised model, our proposed approach\nachieves a significant relative improvement in terms of recommendation\ndiversity (3%), down-stream user-engagement (4.2%) and per-sentence word\nrepetitions (82%).",
          "link": "http://arxiv.org/abs/2108.04452",
          "publishedOn": "2021-08-11T01:55:20.557Z",
          "wordCount": 598,
          "title": "High Quality Related Search Query Suggestions using Deep Reinforcement Learning. (arXiv:2108.04452v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Recently, Graph Convolution Network (GCN) based methods have achieved\noutstanding performance for recommendation. These methods embed users and items\nin Euclidean space, and perform graph convolution on user-item interaction\ngraphs. However, real-world datasets usually exhibit tree-like hierarchical\nstructures, which make Euclidean space less effective in capturing user-item\nrelationship. In contrast, hyperbolic space, as a continuous analogue of a\ntree-graph, provides a promising alternative. In this paper, we propose a fully\nhyperbolic GCN model for recommendation, where all operations are performed in\nhyperbolic space. Utilizing the advantage of hyperbolic space, our method is\nable to embed users/items with less distortion and capture user-item\ninteraction relationship more accurately. Extensive experiments on public\nbenchmark datasets show that our method outperforms both Euclidean and\nhyperbolic counterparts and requires far lower embedding dimensionality to\nachieve comparable performance.",
          "link": "http://arxiv.org/abs/2108.04607",
          "publishedOn": "2021-08-11T01:55:20.547Z",
          "wordCount": 567,
          "title": "Fully Hyperbolic Graph Convolution Network for Recommendation. (arXiv:2108.04607v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet-Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1\">Guillaume Salha-Galvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1\">Manuel Moussallam</a>",
          "description": "Collaborative Metric Learning (CML) recently emerged as a powerful paradigm\nfor recommendation based on implicit feedback collaborative filtering. However,\nstandard CML methods learn fixed user and item representations, which fails to\ncapture the complex interests of users. Existing extensions of CML also either\nignore the heterogeneity of user-item relations, i.e. that a user can\nsimultaneously like very different items, or the latent item-item relations,\ni.e. that a user's preference for an item depends, not only on its intrinsic\ncharacteristics, but also on items they previously interacted with. In this\npaper, we present a hierarchical CML model that jointly captures latent\nuser-item and item-item relations from implicit data. Our approach is inspired\nby translation mechanisms from knowledge graph embedding and leverages\nmemory-based attention networks. We empirically show the relevance of this\njoint relational modeling, by outperforming existing CML models on\nrecommendation tasks on several real-world datasets. Our experiments also\nemphasize the limits of current CML relational models on very sparse datasets.",
          "link": "http://arxiv.org/abs/2108.04655",
          "publishedOn": "2021-08-11T01:55:20.511Z",
          "wordCount": 602,
          "title": "Hierarchical Latent Relation Modeling for Collaborative Metric Learning. (arXiv:2108.04655v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_C/0/1/0/all/0/1\">Changhua Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shanshan Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1\">Junfeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Wenwu Ou</a>",
          "description": "Click-Through Rate (CTR) prediction is one of the core tasks in recommender\nsystems (RS). It predicts a personalized click probability for each user-item\npair. Recently, researchers have found that the performance of CTR model can be\nimproved greatly by taking user behavior sequence into consideration,\nespecially long-term user behavior sequence. The report on an e-commerce\nwebsite shows that 23\\% of users have more than 1000 clicks during the past 5\nmonths. Though there are numerous works focus on modeling sequential user\nbehaviors, few works can handle long-term user behavior sequence due to the\nstrict inference time constraint in real world system. Two-stage methods are\nproposed to push the limit for better performance. At the first stage, an\nauxiliary task is designed to retrieve the top-$k$ similar items from long-term\nuser behavior sequence. At the second stage, the classical attention mechanism\nis conducted between the candidate item and $k$ items selected in the first\nstage. However, information gap happens between retrieval stage and the main\nCTR task. This goal divergence can greatly diminishing the performance gain of\nlong-term user sequence. In this paper, inspired by Reformer, we propose a\nlocality-sensitive hashing (LSH) method called ETA (End-to-end Target\nAttention) which can greatly reduce the training and inference cost and make\nthe end-to-end training with long-term user behavior sequence possible. Both\noffline and online experiments confirm the effectiveness of our model. We\ndeploy ETA into a large-scale real world E-commerce system and achieve extra\n3.1\\% improvements on GMV (Gross Merchandise Value) compared to a two-stage\nlong user sequence CTR model.",
          "link": "http://arxiv.org/abs/2108.04468",
          "publishedOn": "2021-08-11T01:55:20.482Z",
          "wordCount": 697,
          "title": "End-to-End User Behavior Retrieval in Click-Through RatePrediction Model. (arXiv:2108.04468v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "User-item interactions in recommendations can be naturally de-noted as a\nuser-item bipartite graph. Given the success of graph neural networks (GNNs) in\ngraph representation learning, GNN-based C methods have been proposed to\nadvance recommender systems. These methods often make recommendations based on\nthe learned user and item embeddings. However, we found that they do not\nperform well wit sparse user-item graphs which are quite common in real-world\nrecommendations. Therefore, in this work, we introduce a novel perspective to\nbuild GNN-based CF methods for recommendations which leads to the proposed\nframework Localized Graph Collaborative Filtering (LGCF). One key advantage of\nLGCF is that it does not need to learn embeddings for each user and item, which\nis challenging in sparse scenarios.\n\nAlternatively, LGCF aims at encoding useful CF information into a localized\ngraph and making recommendations based on such graph. Extensive experiments on\nvarious datasets validate the effectiveness of LGCF especially in sparse\nscenarios. Furthermore, empirical results demonstrate that LGCF provides\ncomplementary information to the embedding-based CF model which can be utilized\nto boost recommendation performance.",
          "link": "http://arxiv.org/abs/2108.04475",
          "publishedOn": "2021-08-11T01:55:20.455Z",
          "wordCount": 607,
          "title": "Localized Graph Collaborative Filtering. (arXiv:2108.04475v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.09491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lingzhi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Ying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sheng Yang</a>",
          "description": "This paper investigates adaptive streaming of one or multiple tiled 360\nvideos from a multi-antenna base station (BS) to one or multiple single-antenna\nusers, respectively, in a multi-carrier wireless system. We aim to maximize the\nvideo quality while keeping rebuffering time small via encoding rate adaptation\nat each group of pictures (GOP) and transmission adaptation at each\n(transmission) slot. To capture the impact of field-of-view (FoV) prediction,\nwe consider three cases of FoV viewing probability distributions, i.e.,\nperfect, imperfect, and unknown FoV viewing probability distributions, and use\nthe average total utility, worst average total utility, and worst total utility\nas the respective performance metrics. In the single-user scenario, we optimize\nthe encoding rates of the tiles, encoding rates of the FoVs, and transmission\nbeamforming vectors for all subcarriers to maximize the total utility in each\ncase. In the multi-user scenario, we adopt rate splitting with successive\ndecoding and optimize the encoding rates of the tiles, encoding rates of the\nFoVs, rates of the common and private messages, and transmission beamforming\nvectors for all subcarriers to maximize the total utility in each case. Then,\nwe separate the challenging optimization problem into multiple tractable\nproblems in each scenario. In the single-user scenario, we obtain a globally\noptimal solution of each problem using transformation techniques and the\nKarush-Kuhn-Tucker (KKT) conditions. In the multi-user scenario, we obtain a\nKKT point of each problem using the concave-convex procedure (CCCP). Finally,\nnumerical results demonstrate that the proposed solutions achieve notable gains\nover existing schemes in all three cases. To the best of our knowledge, this is\nthe first work revealing the impact of FoV prediction on the performance of\nadaptive streaming of tiled 360 videos.",
          "link": "http://arxiv.org/abs/2107.09491",
          "publishedOn": "2021-08-17T01:54:46.306Z",
          "wordCount": 773,
          "title": "Adaptive Streaming of 360 Videos with Perfect, Imperfect, and Unknown FoV Viewing Probabilities in Wireless Networks. (arXiv:2107.09491v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiawen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_J/0/1/0/all/0/1\">Jinye Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1\">Kenny Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>",
          "description": "The analytical description of charts is an exciting and important research\narea with many applications in academia and industry. Yet, this challenging\ntask has received limited attention from the computational linguistics research\ncommunity. This paper proposes \\textsf{AutoChart}, a large dataset for the\nanalytical description of charts, which aims to encourage more research into\nthis important area. Specifically, we offer a novel framework that generates\nthe charts and their analytical description automatically. We conducted\nextensive human and machine evaluations on the generated charts and\ndescriptions and demonstrate that the generated texts are informative,\ncoherent, and relevant to the corresponding charts.",
          "link": "http://arxiv.org/abs/2108.06897",
          "publishedOn": "2021-08-17T01:54:46.251Z",
          "wordCount": 543,
          "title": "AutoChart: A Dataset for Chart-to-Text Generation Task. (arXiv:2108.06897v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1\">Maksim Siniukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1\">Dmitriy Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>",
          "description": "Video-quality measurement plays a critical role in the development of\nvideo-processing applications. In this paper, we show how video preprocessing\ncan artificially increase the popular quality metric VMAF and its\ntuning-resistant version, VMAF NEG. We propose a pipeline that tunes\nprocessing-algorithm parameters to increase VMAF by up to 218.8%. A subjective\ncomparison revealed that for most preprocessing methods, a video's visual\nquality drops or stays unchanged. We also show that some preprocessing methods\ncan increase VMAF NEG scores by up to 23.6%.",
          "link": "http://arxiv.org/abs/2107.04510",
          "publishedOn": "2021-08-17T01:54:46.221Z",
          "wordCount": 550,
          "title": "Hacking VMAF and VMAF NEG: vulnerability to different preprocessing methods. (arXiv:2107.04510v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06515",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhonglan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Dongqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingfan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qilong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingyu Liu</a>",
          "description": "In industry, there exist plenty of scenarios where old gray photos need to be\nautomatically colored, such as video sites and archives. In this paper, we\npresent the HistoryNet focusing on historical person's diverse high fidelity\nclothing colorization based on fine grained semantic understanding and prior.\nColorization of historical persons is realistic and practical, however,\nexisting methods do not perform well in the regards. In this paper, a\nHistoryNet including three parts, namely, classification, fine grained semantic\nparsing and colorization, is proposed. Classification sub-module supplies\nclassifying of images according to the eras, nationalities and garment types;\nParsing sub-network supplies the semantic for person contours, clothing and\nbackground in the image to achieve more accurate colorization of clothes and\npersons and prevent color overflow. In the training process, we integrate\nclassification and semantic parsing features into the coloring generation\nnetwork to improve colorization. Through the design of classification and\nparsing subnetwork, the accuracy of image colorization can be improved and the\nboundary of each part of image can be more clearly. Moreover, we also propose a\nnovel Modern Historical Movies Dataset (MHMD) containing 1,353,166 images and\n42 labels of eras, nationalities, and garment types for automatic colorization\nfrom 147 historical movies or TV series made in modern time. Various\nquantitative and qualitative comparisons demonstrate that our method\noutperforms the state-of-the-art colorization methods, especially on military\nuniforms, which has correct colors according to the historical literatures.",
          "link": "http://arxiv.org/abs/2108.06515",
          "publishedOn": "2021-08-17T01:54:46.195Z",
          "wordCount": 694,
          "title": "Focusing on Persons: Colorizing Old Images Learning from Modern Historical Movies. (arXiv:2108.06515v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1\">Hassan Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_W/0/1/0/all/0/1\">Wei-Hong Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>",
          "description": "We present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Specifically, our\nVideo-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts\nmultimodal representations that are rich enough to benefit a variety of\ndownstream tasks. We train VATT end-to-end from scratch using multimodal\ncontrastive losses and evaluate its performance by the downstream tasks of\nvideo action recognition, audio event classification, image classification, and\ntext-to-video retrieval. Furthermore, we study a modality-agnostic\nsingle-backbone Transformer by sharing weights among the three modalities. We\nshow that the convolution-free VATT outperforms state-of-the-art ConvNet-based\narchitectures in the downstream tasks. Especially, VATT's vision Transformer\nachieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and\n41.1% on Moments in Time, new records while avoiding supervised pre-training.\nTransferring to image classification leads to 78.7% top-1 accuracy on ImageNet\ncompared to 64.7% by training the same Transformer from scratch, showing the\ngeneralizability of our model despite the domain gap between videos and images.\nVATT's audio Transformer also sets a new record on waveform-based audio event\nrecognition by achieving the mAP of 39.4% on AudioSet without any supervised\npre-training. VATT's source code is publicly available.",
          "link": "http://arxiv.org/abs/2104.11178",
          "publishedOn": "2021-08-16T00:47:31.743Z",
          "wordCount": 689,
          "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text. (arXiv:2104.11178v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>",
          "description": "Hateful and offensive content detection has been extensively explored in a\nsingle modality such as text. However, such toxic information could also be\ncommunicated via multimodal content such as online memes. Therefore, detecting\nmultimodal hateful content has recently garnered much attention in academic and\nindustry research communities. This paper aims to contribute to this emerging\nresearch topic by proposing DisMultiHate, which is a novel framework that\nperformed the classification of multimodal hateful content. Specifically,\nDisMultiHate is designed to disentangle target entities in multimodal memes to\nimprove hateful content classification and explainability. We conduct extensive\nexperiments on two publicly available hateful and offensive memes datasets. Our\nexperiment results show that DisMultiHate is able to outperform\nstate-of-the-art unimodal and multimodal baselines in the hateful meme\nclassification task. Empirical case studies were also conducted to demonstrate\nDisMultiHate's ability to disentangle target entities in memes and ultimately\nshowcase DisMultiHate's explainability of the multimodal hateful content\nclassification task.",
          "link": "http://arxiv.org/abs/2108.06207",
          "publishedOn": "2021-08-16T00:47:31.602Z",
          "wordCount": 597,
          "title": "Disentangling Hate in Online Memes. (arXiv:2108.06207v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthula_P/0/1/0/all/0/1\">Praneeth Chakravarthula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tursun_O/0/1/0/all/0/1\">Okan Tursun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Didyk_P/0/1/0/all/0/1\">Piotr Didyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_H/0/1/0/all/0/1\">Henry Fuchs</a>",
          "description": "Computer-generated holographic (CGH) displays show great potential and are\nemerging as the next-generation displays for augmented and virtual reality, and\nautomotive heads-up displays. One of the critical problems harming the wide\nadoption of such displays is the presence of speckle noise inherent to\nholography, that compromises its quality by introducing perceptible artifacts.\nAlthough speckle noise suppression has been an active research area, the\nprevious works have not considered the perceptual characteristics of the Human\nVisual System (HVS), which receives the final displayed imagery. However, it is\nwell studied that the sensitivity of the HVS is not uniform across the visual\nfield, which has led to gaze-contingent rendering schemes for maximizing the\nperceptual quality in various computer-generated imagery. Inspired by this, we\npresent the first method that reduces the \"perceived speckle noise\" by\nintegrating foveal and peripheral vision characteristics of the HVS, along with\nthe retinal point spread function, into the phase hologram computation.\nSpecifically, we introduce the anatomical and statistical retinal receptor\ndistribution into our computational hologram optimization, which places a\nhigher priority on reducing the perceived foveal speckle noise while being\nadaptable to any individual's optical aberration on the retina. Our method\ndemonstrates superior perceptual quality on our emulated holographic display.\nOur evaluations with objective measurements and subjective studies demonstrate\na significant reduction of the human perceived noise.",
          "link": "http://arxiv.org/abs/2108.06192",
          "publishedOn": "2021-08-16T00:47:31.385Z",
          "wordCount": 669,
          "title": "Gaze-Contingent Retinal Speckle Suppression for Perceptually-Matched Foveated Holographic Displays. (arXiv:2108.06192v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kucherenko_T/0/1/0/all/0/1\">Taras Kucherenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_R/0/1/0/all/0/1\">Rajmund Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_M/0/1/0/all/0/1\">Michael Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>",
          "description": "Embodied conversational agents benefit from being able to accompany their\nspeech with gestures. Although many data-driven approaches to gesture\ngeneration have been proposed in recent years, it is still unclear whether such\nsystems can consistently generate gestures that convey meaning. We investigate\nwhich gesture properties (phase, category, and semantics) can be predicted from\nspeech text and/or audio using contemporary deep learning. In extensive\nexperiments, we show that gesture properties related to gesture meaning\n(semantics and category) are predictable from text features (time-aligned BERT\nembeddings) alone, but not from prosodic audio features, while rhythm-related\ngesture properties (phase) on the other hand can be predicted from either\naudio, text (with word-level timing information), or both. These results are\nencouraging as they indicate that it is possible to equip an embodied agent\nwith content-wise meaningful co-speech gestures using a machine-learning model.",
          "link": "http://arxiv.org/abs/2108.05762",
          "publishedOn": "2021-08-13T01:56:54.957Z",
          "wordCount": 583,
          "title": "Multimodal analysis of the predictability of hand-gesture properties. (arXiv:2108.05762v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhihao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>",
          "description": "The combination of the traditional convolutional network (i.e., an\nauto-encoder) and the graph convolutional network has attracted much attention\nin clustering, in which the auto-encoder extracts the node attribute feature\nand the graph convolutional network captures the topological graph feature.\nHowever, the existing works (i) lack a flexible combination mechanism to\nadaptively fuse those two kinds of features for learning the discriminative\nrepresentation and (ii) overlook the multi-scale information embedded at\ndifferent layers for subsequent cluster assignment, leading to inferior\nclustering results. To this end, we propose a novel deep clustering method\nnamed Attention-driven Graph Clustering Network (AGCN). Specifically, AGCN\nexploits a heterogeneity-wise fusion module to dynamically fuse the node\nattribute feature and the topological graph feature. Moreover, AGCN develops a\nscale-wise fusion module to adaptively aggregate the multi-scale features\nembedded at different layers. Based on a unified optimization framework, AGCN\ncan jointly perform feature learning and cluster assignment in an unsupervised\nfashion. Compared with the existing deep clustering methods, our method is more\nflexible and effective since it comprehensively considers the numerous and\ndiscriminative information embedded in the network and directly produces the\nclustering results. Extensive quantitative and qualitative results on commonly\nused benchmark datasets validate that our AGCN consistently outperforms\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.05499",
          "publishedOn": "2021-08-13T01:56:54.950Z",
          "wordCount": 636,
          "title": "Attention-driven Graph Clustering Network. (arXiv:2108.05499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Marco Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xianjun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuanjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuzhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>",
          "description": "We propose a novel neural model compression strategy combining data\naugmentation, knowledge transfer, pruning, and quantization for device-robust\nacoustic scene classification (ASC). Specifically, we tackle the ASC task in a\nlow-resource environment leveraging a recently proposed advanced neural network\npruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a\nsub-network neural model associated with a small amount non-zero model\nparameters. The effectiveness of LTH for low-complexity acoustic modeling is\nassessed by investigating various data augmentation and compression schemes,\nand we report an efficient joint framework for low-complexity multi-device ASC,\ncalled Acoustic Lottery. Acoustic Lottery could compress an ASC model over\n$1/10^{4}$ and attain a superior performance (validation accuracy of 74.01% and\nLog loss of 0.76) compared to its not compressed seed model. All results\nreported in this work are based on a joint effort of four groups, namely\nGT-USTC-UKE-Tencent, aiming to address the \"Low-Complexity Acoustic Scene\nClassification (ASC) with Multiple Devices\" in the DCASE 2021 Challenge Task\n1a.",
          "link": "http://arxiv.org/abs/2107.01461",
          "publishedOn": "2021-08-13T01:56:54.874Z",
          "wordCount": 682,
          "title": "A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification. (arXiv:2107.01461v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santos_P/0/1/0/all/0/1\">Paula Santos</a>",
          "description": "Lung X-ray images, if processed using statistical and computational methods,\ncan distinguish pneumonia from COVID-19. The present work shows that it is\npossible to extract lung X-ray characteristics to improve the methods of\nexamining and diagnosing patients with suspected COVID-19, distinguishing them\nfrom malaria, dengue, H1N1, tuberculosis, and Streptococcus pneumonia. More\nprecisely, an intelligent computational model was developed to process lung\nX-ray images and classify whether the image is of a patient with COVID-19. The\nimages were processed and extracted their characteristics. These\ncharacteristics were the input data for an unsupervised statistical learning\nmethod, PCA, and clustering, which identified specific attributes of X-ray\nimages with Covid-19. The introduction of statistical models allowed a fast\nalgorithm, which used the X-means clustering method associated with the\nBayesian Information Criterion (CIB). The developed algorithm efficiently\ndistinguished each pulmonary pathology from X-ray images. The method exhibited\nexcellent sensitivity. The average recognition accuracy of COVID-19 was 0.93\nand 0.051.",
          "link": "http://arxiv.org/abs/2108.05536",
          "publishedOn": "2021-08-13T01:56:54.850Z",
          "wordCount": 666,
          "title": "Intelligent computational model for the classification of Covid-19 with chest radiography compared to other respiratory diseases. (arXiv:2108.05536v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chin-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chun-Yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "This paper proposes a new self-attention based model for music score\ninfilling, i.e., to generate a polyphonic music sequence that fills in the gap\nbetween given past and future contexts. While existing approaches can only fill\nin a short segment with a fixed number of notes, or a fixed time span between\nthe past and future contexts, our model can infill a variable number of notes\n(up to 128) for different time spans. We achieve so with three major technical\ncontributions. First, we adapt XLNet, an autoregressive model originally\nproposed for unsupervised model pre-training, to music score infilling. Second,\nwe propose a new, musically specialized positional encoding called relative bar\nencoding that better informs the model of notes' position within the past and\nfuture context. Third, to capitalize relative bar encoding, we perform\nlook-ahead onset prediction to predict the onset of a note one time step before\npredicting the other attributes of the note. We compare our proposed model with\ntwo strong baselines and show that our model is superior in both objective and\nsubjective analyses.",
          "link": "http://arxiv.org/abs/2108.05064",
          "publishedOn": "2021-08-12T01:56:20.112Z",
          "wordCount": 625,
          "title": "Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding. (arXiv:2108.05064v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05021",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1\">Yuanhao Gong</a>",
          "description": "Image smoothing is a fundamental task in signal processing. For such task,\nbox filter is well-known. However, box filter can not keep some features of the\nsignal, such as edges, corners and the jump in the step function. In this\npaper, we present a one-sided box filter that can smooth the signal but keep\nthe discontinuous features in the signal. More specifically, we perform box\nfilter on eight one-sided windows, leading to a one-sided box filter that can\npreserve corners and edges. Our filter inherits the constant $O(1)$\ncomputational complexity of the original box filter with respect to the window\nsize and also the linear $O(N)$ computational complexity with respect to the\ntotal number of samples. We performance several experiments to show the\nefficiency and effectiveness of this filter. We further compare our filter with\nother the-state-of-the-art edge preserving methods. Our filter can be deployed\nin a large range of applications where the classical box filter is adopted.",
          "link": "http://arxiv.org/abs/2108.05021",
          "publishedOn": "2021-08-12T01:56:20.082Z",
          "wordCount": 606,
          "title": "One-Sided Box Filter for Edge Preserving Image Smoothing. (arXiv:2108.05021v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parida_K/0/1/0/all/0/1\">Kranti Kumar Parida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Siddharth Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matiyali_N/0/1/0/all/0/1\">Neeraj Matiyali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gaurav Sharma</a>",
          "description": "Binaural audio gives the listener the feeling of being in the recording place\nand enhances the immersive experience if coupled with AR/VR. But the problem\nwith binaural audio recording is that it requires a specialized setup which is\nnot possible to fabricate within handheld devices as compared to traditional\nmono audio that can be recorded with a single microphone. In order to overcome\nthis drawback, prior works have tried to uplift the mono recorded audio to\nbinaural audio as a post processing step conditioning on the visual input. But\nall the prior approaches missed other most important information required for\nthe task, i.e. distance of different sound producing objects from the recording\nsetup. In this work, we argue that the depth map of the scene can act as a\nproxy for encoding distance information of objects in the scene and show that\nadding depth features along with image features improves the performance both\nqualitatively and quantitatively. We propose a novel encoder-decoder\narchitecture, where we use a hierarchical attention mechanism to encode the\nimage and depth feature extracted from individual transformer backbone, with\naudio features at each layer of the decoder.",
          "link": "http://arxiv.org/abs/2108.04906",
          "publishedOn": "2021-08-12T01:56:20.059Z",
          "wordCount": 640,
          "title": "Depth Infused Binaural Audio Generation using Hierarchical Cross-Modal Attention. (arXiv:2108.04906v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>",
          "description": "This paper proposes a novel model for recognizing images with composite\nattribute-object concepts, notably for composite concepts that are unseen\nduring model training. We aim to explore the three key properties required by\nthe task --- relation-aware, consistent, and decoupled --- to learn rich and\nrobust features for primitive concepts that compose attribute-object pairs. To\nthis end, we propose the Blocked Message Passing Network (BMP-Net). The model\nconsists of two modules. The concept module generates semantically meaningful\nfeatures for primitive concepts, whereas the visual module extracts visual\nfeatures for attributes and objects from input images. A message passing\nmechanism is used in the concept module to capture the relations between\nprimitive concepts. Furthermore, to prevent the model from being biased towards\nseen composite concepts and reduce the entanglement between attributes and\nobjects, we propose a blocking mechanism that equalizes the information\navailable to the model for both seen and unseen concepts. Extensive experiments\nand ablation studies on two benchmarks show the efficacy of the proposed model.",
          "link": "http://arxiv.org/abs/2108.04603",
          "publishedOn": "2021-08-11T01:55:20.491Z",
          "wordCount": 612,
          "title": "Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition. (arXiv:2108.04603v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lecci_M/0/1/0/all/0/1\">Mattia Lecci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drago_M/0/1/0/all/0/1\">Matteo Drago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanella_A/0/1/0/all/0/1\">Andrea Zanella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorzi_M/0/1/0/all/0/1\">Michele Zorzi</a>",
          "description": "Thanks to recent advancements in the technology, eXtended Reality (XR)\napplications are gaining a lot of momentum, and they will surely become\nincreasingly popular in the next decade. These new applications, however,\nrequire a step forward also in terms of models to simulate and analyze this\ntype of traffic sources in modern communication networks, in order to guarantee\nto the users state of the art performance and Quality of Experience (QoE).\nRecognizing this need, in this work, we present a novel open-source traffic\nmodel, which researchers can use as a starting point both for improvements of\nthe model itself and for the design of optimized algorithms for the\ntransmission of these peculiar data flows. Along with the mathematical model\nand the code, we also share with the community the traces that we gathered for\nour study, collected from freely available applications such as Minecraft VR,\nGoogle Earth VR, and Virus Popper. Finally, we propose a roadmap for the\nconstruction of an end-to-end framework that fills this gap in the current\nstate of the art.",
          "link": "http://arxiv.org/abs/2108.04577",
          "publishedOn": "2021-08-11T01:55:20.435Z",
          "wordCount": 676,
          "title": "An Open Framework for Analyzing and Modeling XR Network Traffic. (arXiv:2108.04577v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Video content creation keeps growing at an incredible pace; yet, creating\nengaging stories remains challenging and requires non-trivial video editing\nexpertise. Many video editing components are astonishingly hard to automate\nprimarily due to the lack of raw video materials. This paper focuses on a new\ntask for computational video editing, namely the task of raking cut\nplausibility. Our key idea is to leverage content that has already been edited\nto learn fine-grained audiovisual patterns that trigger cuts. To do this, we\nfirst collected a data source of more than 10K videos, from which we extract\nmore than 255K cuts. We devise a model that learns to discriminate between real\nand artificial cuts via contrastive learning. We set up a new task and a set of\nbaselines to benchmark video cut generation. We observe that our proposed model\noutperforms the baselines by large margins. To demonstrate our model in\nreal-world applications, we conduct human studies in a collection of unedited\nvideos. The results show that our model does a better job at cutting than\nrandom and alternative baselines.",
          "link": "http://arxiv.org/abs/2108.04294",
          "publishedOn": "2021-08-11T01:55:20.416Z",
          "wordCount": 624,
          "title": "Learning to Cut by Watching Movies. (arXiv:2108.04294v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xianbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.",
          "link": "http://arxiv.org/abs/2103.13689",
          "publishedOn": "2021-08-11T01:55:20.299Z",
          "wordCount": 670,
          "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.14790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poliarnyi_N/0/1/0/all/0/1\">Nikolai Poliarnyi</a>",
          "description": "We present an out-of-core variational approach for surface reconstruction\nfrom a set of aligned depth maps. Input depth maps are supposed to be\nreconstructed from regular photos or/and can be a representation of terrestrial\nLIDAR point clouds. Our approach is based on surface reconstruction via total\ngeneralized variation minimization ($TGV$) because of its strong\nvisibility-based noise-filtering properties and GPU-friendliness. Our main\ncontribution is an out-of-core OpenCL-accelerated adaptation of this numerical\nalgorithm which can handle arbitrarily large real-world scenes with scale\ndiversity.",
          "link": "http://arxiv.org/abs/2107.14790",
          "publishedOn": "2021-08-17T01:54:53.920Z",
          "wordCount": 560,
          "title": "Out-of-Core Surface Reconstruction via Global $TGV$ Minimization. (arXiv:2107.14790v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03002",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>",
          "description": "More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.",
          "link": "http://arxiv.org/abs/2108.03002",
          "publishedOn": "2021-08-17T01:54:53.631Z",
          "wordCount": 666,
          "title": "Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>",
          "description": "Understanding complex social interactions among agents is a key challenge for\ntrajectory prediction. Most existing methods consider the interactions between\npairwise traffic agents or in a local area, while the nature of interactions is\nunlimited, involving an uncertain number of agents and non-local areas\nsimultaneously. Besides, they treat heterogeneous traffic agents the same,\nnamely those among agents of different categories, while neglecting people's\ndiverse reaction patterns toward traffic agents in ifferent categories. To\naddress these problems, we propose a simple yet effective Unlimited\nNeighborhood Interaction Network (UNIN), which predicts trajectories of\nheterogeneous agents in multiple categories. Specifically, the proposed\nunlimited neighborhood interaction module generates the fused-features of all\nagents involved in an interaction simultaneously, which is adaptive to any\nnumber of agents and any range of interaction area. Meanwhile, a hierarchical\ngraph attention module is proposed to obtain category-to-category interaction\nand agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model\nare estimated for generating the future trajectories. Extensive experimental\nresults on benchmark datasets demonstrate a significant performance improvement\nof our method over the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.00238",
          "publishedOn": "2021-08-17T01:54:53.614Z",
          "wordCount": 634,
          "title": "Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction. (arXiv:2108.00238v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_A/0/1/0/all/0/1\">Austin Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>",
          "description": "Visual engagement in social media platforms comprises interactions with photo\nposts including comments, shares, and likes. In this paper, we leverage such\nvisual engagement clues as supervisory signals for representation learning.\nHowever, learning from engagement signals is non-trivial as it is not clear how\nto bridge the gap between low-level visual information and high-level social\ninteractions. We present VisE, a weakly supervised learning approach, which\nmaps social images to pseudo labels derived by clustered engagement signals. We\nthen study how models trained in this way benefit subjective downstream\ncomputer vision tasks such as emotion recognition or political bias detection.\nThrough extensive studies, we empirically demonstrate the effectiveness of VisE\nacross a diverse set of classification tasks beyond the scope of conventional\nrecognition.",
          "link": "http://arxiv.org/abs/2104.07767",
          "publishedOn": "2021-08-17T01:54:53.597Z",
          "wordCount": 600,
          "title": "Exploring Visual Engagement Signals for Representation Learning. (arXiv:2104.07767v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nesti_F/0/1/0/all/0/1\">Federico Nesti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_A/0/1/0/all/0/1\">Alessandro Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttazzo_G/0/1/0/all/0/1\">Giorgio Buttazzo</a>",
          "description": "Over the last few years, convolutional neural networks (CNNs) have proved to\nreach super-human performance in visual recognition tasks. However, CNNs can\neasily be fooled by adversarial examples, i.e., maliciously-crafted images that\nforce the networks to predict an incorrect output while being extremely similar\nto those for which a correct output is predicted. Regular adversarial examples\nare not robust to input image transformations, which can then be used to detect\nwhether an adversarial example is presented to the network. Nevertheless, it is\nstill possible to generate adversarial examples that are robust to such\ntransformations.\n\nThis paper extensively explores the detection of adversarial examples via\nimage transformations and proposes a novel methodology, called \\textit{defense\nperturbation}, to detect robust adversarial examples with the same input\ntransformations the adversarial examples are robust to. Such a \\textit{defense\nperturbation} is shown to be an effective counter-measure to robust adversarial\nexamples.\n\nFurthermore, multi-network adversarial examples are introduced. This kind of\nadversarial examples can be used to simultaneously fool multiple networks,\nwhich is critical in systems that use network redundancy, such as those based\non architectures with majority voting over multiple CNNs. An extensive set of\nexperiments based on state-of-the-art CNNs trained on the Imagenet dataset is\nfinally reported.",
          "link": "http://arxiv.org/abs/2101.11466",
          "publishedOn": "2021-08-17T01:54:53.571Z",
          "wordCount": 665,
          "title": "Detecting Adversarial Examples by Input Transformations, Defense Perturbations, and Voting. (arXiv:2101.11466v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Aming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Few-shot object detection (FSOD) aims to strengthen the performance of novel\nobject detection with few labeled samples. To alleviate the constraint of few\nsamples, enhancing the generalization ability of learned features for novel\nobjects plays a key role. Thus, the feature learning process of FSOD should\nfocus more on intrinsical object characteristics, which are invariant under\ndifferent visual changes and therefore are helpful for feature generalization.\nUnlike previous attempts of the meta-learning paradigm, in this paper, we\nexplore how to enhance object features with intrinsical characteristics that\nare universal across different object categories. We propose a new prototype,\nnamely universal prototype, that is learned from all object categories. Besides\nthe advantage of characterizing invariant characteristics, the universal\nprototypes alleviate the impact of unbalanced object categories. After\nenhancing object features with the universal prototypes, we impose a\nconsistency loss to maximize the agreement between the enhanced features and\nthe original ones, which is beneficial for learning invariant object\ncharacteristics. Thus, we develop a new framework of few-shot object detection\nwith universal prototypes ({FSOD}^{up}) that owns the merit of feature\ngeneralization towards novel objects. Experimental results on PASCAL VOC and MS\nCOCO show the effectiveness of {FSOD}^{up}. Particularly, for the 1-shot case\nof VOC Split2, {FSOD}^{up} outperforms the baseline by 6.8% in terms of mAP.",
          "link": "http://arxiv.org/abs/2103.01077",
          "publishedOn": "2021-08-17T01:54:53.564Z",
          "wordCount": 679,
          "title": "Universal-Prototype Enhancing for Few-Shot Object Detection. (arXiv:2103.01077v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiahong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Miao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "With the advance of deep learning technology, automatic video generation from\naudio or text has become an emerging and promising research topic. In this\npaper, we present a novel approach to synthesize video from the text. The\nmethod builds a phoneme-pose dictionary and trains a generative adversarial\nnetwork (GAN) to generate video from interpolated phoneme poses. Compared to\naudio-driven video generation algorithms, our approach has a number of\nadvantages: 1) It only needs a fraction of the training data used by an\naudio-driven approach; 2) It is more flexible and not subject to vulnerability\ndue to speaker variation; 3) It significantly reduces the preprocessing,\ntraining and inference time. We perform extensive experiments to compare the\nproposed method with state-of-the-art talking face generation methods on a\nbenchmark dataset and datasets of our own. The results demonstrate the\neffectiveness and superiority of our approach.",
          "link": "http://arxiv.org/abs/2104.14631",
          "publishedOn": "2021-08-17T01:54:53.547Z",
          "wordCount": 617,
          "title": "Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme-Pose Dictionary. (arXiv:2104.14631v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1\">Kea Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1\">Tyler Derr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "While image understanding on recognition-level has achieved remarkable\nadvancements, reliable visual scene understanding requires comprehensive image\nunderstanding on recognition-level but also cognition-level, which calls for\nexploiting the multi-source information as well as learning different levels of\nunderstanding and extensive commonsense knowledge. In this paper, we propose a\nnovel Cognitive Attention Network (CAN) for visual commonsense reasoning to\nachieve interpretable visual understanding. Specifically, we first introduce an\nimage-text fusion module to fuse information from images and text collectively.\nSecond, a novel inference module is designed to encode commonsense among image,\nquery and response. Extensive experiments on large-scale Visual Commonsense\nReasoning (VCR) benchmark dataset demonstrate the effectiveness of our\napproach. The implementation is publicly available at\nhttps://github.com/tanjatang/CAN",
          "link": "http://arxiv.org/abs/2108.02924",
          "publishedOn": "2021-08-17T01:54:53.528Z",
          "wordCount": 579,
          "title": "Interpretable Visual Understanding with Cognitive Attention Network. (arXiv:2108.02924v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Harim Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Myeong-Seok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "The 3D Morphable Model (3DMM), which is a Principal Component Analysis (PCA)\nbased statistical model that represents a 3D face using linear basis functions,\nhas shown promising results for reconstructing 3D faces from single-view\nin-the-wild images. However, 3DMM has restricted representation power due to\nthe limited number of 3D scans and the global linear basis. To address the\nlimitations of 3DMM, we propose a straightforward learning-based method that\nreconstructs a 3D face mesh through Free-Form Deformation (FFD) for the first\ntime. FFD is a geometric modeling method that embeds a reference mesh within a\nparallelepiped grid and deforms the mesh by moving the sparse control points of\nthe grid. As FFD is based on mathematically defined basis functions, it has no\nlimitation in representation power. Thus, we can recover accurate 3D face\nmeshes by estimating appropriate deviation of control points as deformation\nparameters. Although both 3DMM and FFD are parametric models, it is difficult\nto predict the effect of the 3DMM parameters on the face shape, while the\ndeformation parameters of FFD are interpretable in terms of their effect on the\nfinal shape of the mesh. This practical advantage of FFD allows the resulting\nmesh and control points to serve as a good starting point for 3D face modeling,\nin that ordinary users can fine-tune the mesh by using widely available 3D\nsoftware tools. Experiments on multiple datasets demonstrate how our method\nsuccessfully estimates the 3D face geometry and facial expressions from 2D face\nimages, achieving comparable performance to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2105.14857",
          "publishedOn": "2021-08-17T01:54:53.522Z",
          "wordCount": 720,
          "title": "Learning Free-Form Deformation for 3D Face Reconstruction from In-The-Wild Images. (arXiv:2105.14857v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Massiceti_D/0/1/0/all/0/1\">Daniela Massiceti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zintgraf_L/0/1/0/all/0/1\">Luisa Zintgraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronskill_J/0/1/0/all/0/1\">John Bronskill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodorou_L/0/1/0/all/0/1\">Lida Theodorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_M/0/1/0/all/0/1\">Matthew Tobias Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutrell_E/0/1/0/all/0/1\">Edward Cutrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_C/0/1/0/all/0/1\">Cecily Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpf_S/0/1/0/all/0/1\">Simone Stumpf</a>",
          "description": "Object recognition has made great advances in the last decade, but\npredominately still relies on many high-quality training examples per object\ncategory. In contrast, learning new objects from only a few examples could\nenable many impactful applications from robotics to user personalization. Most\nfew-shot learning research, however, has been driven by benchmark datasets that\nlack the high variation that these applications will face when deployed in the\nreal-world. To close this gap, we present the ORBIT dataset and benchmark,\ngrounded in the real-world application of teachable object recognizers for\npeople who are blind/low-vision. The dataset contains 3,822 videos of 486\nobjects recorded by people who are blind/low-vision on their mobile phones. The\nbenchmark reflects a realistic, highly challenging recognition problem,\nproviding a rich playground to drive research in robustness to few-shot,\nhigh-variation conditions. We set the benchmark's first state-of-the-art and\nshow there is massive scope for further innovation, holding the potential to\nimpact a broad range of real-world vision applications including tools for the\nblind/low-vision community. We release the dataset at\nhttps://doi.org/10.25383/city.14294597 and benchmark code at\nhttps://github.com/microsoft/ORBIT-Dataset.",
          "link": "http://arxiv.org/abs/2104.03841",
          "publishedOn": "2021-08-17T01:54:53.516Z",
          "wordCount": 689,
          "title": "ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition. (arXiv:2104.03841v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shitong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>",
          "description": "Point clouds acquired from scanning devices are often perturbed by noise,\nwhich affects downstream tasks such as surface reconstruction and analysis. The\ndistribution of a noisy point cloud can be viewed as the distribution of a set\nof noise-free samples $p(x)$ convolved with some noise model $n$, leading to\n$(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy\npoint cloud, we propose to increase the log-likelihood of each point from $p *\nn$ via gradient ascent -- iteratively updating each point's position. Since $p\n* n$ is unknown at test-time, and we only need the score (i.e., the gradient of\nthe log-probability function) to perform gradient ascent, we propose a neural\nnetwork architecture to estimate the score of $p * n$ given only noisy point\nclouds as input. We derive objective functions for training the network and\ndevelop a denoising algorithm leveraging on the estimated scores. Experiments\ndemonstrate that the proposed model outperforms state-of-the-art methods under\na variety of noise models, and shows the potential to be applied in other tasks\nsuch as point cloud upsampling. The code is available at\n\\url{https://github.com/luost26/score-denoise}.",
          "link": "http://arxiv.org/abs/2107.10981",
          "publishedOn": "2021-08-17T01:54:53.509Z",
          "wordCount": 644,
          "title": "Score-Based Point Cloud Denoising. (arXiv:2107.10981v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>",
          "description": "In this work, we address the problem of unsupervised domain adaptation for\nperson re-ID where annotations are available for the source domain but not for\ntarget. Previous methods typically follow a two-stage optimization pipeline,\nwhere the network is first pre-trained on source and then fine-tuned on target\nwith pseudo labels created by feature clustering. Such methods sustain two main\nlimitations. (1) The label noise may hinder the learning of discriminative\nfeatures for recognizing target classes. (2) The domain gap may hinder\nknowledge transferring from source to target. We propose three types of\ntechnical schemes to alleviate these issues. First, we propose a cluster-wise\ncontrastive learning algorithm (CCL) by iterative optimization of feature\nlearning and cluster refinery to learn noise-tolerant representations in the\nunsupervised manner. Second, we adopt a progressive domain adaptation (PDA)\nstrategy to gradually mitigate the domain gap between source and target data.\nThird, we propose Fourier augmentation (FA) for further maximizing the class\nseparability of re-ID models by imposing extra constraints in the Fourier\nspace. We observe that these proposed schemes are capable of facilitating the\nlearning of discriminative feature representations. Experiments demonstrate\nthat our method consistently achieves notable improvements over the\nstate-of-the-art unsupervised re-ID methods on multiple benchmarks, e.g.,\nsurpassing MMT largely by 8.1\\%, 9.9\\%, 11.4\\% and 11.1\\% mAP on the\nMarket-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks,\nrespectively.",
          "link": "http://arxiv.org/abs/2108.03439",
          "publishedOn": "2021-08-17T01:54:53.503Z",
          "wordCount": 684,
          "title": "Towards Discriminative Representation Learning for Unsupervised Person Re-identification. (arXiv:2108.03439v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>",
          "description": "While the untargeted black-box transferability of adversarial perturbations\nhas been extensively studied before, changing an unseen model's decisions to a\nspecific `targeted' class remains a challenging feat. In this paper, we propose\na new generative approach for highly transferable targeted perturbations\n(\\ours). We note that the existing methods are less suitable for this task due\nto their reliance on class-boundary information that changes from one model to\nanother, thus reducing transferability. In contrast, our approach matches the\nperturbed image `distribution' with that of the target class, leading to high\ntargeted transferability rates. To this end, we propose a new objective\nfunction that not only aligns the global distributions of source and target\nimages, but also matches the local neighbourhood structure between the two\ndomains. Based on the proposed objective, we train a generator function that\ncan adaptively synthesize perturbations specific to a given input. Our\ngenerative approach is independent of the source or target domain labels, while\nconsistently performs well against state-of-the-art methods on a wide range of\nattack settings. As an example, we achieve $32.63\\%$ target transferability\nfrom (an adversarially weak) VGG19$_{BN}$ to (a strong) WideResNet on ImageNet\nval. set, which is 4$\\times$ higher than the previous best generative attack\nand 16$\\times$ better than instance-specific iterative attack. Code is\navailable at: {\\small\\url{https://github.com/Muzammal-Naseer/TTP}}.",
          "link": "http://arxiv.org/abs/2103.14641",
          "publishedOn": "2021-08-17T01:54:53.495Z",
          "wordCount": 688,
          "title": "On Generating Transferable Targeted Perturbations. (arXiv:2103.14641v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>",
          "description": "Human behavior understanding with unmanned aerial vehicles (UAVs) is of great\nsignificance for a wide range of applications, which simultaneously brings an\nurgent demand of large, challenging, and comprehensive benchmarks for the\ndevelopment and evaluation of UAV-based models. However, existing benchmarks\nhave limitations in terms of the amount of captured data, types of data\nmodalities, categories of provided tasks, and diversities of subjects and\nenvironments. Here we propose a new benchmark - UAVHuman - for human behavior\nunderstanding with UAVs, which contains 67,428 multi-modal video sequences and\n119 subjects for action recognition, 22,476 frames for pose estimation, 41,290\nframes and 1,144 identities for person re-identification, and 22,263 frames for\nattribute recognition. Our dataset was collected by a flying UAV in multiple\nurban and rural districts in both daytime and nighttime over three months,\nhence covering extensive diversities w.r.t subjects, backgrounds,\nilluminations, weathers, occlusions, camera motions, and UAV flying attitudes.\nSuch a comprehensive and challenging benchmark shall be able to promote the\nresearch of UAV-based human behavior understanding, including action\nrecognition, pose estimation, re-identification, and attribute recognition.\nFurthermore, we propose a fisheye-based action recognition method that\nmitigates the distortions in fisheye videos via learning unbounded\ntransformations guided by flat RGB videos. Experiments show the efficacy of our\nmethod on the UAV-Human dataset. The project page:\nhttps://github.com/SUTDCV/UAV-Human",
          "link": "http://arxiv.org/abs/2104.00946",
          "publishedOn": "2021-08-17T01:54:53.477Z",
          "wordCount": 716,
          "title": "UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles. (arXiv:2104.00946v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiehong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zewei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songcen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqing Li</a>",
          "description": "Category-level 6D object pose and size estimation is to predict full pose\nconfigurations of rotation, translation, and size for object instances observed\nin single, arbitrary views of cluttered scenes. In this paper, we propose a new\nmethod of Dual Pose Network with refined learning of pose consistency for this\ntask, shortened as DualPoseNet. DualPoseNet stacks two parallel pose decoders\non top of a shared pose encoder, where the implicit decoder predicts object\nposes with a working mechanism different from that of the explicit one; they\nthus impose complementary supervision on the training of pose encoder. We\nconstruct the encoder based on spherical convolutions, and design a module of\nSpherical Fusion wherein for a better embedding of pose-sensitive features from\nthe appearance and shape observations. Given no testing CAD models, it is the\nnovel introduction of the implicit decoder that enables the refined pose\nprediction during testing, by enforcing the predicted pose consistency between\nthe two decoders using a self-adaptive loss term. Thorough experiments on\nbenchmarks of both category- and instance-level object pose datasets confirm\nefficacy of our designs. DualPoseNet outperforms existing methods with a large\nmargin in the regime of high precision. Our code is released publicly at\nhttps://github.com/Gorilla-Lab-SCUT/DualPoseNet.",
          "link": "http://arxiv.org/abs/2103.06526",
          "publishedOn": "2021-08-17T01:54:53.470Z",
          "wordCount": 711,
          "title": "DualPoseNet: Category-level 6D Object Pose and Size Estimation Using Dual Pose Network with Refined Learning of Pose Consistency. (arXiv:2103.06526v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasson_Y/0/1/0/all/0/1\">Yana Hasson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Our work aims to obtain 3D reconstruction of hands and manipulated objects\nfrom monocular videos. Reconstructing hand-object manipulations holds a great\npotential for robotics and learning from human demonstrations. The supervised\nlearning approach to this problem, however, requires 3D supervision and remains\nlimited to constrained laboratory settings and simulators for which 3D ground\ntruth is available. In this paper we first propose a learning-free fitting\napproach for hand-object reconstruction which can seamlessly handle two-hand\nobject interactions. Our method relies on cues obtained with common methods for\nobject detection, hand pose estimation and instance segmentation. We\nquantitatively evaluate our approach and show that it can be applied to\ndatasets with varying levels of difficulty for which training data is\nunavailable.",
          "link": "http://arxiv.org/abs/2108.07044",
          "publishedOn": "2021-08-17T01:54:53.450Z",
          "wordCount": 560,
          "title": "Towards unconstrained joint hand-object reconstruction from RGB videos. (arXiv:2108.07044v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjisoa_M/0/1/0/all/0/1\">Micha&#xeb;l Ramamonjisoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firman_M/0/1/0/all/0/1\">Michael Firman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1\">Jamie Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turmukhambetov_D/0/1/0/all/0/1\">Daniyar Turmukhambetov</a>",
          "description": "We present a novel method for predicting accurate depths from monocular\nimages with high efficiency. This optimal efficiency is achieved by exploiting\nwavelet decomposition, which is integrated in a fully differentiable\nencoder-decoder architecture. We demonstrate that we can reconstruct\nhigh-fidelity depth maps by predicting sparse wavelet coefficients. In contrast\nwith previous works, we show that wavelet coefficients can be learned without\ndirect supervision on coefficients. Instead we supervise only the final depth\nimage that is reconstructed through the inverse wavelet transform. We\nadditionally show that wavelet coefficients can be learned in fully\nself-supervised scenarios, without access to ground-truth depth. Finally, we\napply our method to different state-of-the-art monocular depth estimation\nmodels, in each case giving similar or better results compared to the original\nmodel, while requiring less than half the multiply-adds in the decoder network.\nCode at https://github.com/nianticlabs/wavelet-monodepth",
          "link": "http://arxiv.org/abs/2106.02022",
          "publishedOn": "2021-08-17T01:54:53.431Z",
          "wordCount": 608,
          "title": "Single Image Depth Prediction with Wavelet Decomposition. (arXiv:2106.02022v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08357",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Wenbin Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_D/0/1/0/all/0/1\">Dehua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Deep learning based methods, especially convolutional neural networks (CNNs)\nhave been successfully applied in the field of single image super-resolution\n(SISR). To obtain better fidelity and visual quality, most of existing networks\nare of heavy design with massive computation. However, the computation\nresources of modern mobile devices are limited, which cannot easily support the\nexpensive cost. To this end, this paper explores a novel frequency-aware\ndynamic network for dividing the input into multiple parts according to its\ncoefficients in the discrete cosine transform (DCT) domain. In practice, the\nhigh-frequency part will be processed using expensive operations and the\nlower-frequency part is assigned with cheap operations to relieve the\ncomputation burden. Since pixels or image patches belong to low-frequency areas\ncontain relatively few textural details, this dynamic network will not affect\nthe quality of resulting super-resolution images. In addition, we embed\npredictors into the proposed dynamic network to end-to-end fine-tune the\nhandcrafted frequency-aware masks. Extensive experiments conducted on benchmark\nSISR models and datasets show that the frequency-aware dynamic network can be\nemployed for various SISR neural architectures to obtain the better tradeoff\nbetween visual quality and computational complexity. For instance, we can\nreduce the FLOPs of SR models by approximate 50% while preserving\nstate-of-the-art SISR performance.",
          "link": "http://arxiv.org/abs/2103.08357",
          "publishedOn": "2021-08-17T01:54:53.425Z",
          "wordCount": 669,
          "title": "Learning Frequency-aware Dynamic Network for Efficient Super-Resolution. (arXiv:2103.08357v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kenny Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pogue_A/0/1/0/all/0/1\">Alexandra Pogue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_B/0/1/0/all/0/1\">Brett T. Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agha_mohammadi_A/0/1/0/all/0/1\">Ali-akbar Agha-mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1\">Ankur Mehta</a>",
          "description": "Monocular depth inference has gained tremendous attention from researchers in\nrecent years and remains as a promising replacement for expensive\ntime-of-flight sensors, but issues with scale acquisition and implementation\noverhead still plague these systems. To this end, this work presents an\nunsupervised learning framework that is able to predict at-scale depth maps and\negomotion, in addition to camera intrinsics, from a sequence of monocular\nimages via a single network. Our method incorporates both spatial and temporal\ngeometric constraints to resolve depth and pose scale factors, which are\nenforced within the supervisory reconstruction loss functions at training time.\nOnly unlabeled stereo sequences are required for training the weights of our\nsingle-network architecture, which reduces overall implementation overhead as\ncompared to previous methods. Our results demonstrate strong performance when\ncompared to the current state-of-the-art on multiple sequences of the KITTI\ndriving dataset and can provide faster training times with its reduced network\ncomplexity.",
          "link": "http://arxiv.org/abs/2011.01354",
          "publishedOn": "2021-08-17T01:54:53.419Z",
          "wordCount": 656,
          "title": "Unsupervised Monocular Depth Learning with Integrated Intrinsics and Spatio-Temporal Constraints. (arXiv:2011.01354v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turkoglu_M/0/1/0/all/0/1\">Mehmet Ozgur Turkoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perich_G/0/1/0/all/0/1\">Gregor Perich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebisch_F/0/1/0/all/0/1\">Frank Liebisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Streit_C/0/1/0/all/0/1\">Constantin Streit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>",
          "description": "The aim of this paper is to map agricultural crops by classifying satellite\nimage time series. Domain experts in agriculture work with crop type labels\nthat are organised in a hierarchical tree structure, where coarse classes (like\norchards) are subdivided into finer ones (like apples, pears, vines, etc.). We\ndevelop a crop classification method that exploits this expert knowledge and\nsignificantly improves the mapping of rare crop types. The three-level label\nhierarchy is encoded in a convolutional, recurrent neural network (convRNN),\nsuch that for each pixel the model predicts three labels at different level of\ngranularity. This end-to-end trainable, hierarchical network architecture\nallows the model to learn joint feature representations of rare classes (e.g.,\napples, pears) at a coarser level (e.g., orchard), thereby boosting\nclassification performance at the fine-grained level. Additionally, labelling\nat different granularity also makes it possible to adjust the output according\nto the classification scores; as coarser labels with high confidence are\nsometimes more useful for agricultural practice than fine-grained but very\nuncertain labels. We validate the proposed method on a new, large dataset that\nwe make public. ZueriCrop covers an area of 50 km x 48 km in the Swiss cantons\nof Zurich and Thurgau with a total of 116'000 individual fields spanning 48\ncrop classes, and 28,000 (multi-temporal) image patches from Sentinel-2. We\ncompare our proposed hierarchical convRNN model with several baselines,\nincluding methods designed for imbalanced class distributions. The hierarchical\napproach performs superior by at least 9.9 percentage points in F1-score.",
          "link": "http://arxiv.org/abs/2102.08820",
          "publishedOn": "2021-08-17T01:54:53.411Z",
          "wordCount": 731,
          "title": "Crop mapping from image time series: deep learning with multi-scale label hierarchies. (arXiv:2102.08820v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1\">Ben Mildenhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tancik_M/0/1/0/all/0/1\">Matthew Tancik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Peter Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>",
          "description": "The rendering procedure used by neural radiance fields (NeRF) samples a scene\nwith a single ray per pixel and may therefore produce renderings that are\nexcessively blurred or aliased when training or testing images observe scene\ncontent at different resolutions. The straightforward solution of supersampling\nby rendering with multiple rays per pixel is impractical for NeRF, because\nrendering each ray requires querying a multilayer perceptron hundreds of times.\nOur solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to\nrepresent the scene at a continuously-valued scale. By efficiently rendering\nanti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable\naliasing artifacts and significantly improves NeRF's ability to represent fine\ndetails, while also being 7% faster than NeRF and half the size. Compared to\nNeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with\nNeRF and by 60% on a challenging multiscale variant of that dataset that we\npresent. Mip-NeRF is also able to match the accuracy of a brute-force\nsupersampled NeRF on our multiscale dataset while being 22x faster.",
          "link": "http://arxiv.org/abs/2103.13415",
          "publishedOn": "2021-08-17T01:54:53.404Z",
          "wordCount": 660,
          "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. (arXiv:2103.13415v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shi-Wei Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jyh-Horng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jo-Yu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1\">Chien-Hao Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Meng-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fang-Pang Lin</a>",
          "description": "In the monsoon season, sudden flood events occur frequently in urban areas,\nwhich hamper the social and economic activities and may threaten the\ninfrastructure and lives. The use of an efficient large-scale waterlogging\nsensing and information system can provide valuable real-time disaster\ninformation to facilitate disaster management and enhance awareness of the\ngeneral public to alleviate losses during and after flood disasters. Therefore,\nin this study, a visual sensing approach driven by deep neural networks and\ninformation and communication technology was developed to provide an end-to-end\nmechanism to realize waterlogging sensing and event-location mapping. The use\nof a deep sensing system in the monsoon season in Taiwan was demonstrated, and\nwaterlogging events were predicted on the island-wide scale. The system could\nsense approximately 2379 vision sources through an internet of video things\nframework and transmit the event-location information in 5 min. The proposed\napproach can sense waterlogging events at a national scale and provide an\nefficient and highly scalable alternative to conventional waterlogging sensing\nmethods.",
          "link": "http://arxiv.org/abs/2103.05927",
          "publishedOn": "2021-08-17T01:54:53.383Z",
          "wordCount": 651,
          "title": "Deep Sensing of Urban Waterlogging. (arXiv:2103.05927v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08501",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Patel_S/0/1/0/all/0/1\">Shaswat Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lohakare_M/0/1/0/all/0/1\">Maithili Lohakare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prajapati_S/0/1/0/all/0/1\">Samyak Prajapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1\">Shaanya Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_N/0/1/0/all/0/1\">Nancy Patel</a>",
          "description": "Patients with long-standing diabetes often fall prey to Diabetic Retinopathy\n(DR) resulting in changes in the retina of the human eye, which may lead to\nloss of vision in extreme cases. The aim of this study is two-fold: (a) create\ndeep learning models that were trained to grade degraded retinal fundus images\nand (b) to create a browser-based application that will aid in diagnostic\nprocedures by highlighting the key features of the fundus image. In this\nresearch work, we have emulated the images plagued by distortions by degrading\nthe images based on multiple different combinations of Light Transmission\nDisturbance, Image Blurring and insertion of Retinal Artifacts. InceptionV3,\nResNet-50 and InceptionResNetV2 were trained and used to classify retinal\nfundus images based on their severity level and then further used in the\ncreation of a browser-based application, which implements the Integration\nGradient (IG) Attribution Mask on the input image and demonstrates the\npredictions made by the model and the probability associated with each class.",
          "link": "http://arxiv.org/abs/2103.08501",
          "publishedOn": "2021-08-17T01:54:53.347Z",
          "wordCount": 657,
          "title": "DiaRet: A browser-based application for the grading of Diabetic Retinopathy with Integrated Gradients. (arXiv:2103.08501v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hezhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Junfu Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Sign language recognition (SLR) is a challenging problem, involving complex\nmanual features, i.e., hand gestures, and fine-grained non-manual features\n(NMFs), i.e., facial expression, mouth shapes, etc. Although manual features\nare dominant, non-manual features also play an important role in the expression\nof a sign word. Specifically, many sign words convey different meanings due to\nnon-manual features, even though they share the same hand gestures. This\nambiguity introduces great challenges in the recognition of sign words. To\ntackle the above issue, we propose a simple yet effective architecture called\nGlobal-local Enhancement Network (GLE-Net), including two mutually promoted\nstreams towards different crucial aspects of SLR. Of the two streams, one\ncaptures the global contextual relationship, while the other stream captures\nthe discriminative fine-grained cues. Moreover, due to the lack of datasets\nexplicitly focusing on this kind of features, we introduce the first\nnon-manual-features-aware isolated Chinese sign language dataset~(NMFs-CSL)\nwith a total vocabulary size of 1,067 sign words in daily life. Extensive\nexperiments on NMFs-CSL and SLR500 datasets demonstrate the effectiveness of\nour method.",
          "link": "http://arxiv.org/abs/2008.10428",
          "publishedOn": "2021-08-17T01:54:53.340Z",
          "wordCount": 645,
          "title": "Global-local Enhancement Network for NMFs-aware Sign Language Recognition. (arXiv:2008.10428v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "Previous adversarial training raises model robustness under the compromise of\naccuracy on natural data. In this paper, we reduce natural accuracy\ndegradation. We use the model logits from one clean model to guide learning of\nanother one robust model, taking into consideration that logits from the well\ntrained clean model embed the most discriminative features of natural data,\n{\\it e.g.}, generalizable classifier boundary. Our solution is to constrain\nlogits from the robust model that takes adversarial examples as input and makes\nit similar to those from the clean model fed with corresponding natural data.\nIt lets the robust model inherit the classifier boundary of the clean model.\nMoreover, we observe such boundary guidance can not only preserve high natural\naccuracy but also benefit model robustness, which gives new insights and\nfacilitates progress for the adversarial community. Finally, extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny ImageNet testify to the\neffectiveness of our method. We achieve new state-of-the-art robustness on\nCIFAR-100 without additional real or synthetic data with auto-attack benchmark\n\\footnote{\\url{https://github.com/fra31/auto-attack}}. Our code is available at\n\\url{https://github.com/dvlab-research/LBGAT}.",
          "link": "http://arxiv.org/abs/2011.11164",
          "publishedOn": "2021-08-17T01:54:53.331Z",
          "wordCount": 638,
          "title": "Learnable Boundary Guided Adversarial Training. (arXiv:2011.11164v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiqiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>",
          "description": "Traditional normalization techniques (e.g., Batch Normalization and Instance\nNormalization) generally and simplistically assume that training and test data\nfollow the same distribution. As distribution shifts are inevitable in\nreal-world applications, well-trained models with previous normalization\nmethods can perform badly in new environments. Can we develop new normalization\nmethods to improve generalization robustness under distribution shifts? In this\npaper, we answer the question by proposing CrossNorm and SelfNorm. CrossNorm\nexchanges channel-wise mean and variance between feature maps to enlarge\ntraining distribution, while SelfNorm uses attention to recalibrate the\nstatistics to bridge gaps between training and test distributions. CrossNorm\nand SelfNorm can complement each other, though exploring different directions\nin statistics usage. Extensive experiments on different fields (vision and\nlanguage), tasks (classification and segmentation), settings (supervised and\nsemi-supervised), and distribution shift types (synthetic and natural) show the\neffectiveness. Code is available at\nhttps://github.com/amazon-research/crossnorm-selfnorm",
          "link": "http://arxiv.org/abs/2102.02811",
          "publishedOn": "2021-08-17T01:54:53.314Z",
          "wordCount": 621,
          "title": "CrossNorm and SelfNorm for Generalization under Distribution Shifts. (arXiv:2102.02811v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Haoxi Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>",
          "description": "Human imitation has become topical recently, driven by GAN's ability to\ndisentangle human pose and body content. However, the latest methods hardly\nfocus on 3D information, and to avoid self-occlusion, a massive amount of input\nimages are needed. In this paper, we propose RIN, a novel volume-based\nframework for reconstructing a textured 3D model from a single picture and\nimitating a subject with the generated model. Specifically, to estimate most of\nthe human texture, we propose a U-Net-like front-to-back translation network.\nWith both front and back images input, the textured volume recovery module\nallows us to color a volumetric human. A sequence of 3D poses then guides the\ncolored volume via Flowable Disentangle Networks as a volume-to-volume\ntranslation task. To project volumes to a 2D plane during training, we design a\ndifferentiable depth-aware renderer. Our experiments demonstrate that our\nvolume-based model is adequate for human imitation, and the back view can be\nestimated reliably using our network. While prior works based on either 2D pose\nor semantic map often fail for the unstable appearance of a human, our\nframework can still produce concrete results, which are competitive to those\nimagined from multi-view input.",
          "link": "http://arxiv.org/abs/2011.12024",
          "publishedOn": "2021-08-17T01:54:53.307Z",
          "wordCount": 688,
          "title": "RIN: Textured Human Model Recovery and Imitation with a Single Image. (arXiv:2011.12024v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzger_N/0/1/0/all/0/1\">Nando Metzger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turkoglu_M/0/1/0/all/0/1\">Mehmet Ozgur Turkoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "Optical satellite sensors cannot see the Earth's surface through clouds.\nDespite the periodic revisit cycle, image sequences acquired by Earth\nobservation satellites are therefore irregularly sampled in time.\nState-of-the-art methods for crop classification (and other time series\nanalysis tasks) rely on techniques that implicitly assume regular temporal\nspacing between observations, such as recurrent neural networks (RNNs). We\npropose to use neural ordinary differential equations (NODEs) in combination\nwith RNNs to classify crop types in irregularly spaced image sequences. The\nresulting ODE-RNN models consist of two steps: an update step, where a\nrecurrent unit assimilates new input data into the model's hidden state; and a\nprediction step, in which NODE propagates the hidden state until the next\nobservation arrives. The prediction step is based on a continuous\nrepresentation of the latent dynamics, which has several advantages. At the\nconceptual level, it is a more natural way to describe the mechanisms that\ngovern the phenological cycle. From a practical point of view, it makes it\npossible to sample the system state at arbitrary points in time, such that one\ncan integrate observations whenever they are available, and extrapolate beyond\nthe last observation. Our experiments show that ODE-RNN indeed improves\nclassification accuracy over common baselines such as LSTM, GRU, and temporal\nconvolution. The gains are most prominent in the challenging scenario where\nonly few observations are available (i.e., frequent cloud cover). Moreover, we\nshow that the ability to extrapolate translates to better classification\nperformance early in the season, which is important for forecasting.",
          "link": "http://arxiv.org/abs/2012.02542",
          "publishedOn": "2021-08-17T01:54:53.301Z",
          "wordCount": 727,
          "title": "Crop Classification under Varying Cloud Cover with Neural Ordinary Differential Equations. (arXiv:2012.02542v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zia Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salekin_A/0/1/0/all/0/1\">Asif Salekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1\">Tauhidur Rahman</a>",
          "description": "Hyperspectral image (HSI) with narrow spectral bands can capture rich\nspectral information, but it sacrifices its spatial resolution in the process.\nMany machine-learning-based HSI super-resolution (SR) algorithms have been\nproposed recently. However, one of the fundamental limitations of these\napproaches is that they are highly dependent on image and camera settings and\ncan only learn to map an input HSI with one specific setting to an output HSI\nwith another. However, different cameras capture images with different spectral\nresponse functions and bands numbers due to the diversity of HSI cameras.\nConsequently, the existing machine-learning-based approaches fail to learn to\nsuper-resolve HSIs for a wide variety of input-output band settings. We propose\na single Meta-Learning-Based Super-Resolution (MLSR) model, which can take in\nHSI images at an arbitrary number of input bands' peak wavelengths and generate\nSR HSIs with an arbitrary number of output bands' peak wavelengths. We leverage\nNTIRE2020 and ICVL datasets to train and validate the performance of the MLSR\nmodel. The results show that the single proposed model can successfully\ngenerate super-resolved HSI bands at arbitrary input-output band settings. The\nresults are better or at least comparable to baselines that are separately\ntrained on a specific input-output band setting.",
          "link": "http://arxiv.org/abs/2103.10614",
          "publishedOn": "2021-08-17T01:54:53.290Z",
          "wordCount": 672,
          "title": "Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings. (arXiv:2103.10614v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haliassos_A/0/1/0/all/0/1\">Alexandros Haliassos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vougioukas_K/0/1/0/all/0/1\">Konstantinos Vougioukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>",
          "description": "Although current deep learning-based face forgery detectors achieve\nimpressive performance in constrained scenarios, they are vulnerable to samples\ncreated by unseen manipulation methods. Some recent works show improvements in\ngeneralisation but rely on cues that are easily corrupted by common\npost-processing operations such as compression. In this paper, we propose\nLipForensics, a detection approach capable of both generalising to novel\nmanipulations and withstanding various distortions. LipForensics targets\nhigh-level semantic irregularities in mouth movements, which are common in many\ngenerated videos. It consists in first pretraining a spatio-temporal network to\nperform visual speech recognition (lipreading), thus learning rich internal\nrepresentations related to natural mouth motion. A temporal network is\nsubsequently finetuned on fixed mouth embeddings of real and forged data in\norder to detect fake videos based on mouth movements without overfitting to\nlow-level, manipulation-specific artefacts. Extensive experiments show that\nthis simple approach significantly surpasses the state-of-the-art in terms of\ngeneralisation to unseen manipulations and robustness to perturbations, as well\nas shed light on the factors responsible for its performance. Code is available\non GitHub.",
          "link": "http://arxiv.org/abs/2012.07657",
          "publishedOn": "2021-08-17T01:54:53.283Z",
          "wordCount": 663,
          "title": "Lips Don't Lie: A Generalisable and Robust Approach to Face Forgery Detection. (arXiv:2012.07657v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soldan_M/0/1/0/all/0/1\">Mattia Soldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Sisi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1\">Jesper Tegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Grounding language queries in videos aims at identifying the time interval\n(or moment) semantically relevant to a language query. The solution to this\nchallenging task demands understanding videos' and queries' semantic content\nand the fine-grained reasoning about their multi-modal interactions. Our key\nidea is to recast this challenge into an algorithmic graph matching problem.\nFueled by recent advances in Graph Neural Networks, we propose to leverage\nGraph Convolutional Networks to model video and textual information as well as\ntheir semantic alignment. To enable the mutual exchange of information across\nthe modalities, we design a novel Video-Language Graph Matching Network\n(VLG-Net) to match video and query graphs. Core ingredients include\nrepresentation graphs built atop video snippets and query tokens separately and\nused to model intra-modality relationships. A Graph Matching layer is adopted\nfor cross-modal context modeling and multi-modal fusion. Finally, moment\ncandidates are created using masked moment attention pooling by fusing the\nmoment's enriched snippet features. We demonstrate superior performance over\nstate-of-the-art grounding methods on three widely used datasets for temporal\nlocalization of moments in videos with language queries: ActivityNet-Captions,\nTACoS, and DiDeMo.",
          "link": "http://arxiv.org/abs/2011.10132",
          "publishedOn": "2021-08-17T01:54:53.264Z",
          "wordCount": 672,
          "title": "VLG-Net: Video-Language Graph Matching Network for Video Grounding. (arXiv:2011.10132v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lianbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoli Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Zero-Shot Learning (ZSL) targets at recognizing unseen categories by\nleveraging auxiliary information, such as attribute embedding. Despite the\nencouraging results achieved, prior ZSL approaches focus on improving the\ndiscriminant power of seen-class features, yet have largely overlooked the\ngeometric structure of the samples and the prototypes. The subsequent\nattribute-based generative adversarial network (GAN), as a result, also\nneglects the topological information in sample generation and further yields\ninferior performances in classifying the visual features of unseen classes. In\nthis paper, we introduce a novel structure-aware feature generation scheme,\ntermed as SA-GAN, to explicitly account for the topological structure in\nlearning both the latent space and the generative networks. Specifically, we\nintroduce a constraint loss to preserve the initial geometric structure when\nlearning a discriminative latent space, and carry out our GAN training with\nadditional supervising signals from a structure-aware discriminator and a\nreconstruction module. The former supervision distinguishes fake and real\nsamples based on their affinity to class prototypes, while the latter aims to\nreconstruct the original feature space from the generated latent space. This\ntopology-preserving mechanism enables our method to significantly enhance the\ngeneralization capability on unseen-classes and consequently improve the\nclassification performance. Experiments on four benchmarks demonstrate that the\nproposed approach consistently outperforms the state of the art. Our code can\nbe found in the supplementary material and will also be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2108.07032",
          "publishedOn": "2021-08-17T01:54:53.258Z",
          "wordCount": 660,
          "title": "Structure-Aware Feature Generation for Zero-Shot Learning. (arXiv:2108.07032v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_L/0/1/0/all/0/1\">Lingjuan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yunpeng Dong</a>",
          "description": "Object detection in optical remote sensing images is an important and\nchallenging task. In recent years, the methods based on convolutional neural\nnetworks have made good progress. However, due to the large variation in object\nscale, aspect ratio, and arbitrary orientation, the detection performance is\ndifficult to be further improved. In this paper, we discuss the role of\ndiscriminative features in object detection, and then propose a Critical\nFeature Capturing Network (CFC-Net) to improve detection accuracy from three\naspects: building powerful feature representation, refining preset anchors, and\noptimizing label assignment. Specifically, we first decouple the classification\nand regression features, and then construct robust critical features adapted to\nthe respective tasks through the Polarization Attention Module (PAM). With the\nextracted discriminative regression features, the Rotation Anchor Refinement\nModule (R-ARM) performs localization refinement on preset horizontal anchors to\nobtain superior rotation anchors. Next, the Dynamic Anchor Learning (DAL)\nstrategy is given to adaptively select high-quality anchors based on their\nability to capture critical features. The proposed framework creates more\npowerful semantic representations for objects in remote sensing images and\nachieves high-performance real-time object detection. Experimental results on\nthree remote sensing datasets including HRSC2016, DOTA, and UCAS-AOD show that\nour method achieves superior detection performance compared with many\nstate-of-the-art approaches. Code and models are available at\nhttps://github.com/ming71/CFC-Net.",
          "link": "http://arxiv.org/abs/2101.06849",
          "publishedOn": "2021-08-17T01:54:53.253Z",
          "wordCount": 702,
          "title": "CFC-Net: A Critical Feature Capturing Network for Arbitrary-Oriented Object Detection in Remote Sensing Images. (arXiv:2101.06849v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anindya Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anirban Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Deep neural networks are the default choice of learning models for computer\nvision tasks. Extensive work has been carried out in recent years on explaining\ndeep models for vision tasks such as classification. However, recent work has\nshown that it is possible for these models to produce substantially different\nattribution maps even when two very similar images are given to the network,\nraising serious questions about trustworthiness. To address this issue, we\npropose a robust attribution training strategy to improve attributional\nrobustness of deep neural networks. Our method carefully analyzes the\nrequirements for attributional robustness and introduces two new regularizers\nthat preserve a model's attribution map during attacks. Our method surpasses\nstate-of-the-art attributional robustness methods by a margin of approximately\n3% to 9% in terms of attribution robustness measures on several datasets\nincluding MNIST, FMNIST, Flower and GTSRB.",
          "link": "http://arxiv.org/abs/2012.14395",
          "publishedOn": "2021-08-17T01:54:53.247Z",
          "wordCount": 613,
          "title": "Enhanced Regularizers for Attributional Robustness. (arXiv:2012.14395v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "With the tremendous advances in the architecture and scale of convolutional\nneural networks (CNNs) over the past few decades, they can easily reach or even\nexceed the performance of humans in certain tasks. However, a recently\ndiscovered shortcoming of CNNs is that they are vulnerable to adversarial\nattacks. Although the adversarial robustness of CNNs can be improved by\nadversarial training, there is a trade-off between standard accuracy and\nadversarial robustness. From the neural architecture perspective, this paper\naims to improve the adversarial robustness of the backbone CNNs that have a\nsatisfactory accuracy. Under a minimal computational overhead, the introduction\nof a dilation architecture is expected to be friendly with the standard\nperformance of the backbone CNN while pursuing adversarial robustness.\nTheoretical analyses on the standard and adversarial error bounds naturally\nmotivate the proposed neural architecture dilation algorithm. Experimental\nresults on real-world datasets and benchmark neural networks demonstrate the\neffectiveness of the proposed algorithm to balance the accuracy and adversarial\nrobustness.",
          "link": "http://arxiv.org/abs/2108.06885",
          "publishedOn": "2021-08-17T01:54:53.240Z",
          "wordCount": 614,
          "title": "Neural Architecture Dilation for Adversarial Robustness. (arXiv:2108.06885v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.01216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huh_J/0/1/0/all/0/1\">Jaesung Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1\">Triantafyllos Afouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "The goal of this paper is speaker diarisation of videos collected 'in the\nwild'. We make three key contributions. First, we propose an automatic\naudio-visual diarisation method for YouTube videos. Our method consists of\nactive speaker detection using audio-visual methods and speaker verification\nusing self-enrolled speaker models. Second, we integrate our method into a\nsemi-automatic dataset creation pipeline which significantly reduces the number\nof hours required to annotate videos with diarisation labels. Finally, we use\nthis pipeline to create a large-scale diarisation dataset called VoxConverse,\ncollected from 'in the wild' videos, which we will release publicly to the\nresearch community. Our dataset consists of overlapping speech, a large and\ndiverse speaker pool, and challenging background conditions.",
          "link": "http://arxiv.org/abs/2007.01216",
          "publishedOn": "2021-08-17T01:54:53.221Z",
          "wordCount": 642,
          "title": "Spot the conversation: speaker diarisation in the wild. (arXiv:2007.01216v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03932",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ayatollahi_F/0/1/0/all/0/1\">Fazael Ayatollahi</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Shokouhi_S/0/1/0/all/0/1\">Shahriar B. Shokouhi</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Mann_R/0/1/0/all/0/1\">Ritse M. Mann</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a> (2 and 3) ((1) Electrical Engineering Department, Iran University of Science and Technology (IUST), Tehran, Iran, (2) Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, the Netherlands, (3) Department of Radiation Oncology, Netherlands Cancer Institute, Amsterdam, the Netherlands)",
          "description": "Purpose: We propose a deep learning-based computer-aided detection (CADe)\nmethod to detect breast lesions in ultrafast DCE-MRI sequences. This method\nuses both the three-dimensional spatial information and temporal information\nobtained from the early-phase of the dynamic acquisition. Methods: The proposed\nCADe method, based on a modified 3D RetinaNet model, operates on ultrafast T1\nweighted sequences, which are preprocessed for motion compensation, temporal\nnormalization, and are cropped before passing into the model. The model is\noptimized to enable the detection of relatively small breast lesions in a\nscreening setting, focusing on detection of lesions that are harder to\ndifferentiate from confounding structures inside the breast. Results: The\nmethod was developed based on a dataset consisting of 489 ultrafast MRI studies\nobtained from 462 patients containing a total of 572 lesions (365 malignant,\n207 benign) and achieved a detection rate, sensitivity, and detection rate of\nbenign lesions of 0.90 (0.876-0.934), 0.95 (0.934-0.980), and 0.81\n(0.751-0.871) at 4 false positives per normal breast with 10-fold\ncross-testing, respectively. Conclusions: The deep learning architecture used\nfor the proposed CADe application can efficiently detect benign and malignant\nlesions on ultrafast DCE-MRI. Furthermore, utilizing the less visible hard-to\ndetect-lesions in training improves the learning process and, subsequently,\ndetection of malignant breast lesions.",
          "link": "http://arxiv.org/abs/2102.03932",
          "publishedOn": "2021-08-17T01:54:53.206Z",
          "wordCount": 723,
          "title": "Automatic Breast Lesion Detection in Ultrafast DCE-MRI Using Deep Learning. (arXiv:2102.03932v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shihao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "This paper focuses on a new problem of estimating human pose and shape from\nsingle polarization images. Polarization camera is known to be able to capture\nthe polarization of reflected lights that preserves rich geometric cues of an\nobject surface. Inspired by the recent applications in surface normal\nreconstruction from polarization images, in this paper, we attempt to estimate\nhuman pose and shape from single polarization images by leveraging the\npolarization-induced geometric cues. A dedicated two-stage pipeline is\nproposed: given a single polarization image, stage one (Polar2Normal) focuses\non the fine detailed human body surface normal estimation; stage two\n(Polar2Shape) then reconstructs clothed human shape from the polarization image\nand the estimated surface normal. To empirically validate our approach, a\ndedicated dataset (PHSPD) is constructed, consisting of over 500K frames with\naccurate pose and shape annotations. Empirical evaluations on this real-world\ndataset as well as a synthetic dataset, SURREAL, demonstrate the effectiveness\nof our approach. It suggests polarization camera as a promising alternative to\nthe more conventional RGB camera for human pose and shape estimation.",
          "link": "http://arxiv.org/abs/2108.06834",
          "publishedOn": "2021-08-17T01:54:50.538Z",
          "wordCount": 628,
          "title": "Human Pose and Shape Estimation from Single Polarization Images. (arXiv:2108.06834v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.00463",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ikoma_H/0/1/0/all/0/1\">Hayato Ikoma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jeon_D/0/1/0/all/0/1\">Daniel S. Jeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_M/0/1/0/all/0/1\">Min H. Kim</a>",
          "description": "Imaging depth and spectrum have been extensively studied in isolation from\neach other for decades. Recently, hyperspectral-depth (HS-D) imaging emerges to\ncapture both information simultaneously by combining two different imaging\nsystems; one for depth, the other for spectrum. While being accurate, this\ncombinational approach induces increased form factor, cost, capture time, and\nalignment/registration problems. In this work, departing from the combinational\nprinciple, we propose a compact single-shot monocular HS-D imaging method. Our\nmethod uses a diffractive optical element (DOE), the point spread function of\nwhich changes with respect to both depth and spectrum. This enables us to\nreconstruct spectrum and depth from a single captured image. To this end, we\ndevelop a differentiable simulator and a neural-network-based reconstruction\nthat are jointly optimized via automatic differentiation. To facilitate\nlearning the DOE, we present a first HS-D dataset by building a benchtop HS-D\nimager that acquires high-quality ground truth. We evaluate our method with\nsynthetic and real experiments by building an experimental prototype and\nachieve state-of-the-art HS-D imaging results.",
          "link": "http://arxiv.org/abs/2009.00463",
          "publishedOn": "2021-08-17T01:54:50.529Z",
          "wordCount": 659,
          "title": "Single-shot Hyperspectral-Depth Imaging with Learned Diffractive Optics. (arXiv:2009.00463v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.11897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>",
          "description": "State-of-the-art methods in image-to-image translation are capable of\nlearning a mapping from a source domain to a target domain with unpaired image\ndata. Though the existing methods have achieved promising results, they still\nproduce visual artifacts, being able to translate low-level information but not\nhigh-level semantics of input images. One possible reason is that generators do\nnot have the ability to perceive the most discriminative parts between the\nsource and target domains, thus making the generated images low quality. In\nthis paper, we propose a new Attention-Guided Generative Adversarial Networks\n(AttentionGAN) for the unpaired image-to-image translation task. AttentionGAN\ncan identify the most discriminative foreground objects and minimize the change\nof the background. The attention-guided generators in AttentionGAN are able to\nproduce attention masks, and then fuse the generation output with the attention\nmasks to obtain high-quality target images. Accordingly, we also design a novel\nattention-guided discriminator which only considers attended regions. Extensive\nexperiments are conducted on several generative tasks with eight public\ndatasets, demonstrating that the proposed method is effective to generate\nsharper and more realistic images compared with existing competitive models.\nThe code is available at https://github.com/Ha0Tang/AttentionGAN.",
          "link": "http://arxiv.org/abs/1911.11897",
          "publishedOn": "2021-08-17T01:54:50.521Z",
          "wordCount": 722,
          "title": "AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks. (arXiv:1911.11897v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>",
          "description": "We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter\npruning), which slims down a CNN by reducing the width (number of output\nchannels) of convolutional layers. Inspired by the neurobiology research about\nthe independence of remembering and forgetting, we propose to re-parameterize a\nCNN into the remembering parts and forgetting parts, where the former learn to\nmaintain the performance and the latter learn to prune. Via training with\nregular SGD on the former but a novel update rule with penalty gradients on the\nlatter, we realize structured sparsity. Then we equivalently merge the\nremembering and forgetting parts into the original architecture with narrower\nlayers. In this sense, ResRep can be viewed as a successful application of\nStructural Re-parameterization. Such a methodology distinguishes ResRep from\nthe traditional learning-based pruning paradigm that applies a penalty on\nparameters to produce sparsity, which may suppress the parameters essential for\nthe remembering. ResRep slims down a standard ResNet-50 with 76.15% accuracy on\nImageNet to a narrower one with only 45% FLOPs and no accuracy drop, which is\nthe first to achieve lossless pruning with such a high compression ratio. The\ncode and models are at https://github.com/DingXiaoH/ResRep.",
          "link": "http://arxiv.org/abs/2007.03260",
          "publishedOn": "2021-08-17T01:54:50.506Z",
          "wordCount": 704,
          "title": "ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting. (arXiv:2007.03260v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12066",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yanming Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chunyan Wang</a>",
          "description": "The work presented in this paper is to propose a reliable high-quality system\nof Convolutional Neural Network (CNN) for brain tumor segmentation with a low\ncomputation requirement. The system consists of a CNN for the main processing\nfor the segmentation, a pre-CNN block for data reduction and post-CNN\nrefinement block. The unique CNN consists of 7 convolution layers involving\nonly 108 kernels and 20308 trainable parameters. It is custom-designed,\nfollowing the proposed paradigm of ASCNN (application specific CNN), to perform\nmono-modality and cross-modality feature extraction, tumor localization and\npixel classification. Each layer fits the task assigned to it, by means of (i)\nappropriate normalization applied to its input data, (ii) correct convolution\nmodes for the assigned task, and (iii) suitable nonlinear transformation to\noptimize the convolution results. In this specific design context, the number\nof kernels in each of the 7 layers is made to be just-sufficient for its task,\ninstead of exponentially growing over the layers, to increase information\ndensity and to reduce randomness in the processing. The proposed activation\nfunction Full-ReLU helps to halve the number of kernels in convolution layers\nof high-pass filtering without degrading processing quality. A large number of\nexperiments with BRATS2018 dataset have been conducted to measure the\nprocessing quality and reproducibility of the proposed system. The results\ndemonstrate that the system reproduces reliably almost the same output to the\nsame input after retraining. The mean dice scores for enhancing tumor, whole\ntumor and tumor core are 77.2%, 89.2% and 76.3%, respectively. The simple\nstructure and reliable high processing quality of the proposed system will\nfacilitate its implementation and medical applications.",
          "link": "http://arxiv.org/abs/2007.12066",
          "publishedOn": "2021-08-17T01:54:50.466Z",
          "wordCount": 739,
          "title": "A Computation-Efficient CNN System for High-Quality Brain Tumor Segmentation. (arXiv:2007.12066v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prokofiev_K/0/1/0/all/0/1\">Kirill Prokofiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sovrasov_V/0/1/0/all/0/1\">Vladislav Sovrasov</a>",
          "description": "Nowadays deep learning-based methods have achieved a remarkable progress at\nthe image classification task among a wide range of commonly used datasets\n(ImageNet, CIFAR, SVHN, Caltech 101, SUN397, etc.). SOTA performance on each of\nthe mentioned datasets is obtained by careful tuning of the model architecture\nand training tricks according to the properties of the target data. Although\nthis approach allows setting academic records, it is unrealistic that an\naverage data scientist would have enough resources to build a sophisticated\ntraining pipeline for every image classification task he meets in practice.\nThis work is focusing on reviewing the latest augmentation and regularization\nmethods for the image classification and exploring ways to automatically choose\nsome of the most important hyperparameters: total number of epochs, initial\nlearning rate value and it's schedule. Having a training procedure equipped\nwith a lightweight modern CNN architecture (like bileNetV3 or EfficientNet),\nsufficient level of regularization and adaptive to data learning rate schedule,\nwe can achieve a reasonable performance on a variety of downstream image\nclassification tasks without manual tuning of parameters to each particular\ntask. Resulting models are computationally efficient and can be deployed to CPU\nusing the OpenVINO toolkit. Source code is available as a part of the OpenVINO\nTraining Extensions (https://github.com/openvinotoolkit/training_extensions).",
          "link": "http://arxiv.org/abs/2108.07049",
          "publishedOn": "2021-08-17T01:54:50.455Z",
          "wordCount": 654,
          "title": "Towards Efficient and Data Agnostic Image Classification Training Pipeline for Embedded Systems. (arXiv:2108.07049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifei Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>",
          "description": "In this paper, we focus on recognizing 3D shapes from arbitrary views, i.e.,\narbitrary numbers and positions of viewpoints. It is a challenging and\nrealistic setting for view-based 3D shape recognition. We propose a canonical\nview representation to tackle this challenge. We first transform the original\nfeatures of arbitrary views to a fixed number of view features, dubbed\ncanonical view representation, by aligning the arbitrary view features to a set\nof learnable reference view features using optimal transport. In this way, each\n3D shape with arbitrary views is represented by a fixed number of canonical\nview features, which are further aggregated to generate a rich and robust 3D\nshape representation for shape recognition. We also propose a canonical view\nfeature separation constraint to enforce that the view features in canonical\nview representation can be embedded into scattered points in a Euclidean space.\nExperiments on the ModelNet40, ScanObjectNN, and RGBD datasets show that our\nmethod achieves competitive results under the fixed viewpoint settings, and\nsignificantly outperforms the applicable methods under the arbitrary view\nsetting.",
          "link": "http://arxiv.org/abs/2108.07084",
          "publishedOn": "2021-08-17T01:54:50.448Z",
          "wordCount": 618,
          "title": "Learning Canonical View Representation for 3D Shape Recognition with Arbitrary Views. (arXiv:2108.07084v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1905.10748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengchu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhumade_H/0/1/0/all/0/1\">Hesham Alhumade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Die Hu</a>",
          "description": "Typical adversarial-training-based unsupervised domain adaptation methods are\nvulnerable when the source and target datasets are highly-complex or exhibit a\nlarge discrepancy between their data distributions. Recently, several\nLipschitz-constraint-based methods have been explored. The satisfaction of\nLipschitz continuity guarantees a remarkable performance on a target domain.\nHowever, they lack a mathematical analysis of why a Lipschitz constraint is\nbeneficial to unsupervised domain adaptation and usually perform poorly on\nlarge-scale datasets. In this paper, we take the principle of utilizing a\nLipschitz constraint further by discussing how it affects the error bound of\nunsupervised domain adaptation. A connection between them is built and an\nillustration of how Lipschitzness reduces the error bound is presented. A\n\\textbf{local smooth discrepancy} is defined to measure Lipschitzness of a\ntarget distribution in a pointwise way. When constructing a deep end-to-end\nmodel, to ensure the effectiveness and stability of unsupervised domain\nadaptation, three critical factors are considered in our proposed optimization\nstrategy, i.e., the sample amount of a target domain, dimension and batchsize\nof samples. Experimental results demonstrate that our model performs well on\nseveral standard benchmarks. Our ablation study shows that the sample amount of\na target domain, the dimension and batchsize of samples indeed greatly impact\nLipschitz-constraint-based methods' ability to handle large-scale datasets.\nCode is available at https://github.com/CuthbertCai/SRDA.",
          "link": "http://arxiv.org/abs/1905.10748",
          "publishedOn": "2021-08-17T01:54:50.442Z",
          "wordCount": 715,
          "title": "Learning Smooth Representation for Unsupervised Domain Adaptation. (arXiv:1905.10748v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "We introduce a novel dataset for architectural style classification,\nconsisting of 9,485 images of church buildings. Both images and style labels\nwere sourced from Wikipedia. The dataset can serve as a benchmark for various\nresearch fields, as it combines numerous real-world challenges: fine-grained\ndistinctions between classes based on subtle visual features, a comparatively\nsmall sample size, a highly imbalanced class distribution, a high variance of\nviewpoints, and a hierarchical organization of labels, where only some images\nare labeled at the most precise level. In addition, we provide 631 bounding box\nannotations of characteristic visual features for 139 churches from four major\ncategories. These annotations can, for example, be useful for research on\nfine-grained classification, where additional expert knowledge about\ndistinctive object parts is often available. Images and annotations are\navailable at: https://doi.org/10.5281/zenodo.5166987",
          "link": "http://arxiv.org/abs/2108.06959",
          "publishedOn": "2021-08-17T01:54:50.427Z",
          "wordCount": 580,
          "title": "WikiChurches: A Fine-Grained Dataset of Architectural Styles with Real-World Challenges. (arXiv:2108.06959v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "Since the Lipschitz properties of convolutional neural networks (CNNs) are\nwidely considered to be related to adversarial robustness, we theoretically\ncharacterize the $\\ell_1$ norm and $\\ell_\\infty$ norm of 2D multi-channel\nconvolutional layers and provide efficient methods to compute the exact\n$\\ell_1$ norm and $\\ell_\\infty$ norm. Based on our theorem, we propose a novel\nregularization method termed norm decay, which can effectively reduce the norms\nof convolutional layers and fully-connected layers. Experiments show that\nnorm-regularization methods, including norm decay, weight decay, and singular\nvalue clipping, can improve generalization of CNNs. However, they can slightly\nhurt adversarial robustness. Observing this unexpected phenomenon, we compute\nthe norms of layers in the CNNs trained with three different adversarial\ntraining frameworks and surprisingly find that adversarially robust CNNs have\ncomparable or even larger layer norms than their non-adversarially robust\ncounterparts. Furthermore, we prove that under a mild assumption, adversarially\nrobust classifiers can be achieved using neural networks, and an adversarially\nrobust neural network can have an arbitrarily large Lipschitz constant. For\nthis reason, enforcing small norms on CNN layers may be neither necessary nor\neffective in achieving adversarial robustness. The code is available at\nhttps://github.com/youweiliang/norm_robustness.",
          "link": "http://arxiv.org/abs/2009.08435",
          "publishedOn": "2021-08-17T01:54:50.409Z",
          "wordCount": 710,
          "title": "Large Norms of CNN Layers Do Not Hurt Adversarial Robustness. (arXiv:2009.08435v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1\">Man M. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raake_A/0/1/0/all/0/1\">Alexander Raake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjia Zhou</a>",
          "description": "Recent colorization works implicitly predict the semantic information while\nlearning to colorize black-and-white images. Consequently, the generated color\nis easier to be overflowed, and the semantic faults are invisible. As a human\nexperience in colorization, our brains first detect and recognize the objects\nin the photo, then imagine their plausible colors based on many similar objects\nwe have seen in real life, and finally colorize them, as described in the\nteaser. In this study, we simulate that human-like action to let our network\nfirst learn to understand the photo, then colorize it. Thus, our work can\nprovide plausible colors at a semantic level. Plus, the semantic information of\nthe learned model becomes understandable and able to interact. Additionally, we\nalso prove that Instance Normalization is also a missing ingredient for\ncolorization, then re-design the inference flow of U-Net to have two streams of\ndata, providing an appropriate way of normalizing the feature maps from the\nblack-and-white image and its semantic map. As a result, our network can\nprovide plausible colors competitive to the typical colorization works for\nspecific objects.",
          "link": "http://arxiv.org/abs/2006.07587",
          "publishedOn": "2021-08-17T01:54:50.397Z",
          "wordCount": 649,
          "title": "Semantic-driven Colorization. (arXiv:2006.07587v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Groisser_B/0/1/0/all/0/1\">Benjamin Groisser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_A/0/1/0/all/0/1\">Alon Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1\">Ron Kimmel</a>",
          "description": "The proliferation of 3D scanning technology has driven a need for methods to\ninterpret geometric data, particularly for human subjects. In this paper we\npropose an elegant fusion of regression (bottom-up) and generative (top-down)\nmethods to fit a parametric template model to raw scan meshes.\n\nOur first major contribution is an intrinsic convolutional mesh U-net\narchitecture that predicts pointwise correspondence to a template surface.\nSoft-correspondence is formulated as coordinates in a newly-constructed\nCartesian space. Modeling correspondence as Euclidean proximity enables\nefficient optimization, both for network training and for the next step of the\nalgorithm.\n\nOur second contribution is a generative optimization algorithm that uses the\nU-net correspondence predictions to guide a parametric Iterative Closest Point\nregistration. By employing pre-trained human surface parametric models we\nmaximally leverage domain-specific prior knowledge.\n\nThe pairing of a mesh-convolutional network with generative model fitting\nenables us to predict correspondence for real human surface scans including\nocclusions, partialities, and varying genus (e.g. from self-contact). We\nevaluate the proposed method on the FAUST correspondence challenge where we\nachieve 20% (33%) improvement over state of the art methods for inter- (intra-)\nsubject correspondence.",
          "link": "http://arxiv.org/abs/2108.06695",
          "publishedOn": "2021-08-17T01:54:50.373Z",
          "wordCount": 618,
          "title": "U-mesh: Human Correspondence Matching with Mesh Convolutional Networks. (arXiv:2108.06695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bowren_J/0/1/0/all/0/1\">Joshua Bowren</a>",
          "description": "Neural networks, specifically deep convolutional neural networks, have\nachieved unprecedented performance in various computer vision tasks, but the\nrationale for the computations and structures of successful neural networks is\nnot fully understood. Theories abound for the aptitude of convolutional neural\nnetworks for image classification, but less is understood about why such models\nwould be capable of complex visual tasks such as inference and anomaly\nidentification. Here, we propose a sparse coding interpretation of neural\nnetworks that have ReLU activation and of convolutional neural networks in\nparticular. In sparse coding, when the model's basis functions are assumed to\nbe orthogonal, the optimal coefficients are given by the soft-threshold\nfunction of the basis functions projected onto the input image. In a\nnon-negative variant of sparse coding, the soft-threshold function becomes a\nReLU. Here, we derive these solutions via sparse coding with orthogonal-assumed\nbasis functions, then we derive the convolutional neural network forward\ntransformation from a modified non-negative orthogonal sparse coding model with\nan exponential prior parameter for each sparse coding coefficient. Next, we\nderive a complete convolutional neural network without normalization and\npooling by adding logistic regression to a hierarchical sparse coding model.\nFinally we motivate potentially more robust forward transformations by\nmaintaining sparse priors in convolutional neural networks as well performing a\nstronger nonlinear transformation.",
          "link": "http://arxiv.org/abs/2108.06622",
          "publishedOn": "2021-08-17T01:54:50.368Z",
          "wordCount": 647,
          "title": "A Sparse Coding Interpretation of Neural Networks and Theoretical Implications. (arXiv:2108.06622v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinglin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>",
          "description": "Although current face manipulation techniques achieve impressive performance\nregarding quality and controllability, they are struggling to generate temporal\ncoherent face videos. In this work, we explore to take full advantage of the\ntemporal coherence for video face forgery detection. To achieve this, we\npropose a novel end-to-end framework, which consists of two major stages. The\nfirst stage is a fully temporal convolution network (FTCN). The key insight of\nFTCN is to reduce the spatial convolution kernel size to 1, while maintaining\nthe temporal convolution kernel size unchanged. We surprisingly find this\nspecial design can benefit the model for extracting the temporal features as\nwell as improve the generalization capability. The second stage is a Temporal\nTransformer network, which aims to explore the long-term temporal coherence.\nThe proposed framework is general and flexible, which can be directly trained\nfrom scratch without any pre-training models or external datasets. Extensive\nexperiments show that our framework outperforms existing methods and remains\neffective when applied to detect new sorts of face forgery videos.",
          "link": "http://arxiv.org/abs/2108.06693",
          "publishedOn": "2021-08-17T01:54:50.363Z",
          "wordCount": 615,
          "title": "Exploring Temporal Coherence for More General Video Face Forgery Detection. (arXiv:2108.06693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>",
          "description": "Object proposals have become an integral preprocessing steps of many vision\npipelines including object detection, weakly supervised detection, object\ndiscovery, tracking, etc. Compared to the learning-free methods, learning-based\nproposals have become popular recently due to the growing interest in object\ndetection. The common paradigm is to learn object proposals from data labeled\nwith a set of object regions and their corresponding categories. However, this\napproach often struggles with novel objects in the open world that are absent\nin the training set. In this paper, we identify that the problem is that the\nbinary classifiers in existing proposal methods tend to overfit to the training\ncategories. Therefore, we propose a classification-free Object Localization\nNetwork (OLN) which estimates the objectness of each region purely by how well\nthe location and shape of a region overlap with any ground-truth object (e.g.,\ncenterness and IoU). This simple strategy learns generalizable objectness and\noutperforms existing proposals on cross-category generalization on COCO, as\nwell as cross-dataset evaluation on RoboNet, Object365, and EpicKitchens.\nFinally, we demonstrate the merit of OLN for long-tail object detection on\nlarge vocabulary dataset, LVIS, where we notice clear improvement in rare and\ncommon categories.",
          "link": "http://arxiv.org/abs/2108.06753",
          "publishedOn": "2021-08-17T01:54:50.357Z",
          "wordCount": 630,
          "title": "Learning Open-World Object Proposals without Learning to Classify. (arXiv:2108.06753v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linjiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "As a challenging task of high-level video understanding, weakly supervised\ntemporal action localization has been attracting increasing attention. With\nonly video annotations, most existing methods seek to handle this task with a\nlocalization-by-classification framework, which generally adopts a selector to\nselect snippets of high probabilities of actions or namely the foreground.\nNevertheless, the existing foreground selection strategies have a major\nlimitation of only considering the unilateral relation from foreground to\nactions, which cannot guarantee the foreground-action consistency. In this\npaper, we present a framework named FAC-Net based on the I3D backbone, on which\nthree branches are appended, named class-wise foreground classification branch,\nclass-agnostic attention branch and multiple instance learning branch. First,\nour class-wise foreground classification branch regularizes the relation\nbetween actions and foreground to maximize the foreground-background\nseparation. Besides, the class-agnostic attention branch and multiple instance\nlearning branch are adopted to regularize the foreground-action consistency and\nhelp to learn a meaningful foreground classifier. Within each branch, we\nintroduce a hybrid attention mechanism, which calculates multiple attention\nscores for each snippet, to focus on both discriminative and\nless-discriminative snippets to capture the full action boundaries.\nExperimental results on THUMOS14 and ActivityNet1.3 demonstrate the\nstate-of-the-art performance of our method. Our code is available at\nhttps://github.com/LeonHLJ/FAC-Net.",
          "link": "http://arxiv.org/abs/2108.06524",
          "publishedOn": "2021-08-17T01:54:50.351Z",
          "wordCount": 653,
          "title": "Foreground-Action Consistency Network for Weakly Supervised Temporal Action Localization. (arXiv:2108.06524v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1\">Taeyeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyenson_B/0/1/0/all/0/1\">Benjamin Pyenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebig_J/0/1/0/all/0/1\">Juergen Liebig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>",
          "description": "Biology is both an important application area and a source of motivation for\ndevelopment of advanced machine learning techniques. Although much attention\nhas been paid to large and complex data sets resulting from high-throughput\nsequencing, advances in high-quality video recording technology have begun to\ngenerate similarly rich data sets requiring sophisticated techniques from both\ncomputer vision and time-series analysis. Moreover, just as studying gene\nexpression patterns in one organism can reveal general principles that apply to\nother organisms, the study of complex social interactions in an experimentally\ntractable model system, such as a laboratory ant colony, can provide general\nprinciples about the dynamics of other social groups. Here, we focus on one\nsuch example from the study of reproductive regulation in small laboratory\ncolonies of more than 50 Harpegnathos ants. These ants can be artificially\ninduced to begin a ~20 day process of hierarchy reformation. Although the\nconclusion of this process is conspicuous to a human observer, it remains\nunclear which behaviors during the transient period are contributing to the\nprocess. To address this issue, we explore the potential application of\nOne-class Classification (OC) to the detection of abnormal states in ant\ncolonies for which behavioral data is only available for the normal societal\nconditions during training. Specifically, we build upon the Deep Support Vector\nData Description (DSVDD) and introduce the Inner-Outlier Generator (IO-GEN)\nthat synthesizes fake \"inner outlier\" observations during training that are\nnear the center of the DSVDD data description. We show that IO-GEN increases\nthe reliability of the final OC classifier relative to other DSVDD baselines.\nThis method can be used to screen video frames for which additional human\nobservation is needed.",
          "link": "http://arxiv.org/abs/2009.08626",
          "publishedOn": "2021-08-17T01:54:50.324Z",
          "wordCount": 801,
          "title": "Identification of Abnormal States in Videos of Ants Undergoing Social Phase Change. (arXiv:2009.08626v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.08396",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lablanche_S/0/1/0/all/0/1\">Sebastien Lablanche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lablanche_G/0/1/0/all/0/1\">Gerard Lablanche</a>",
          "description": "In this paper we explain a process of super-resolution reconstruction\nallowing to increase the resolution of an image.The need for high-resolution\ndigital images exists in diverse domains, for example the medical and spatial\ndomains. The obtaining of high-resolution digital images can be made at the\ntime of the shooting, but it is often synonymic of important costs because of\nthe necessary material to avoid such costs, it is known how to use methods of\nsuper-resolution reconstruction, consisting from one or several low resolution\nimages to obtain a high-resolution image. The american patent US 9208537\ndescribes such an algorithm. A zone of one low-resolution image is isolated and\ncategorized according to the information contained in pixels forming the\nborders of the zone. The category of it zone determines the type of\ninterpolation used to add pixels in aforementioned zone, to increase the\nneatness of the images. It is also known how to reconstruct a low-resolution\nimage there high-resolution image by using a model of super-resolution\nreconstruction whose learning is based on networks of neurons and on image or a\npicture library. The demand of chinese patent CN 107563965 and the scientist\npublication \"Pixel Recursive Super Resolution\", R. Dahl, M. Norouzi, J. Shlens\npropose such methods. The aim of this paper is to demonstrate that it is\npossible to reconstruct coherent human faces from very degraded pixelated\nimages with a very fast algorithm, more faster than compressed sensing (CS),\neasier to compute and without deep learning, so without important technology\nresources, i.e. a large database of thousands training images (see\narXiv:2003.13063).\n\nThis technological breakthrough has been patented in 2018 with the demand of\nFrench patent FR 1855485 (https://patents.google.com/patent/FR3082980A1, see\nthe HAL reference https://hal.archives-ouvertes.fr/hal-01875898v1).",
          "link": "http://arxiv.org/abs/1904.08396",
          "publishedOn": "2021-08-17T01:54:50.319Z",
          "wordCount": 794,
          "title": "Process of image super-resolution. (arXiv:1904.08396v8 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.06022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xudong Li</a>",
          "description": "To discover powerful yet compact models is an important goal of neural\narchitecture search. Previous two-stage one-shot approaches are limited by\nsearch space with a fixed depth. It seems handy to include an additional skip\nconnection in the search space to make depths variable. However, it creates a\nlarge range of perturbation during supernet training and it has difficulty\ngiving a confident ranking for subnetworks. In this paper, we discover that\nskip connections bring about significant feature inconsistency compared with\nother operations, which potentially degrades the supernet performance. Based on\nthis observation, we tackle the problem by imposing an equivariant learnable\nstabilizer to homogenize such disparities. Experiments show that our proposed\nstabilizer helps to improve the supernet's convergence as well as ranking\nperformance. With an evolutionary search backend that incorporates the\nstabilized supernet as an evaluator, we derive a family of state-of-the-art\narchitectures, the SCARLET series of several depths, especially SCARLET-A\nobtains 76.9% top-1 accuracy on ImageNet. Code is available at\nhttps://github.com/xiaomi-automl/ScarletNAS.",
          "link": "http://arxiv.org/abs/1908.06022",
          "publishedOn": "2021-08-17T01:54:50.313Z",
          "wordCount": 688,
          "title": "SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search. (arXiv:1908.06022v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasu_M/0/1/0/all/0/1\">Malolan Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abreu_N/0/1/0/all/0/1\">Nelson Abreu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasquez_R/0/1/0/all/0/1\">Raysa V&#xe1;squez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">Christian L&#xf3;pez</a>",
          "description": "Vehicle counting systems can help with vehicle analysis and traffic incident\ndetection. Unfortunately, most existing methods require some level of human\ninput to identify the Region of interest (ROI), movements of interest, or to\nestablish a reference point or line to count vehicles from traffic cameras.\nThis work introduces a method to count vehicles from traffic videos that\nautomatically identifies the ROI for the camera, as well as the driving\ntrajectories of the vehicles. This makes the method feasible to use with\nPan-Tilt-Zoom cameras, which are frequently used in developing countries.\nPreliminary results indicate that the proposed method achieves an average\nintersection over the union of 57.05% for the ROI and a mean absolute error of\njust 17.44% at counting vehicles of the traffic video cameras tested.",
          "link": "http://arxiv.org/abs/2108.07135",
          "publishedOn": "2021-08-17T01:54:50.301Z",
          "wordCount": 578,
          "title": "Vehicle-counting with Automatic Region-of-Interest and Driving-Trajectory detection. (arXiv:2108.07135v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinghong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shangxuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yudong Chen</a>",
          "description": "Hashing technology has been widely used in image retrieval due to its\ncomputational and storage efficiency. Recently, deep unsupervised hashing\nmethods have attracted increasing attention due to the high cost of human\nannotations in the real world and the superiority of deep learning technology.\nHowever, most deep unsupervised hashing methods usually pre-compute a\nsimilarity matrix to model the pairwise relationship in the pre-trained feature\nspace. Then this similarity matrix would be used to guide hash learning, in\nwhich most of the data pairs are treated equivalently. The above process is\nconfronted with the following defects: 1) The pre-computed similarity matrix is\ninalterable and disconnected from the hash learning process, which cannot\nexplore the underlying semantic information. 2) The informative data pairs may\nbe buried by the large number of less-informative data pairs. To solve the\naforementioned problems, we propose a \\textbf{Deep Self-Adaptive\nHashing~(DSAH)} model to adaptively capture the semantic information with two\nspecial designs: \\textbf{Adaptive Neighbor Discovery~(AND)} and\n\\textbf{Pairwise Information Content~(PIC)}. Firstly, we adopt the AND to\ninitially construct a neighborhood-based similarity matrix, and then refine\nthis initial similarity matrix with a novel update strategy to further\ninvestigate the semantic structure behind the learned representation. Secondly,\nwe measure the priorities of data pairs with PIC and assign adaptive weights to\nthem, which is relies on the assumption that more dissimilar data pairs contain\nmore discriminative information for hash learning. Extensive experiments on\nseveral benchmark datasets demonstrate that the above two technologies\nfacilitate the deep hashing model to achieve superior performance in a\nself-adaptive manner.",
          "link": "http://arxiv.org/abs/2108.07094",
          "publishedOn": "2021-08-17T01:54:50.295Z",
          "wordCount": 698,
          "title": "Deep Self-Adaptive Hashing for Image Retrieval. (arXiv:2108.07094v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.09843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Weijun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaohu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Rongxing Lu</a>",
          "description": "Although federated learning improves privacy of training data by exchanging\nlocal gradients or parameters rather than raw data, the adversary still can\nleverage local gradients and parameters to obtain local training data by\nlaunching reconstruction and membership inference attacks. To defend such\nprivacy attacks, many noises perturbation methods (like differential privacy or\nCountSketch matrix) have been widely designed. However, the strong defence\nability and high learning accuracy of these schemes cannot be ensured at the\nsame time, which will impede the wide application of FL in practice (especially\nfor medical or financial institutions that require both high accuracy and\nstrong privacy guarantee). To overcome this issue, in this paper, we propose\n\\emph{an efficient model perturbation method for federated learning} to defend\nreconstruction and membership inference attacks launched by curious clients. On\nthe one hand, similar to the differential privacy, our method also selects\nrandom numbers as perturbed noises added to the global model parameters, and\nthus it is very efficient and easy to be integrated in practice. Meanwhile, the\nrandom selected noises are positive real numbers and the corresponding value\ncan be arbitrarily large, and thus the strong defence ability can be ensured.\nOn the other hand, unlike differential privacy or other perturbation methods\nthat cannot eliminate the added noises, our method allows the server to recover\nthe true gradients by eliminating the added noises. Therefore, our method does\nnot hinder learning accuracy at all.",
          "link": "http://arxiv.org/abs/2002.09843",
          "publishedOn": "2021-08-17T01:54:50.278Z",
          "wordCount": 750,
          "title": "An Accuracy-Lossless Perturbation Method for Defending Privacy Attacks in Federated Learning. (arXiv:2002.09843v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrzanowski_W/0/1/0/all/0/1\">Wojciech Chrzanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>",
          "description": "Automatic 3D neuron reconstruction is critical for analysing the morphology\nand functionality of neurons in brain circuit activities. However, the\nperformance of existing tracing algorithms is hinged by the low image quality.\nRecently, a series of deep learning based segmentation methods have been\nproposed to improve the quality of raw 3D optical image stacks by removing\nnoises and restoring neuronal structures from low-contrast background. Due to\nthe variety of neuron morphology and the lack of large neuron datasets, most of\ncurrent neuron segmentation models rely on introducing complex and\nspecially-designed submodules to a base architecture with the aim of encoding\nbetter feature representations. Though successful, extra burden would be put on\ncomputation during inference. Therefore, rather than modifying the base\nnetwork, we shift our focus to the dataset itself. The encoder-decoder backbone\nused in most neuron segmentation models attends only intra-volume voxel points\nto learn structural features of neurons but neglect the shared intrinsic\nsemantic features of voxels belonging to the same category among different\nvolumes, which is also important for expressive representation learning. Hence,\nto better utilise the scarce dataset, we propose to explicitly exploit such\nintrinsic features of voxels through a novel voxel-level cross-volume\nrepresentation learning paradigm on the basis of an encoder-decoder\nsegmentation model. Our method introduces no extra cost during inference.\nEvaluated on 42 3D neuron images from BigNeuron project, our proposed method is\ndemonstrated to improve the learning ability of the original segmentation model\nand further enhancing the reconstruction performance.",
          "link": "http://arxiv.org/abs/2108.06522",
          "publishedOn": "2021-08-17T01:54:50.271Z",
          "wordCount": 697,
          "title": "Voxel-wise Cross-Volume Representation Learning for 3D Neuron Reconstruction. (arXiv:2108.06522v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hanbyul Joo</a>",
          "description": "Most existing monocular 3D pose estimation approaches only focus on a single\nbody part, neglecting the fact that the essential nuance of human motion is\nconveyed through a concert of subtle movements of face, hands, and body. In\nthis paper, we present FrankMocap, a fast and accurate whole-body 3D pose\nestimation system that can produce 3D face, hands, and body simultaneously from\nin-the-wild monocular images. The core idea of FrankMocap is its modular\ndesign: We first run 3D pose regression methods for face, hands, and body\nindependently, followed by composing the regression outputs via an integration\nmodule. The separate regression modules allow us to take full advantage of\ntheir state-of-the-art performances without compromising the original accuracy\nand reliability in practice. We develop three different integration modules\nthat trade off between latency and accuracy. All of them are capable of\nproviding simple yet effective solutions to unify the separate outputs into\nseamless whole-body pose estimation results. We quantitatively and\nqualitatively demonstrate that our modularized system outperforms both the\noptimization-based and end-to-end methods of estimating whole-body pose.",
          "link": "http://arxiv.org/abs/2108.06428",
          "publishedOn": "2021-08-17T01:54:50.261Z",
          "wordCount": 643,
          "title": "FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration. (arXiv:2108.06428v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mac_B/0/1/0/all/0/1\">Brandon Mac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moody_A/0/1/0/all/0/1\">Alan R. Moody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_A/0/1/0/all/0/1\">April Khademi</a>",
          "description": "One of the key limitations in machine learning models is poor performance on\ndata that is out of the domain of the training distribution. This is especially\ntrue for image analysis in magnetic resonance (MR) imaging, as variations in\nhardware and software create non-standard intensities, contrasts, and noise\ndistributions across scanners. Recently, image translation models have been\nproposed to augment data across domains to create synthetic data points. In\nthis paper, we investigate the application an unsupervised image translation\nmodel to augment MR images from a source dataset to a target dataset.\nSpecifically, we want to evaluate how well these models can create synthetic\ndata points representative of the target dataset through image translation, and\nto see if a segmentation model trained these synthetic data points would\napproach the performance of a model trained directly on the target dataset. We\nconsider three configurations of augmentation between datasets consisting of\ntranslation between images, between scanner vendors, and from labels to images.\nIt was found that the segmentation models trained on synthetic data from labels\nto images configuration yielded the closest performance to the segmentation\nmodel trained directly on the target dataset. The Dice coeffcient score per\neach target vendor (GE, Siemens, Philips) for training on synthetic data was\n0.63, 0.64, and 0.58, compared to training directly on target dataset was 0.65,\n0.72, and 0.61.",
          "link": "http://arxiv.org/abs/2108.06434",
          "publishedOn": "2021-08-17T01:54:50.237Z",
          "wordCount": 660,
          "title": "Adapting to Unseen Vendor Domains for MRI Lesion Segmentation. (arXiv:2108.06434v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>",
          "description": "Generating conversational gestures from speech audio is challenging due to\nthe inherent one-to-many mapping between audio and body motions. Conventional\nCNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of\nall possible target motions, resulting in plain/boring motions during\ninference. In order to overcome this problem, we propose a novel conditional\nvariational autoencoder (VAE) that explicitly models one-to-many\naudio-to-motion mapping by splitting the cross-modal latent code into shared\ncode and motion-specific code. The shared code mainly models the strong\ncorrelation between audio and motion (such as the synchronized audio and motion\nbeats), while the motion-specific code captures diverse motion information\nindependent of the audio. However, splitting the latent code into two parts\nposes training difficulties for the VAE model. A mapping network facilitating\nrandom sampling along with other techniques including relaxed motion loss,\nbicycle constraint, and diversity loss are designed to better train the VAE.\nExperiments on both 3D and 2D motion datasets verify that our method generates\nmore realistic and diverse motions than state-of-the-art methods,\nquantitatively and qualitatively. Finally, we demonstrate that our method can\nbe readily used to generate motion sequences with user-specified motion clips\non the timeline. Code and more results are at\nhttps://jingli513.github.io/audio2gestures.",
          "link": "http://arxiv.org/abs/2108.06720",
          "publishedOn": "2021-08-17T01:54:50.224Z",
          "wordCount": 648,
          "title": "Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders. (arXiv:2108.06720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zeyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_P/0/1/0/all/0/1\">Pierce KH Chow</a>",
          "description": "Accurate automatic liver and tumor segmentation plays a vital role in\ntreatment planning and disease monitoring. Recently, deep convolutional neural\nnetwork (DCNNs) has obtained tremendous success in 2D and 3D medical image\nsegmentation. However, 2D DCNNs cannot fully leverage the inter-slice\ninformation, while 3D DCNNs are computationally expensive and memory intensive.\nTo address these issues, we first propose a novel dense-sparse training flow\nfrom a data perspective, in which, densely adjacent slices and sparsely\nadjacent slices are extracted as inputs for regularizing DCNNs, thereby\nimproving the model performance. Moreover, we design a 2.5D light-weight\nnnU-Net from a network perspective, in which, depthwise separable convolutions\nare adopted to improve the efficiency. Extensive experiments on the LiTS\ndataset have demonstrated the superiority of the proposed method.",
          "link": "http://arxiv.org/abs/2108.06761",
          "publishedOn": "2021-08-17T01:54:50.216Z",
          "wordCount": 599,
          "title": "Multi-Slice Dense-Sparse Learning for Efficient Liver and Tumor Segmentation. (arXiv:2108.06761v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_B/0/1/0/all/0/1\">Baitan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>",
          "description": "Considering the fact that students have different abilities to understand the\nknowledge imparted by teachers, a multi-granularity distillation mechanism is\nproposed for transferring more understandable knowledge for student networks. A\nmulti-granularity self-analyzing module of the teacher network is designed,\nwhich enables the student network to learn knowledge from different teaching\npatterns. Furthermore, a stable excitation scheme is proposed for robust\nsupervision for the student training. The proposed distillation mechanism can\nbe embedded into different distillation frameworks, which are taken as\nbaselines. Experiments show the mechanism improves the accuracy by 0.58% on\naverage and by 1.08% in the best over the baselines, which makes its\nperformance superior to the state-of-the-arts. It is also exploited that the\nstudent's ability of fine-tuning and robustness to noisy inputs can be improved\nvia the proposed mechanism. The code is available at\nhttps://github.com/shaoeric/multi-granularity-distillation.",
          "link": "http://arxiv.org/abs/2108.06681",
          "publishedOn": "2021-08-17T01:54:50.210Z",
          "wordCount": 568,
          "title": "Multi-granularity for knowledge distillation. (arXiv:2108.06681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06515",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhonglan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Dongqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingfan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qilong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingyu Liu</a>",
          "description": "In industry, there exist plenty of scenarios where old gray photos need to be\nautomatically colored, such as video sites and archives. In this paper, we\npresent the HistoryNet focusing on historical person's diverse high fidelity\nclothing colorization based on fine grained semantic understanding and prior.\nColorization of historical persons is realistic and practical, however,\nexisting methods do not perform well in the regards. In this paper, a\nHistoryNet including three parts, namely, classification, fine grained semantic\nparsing and colorization, is proposed. Classification sub-module supplies\nclassifying of images according to the eras, nationalities and garment types;\nParsing sub-network supplies the semantic for person contours, clothing and\nbackground in the image to achieve more accurate colorization of clothes and\npersons and prevent color overflow. In the training process, we integrate\nclassification and semantic parsing features into the coloring generation\nnetwork to improve colorization. Through the design of classification and\nparsing subnetwork, the accuracy of image colorization can be improved and the\nboundary of each part of image can be more clearly. Moreover, we also propose a\nnovel Modern Historical Movies Dataset (MHMD) containing 1,353,166 images and\n42 labels of eras, nationalities, and garment types for automatic colorization\nfrom 147 historical movies or TV series made in modern time. Various\nquantitative and qualitative comparisons demonstrate that our method\noutperforms the state-of-the-art colorization methods, especially on military\nuniforms, which has correct colors according to the historical literatures.",
          "link": "http://arxiv.org/abs/2108.06515",
          "publishedOn": "2021-08-17T01:54:50.203Z",
          "wordCount": 694,
          "title": "Focusing on Persons: Colorizing Old Images Learning from Modern Historical Movies. (arXiv:2108.06515v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_X/0/1/0/all/0/1\">Xin Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>",
          "description": "Unsupervised representation learning has achieved outstanding performances\nusing centralized data available on the Internet. However, the increasing\nawareness of privacy protection limits sharing of decentralized unlabeled image\ndata that grows explosively in multiple parties (e.g., mobile phones and\ncameras). As such, a natural problem is how to leverage these data to learn\nvisual representations for downstream tasks while preserving data privacy. To\naddress this problem, we propose a novel federated unsupervised learning\nframework, FedU. In this framework, each party trains models from unlabeled\ndata independently using contrastive learning with an online network and a\ntarget network. Then, a central server aggregates trained models and updates\nclients' models with the aggregated model. It preserves data privacy as each\nparty only has access to its raw data. Decentralized data among multiple\nparties are normally non-independent and identically distributed (non-IID),\nleading to performance degradation. To tackle this challenge, we propose two\nsimple but effective methods: 1) We design the communication protocol to upload\nonly the encoders of online networks for server aggregation and update them\nwith the aggregated encoder; 2) We introduce a new module to dynamically decide\nhow to update predictors based on the divergence caused by non-IID. The\npredictor is the other component of the online network. Extensive experiments\nand ablations demonstrate the effectiveness and significance of FedU. It\noutperforms training with only one party by over 5% and other methods by over\n14% in linear and semi-supervised evaluation on non-IID data.",
          "link": "http://arxiv.org/abs/2108.06492",
          "publishedOn": "2021-08-17T01:54:50.196Z",
          "wordCount": 696,
          "title": "Collaborative Unsupervised Visual Representation Learning from Decentralized Data. (arXiv:2108.06492v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Changwoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hojun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>",
          "description": "We present PICCOLO, a simple and efficient algorithm for omnidirectional\nlocalization. Given a colored point cloud and a 360 panorama image of a scene,\nour objective is to recover the camera pose at which the panorama image is\ntaken. Our pipeline works in an off-the-shelf manner with a single image given\nas a query and does not require any training of neural networks or collecting\nground-truth poses of images. Instead, we match each point cloud color to the\nholistic view of the panorama image with gradient-descent optimization to find\nthe camera pose. Our loss function, called sampling loss, is point\ncloud-centric, evaluated at the projected location of every point in the point\ncloud. In contrast, conventional photometric loss is image-centric, comparing\ncolors at each pixel location. With a simple change in the compared entities,\nsampling loss effectively overcomes the severe visual distortion of\nomnidirectional images, and enjoys the global context of the 360 view to handle\nchallenging scenarios for visual localization. PICCOLO outperforms existing\nomnidirectional localization algorithms in both accuracy and stability when\nevaluated in various environments.",
          "link": "http://arxiv.org/abs/2108.06545",
          "publishedOn": "2021-08-17T01:54:50.190Z",
          "wordCount": 612,
          "title": "PICCOLO: Point Cloud-Centric Omnidirectional Localization. (arXiv:2108.06545v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1\">Or Bar-Shira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "In this chapter, we review biomedical applications and breakthroughs via\nleveraging algorithm unrolling, an important technique that bridges between\ntraditional iterative algorithms and modern deep learning techniques. To\nprovide context, we start by tracing the origin of algorithm unrolling and\nproviding a comprehensive tutorial on how to unroll iterative algorithms into\ndeep networks. We then extensively cover algorithm unrolling in a wide variety\nof biomedical imaging modalities and delve into several representative recent\nworks in detail. Indeed, there is a rich history of iterative algorithms for\nbiomedical image synthesis, which makes the field ripe for unrolling\ntechniques. In addition, we put algorithm unrolling into a broad perspective,\nin order to understand why it is particularly effective and discuss recent\ntrends. Finally, we conclude the chapter by discussing open challenges, and\nsuggesting future research directions.",
          "link": "http://arxiv.org/abs/2108.06637",
          "publishedOn": "2021-08-17T01:54:50.169Z",
          "wordCount": 566,
          "title": "Deep Algorithm Unrolling for Biomedical Imaging. (arXiv:2108.06637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Video captioning targets interpreting the complex visual contents as text\ndescriptions, which requires the model to fully understand video scenes\nincluding objects and their interactions. Prevailing methods adopt\noff-the-shelf object detection networks to give object proposals and use the\nattention mechanism to model the relations between objects. They often miss\nsome undefined semantic concepts of the pretrained model and fail to identify\nexact predicate relationships between objects. In this paper, we investigate an\nopen research task of generating text descriptions for the given videos, and\npropose Cross-Modal Graph (CMG) with meta concepts for video captioning.\nSpecifically, to cover the useful semantic concepts in video captions, we\nweakly learn the corresponding visual regions for text descriptions, where the\nassociated visual regions and textual words are named cross-modal meta\nconcepts. We further build meta concept graphs dynamically with the learned\ncross-modal meta concepts. We also construct holistic video-level and local\nframe-level video graphs with the predicted predicates to model video sequence\nstructures. We validate the efficacy of our proposed techniques with extensive\nexperiments and achieve state-of-the-art results on two public datasets.",
          "link": "http://arxiv.org/abs/2108.06458",
          "publishedOn": "2021-08-17T01:54:50.152Z",
          "wordCount": 617,
          "title": "Cross-Modal Graph with Meta Concepts for Video Captioning. (arXiv:2108.06458v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1\">Vinod Kumar Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sukhdeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>",
          "description": "Handwritten character recognition (HCR) is a challenging learning problem in\npattern recognition, mainly due to similarity in structure of characters,\ndifferent handwriting styles, noisy datasets and a large variety of languages\nand scripts. HCR problem is studied extensively for a few decades but there is\nvery limited research on script independent models. This is because of factors,\nlike, diversity of scripts, focus of the most of conventional research efforts\non handcrafted feature extraction techniques which are language/script specific\nand are not always available, and unavailability of public datasets and codes\nto reproduce the results. On the other hand, deep learning has witnessed huge\nsuccess in different areas of pattern recognition, including HCR, and provides\nend-to-end learning, i.e., automated feature extraction and recognition. In\nthis paper, we have proposed a novel deep learning architecture which exploits\ntransfer learning and image-augmentation for end-to-end learning for script\nindependent handwritten character recognition, called HCR-Net. The network is\nbased on a novel transfer learning approach for HCR, where some of lower layers\nof a pre-trained VGG16 network are utilised. Due to transfer learning and\nimage-augmentation, HCR-Net provides faster training, better performance and\nbetter generalisations. The experimental results on publicly available datasets\nof Bangla, Punjabi, Hindi, English, Swedish, Urdu, Farsi, Tibetan, Kannada,\nMalayalam, Telugu, Marathi, Nepali and Arabic languages prove the efficacy of\nHCR-Net and establishes several new benchmarks. For reproducibility of the\nresults and for the advancements of the HCR research, complete code is publicly\nreleased at \\href{https://github.com/jmdvinodjmd/HCR-Net}{GitHub}.",
          "link": "http://arxiv.org/abs/2108.06663",
          "publishedOn": "2021-08-17T01:54:50.123Z",
          "wordCount": 704,
          "title": "HCR-Net: A deep learning based script independent handwritten character recognition network. (arXiv:2108.06663v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruohao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Dantong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenbo Li</a>",
          "description": "Most recent transformer-based models show impressive performance on vision\ntasks, even better than Convolution Neural Networks (CNN). In this work, we\npresent a novel, flexible, and effective transformer-based model for\nhigh-quality instance segmentation. The proposed method, Segmenting Objects\nwith TRansformers (SOTR), simplifies the segmentation pipeline, building on an\nalternative CNN backbone appended with two parallel subtasks: (1) predicting\nper-instance category via transformer and (2) dynamically generating\nsegmentation mask with the multi-level upsampling module. SOTR can effectively\nextract lower-level feature representations and capture long-range context\ndependencies by Feature Pyramid Network (FPN) and twin transformer,\nrespectively. Meanwhile, compared with the original transformer, the proposed\ntwin transformer is time- and resource-efficient since only a row and a column\nattention are involved to encode pixels. Moreover, SOTR is easy to be\nincorporated with various CNN backbones and transformer model variants to make\nconsiderable improvements for the segmentation accuracy and training\nconvergence. Extensive experiments show that our SOTR performs well on the MS\nCOCO dataset and surpasses state-of-the-art instance segmentation approaches.\nWe hope our simple but strong framework could serve as a preferment baseline\nfor instance-level recognition. Our code is available at\nhttps://github.com/easton-cau/SOTR.",
          "link": "http://arxiv.org/abs/2108.06747",
          "publishedOn": "2021-08-17T01:54:50.092Z",
          "wordCount": 622,
          "title": "SOTR: Segmenting Objects with Transformers. (arXiv:2108.06747v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Thanh T. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thang V. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tung T. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>",
          "description": "Chest radiograph (CXR) interpretation in pediatric patients is error-prone\nand requires a high level of understanding of radiologic expertise. Recently,\ndeep convolutional neural networks (D-CNNs) have shown remarkable performance\nin interpreting CXR in adults. However, there is a lack of evidence indicating\nthat D-CNNs can recognize accurately multiple lung pathologies from pediatric\nCXR scans. In particular, the development of diagnostic models for the\ndetection of pediatric chest diseases faces significant challenges such as (i)\nlack of physician-annotated datasets and (ii) class imbalance problems. In this\npaper, we retrospectively collect a large dataset of 5,017 pediatric CXR scans,\nfor which each is manually labeled by an experienced radiologist for the\npresence of 10 common pathologies. A D-CNN model is then trained on 3,550\nannotated scans to classify multiple pediatric lung pathologies automatically.\nTo address the high-class imbalance issue, we propose to modify and apply\n\"Distribution-Balanced loss\" for training D-CNNs which reshapes the standard\nBinary-Cross Entropy loss (BCE) to efficiently learn harder samples by\ndown-weighting the loss assigned to the majority classes. On an independent\ntest set of 777 studies, the proposed approach yields an area under the\nreceiver operating characteristic (AUC) of 0.709 (95% CI, 0.690-0.729). The\nsensitivity, specificity, and F1-score at the cutoff value are 0.722\n(0.694-0.750), 0.579 (0.563-0.595), and 0.389 (0.373-0.405), respectively.\nThese results significantly outperform previous state-of-the-art methods on\nmost of the target diseases. Moreover, our ablation studies validate the\neffectiveness of the proposed loss function compared to other standard losses,\ne.g., BCE and Focal Loss, for this learning task. Overall, we demonstrate the\npotential of D-CNNs in interpreting pediatric CXRs.",
          "link": "http://arxiv.org/abs/2108.06486",
          "publishedOn": "2021-08-17T01:54:50.076Z",
          "wordCount": 740,
          "title": "Learning to Automatically Diagnose Multiple Diseases in Pediatric Chest Radiographs Using Deep Convolutional Neural Networks. (arXiv:2108.06486v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuting He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Youyong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaomei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaobo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dillenseger_J/0/1/0/all/0/1\">Jean-Louis Dillenseger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coatrieux_J/0/1/0/all/0/1\">Jean-Louis Coatrieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanyu Yang</a>",
          "description": "Renal compartment segmentation on CT images targets on extracting the 3D\nstructure of renal compartments from abdominal CTA images and is of great\nsignificance to the diagnosis and treatment for kidney diseases. However, due\nto the unclear compartment boundary, thin compartment structure and large\nanatomy variation of 3D kidney CT images, deep-learning based renal compartment\nsegmentation is a challenging task. We propose a novel weakly supervised\nlearning framework, Cycle Prototype Network, for 3D renal compartment\nsegmentation. It has three innovations: 1) A Cycle Prototype Learning (CPL) is\nproposed to learn consistency for generalization. It learns from pseudo labels\nthrough the forward process and learns consistency regularization through the\nreverse process. The two processes make the model robust to noise and\nlabel-efficient. 2) We propose a Bayes Weakly Supervised Module (BWSM) based on\ncross-period prior knowledge. It learns prior knowledge from cross-period\nunlabeled data and perform error correction automatically, thus generates\naccurate pseudo labels. 3) We present a Fine Decoding Feature Extractor (FDFE)\nfor fine-grained feature extraction. It combines global morphology information\nand local detail information to obtain feature maps with sharp detail, so the\nmodel will achieve fine segmentation on thin structures. Our model achieves\nDice of 79.1% and 78.7% with only four labeled images, achieving a significant\nimprovement by about 20% than typical prototype model PANet.",
          "link": "http://arxiv.org/abs/2108.06669",
          "publishedOn": "2021-08-17T01:54:50.063Z",
          "wordCount": 686,
          "title": "CPNet: Cycle Prototype Network for Weakly-supervised 3D Renal Compartments Segmentation on CT Images. (arXiv:2108.06669v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_D/0/1/0/all/0/1\">Dung V. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>",
          "description": "X-ray imaging in DICOM format is the most commonly used imaging modality in\nclinical practice, resulting in vast, non-normalized databases. This leads to\nan obstacle in deploying AI solutions for analyzing medical images, which often\nrequires identifying the right body part before feeding the image into a\nspecified AI model. This challenge raises the need for an automated and\nefficient approach to classifying body parts from X-ray scans. Unfortunately,\nto the best of our knowledge, there is no open tool or framework for this task\nto date. To fill this lack, we introduce a DICOM Imaging Router that deploys\ndeep CNNs for categorizing unknown DICOM X-ray images into five anatomical\ngroups: abdominal, adult chest, pediatric chest, spine, and others. To this\nend, a large-scale X-ray dataset consisting of 16,093 images has been collected\nand manually classified. We then trained a set of state-of-the-art deep CNNs\nusing a training set of 11,263 images. These networks were then evaluated on an\nindependent test set of 2,419 images and showed superior performance in\nclassifying the body parts. Specifically, our best performing model achieved a\nrecall of 0.982 (95% CI, 0.977-0.988), a precision of 0.985 (95% CI,\n0.975-0.989) and a F1-score of 0.981 (95% CI, 0.976-0.987), whilst requiring\nless computation for inference (0.0295 second per image). Our external validity\non 1,000 X-ray images shows the robustness of the proposed approach across\nhospitals. These remarkable performances indicate that deep CNNs can accurately\nand effectively differentiate human body parts from X-ray scans, thereby\nproviding potential benefits for a wide range of applications in clinical\nsettings. The dataset, codes, and trained deep learning models from this study\nwill be made publicly available on our project website at https://vindr.ai/.",
          "link": "http://arxiv.org/abs/2108.06490",
          "publishedOn": "2021-08-17T01:54:50.030Z",
          "wordCount": 756,
          "title": "DICOM Imaging Router: An Open Deep Learning Framework for Classification of Body Parts from DICOM X-ray Scans. (arXiv:2108.06490v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "Facial expression recognition (FER) has emerged as an important component of\nhuman-computer interaction systems. Despite recent advancements in FER,\nperformance often drops significantly for non-frontal facial images. We propose\nContrastive Learning of Multi-view facial Expressions (CL-MEx) to exploit\nfacial images captured simultaneously from different angles towards FER. CL-MEx\nis a two-step training framework. In the first step, an encoder network is\npre-trained with the proposed self-supervised contrastive loss, where it learns\nto generate view-invariant embeddings for different views of a subject. The\nmodel is then fine-tuned with labeled data in a supervised setting. We\ndemonstrate the performance of the proposed method on two multi-view FER\ndatasets, KDEF and DDCF, where state-of-the-art performances are achieved.\nFurther experiments show the robustness of our method in dealing with\nchallenging angles and reduced amounts of labeled data.",
          "link": "http://arxiv.org/abs/2108.06723",
          "publishedOn": "2021-08-17T01:54:49.999Z",
          "wordCount": 580,
          "title": "Self-supervised Contrastive Learning of Multi-view Facial Expressions. (arXiv:2108.06723v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_T/0/1/0/all/0/1\">Takaki Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prugel_Bennett_A/0/1/0/all/0/1\">Adam Pr&#xfc;gel-Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_S/0/1/0/all/0/1\">Stefan B. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizarro_O/0/1/0/all/0/1\">Oscar Pizarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thornton_B/0/1/0/all/0/1\">Blair Thornton</a>",
          "description": "This paper describes Georeference Contrastive Learning of visual\nRepresentation (GeoCLR) for efficient training of deep-learning Convolutional\nNeural Networks (CNNs). The method leverages georeference information by\ngenerating a similar image pair using images taken of nearby locations, and\ncontrasting these with an image pair that is far apart. The underlying\nassumption is that images gathered within a close distance are more likely to\nhave similar visual appearance, where this can be reasonably satisfied in\nseafloor robotic imaging applications where image footprints are limited to\nedge lengths of a few metres and are taken so that they overlap along a\nvehicle's trajectory, whereas seafloor substrates and habitats have patch sizes\nthat are far larger. A key advantage of this method is that it is\nself-supervised and does not require any human input for CNN training. The\nmethod is computationally efficient, where results can be generated between\ndives during multi-day AUV missions using computational resources that would be\naccessible during most oceanic field trials. We apply GeoCLR to habitat\nclassification on a dataset that consists of ~86k images gathered using an\nAutonomous Underwater Vehicle (AUV). We demonstrate how the latent\nrepresentations generated by GeoCLR can be used to efficiently guide human\nannotation efforts, where the semi-supervised framework improves classification\naccuracy by an average of 11.8 % compared to state-of-the-art transfer learning\nusing the same CNN and equivalent number of human annotations for training.",
          "link": "http://arxiv.org/abs/2108.06421",
          "publishedOn": "2021-08-17T01:54:49.985Z",
          "wordCount": 676,
          "title": "GeoCLR: Georeference Contrastive Learning for Efficient Seafloor Image Interpretation. (arXiv:2108.06421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>",
          "description": "Person re-identification (ReID) aims to re-identify a person from\nnon-overlapping camera views. Since person ReID data contains sensitive\npersonal information, researchers have adopted federated learning, an emerging\ndistributed training method, to mitigate the privacy leakage risks. However,\nexisting studies rely on data labels that are laborious and time-consuming to\nobtain. We present FedUReID, a federated unsupervised person ReID system to\nlearn person ReID models without any labels while preserving privacy. FedUReID\nenables in-situ model training on edges with unlabeled data. A cloud server\naggregates models from edges instead of centralizing raw data to preserve data\nprivacy. Moreover, to tackle the problem that edges vary in data volumes and\ndistributions, we personalize training in edges with joint optimization of\ncloud and edge. Specifically, we propose personalized epoch to reassign\ncomputation throughout training, personalized clustering to iteratively predict\nsuitable labels for unlabeled data, and personalized update to adapt the server\naggregated model to each edge. Extensive experiments on eight person ReID\ndatasets demonstrate that FedUReID not only achieves higher accuracy but also\nreduces computation cost by 29%. Our FedUReID system with the joint\noptimization will shed light on implementing federated learning to more\nmultimedia tasks without data labels.",
          "link": "http://arxiv.org/abs/2108.06493",
          "publishedOn": "2021-08-17T01:54:49.979Z",
          "wordCount": 653,
          "title": "Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised Person Re-identification. (arXiv:2108.06493v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dennis Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>",
          "description": "Recent progress in 3D object detection from single images leverages monocular\ndepth estimation as a way to produce 3D pointclouds, turning cameras into\npseudo-lidar sensors. These two-stage detectors improve with the accuracy of\nthe intermediate depth estimation network, which can itself be improved without\nmanual labels via large-scale self-supervised learning. However, they tend to\nsuffer from overfitting more than end-to-end methods, are more complex, and the\ngap with similar lidar-based detectors remains significant. In this work, we\npropose an end-to-end, single stage, monocular 3D object detector, DD3D, that\ncan benefit from depth pre-training like pseudo-lidar methods, but without\ntheir limitations. Our architecture is designed for effective information\ntransfer between depth estimation and 3D detection, allowing us to scale with\nthe amount of unlabeled pre-training data. Our method achieves state-of-the-art\nresults on two challenging benchmarks, with 16.34% and 9.28% AP for Cars and\nPedestrians (respectively) on the KITTI-3D benchmark, and 41.5% mAP on\nNuScenes.",
          "link": "http://arxiv.org/abs/2108.06417",
          "publishedOn": "2021-08-17T01:54:49.973Z",
          "wordCount": 598,
          "title": "Is Pseudo-Lidar needed for Monocular 3D Object detection?. (arXiv:2108.06417v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongbin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiaoyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsui Hin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Huaqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "We present MMOCR-an open-source toolbox which provides a comprehensive\npipeline for text detection and recognition, as well as their downstream tasks\nsuch as named entity recognition and key information extraction. MMOCR\nimplements 14 state-of-the-art algorithms, which is significantly more than all\nthe existing open-source OCR projects we are aware of to date. To facilitate\nfuture research and industrial applications of text recognition-related\nproblems, we also provide a large number of trained models and detailed\nbenchmarks to give insights into the performance of text detection, recognition\nand understanding. MMOCR is publicly released at\nhttps://github.com/open-mmlab/mmocr.",
          "link": "http://arxiv.org/abs/2108.06543",
          "publishedOn": "2021-08-17T01:54:49.951Z",
          "wordCount": 563,
          "title": "MMOCR: A Comprehensive Toolbox for Text Detection, Recognition and Understanding. (arXiv:2108.06543v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarna_A/0/1/0/all/0/1\">Aaron Sarna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1\">Dilip Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maschinot_A/0/1/0/all/0/1\">Aaron Maschinot</a>",
          "description": "Disentangled visual representations have largely been studied with generative\nmodels such as Variational AutoEncoders (VAEs). While prior work has focused on\ngenerative methods for disentangled representation learning, these approaches\ndo not scale to large datasets due to current limitations of generative models.\nInstead, we explore regularization methods with contrastive learning, which\ncould result in disentangled representations that are powerful enough for large\nscale datasets and downstream applications. However, we find that unsupervised\ndisentanglement is difficult to achieve due to optimization and initialization\nsensitivity, with trade-offs in task performance. We evaluate disentanglement\nwith downstream tasks, analyze the benefits and disadvantages of each\nregularization used, and discuss future directions.",
          "link": "http://arxiv.org/abs/2108.06613",
          "publishedOn": "2021-08-17T01:54:49.945Z",
          "wordCount": 559,
          "title": "Unsupervised Disentanglement without Autoencoding: Pitfalls and Future Directions. (arXiv:2108.06613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1\">Reza Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhier_L/0/1/0/all/0/1\">Lucas Rouhier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>",
          "description": "Labeling vertebral discs from MRI scans is important for the proper diagnosis\nof spinal related diseases, including multiple sclerosis, amyotrophic lateral\nsclerosis, degenerative cervical myelopathy and cancer. Automatic labeling of\nthe vertebral discs in MRI data is a difficult task because of the similarity\nbetween discs and bone area, the variability in the geometry of the spine and\nsurrounding tissues across individuals, and the variability across scans\n(manufacturers, pulse sequence, image contrast, resolution and artefacts). In\nprevious studies, vertebral disc labeling is often done after a disc detection\nstep and mostly fails when the localization algorithm misses discs or has false\npositive detection. In this work, we aim to mitigate this problem by\nreformulating the semantic vertebral disc labeling using the pose estimation\ntechnique. To do so, we propose a stacked hourglass network with multi-level\nattention mechanism to jointly learn intervertebral disc position and their\nskeleton structure. The proposed deep learning model takes into account the\nstrength of semantic segmentation and pose estimation technique to handle the\nmissing area and false positive detection. To further improve the performance\nof the proposed method, we propose a skeleton-based search space to reduce\nfalse positive detection. The proposed method evaluated on spine generic public\nmulti-center dataset and demonstrated better performance comparing to previous\nwork, on both T1w and T2w contrasts. The method is implemented in ivadomed\n(https://ivadomed.org).",
          "link": "http://arxiv.org/abs/2108.06554",
          "publishedOn": "2021-08-17T01:54:49.939Z",
          "wordCount": 685,
          "title": "Stacked Hourglass Network with a Multi-level Attention Mechanism: Where to Look for Intervertebral Disc Labeling. (arXiv:2108.06554v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaoshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>",
          "description": "In this paper, we present a self-training method, named ST3D++, with a\nholistic pseudo label denoising pipeline for unsupervised domain adaptation on\n3D object detection. ST3D++ aims at reducing noise in pseudo label generation\nas well as alleviating the negative impacts of noisy pseudo labels on model\ntraining. First, ST3D++ pre-trains the 3D object detector on the labeled source\ndomain with random object scaling (ROS) which is designed to reduce target\ndomain pseudo label noise arising from object scale bias of the source domain.\nThen, the detector is progressively improved through alternating between\ngenerating pseudo labels and training the object detector with pseudo-labeled\ntarget domain data. Here, we equip the pseudo label generation process with a\nhybrid quality-aware triplet memory to improve the quality and stability of\ngenerated pseudo labels. Meanwhile, in the model training stage, we propose a\nsource data assisted training strategy and a curriculum data augmentation\npolicy to effectively rectify noisy gradient directions and avoid model\nover-fitting to noisy pseudo labeled data. These specific designs enable the\ndetector to be trained on meticulously refined pseudo labeled target data with\ndenoised training signals, and thus effectively facilitate adapting an object\ndetector to a target domain without requiring annotations. Finally, our method\nis assessed on four 3D benchmark datasets (i.e., Waymo, KITTI, Lyft, and\nnuScenes) for three common categories (i.e., car, pedestrian and bicycle).\nST3D++ achieves state-of-the-art performance on all evaluated settings,\noutperforming the corresponding baseline by a large margin (e.g., 9.6% $\\sim$\n38.16% on Waymo $\\rightarrow$ KITTI in terms of AP$_{\\text{3D}}$), and even\nsurpasses the fully supervised oracle results on the KITTI 3D object detection\nbenchmark with target prior. Code will be available.",
          "link": "http://arxiv.org/abs/2108.06682",
          "publishedOn": "2021-08-17T01:54:49.927Z",
          "wordCount": 722,
          "title": "ST3D++: Denoised Self-training for Unsupervised Domain Adaptation on 3D Object Detection. (arXiv:2108.06682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chaoxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "Intelligent vehicles clearly benefit from the expanded Field of View (FoV) of\nthe 360-degree sensors, but the vast majority of available semantic\nsegmentation training images are captured with pinhole cameras. In this work,\nwe look at this problem through the lens of domain adaptation and bring\npanoramic semantic segmentation to a setting, where labelled training data\noriginates from a different distribution of conventional pinhole camera images.\nFirst, we formalize the task of unsupervised domain adaptation for panoramic\nsemantic segmentation, where a network trained on labelled examples from the\nsource domain of pinhole camera data is deployed in a different target domain\nof panoramic images, for which no labels are available. To validate this idea,\nwe collect and publicly release DensePASS - a novel densely annotated dataset\nfor panoramic segmentation under cross-domain conditions, specifically built to\nstudy the Pinhole-to-Panoramic transfer and accompanied with pinhole camera\ntraining examples obtained from Cityscapes. DensePASS covers both, labelled-\nand unlabelled 360-degree images, with the labelled data comprising 19 classes\nwhich explicitly fit the categories available in the source domain (i.e.\npinhole) data. To meet the challenge of domain shift, we leverage the current\nprogress of attention-based mechanisms and build a generic framework for\ncross-domain panoramic semantic segmentation based on different variants of\nattention-augmented domain adaptation modules. Our framework facilitates\ninformation exchange at local- and global levels when learning the domain\ncorrespondences and improves the domain adaptation performance of two standard\nsegmentation networks by 6.05% and 11.26% in Mean IoU.",
          "link": "http://arxiv.org/abs/2108.06383",
          "publishedOn": "2021-08-17T01:54:49.905Z",
          "wordCount": 720,
          "title": "DensePASS: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation with Attention-Augmented Context Exchange. (arXiv:2108.06383v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong Q. Nguyen</a>",
          "description": "The mainstream image captioning models rely on Convolutional Neural Network\n(CNN) image features to generate captions via recurrent models. Recently, image\nscene graphs have been used to augment captioning models so as to leverage\ntheir structural semantics, such as object entities, relationships and\nattributes. Several studies have noted that the naive use of scene graphs from\na black-box scene graph generator harms image captioning performance and that\nscene graph-based captioning models have to incur the overhead of explicit use\nof image features to generate decent captions. Addressing these challenges, we\npropose \\textbf{SG2Caps}, a framework that utilizes only the scene graph labels\nfor competitive image captioning performance. The basic idea is to close the\nsemantic gap between the two scene graphs - one derived from the input image\nand the other from its caption. In order to achieve this, we leverage the\nspatial location of objects and the Human-Object-Interaction (HOI) labels as an\nadditional HOI graph. SG2Caps outperforms existing scene graph-only captioning\nmodels by a large margin, indicating scene graphs as a promising representation\nfor image captioning. Direct utilization of scene graph labels avoids expensive\ngraph convolutions over high-dimensional CNN features resulting in 49% fewer\ntrainable parameters. Our code is available at:\nhttps://github.com/Kien085/SG2Caps",
          "link": "http://arxiv.org/abs/2102.04990",
          "publishedOn": "2021-08-17T01:54:49.382Z",
          "wordCount": 683,
          "title": "In Defense of Scene Graphs for Image Captioning. (arXiv:2102.04990v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1\">Jeff Mentch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_J/0/1/0/all/0/1\">Jerry Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Measuring the acoustic characteristics of a space is often done by capturing\nits impulse response (IR), a representation of how a full-range stimulus sound\nexcites it. This work generates an IR from a single image, which can then be\napplied to other signals using convolution, simulating the reverberant\ncharacteristics of the space shown in the image. Recording these IRs is both\ntime-intensive and expensive, and often infeasible for inaccessible locations.\nWe use an end-to-end neural network architecture to generate plausible audio\nimpulse responses from single images of acoustic environments. We evaluate our\nmethod both by comparisons to ground truth data and by human expert evaluation.\nWe demonstrate our approach by generating plausible impulse responses from\ndiverse settings and formats including well known places, musical halls, rooms\nin paintings, images from animations and computer games, synthetic environments\ngenerated from text, panoramic images, and video conference backgrounds.",
          "link": "http://arxiv.org/abs/2103.14201",
          "publishedOn": "2021-08-17T01:54:49.372Z",
          "wordCount": 623,
          "title": "Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis. (arXiv:2103.14201v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Ashraful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radke_R/0/1/0/all/0/1\">Richard Radke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>",
          "description": "Tremendous progress has been made in visual representation learning, notably\nwith the recent success of self-supervised contrastive learning methods.\nSupervised contrastive learning has also been shown to outperform its\ncross-entropy counterparts by leveraging labels for choosing where to contrast.\nHowever, there has been little work to explore the transfer capability of\ncontrastive learning to a different domain. In this paper, we conduct a\ncomprehensive study on the transferability of learned representations of\ndifferent contrastive approaches for linear evaluation, full-network transfer,\nand few-shot recognition on 12 downstream datasets from different domains, and\nobject detection tasks on MSCOCO and VOC0712. The results show that the\ncontrastive approaches learn representations that are easily transferable to a\ndifferent downstream task. We further observe that the joint objective of\nself-supervised contrastive loss with cross-entropy/supervised-contrastive loss\nleads to better transferability of these models over their supervised\ncounterparts. Our analysis reveals that the representations learned from the\ncontrastive approaches contain more low/mid-level semantics than cross-entropy\nmodels, which enables them to quickly adapt to a new task. Our codes and models\nwill be publicly available to facilitate future research on transferability of\nvisual representations.",
          "link": "http://arxiv.org/abs/2103.13517",
          "publishedOn": "2021-08-17T01:54:49.353Z",
          "wordCount": 677,
          "title": "A Broad Study on the Transferability of Visual Representations with Contrastive Learning. (arXiv:2103.13517v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "Data quantity and quality are crucial factors for data-driven learning\nmethods. In some target problem domains, there are not many data samples\navailable, which could significantly hinder the learning process. While data\nfrom similar domains may be leveraged to help through domain adaptation,\nobtaining high-quality labeled data for those source domains themselves could\nbe difficult or costly. To address such challenges on data insufficiency for\nclassification problem in a target domain, we propose a weak adaptation\nlearning (WAL) approach that leverages unlabeled data from a similar source\ndomain, a low-cost weak annotator that produces labels based on task-specific\nheuristics, labeling rules, or other methods (albeit with inaccuracy), and a\nsmall amount of labeled data in the target domain. Our approach first conducts\na theoretical analysis on the error bound of the trained classifier with\nrespect to the data quantity and the performance of the weak annotator, and\nthen introduces a multi-stage weak adaptation learning method to learn an\naccurate classifier by lowering the error bound. Our experiments demonstrate\nthe effectiveness of our approach in learning an accurate classifier with\nlimited labeled data in the target domain and unlabeled data in the source\ndomain.",
          "link": "http://arxiv.org/abs/2102.07358",
          "publishedOn": "2021-08-17T01:54:49.337Z",
          "wordCount": 668,
          "title": "Weak Adaptation Learning -- Addressing Cross-domain Data Insufficiency with Weak Annotator. (arXiv:2102.07358v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKay_B/0/1/0/all/0/1\">Bob McKay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Skeleton sequences are lightweight and compact, thus are ideal candidates for\naction recognition on edge devices. Recent skeleton-based action recognition\nmethods extract features from 3D joint coordinates as spatial-temporal cues,\nusing these representations in a graph neural network for feature fusion to\nboost recognition performance. The use of first- and second-order features,\n\\ie{} joint and bone representations, has led to high accuracy. Nonetheless,\nmany models are still confused by actions that have similar motion\ntrajectories. To address these issues, we propose fusing third-order features\nin the form of angular encoding into modern architectures to robustly capture\nthe relationships between joints and body parts. This simple fusion with\npopular spatial-temporal graph neural networks achieves new state-of-the-art\naccuracy in two large benchmarks, including NTU60 and NTU120, while employing\nfewer parameters and reduced run time. Our source code is publicly available\nat: https://github.com/ZhenyueQin/Angular-Skeleton-Encoding.",
          "link": "http://arxiv.org/abs/2105.01563",
          "publishedOn": "2021-08-17T01:54:49.319Z",
          "wordCount": 641,
          "title": "Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition. (arXiv:2105.01563v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qidong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>",
          "description": "Recent research shows deep neural networks are vulnerable to different types\nof attacks, such as adversarial attack, data poisoning attack and backdoor\nattack. Among them, backdoor attack is the most cunning one and can occur in\nalmost every stage of deep learning pipeline. Therefore, backdoor attack has\nattracted lots of interests from both academia and industry. However, most\nexisting backdoor attack methods are either visible or fragile to some\neffortless pre-processing such as common data transformations. To address these\nlimitations, we propose a robust and invisible backdoor attack called \"Poison\nInk\". Concretely, we first leverage the image structures as target poisoning\nareas, and fill them with poison ink (information) to generate the trigger\npattern. As the image structure can keep its semantic meaning during the data\ntransformation, such trigger pattern is inherently robust to data\ntransformations. Then we leverage a deep injection network to embed such\ntrigger pattern into the cover image to achieve stealthiness. Compared to\nexisting popular backdoor attack methods, Poison Ink outperforms both in\nstealthiness and robustness. Through extensive experiments, we demonstrate\nPoison Ink is not only general to different datasets and network architectures,\nbut also flexible for different attack scenarios. Besides, it also has very\nstrong resistance against many state-of-the-art defense techniques.",
          "link": "http://arxiv.org/abs/2108.02488",
          "publishedOn": "2021-08-17T01:54:49.301Z",
          "wordCount": 666,
          "title": "Poison Ink: Robust and Invisible Backdoor Attack. (arXiv:2108.02488v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dae-Hyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dong-Kyun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sung-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Ji-Hoon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Brain-computer interface (BCI) is used for communication between humans and\ndevices by recognizing status and intention of humans. Communication between\nhumans and a drone using electroencephalogram (EEG) signals is one of the most\nchallenging issues in the BCI domain. In particular, the control of drone\nswarms (the direction and formation) has more advantages compared to the\ncontrol of a drone. The visual imagery (VI) paradigm is that subjects visually\nimagine specific objects or scenes. Reduction of the variability among EEG\nsignals of subjects is essential for practical BCI-based systems. In this\nstudy, we proposed the subepoch-wise feature encoder (SEFE) to improve the\nperformances in the subject-independent tasks by using the VI dataset. This\nstudy is the first attempt to demonstrate the possibility of generalization\namong subjects in the VI-based BCI. We used the leave-one-subject-out\ncross-validation for evaluating the performances. We obtained higher\nperformances when including our proposed module than excluding our proposed\nmodule. The DeepConvNet with SEFE showed the highest performance of 0.72 among\nsix different decoding models. Hence, we demonstrated the feasibility of\ndecoding the VI dataset in the subject-independent task with robust\nperformances by using our proposed module.",
          "link": "http://arxiv.org/abs/2106.04026",
          "publishedOn": "2021-08-17T01:54:49.279Z",
          "wordCount": 679,
          "title": "Subject-Independent Brain-Computer Interface for Decoding High-Level Visual Imagery Tasks. (arXiv:2106.04026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "We present a graph-convolution-reinforced transformer, named Mesh Graphormer,\nfor 3D human pose and mesh reconstruction from a single image. Recently both\ntransformers and graph convolutional neural networks (GCNNs) have shown\npromising progress in human mesh reconstruction. Transformer-based approaches\nare effective in modeling non-local interactions among 3D mesh vertices and\nbody joints, whereas GCNNs are good at exploiting neighborhood vertex\ninteractions based on a pre-specified mesh topology. In this paper, we study\nhow to combine graph convolutions and self-attentions in a transformer to model\nboth local and global interactions. Experimental results show that our proposed\nmethod, Mesh Graphormer, significantly outperforms the previous\nstate-of-the-art methods on multiple benchmarks, including Human3.6M, 3DPW, and\nFreiHAND datasets. Code and pre-trained models are available at\nhttps://github.com/microsoft/MeshGraphormer",
          "link": "http://arxiv.org/abs/2104.00272",
          "publishedOn": "2021-08-17T01:54:49.262Z",
          "wordCount": 575,
          "title": "Mesh Graphormer. (arXiv:2104.00272v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14403",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>",
          "description": "We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages. In this framework, the slice images of a CT-scan volume are first\nproprocessed using segmentation techniques to filter out images of closed lung,\nand to remove the useless background. Then a resampling method is used to\nselect one or multiple sets of a fixed number of slice images for training and\nvalidation. A 3D CNN network with BERT is used to classify this set of selected\nslice images. In this network, an embedding feature is also extracted. In cases\nwhere there are more than one set of slice images in a volume, the features of\nall sets are extracted and pooled into a global feature vector for the whole\nCT-scan volume. A simple multiple-layer perceptron (MLP) network is used to\nfurther classify the aggregated feature vector. The models are trained and\nevaluated on the provided training and validation datasets. On the validation\ndataset, the accuracy is 0.9278 and the F1 score is 0.9261.",
          "link": "http://arxiv.org/abs/2106.14403",
          "publishedOn": "2021-08-17T01:54:49.257Z",
          "wordCount": 677,
          "title": "A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images. (arXiv:2106.14403v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "Despite much recent progress in video-based person re-identification (re-ID),\nthe current state-of-the-art still suffers from common real-world challenges\nsuch as appearance similarity among various people, occlusions, and frame\nmisalignment. To alleviate these problems, we propose Spatio-Temporal\nRepresentation Factorization (STRF), a flexible new computational unit that can\nbe used in conjunction with most existing 3D convolutional neural network\narchitectures for re-ID. The key innovations of STRF over prior work include\nexplicit pathways for learning discriminative temporal and spatial features,\nwith each component further factorized to capture complementary person-specific\nappearance and motion information. Specifically, temporal factorization\ncomprises two branches, one each for static features (e.g., the color of\nclothes) that do not change much over time, and dynamic features (e.g., walking\npatterns) that change over time. Further, spatial factorization also comprises\ntwo branches to learn both global (coarse segments) as well as local (finer\nsegments) appearance features, with the local features particularly useful in\ncases of occlusion or spatial misalignment. These two factorization operations\ntaken together result in a modular architecture for our parameter-wise light\nSTRF unit that can be plugged in between any two 3D convolutional layers,\nresulting in an end-to-end learning framework. We empirically show that STRF\nimproves performance of various existing baseline architectures while\ndemonstrating new state-of-the-art results using standard person re-ID\nevaluation protocols on three benchmarks.",
          "link": "http://arxiv.org/abs/2107.11878",
          "publishedOn": "2021-08-17T01:54:49.245Z",
          "wordCount": 697,
          "title": "Spatio-Temporal Representation Factorization for Video-based Person Re-Identification. (arXiv:2107.11878v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Delong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhifu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>",
          "description": "Gun violence is a severe problem in the world, particularly in the United\nStates. Deep learning methods have been studied to detect guns in surveillance\nvideo cameras or smart IP cameras and to send a real-time alert to security\npersonals. One problem for the development of gun detection algorithms is the\nlack of large public datasets. In this work, we first publish a dataset with\n51K annotated gun images for gun detection and other 51K cropped gun chip\nimages for gun classification we collect from a few different sources. To our\nknowledge, this is the largest dataset for the study of gun detection. This\ndataset can be downloaded at www.linksprite.com/gun-detection-datasets. We\npresent a gun detection system using a smart IP camera as an embedded edge\ndevice, and a cloud server as a manager for device, data, alert, and to further\nreduce the false positive rate. We study to find solutions for gun detection in\nan embedded device, and for gun classification on the edge device and the cloud\nserver. This edge/cloud framework makes the deployment of gun detection in the\nreal world possible.",
          "link": "http://arxiv.org/abs/2105.01058",
          "publishedOn": "2021-08-17T01:54:49.216Z",
          "wordCount": 665,
          "title": "A Dataset and System for Real-Time Gun Detection in Surveillance Video Using Deep Learning. (arXiv:2105.01058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-08-17T01:54:49.209Z",
          "wordCount": 653,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Learning from a limited number of samples is challenging since the learned\nmodel can easily become overfitted based on the biased distribution formed by\nonly a few training examples. In this paper, we calibrate the distribution of\nthese few-sample classes by transferring statistics from the classes with\nsufficient examples, then an adequate number of examples can be sampled from\nthe calibrated distribution to expand the inputs to the classifier. We assume\nevery dimension in the feature representation follows a Gaussian distribution\nso that the mean and the variance of the distribution can borrow from that of\nsimilar classes whose statistics are better estimated with an adequate number\nof samples. Our method can be built on top of off-the-shelf pretrained feature\nextractors and classification models without extra parameters. We show that a\nsimple logistic regression classifier trained using the features sampled from\nour calibrated distribution can outperform the state-of-the-art accuracy on two\ndatasets (~5% improvement on miniImageNet compared to the next best). The\nvisualization of these generated features demonstrates that our calibrated\ndistribution is an accurate estimation.",
          "link": "http://arxiv.org/abs/2101.06395",
          "publishedOn": "2021-08-17T01:54:49.201Z",
          "wordCount": 659,
          "title": "Free Lunch for Few-shot Learning: Distribution Calibration. (arXiv:2101.06395v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+lin_G/0/1/0/all/0/1\">Guosheng lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "Currently, the state-of-the-art methods treat few-shot semantic segmentation\ntask as a conditional foreground-background segmentation problem, assuming each\nclass is independent. In this paper, we introduce the concept of meta-class,\nwhich is the meta information (e.g. certain middle-level features) shareable\namong all classes. To explicitly learn meta-class representations in few-shot\nsegmentation task, we propose a novel Meta-class Memory based few-shot\nsegmentation method (MM-Net), where we introduce a set of learnable memory\nembeddings to memorize the meta-class information during the base class\ntraining and transfer to novel classes during the inference stage. Moreover,\nfor the $k$-shot scenario, we propose a novel image quality measurement module\nto select images from the set of support images. A high-quality class prototype\ncould be obtained with the weighted sum of support image features based on the\nquality measure. Experiments on both PASCAL-$5^i$ and COCO dataset shows that\nour proposed method is able to achieve state-of-the-art results in both 1-shot\nand 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\\% mIoU on\nthe COCO dataset in 1-shot setting, which is 5.1\\% higher than the previous\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02958",
          "publishedOn": "2021-08-17T01:54:49.194Z",
          "wordCount": 641,
          "title": "Learning Meta-class Memory for Few-Shot Semantic Segmentation. (arXiv:2108.02958v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Beibei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>",
          "description": "Gait recognition is one of the most important biometric technologies and has\nbeen applied in many fields. Recent gait recognition frameworks represent each\ngait frame by descriptors extracted from either global appearances or local\nregions of humans. However, the representations based on global information\noften neglect the details of the gait frame, while local region based\ndescriptors cannot capture the relations among neighboring regions, thus\nreducing their discriminativeness. In this paper, we propose a novel feature\nextraction and fusion framework to achieve discriminative feature\nrepresentations for gait recognition. Towards this goal, we take advantage of\nboth global visual information and local region details and develop a Global\nand Local Feature Extractor (GLFE). Specifically, our GLFE module is composed\nof our newly designed multiple global and local convolutional layers (GLConv)\nto ensemble global and local features in a principle manner. Furthermore, we\npresent a novel operation, namely Local Temporal Aggregation (LTA), to further\npreserve the spatial information by reducing the temporal resolution to obtain\nhigher spatial resolution. With the help of our GLFE and LTA, our method\nsignificantly improves the discriminativeness of our visual features, thus\nimproving the gait recognition performance. Extensive experiments demonstrate\nthat our proposed method outperforms state-of-the-art gait recognition methods\non two popular datasets.",
          "link": "http://arxiv.org/abs/2011.01461",
          "publishedOn": "2021-08-17T01:54:49.178Z",
          "wordCount": 677,
          "title": "Gait Recognition via Effective Global-Local Feature Representation and Local Temporal Aggregation. (arXiv:2011.01461v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guo-Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yifan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Knowledge distillation (KD) is a popular method to train efficient networks\n(\"student\") with the help of high-capacity networks (\"teacher\"). Traditional\nmethods use the teacher's soft logits as extra supervision to train the student\nnetwork. In this paper, we argue that it is more advantageous to make the\nstudent mimic the teacher's features in the penultimate layer. Not only the\nstudent can directly learn more effective information from the teacher feature,\nfeature mimicking can also be applied for teachers trained without a softmax\nlayer. Experiments show that it can achieve higher accuracy than traditional\nKD. To further facilitate feature mimicking, we decompose a feature vector into\nthe magnitude and the direction. We argue that the teacher should give more\nfreedom to the student feature's magnitude, and let the student pay more\nattention on mimicking the feature direction. To meet this requirement, we\npropose a loss term based on locality-sensitive hashing (LSH). With the help of\nthis new loss, our method indeed mimics feature directions more accurately,\nrelaxes constraints on feature magnitudes, and achieves state-of-the-art\ndistillation accuracy. We provide theoretical analyses of how LSH facilitates\nfeature direction mimicking, and further extend feature mimicking to\nmulti-label recognition and object detection.",
          "link": "http://arxiv.org/abs/2011.01424",
          "publishedOn": "2021-08-17T01:54:49.173Z",
          "wordCount": 664,
          "title": "Distilling Knowledge by Mimicking Features. (arXiv:2011.01424v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1\">Cheng Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>",
          "description": "This paper tackles the task of category-level pose estimation for garments.\nWith a near infinite degree of freedom, a garment's full configuration (i.e.,\nposes) is often described by the per-vertex 3D locations of its entire 3D\nsurface. However, garments are also commonly subject to extreme cases of\nself-occlusion, especially when folded or crumpled, making it challenging to\nperceive their full 3D surface. To address these challenges, we propose\nGarmentNets, where the key idea is to formulate the deformable object pose\nestimation problem as a shape completion task in the canonical space. This\ncanonical space is defined across garments instances within a category,\ntherefore, specifies the shared category-level pose. By mapping the observed\npartial surface to the canonical space and completing it in this space, the\noutput representation describes the garment's full configuration using a\ncomplete 3D mesh with the per-vertex canonical coordinate label. To properly\nhandle the thin 3D structure presented on garments, we proposed a novel 3D\nshape representation using the generalized winding number field. Experiments\ndemonstrate that GarmentNets is able to generalize to unseen garment instances\nand achieve significantly better performance compared to alternative\napproaches.",
          "link": "http://arxiv.org/abs/2104.05177",
          "publishedOn": "2021-08-17T01:54:49.167Z",
          "wordCount": 663,
          "title": "GarmentNets: Category-Level Pose Estimation for Garments via Canonical Space Shape Completion. (arXiv:2104.05177v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1\">Maksim Siniukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1\">Dmitriy Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>",
          "description": "Video-quality measurement plays a critical role in the development of\nvideo-processing applications. In this paper, we show how video preprocessing\ncan artificially increase the popular quality metric VMAF and its\ntuning-resistant version, VMAF NEG. We propose a pipeline that tunes\nprocessing-algorithm parameters to increase VMAF by up to 218.8%. A subjective\ncomparison revealed that for most preprocessing methods, a video's visual\nquality drops or stays unchanged. We also show that some preprocessing methods\ncan increase VMAF NEG scores by up to 23.6%.",
          "link": "http://arxiv.org/abs/2107.04510",
          "publishedOn": "2021-08-17T01:54:49.136Z",
          "wordCount": 550,
          "title": "Hacking VMAF and VMAF NEG: vulnerability to different preprocessing methods. (arXiv:2107.04510v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Prasen Kumar Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bisht_I/0/1/0/all/0/1\">Ira Bisht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>",
          "description": "Background: Underwater images, in general, suffer from low contrast and high\ncolor distortions due to the non-uniform attenuation of the light as it\npropagates through the water. In addition, the degree of attenuation varies\nwith the wavelength resulting in the asymmetric traversing of colors. Despite\nthe prolific works for underwater image restoration (UIR) using deep learning,\nthe above asymmetricity has not been addressed in the respective network\nengineering.\n\nContributions: As the first novelty, this paper shows that attributing the\nright receptive field size (context) based on the traversing range of the color\nchannel may lead to a substantial performance gain for the task of UIR.\nFurther, it is important to suppress the irrelevant multi-contextual features\nand increase the representational power of the model. Therefore, as a second\nnovelty, we have incorporated an attentive skip mechanism to adaptively refine\nthe learned multi-contextual features. The proposed framework, called Deep\nWaveNet, is optimized using the traditional pixel-wise and feature-based cost\nfunctions. An extensive set of experiments have been carried out to show the\nefficacy of the proposed scheme over existing best-published literature on\nbenchmark datasets. More importantly, we have demonstrated a comprehensive\nvalidation of enhanced images across various high-level vision tasks, e.g.,\nunderwater image semantic segmentation, and diver's 2D pose estimation. A\nsample video to exhibit our real-world performance is available at\n\\url{https://tinyurl.com/yzcrup9n}. Also, we have open-sourced our framework at\n\\url{https://github.com/pksvision/Deep-WaveNet-UnderwaterImage-Restoration}.",
          "link": "http://arxiv.org/abs/2106.07910",
          "publishedOn": "2021-08-17T01:54:49.130Z",
          "wordCount": 698,
          "title": "Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration. (arXiv:2106.07910v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shumeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaixin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "Deep learning has achieved promising segmentation performance on 3D left\natrium MR images. However, annotations for segmentation tasks are expensive,\ncostly and difficult to obtain. In this paper, we introduce a novel\nhierarchical consistency regularized mean teacher framework for 3D left atrium\nsegmentation. In each iteration, the student model is optimized by multi-scale\ndeep supervision and hierarchical consistency regularization, concurrently.\nExtensive experiments have shown that our method achieves competitive\nperformance as compared with full annotation, outperforming other\nstate-of-the-art semi-supervised segmentation methods.",
          "link": "http://arxiv.org/abs/2105.10369",
          "publishedOn": "2021-08-17T01:54:49.123Z",
          "wordCount": 583,
          "title": "Hierarchical Consistency Regularized Mean Teacher for Semi-supervised 3D Left Atrium Segmentation. (arXiv:2105.10369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Golestaneh_S/0/1/0/all/0/1\">S. Alireza Golestaneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadsetan_S/0/1/0/all/0/1\">Saba Dadsetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>",
          "description": "The goal of No-Reference Image Quality Assessment (NR-IQA) is to estimate the\nperceptual image quality in accordance with subjective evaluations, it is a\ncomplex and unsolved problem due to the absence of the pristine reference\nimage. In this paper, we propose a novel model to address the NR-IQA task by\nleveraging a hybrid approach that benefits from Convolutional Neural Networks\n(CNNs) and self-attention mechanism in Transformers to extract both local and\nnon-local features from the input image. We capture local structure information\nof the image via CNNs, then to circumvent the locality bias among the extracted\nCNNs features and obtain a non-local representation of the image, we utilize\nTransformers on the extracted features where we model them as a sequential\ninput to the Transformer model. Furthermore, to improve the monotonicity\ncorrelation between the subjective and objective scores, we utilize the\nrelative distance information among the images within each batch and enforce\nthe relative ranking among them. Last but not least, we observe that the\nperformance of NR-IQA models degrades when we apply equivariant transformations\n(e.g. horizontal flipping) to the inputs. Therefore, we propose a method that\nleverages self-consistency as a source of self-supervision to improve the\nrobustness of NRIQA models. Specifically, we enforce self-consistency between\nthe outputs of our quality assessment model for each image and its\ntransformation (horizontally flipped) to utilize the rich self-supervisory\ninformation and reduce the uncertainty of the model. To demonstrate the\neffectiveness of our work, we evaluate it on seven standard IQA datasets (both\nsynthetic and authentic) and show that our model achieves state-of-the-art\nresults on various datasets.",
          "link": "http://arxiv.org/abs/2108.06858",
          "publishedOn": "2021-08-17T01:54:49.117Z",
          "wordCount": 704,
          "title": "No-Reference Image Quality Assessment via Transformers, Relative Ranking, and Self-Consistency. (arXiv:2108.06858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhuo Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenzhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dewen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietikainen_M/0/1/0/all/0/1\">Matti Pietik&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>",
          "description": "Recently, deep Convolutional Neural Networks (CNNs) can achieve human-level\nperformance in edge detection with the rich and abstract edge representation\ncapacities. However, the high performance of CNN based edge detection is\nachieved with a large pretrained CNN backbone, which is memory and energy\nconsuming. In addition, it is surprising that the previous wisdom from the\ntraditional edge detectors, such as Canny, Sobel, and LBP are rarely\ninvestigated in the rapid-developing deep learning era. To address these\nissues, we propose a simple, lightweight yet effective architecture named Pixel\nDifference Network (PiDiNet) for efficient edge detection. Extensive\nexperiments on BSDS500, NYUD, and Multicue are provided to demonstrate its\neffectiveness, and its high training and inference efficiency. Surprisingly,\nwhen training from scratch with only the BSDS500 and VOC datasets, PiDiNet can\nsurpass the recorded result of human perception (0.807 vs. 0.803 in ODS\nF-measure) on the BSDS500 dataset with 100 FPS and less than 1M parameters. A\nfaster version of PiDiNet with less than 0.1M parameters can still achieve\ncomparable performance among state of the arts with 200 FPS. Results on the\nNYUD and Multicue datasets show similar observations. The codes are available\nat https://github.com/zhuoinoulu/pidinet.",
          "link": "http://arxiv.org/abs/2108.07009",
          "publishedOn": "2021-08-17T01:54:49.111Z",
          "wordCount": 639,
          "title": "Pixel Difference Networks for Efficient Edge Detection. (arXiv:2108.07009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_H/0/1/0/all/0/1\">Hemanth Sai Ram Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vani_M/0/1/0/all/0/1\">M Vani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "The capsule network is a distinct and promising segment of the neural network\nfamily that drew attention due to its unique ability to maintain the\nequivariance property by preserving the spatial relationship amongst the\nfeatures. The capsule network has attained unprecedented success over image\nclassification tasks with datasets such as MNIST and affNIST by encoding the\ncharacteristic features into the capsules and building the parse-tree\nstructure. However, on the datasets involving complex foreground and background\nregions such as CIFAR-10, the performance of the capsule network is sub-optimal\ndue to its naive data routing policy and incompetence towards extracting\ncomplex features. This paper proposes a new design strategy for capsule network\narchitecture for efficiently dealing with complex images. The proposed method\nincorporates wide bottleneck residual modules and the Squeeze and Excitation\nattention blocks upheld by the modified FM routing algorithm to address the\ndefined problem. A wide bottleneck residual module facilitates extracting\ncomplex features followed by the squeeze and excitation attention block to\nenable channel-wise attention by suppressing the trivial features. This setup\nallows channel inter-dependencies at almost no computational cost, thereby\nenhancing the representation ability of capsules on complex images. We\nextensively evaluate the performance of the proposed model on three publicly\navailable datasets, namely CIFAR-10, Fashion MNIST, and SVHN, to outperform the\ntop-5 performance on CIFAR-10 and Fashion MNIST with highly competitive\nperformance on the SVHN dataset.",
          "link": "http://arxiv.org/abs/2108.03627",
          "publishedOn": "2021-08-17T01:54:49.093Z",
          "wordCount": 702,
          "title": "WideCaps: A Wide Attention based Capsule Network for Image Classification. (arXiv:2108.03627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yazhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingkun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Self-supervised learning for depth estimation possesses several advantages\nover supervised learning. The benefits of no need for ground-truth depth,\nonline fine-tuning, and better generalization with unlimited data attract\nresearchers to seek self-supervised solutions. In this work, we propose a new\nself-supervised framework for stereo matching utilizing multiple images\ncaptured at aligned camera positions. A cross photometric loss, an\nuncertainty-aware mutual-supervision loss, and a new smoothness loss are\nintroduced to optimize the network in learning disparity maps end-to-end\nwithout ground-truth depth information. To train this framework, we build a new\nmultiscopic dataset consisting of synthetic images rendered by 3D engines and\nreal images captured by real cameras. After being trained with only the\nsynthetic images, our network can perform well in unseen outdoor scenes. Our\nexperiment shows that our model obtains better disparity maps than previous\nunsupervised methods on the KITTI dataset and is comparable to supervised\nmethods when generalized to unseen data. Our source code and dataset are\navailable at https://sites.google.com/view/multiscopic.",
          "link": "http://arxiv.org/abs/2104.04170",
          "publishedOn": "2021-08-17T01:54:49.087Z",
          "wordCount": 646,
          "title": "Stereo Matching by Self-supervision of Multiscopic Vision. (arXiv:2104.04170v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>",
          "description": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.",
          "link": "http://arxiv.org/abs/2108.03857",
          "publishedOn": "2021-08-17T01:54:49.080Z",
          "wordCount": 613,
          "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>",
          "description": "Vision transformer has achieved competitive performance on a variety of\ncomputer vision applications. However, their storage, run-time memory, and\ncomputational demands are hindering the deployment to mobile devices. Here we\npresent a vision transformer pruning approach, which identifies the impacts of\ndimensions in each layer of transformer and then executes pruning accordingly.\nBy encouraging dimension-wise sparsity in the transformer, important dimensions\nautomatically emerge. A great number of dimensions with small importance scores\ncan be discarded to achieve a high pruning ratio without significantly\ncompromising accuracy. The pipeline for vision transformer pruning is as\nfollows: 1) training with sparsity regularization; 2) pruning dimensions of\nlinear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of\nthe proposed algorithm are well evaluated and analyzed on ImageNet dataset to\ndemonstrate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2104.08500",
          "publishedOn": "2021-08-17T01:54:49.074Z",
          "wordCount": 614,
          "title": "Vision Transformer Pruning. (arXiv:2104.08500v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>",
          "description": "Most existing Siamese-based tracking methods execute the classification and\nregression of the target object based on the similarity maps. However, they\neither employ a single map from the last convolutional layer which degrades the\nlocalization accuracy in complex scenarios or separately use multiple maps for\ndecision making, introducing intractable computations for aerial mobile\nplatforms. Thus, in this work, we propose an efficient and effective\nhierarchical feature transformer (HiFT) for aerial tracking. Hierarchical\nsimilarity maps generated by multi-level convolutional layers are fed into the\nfeature transformer to achieve the interactive fusion of spatial (shallow\nlayers) and semantics cues (deep layers). Consequently, not only the global\ncontextual information can be raised, facilitating the target search, but also\nour end-to-end architecture with the transformer can efficiently learn the\ninterdependencies among multi-level features, thereby discovering a\ntracking-tailored feature space with strong discriminability. Comprehensive\nevaluations on four aerial benchmarks have proven the effectiveness of HiFT.\nReal-world tests on the aerial platform have strongly validated its\npracticability with a real-time speed. Our code is available at\nhttps://github.com/vision4robotics/HiFT.",
          "link": "http://arxiv.org/abs/2108.00202",
          "publishedOn": "2021-08-17T01:54:49.067Z",
          "wordCount": 638,
          "title": "HiFT: Hierarchical Feature Transformer for Aerial Tracking. (arXiv:2108.00202v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>",
          "description": "Graph-based multi-view clustering has become an active topic due to the\nefficiency in characterizing both the complex structure and relationship\nbetween multimedia data. However, existing methods have the following\nshortcomings: (1) They are inefficient or even fail for graph learning in large\nscale due to the graph construction and eigen-decomposition. (2) They cannot\nwell exploit both the complementary information and spatial structure embedded\nin graphs of different views. To well exploit complementary information and\ntackle the scalability issue plaguing graph-based multi-view clustering, we\npropose an efficient multiple graph learning model via a small number of anchor\npoints and tensor Schatten p-norm minimization. Specifically, we construct a\nhidden and tractable large graph by anchor graph for each view and well exploit\ncomplementary information embedded in anchor graphs of different views by\ntensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,\nwhich scales linearly with the data size, to solve our proposed model.\nExtensive experimental results on several datasets indicate that our proposed\nmethod outperforms some state-of-the-art multi-view clustering algorithms.",
          "link": "http://arxiv.org/abs/2106.15382",
          "publishedOn": "2021-08-17T01:54:49.049Z",
          "wordCount": 629,
          "title": "Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1\">Victor G. Turrisi da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "This paper presents solo-learn, a library of self-supervised methods for\nvisual representation learning. Implemented in Python, using Pytorch and\nPytorch lightning, the library fits both research and industry needs by\nfeaturing distributed training pipelines with mixed-precision, faster data\nloading via Nvidia DALI, online linear evaluation for better prototyping, and\nmany additional training tricks. Our goal is to provide an easy-to-use library\ncomprising a large amount of Self-supervised Learning (SSL) methods, that can\nbe easily extended and fine-tuned by the community. solo-learn opens up avenues\nfor exploiting large-budget SSL solutions on inexpensive smaller\ninfrastructures and seeks to democratize SSL by making it accessible to all.\nThe source code is available at https://github.com/vturrisi/solo-learn.",
          "link": "http://arxiv.org/abs/2108.01775",
          "publishedOn": "2021-08-17T01:54:49.043Z",
          "wordCount": 583,
          "title": "Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. (arXiv:2108.01775v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khawaled_S/0/1/0/all/0/1\">Samah Khawaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>",
          "description": "Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays an important role in the safe deployment of\nreal-world medical applications and research-oriented processing pipelines, and\nin improving generalization capabilities. Currently available approaches for\nuncertainty estimation, including the variational encoder-decoder architecture\nand the inference-time dropout approach, require specific network architectures\nand assume parametric distribution of the latent space which may result in\nsub-optimal characterization of the posterior distribution for the predicted\ndeformation-fields. We introduce the NPBDREG, a fully non-parametric Bayesian\nframework for unsupervised DNN-based deformable image registration by combining\nan \\texttt{Adam} optimizer with stochastic gradient Langevin dynamics (SGLD) to\ncharacterize the true posterior distribution through posterior sampling. The\nNPBDREG provides a principled non-parametric way to characterize the true\nposterior distribution, thus providing improved uncertainty estimates and\nconfidence measures in a theoretically well-founded and computationally\nefficient way. We demonstrated the added-value of NPBDREG, compared to the\nbaseline probabilistic \\texttt{VoxelMorph} unsupervised model (PrVXM), on brain\nMRI images registration using $390$ image pairs from four publicly available\ndatabases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a slight\nimprovement in the registration accuracy compared to PrVXM (Dice score of\n$0.73$ vs. $0.68$, $p \\ll 0.01$), a better generalization capability for data\ncorrupted by a mixed structure noise (e.g Dice score of $0.729$ vs. $0.686$ for\n$\\alpha=0.2$) and last but foremost, a significantly better correlation of the\npredicted uncertainty with out-of-distribution data ($r>0.95$ vs. $r<0.5$).",
          "link": "http://arxiv.org/abs/2108.06771",
          "publishedOn": "2021-08-17T01:54:49.019Z",
          "wordCount": 676,
          "title": "NPBDREG: A Non-parametric Bayesian Deep-Learning Based Approach for Diffeomorphic Brain MRI Registration. (arXiv:2108.06771v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1\">Rowel Atienza</a>",
          "description": "Scene text recognition (STR) is a challenging task in computer vision due to\nthe large number of possible text appearances in natural scenes. Most STR\nmodels rely on synthetic datasets for training since there are no sufficiently\nbig and publicly available labelled real datasets. Since STR models are\nevaluated using real data, the mismatch between training and testing data\ndistributions results into poor performance of models especially on challenging\ntext that are affected by noise, artifacts, geometry, structure, etc. In this\npaper, we introduce STRAug which is made of 36 image augmentation functions\ndesigned for STR. Each function mimics certain text image properties that can\nbe found in natural scenes, caused by camera sensors, or induced by signal\nprocessing operations but poorly represented in the training dataset. When\napplied to strong baseline models using RandAugment, STRAug significantly\nincreases the overall absolute accuracy of STR models across regular and\nirregular test datasets by as much as 2.10% on Rosetta, 1.48% on R2AM, 1.30% on\nCRNN, 1.35% on RARE, 1.06% on TRBA and 0.89% on GCRNN. The diversity and\nsimplicity of API provided by STRAug functions enable easy replication and\nvalidation of existing data augmentation methods for STR. STRAug is available\nat https://github.com/roatienza/straug.",
          "link": "http://arxiv.org/abs/2108.06949",
          "publishedOn": "2021-08-17T01:54:49.012Z",
          "wordCount": 640,
          "title": "Data Augmentation for Scene Text Recognition. (arXiv:2108.06949v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinyue Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yujin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongliang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>",
          "description": "The classic Monte Carlo path tracing can achieve high quality rendering at\nthe cost of heavy computation. Recent works make use of deep neural networks to\naccelerate this process, by improving either low-resolution or fewer-sample\nrendering with super-resolution or denoising neural networks in\npost-processing. However, denoising and super-resolution have only been\nconsidered separately in previous work. We show in this work that Monte Carlo\npath tracing can be further accelerated by joint super-resolution and denoising\n(SRD) in post-processing. This new type of joint filtering allows only a\nlow-resolution and fewer-sample (thus noisy) image to be rendered by path\ntracing, which is then fed into a deep neural network to produce a\nhigh-resolution and clean image. The main contribution of this work is a new\nend-to-end network architecture, specifically designed for the SRD task. It\ncontains two cascaded stages with shared components. We discover that denoising\nand super-resolution require very different receptive fields, a key insight\nthat leads to the introduction of deformable convolution into the network\ndesign. Extensive experiments show that the proposed method outperforms\nprevious methods and their variants adopted for the SRD task.",
          "link": "http://arxiv.org/abs/2108.06915",
          "publishedOn": "2021-08-17T01:54:48.989Z",
          "wordCount": 626,
          "title": "End-to-End Adaptive Monte Carlo Denoising and Super-Resolution. (arXiv:2108.06915v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>",
          "description": "Nowadays, Deep Convolutional Neural Networks (DCNNs) are widely used in\nfabric defect detection, which come with the cost of expensive training and\ncomplex model parameters. With the observation that most fabrics are defect\nfree in practice, a two-step Cascaded Zoom-In Network (CZI-Net) is proposed for\npatterned fabric defect detection. In the CZI-Net, the Aggregated HOG (A-HOG)\nand SIFT features are used to instead of simple convolution filters for feature\nextraction. Moreover, in order to extract more distinctive features, the\nfeature representation layer and full connection layer are included in the\nCZI-Net. In practice, Most defect-free fabrics only involve in the first step\nof our method and avoid a costive computation in the second step, which makes\nvery fast fabric detection. More importantly, we propose the\nLocality-constrained Reconstruction Error (LCRE) in the first step and\nRestrictive Locality-constrained Coding (RLC), Bag-of-Indexes (BoI) methods in\nthe second step. We also analyse the connections between different coding\nmethods and conclude that the index of visual words plays an essential role in\nthe coding methods. In conclusion, experiments based on real-world datasets are\nimplemented and demonstrate that our proposed method is not only\ncomputationally simple but also with high detection accuracy.",
          "link": "http://arxiv.org/abs/2108.06760",
          "publishedOn": "2021-08-17T01:54:48.982Z",
          "wordCount": 635,
          "title": "A Cascaded Zoom-In Network for Patterned Fabric Defect Detection. (arXiv:2108.06760v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arindam Chaudhuri</a>",
          "description": "BSplines are one of the most promising curves in computer graphics. They are\nblessed with some superior geometric properties which make them an ideal\ncandidate for several applications in computer aided design industry. In this\narticle, some basic properties of B-Spline curves are presented. Two\nsignificant B-Spline properties viz convex hull property and repeated points\neffects are discussed. The BSplines computation in computational devices is\nalso illustrated. An industry application based on image processing where\nB-Spline curve reconstructs the 3D surfaces for CT image datasets of inner\norgans further highlights the strength of these curves",
          "link": "http://arxiv.org/abs/2108.06617",
          "publishedOn": "2021-08-17T01:54:48.927Z",
          "wordCount": 522,
          "title": "B-Splines. (arXiv:2108.06617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_D/0/1/0/all/0/1\">Donghyeon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Youngmin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1\">Bumsub Ham</a>",
          "description": "We address the problem of generalized zero-shot semantic segmentation (GZS3)\npredicting pixel-wise semantic labels for seen and unseen classes. Most GZS3\nmethods adopt a generative approach that synthesizes visual features of unseen\nclasses from corresponding semantic ones (e.g., word2vec) to train novel\nclassifiers for both seen and unseen classes. Although generative methods show\ndecent performance, they have two limitations: (1) the visual features are\nbiased towards seen classes; (2) the classifier should be retrained whenever\nnovel unseen classes appear. We propose a discriminative approach to address\nthese limitations in a unified framework. To this end, we leverage visual and\nsemantic encoders to learn a joint embedding space, where the semantic encoder\ntransforms semantic features to semantic prototypes that act as centers for\nvisual features of corresponding classes. Specifically, we introduce\nboundary-aware regression (BAR) and semantic consistency (SC) losses to learn\ndiscriminative features. Our approach to exploiting the joint embedding space,\ntogether with BAR and SC terms, alleviates the seen bias problem. At test time,\nwe avoid the retraining process by exploiting semantic prototypes as a\nnearest-neighbor (NN) classifier. To further alleviate the bias problem, we\nalso propose an inference technique, dubbed Apollonius calibration (AC), that\nmodulates the decision boundary of the NN classifier to the Apollonius circle\nadaptively. Experimental results demonstrate the effectiveness of our\nframework, achieving a new state of the art on standard benchmarks.",
          "link": "http://arxiv.org/abs/2108.06536",
          "publishedOn": "2021-08-17T01:54:48.914Z",
          "wordCount": 669,
          "title": "Exploiting a Joint Embedding Space for Generalized Zero-Shot Semantic Segmentation. (arXiv:2108.06536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkpen_D/0/1/0/all/0/1\">Diana Inkpen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Roby_A/0/1/0/all/0/1\">Ahmed El-Roby</a>",
          "description": "Adversarial domain adaptation has made impressive advances in transferring\nknowledge from the source domain to the target domain by aligning feature\ndistributions of both domains. These methods focus on minimizing domain\ndivergence and regard the adaptability, which is measured as the expected error\nof the ideal joint hypothesis on these two domains, as a small constant.\nHowever, these approaches still face two issues: (1) Adversarial domain\nalignment distorts the original feature distributions, deteriorating the\nadaptability; (2) Transforming feature representations to be domain-invariant\nneeds to sacrifice domain-specific variations, resulting in weaker\ndiscriminability. In order to alleviate these issues, we propose\ncategory-invariant feature enhancement (CIFE), a general mechanism that\nenhances the adversarial domain adaptation through optimizing the adaptability.\nSpecifically, the CIFE approach introduces category-invariant features to boost\nthe discriminability of domain-invariant features with preserving the\ntransferability. Experiments show that the CIFE could improve upon\nrepresentative adversarial domain adaptation methods to yield state-of-the-art\nresults on five benchmarks.",
          "link": "http://arxiv.org/abs/2108.06583",
          "publishedOn": "2021-08-17T01:54:48.908Z",
          "wordCount": 601,
          "title": "Towards Category and Domain Alignment: Category-Invariant Feature Enhancement for Adversarial Domain Adaptation. (arXiv:2108.06583v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Shiyu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoqun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_W/0/1/0/all/0/1\">Weize Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong-Ming Yan</a>",
          "description": "Removing undesirable specular highlight from a single input image is of\ncrucial importance to many computer vision and graphics tasks. Existing methods\ntypically remove specular highlight for medical images and specific-object\nimages, however, they cannot handle the images with text. In addition, the\nimpact of specular highlight on text recognition is rarely studied by text\ndetection and recognition community. Therefore, in this paper, we first raise\nand study the text-aware single image specular highlight removal problem. The\ncore goal is to improve the accuracy of text detection and recognition by\nremoving the highlight from text images. To tackle this challenging problem, we\nfirst collect three high-quality datasets with fine-grained annotations, which\nwill be appropriately released to facilitate the relevant research. Then, we\ndesign a novel two-stage network, which contains a highlight detection network\nand a highlight removal network. The output of highlight detection network\nprovides additional information about highlight regions to guide the subsequent\nhighlight removal network. Moreover, we suggest a measurement set including the\nend-to-end text detection and recognition evaluation and auxiliary visual\nquality evaluation. Extensive experiments on our collected datasets demonstrate\nthe superior performance of the proposed method.",
          "link": "http://arxiv.org/abs/2108.06881",
          "publishedOn": "2021-08-17T01:54:48.891Z",
          "wordCount": 624,
          "title": "Text-Aware Single Image Specular Highlight Removal. (arXiv:2108.06881v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>",
          "description": "Food image classification is challenging for real-world applications since\nexisting methods require static datasets for training and are not capable of\nlearning from sequentially available new food images. Online continual learning\naims to learn new classes from data stream by using each new data only once\nwithout forgetting the previously learned knowledge. However, none of the\nexisting works target food image analysis, which is more difficult to learn\nincrementally due to its high intra-class variation with the unbalanced and\nunpredictable characteristics of future food class distribution. In this paper,\nwe address these issues by introducing (1) a novel clustering based exemplar\nselection algorithm to store the most representative data belonging to each\nlearned food for knowledge replay, and (2) an effective online learning regime\nusing balanced training batch along with the knowledge distillation on\naugmented exemplars to maintain the model performance on all learned classes.\nOur method is evaluated on a challenging large scale food image database,\nFood-1K, by varying the number of newly added food classes. Our results show\nsignificant improvements compared with existing state-of-the-art online\ncontinual learning methods, showing great potential to achieve lifelong\nlearning for food image classification in real world.",
          "link": "http://arxiv.org/abs/2108.06781",
          "publishedOn": "2021-08-17T01:54:48.885Z",
          "wordCount": 631,
          "title": "Online Continual Learning For Visual Food Classification. (arXiv:2108.06781v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>",
          "description": "Conventional video inpainting is neither object-oriented nor occlusion-aware,\nmaking it liable to obvious artifacts when large occluded object regions are\ninpainted. This paper presents occlusion-aware video object inpainting, which\nrecovers both the complete shape and appearance for occluded objects in videos\ngiven their visible mask segmentation.\n\nTo facilitate this new research, we construct the first large-scale video\nobject inpainting benchmark YouTube-VOI to provide realistic occlusion\nscenarios with both occluded and visible object masks available. Our technical\ncontribution VOIN jointly performs video object shape completion and occluded\ntexture generation. In particular, the shape completion module models\nlong-range object coherence while the flow completion module recovers accurate\nflow with sharp motion boundary, for propagating temporally-consistent texture\nto the same moving object across frames. For more realistic results, VOIN is\noptimized using both T-PatchGAN and a new spatio-temporal attention-based\nmulti-class discriminator.\n\nFinally, we compare VOIN and strong baselines on YouTube-VOI. Experimental\nresults clearly demonstrate the efficacy of our method including inpainting\ncomplex and dynamic objects. VOIN degrades gracefully with inaccurate input\nvisible mask.",
          "link": "http://arxiv.org/abs/2108.06765",
          "publishedOn": "2021-08-17T01:54:48.876Z",
          "wordCount": 598,
          "title": "Occlusion-Aware Video Object Inpainting. (arXiv:2108.06765v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianzhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuansheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>",
          "description": "Most publicly available datasets for image classification are with single\nlabels, while images are inherently multi-labeled in our daily life. Such an\nannotation gap makes many pre-trained single-label classification models fail\nin practical scenarios. This annotation issue is more concerned for aerial\nimages: Aerial data collected from sensors naturally cover a relatively large\nland area with multiple labels, while annotated aerial datasets, which are\npublicly available (e.g., UCM, AID), are single-labeled. As manually annotating\nmulti-label aerial images would be time/labor-consuming, we propose a novel\nself-correction integrated domain adaptation (SCIDA) method for automatic\nmulti-label learning. SCIDA is weakly supervised, i.e., automatically learning\nthe multi-label image classification model from using massive, publicly\navailable single-label images. To achieve this goal, we propose a novel\nLabel-Wise self-Correction (LWC) module to better explore underlying label\ncorrelations. This module also makes the unsupervised domain adaptation (UDA)\nfrom single- to multi-label data possible. For model training, the proposed\nmodel only uses single-label information yet requires no prior knowledge of\nmulti-labeled data; and it predicts labels for multi-label aerial images. In\nour experiments, trained with single-labeled MAI-AID-s and MAI-UCM-s datasets,\nthe proposed model is tested directly on our collected Multi-scene Aerial Image\n(MAI) dataset.",
          "link": "http://arxiv.org/abs/2108.06810",
          "publishedOn": "2021-08-17T01:54:48.868Z",
          "wordCount": 643,
          "title": "SCIDA: Self-Correction Integrated Domain Adaptation from Single- to Multi-label Aerial Images. (arXiv:2108.06810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neseem_M/0/1/0/all/0/1\">Marina Neseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reda_S/0/1/0/all/0/1\">Sherief Reda</a>",
          "description": "Convolutional Neural Networks achieve state-of-the-art accuracy in object\ndetection tasks. However, they have large computational and energy requirements\nthat challenge their deployment on resource-constrained edge devices. Object\ndetection takes an image as an input, and identifies the existing object\nclasses as well as their locations in the image. In this paper, we leverage the\nprior knowledge about the probabilities that different object categories can\noccur jointly to increase the efficiency of object detection models. In\nparticular, our technique clusters the object categories based on their spatial\nco-occurrence probability. We use those clusters to design an adaptive network.\nDuring runtime, a branch controller decides which part(s) of the network to\nexecute based on the spatial context of the input frame. Our experiments using\nCOCO dataset show that our adaptive object detection model achieves up to 45%\nreduction in the energy consumption, and up to 27% reduction in the latency,\nwith a small loss in the average precision (AP) of object detection.",
          "link": "http://arxiv.org/abs/2108.06850",
          "publishedOn": "2021-08-17T01:54:48.863Z",
          "wordCount": 612,
          "title": "AdaCon: Adaptive Context-Aware Object Detection for Resource-Constrained Embedded Devices. (arXiv:2108.06850v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saavedra_Ruiz_M/0/1/0/all/0/1\">Miguel Saavedra-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_Vargas_A/0/1/0/all/0/1\">Ana Mario Pinto-Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Cano_V/0/1/0/all/0/1\">Victor Romero-Cano</a>",
          "description": "Autonomous landing is a capability that is essential to achieve the full\npotential of multi-rotor drones in many social and industrial applications. The\nimplementation and testing of this capability on physical platforms is risky\nand resource-intensive; hence, in order to ensure both a sound design process\nand a safe deployment, simulations are required before implementing a physical\nprototype. This paper presents the development of a monocular visual system,\nusing a software-in-the-loop methodology, that autonomously and efficiently\nlands a quadcopter drone on a predefined landing pad, thus reducing the risks\nof the physical testing stage. In addition to ensuring that the autonomous\nlanding system as a whole fulfils the design requirements using a Gazebo-based\nsimulation, our approach provides a tool for safe parameter tuning and design\ntesting prior to physical implementation. Finally, the proposed monocular\nvision-only approach to landing pad tracking made it possible to effectively\nimplement the system in an F450 quadcopter drone with the standard\ncomputational capabilities of an Odroid XU4 embedded processor.",
          "link": "http://arxiv.org/abs/2108.06616",
          "publishedOn": "2021-08-17T01:54:48.847Z",
          "wordCount": 616,
          "title": "Monocular visual autonomous landing system for quadcopter drones using software in the loop. (arXiv:2108.06616v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+She_M/0/1/0/all/0/1\">Mengkun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakath_D/0/1/0/all/0/1\">David Nakath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koser_K/0/1/0/all/0/1\">Kevin K&#xf6;ser</a>",
          "description": "Underwater cameras are typically placed behind glass windows to protect them\nfrom the water. Spherical glass, a dome port, is well suited for high water\npressures at great depth, allows for a large field of view, and avoids\nrefraction if a pinhole camera is positioned exactly at the sphere's center.\nAdjusting a real lens perfectly to the dome center is a challenging task, both\nin terms of how to actually guide the centering process (e.g. visual servoing)\nand how to measure the alignment quality, but also, how to mechanically perform\nthe alignment. Consequently, such systems are prone to being decentered by some\noffset, leading to challenging refraction patterns at the sphere that\ninvalidate the pinhole camera model. We show that the overall camera system\nbecomes an axial camera, even for thick domes as used for deep sea exploration\nand provide a non-iterative way to compute the center of refraction without\nrequiring knowledge of exact air, glass or water properties. We also analyze\nthe refractive geometry at the sphere, looking at effects such as forward- vs.\nbackward decentering, iso-refraction curves and obtain a 6th-degree polynomial\nequation for forward projection of 3D points in thin domes. We then propose a\npure underwater calibration procedure to estimate the decentering from multiple\nimages. This estimate can either be used during adjustment to guide the\nmechanical position of the lens, or can be considered in photogrammetric\nunderwater applications.",
          "link": "http://arxiv.org/abs/2108.06575",
          "publishedOn": "2021-08-17T01:54:48.842Z",
          "wordCount": 667,
          "title": "Refractive Geometry for Underwater Domes. (arXiv:2108.06575v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cahall_D/0/1/0/all/0/1\">Daniel E. Cahall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal C. Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathallah_Shaykh_H/0/1/0/all/0/1\">Hassan M. Fathallah-Shaykh</a>",
          "description": "Magnetic resonance imaging (MRI) is routinely used for brain tumor diagnosis,\ntreatment planning, and post-treatment surveillance. Recently, various models\nbased on deep neural networks have been proposed for the pixel-level\nsegmentation of tumors in brain MRIs. However, the structural variations,\nspatial dissimilarities, and intensity inhomogeneity in MRIs make segmentation\na challenging task. We propose a new end-to-end brain tumor segmentation\narchitecture based on U-Net that integrates Inception modules and dilated\nconvolutions into its contracting and expanding paths. This allows us to\nextract local structural as well as global contextual information. We performed\nsegmentation of glioma sub-regions, including tumor core, enhancing tumor, and\nwhole tumor using Brain Tumor Segmentation (BraTS) 2018 dataset. Our proposed\nmodel performed significantly better than the state-of-the-art U-Net-based\nmodel ($p<0.05$) for tumor core and whole tumor segmentation.",
          "link": "http://arxiv.org/abs/2108.06772",
          "publishedOn": "2021-08-17T01:54:48.836Z",
          "wordCount": 572,
          "title": "Dilated Inception U-Net (DIU-Net) for Brain Tumor Segmentation. (arXiv:2108.06772v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>",
          "description": "Unsupervised person re-identification (re-ID) has attracted increasing\nresearch interests because of its scalability and possibility for real-world\napplications. State-of-the-art unsupervised re-ID methods usually follow a\nclustering-based strategy, which generates pseudo labels by clustering and\nmaintains a memory to store instance features and represent the centroid of the\nclusters for contrastive learning. This approach suffers two problems. First,\nthe centroid generated by unsupervised learning may not be a perfect prototype.\nForcing images to get closer to the centroid emphasizes the result of\nclustering, which could accumulate clustering errors during iterations. Second,\nprevious methods utilize features obtained at different training iterations to\nrepresent one centroid, which is not consistent with the current training\nsample, since the features are not directly comparable. To this end, we propose\nan unsupervised re-ID approach with a stochastic learning strategy.\nSpecifically, we adopt a stochastic updated memory, where a random instance\nfrom a cluster is used to update the cluster-level memory for contrastive\nlearning. In this way, the relationship between randomly selected pair of\nimages are learned to avoid the training bias caused by unreliable pseudo\nlabels. The stochastic memory is also always up-to-date for classifying to keep\nthe consistency. Besides, to relieve the issue of camera variance, a unified\ndistance matrix is proposed during clustering, where the distance bias from\ndifferent camera domain is reduced and the variances of identities is\nemphasized.",
          "link": "http://arxiv.org/abs/2108.06938",
          "publishedOn": "2021-08-17T01:54:48.830Z",
          "wordCount": 658,
          "title": "Unsupervised Person Re-identification with Stochastic Training Strategy. (arXiv:2108.06938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peisheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>",
          "description": "Diabetic retinopathy (DR) is one of the most common eye conditions among\ndiabetic patients. However, vision loss occurs primarily in the late stages of\nDR, and the symptoms of visual impairment, ranging from mild to severe, can\nvary greatly, adding to the burden of diagnosis and treatment in clinical\npractice. Deep learning methods based on retinal images have achieved\nremarkable success in automatic DR grading, but most of them neglect that the\npresence of diabetes usually affects both eyes, and ophthalmologists usually\ncompare both eyes concurrently for DR diagnosis, leaving correlations between\nleft and right eyes unexploited. In this study, simulating the diagnostic\nprocess, we propose a two-stream binocular network to capture the subtle\ncorrelations between left and right eyes, in which, paired images of eyes are\nfed into two identical subnetworks separately during training. We design a\ncontrastive grading loss to learn binocular correlation for five-class DR\ndetection, which maximizes inter-class dissimilarity while minimizing the\nintra-class difference. Experimental results on the EyePACS dataset show the\nsuperiority of the proposed binocular model, outperforming monocular methods by\na large margin.",
          "link": "http://arxiv.org/abs/2108.06763",
          "publishedOn": "2021-08-17T01:54:48.824Z",
          "wordCount": 659,
          "title": "Two Eyes Are Better Than One: Exploiting Binocular Correlation for Diabetic Retinopathy Severity Grading. (arXiv:2108.06763v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berjon_D/0/1/0/all/0/1\">Daniel Berj&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuevas_C/0/1/0/all/0/1\">Carlos Cuevas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Narciso Garc&#xed;a</a>",
          "description": "Augmented reality applications are beginning to change the way sports are\nbroadcast, providing richer experiences and valuable insights to fans. The\nfirst step of augmented reality systems is camera calibration, possibly based\non detecting the line markings of the field of play. Most existing proposals\nfor line detection rely on edge detection and Hough transform, but optical\ndistortion and extraneous edges cause inaccurate or spurious detections of line\nmarkings. We propose a novel strategy to automatically and accurately segment\nline markings based on a stochastic watershed transform that is robust to\noptical distortions, since it makes no assumptions about line straightness, and\nis unaffected by the presence of players or the ball in the field of play.\nFirstly, the playing field as a whole is segmented completely eliminating the\nstands and perimeter boards. Then the line markings are extracted.\n\nThe strategy has been tested on a new and public database composed by 60\nannotated images from matches in five stadiums. The results obtained have\nproven that the proposed segmentation algorithm allows successful and precise\ndetection of most line mark pixels.",
          "link": "http://arxiv.org/abs/2108.06432",
          "publishedOn": "2021-08-17T01:54:48.808Z",
          "wordCount": 622,
          "title": "Soccer line mark segmentation with stochastic watershed transform. (arXiv:2108.06432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Annan Li</a>",
          "description": "Fine-grained action recognition is attracting increasing attention due to the\nemerging demand of specific action understanding in real-world applications,\nwhereas the data of rare fine-grained categories is very limited. Therefore, we\npropose the few-shot fine-grained action recognition problem, aiming to\nrecognize novel fine-grained actions with only few samples given for each\nclass. Although progress has been made in coarse-grained actions, existing\nfew-shot recognition methods encounter two issues handling fine-grained\nactions: the inability to capture subtle action details and the inadequacy in\nlearning from data with low inter-class variance. To tackle the first issue, a\nhuman vision inspired bidirectional attention module (BAM) is proposed.\nCombining top-down task-driven signals with bottom-up salient stimuli, BAM\ncaptures subtle action details by accurately highlighting informative\nspatio-temporal regions. To address the second issue, we introduce contrastive\nmeta-learning (CML). Compared with the widely adopted ProtoNet-based method,\nCML generates more discriminative video representations for low inter-class\nvariance data, since it makes full use of potential contrastive pairs in each\ntraining episode. Furthermore, to fairly compare different models, we establish\nspecific benchmark protocols on two large-scale fine-grained action recognition\ndatasets. Extensive experiments show that our method consistently achieves\nstate-of-the-art performance across evaluated tasks.",
          "link": "http://arxiv.org/abs/2108.06647",
          "publishedOn": "2021-08-17T01:54:48.802Z",
          "wordCount": 642,
          "title": "Few-Shot Fine-Grained Action Recognition via Bidirectional Attention and Contrastive Meta-Learning. (arXiv:2108.06647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiangeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Charles R. Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>",
          "description": "In autonomous driving, a LiDAR-based object detector should perform reliably\nat different geographic locations and under various weather conditions. While\nrecent 3D detection research focuses on improving performance within a single\ndomain, our study reveals that the performance of modern detectors can drop\ndrastically cross-domain. In this paper, we investigate unsupervised domain\nadaptation (UDA) for LiDAR-based 3D object detection. On the Waymo Domain\nAdaptation dataset, we identify the deteriorating point cloud quality as the\nroot cause of the performance drop. To address this issue, we present Semantic\nPoint Generation (SPG), a general approach to enhance the reliability of LiDAR\ndetectors against domain shifts. Specifically, SPG generates semantic points at\nthe predicted foreground regions and faithfully recovers missing parts of the\nforeground objects, which are caused by phenomena such as occlusions, low\nreflectance or weather interference. By merging the semantic points with the\noriginal points, we obtain an augmented point cloud, which can be directly\nconsumed by modern LiDAR-based detectors. To validate the wide applicability of\nSPG, we experiment with two representative detectors, PointPillars and PV-RCNN.\nOn the UDA task, SPG significantly improves both detectors across all object\ncategories of interest and at all difficulty levels. SPG can also benefit\nobject detection in the original domain. On the Waymo Open Dataset and KITTI,\nSPG improves 3D detection results of these two methods across all categories.\nCombined with PV-RCNN, SPG achieves state-of-the-art 3D detection results on\nKITTI.",
          "link": "http://arxiv.org/abs/2108.06709",
          "publishedOn": "2021-08-17T01:54:48.794Z",
          "wordCount": 682,
          "title": "SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation. (arXiv:2108.06709v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1\">Sara Abdali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilescu_M/0/1/0/all/0/1\">M. Alex O. Vasilescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1\">Evangelos E. Papalexakis</a>",
          "description": "Generative neural network architectures such as GANs, may be used to generate\nsynthetic instances to compensate for the lack of real data. However, they may\nbe employed to create media that may cause social, political or economical\nupheaval. One emerging media is \"Deepfake\".Techniques that can discriminate\nbetween such media is indispensable. In this paper, we propose a modified\nmultilinear (tensor) method, a combination of linear and multilinear\nregressions for representing fake and real data. We test our approach by\nrepresenting Deepfakes with our modified multilinear (tensor) approach and\nperform SVM classification with encouraging results.",
          "link": "http://arxiv.org/abs/2108.06702",
          "publishedOn": "2021-08-17T01:54:48.788Z",
          "wordCount": 532,
          "title": "Deepfake Representation with Multilinear Regression. (arXiv:2108.06702v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1\">Jiayao Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sifan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yubo Cui</a>",
          "description": "3D single object tracking is a key issue for robotics. In this paper, we\npropose a transformer module called Point-Track-Transformer (PTT) for point\ncloud-based 3D single object tracking. PTT module contains three blocks for\nfeature embedding, position encoding, and self-attention feature computation.\nFeature embedding aims to place features closer in the embedding space if they\nhave similar semantic information. Position encoding is used to encode\ncoordinates of point clouds into high dimension distinguishable features.\nSelf-attention generates refined attention features by computing attention\nweights. Besides, we embed the PTT module into the open-source state-of-the-art\nmethod P2B to construct PTT-Net. Experiments on the KITTI dataset reveal that\nour PTT-Net surpasses the state-of-the-art by a noticeable margin (~10\\%).\nAdditionally, PTT-Net could achieve real-time performance (~40FPS) on NVIDIA\n1080Ti GPU. Our code is open-sourced for the robotics community at\nhttps://github.com/shanjiayao/PTT.",
          "link": "http://arxiv.org/abs/2108.06455",
          "publishedOn": "2021-08-17T01:54:48.783Z",
          "wordCount": 584,
          "title": "PTT: Point-Track-Transformer Module for 3D Single Object Tracking in Point Clouds. (arXiv:2108.06455v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.00996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1\">Pedro C. Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1\">Jo&#xe3;o Ribeiro Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_M/0/1/0/all/0/1\">Mohsen Saffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sequeira_A/0/1/0/all/0/1\">Ana F. Sequeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">Jaime S. Cardoso</a>",
          "description": "The recent Covid-19 pandemic and the fact that wearing masks in public is now\nmandatory in several countries, created challenges in the use of face\nrecognition systems (FRS). In this work, we address the challenge of masked\nface recognition (MFR) and focus on evaluating the verification performance in\nFRS when verifying masked vs unmasked faces compared to verifying only unmasked\nfaces. We propose a methodology that combines the traditional triplet loss and\nthe mean squared error (MSE) intending to improve the robustness of an MFR\nsystem in the masked-unmasked comparison mode. The results obtained by our\nproposed method show improvements in a detailed step-wise ablation study. The\nconducted study showed significant performance gains induced by our proposed\ntraining paradigm and modified triplet loss on two evaluation databases.",
          "link": "http://arxiv.org/abs/2108.00996",
          "publishedOn": "2021-08-17T01:54:48.777Z",
          "wordCount": 663,
          "title": "My Eyes Are Up Here: Promoting Focus on Uncovered Regions in Masked Face Recognition. (arXiv:2108.00996v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huanqiang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingfu Zhang</a>",
          "description": "This paper investigates the problem of reconstructing hyperspectral (HS)\nimages from single RGB images captured by commercial cameras, \\textbf{without}\nusing paired HS and RGB images during training. To tackle this challenge, we\npropose a new lightweight and end-to-end learning-based framework.\nSpecifically, on the basis of the intrinsic imaging degradation model of RGB\nimages from HS images, we progressively spread the differences between input\nRGB images and re-projected RGB images from recovered HS images via effective\nunsupervised camera spectral response function estimation. To enable the\nlearning without paired ground-truth HS images as supervision, we adopt the\nadversarial learning manner and boost it with a simple yet effective\n$\\mathcal{L}_1$ gradient clipping scheme. Besides, we embed the semantic\ninformation of input RGB images to locally regularize the unsupervised\nlearning, which is expected to promote pixels with identical semantics to have\nconsistent spectral signatures. In addition to conducting quantitative\nexperiments over two widely-used datasets for HS image reconstruction from\nsynthetic RGB images, we also evaluate our method by applying recovered HS\nimages from real RGB images to HS-based visual tracking. Extensive results show\nthat our method significantly outperforms state-of-the-art unsupervised methods\nand even exceeds the latest supervised method under some settings. The source\ncode is public available at\nhttps://github.com/zbzhzhy/Unsupervised-Spectral-Reconstruction.",
          "link": "http://arxiv.org/abs/2108.06659",
          "publishedOn": "2021-08-17T01:54:48.762Z",
          "wordCount": 651,
          "title": "Semantic-embedded Unsupervised Spectral Reconstruction from Single RGB Images in the Wild. (arXiv:2108.06659v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>",
          "description": "Three popular feature descriptors of computer vision such as SIFT, SURF, and\nORB compared and evaluated. The number of correct features extracted and\nmatched for the original hand hygiene pose-Rub hands palm to palm image and\nrotated image. An accuracy score calculated based on the total number of\nmatches and the correct number of matches produced. The experiment demonstrated\nthat ORB algorithm outperforms by giving the high number of correct matches in\nless amount of time. ORB feature detection technique applied over handwashing\nvideo recordings for feature extraction and hand hygiene pose classification as\na future work. OpenCV utilized to apply the algorithms within python scripts.",
          "link": "http://arxiv.org/abs/2108.06537",
          "publishedOn": "2021-08-17T01:54:48.756Z",
          "wordCount": 534,
          "title": "Feature Identification and Matching for Hand Hygiene Pose. (arXiv:2108.06537v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiahui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Bin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yueqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwen Yu</a>",
          "description": "Surface defect detection plays an increasingly important role in\nmanufacturing industry to guarantee the product quality. Many deep learning\nmethods have been widely used in surface defect detection tasks, and have been\nproven to perform well in defects classification and location. However, deep\nlearning-based detection methods often require plenty of data for training,\nwhich fail to apply to the real industrial scenarios since the distribution of\ndefect categories is often imbalanced. In other words, common defect classes\nhave many samples but rare defect classes have extremely few samples, and it is\ndifficult for these methods to well detect rare defect classes. To solve the\nimbalanced distribution problem, in this paper we propose TL-SDD: a novel\nTransfer Learning-based method for Surface Defect Detection. First, we adopt a\ntwo-phase training scheme to transfer the knowledge from common defect classes\nto rare defect classes. Second, we propose a novel Metric-based Surface Defect\nDetection (M-SDD) model. We design three modules for this model: (1) feature\nextraction module: containing feature fusion which combines high-level semantic\ninformation with low-level structural information. (2) feature reweighting\nmodule: transforming examples to a reweighting vector that indicates the\nimportance of features. (3) distance metric module: learning a metric space in\nwhich defects are classified by computing distances to representations of each\ncategory. Finally, we validate the performance of our proposed method on a real\ndataset including surface defects of aluminum profiles. Compared to the\nbaseline methods, the performance of our proposed method has improved by up to\n11.98% for rare defect classes.",
          "link": "http://arxiv.org/abs/2108.06939",
          "publishedOn": "2021-08-17T01:54:48.751Z",
          "wordCount": 704,
          "title": "TL-SDD: A Transfer Learning-Based Method for Surface Defect Detection with Few Samples. (arXiv:2108.06939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sehun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_H/0/1/0/all/0/1\">Hyunjun Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>",
          "description": "Most recent studies on detecting and localizing temporal anomalies have\nmainly employed deep neural networks to learn the normal patterns of temporal\ndata in an unsupervised manner. Unlike them, the goal of our work is to fully\nutilize instance-level (or weak) anomaly labels, which only indicate whether\nany anomalous events occurred or not in each instance of temporal data. In this\npaper, we present WETAS, a novel framework that effectively identifies\nanomalous temporal segments (i.e., consecutive time points) in an input\ninstance. WETAS learns discriminative features from the instance-level labels\nso that it infers the sequential order of normal and anomalous segments within\neach instance, which can be used as a rough segmentation mask. Based on the\ndynamic time warping (DTW) alignment between the input instance and its\nsegmentation mask, WETAS obtains the result of temporal segmentation, and\nsimultaneously, it further enhances itself by using the mask as additional\nsupervision. Our experiments show that WETAS considerably outperforms other\nbaselines in terms of the localization of temporal anomalies, and also it\nprovides more informative results than point-level detection methods.",
          "link": "http://arxiv.org/abs/2108.06816",
          "publishedOn": "2021-08-17T01:54:48.745Z",
          "wordCount": 636,
          "title": "Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping. (arXiv:2108.06816v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanjun Wu</a>",
          "description": "Automatic security inspection using computer vision technology is a\nchallenging task in real-world scenarios due to various factors, including\nintra-class variance, class imbalance, and occlusion. Most of the previous\nmethods rarely solve the cases that the prohibited items are deliberately\nhidden in messy objects due to the lack of large-scale datasets, restricted\ntheir applications in real-world scenarios. Towards real-world prohibited item\ndetection, we collect a large-scale dataset, named as PIDray, which covers\nvarious cases in real-world scenarios for prohibited item detection, especially\nfor deliberately hidden items. With an intensive amount of effort, our dataset\ncontains $12$ categories of prohibited items in $47,677$ X-ray images with\nhigh-quality annotated segmentation masks and bounding boxes. To the best of\nour knowledge, it is the largest prohibited items detection dataset to date.\nMeanwhile, we design the selective dense attention network (SDANet) to\nconstruct a strong baseline, which consists of the dense attention module and\nthe dependency refinement module. The dense attention module formed by the\nspatial and channel-wise dense attentions, is designed to learn the\ndiscriminative features to boost the performance. The dependency refinement\nmodule is used to exploit the dependencies of multi-scale features. Extensive\nexperiments conducted on the collected PIDray dataset demonstrate that the\nproposed method performs favorably against the state-of-the-art methods,\nespecially for detecting the deliberately hidden items.",
          "link": "http://arxiv.org/abs/2108.07020",
          "publishedOn": "2021-08-17T01:54:48.731Z",
          "wordCount": 661,
          "title": "Towards Real-World Prohibited Item Detection: A Large-Scale X-ray Benchmark. (arXiv:2108.07020v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jinkun Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhijie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "3D object detection is an important task in computer vision. Most existing\nmethods require a large number of high-quality 3D annotations, which are\nexpensive to collect. Especially for outdoor scenes, the problem becomes more\nsevere due to the sparseness of the point cloud and the complexity of urban\nscenes. Semi-supervised learning is a promising technique to mitigate the data\nannotation issue. Inspired by this, we propose a novel semi-supervised\nframework based on pseudo-labeling for outdoor 3D object detection tasks. We\ndesign the Adaptive Class Confidence Selection module (ACCS) to generate\nhigh-quality pseudo-labels. Besides, we propose Holistic Point Cloud\nAugmentation (HPCA) for unlabeled data to improve robustness. Experiments on\nthe KITTI benchmark demonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.06649",
          "publishedOn": "2021-08-17T01:54:48.725Z",
          "wordCount": 567,
          "title": "Semi-supervised 3D Object Detection via Adaptive Pseudo-Labeling. (arXiv:2108.06649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tantawy_D/0/1/0/all/0/1\">Dina Tantawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahran_M/0/1/0/all/0/1\">Mohamed Zahran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wassal_A/0/1/0/all/0/1\">Amr Wassal</a>",
          "description": "Since its invention, Generative adversarial networks (GANs) have shown\noutstanding results in many applications. Generative Adversarial Networks are\npowerful yet, resource-hungry deep-learning models. Their main difference from\nordinary deep learning models is the nature of their output. For example, GAN\noutput can be a whole image versus other models detecting objects or\nclassifying images. Thus, the architecture and numeric precision of the network\naffect the quality and speed of the solution. Hence, accelerating GANs is\npivotal. Accelerating GANs can be classified into three main tracks: (1) Memory\ncompression, (2) Computation optimization, and (3) Data-flow optimization.\nBecause data transfer is the main source of energy usage, memory compression\nleads to the most savings. Thus, in this paper, we survey memory compression\ntechniques for CNN-Based GANs. Additionally, the paper summarizes opportunities\nand challenges in GANs acceleration and suggests open research problems to be\nfurther investigated.",
          "link": "http://arxiv.org/abs/2108.06626",
          "publishedOn": "2021-08-17T01:54:48.690Z",
          "wordCount": 590,
          "title": "A Survey on GAN Acceleration Using Memory Compression Technique. (arXiv:2108.06626v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with a limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-08-17T01:54:48.668Z",
          "wordCount": 608,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>",
          "description": "Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled\nsource domain and an unsupervised loss in an unlabeled target domain, which\noften faces more severe overfitting (than classical supervised learning) as the\nsupervised source loss has clear domain gap and the unsupervised target loss is\noften noisy due to the lack of annotations. This paper presents RDA, a robust\ndomain adaptation technique that introduces adversarial attacking to mitigate\noverfitting in UDA. We achieve robust domain adaptation by a novel Fourier\nadversarial attacking (FAA) method that allows large magnitude of perturbation\nnoises but has minimal modification of image semantics, the former is critical\nto the effectiveness of its generated adversarial samples due to the existence\nof 'domain gaps'. Specifically, FAA decomposes images into multiple frequency\ncomponents (FCs) and generates adversarial samples by just perturbating certain\nFCs that capture little semantic information. With FAA-generated samples, the\ntraining can continue the 'random walk' and drift into an area with a flat loss\nlandscape, leading to more robust domain adaptation. Extensive experiments over\nmultiple domain adaptation tasks show that RDA can work with different computer\nvision tasks with superior performance.",
          "link": "http://arxiv.org/abs/2106.02874",
          "publishedOn": "2021-08-17T01:54:48.662Z",
          "wordCount": 670,
          "title": "RDA: Robust Domain Adaptation via Fourier Adversarial Attacking. (arXiv:2106.02874v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pohn_D/0/1/0/all/0/1\">Daniela P&#xf6;hn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillmann_P/0/1/0/all/0/1\">Peter Hillmann</a>",
          "description": "With the pandemic of COVID-19, people around the world increasingly work from\nhome. Each natural person typically has several digital identities with\ndifferent associated information. During the last years, various identity and\naccess management approaches have gained attraction, helping for example to\naccess other organization's services within trust boundaries. The resulting\nheterogeneity creates a high complexity to differentiate between these\napproaches and scenarios as participating entity; combining them is even\nharder. Last but not least, various actors have a different understanding or\nperspective of the terms, like 'service', in this context. Our paper describes\na reference service with standard components in generic federated identity\nmanagement. This is utilized with modern Enterprise Architecture using the\nframework ArchiMate. The proposed universal federated identity management\nservice model (FIMSM) is applied to describe various federated identity\nmanagement scenarios in a generic service-oriented way. The presented reference\ndesign is approved in multiple aspects and is easily applicable in numerous\nscenarios.",
          "link": "http://arxiv.org/abs/2108.06701",
          "publishedOn": "2021-08-17T01:54:48.603Z",
          "wordCount": 649,
          "title": "Reference Service Model for Federated Identity Management. (arXiv:2108.06701v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1\">Niamh Belton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1\">Aonghus Lawlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1\">Kathleen M. Curran</a>",
          "description": "Noisy data present in medical imaging datasets can often aid the development\nof robust models that are equipped to handle real-world data. However, if the\nbad data contains insufficient anatomical information, it can have a severe\nnegative effect on the model's performance. We propose a novel methodology\nusing a semi-supervised Siamese network to identify bad data. This method\nrequires only a small pool of 'reference' medical images to be reviewed by a\nnon-expert human to ensure the major anatomical structures are present in the\nField of View. The model trains on this reference set and identifies bad data\nby using the Siamese network to compute the distance between the reference set\nand all other medical images in the dataset. This methodology achieves an Area\nUnder the Curve (AUC) of 0.989 for identifying bad data. Code will be available\nat https://git.io/JYFuV.",
          "link": "http://arxiv.org/abs/2108.07130",
          "publishedOn": "2021-08-17T01:54:48.582Z",
          "wordCount": 594,
          "title": "Semi-Supervised Siamese Network for Identifying Bad Data in Medical Imaging Datasets. (arXiv:2108.07130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rundi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changxi Zheng</a>",
          "description": "Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation --- describing a shape\nas a sequence of computer-aided design (CAD) operations. Unlike meshes and\npoint clouds, CAD models encode the user creation process of 3D shapes, widely\nused in numerous industrial and engineering design tasks. However, the\nsequential and irregular structure of CAD operations poses significant\nchallenges for existing 3D generative models. Drawing an analogy between CAD\noperations and natural language, we propose a CAD generative network based on\nthe Transformer. We demonstrate the performance of our model for both shape\nautoencoding and random shape generation. To train our network, we create a new\nCAD dataset consisting of 178,238 models and their CAD construction sequences.\nWe have made this dataset publicly available to promote future research on this\ntopic.",
          "link": "http://arxiv.org/abs/2105.09492",
          "publishedOn": "2021-08-17T01:54:48.561Z",
          "wordCount": 649,
          "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arai_H/0/1/0/all/0/1\">Hayato Arai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onga_Y/0/1/0/all/0/1\">Yuto Onga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikuta_K/0/1/0/all/0/1\">Kumpei Ikuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chayama_Y/0/1/0/all/0/1\">Yusuke Chayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oishi_K/0/1/0/all/0/1\">Kenichi Oishi</a>",
          "description": "To build a robust and practical content-based image retrieval (CBIR) system\nthat is applicable to a clinical brain MRI database, we propose a new framework\n-- Disease-oriented image embedding with pseudo-scanner standardization\n(DI-PSS) -- that consists of two core techniques, data harmonization and a\ndimension reduction algorithm. Our DI-PSS uses skull stripping and\nCycleGAN-based image transformations that map to a standard brain followed by\ntransformation into a brain image taken with a given reference scanner. Then,\nour 3D convolutioinal autoencoders (3D-CAE) with deep metric learning acquires\na low-dimensional embedding that better reflects the characteristics of the\ndisease. The effectiveness of our proposed framework was tested on the\nT1-weighted MRIs selected from the Alzheimer's Disease Neuroimaging Initiative\nand the Parkinson's Progression Markers Initiative. We confirmed that our PSS\ngreatly reduced the variability of low-dimensional embeddings caused by\ndifferent scanner and datasets. Compared with the baseline condition, our PSS\nreduced the variability in the distance from Alzheimer's disease (AD) to\nclinically normal (CN) and Parkinson disease (PD) cases by 15.8-22.6% and\n18.0-29.9%, respectively. These properties allow DI-PSS to generate lower\ndimensional representations that are more amenable to disease classification.\nIn AD and CN classification experiments based on spectral clustering, PSS\nimproved the average accuracy and macro-F1 by 6.2% and 10.7%, respectively.\nGiven the potential of the DI-PSS for harmonizing images scanned by MRI\nscanners that were not used to scan the training data, we expect that the\nDI-PSS is suitable for application to a large number of legacy MRIs scanned in\nheterogeneous environments.",
          "link": "http://arxiv.org/abs/2108.06518",
          "publishedOn": "2021-08-17T01:54:48.532Z",
          "wordCount": 707,
          "title": "Disease-oriented image embedding with pseudo-scanner standardization for content-based image retrieval on 3D brain MRI. (arXiv:2108.06518v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kaiming He</a>",
          "description": "This paper does not describe a novel method. Instead, it studies a\nstraightforward, incremental, yet must-know baseline given the recent progress\nin computer vision: self-supervised learning for Vision Transformers (ViT).\nWhile the training recipes for standard convolutional networks have been highly\nmature and robust, the recipes for ViT are yet to be built, especially in the\nself-supervised scenarios where training becomes more challenging. In this\nwork, we go back to basics and investigate the effects of several fundamental\ncomponents for training self-supervised ViT. We observe that instability is a\nmajor issue that degrades accuracy, and it can be hidden by apparently good\nresults. We reveal that these results are indeed partial failure, and they can\nbe improved when training is made more stable. We benchmark ViT results in MoCo\nv3 and several other self-supervised frameworks, with ablations in various\naspects. We discuss the currently positive evidence as well as challenges and\nopen questions. We hope that this work will provide useful data points and\nexperience for future research.",
          "link": "http://arxiv.org/abs/2104.02057",
          "publishedOn": "2021-08-17T01:54:48.526Z",
          "wordCount": 660,
          "title": "An Empirical Study of Training Self-Supervised Vision Transformers. (arXiv:2104.02057v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Ailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yanfei Zhong</a>",
          "description": "For high spatial resolution (HSR) remote sensing images, bitemporal\nsupervised learning always dominates change detection using many pairwise\nlabeled bitemporal images. However, it is very expensive and time-consuming to\npairwise label large-scale bitemporal HSR remote sensing images. In this paper,\nwe propose single-temporal supervised learning (STAR) for change detection from\na new perspective of exploiting object changes in unpaired images as\nsupervisory signals. STAR enables us to train a high-accuracy change detector\nonly using \\textbf{unpaired} labeled images and generalize to real-world\nbitemporal images. To evaluate the effectiveness of STAR, we design a simple\nyet effective change detector called ChangeStar, which can reuse any deep\nsemantic segmentation architecture by the ChangeMixin module. The comprehensive\nexperimental results show that ChangeStar outperforms the baseline with a large\nmargin under single-temporal supervision and achieves superior performance\nunder bitemporal supervision. Code is available at\nhttps://github.com/Z-Zheng/ChangeStar",
          "link": "http://arxiv.org/abs/2108.07002",
          "publishedOn": "2021-08-17T01:54:48.515Z",
          "wordCount": 591,
          "title": "Change is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery. (arXiv:2108.07002v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shihua/0/1/0/all/0/1\">Shihua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1\">Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhichao/0/1/0/all/0/1\">Zhichao</a>, Lu, Ran, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng/0/1/0/all/0/1\">Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng/0/1/0/all/0/1\">Cheng</a>, He",
          "description": "Recent advancements in deep neural networks have made remarkable\nleap-forwards in dense image prediction. However, the issue of feature\nalignment remains as neglected by most existing approaches for simplicity.\nDirect pixel addition between upsampled and local features leads to feature\nmaps with misaligned contexts that, in turn, translate to mis-classifications\nin prediction, especially on object boundaries. In this paper, we propose a\nfeature alignment module that learns transformation offsets of pixels to\ncontextually align upsampled higher-level features; and another feature\nselection module to emphasize the lower-level features with rich spatial\ndetails. We then integrate these two modules in a top-down pyramidal\narchitecture and present the Feature-aligned Pyramid Network (FaPN). Extensive\nexperimental evaluations on four dense prediction tasks and four datasets have\ndemonstrated the efficacy of FaPN, yielding an overall improvement of 1.2 - 2.6\npoints in AP / mIoU over FPN when paired with Faster / Mask R-CNN. In\nparticular, our FaPN achieves the state-of-the-art of 56.7% mIoU on ADE20K when\nintegrated within Mask-Former. The code is available from\nhttps://github.com/EMI-Group/FaPN.",
          "link": "http://arxiv.org/abs/2108.07058",
          "publishedOn": "2021-08-17T01:54:48.487Z",
          "wordCount": 609,
          "title": "FaPN: Feature-aligned Pyramid Network for Dense Image Prediction. (arXiv:2108.07058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jia Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1\">Andre Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jingxuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuli_M/0/1/0/all/0/1\">Mathieu Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "Neural Architecture Search (NAS) has shifted network design from using human\nintuition to leveraging search algorithms guided by evaluation metrics. We\nstudy channel size optimization in convolutional neural networks (CNN) and\nidentify the role it plays in model accuracy and complexity. Current channel\nsize selection methods are generally limited by discrete sample spaces while\nsuffering from manual iteration and simple heuristics. To solve this, we\nintroduce an efficient dynamic scaling algorithm -- CONet -- that automatically\noptimizes channel sizes across network layers for a given CNN. Two metrics --\n``\\textit{Rank}\" and \"\\textit{Rank Average Slope}\" -- are introduced to\nidentify the information accumulated in training. The algorithm dynamically\nscales channel sizes up or down over a fixed searching phase. We conduct\nexperiments on CIFAR10/100 and ImageNet datasets and show that CONet can find\nefficient and accurate architectures searched in ResNet, DARTS, and DARTS+\nspaces that outperform their baseline models.",
          "link": "http://arxiv.org/abs/2108.06822",
          "publishedOn": "2021-08-17T01:54:48.481Z",
          "wordCount": 609,
          "title": "CONet: Channel Optimization for Convolutional Neural Networks. (arXiv:2108.06822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junheum Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chul Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>",
          "description": "We propose a novel video frame interpolation algorithm based on asymmetric\nbilateral motion estimation (ABME), which synthesizes an intermediate frame\nbetween two input frames. First, we predict symmetric bilateral motion fields\nto interpolate an anchor frame. Second, we estimate asymmetric bilateral\nmotions fields from the anchor frame to the input frames. Third, we use the\nasymmetric fields to warp the input frames backward and reconstruct the\nintermediate frame. Last, to refine the intermediate frame, we develop a new\nsynthesis network that generates a set of dynamic filters and a residual frame\nusing local and global information. Experimental results show that the proposed\nalgorithm achieves excellent performance on various datasets. The source codes\nand pretrained models are available at https://github.com/JunHeum/ABME.",
          "link": "http://arxiv.org/abs/2108.06815",
          "publishedOn": "2021-08-17T01:54:48.441Z",
          "wordCount": 559,
          "title": "Asymmetric Bilateral Motion Estimation for Video Frame Interpolation. (arXiv:2108.06815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuyun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yufei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper aims to explain adversarial attacks in terms of how adversarial\nperturbations contribute to the attacking task. We estimate attributions of\ndifferent image regions to the decrease of the attacking cost based on the\nShapley value. We define and quantify interactions among adversarial\nperturbation pixels, and decompose the entire perturbation map into relatively\nindependent perturbation components. The decomposition of the perturbation map\nshows that adversarially-trained DNNs have more perturbation components in the\nforeground than normally-trained DNNs. Moreover, compared to the\nnormally-trained DNN, the adversarially-trained DNN have more components which\nmainly decrease the score of the true category. Above analyses provide new\ninsights into the understanding of adversarial attacks.",
          "link": "http://arxiv.org/abs/2108.06895",
          "publishedOn": "2021-08-17T01:54:48.429Z",
          "wordCount": 555,
          "title": "Interpreting Attributions and Interactions of Adversarial Attacks. (arXiv:2108.06895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "We study how to evaluate the quantitative information content of a region\nwithin an image for a particular label. To this end, we bridge class activation\nmaps with information theory. We develop an informative class activation map\n(infoCAM). Given a classification task, infoCAM depict how to accumulate\ninformation of partial regions to that of the entire image toward a label.\nThus, we can utilise infoCAM to locate the most informative features for a\nlabel. When applied to an image classification task, infoCAM performs better\nthan the traditional classification map in the weakly supervised object\nlocalisation task. We achieve state-of-the-art results on Tiny-ImageNet.",
          "link": "http://arxiv.org/abs/2106.10472",
          "publishedOn": "2021-08-17T01:54:48.257Z",
          "wordCount": 564,
          "title": "Informative Class Activation Maps. (arXiv:2106.10472v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1\">Jeova F. S. Rocha Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felzenszwalb_P/0/1/0/all/0/1\">Pedro Felzenszwalb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_M/0/1/0/all/0/1\">Marilyn Vazquez</a>",
          "description": "Image segmentation algorithms often depend on appearance models that\ncharacterize the distribution of pixel values in different image regions. We\ndescribe a new approach for estimating appearance models directly from an\nimage, without explicit consideration of the pixels that make up each region.\nOur approach is based on novel algebraic expressions that relate local image\nstatistics to the appearance of spatially coherent regions. We describe two\nalgorithms that can use the aforementioned algebraic expressions to estimate\nappearance models directly from an image. The first algorithm solves a system\nof linear and quadratic equations using a least squares formulation. The second\nalgorithm is a spectral method based on an eigenvector computation. We present\nexperimental results that demonstrate the proposed methods work well in\npractice and lead to effective image segmentation algorithms.",
          "link": "http://arxiv.org/abs/2102.11121",
          "publishedOn": "2021-08-17T01:54:48.228Z",
          "wordCount": 597,
          "title": "Direct Estimation of Appearance Models for Segmentation. (arXiv:2102.11121v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jihoon Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuu_C/0/1/0/all/0/1\">Cheng-hsin Wuu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsuan-ru Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>",
          "description": "We contribute HAA500, a manually annotated human-centric atomic action\ndataset for action recognition on 500 classes with over 591K labeled frames. To\nminimize ambiguities in action classification, HAA500 consists of highly\ndiversified classes of fine-grained atomic actions, where only consistent\nactions fall under the same label, e.g., \"Baseball Pitching\" vs \"Free Throw in\nBasketball\". Thus HAA500 is different from existing atomic action datasets,\nwhere coarse-grained atomic actions were labeled with coarse action-verbs such\nas \"Throw\". HAA500 has been carefully curated to capture the precise movement\nof human figures with little class-irrelevant motions or spatio-temporal label\nnoises. The advantages of HAA500 are fourfold: 1) human-centric actions with a\nhigh average of 69.7% detectable joints for the relevant human poses; 2) high\nscalability since adding a new class can be done under 20-60 minutes; 3)\ncurated videos capturing essential elements of an atomic action without\nirrelevant frames; 4) fine-grained atomic action classes. Our extensive\nexperiments including cross-data validation using datasets collected in the\nwild demonstrate the clear benefits of human-centric and atomic characteristics\nof HAA500, which enable training even a baseline deep learning model to improve\nprediction by attending to atomic human poses. We detail the HAA500 dataset\nstatistics and collection methodology and compare quantitatively with existing\naction recognition datasets.",
          "link": "http://arxiv.org/abs/2009.05224",
          "publishedOn": "2021-08-17T01:54:48.208Z",
          "wordCount": 685,
          "title": "HAA500: Human-Centric Atomic Action Dataset with Curated Videos. (arXiv:2009.05224v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>",
          "description": "Generalized zero-shot learning (GZSL) aims to classify samples under the\nassumption that some classes are not observable during training. To bridge the\ngap between the seen and unseen classes, most GZSL methods attempt to associate\nthe visual features of seen classes with attributes or to generate unseen\nsamples directly. Nevertheless, the visual features used in the prior\napproaches do not necessarily encode semantically related information that the\nshared attributes refer to, which degrades the model generalization to unseen\nclasses. To address this issue, in this paper, we propose a novel semantics\ndisentangling framework for the generalized zero-shot learning task (SDGZSL),\nwhere the visual features of unseen classes are firstly estimated by a\nconditional VAE and then factorized into semantic-consistent and\nsemantic-unrelated latent vectors. In particular, a total correlation penalty\nis applied to guarantee the independence between the two factorized\nrepresentations, and the semantic consistency of which is measured by the\nderived relation network. Extensive experiments conducted on four GZSL\nbenchmark datasets have evidenced that the semantic-consistent features\ndisentangled by the proposed SDGZSL are more generalizable in tasks of\ncanonical and generalized zero-shot learning. Our source code is available at\nhttps://github.com/uqzhichen/SDGZSL.",
          "link": "http://arxiv.org/abs/2101.07978",
          "publishedOn": "2021-08-17T01:54:48.190Z",
          "wordCount": 682,
          "title": "Semantics Disentangling for Generalized Zero-Shot Learning. (arXiv:2101.07978v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.06814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>",
          "description": "As billions of personal data being shared through social media and network,\nthe data privacy and security have drawn an increasing attention. Several\nattempts have been made to alleviate the leakage of identity information from\nface photos, with the aid of, e.g., image obfuscation techniques. However, most\nof the present results are either perceptually unsatisfactory or ineffective\nagainst face recognition systems. Our goal in this paper is to develop a\ntechnique that can encrypt the personal photos such that they can protect users\nfrom unauthorized face recognition systems but remain visually identical to the\noriginal version for human beings. To achieve this, we propose a targeted\nidentity-protection iterative method (TIP-IM) to generate adversarial identity\nmasks which can be overlaid on facial images, such that the original identities\ncan be concealed without sacrificing the visual quality. Extensive experiments\ndemonstrate that TIP-IM provides 95\\%+ protection success rate against various\nstate-of-the-art face recognition models under practical test scenarios.\nBesides, we also show the practical and effective applicability of our method\non a commercial API service.",
          "link": "http://arxiv.org/abs/2003.06814",
          "publishedOn": "2021-08-17T01:54:48.183Z",
          "wordCount": 659,
          "title": "Towards Face Encryption by Generating Adversarial Identity Masks. (arXiv:2003.06814v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1\">Kyungseo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Computer-aided diagnosis has recently received attention for its advantage of\nlow cost and time efficiency. Although deep learning played a major role in the\nrecent success of acne detection, there are still several challenges such as\ncolor shift by inconsistent illumination, variation in scales, and high density\ndistribution. To address these problems, we propose an acne detection network\nwhich consists of three components, specifically: Composite Feature Refinement,\nDynamic Context Enhancement, and Mask-Aware Multi-Attention. First, Composite\nFeature Refinement integrates semantic information and fine details to enrich\nfeature representation, which mitigates the adverse impact of imbalanced\nillumination. Then, Dynamic Context Enhancement controls different receptive\nfields of multi-scale features for context enhancement to handle scale\nvariation. Finally, Mask-Aware Multi-Attention detects densely arranged and\nsmall acne by suppressing uninformative regions and highlighting probable acne\nregions. Experiments are performed on acne image dataset ACNE04 and natural\nimage dataset PASCAL VOC 2007. We demonstrate how our method achieves the\nstate-of-the-art result on ACNE04 and competitive performance with previous\nstate-of-the-art methods on the PASCAL VOC 2007.",
          "link": "http://arxiv.org/abs/2105.14891",
          "publishedOn": "2021-08-17T01:54:48.170Z",
          "wordCount": 641,
          "title": "ACNet: Mask-Aware Attention with Dynamic Context Enhancement for Robust Acne Detection. (arXiv:2105.14891v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Existing methods for arbitrary-shaped text detection in natural scenes face\ntwo critical issues, i.e., 1) fracture detections at the gaps in a text\ninstance; and 2) inaccurate detections of arbitrary-shaped text instances with\ndiverse background context. To address these issues, we propose a novel method\nnamed Intra- and Inter-Instance Collaborative Learning (I3CL). Specifically, to\naddress the first issue, we design an effective convolutional module with\nmultiple receptive fields, which is able to collaboratively learn better\ncharacter and gap feature representations at local and long ranges inside a\ntext instance. To address the second issue, we devise an instance-based\ntransformer module to exploit the dependencies between different text instances\nand a global context module to exploit the semantic context from the shared\nbackground, which are able to collaboratively learn more discriminative text\nfeature representation. In this way, I3CL can effectively exploit the intra-\nand inter-instance dependencies together in a unified end-to-end trainable\nframework. Besides, to make full use of the unlabeled data, we design an\neffective semi-supervised learning method to leverage the pseudo labels via an\nensemble strategy. Without bells and whistles, experimental results show that\nthe proposed I3CL sets new state-of-the-art results on three challenging public\nbenchmarks, i.e., an F-measure of 77.5% on ICDAR2019-ArT, 86.9% on Total-Text,\nand 86.4% on CTW-1500. Notably, our I3CL with the ResNeSt-101 backbone ranked\n1st place on the ICDAR2019-ArT leaderboard. The source code will be made\npublicly available.",
          "link": "http://arxiv.org/abs/2108.01343",
          "publishedOn": "2021-08-17T01:54:48.146Z",
          "wordCount": 692,
          "title": "I3CL:Intra- and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection. (arXiv:2108.01343v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+John_T/0/1/0/all/0/1\">Thrupthi Ann John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>",
          "description": "As Deep Neural Network models for face processing tasks approach human-like\nperformance, their deployment in critical applications such as law enforcement\nand access control has seen an upswing, where any failure may have far-reaching\nconsequences. We need methods to build trust in deployed systems by making\ntheir working as transparent as possible. Existing visualization algorithms are\ndesigned for object recognition and do not give insightful results when applied\nto the face domain. In this work, we present 'Canonical Saliency Maps', a new\nmethod that highlights relevant facial areas by projecting saliency maps onto a\ncanonical face model. We present two kinds of Canonical Saliency Maps:\nimage-level maps and model-level maps. Image-level maps highlight facial\nfeatures responsible for the decision made by a deep face model on a given\nimage, thus helping to understand how a DNN made a prediction on the image.\nModel-level maps provide an understanding of what the entire DNN model focuses\non in each task and thus can be used to detect biases in the model. Our\nqualitative and quantitative results show the usefulness of the proposed\ncanonical saliency maps, which can be used on any deep face model regardless of\nthe architecture.",
          "link": "http://arxiv.org/abs/2105.01386",
          "publishedOn": "2021-08-17T01:54:48.121Z",
          "wordCount": 682,
          "title": "Canonical Saliency Maps: Decoding Deep Face Models. (arXiv:2105.01386v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15765",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1\">Jinli Suo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>",
          "description": "High resolution images are widely used in our daily life, whereas high-speed\nvideo capture is challenging due to the low frame rate of cameras working at\nthe high resolution mode. Digging deeper, the main bottleneck lies in the low\nthroughput of existing imaging systems. Towards this end, snapshot compressive\nimaging (SCI) was proposed as a promising solution to improve the throughput of\nimaging systems by compressive sampling and computational reconstruction.\nDuring acquisition, multiple high-speed images are encoded and collapsed to a\nsingle measurement. After this, algorithms are employed to retrieve the video\nframes from the coded snapshot. Recently developed Plug-and-Play (PnP)\nalgorithms make it possible for SCI reconstruction in large-scale problems.\nHowever, the lack of high-resolution encoding systems still precludes SCI's\nwide application. In this paper, we build a novel hybrid coded aperture\nsnapshot compressive imaging (HCA-SCI) system by incorporating a dynamic liquid\ncrystal on silicon and a high-resolution lithography mask. We further implement\na PnP reconstruction algorithm with cascaded denoisers for high quality\nreconstruction. Based on the proposed HCA-SCI system and algorithm, we achieve\na 10-mega pixel SCI system to capture high-speed scenes, leading to a high\nthroughput of 4.6G voxels per second. Both simulation and real data experiments\nverify the feasibility and performance of our proposed HCA-SCI scheme.",
          "link": "http://arxiv.org/abs/2106.15765",
          "publishedOn": "2021-08-17T01:54:48.102Z",
          "wordCount": 689,
          "title": "10-mega pixel snapshot compressive imaging with a hybrid coded aperture. (arXiv:2106.15765v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shermin_T/0/1/0/all/0/1\">Tasfia Shermin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Shyh Wei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohel_F/0/1/0/all/0/1\">Ferdous Sohel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1\">Manzur Murshed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guojun Lu</a>",
          "description": "Embedding learning (EL) and feature synthesizing (FS) are two of the popular\ncategories of fine-grained GZSL methods. EL or FS using global features cannot\ndiscriminate fine details in the absence of local features. On the other hand,\nEL or FS methods exploiting local features either neglect direct attribute\nguidance or global information. Consequently, neither method performs well. In\nthis paper, we propose to explore global and direct attribute-supervised local\nvisual features for both EL and FS categories in an integrated manner for\nfine-grained GZSL. The proposed integrated network has an EL sub-network and a\nFS sub-network. Consequently, the proposed integrated network can be tested in\ntwo ways. We propose a novel two-step dense attention mechanism to discover\nattribute-guided local visual features. We introduce new mutual learning\nbetween the sub-networks to exploit mutually beneficial information for\noptimization. Moreover, we propose to compute source-target class similarity\nbased on mutual information and transfer-learn the target classes to reduce\nbias towards the source domain during testing. We demonstrate that our proposed\nmethod outperforms contemporary methods on benchmark datasets.",
          "link": "http://arxiv.org/abs/2101.02141",
          "publishedOn": "2021-08-17T01:54:48.001Z",
          "wordCount": 661,
          "title": "Integrated Generalized Zero-Shot Learning for Fine-Grained Classification. (arXiv:2101.02141v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Haoxi Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>",
          "description": "Prevalence of deeper networks driven by self-attention is in stark contrast\nto underexplored point-based methods. In this paper, we propose groupwise\nself-attention as the basic block to construct our network: SepNet. Our\nproposed module can effectively capture both local and global dependencies.\nThis module computes the features of a group based on the summation of the\nweighted features of any point within the group. For convenience, we generalize\ngroupwise operations to assemble this module. To further facilitate our\nnetworks, we deepen and widen SepNet on the tasks of segmentation and\nclassification respectively, and verify its practicality. Specifically, SepNet\nachieves state-of-the-art for the tasks of classification and segmentation on\nmost of the datasets. We show empirical evidence that SepNet can obtain extra\naccuracy in classification or segmentation from increased width or depth,\nrespectively.",
          "link": "http://arxiv.org/abs/2011.14285",
          "publishedOn": "2021-08-17T01:54:47.992Z",
          "wordCount": 602,
          "title": "Deeper or Wider Networks of Point Clouds with Self-attention?. (arXiv:2011.14285v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ajian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenxu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1\">Anyang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zijian Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalante_H/0/1/0/all/0/1\">Hugo Jair Escalante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>",
          "description": "The threat of 3D masks to face recognition systems is increasingly serious\nand has been widely concerned by researchers. To facilitate the study of the\nalgorithms, a large-scale High-Fidelity Mask dataset, namely CASIA-SURF\nHiFiMask (briefly HiFiMask) has been collected. Specifically, it consists of a\ntotal amount of 54, 600 videos which are recorded from 75 subjects with 225\nrealistic masks under 7 new kinds of sensors. Based on this dataset and\nProtocol 3 which evaluates both the discrimination and generalization ability\nof the algorithm under the open set scenarios, we organized a 3D High-Fidelity\nMask Face Presentation Attack Detection Challenge to boost the research of 3D\nmask-based attack detection. It attracted 195 teams for the development phase\nwith a total of 18 teams qualifying for the final round. All the results were\nverified and re-run by the organizing team, and the results were used for the\nfinal ranking. This paper presents an overview of the challenge, including the\nintroduction of the dataset used, the definition of the protocol, the\ncalculation of the evaluation criteria, and the summary and publication of the\ncompetition results. Finally, we focus on introducing and analyzing the top\nranking algorithms, the conclusion summary, and the research ideas for mask\nattack detection provided by this competition.",
          "link": "http://arxiv.org/abs/2108.06968",
          "publishedOn": "2021-08-17T01:54:47.969Z",
          "wordCount": 660,
          "title": "3D High-Fidelity Mask Face Presentation Attack Detection Challenge. (arXiv:2108.06968v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu-Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng-Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Sparse voxel-based 3D convolutional neural networks (CNNs) are widely used\nfor various 3D vision tasks. Sparse voxel-based 3D CNNs create sparse non-empty\nvoxels from the 3D input and perform 3D convolution operations on them only. We\npropose a simple yet effective padding scheme --- interpolation-aware padding\nto pad a few empty voxels adjacent to the non-empty voxels and involve them in\nthe 3D CNN computation so that all neighboring voxels exist when computing\npoint-wise features via the trilinear interpolation. For fine-grained 3D vision\ntasks where point-wise features are essential, like semantic segmentation and\n3D detection, our network achieves higher prediction accuracy than the existing\nnetworks using the nearest neighbor interpolation or the normalized trilinear\ninterpolation with the zero-padding or the octree-padding scheme. Through\nextensive comparisons on various 3D segmentation and detection tasks, we\ndemonstrate the superiority of 3D sparse CNNs with our padding scheme in\nconjunction with feature interpolation.",
          "link": "http://arxiv.org/abs/2108.06925",
          "publishedOn": "2021-08-17T01:54:47.963Z",
          "wordCount": 583,
          "title": "Interpolation-Aware Padding for 3D Sparse Convolutional Neural Networks. (arXiv:2108.06925v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.06555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaogang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "Adversarial training is promising for improving robustness of deep neural\nnetworks towards adversarial perturbations, especially on the classification\ntask. The effect of this type of training on semantic segmentation, contrarily,\njust commences. We make the initial attempt to explore the defense strategy on\nsemantic segmentation by formulating a general adversarial training procedure\nthat can perform decently on both adversarial and clean samples. We propose a\ndynamic divide-and-conquer adversarial training (DDC-AT) strategy to enhance\nthe defense effect, by setting additional branches in the target model during\ntraining, and dealing with pixels with diverse properties towards adversarial\nperturbation. Our dynamical division mechanism divides pixels into multiple\nbranches automatically. Note all these additional branches can be abandoned\nduring inference and thus leave no extra parameter and computation cost.\nExtensive experiments with various segmentation models are conducted on PASCAL\nVOC 2012 and Cityscapes datasets, in which DDC-AT yields satisfying performance\nunder both white- and black-box attack.",
          "link": "http://arxiv.org/abs/2003.06555",
          "publishedOn": "2021-08-17T01:54:47.944Z",
          "wordCount": 617,
          "title": "Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation. (arXiv:2003.06555v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sheyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lina Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1\">Sonal Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowsell_C/0/1/0/all/0/1\">Corwyn Rowsell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damaskinos_S/0/1/0/all/0/1\">Savvas Damaskinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhou Wang</a>",
          "description": "AI technology has made remarkable achievements in computational pathology\n(CPath), especially with the help of deep neural networks. However, the network\nperformance is highly related to architecture design, which commonly requires\nhuman experts with domain knowledge. In this paper, we combat this challenge\nwith the recent advance in neural architecture search (NAS) to find an optimal\nnetwork for CPath applications. In particular, we use differentiable\narchitecture search (DARTS) for its efficiency. We first adopt a probing metric\nto show that the original DARTS lacks proper hyperparameter tuning on the CIFAR\ndataset, and how the generalization issue can be addressed using an adaptive\noptimization strategy. We then apply our searching framework on CPath\napplications by searching for the optimum network architecture on a\nhistological tissue type dataset (ADP). Results show that the searched network\noutperforms state-of-the-art networks in terms of prediction accuracy and\ncomputation complexity. We further conduct extensive experiments to demonstrate\nthe transferability of the searched network to new CPath applications, the\nrobustness against downscaled inputs, as well as the reliability of\npredictions.",
          "link": "http://arxiv.org/abs/2108.06859",
          "publishedOn": "2021-08-17T01:54:47.937Z",
          "wordCount": 616,
          "title": "Probeable DARTS with Application to Computational Pathology. (arXiv:2108.06859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1905.01722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>",
          "description": "Video-based person re-identification has drawn massive attention in recent\nyears due to its extensive applications in video surveillance. While deep\nlearning-based methods have led to significant progress, these methods are\nlimited by ineffectively using complementary information, which is blamed on\nnecessary data augmentation in the training process. Data augmentation has been\nwidely used to mitigate the over-fitting trap and improve the ability of\nnetwork representation. However, the previous methods adopt image-based data\naugmentation scheme to individually process the input frames, which corrupts\nthe complementary information between consecutive frames and causes performance\ndegradation. Extensive experiments on three benchmark datasets demonstrate that\nour framework outperforms the most recent state-of-the-art methods. We also\nperform cross-dataset validation to prove the generality of our method.",
          "link": "http://arxiv.org/abs/1905.01722",
          "publishedOn": "2021-08-17T01:54:47.931Z",
          "wordCount": 609,
          "title": "Intra-clip Aggregation for Video Person Re-identification. (arXiv:1905.01722v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Most polyp segmentation methods use CNNs as their backbone, leading to two\nkey issues when exchanging information between the encoder and decoder: 1)\ntaking into account the differences in contribution between different-level\nfeatures; and 2) designing effective mechanism for fusing these features.\nDifferent from existing CNN-based methods, we adopt a transformer encoder,\nwhich learns more powerful and robust representations. In addition, considering\nthe image acquisition influence and elusive properties of polyps, we introduce\nthree novel modules, including a cascaded fusion module (CFM), a camouflage\nidentification module (CIM), a and similarity aggregation module (SAM). Among\nthese, the CFM is used to collect the semantic and location information of\npolyps from high-level features, while the CIM is applied to capture polyp\ninformation disguised in low-level features. With the help of the SAM, we\nextend the pixel features of the polyp area with high-level semantic position\ninformation to the entire polyp area, thereby effectively fusing cross-level\nfeatures. The proposed model, named \\ourmodel, effectively suppresses noises in\nthe features and significantly improves their expressive capabilities.\nExtensive experiments on five widely adopted datasets show that the proposed\nmodel is more robust to various challenging situations (e.g., appearance\nchanges, small objects) than existing methods, and achieves the new\nstate-of-the-art performance. The proposed model is available at\nhttps://github.com/DengPingFan/Polyp-PVT .",
          "link": "http://arxiv.org/abs/2108.06932",
          "publishedOn": "2021-08-17T01:54:47.924Z",
          "wordCount": 654,
          "title": "Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+kim_D/0/1/0/all/0/1\">Dohyung kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1\">Bumsub Ham</a>",
          "description": "We address the problem of network quantization, that is, reducing bit-widths\nof weights and/or activations to lighten network architectures. Quantization\nmethods use a rounding function to map full-precision values to the nearest\nquantized ones, but this operation is not differentiable. There are mainly two\napproaches to training quantized networks with gradient-based optimizers.\nFirst, a straight-through estimator (STE) replaces the zero derivative of the\nrounding with that of an identity function, which causes a gradient mismatch\nproblem. Second, soft quantizers approximate the rounding with continuous\nfunctions at training time, and exploit the rounding for quantization at test\ntime. This alleviates the gradient mismatch, but causes a quantizer gap\nproblem. We alleviate both problems in a unified framework. To this end, we\nintroduce a novel quantizer, dubbed a distance-aware quantizer (DAQ), that\nmainly consists of a distance-aware soft rounding (DASR) and a temperature\ncontroller. To alleviate the gradient mismatch problem, DASR approximates the\ndiscrete rounding with the kernel soft argmax, which is based on our insight\nthat the quantization can be formulated as a distance-based assignment problem\nbetween full-precision values and quantized ones. The controller adjusts the\ntemperature parameter in DASR adaptively according to the input, addressing the\nquantizer gap problem. Experimental results on standard benchmarks show that\nDAQ outperforms the state of the art significantly for various bit-widths\nwithout bells and whistles.",
          "link": "http://arxiv.org/abs/2108.06983",
          "publishedOn": "2021-08-17T01:54:47.918Z",
          "wordCount": 646,
          "title": "Distance-aware Quantization. (arXiv:2108.06983v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_N/0/1/0/all/0/1\">Ni Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangzhu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xiaoliang Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chuyang Ye</a>",
          "description": "Brain lesion segmentation provides a valuable tool for clinical diagnosis,\nand convolutional neural networks (CNNs) have achieved unprecedented success in\nthe task. Data augmentation is a widely used strategy that improves the\ntraining of CNNs, and the design of the augmentation method for brain lesion\nsegmentation is still an open problem. In this work, we propose a simple data\naugmentation approach, dubbed as CarveMix, for CNN-based brain lesion\nsegmentation. Like other \"mix\"-based methods, such as Mixup and CutMix,\nCarveMix stochastically combines two existing labeled images to generate new\nlabeled samples. Yet, unlike these augmentation strategies based on image\ncombination, CarveMix is lesion-aware, where the combination is performed with\nan attention on the lesions and a proper annotation is created for the\ngenerated image. Specifically, from one labeled image we carve a region of\ninterest (ROI) according to the lesion location and geometry, and the size of\nthe ROI is sampled from a probability distribution. The carved ROI then\nreplaces the corresponding voxels in a second labeled image, and the annotation\nof the second image is replaced accordingly as well. In this way, we generate\nnew labeled images for network training and the lesion information is\npreserved. To evaluate the proposed method, experiments were performed on two\nbrain lesion datasets. The results show that our method improves the\nsegmentation accuracy compared with other simple data augmentation approaches.",
          "link": "http://arxiv.org/abs/2108.06883",
          "publishedOn": "2021-08-17T01:54:47.912Z",
          "wordCount": 674,
          "title": "CarveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation. (arXiv:2108.06883v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Aming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "To improve the generalization of detectors, for domain adaptive object\ndetection (DAOD), recent advances mainly explore aligning feature-level\ndistributions between the source and single-target domain, which may neglect\nthe impact of domain-specific information existing in the aligned features.\nTowards DAOD, it is important to extract domain-invariant object\nrepresentations. To this end, in this paper, we try to disentangle\ndomain-invariant representations from domain-specific representations. And we\npropose a novel disentangled method based on vector decomposition. Firstly, an\nextractor is devised to separate domain-invariant representations from the\ninput, which are used for extracting object proposals. Secondly,\ndomain-specific representations are introduced as the differences between the\ninput and domain-invariant representations. Through the difference operation,\nthe gap between the domain-specific and domain-invariant representations is\nenlarged, which promotes domain-invariant representations to contain more\ndomain-irrelevant information. In the experiment, we separately evaluate our\nmethod on the single- and compound-target case. For the single-target case,\nexperimental results of four domain-shift scenes show our method obtains a\nsignificant performance gain over baseline methods. Moreover, for the\ncompound-target case (i.e., the target is a compound of two different domains\nwithout domain labels), our method outperforms baseline methods by around 4%,\nwhich demonstrates the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.06685",
          "publishedOn": "2021-08-17T01:54:47.893Z",
          "wordCount": 637,
          "title": "Vector-Decomposed Disentanglement for Domain-Invariant Object Detection. (arXiv:2108.06685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_P/0/1/0/all/0/1\">Puspita Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Surbhi Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Richa Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatsa_M/0/1/0/all/0/1\">Mayank Vatsa</a>",
          "description": "Identifying and mitigating bias in deep learning algorithms has gained\nsignificant popularity in the past few years due to its impact on the society.\nResearchers argue that models trained on balanced datasets with good\nrepresentation provide equal and unbiased performance across subgroups.\nHowever, \\textit{can seemingly unbiased pre-trained model become biased when\ninput data undergoes certain distortions?} For the first time, we attempt to\nanswer this question in the context of face recognition. We provide a\nsystematic analysis to evaluate the performance of four state-of-the-art deep\nface recognition models in the presence of image distortions across different\n\\textit{gender} and \\textit{race} subgroups. We have observed that image\ndistortions have a relationship with the performance gap of the model across\ndifferent subgroups.",
          "link": "http://arxiv.org/abs/2108.06581",
          "publishedOn": "2021-08-17T01:54:47.888Z",
          "wordCount": 572,
          "title": "Unravelling the Effect of Image Distortions for Biased Prediction of Pre-trained Face Recognition Models. (arXiv:2108.06581v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1\">Peter Cho-Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lingyang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torgonskiy_M/0/1/0/all/0/1\">Maxim Torgonskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>",
          "description": "Interpreting the decision logic behind effective deep convolutional neural\nnetworks (CNN) on images complements the success of deep learning models.\nHowever, the existing methods can only interpret some specific decision logic\non individual or a small number of images. To facilitate human\nunderstandability and generalization ability, it is important to develop\nrepresentative interpretations that interpret common decision logics of a CNN\non a large group of similar images, which reveal the common semantics data\ncontributes to many closely related predictions. In this paper, we develop a\nnovel unsupervised approach to produce a highly representative interpretation\nfor a large number of similar images. We formulate the problem of finding\nrepresentative interpretations as a co-clustering problem, and convert it into\na submodular cost submodular cover problem based on a sample of the linear\ndecision boundaries of a CNN. We also present a visualization and similarity\nranking method. Our extensive experiments demonstrate the excellent performance\nof our method.",
          "link": "http://arxiv.org/abs/2108.06384",
          "publishedOn": "2021-08-17T01:54:47.881Z",
          "wordCount": 601,
          "title": "Finding Representative Interpretations on Convolutional Neural Networks. (arXiv:2108.06384v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Deepak Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>",
          "description": "Accurate detection and segmentation of marine debris is important for keeping\nthe water bodies clean. This paper presents a novel dataset for marine debris\nsegmentation collected using a Forward Looking Sonar (FLS). The dataset\nconsists of 1868 FLS images captured using ARIS Explorer 3000 sensor. The\nobjects used to produce this dataset contain typical house-hold marine debris\nand distractor marine objects (tires, hooks, valves,etc), divided in 11 classes\nplus a background class. Performance of state of the art semantic segmentation\narchitectures with a variety of encoders have been analyzed on this dataset and\npresented as baseline results. Since the images are grayscale, no pretrained\nweights have been used. Comparisons are made using Intersection over Union\n(IoU). The best performing model is Unet with ResNet34 backbone at 0.7481 mIoU.\nThe dataset is available at\nhttps://github.com/mvaldenegro/marine-debris-fls-datasets/",
          "link": "http://arxiv.org/abs/2108.06800",
          "publishedOn": "2021-08-17T01:54:47.875Z",
          "wordCount": 586,
          "title": "The Marine Debris Dataset for Forward-Looking Sonar Semantic Segmentation. (arXiv:2108.06800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shihao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoqin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shoushun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "Event camera is an emerging imaging sensor for capturing dynamics of moving\nobjects as events, which motivates our work in estimating 3D human pose and\nshape from the event signals. Events, on the other hand, have their unique\nchallenges: rather than capturing static body postures, the event signals are\nbest at capturing local motions. This leads us to propose a two-stage deep\nlearning approach, called EventHPE. The first-stage, FlowNet, is trained by\nunsupervised learning to infer optical flow from events. Both events and\noptical flow are closely related to human body dynamics, which are fed as input\nto the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate\nthe discrepancy between image-based flow (optical flow) and shape-based flow\n(vertices movement of human body shape), a novel flow coherence loss is\nintroduced by exploiting the fact that both flows are originated from the\nidentical human motion. An in-house event-based 3D human dataset is curated\nthat comes with 3D pose and shape annotations, which is by far the largest one\nto our knowledge. Empirical evaluations on DHP19 dataset and our in-house\ndataset demonstrate the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2108.06819",
          "publishedOn": "2021-08-17T01:54:47.870Z",
          "wordCount": 638,
          "title": "EventHPE: Event-based 3D Human Pose and Shape Estimation. (arXiv:2108.06819v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pantazis_O/0/1/0/all/0/1\">Omiros Pantazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brostow_G/0/1/0/all/0/1\">Gabriel Brostow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_K/0/1/0/all/0/1\">Kate Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>",
          "description": "We address the problem of learning self-supervised representations from\nunlabeled image collections. Unlike existing approaches that attempt to learn\nuseful features by maximizing similarity between augmented versions of each\ninput image or by speculatively picking negative samples, we instead also make\nuse of the natural variation that occurs in image collections that are captured\nusing static monitoring cameras. To achieve this, we exploit readily available\ncontext data that encodes information such as the spatial and temporal\nrelationships between the input images. We are able to learn representations\nthat are surprisingly effective for downstream supervised classification, by\nfirst identifying high probability positive pairs at training time, i.e. those\nimages that are likely to depict the same visual concept. For the critical task\nof global biodiversity monitoring, this results in image features that can be\nadapted to challenging visual species classification tasks with limited human\nsupervision. We present results on four different camera trap image\ncollections, across three different families of self-supervised learning\nmethods, and show that careful image selection at training time results in\nsuperior performance compared to existing baselines such as conventional\nself-supervised training and transfer learning.",
          "link": "http://arxiv.org/abs/2108.06435",
          "publishedOn": "2021-08-17T01:54:47.853Z",
          "wordCount": 633,
          "title": "Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring. (arXiv:2108.06435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Antoine Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tuan-Hung Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>",
          "description": "In this work, we address the task of unsupervised domain adaptation (UDA) for\nsemantic segmentation in presence of multiple target domains: The objective is\nto train a single model that can handle all these domains at test time. Such a\nmulti-target adaptation is crucial for a variety of scenarios that real-world\nautonomous systems must handle. It is a challenging setup since one faces not\nonly the domain gap between the labeled source set and the unlabeled target\nset, but also the distribution shifts existing within the latter among the\ndifferent target domains. To this end, we introduce two adversarial frameworks:\n(i) multi-discriminator, which explicitly aligns each target domain to its\ncounterparts, and (ii) multi-target knowledge transfer, which learns a\ntarget-agnostic model thanks to a multi-teacher/single-student distillation\nmechanism.The evaluation is done on four newly-proposed multi-target benchmarks\nfor UDA in semantic segmentation. In all tested scenarios, our approaches\nconsistently outperform baselines, setting competitive standards for the novel\ntask.",
          "link": "http://arxiv.org/abs/2108.06962",
          "publishedOn": "2021-08-17T01:54:47.848Z",
          "wordCount": 605,
          "title": "Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation. (arXiv:2108.06962v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Simon Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirghodsi_S/0/1/0/all/0/1\">Sohrab Amirghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Sarah Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Image harmonization aims to improve the quality of image compositing by\nmatching the \"appearance\" (\\eg, color tone, brightness and contrast) between\nforeground and background images. However, collecting large-scale annotated\ndatasets for this task requires complex professional retouching. Instead, we\npropose a novel Self-Supervised Harmonization framework (SSH) that can be\ntrained using just \"free\" natural images without being edited. We reformulate\nthe image harmonization problem from a representation fusion perspective, which\nseparately processes the foreground and background examples, to address the\nbackground occlusion issue. This framework design allows for a dual data\naugmentation method, where diverse [foreground, background, pseudo GT] triplets\ncan be generated by cropping an image with perturbations using 3D color lookup\ntables (LUTs). In addition, we build a real-world harmonization dataset as\ncarefully created by expert users, for evaluation and benchmarking purposes.\nOur results show that the proposed self-supervised method outperforms previous\nstate-of-the-art methods in terms of reference metrics, visual quality, and\nsubject user study. Code and dataset are available at\n\\url{https://github.com/VITA-Group/SSHarmonization}.",
          "link": "http://arxiv.org/abs/2108.06805",
          "publishedOn": "2021-08-17T01:54:47.842Z",
          "wordCount": 616,
          "title": "SSH: A Self-Supervised Framework for Image Harmonization. (arXiv:2108.06805v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>",
          "description": "Vision-and-language pretraining (VLP) aims to learn generic multimodal\nrepresentations from massive image-text pairs. While various successful\nattempts have been proposed, learning fine-grained semantic alignments between\nimage-text pairs plays a key role in their approaches. Nevertheless, most\nexisting VLP approaches have not fully utilized the intrinsic knowledge within\nthe image-text pairs, which limits the effectiveness of the learned alignments\nand further restricts the performance of their models. To this end, we\nintroduce a new VLP method called ROSITA, which integrates the cross- and\nintra-modal knowledge in a unified scene graph to enhance the semantic\nalignments. Specifically, we introduce a novel structural knowledge masking\n(SKM) strategy to use the scene graph structure as a priori to perform masked\nlanguage (region) modeling, which enhances the semantic alignments by\neliminating the interference information within and across modalities.\nExtensive ablation studies and comprehensive analysis verifies the\neffectiveness of ROSITA in semantic alignments. Pretrained with both in-domain\nand out-of-domain datasets, ROSITA significantly outperforms existing\nstate-of-the-art VLP methods on three typical vision-and-language tasks over\nsix benchmark datasets.",
          "link": "http://arxiv.org/abs/2108.07073",
          "publishedOn": "2021-08-17T01:54:47.836Z",
          "wordCount": 634,
          "title": "ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration. (arXiv:2108.07073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yongwei Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guiqing Li</a>",
          "description": "In this paper, we propose $\\text{HF}^2$-VAD, a Hybrid framework that\nintegrates Flow reconstruction and Frame prediction seamlessly to handle Video\nAnomaly Detection. Firstly, we design the network of ML-MemAE-SC (Multi-Level\nMemory modules in an Autoencoder with Skip Connections) to memorize normal\npatterns for optical flow reconstruction so that abnormal events can be\nsensitively identified with larger flow reconstruction errors. More\nimportantly, conditioned on the reconstructed flows, we then employ a\nConditional Variational Autoencoder (CVAE), which captures the high correlation\nbetween video frame and optical flow, to predict the next frame given several\nprevious frames. By CVAE, the quality of flow reconstruction essentially\ninfluences that of frame prediction. Therefore, poorly reconstructed optical\nflows of abnormal events further deteriorate the quality of the final predicted\nfuture frame, making the anomalies more detectable. Experimental results\ndemonstrate the effectiveness of the proposed method. Code is available at\n\\href{https://github.com/LiUzHiAn/hf2vad}{https://github.com/LiUzHiAn/hf2vad}.",
          "link": "http://arxiv.org/abs/2108.06852",
          "publishedOn": "2021-08-17T01:54:47.830Z",
          "wordCount": 608,
          "title": "A Hybrid Video Anomaly Detection Framework via Memory-Augmented Flow Reconstruction and Flow-Guided Frame Prediction. (arXiv:2108.06852v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06460",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hong_K/0/1/0/all/0/1\">Kai Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Cailian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yancheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>",
          "description": "This work presents an unsupervised deep learning scheme that exploiting\nhigh-dimensional assisted score-based generative model for color image\nrestoration tasks. Considering that the sample number and internal dimension in\nscore-based generative model have key influence on estimating the gradients of\ndata distribution, two different high-dimensional ways are proposed: The\nchannel-copy transformation increases the sample number and the pixel-scale\ntransformation decreases feasible space dimension. Subsequently, a set of\nhigh-dimensional tensors represented by these transformations are used to train\nthe network through denoising score matching. Then, sampling is performed by\nannealing Langevin dynamics and alternative data-consistency update.\nFurthermore, to alleviate the difficulty of learning high-dimensional\nrepresentation, a progressive strategy is proposed to leverage the performance.\nThe proposed unsupervised learning and iterative restoration algo-rithm, which\ninvolves a pre-trained generative network to obtain prior, has transparent and\nclear interpretation compared to other data-driven approaches. Experimental\nresults on demosaicking and inpainting conveyed the remarkable performance and\ndiversity of our proposed method.",
          "link": "http://arxiv.org/abs/2108.06460",
          "publishedOn": "2021-08-17T01:54:47.804Z",
          "wordCount": 614,
          "title": "High-dimensional Assisted Generative Model for Color Image Restoration. (arXiv:2108.06460v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_M/0/1/0/all/0/1\">Mohammad Reza Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarei_A/0/1/0/all/0/1\">Ariyan Zarei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Hoshin V. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnard_K/0/1/0/all/0/1\">Kobus Barnard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrangi_A/0/1/0/all/0/1\">Ali Behrangi</a>",
          "description": "Accurate and timely estimation of precipitation is critical for issuing\nhazard warnings (e.g., for flash floods or landslides). Current remotely sensed\nprecipitation products have a few hours of latency, associated with the\nacquisition and processing of satellite data. By applying a robust nowcasting\nsystem to these products, it is (in principle) possible to reduce this latency\nand improve their applicability, value, and impact. However, the development of\nsuch a system is complicated by the chaotic nature of the atmosphere, and the\nconsequent rapid changes that can occur in the structures of precipitation\nsystems In this work, we develop two approaches (hereafter referred to as\nNowcasting-Nets) that use Recurrent and Convolutional deep neural network\nstructures to address the challenge of precipitation nowcasting. A total of\nfive models are trained using Global Precipitation Measurement (GPM) Integrated\nMulti-satellitE Retrievals for GPM (IMERG) precipitation data over the Eastern\nContiguous United States (CONUS) and then tested against independent data for\nthe Eastern and Western CONUS. The models were designed to provide forecasts\nwith a lead time of up to 1.5 hours and, by using a feedback loop approach, the\nability of the models to extend the forecast time to 4.5 hours was also\ninvestigated. Model performance was compared against the Random Forest (RF) and\nLinear Regression (LR) machine learning methods, and also against a persistence\nbenchmark (BM) that used the most recent observation as the forecast.\nIndependent IMERG observations were used as a reference, and experiments were\nconducted to examine both overall statistics and case studies involving\nspecific precipitation events. Overall, the forecasts provided by the\nNowcasting-Net models are superior, with the Convolutional Nowcasting Network\nwith Residual Head (CNC-R) achieving 25%, 28%, and 46% improvement in the test\n...",
          "link": "http://arxiv.org/abs/2108.06868",
          "publishedOn": "2021-08-17T01:54:47.798Z",
          "wordCount": 746,
          "title": "Nowcasting-Nets: Deep Neural Network Structures for Precipitation Nowcasting Using IMERG. (arXiv:2108.06868v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guodong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>",
          "description": "Over the past few years, the success in action recognition on short trimmed\nvideos has led more investigations towards the temporal segmentation of actions\nin untrimmed long videos. Recently, supervised approaches have achieved\nexcellent performance in segmenting complex human actions in untrimmed videos.\nHowever, besides action labels, such approaches also require the start and end\npoints of each action, which is expensive and tedious to collect.\n\nIn this paper, we aim to learn the action segments taking only the high-level\nactivity labels as input. Under the setting where no action-level supervision\nis provided, Hungarian matching is often used to find the mapping between\nsegments and ground truth actions to evaluate the model and report the\nperformance. On the one hand, we show that with the high-level supervision, we\nare able to generalize the Hungarian matching settings from the current video\nand activity level to the global level. The extended global-level matching\nallows for the shared actions across activities. On the other hand, we propose\na novel action discovery framework that automatically discovers constituent\nactions in videos with the activity classification task. Specifically, we\ndefine a finite number of prototypes to form a dual representation of a video\nsequence. These collectively learned prototypes are considered discovered\nactions. This classification setting endows our approach the capability of\ndiscovering potentially shared actions across multiple complex activities.\nExtensive experiments demonstrate that the discovered actions are helpful in\nperforming temporal action segmentation and activity recognition.",
          "link": "http://arxiv.org/abs/2108.06706",
          "publishedOn": "2021-08-17T01:54:47.791Z",
          "wordCount": 677,
          "title": "Temporal Action Segmentation with High-level Complex Activity Labels. (arXiv:2108.06706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianchao Yang</a>",
          "description": "Generative Adversarial Networks (GANs) have witnessed prevailing success in\nyielding outstanding images, however, they are burdensome to deploy on\nresource-constrained devices due to ponderous computational costs and hulking\nmemory usage. Although recent efforts on compressing GANs have acquired\nremarkable results, they still exist potential model redundancies and can be\nfurther compressed. To solve this issue, we propose a novel online\nmulti-granularity distillation (OMGD) scheme to obtain lightweight GANs, which\ncontributes to generating high-fidelity images with low computational demands.\nWe offer the first attempt to popularize single-stage online distillation for\nGAN-oriented compression, where the progressively promoted teacher generator\nhelps to refine the discriminator-free based student generator. Complementary\nteacher generators and network layers provide comprehensive and\nmulti-granularity concepts to enhance visual fidelity from diverse dimensions.\nExperimental results on four benchmark datasets demonstrate that OMGD successes\nto compress 40x MACs and 82.5X parameters on Pix2Pix and CycleGAN, without loss\nof image quality. It reveals that OMGD provides a feasible solution for the\ndeployment of real-time image translation on resource-constrained devices. Our\ncode and models are made public at: https://github.com/bytedance/OMGD.",
          "link": "http://arxiv.org/abs/2108.06908",
          "publishedOn": "2021-08-17T01:54:47.786Z",
          "wordCount": 614,
          "title": "Online Multi-Granularity Distillation for GAN Compression. (arXiv:2108.06908v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruikui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruijie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>",
          "description": "The transferability and robustness of adversarial examples are two practical\nyet important properties for black-box adversarial attacks. In this paper, we\nexplore effective mechanisms to boost both of them from the perspective of\nnetwork hierarchy, where a typical network can be hierarchically divided into\noutput stage, intermediate stage and input stage. Since over-specialization of\nsource model, we can hardly improve the transferability and robustness of the\nadversarial perturbations in the output stage. Therefore, we focus on the\nintermediate and input stages in this paper and propose a transferable and\nrobust adversarial perturbation generation (TRAP) method. Specifically, we\npropose the dynamically guided mechanism to continuously calculate accurate\ndirectional guidances for perturbation generation in the intermediate stage. In\nthe input stage, instead of the single-form transformation augmentations\nadopted in the existing methods, we leverage multiform affine transformation\naugmentations to further enrich the input diversity and boost the robustness\nand transferability of the adversarial perturbations. Extensive experiments\ndemonstrate that our TRAP achieves impressive transferability and high\nrobustness against certain interferences.",
          "link": "http://arxiv.org/abs/2108.07033",
          "publishedOn": "2021-08-17T01:54:47.768Z",
          "wordCount": 613,
          "title": "Exploring Transferable and Robust Adversarial Perturbation Generation from the Perspective of Network Hierarchy. (arXiv:2108.07033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>",
          "description": "Recent works have theoretically and empirically shown that deep neural\nnetworks (DNNs) have an inherent vulnerability to small perturbations. Applying\nthe Deep k-Nearest Neighbors (DkNN) classifier, we observe a dramatically\nincreasing robustness-accuracy trade-off as the layer goes deeper. In this\nwork, we propose a Deep Adversarially-Enhanced k-Nearest Neighbors (DAEkNN)\nmethod which achieves higher robustness than DkNN and mitigates the\nrobustness-accuracy trade-off in deep layers through two key elements. First,\nDAEkNN is based on an adversarially trained model. Second, DAEkNN makes\npredictions by leveraging a weighted combination of benign and adversarial\ntraining data. Empirically, we find that DAEkNN improves both the robustness\nand the robustness-accuracy trade-off on MNIST and CIFAR-10 datasets.",
          "link": "http://arxiv.org/abs/2108.06797",
          "publishedOn": "2021-08-17T01:54:47.762Z",
          "wordCount": 539,
          "title": "Deep Adversarially-Enhanced k-Nearest Neighbors. (arXiv:2108.06797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_T/0/1/0/all/0/1\">Tianrui Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Annan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_X/0/1/0/all/0/1\">Xinyu Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>",
          "description": "Video-based person re-identification (Re-ID) which aims to associate people\nacross non-overlapping cameras using surveillance video is a challenging task.\nPedestrian attribute, such as gender, age and clothing characteristics contains\nrich and supplementary information but is less explored in video person Re-ID.\nIn this work, we propose a novel network architecture named Attribute Salience\nAssisted Network (ASA-Net) for attribute-assisted video person Re-ID, which\nachieved considerable improvement to existing works by two methods.First, to\nlearn a better separation of the target from background, we propose to learn\nthe visual attention from middle-level attribute instead of high-level\nidentities. The proposed Attribute Salient Region Enhance (ASRE) module can\nattend more accurately on the body of pedestrian. Second, we found that many\nidentity-irrelevant but object or subject-relevant factors like the view angle\nand movement of the target pedestrian can greatly influence the two dimensional\nappearance of a pedestrian. This problem can be mitigated by investigating both\nidentity-relevant and identity-irrelevant attributes via a novel triplet loss\nwhich is referred as the Pose~\\&~Motion-Invariant (PMI) triplet loss.",
          "link": "http://arxiv.org/abs/2108.06946",
          "publishedOn": "2021-08-17T01:54:47.756Z",
          "wordCount": 604,
          "title": "Video Person Re-identification using Attribute-enhanced Features. (arXiv:2108.06946v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haobin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xinyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "Lacking the ability to sense ambient environments effectively, blind and\nvisually impaired people (BVIP) face difficulty in walking outdoors, especially\nin urban areas. Therefore, tools for assisting BVIP are of great importance. In\nthis paper, we propose a novel \"flying guide dog\" prototype for BVIP assistance\nusing drone and street view semantic segmentation. Based on the walkable areas\nextracted from the segmentation prediction, the drone can adjust its movement\nautomatically and thus lead the user to walk along the walkable path. By\nrecognizing the color of pedestrian traffic lights, our prototype can help the\nuser to cross a street safely. Furthermore, we introduce a new dataset named\nPedestrian and Vehicle Traffic Lights (PVTL), which is dedicated to traffic\nlight recognition. The result of our user study in real-world scenarios shows\nthat our prototype is effective and easy to use, providing new insight into\nBVIP assistance.",
          "link": "http://arxiv.org/abs/2108.07007",
          "publishedOn": "2021-08-17T01:54:47.665Z",
          "wordCount": 632,
          "title": "Flying Guide Dog: Walkable Path Discovery for the Visually Impaired Utilizing Drones and Transformer-based Semantic Segmentation. (arXiv:2108.07007v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chaochao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_T/0/1/0/all/0/1\">Thomas Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoan Li</a>",
          "description": "Registration of 3D anatomic structures to their 2D dual fluoroscopic X-ray\nimages is a widely used motion tracking technique. However, deep learning\nimplementation is often impeded by a paucity of medical images and ground\ntruths. In this study, we proposed a transfer learning strategy for 3D-to-2D\nregistration using deep neural networks trained from an artificial dataset.\nDigitally reconstructed radiographs (DRRs) and radiographic skull landmarks\nwere automatically created from craniocervical CT data of a female subject.\nThey were used to train a residual network (ResNet) for landmark detection and\na cycle generative adversarial network (GAN) to eliminate the style difference\nbetween DRRs and actual X-rays. Landmarks on the X-rays experiencing GAN style\ntranslation were detected by the ResNet, and were used in triangulation\noptimization for 3D-to-2D registration of the skull in actual dual-fluoroscope\nimages (with a non-orthogonal setup, point X-ray sources, image distortions,\nand partially captured skull regions). The registration accuracy was evaluated\nin multiple scenarios of craniocervical motions. In walking, learning-based\nregistration for the skull had angular/position errors of 3.9 +- 2.1 deg / 4.6\n+- 2.2 mm. However, the accuracy was lower during functional neck activity, due\nto overly small skull regions imaged on the dual fluoroscopic images at\nend-range positions. The methodology to strategically augment artificial\ntraining data can tackle the complicated skull registration scenario, and has\npotentials to extend to widespread registration scenarios.",
          "link": "http://arxiv.org/abs/2108.06466",
          "publishedOn": "2021-08-17T01:54:47.644Z",
          "wordCount": 683,
          "title": "Transfer Learning from an Artificial Radiograph-landmark Dataset for Registration of the Anatomic Skull Model to Dual Fluoroscopic X-ray Images. (arXiv:2108.06466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bultmann_S/0/1/0/all/0/1\">Simon Bultmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quenzel_J/0/1/0/all/0/1\">Jan Quenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "Unmanned aerial vehicles (UAVs) equipped with multiple complementary sensors\nhave tremendous potential for fast autonomous or remote-controlled semantic\nscene analysis, e.g., for disaster examination. In this work, we propose a UAV\nsystem for real-time semantic inference and fusion of multiple sensor\nmodalities. Semantic segmentation of LiDAR scans and RGB images, as well as\nobject detection on RGB and thermal images, run online onboard the UAV computer\nusing lightweight CNN architectures and embedded inference accelerators. We\nfollow a late fusion approach where semantic information from multiple\nmodalities augments 3D point clouds and image segmentation masks while also\ngenerating an allocentric semantic map. Our system provides augmented semantic\nimages and point clouds with $\\approx\\,$9$\\,$Hz. We evaluate the integrated\nsystem in real-world experiments in an urban environment.",
          "link": "http://arxiv.org/abs/2108.06608",
          "publishedOn": "2021-08-17T01:54:47.587Z",
          "wordCount": 575,
          "title": "Real-Time Multi-Modal Semantic Fusion on Unmanned Aerial Vehicles. (arXiv:2108.06608v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Binghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>",
          "description": "Few-shot semantic segmentation is a challenging task of predicting object\ncategories in pixel-wise with only few annotated samples. However, existing\napproaches still face two main challenges. First, huge feature distinction\nbetween support and query images causes knowledge transferring barrier, which\nharms the segmentation performance. Second, few support samples cause\nunrepresentative of support features, hardly to guide high-quality query\nsegmentation. To deal with the above two issues, we propose self-distillation\nembedded supervised affinity attention model (SD-AANet) to improve the\nperformance of few-shot segmentation task. Specifically, the self-distillation\nguided prototype module (SDPM) extracts intrinsic prototype by\nself-distillation between support and query to capture representative features.\nThe supervised affinity attention module (SAAM) adopts support ground truth to\nguide the production of high quality query attention map, which can learn\naffinity information to focus on whole area of query target. Extensive\nexperiments prove that our SD-AANet significantly improves the performance\ncomparing with existing methods. Comprehensive ablation experiments and\nvisualization studies also show the significant effect of SDPM and SAAM for\nfew-shot segmentation task. On benchmark datasets, PASCAL-5i and COCO-20i, our\nproposed SD-AANet both achieve state-of-the-art results. Our code will be\npublicly available soon.",
          "link": "http://arxiv.org/abs/2108.06600",
          "publishedOn": "2021-08-17T01:54:47.575Z",
          "wordCount": 636,
          "title": "A Self-Distillation Embedded Supervised Affinity Attention Model for Few-Shot Segmentation. (arXiv:2108.06600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "A lifespan face synthesis (LFS) model aims to generate a set of\nphoto-realistic face images of a person's whole life, given only one snapshot\nas reference. The generated face image given a target age code is expected to\nbe age-sensitive reflected by bio-plausible transformations of shape and\ntexture, while being identity preserving. This is extremely challenging because\nthe shape and texture characteristics of a face undergo separate and highly\nnonlinear transformations w.r.t. age. Most recent LFS models are based on\ngenerative adversarial networks (GANs) whereby age code conditional\ntransformations are applied to a latent face representation. They benefit\ngreatly from the recent advancements of GANs. However, without explicitly\ndisentangling their latent representations into the texture, shape and identity\nfactors, they are fundamentally limited in modeling the nonlinear age-related\ntransformation on texture and shape whilst preserving identity. In this work, a\nnovel LFS model is proposed to disentangle the key face characteristics\nincluding shape, texture and identity so that the unique shape and texture age\ntransformations can be modeled effectively. This is achieved by extracting\nshape, texture and identity features separately from an encoder. Critically,\ntwo transformation modules, one conditional convolution based and the other\nchannel attention based, are designed for modeling the nonlinear shape and\ntexture feature transformations respectively. This is to accommodate their\nrather distinct aging processes and ensure that our synthesized images are both\nage-sensitive and identity preserving. Extensive experiments show that our LFS\nmodel is clearly superior to the state-of-the-art alternatives. Codes and demo\nare available on our project website:\n\\url{https://senhe.github.io/projects/iccv_2021_lifespan_face}.",
          "link": "http://arxiv.org/abs/2108.02874",
          "publishedOn": "2021-08-16T01:57:42.822Z",
          "wordCount": 711,
          "title": "Disentangled Lifespan Face Synthesis. (arXiv:2108.02874v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xuejing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>",
          "description": "The nonlocal-based blocks are designed for capturing long-range\nspatial-temporal dependencies in computer vision tasks. Although having shown\nexcellent performance, they still lack the mechanism to encode the rich,\nstructured information among elements in an image or video. In this paper, to\ntheoretically analyze the property of these nonlocal-based blocks, we provide a\nnew perspective to interpret them, where we view them as a set of graph filters\ngenerated on a fully-connected graph. Specifically, when choosing the Chebyshev\ngraph filter, a unified formulation can be derived for explaining and analyzing\nthe existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage,\ndouble attention block). Furthermore, by concerning the property of spectral,\nwe propose an efficient and robust spectral nonlocal block, which can be more\nrobust and flexible to catch long-range dependencies when inserted into deep\nneural networks than the existing nonlocal blocks. Experimental results\ndemonstrate the clear-cut improvements and practical applicabilities of our\nmethod on image classification, action recognition, semantic segmentation, and\nperson re-identification tasks.",
          "link": "http://arxiv.org/abs/2108.02451",
          "publishedOn": "2021-08-16T00:47:33.983Z",
          "wordCount": 627,
          "title": "Unifying Nonlocal Blocks for Neural Networks. (arXiv:2108.02451v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiafei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheston Tan</a>",
          "description": "Recent advancements in deep learning, computer vision, and embodied AI have\ngiven rise to synthetic causal reasoning video datasets. These datasets\nfacilitate the development of AI algorithms that can reason about physical\ninteractions between objects. However, datasets thus far have primarily focused\non elementary physical events such as rolling or falling. There is currently a\nscarcity of datasets that focus on the physical interactions that humans\nperform daily with objects in the real world. To address this scarcity, we\nintroduce SPACE: A Simulator for Physical Interactions and Causal Learning in\n3D Environments. The SPACE simulator allows us to generate the SPACE dataset, a\nsynthetic video dataset in a 3D environment, to systematically evaluate\nphysics-based models on a range of physical causal reasoning tasks. Inspired by\ndaily object interactions, the SPACE dataset comprises videos depicting three\ntypes of physical events: containment, stability and contact. These events make\nup the vast majority of the basic physical interactions between objects. We\nthen further evaluate it with a state-of-the-art physics-based deep model and\nshow that the SPACE dataset improves the learning of intuitive physics with an\napproach inspired by curriculum learning. Repository:\nhttps://github.com/jiafei1224/SPACE",
          "link": "http://arxiv.org/abs/2108.06180",
          "publishedOn": "2021-08-16T00:47:33.910Z",
          "wordCount": 648,
          "title": "SPACE: A Simulator for Physical Interactions and Causal Learning in 3D Environments. (arXiv:2108.06180v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berghoff_C/0/1/0/all/0/1\">Christian Berghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielik_P/0/1/0/all/0/1\">Pavol Bielik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neu_M/0/1/0/all/0/1\">Matthias Neu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsankov_P/0/1/0/all/0/1\">Petar Tsankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twickel_A/0/1/0/all/0/1\">Arndt von Twickel</a>",
          "description": "In the last years, AI systems, in particular neural networks, have seen a\ntremendous increase in performance, and they are now used in a broad range of\napplications. Unlike classical symbolic AI systems, neural networks are trained\nusing large data sets and their inner structure containing possibly billions of\nparameters does not lend itself to human interpretation. As a consequence, it\nis so far not feasible to provide broad guarantees for the correct behaviour of\nneural networks during operation if they process input data that significantly\ndiffer from those seen during training. However, many applications of AI\nsystems are security- or safety-critical, and hence require obtaining\nstatements on the robustness of the systems when facing unexpected events,\nwhether they occur naturally or are induced by an attacker in a targeted way.\nAs a step towards developing robust AI systems for such applications, this\npaper presents how the robustness of AI systems can be practically examined and\nwhich methods and metrics can be used to do so. The robustness testing\nmethodology is described and analysed for the example use case of traffic sign\nrecognition in autonomous driving.",
          "link": "http://arxiv.org/abs/2108.06159",
          "publishedOn": "2021-08-16T00:47:33.891Z",
          "wordCount": 686,
          "title": "Robustness testing of AI systems: A case study for traffic sign recognition. (arXiv:2108.06159v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shoukui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Erjin Zhou</a>",
          "description": "Human pose estimation deeply relies on visual clues and anatomical\nconstraints between parts to locate keypoints. Most existing CNN-based methods\ndo well in visual representation, however, lacking in the ability to explicitly\nlearn the constraint relationships between keypoints. In this paper, we propose\na novel approach based on Token representation for human Pose\nestimation~(TokenPose). In detail, each keypoint is explicitly embedded as a\ntoken to simultaneously learn constraint relationships and appearance cues from\nimages. Extensive experiments show that the small and large TokenPose models\nare on par with state-of-the-art CNN-based counterparts while being more\nlightweight. Specifically, our TokenPose-S and TokenPose-L achieve $72.5$ AP\nand $75.8$ AP on COCO validation dataset respectively, with significant\nreduction in parameters ($\\downarrow80.6\\%$; $\\downarrow$ $56.8\\%$) and GFLOPs\n($\\downarrow$ $75.3\\%$; $\\downarrow$ $24.7\\%$). Code is publicly available.",
          "link": "http://arxiv.org/abs/2104.03516",
          "publishedOn": "2021-08-16T00:47:33.874Z",
          "wordCount": 623,
          "title": "TokenPose: Learning Keypoint Tokens for Human Pose Estimation. (arXiv:2104.03516v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>",
          "description": "Weakly-supervised learning (WSL) has recently triggered substantial interest\nas it mitigates the lack of pixel-wise annotations.\n\nGiven global image labels, WSL methods yield pixel-level predictions\n(segmentations), which enable to interpret class predictions. Despite their\nrecent success, mostly with natural images, such methods can face important\nchallenges when the foreground and background regions have similar visual cues,\nyielding high false-positive rates in segmentations, as is the case in\nchallenging histology images. WSL training is commonly driven by standard\nclassification losses, which implicitly maximize model confidence, and locate\nthe discriminative regions linked to classification decisions. Therefore, they\nlack mechanisms for modeling explicitly non-discriminative regions and reducing\nfalse-positive rates. We propose novel regularization terms, which enable the\nmodel to seek both non-discriminative and discriminative regions, while\ndiscouraging unbalanced segmentations. We introduce high uncertainty as a\ncriterion to localize non-discriminative regions that do not affect classifier\ndecision, and describe it with original Kullback-Leibler (KL) divergence losses\nevaluating the deviation of posterior predictions from the uniform\ndistribution. Our KL terms encourage high uncertainty of the model when the\nlatter inputs the latent non-discriminative regions. Our loss integrates: (i) a\ncross-entropy seeking a foreground, where model confidence about class\nprediction is high; (ii) a KL regularizer seeking a background, where model\nuncertainty is high; and (iii) log-barrier terms discouraging unbalanced\nsegmentations. Comprehensive experiments and ablation studies over the public\nGlaS colon cancer data and a Camelyon16 patch-based benchmark for breast cancer\nshow substantial improvements over state-of-the-art WSL methods, and confirm\nthe effect of our new regularizers.",
          "link": "http://arxiv.org/abs/2011.07221",
          "publishedOn": "2021-08-16T00:47:33.869Z",
          "wordCount": 741,
          "title": "Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty. (arXiv:2011.07221v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06757",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kamran_S/0/1/0/all/0/1\">Sharif Amit Kamran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_K/0/1/0/all/0/1\">Khondker Fariha Hossain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tavakkoli_A/0/1/0/all/0/1\">Alireza Tavakkoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuckerbrod_S/0/1/0/all/0/1\">Stewart Lee Zuckerbrod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baker_S/0/1/0/all/0/1\">Salah A. Baker</a>",
          "description": "In Fluorescein Angiography (FA), an exogenous dye is injected in the\nbloodstream to image the vascular structure of the retina. The injected dye can\ncause adverse reactions such as nausea, vomiting, anaphylactic shock, and even\ndeath. In contrast, color fundus imaging is a non-invasive technique used for\nphotographing the retina but does not have sufficient fidelity for capturing\nits vascular structure. The only non-invasive method for capturing retinal\nvasculature is optical coherence tomography-angiography (OCTA). However, OCTA\nequipment is quite expensive, and stable imaging is limited to small areas on\nthe retina. In this paper, we propose a novel conditional generative\nadversarial network (GAN) capable of simultaneously synthesizing FA images from\nfundus photographs while predicting retinal degeneration. The proposed system\nhas the benefit of addressing the problem of imaging retinal vasculature in a\nnon-invasive manner as well as predicting the existence of retinal\nabnormalities. We use a semi-supervised approach to train our GAN using\nmultiple weighted losses on different modalities of data. Our experiments\nvalidate that the proposed architecture exceeds recent state-of-the-art\ngenerative networks for fundus-to-angiography synthesis. Moreover, our vision\ntransformer-based discriminators generalize quite well on out-of-distribution\ndata sets for retinal disease prediction.",
          "link": "http://arxiv.org/abs/2104.06757",
          "publishedOn": "2021-08-16T00:47:33.863Z",
          "wordCount": 688,
          "title": "VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers. (arXiv:2104.06757v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yiting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>",
          "description": "Domain adaptation for semantic segmentation enables to alleviate the need for\nlarge-scale pixel-wise annotations. Recently, self-supervised learning (SSL)\nwith a combination of image-to-image translation shows great effectiveness in\nadaptive segmentation. The most common practice is to perform SSL along with\nimage translation to well align a single domain (the source or target).\nHowever, in this single-domain paradigm, unavoidable visual inconsistency\nraised by image translation may affect subsequent learning. In this paper,\nbased on the observation that domain adaptation frameworks performed in the\nsource and target domain are almost complementary in terms of image translation\nand SSL, we propose a novel dual path learning (DPL) framework to alleviate\nvisual inconsistency. Concretely, DPL contains two complementary and\ninteractive single-domain adaptation pipelines aligned in source and target\ndomain respectively. The inference of DPL is extremely simple, only one\nsegmentation model in the target domain is employed. Novel technologies such as\ndual path image translation and dual path adaptive segmentation are proposed to\nmake two paths promote each other in an interactive manner. Experiments on\nGTA5$\\rightarrow$Cityscapes and SYNTHIA$\\rightarrow$Cityscapes scenarios\ndemonstrate the superiority of our DPL model over the state-of-the-art methods.\nThe code and models are available at: \\url{https://github.com/royee182/DPL}",
          "link": "http://arxiv.org/abs/2108.06337",
          "publishedOn": "2021-08-16T00:47:33.858Z",
          "wordCount": 643,
          "title": "Dual Path Learning for Domain Adaptation of Semantic Segmentation. (arXiv:2108.06337v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kucherenko_T/0/1/0/all/0/1\">Taras Kucherenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_R/0/1/0/all/0/1\">Rajmund Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonell_P/0/1/0/all/0/1\">Patrik Jonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_M/0/1/0/all/0/1\">Michael Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>",
          "description": "We propose a new framework for gesture generation, aiming to allow\ndata-driven approaches to produce more semantically rich gestures. Our approach\nfirst predicts whether to gesture, followed by a prediction of the gesture\nproperties. Those properties are then used as conditioning for a modern\nprobabilistic gesture-generation model capable of high-quality output. This\nempowers the approach to generate gestures that are both diverse and\nrepresentational. Follow-ups and more information can be found on the project\npage: https://svito-zar.github.io/speech2properties2gestures/ .",
          "link": "http://arxiv.org/abs/2106.14736",
          "publishedOn": "2021-08-16T00:47:33.852Z",
          "wordCount": 587,
          "title": "Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for Generating Representational Gestures from Speech. (arXiv:2106.14736v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Benefiting from large-scale pre-training, we have witnessed significant\nperformance boost on the popular Visual Question Answering (VQA) task. Despite\nrapid progress, it remains unclear whether these state-of-the-art (SOTA) models\nare robust when encountering examples in the wild. To study this, we introduce\nAdversarial VQA, a new large-scale VQA benchmark, collected iteratively via an\nadversarial human-and-model-in-the-loop procedure. Through this new benchmark,\nwe discover several interesting findings. (i) Surprisingly, we find that during\ndataset collection, non-expert annotators can easily attack SOTA VQA models\nsuccessfully. (ii) Both large-scale pre-trained models and adversarial training\nmethods achieve far worse performance on the new benchmark than over standard\nVQA v2 dataset, revealing the fragility of these models while demonstrating the\neffectiveness of our adversarial dataset. (iii) When used for data\naugmentation, our dataset can effectively boost model performance on other\nrobust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light\non robustness study in the community and serve as a valuable benchmark for\nfuture work.",
          "link": "http://arxiv.org/abs/2106.00245",
          "publishedOn": "2021-08-16T00:47:33.836Z",
          "wordCount": 649,
          "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models. (arXiv:2106.00245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Celong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>",
          "description": "In this paper, we present an efficient and robust deep learning solution for\nnovel view synthesis of complex scenes. In our approach, a 3D scene is\nrepresented as a light field, i.e., a set of rays, each of which has a\ncorresponding color when reaching the image plane. For efficient novel view\nrendering, we adopt a 4D parameterization of the light field, where each ray is\ncharacterized by a 4D parameter. We then formulate the light field as a 4D\nfunction that maps 4D coordinates to corresponding color values. We train a\ndeep fully connected network to optimize this implicit function and memorize\nthe 3D scene. Then, the scene-specific model is used to synthesize novel views.\nDifferent from previous light field approaches which require dense view\nsampling to reliably render novel views, our method can render novel views by\nsampling rays and querying the color for each ray from the network directly,\nthus enabling high-quality light field rendering with a sparser set of training\nimages. Our method achieves state-of-the-art novel view synthesis results while\nmaintaining an interactive frame rate.",
          "link": "http://arxiv.org/abs/2105.07112",
          "publishedOn": "2021-08-16T00:47:33.830Z",
          "wordCount": 722,
          "title": "NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. (arXiv:2105.07112v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antoniadis_P/0/1/0/all/0/1\">Panagiotis Antoniadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_I/0/1/0/all/0/1\">Ioannis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filntisis_P/0/1/0/all/0/1\">Panagiotis P. Filntisis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>",
          "description": "In this work we tackle the task of video-based audio-visual emotion\nrecognition, within the premises of the 2nd Workshop and Competition on\nAffective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions,\nhead/body orientation and low image resolution constitute factors that can\npotentially hinder performance in case of methodologies that solely rely on the\nextraction and analysis of facial features. In order to alleviate this problem,\nwe leverage both bodily and contextual features, as part of a broader emotion\nrecognition framework. We choose to use a standard CNN-RNN cascade as the\nbackbone of our proposed model for sequence-to-sequence (seq2seq) learning.\nApart from learning through the RGB input modality, we construct an aural\nstream which operates on sequences of extracted mel-spectrograms. Our extensive\nexperiments on the challenging and newly assembled Aff-Wild2 dataset verify the\nvalidity of our intuitive multi-stream and multi-modal approach towards emotion\nrecognition in-the-wild. Emphasis is being laid on the the beneficial influence\nof the human body and scene context, as aspects of the emotion recognition\nprocess that have been left relatively unexplored up to this point. All the\ncode was implemented using PyTorch and is publicly available.",
          "link": "http://arxiv.org/abs/2107.03465",
          "publishedOn": "2021-08-16T00:47:33.823Z",
          "wordCount": 688,
          "title": "An audiovisual and contextual approach for categorical and continuous emotion recognition in-the-wild. (arXiv:2107.03465v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Monocular 3D object detection is an important task for autonomous driving\nconsidering its advantage of low cost. It is much more challenging than\nconventional 2D cases due to its inherent ill-posed property, which is mainly\nreflected in the lack of depth information. Recent progress on 2D detection\noffers opportunities to better solving this problem. However, it is non-trivial\nto make a general adapted 2D detector work in this 3D task. In this paper, we\nstudy this problem with a practice built on a fully convolutional single-stage\ndetector and propose a general framework FCOS3D. Specifically, we first\ntransform the commonly defined 7-DoF 3D targets to the image domain and\ndecouple them as 2D and 3D attributes. Then the objects are distributed to\ndifferent feature levels with consideration of their 2D scales and assigned\nonly according to the projected 3D-center for the training procedure.\nFurthermore, the center-ness is redefined with a 2D Gaussian distribution based\non the 3D-center to fit the 3D target formulation. All of these make this\nframework simple yet effective, getting rid of any 2D detection or 2D-3D\ncorrespondence priors. Our solution achieves 1st place out of all the\nvision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020.\nCode and models are released at https://github.com/open-mmlab/mmdetection3d.",
          "link": "http://arxiv.org/abs/2104.10956",
          "publishedOn": "2021-08-16T00:47:33.818Z",
          "wordCount": 712,
          "title": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection. (arXiv:2104.10956v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.06519",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1\">Ganna Platonova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1\">Dalibor Stys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1\">Pavel Soucek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1\">Kirill Lonhus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1\">Jan Valenta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1\">Renata Rychtarikova</a>",
          "description": "The most realistic information about the transparent sample such as a live\ncell can be obtained only using bright-field light microscopy. At\nhigh-intensity pulsing LED illumination, we captured a primary\n12-bit-per-channel (bpc) response from an observed sample using a bright-field\nmicroscope equipped with a high-resolution (4872x3248) image sensor. In order\nto suppress data distortions originating from the light interactions with\nelements in the optical path, poor sensor reproduction (geometrical defects of\nthe camera sensor and some peculiarities of sensor sensitivity), we propose a\nspectroscopic approach for the correction of this uncompressed 12-bpc data by\nsimultaneous calibration of all parts of the experimental arrangement.\nMoreover, the final intensities of the corrected images are proportional to the\nphoton fluxes detected by a camera sensor. It can be visualized in 8-bpc\nintensity depth after the Least Information Loss compression.",
          "link": "http://arxiv.org/abs/1903.06519",
          "publishedOn": "2021-08-16T00:47:33.811Z",
          "wordCount": 652,
          "title": "Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao-Jung Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krylov_V/0/1/0/all/0/1\">Vladimir Krylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahyot_R/0/1/0/all/0/1\">Rozenn Dahyot</a>",
          "description": "In this paper we propose an approach to perform semantic segmentation of 3D\npoint cloud data by importing the geographic information from a 2D GIS layer\n(OpenStreetMap). The proposed automatic procedure identifies meaningful units\nsuch as buildings and adjusts their locations to achieve best fit between the\nGIS polygonal perimeters and the point cloud. Our processing pipeline is\npresented and illustrated by segmenting point cloud data of Trinity College\nDublin (Ireland) campus constructed from optical imagery collected by a drone.",
          "link": "http://arxiv.org/abs/2108.06306",
          "publishedOn": "2021-08-16T00:47:33.787Z",
          "wordCount": 516,
          "title": "3D point cloud segmentation using GIS. (arXiv:2108.06306v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Termritthikun_C/0/1/0/all/0/1\">Chakkrit Termritthikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamtsho_Y/0/1/0/all/0/1\">Yeshi Jamtsho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ieamsaard_J/0/1/0/all/0/1\">Jirarat Ieamsaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muneesawang_P/0/1/0/all/0/1\">Paisarn Muneesawang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Ivan Lee</a>",
          "description": "The goals of this research were to search for Convolutional Neural Network\n(CNN) architectures, suitable for an on-device processor with limited computing\nresources, performing at substantially lower Network Architecture Search (NAS)\ncosts. A new algorithm entitled an Early Exit Population Initialisation (EE-PI)\nfor Evolutionary Algorithm (EA) was developed to achieve both goals. The EE-PI\nreduces the total number of parameters in the search process by filtering the\nmodels with fewer parameters than the maximum threshold. It will look for a new\nmodel to replace those models with parameters more than the threshold. Thereby,\nreducing the number of parameters, memory usage for model storage and\nprocessing time while maintaining the same performance or accuracy. The search\ntime was reduced to 0.52 GPU day. This is a huge and significant achievement\ncompared to the NAS of 4 GPU days achieved using NSGA-Net, 3,150 GPU days by\nthe AmoebaNet model, and the 2,000 GPU days by the NASNet model. As well, Early\nExit Evolutionary Algorithm networks (EEEA-Nets) yield network architectures\nwith minimal error and computational cost suitable for a given dataset as a\nclass of network algorithms. Using EEEA-Net on CIFAR-10, CIFAR-100, and\nImageNet datasets, our experiments showed that EEEA-Net achieved the lowest\nerror rate among state-of-the-art NAS models, with 2.46% for CIFAR-10, 15.02%\nfor CIFAR-100, and 23.8% for ImageNet dataset. Further, we implemented this\nimage recognition architecture for other tasks, such as object detection,\nsemantic segmentation, and keypoint detection tasks, and, in our experiments,\nEEEA-Net-C2 outperformed MobileNet-V3 on all of these various tasks. (The\nalgorithm code is available at https://github.com/chakkritte/EEEA-Net).",
          "link": "http://arxiv.org/abs/2108.06156",
          "publishedOn": "2021-08-16T00:47:33.781Z",
          "wordCount": 754,
          "title": "EEEA-Net: An Early Exit Evolutionary Neural Architecture Search. (arXiv:2108.06156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets, often with better accuracy and fewer\nparameters. Our model eliminates the requirement for class token and positional\nembeddings through a novel sequence pooling strategy and the use of\nconvolution/s. It is flexible in terms of model size, and can have as little as\n0.28M parameters while achieving good results. Our model can reach 98.00%\naccuracy when training from scratch on CIFAR-10, which is a significant\nimprovement over previous Transformer based models. It also outperforms many\nmodern CNN based approaches, such as ResNet, and even some recent NAS-based\napproaches, such as Proxyless-NAS. Our simple and compact design democratizes\ntransformers by making them accessible to those with limited computing\nresources and/or dealing with small datasets. Our method also works on larger\ndatasets, such as ImageNet (82.71% accuracy with 29% parameters of ViT), and\nNLP tasks as well. Our code and pre-trained models are publicly available at\nhttps://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-08-16T00:47:33.774Z",
          "wordCount": 760,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Kenji Suzuki</a>",
          "description": "Affective Behavior Analysis is an important part in human-computer\ninteraction. Existing multi-task affective behavior recognition methods suffer\nfrom the problem of incomplete labeled datasets. To tackle this problem, this\npaper presents a semi-supervised model with a mean teacher framework to\nleverage additional unlabeled data. To be specific, a multi-task model is\nproposed to learn three different kinds of facial affective representations\nsimultaneously. After that, the proposed model is assigned to be student and\nteacher networks. When training with unlabeled data, the teacher network is\nemployed to predict pseudo labels for student network training, which allows it\nto learn from unlabeled data. Experimental results showed that our proposed\nmethod achieved much better performance than baseline model and ranked 4th in\nboth competition track 1 and track 2, and 6th in track 3, which verifies that\nthe proposed network can effectively learn from incomplete datasets.",
          "link": "http://arxiv.org/abs/2107.04225",
          "publishedOn": "2021-08-16T00:47:33.767Z",
          "wordCount": 622,
          "title": "A Multi-task Mean Teacher for Semi-supervised Facial Affective Behavior Analysis. (arXiv:2107.04225v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1\">Hassan Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_W/0/1/0/all/0/1\">Wei-Hong Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>",
          "description": "We present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Specifically, our\nVideo-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts\nmultimodal representations that are rich enough to benefit a variety of\ndownstream tasks. We train VATT end-to-end from scratch using multimodal\ncontrastive losses and evaluate its performance by the downstream tasks of\nvideo action recognition, audio event classification, image classification, and\ntext-to-video retrieval. Furthermore, we study a modality-agnostic\nsingle-backbone Transformer by sharing weights among the three modalities. We\nshow that the convolution-free VATT outperforms state-of-the-art ConvNet-based\narchitectures in the downstream tasks. Especially, VATT's vision Transformer\nachieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and\n41.1% on Moments in Time, new records while avoiding supervised pre-training.\nTransferring to image classification leads to 78.7% top-1 accuracy on ImageNet\ncompared to 64.7% by training the same Transformer from scratch, showing the\ngeneralizability of our model despite the domain gap between videos and images.\nVATT's audio Transformer also sets a new record on waveform-based audio event\nrecognition by achieving the mAP of 39.4% on AudioSet without any supervised\npre-training. VATT's source code is publicly available.",
          "link": "http://arxiv.org/abs/2104.11178",
          "publishedOn": "2021-08-16T00:47:33.756Z",
          "wordCount": 689,
          "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text. (arXiv:2104.11178v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michele_B/0/1/0/all/0/1\">Bj&#xf6;rn Michele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1\">Gilles Puy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>",
          "description": "While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D\nimages, its application to 3D data is still recent and scarce, with just a few\nmethods limited to classification. We present the first generative approach for\nboth ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both\nclassification and, for the first time, semantic segmentation. We show that it\nreaches or outperforms the state of the art on ModelNet40 classification for\nboth inductive ZSL and inductive GZSL. For semantic segmentation, we created\nthree benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and\nSemanticKITTI. Our experiments show that our method outperforms strong\nbaselines, which we additionally propose for this task.",
          "link": "http://arxiv.org/abs/2108.06230",
          "publishedOn": "2021-08-16T00:47:33.739Z",
          "wordCount": 557,
          "title": "Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Cloud. (arXiv:2108.06230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weixiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongtao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirikjian_G/0/1/0/all/0/1\">Gregory Chirikjian</a>",
          "description": "Probabilistic point cloud registration methods are becoming more popular\nbecause of their robustness. However, unlike point-to-plane variants of\niterative closest point (ICP) which incorporate local surface geometric\ninformation such as surface normals, most probabilistic methods (e.g., coherent\npoint drift (CPD)) ignore such information and build Gaussian mixture models\n(GMMs) with isotropic Gaussian covariances. This results in sphere-like GMM\ncomponents which only penalize the point-to-point distance between the two\npoint clouds. In this paper, we propose a novel method called CPD with Local\nSurface Geometry (LSG-CPD) for rigid point cloud registration. Our method\nadaptively adds different levels of point-to-plane penalization on top of the\npoint-to-point penalization based on the flatness of the local surface. This\nresults in GMM components with anisotropic covariances. We formulate point\ncloud registration as a maximum likelihood estimation (MLE) problem and solve\nit with the Expectation-Maximization (EM) algorithm. In the E step, we\ndemonstrate that the computation can be recast into simple matrix manipulations\nand efficiently computed on a GPU. In the M step, we perform an unconstrained\noptimization on a matrix Lie group to efficiently update the rigid\ntransformation of the registration. The proposed method outperforms\nstate-of-the-art algorithms in terms of accuracy and robustness on various\ndatasets captured with range scanners, RGBD cameras, and LiDARs. Also, it is\nsignificantly faster than modern implementations of CPD. The source code is\navailable at https://github.com/ChirikjianLab/LSG-CPD.git.",
          "link": "http://arxiv.org/abs/2103.15039",
          "publishedOn": "2021-08-16T00:47:33.733Z",
          "wordCount": 702,
          "title": "LSG-CPD: Coherent Point Drift with Local Surface Geometry for Point Cloud Registration. (arXiv:2103.15039v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhongyi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haoliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>",
          "description": "Unsupervised domain adaptation (UDA) enables a learning machine to adapt from\na labeled source domain to an unlabeled domain under the distribution shift.\nThanks to the strong representation ability of deep neural networks, recent\nremarkable achievements in UDA resort to learning domain-invariant features.\nIntuitively, the hope is that a good feature representation, together with the\nhypothesis learned from the source domain, can generalize well to the target\ndomain. However, the learning processes of domain-invariant features and source\nhypothesis inevitably involve domain-specific information that would degrade\nthe generalizability of UDA models on the target domain. In this paper,\nmotivated by the lottery ticket hypothesis that only partial parameters are\nessential for generalization, we find that only partial parameters are\nessential for learning domain-invariant information and generalizing well in\nUDA. Such parameters are termed transferable parameters. In contrast, the other\nparameters tend to fit domain-specific details and often fail to generalize,\nwhich we term as untransferable parameters. Driven by this insight, we propose\nTransferable Parameter Learning (TransPar) to reduce the side effect brought by\ndomain-specific information in the learning process and thus enhance the\nmemorization of domain-invariant information. Specifically, according to the\ndistribution discrepancy degree, we divide all parameters into transferable and\nuntransferable ones in each training iteration. We then perform separate\nupdates rules for the two types of parameters. Extensive experiments on image\nclassification and regression tasks (keypoint detection) show that TransPar\noutperforms prior arts by non-trivial margins. Moreover, experiments\ndemonstrate that TransPar can be integrated into the most popular deep UDA\nnetworks and be easily extended to handle any data distribution shift\nscenarios.",
          "link": "http://arxiv.org/abs/2108.06129",
          "publishedOn": "2021-08-16T00:47:33.726Z",
          "wordCount": 703,
          "title": "Learning Transferable Parameters for Unsupervised Domain Adaptation. (arXiv:2108.06129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burov_A/0/1/0/all/0/1\">Andrei Burov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>",
          "description": "We present a novel method for temporal coherent reconstruction and tracking\nof clothed humans. Given a monocular RGB-D sequence, we learn a person-specific\nbody model which is based on a dynamic surface function network. To this end,\nwe explicitly model the surface of the person using a multi-layer perceptron\n(MLP) which is embedded into the canonical space of the SMPL body model. With\nclassical forward rendering, the represented surface can be rasterized using\nthe topology of a template mesh. For each surface point of the template mesh,\nthe MLP is evaluated to predict the actual surface location. To handle\npose-dependent deformations, the MLP is conditioned on the SMPL pose\nparameters. We show that this surface representation as well as the pose\nparameters can be learned in a self-supervised fashion using the principle of\nanalysis-by-synthesis and differentiable rasterization. As a result, we are\nable to reconstruct a temporally coherent mesh sequence from the input data.\nThe underlying surface representation can be used to synthesize new animations\nof the reconstructed person including pose-dependent deformations.",
          "link": "http://arxiv.org/abs/2104.03978",
          "publishedOn": "2021-08-16T00:47:33.719Z",
          "wordCount": 648,
          "title": "Dynamic Surface Function Networks for Clothed Human Bodies. (arXiv:2104.03978v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youpeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grammenos_R/0/1/0/all/0/1\">Ryan Grammenos</a>",
          "description": "The ever-increasing amount of global refuse is overwhelming the waste and\nrecycling management industries. The need for smart systems for environmental\nmonitoring and the enhancement of recycling processes is thus greater than\never. Amongst these efforts lies IBM's Wastenet project which aims to improve\nrecycling by using artificial intelligence for waste classification. The work\nreported in this paper builds on this project through the use of transfer\nlearning and data augmentation techniques to ameliorate classification\naccuracy. Starting with a convolutional neural network (CNN), a systematic\napproach is followed for selecting appropriate splitting ratios and for tuning\nmultiple training parameters including learning rate schedulers, layers\nfreezing, batch sizes and loss functions, in the context of the given scenario\nwhich requires classification of waste into different recycling types. Results\nare compared and contrasted using 10-fold cross validation and demonstrate that\nthe model developed achieves a 91.21% test accuracy. Subsequently, a range of\ndata augmentation techniques are then incorporated into this work including\nflipping, rotation, shearing, zooming, and brightness control. Results show\nthat these augmentation techniques further improve the test accuracy of the\nfinal model to 95.40%. Unlike other work reported in the field, this paper\nprovides full details regarding the training of the model. Furthermore, the\ncode for this work has been made open-source and we have demonstrated that the\nmodel can perform successful real-time classification of recycling waste items\nusing a standard computer webcam.",
          "link": "http://arxiv.org/abs/2108.06274",
          "publishedOn": "2021-08-16T00:47:33.712Z",
          "wordCount": 676,
          "title": "Towards artificially intelligent recycling Improving image processing for waste classification. (arXiv:2108.06274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1909.11855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>",
          "description": "The transformer self-attention network has been extensively used in research\ndomains such as computer vision, image processing, and natural language\nprocessing. The transformer, however, has not been actively used in graph\nneural networks, where constructing an advanced aggregation function is\nessential. To this end, we present an effective model, named UGformer, which --\nby leveraging a transformer self-attention mechanism followed by a recurrent\ntransition -- induces an advanced aggregation function to learn graph\nrepresentations. Experimental results show that UGformer achieves\nstate-of-the-art accuracies on well-known benchmark datasets for graph\nclassification.",
          "link": "http://arxiv.org/abs/1909.11855",
          "publishedOn": "2021-08-16T00:47:33.693Z",
          "wordCount": 643,
          "title": "Universal Graph Transformer Self-Attention Networks. (arXiv:1909.11855v10 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1\">Giuseppe Scarpa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciotola_M/0/1/0/all/0/1\">Matteo Ciotola</a>",
          "description": "A reliable quality assessment procedure for pansharpening methods is of\ncritical importance for the development of the related solutions.\nUnfortunately, the lack of ground-truths to be used as guidance for an\nobjective evaluation has pushed the community to resort to either\nreference-based reduced-resolution indexes or to no-reference subjective\nquality indexes that can be applied on full-resolution datasets. In particular,\nthe reference-based approach leverages on Wald's protocol, a resolution\ndegradation process that allows one to synthesize data with related ground\ntruth. Both solutions, however, present critical shortcomings that we aim to\nmitigate in this work by means of an alternative no-reference full-resolution\nframework. On one side we introduce a protocol, namely the reprojection\nprotocol, which allows to handle the spectral fidelity problem. On the other\nside, a new index of the spatial consistency between the pansharpened image and\nthe panchromatic band at full resolution is proposed. The experimental results\nshow the effectiveness of the proposed approach which is confirmed also by\nvisual inspection.",
          "link": "http://arxiv.org/abs/2108.06144",
          "publishedOn": "2021-08-16T00:47:33.686Z",
          "wordCount": 587,
          "title": "Full-resolution quality assessment for pansharpening. (arXiv:2108.06144v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianzhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yating Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "Geometry Projection is a powerful depth estimation method in monocular 3D\nobject detection. It estimates depth dependent on heights, which introduces\nmathematical priors into the deep model. But projection process also introduces\nthe error amplification problem, in which the error of the estimated height\nwill be amplified and reflected greatly at the output depth. This property\nleads to uncontrollable depth inferences and also damages the training\nefficiency. In this paper, we propose a Geometry Uncertainty Projection Network\n(GUP Net) to tackle the error amplification problem at both inference and\ntraining stages. Specifically, a GUP module is proposed to obtains the\ngeometry-guided uncertainty of the inferred depth, which not only provides high\nreliable confidence for each depth but also benefits depth learning.\nFurthermore, at the training stage, we propose a Hierarchical Task Learning\nstrategy to reduce the instability caused by error amplification. This learning\nalgorithm monitors the learning situation of each task by a proposed indicator\nand adaptively assigns the proper loss weights for different tasks according to\ntheir pre-tasks situation. Based on that, each task starts learning only when\nits pre-tasks are learned well, which can significantly improve the stability\nand efficiency of the training process. Extensive experiments demonstrate the\neffectiveness of the proposed method. The overall model can infer more reliable\nobject depth than existing methods and outperforms the state-of-the-art\nimage-based monocular 3D detectors by 3.74% and 4.7% AP40 of the car and\npedestrian categories on the KITTI benchmark.",
          "link": "http://arxiv.org/abs/2107.13774",
          "publishedOn": "2021-08-16T00:47:33.679Z",
          "wordCount": 708,
          "title": "Geometry Uncertainty Projection Network for Monocular 3D Object Detection. (arXiv:2107.13774v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "GAN inversion aims to invert a given image back into the latent space of a\npretrained GAN model, for the image to be faithfully reconstructed from the\ninverted code by the generator. As an emerging technique to bridge the real and\nfake image domains, GAN inversion plays an essential role in enabling the\npretrained GAN models such as StyleGAN and BigGAN to be used for real image\nediting applications. Meanwhile, GAN inversion also provides insights on the\ninterpretation of GAN's latent space and how the realistic images can be\ngenerated. In this paper, we provide an overview of GAN inversion with a focus\non its recent algorithms and applications. We cover important techniques of GAN\ninversion and their applications to image restoration and image manipulation.\nWe further elaborate on some trends and challenges for future directions.",
          "link": "http://arxiv.org/abs/2101.05278",
          "publishedOn": "2021-08-16T00:47:33.659Z",
          "wordCount": 630,
          "title": "GAN Inversion: A Survey. (arXiv:2101.05278v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shugong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_S/0/1/0/all/0/1\">Shiyi Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yue Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyong Chen</a>",
          "description": "Although recent works based on deep learning have made progress in improving\nrecognition accuracy on scene text recognition, how to handle low-quality text\nimages in end-to-end deep networks remains a research challenge. In this paper,\nwe propose an Iterative Fusion based Recognizer (IFR) for low quality scene\ntext recognition, taking advantage of refined text images input and robust\nfeature representation. IFR contains two branches which focus on scene text\nrecognition and low quality scene text image recovery respectively. We utilize\nan iterative collaboration between two branches, which can effectively\nalleviate the impact of low quality input. A feature fusion module is proposed\nto strengthen the feature representation of the two branches, where the\nfeatures from the Recognizer are Fused with image Restoration branch, referred\nto as RRF. Without changing the recognition network structure, extensive\nquantitative and qualitative experimental results show that the proposed method\nsignificantly outperforms the baseline methods in boosting the recognition\naccuracy of benchmark datasets and low resolution images in TextZoom dataset.",
          "link": "http://arxiv.org/abs/2108.06166",
          "publishedOn": "2021-08-16T00:47:33.653Z",
          "wordCount": 610,
          "title": "IFR: Iterative Fusion Based Recognizer For Low Quality Scene Text Recognition. (arXiv:2108.06166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haomin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">T.Y. Alvin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_C/0/1/0/all/0/1\">Catalina Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_Z/0/1/0/all/0/1\">Zelia Correa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>",
          "description": "Algorithmic decision support is rapidly becoming a staple of personalized\nmedicine, especially for high-stakes recommendations in which access to certain\ninformation can drastically alter the course of treatment, and thus, patient\noutcome; a prominent example is radiomics for cancer subtyping. Because in\nthese scenarios the stakes are high, it is desirable for decision systems to\nnot only provide recommendations but supply transparent reasoning in support\nthereof. For learning-based systems, this can be achieved through an\ninterpretable design of the inference pipeline. Herein we describe an automated\nyet interpretable system for uveal melanoma subtyping with digital cytology\nimages from fine needle aspiration biopsies. Our method embeds every\nautomatically segmented cell of a candidate cytology image as a point in a 2D\nmanifold defined by many representative slides, which enables reasoning about\nthe cell-level composition of the tissue sample, paving the way for\ninterpretable subtyping of the biopsy. Finally, a rule-based slide-level\nclassification algorithm is trained on the partitions of the circularly\ndistorted 2D manifold. This process results in a simple rule set that is\nevaluated automatically but highly transparent for human verification. On our\nin house cytology dataset of 88 uveal melanoma patients, the proposed method\nachieves an accuracy of 87.5% that compares favorably to all competing\napproaches, including deep \"black box\" models. The method comes with a user\ninterface to facilitate interaction with cell-level content, which may offer\nadditional insights for pathological assessment.",
          "link": "http://arxiv.org/abs/2108.06246",
          "publishedOn": "2021-08-16T00:47:33.637Z",
          "wordCount": 691,
          "title": "An Interpretable Algorithm for Uveal Melanoma Subtyping from Whole Slide Cytology Images. (arXiv:2108.06246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Depu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zejia Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>",
          "description": "The recently-developed DETR approach applies the transformer encoder and\ndecoder architecture to object detection and achieves promising performance. In\nthis paper, we handle the critical issue, slow training convergence, and\npresent a conditional cross-attention mechanism for fast DETR training. Our\napproach is motivated by that the cross-attention in DETR relies highly on the\ncontent embeddings for localizing the four extremities and predicting the box,\nwhich increases the need for high-quality content embeddings and thus the\ntraining difficulty. Our approach, named conditional DETR, learns a conditional\nspatial query from the decoder embedding for decoder multi-head\ncross-attention. The benefit is that through the conditional spatial query,\neach cross-attention head is able to attend to a band containing a distinct\nregion, e.g., one object extremity or a region inside the object box. This\nnarrows down the spatial range for localizing the distinct regions for object\nclassification and box regression, thus relaxing the dependence on the content\nembeddings and easing the training. Empirical results show that conditional\nDETR converges 6.7x faster for the backbones R50 and R101 and 10x faster for\nstronger backbones DC5-R50 and DC5-R101. Code is available at\nhttps://git.io/ConditionalDETR.",
          "link": "http://arxiv.org/abs/2108.06152",
          "publishedOn": "2021-08-16T00:47:33.631Z",
          "wordCount": 633,
          "title": "Conditional DETR for Fast Training Convergence. (arXiv:2108.06152v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>",
          "description": "Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, most existing\nlearning-based approaches usually suffer from limited manually annotated\nmedical data, which poses a major practical problem for accurate and robust\nmedical image segmentation. In addition, most existing semi-supervised\napproaches are usually not robust compared with the supervised counterparts,\nand also lack explicit modeling of geometric structure and semantic\ninformation, both of which limit the segmentation accuracy. In this work, we\npresent SimCVD, a simple contrastive distillation framework that significantly\nadvances state-of-the-art voxel-wise representation learning. We first describe\nan unsupervised training strategy, which takes two views of an input volume and\npredicts their signed distance maps of object boundaries in a contrastive\nobjective, with only two independent dropout as mask. This simple approach\nworks surprisingly well, performing on the same level as previous fully\nsupervised methods with much less labeled data. We hypothesize that dropout can\nbe viewed as a minimal form of data augmentation and makes the network robust\nto representation collapse. Then, we propose to perform structural distillation\nby distilling pair-wise similarities. We evaluate SimCVD on two popular\ndatasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\ndataset. The results on the LA dataset demonstrate that, in two types of\nlabeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\nprevious best results. Our method can be trained in an end-to-end fashion,\nshowing the promise of utilizing SimCVD as a general framework for downstream\ntasks, such as medical image synthesis and registration.",
          "link": "http://arxiv.org/abs/2108.06227",
          "publishedOn": "2021-08-16T00:47:33.612Z",
          "wordCount": 724,
          "title": "SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tailor_S/0/1/0/all/0/1\">Shyam A. Tailor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_R/0/1/0/all/0/1\">Ren&#xe9; de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azevedo_T/0/1/0/all/0/1\">Tiago Azevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1\">Matthew Mattina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_P/0/1/0/all/0/1\">Partha Maji</a>",
          "description": "In recent years graph neural network (GNN)-based approaches have become a\npopular strategy for processing point cloud data, regularly achieving\nstate-of-the-art performance on a variety of tasks. To date, the research\ncommunity has primarily focused on improving model expressiveness, with\nsecondary thought given to how to design models that can run efficiently on\nresource constrained mobile devices including smartphones or mixed reality\nheadsets. In this work we make a step towards improving the efficiency of these\nmodels by making the observation that these GNN models are heavily limited by\nthe representational power of their first, feature extracting, layer. We find\nthat it is possible to radically simplify these models so long as the feature\nextraction layer is retained with minimal degradation to model performance;\nfurther, we discover that it is possible to improve performance overall on\nModelNet40 and S3DIS by improving the design of the feature extractor. Our\napproach reduces memory consumption by 20$\\times$ and latency by up to\n9.9$\\times$ for graph layers in models such as DGCNN; overall, we achieve\nspeed-ups of up to 4.5$\\times$ and peak memory reductions of 72.5%.",
          "link": "http://arxiv.org/abs/2108.06317",
          "publishedOn": "2021-08-16T00:47:33.570Z",
          "wordCount": 645,
          "title": "Towards Efficient Point Cloud Graph Neural Networks Through Architectural Simplification. (arXiv:2108.06317v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08871",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Haan_K/0/1/0/all/0/1\">Kevin de Haan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yijie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuckerman_J/0/1/0/all/0/1\">Jonathan E. Zuckerman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisk_A/0/1/0/all/0/1\">Anthony E. Sisk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_M/0/1/0/all/0/1\">Miguel F. P. Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jen_K/0/1/0/all/0/1\">Kuang-Yu Jen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nobori_A/0/1/0/all/0/1\">Alexander Nobori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liou_S/0/1/0/all/0/1\">Sofia Liou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Sarah Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riahi_R/0/1/0/all/0/1\">Rana Riahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivenson_Y/0/1/0/all/0/1\">Yair Rivenson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wallace_W/0/1/0/all/0/1\">W. Dean Wallace</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>",
          "description": "Pathology is practiced by visual inspection of histochemically stained\nslides. Most commonly, the hematoxylin and eosin (H&E) stain is used in the\ndiagnostic workflow and it is the gold standard for cancer diagnosis. However,\nin many cases, especially for non-neoplastic diseases, additional \"special\nstains\" are used to provide different levels of contrast and color to tissue\ncomponents and allow pathologists to get a clearer diagnostic picture. In this\nstudy, we demonstrate the utility of supervised learning-based computational\nstain transformation from H&E to different special stains (Masson's Trichrome,\nperiodic acid-Schiff and Jones silver stain) using tissue sections from kidney\nneedle core biopsies. Based on evaluation by three renal pathologists, followed\nby adjudication by a fourth renal pathologist, we show that the generation of\nvirtual special stains from existing H&E images improves the diagnosis in\nseveral non-neoplastic kidney diseases sampled from 58 unique subjects. A\nsecond study performed by three pathologists found that the quality of the\nspecial stains generated by the stain transformation network was statistically\nequivalent to those generated through standard histochemical staining. As the\ntransformation of H&E images into special stains can be achieved within 1 min\nor less per patient core specimen slide, this stain-to-stain transformation\nframework can improve the quality of the preliminary diagnosis when additional\nspecial stains are needed, along with significant savings in time and cost,\nreducing the burden on healthcare system and patients.",
          "link": "http://arxiv.org/abs/2008.08871",
          "publishedOn": "2021-08-16T00:47:33.564Z",
          "wordCount": 763,
          "title": "Deep learning-based transformation of the H&E stain into special stains. (arXiv:2008.08871v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nesti_F/0/1/0/all/0/1\">Federico Nesti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossolini_G/0/1/0/all/0/1\">Giulio Rossolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Saasha Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_A/0/1/0/all/0/1\">Alessandro Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttazzo_G/0/1/0/all/0/1\">Giorgio Buttazzo</a>",
          "description": "Deep learning and convolutional neural networks allow achieving impressive\nperformance in computer vision tasks, such as object detection and semantic\nsegmentation (SS). However, recent studies have shown evident weaknesses of\nsuch models against adversarial perturbations. In a real-world scenario\ninstead, like autonomous driving, more attention should be devoted to\nreal-world adversarial examples (RWAEs), which are physical objects (e.g.,\nbillboards and printable patches) optimized to be adversarial to the entire\nperception pipeline. This paper presents an in-depth evaluation of the\nrobustness of popular SS models by testing the effects of both digital and\nreal-world adversarial patches. These patches are crafted with powerful attacks\nenriched with a novel loss function. Firstly, an investigation on the\nCityscapes dataset is conducted by extending the Expectation Over\nTransformation (EOT) paradigm to cope with SS. Then, a novel attack\noptimization, called scene-specific attack, is proposed. Such an attack\nleverages the CARLA driving simulator to improve the transferability of the\nproposed EOT-based attack to a real 3D environment. Finally, a printed physical\nbillboard containing an adversarial patch was tested in an outdoor driving\nscenario to assess the feasibility of the studied attacks in the real world.\nExhaustive experiments revealed that the proposed attack formulations\noutperform previous work to craft both digital and real-world adversarial\npatches for SS. At the same time, the experimental results showed how these\nattacks are notably less effective in the real world, hence questioning the\npractical relevance of adversarial attacks to SS models for autonomous/assisted\ndriving.",
          "link": "http://arxiv.org/abs/2108.06179",
          "publishedOn": "2021-08-16T00:47:33.549Z",
          "wordCount": 692,
          "title": "Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks. (arXiv:2108.06179v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.11582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_P/0/1/0/all/0/1\">Panhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xuejing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Lizhu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_A/0/1/0/all/0/1\">Anlong Ming</a>",
          "description": "Occlusion relationship reasoning based on convolution neural networks\nconsists of two subtasks: occlusion boundary extraction and occlusion\norientation inference. Due to the essential differences between the two\nsubtasks in the feature expression at the higher and lower stages, it is\nchallenging to carry on them simultaneously in one network. To address this\nissue, we propose a novel Dual-path Decoder Network, which uniformly extracts\nocclusion information at higher stages and separates into two paths to recover\nboundary and occlusion orientation respectively in lower stages. Besides,\nconsidering the restriction of occlusion orientation presentation to occlusion\norientation learning, we design a new orthogonal representation for occlusion\norientation and proposed the Orthogonal Orientation Regression loss which can\nget rid of the unfitness between occlusion representation and learning and\nfurther prompt the occlusion orientation learning. Finally, we apply a\nmulti-scale loss together with our proposed orientation regression loss to\nguide the boundary and orientation path learning respectively. Experiments\ndemonstrate that our proposed method achieves state-of-the-art results on PIOD\nand BSDS ownership datasets.",
          "link": "http://arxiv.org/abs/1911.11582",
          "publishedOn": "2021-08-16T00:47:33.527Z",
          "wordCount": 655,
          "title": "DDNet: Dual-path Decoder Network for Occlusion Relationship Reasoning. (arXiv:1911.11582v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>",
          "description": "Vision-language Navigation (VLN) tasks require an agent to navigate\nstep-by-step while perceiving the visual observations and comprehending a\nnatural language instruction. Large data bias, which is caused by the disparity\nratio between the small data scale and large navigation space, makes the VLN\ntask challenging. Previous works have proposed various data augmentation\nmethods to reduce data bias. However, these works do not explicitly reduce the\ndata bias across different house scenes. Therefore, the agent would overfit to\nthe seen scenes and achieve poor navigation performance in the unseen scenes.\nTo tackle this problem, we propose the Random Environmental Mixup (REM) method,\nwhich generates cross-connected house scenes as augmented data via mixuping\nenvironment. Specifically, we first select key viewpoints according to the room\nconnection graph for each scene. Then, we cross-connect the key views of\ndifferent scenes to construct augmented scenes. Finally, we generate augmented\ninstruction-path pairs in the cross-connected scenes. The experimental results\non benchmark datasets demonstrate that our augmentation data via REM help the\nagent reduce its performance gap between the seen and unseen environment and\nimprove the overall performance, making our model the best existing approach on\nthe standard VLN benchmark.",
          "link": "http://arxiv.org/abs/2106.07876",
          "publishedOn": "2021-08-16T00:47:33.521Z",
          "wordCount": 661,
          "title": "Vision-Language Navigation with Random Environmental Mixup. (arXiv:2106.07876v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anthony Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murez_Z/0/1/0/all/0/1\">Zak Murez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_N/0/1/0/all/0/1\">Nikhil Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudas_S/0/1/0/all/0/1\">Sof&#xed;a Dudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawke_J/0/1/0/all/0/1\">Jeffrey Hawke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badrinarayanan_V/0/1/0/all/0/1\">Vijay Badrinarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kendall_A/0/1/0/all/0/1\">Alex Kendall</a>",
          "description": "Driving requires interacting with road agents and predicting their future\nbehaviour in order to navigate safely. We present FIERY: a probabilistic future\nprediction model in bird's-eye view from monocular cameras. Our model predicts\nfuture instance segmentation and motion of dynamic agents that can be\ntransformed into non-parametric future trajectories. Our approach combines the\nperception, sensor fusion and prediction components of a traditional autonomous\ndriving stack by estimating bird's-eye-view prediction directly from surround\nRGB monocular camera inputs. FIERY learns to model the inherent stochastic\nnature of the future solely from camera driving data in an end-to-end manner,\nwithout relying on HD maps, and predicts multimodal future trajectories. We\nshow that our model outperforms previous prediction baselines on the NuScenes\nand Lyft datasets. The code and trained models are available at\nhttps://github.com/wayveai/fiery.",
          "link": "http://arxiv.org/abs/2104.10490",
          "publishedOn": "2021-08-16T00:47:33.516Z",
          "wordCount": 618,
          "title": "FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras. (arXiv:2104.10490v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drees_D/0/1/0/all/0/1\">Dominik Drees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eilers_F/0/1/0/all/0/1\">Florian Eilers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyi Jiang</a>",
          "description": "The random walker method for image segmentation is a popular tool for\nsemi-automatic image segmentation, especially in the biomedical field. However,\nits linear asymptotic run time and memory requirements make application to 3D\ndatasets of increasing sizes impractical. We propose a hierarchical framework\nthat, to the best of our knowledge, is the first attempt to overcome these\nrestrictions for the random walker algorithm and achieves sublinear run time\nand constant memory complexity. The goal of this framework is -- rather than\nimproving the segmentation quality compared to the baseline method -- to make\ninteractive segmentation on out-of-core datasets possible. The method is\nevaluated quantitavely on synthetic data and the CT-ORG dataset where the\nexpected improvements in algorithm run time while maintaining high segmentation\nquality are confirmed. The incremental (i.e., interaction update) run time is\ndemonstrated to be in seconds on a standard PC even for volumes of hundreds of\nGigabytes in size. In a small case study the applicability to large real world\nfrom current biomedical research is demonstrated. An implementation of the\npresented method is publicly available in version 5.2 of the widely used volume\nrendering and processing software Voreen (https://www.uni-muenster.de/Voreen/).",
          "link": "http://arxiv.org/abs/2103.09564",
          "publishedOn": "2021-08-16T00:47:33.510Z",
          "wordCount": 657,
          "title": "Hierarchical Random Walker Segmentation for Large Volumetric Biomedical Images. (arXiv:2103.09564v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenpeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuemiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>",
          "description": "Existing GAN inversion methods are stuck in a paradox that the inverted codes\ncan either achieve high-fidelity reconstruction, or retain the editing\ncapability. Having only one of them clearly cannot realize real image editing.\nIn this paper, we resolve this paradox by introducing consecutive images (\\eg,\nvideo frames or the same person with different poses) into the inversion\nprocess. The rationale behind our solution is that the continuity of\nconsecutive images leads to inherent editable directions. This inborn property\nis used for two unique purposes: 1) regularizing the joint inversion process,\nsuch that each of the inverted code is semantically accessible from one of the\nother and fastened in a editable domain; 2) enforcing inter-image coherence,\nsuch that the fidelity of each inverted code can be maximized with the\ncomplement of other images. Extensive experiments demonstrate that our\nalternative significantly outperforms state-of-the-art methods in terms of\nreconstruction fidelity and editability on both the real image dataset and\nsynthesis dataset. Furthermore, our method provides the first support of\nvideo-based GAN inversion, and an interesting application of unsupervised\nsemantic transfer from consecutive images. Source code can be found at:\n\\url{https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs}.",
          "link": "http://arxiv.org/abs/2107.13812",
          "publishedOn": "2021-08-16T00:47:33.505Z",
          "wordCount": 662,
          "title": "From Continuity to Editability: Inverting GANs with Consecutive Images. (arXiv:2107.13812v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quang Huy Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_J/0/1/0/all/0/1\">Jae Kyu Suhr</a>",
          "description": "Autonomous parking systems start with the detection of available parking\nslots. Parking slot detection performance has been dramatically improved by\ndeep learning techniques. Deep learning-based object detection methods can be\ncategorized into one-stage and two-stage approaches. Although it is well-known\nthat the two-stage approach outperforms the one-stage approach in general\nobject detection, they have performed similarly in parking slot detection so\nfar. We consider this is because the two-stage approach has not yet been\nadequately specialized for parking slot detection. Thus, this paper proposes a\nhighly specialized two-stage parking slot detector that uses region-specific\nmulti-scale feature extraction. In the first stage, the proposed method finds\nthe entrance of the parking slot as a region proposal by estimating its center,\nlength, and orientation. The second stage of this method designates specific\nregions that most contain the desired information and extracts features from\nthem. That is, features for the location and orientation are separately\nextracted from only the specific regions that most contain the locational and\norientational information. In addition, multi-resolution feature maps are\nutilized to increase both positioning and classification accuracies. A\nhigh-resolution feature map is used to extract detailed information (location\nand orientation), while another low-resolution feature map is used to extract\nsemantic information (type and occupancy). In experiments, the proposed method\nwas quantitatively evaluated with two large-scale public parking slot detection\ndatasets and outperformed previous methods, including both one-stage and\ntwo-stage approaches.",
          "link": "http://arxiv.org/abs/2108.06185",
          "publishedOn": "2021-08-16T00:47:33.481Z",
          "wordCount": 671,
          "title": "CNN-based Two-Stage Parking Slot Detection Using Region-Specific Multi-Scale Feature Extraction. (arXiv:2108.06185v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1\">Junjie Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>",
          "description": "Image quality assessment (IQA) is an important research topic for\nunderstanding and improving visual experience. The current state-of-the-art IQA\nmethods are based on convolutional neural networks (CNNs). The performance of\nCNN-based models is often compromised by the fixed shape constraint in batch\ntraining. To accommodate this, the input images are usually resized and cropped\nto a fixed shape, causing image quality degradation. To address this, we design\na multi-scale image quality Transformer (MUSIQ) to process native resolution\nimages with varying sizes and aspect ratios. With a multi-scale image\nrepresentation, our proposed method can capture image quality at different\ngranularities. Furthermore, a novel hash-based 2D spatial embedding and a scale\nembedding is proposed to support the positional embedding in the multi-scale\nrepresentation. Experimental results verify that our method can achieve\nstate-of-the-art performance on multiple large scale IQA datasets such as\nPaQ-2-PiQ, SPAQ and KonIQ-10k.",
          "link": "http://arxiv.org/abs/2108.05997",
          "publishedOn": "2021-08-16T00:47:33.453Z",
          "wordCount": 578,
          "title": "MUSIQ: Multi-scale Image Quality Transformer. (arXiv:2108.05997v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1\">Tianrui Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1\">Divya Kothandaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyamoorthy_A/0/1/0/all/0/1\">Adarsh Jagan Sathyamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a new learning-based method for identifying safe and navigable\nregions in off-road terrains and unstructured environments from RGB images. Our\napproach consists of classifying groups of terrains based on their navigability\nlevels using coarse-grained semantic segmentation. We propose a bottleneck\ntransformer-based deep neural network architecture that uses a novel group-wise\nattention mechanism to distinguish between navigability levels of different\nterrains. Our group-wise attention heads enable the network to explicitly focus\non the different groups and improve the accuracy. We show through extensive\nevaluations on the RUGD and RELLIS-3D datasets that our learning algorithm\nimproves visual perception accuracy in off-road terrains for navigation. We\ncompare our approach with prior work on these datasets and achieve an\nimprovement over the state-of-the-art mIoU by 6.74-39.1% on RUGD and\n3.82-10.64% on RELLIS-3D. In addition, we deploy our method on a Clearpath\nJackal robot. Our approach improves the performance of the navigation algorithm\nin terms of average progress towards the goal by 54.73% and the false positives\nin terms of forbidden region by 29.96%.",
          "link": "http://arxiv.org/abs/2103.04233",
          "publishedOn": "2021-08-16T00:47:33.447Z",
          "wordCount": 644,
          "title": "GANav: Group-wise Attention Network for Classifying Navigable Regions in Unstructured Outdoor Environments. (arXiv:2103.04233v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1\">Hrishav Bakul Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mg_T/0/1/0/all/0/1\">Theint Haythi Mg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_P/0/1/0/all/0/1\">Pradip Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_C/0/1/0/all/0/1\">Chayan Sarkar</a>",
          "description": "Robots in our daily surroundings are increasing day by day. Their usability\nand acceptability largely depend on their explicit and implicit interaction\ncapability with fellow human beings. As a result, social behavior is one of the\nmost sought-after qualities that a robot can possess. However, there is no\nspecific aspect and/or feature that defines socially acceptable behavior and it\nlargely depends on the situation, application, and society. In this article, we\ninvestigate one such social behavior for collocated robots. Imagine a group of\npeople is interacting with each other and we want to join the group. We as\nhuman beings do it in a socially acceptable manner, i.e., within the group, we\ndo position ourselves in such a way that we can participate in the group\nactivity without disturbing/obstructing anybody. To possess such a quality,\nfirst, a robot needs to determine the formation of the group and then determine\na position for itself, which we humans do implicitly. The theory of f-formation\ncan be utilized for this purpose. As the types of formations can be very\ndiverse, detecting the social groups is not a trivial task. In this article, we\nprovide a comprehensive survey of the existing work on social interaction and\ngroup detection using f-formation for robotics and other applications. We also\nput forward a novel holistic survey framework combining all the possible\nconcerns and modules relevant to this problem. We define taxonomies based on\nmethods, camera views, datasets, detection capabilities and scale, evaluation\napproaches, and application areas. We discuss certain open challenges and\nlimitations in current literature along with possible future research\ndirections based on this framework. In particular, we discuss the existing\nmethods/techniques and their relative merits and demerits, applications, and\nprovide a set of unsolved but relevant problems in this domain.",
          "link": "http://arxiv.org/abs/2108.06181",
          "publishedOn": "2021-08-16T00:47:33.429Z",
          "wordCount": 790,
          "title": "Detecting socially interacting groups using f-formation: A survey of taxonomy, methods, datasets, applications, challenges, and future research directions. (arXiv:2108.06181v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohaimenuzzaman_M/0/1/0/all/0/1\">Md Mohaimenuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1\">Christoph Bergmeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_B/0/1/0/all/0/1\">Bernd Meyer</a>",
          "description": "Deep Learning has celebrated resounding successes in many application areas\nof relevance to the Internet-of-Things, for example, computer vision and\nmachine listening. To fully harness the power of deep leaning for the IoT,\nthese technologies must ultimately be brought directly to the edge. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization and the recent advancement of XNOR-Net. This paper examines the\nsuitability of these techniques for audio classification in microcontrollers.\nWe present an XNOR-Net for end-to-end raw audio classification and a\ncomprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring about 8x more computation. We\nshow that these insights are consistent between raw audio classification and\nimage classification using standard benchmark sets.To the best of our\nknowledge, this is the first study applying XNOR to end-to-end audio\nclassification and evaluating it in the context of alternative techniques. All\ncode is publicly available on GitHub.",
          "link": "http://arxiv.org/abs/2108.06128",
          "publishedOn": "2021-08-16T00:47:33.404Z",
          "wordCount": 682,
          "title": "Pruning vs XNOR-Net: A Comprehensive Study on Deep Learning for Audio Classification in Microcontrollers. (arXiv:2108.06128v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taher_M/0/1/0/all/0/1\">Mohammad Reza Hosseinzadeh Taher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghighi_F/0/1/0/all/0/1\">Fatemeh Haghighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruibin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotway_M/0/1/0/all/0/1\">Michael B. Gotway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jianming Liang</a>",
          "description": "Transfer learning from supervised ImageNet models has been frequently used in\nmedical image analysis. Yet, no large-scale evaluation has been conducted to\nbenchmark the efficacy of newly-developed pre-training techniques for medical\nimage analysis, leaving several important questions unanswered. As the first\nstep in this direction, we conduct a systematic study on the transferability of\nmodels pre-trained on iNat2021, the most recent large-scale fine-grained\ndataset, and 14 top self-supervised ImageNet models on 7 diverse medical tasks\nin comparison with the supervised ImageNet model. Furthermore, we present a\npractical approach to bridge the domain gap between natural and medical images\nby continually (pre-)training supervised ImageNet models on medical images. Our\ncomprehensive evaluation yields new insights: (1) pre-trained models on\nfine-grained data yield distinctive local representations that are more\nsuitable for medical segmentation tasks, (2) self-supervised ImageNet models\nlearn holistic features more effectively than supervised ImageNet models, and\n(3) continual pre-training can bridge the domain gap between natural and\nmedical images. We hope that this large-scale open evaluation of transfer\nlearning can direct the future research of deep learning for medical imaging.\nAs open science, all codes and pre-trained models are available on our GitHub\npage https://github.com/JLiangLab/BenchmarkTransferLearning.",
          "link": "http://arxiv.org/abs/2108.05930",
          "publishedOn": "2021-08-16T00:47:33.395Z",
          "wordCount": 671,
          "title": "A Systematic Benchmarking Analysis of Transfer Learning for Medical Image Analysis. (arXiv:2108.05930v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huanfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Menghui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenxia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1\">Qiangqiang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In the fields of image restoration and image fusion, model-driven methods and\ndata-driven methods are the two representative frameworks. However, both\napproaches have their respective advantages and disadvantages. The model-driven\nmethods consider the imaging mechanism, which is deterministic and\ntheoretically reasonable; however, they cannot easily model complicated\nnonlinear problems. The data-driven methods have a stronger prior knowledge\nlearning capability for huge data, especially for nonlinear statistical\nfeatures; however, the interpretability of the networks is poor, and they are\nover-dependent on training data. In this paper, we systematically investigate\nthe coupling of model-driven and data-driven methods, which has rarely been\nconsidered in the remote sensing image restoration and fusion communities. We\nare the first to summarize the coupling approaches into the following three\ncategories: 1) data-driven and model-driven cascading methods; 2) variational\nmodels with embedded learning; and 3) model-constrained network learning\nmethods. The typical existing and potential coupling methods for remote sensing\nimage restoration and fusion are introduced with application examples. This\npaper also gives some new insights into the potential future directions, in\nterms of both methods and applications.",
          "link": "http://arxiv.org/abs/2108.06073",
          "publishedOn": "2021-08-16T00:47:33.390Z",
          "wordCount": 626,
          "title": "Coupling Model-Driven and Data-Driven Methods for Remote Sensing Image Restoration and Fusion. (arXiv:2108.06073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hyeon_Woo_N/0/1/0/all/0/1\">Nam Hyeon-Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Bin_M/0/1/0/all/0/1\">Moon Ye-Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>",
          "description": "To overcome the burdens on frequent model uploads and downloads during\nfederated learning (FL), we propose a communication-efficient\nre-parameterization, FedPara. Our method re-parameterizes the model's layers\nusing low-rank matrices or tensors followed by the Hadamard product. Different\nfrom the conventional low-rank parameterization, our method is not limited to\nlow-rank constraints. Thereby, our FedPara has a larger capacity than the\nlow-rank one, even with the same number of parameters. It can achieve\ncomparable performance to the original models while requiring 2.8 to 10.1 times\nlower communication costs than the original models, which is not achievable by\nthe traditional low-rank parameterization. Moreover, the efficiency can be\nfurther improved by combining our method and other efficient FL techniques\nbecause our method is compatible with others. We also extend our method to a\npersonalized FL application, pFedPara, which separates parameters into global\nand local ones. We show that pFedPara outperforms competing personalized FL\nmethods with more than three times fewer parameters.",
          "link": "http://arxiv.org/abs/2108.06098",
          "publishedOn": "2021-08-16T00:47:33.256Z",
          "wordCount": 594,
          "title": "FedPara: Low-rank Hadamard Product Parameterization for Efficient Federated Learning. (arXiv:2108.06098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fengming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xuelei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianhang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiguang Liu</a>",
          "description": "Real-time detection and tracking of fast-moving objects have achieved great\nsuccess in various fields. However, many existing methods, especially low-cost\nones, are difficult to achieve real-time and long-term object detection and\ntracking. Here, a non-imaging strategy is proposed, including two stages, to\nrealize fast-moving object detection and tracking in real-time and for the long\nterm: 1) a contour-moments-based method is proposed to optimize the Hadamard\npattern sequence. And then reconstructing projection curves of the object based\non single-pixel imaging technology. The projection curve, which including the\nobject location information, is reconstructed directly with the measurements\ncollected by a single-pixel detector; 2) The fastest changing position in the\nprojection curve can be obtained by solving first-order gradients. A gradient\ndifferential is used in two first-order gradients to calculate a differential\ncurve with the sudden change positions. Finally, we can obtain the boundary\ninformation of the fast-moving object. We experimentally demonstrate that our\napproach can achieve a temporal resolution of 105 frames per second at a 1.28%\nsampling rate by using a 22,000 Hz digital micro-mirror device. The detection\nand tracking algorithm of the proposed strategy is computationally efficient.\nCompared with the state-of-the-art methods, our approach can make the sampling\nrate lower. Additionally, the strategy acquires not more than 1MB of data for\neach frame, which is capable of fast-moving object real-time and long-term\ndetection and tracking.",
          "link": "http://arxiv.org/abs/2108.06009",
          "publishedOn": "2021-08-16T00:47:33.232Z",
          "wordCount": 667,
          "title": "Non-imaging real-time detection and tracking of fast-moving objects. (arXiv:2108.06009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Haocheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xinyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhao Wu</a>",
          "description": "Due to the sparsity and irregularity of the 3D data, approaches that directly\nprocess points have become popular. Among all point-based models,\nTransformer-based models have achieved state-of-the-art performance by fully\npreserving point interrelation. However, most of them spend high percentage of\ntotal time on sparse data accessing (e.g., Farthest Point Sampling (FPS) and\nneighbor points query), which becomes the computation burden. Therefore, we\npresent a novel 3D Transformer, called Point-Voxel Transformer (PVT) that\nleverages self-attention computation in points to gather global context\nfeatures, while performing multi-head self-attention (MSA) computation in\nvoxels to capture local information and reduce the irregular data access.\nAdditionally, to further reduce the cost of MSA computation, we design a cyclic\nshifted boxing scheme which brings greater efficiency by limiting the MSA\ncomputation to non-overlapping local boxes while also preserving cross-box\nconnection. Our method fully exploits the potentials of Transformer\narchitecture, paving the road to efficient and accurate recognition results.\nEvaluated on classification and segmentation benchmarks, our PVT not only\nachieves strong accuracy but outperforms previous state-of-the-art\nTransformer-based models with 9x measured speedup on average. For 3D object\ndetection task, we replace the primitives in Frustrum PointNet with PVT layer\nand achieve the improvement of 8.6%.",
          "link": "http://arxiv.org/abs/2108.06076",
          "publishedOn": "2021-08-16T00:47:33.225Z",
          "wordCount": 645,
          "title": "Point-Voxel Transformer: An Efficient Approach To 3D Deep Learning. (arXiv:2108.06076v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiaopeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Riquan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Litong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huabin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>",
          "description": "Deep semi-supervised learning (SSL) has experienced significant attention in\nrecent years, to leverage a huge amount of unlabeled data to improve the\nperformance of deep learning with limited labeled data. Pseudo-labeling is a\npopular approach to expand the labeled dataset. However, whether there is a\nmore effective way of labeling remains an open problem. In this paper, we\npropose to label only the most representative samples to expand the labeled\nset. Representative samples, selected by indegree of corresponding nodes on a\ndirected k-nearest neighbor (kNN) graph, lie in the k-nearest neighborhood of\nmany other samples. We design a graph neural network (GNN) labeler to label\nthem in a progressive learning manner. Aided by the progressive GNN labeler,\nour deep SSL approach outperforms state-of-the-art methods on several popular\nSSL benchmarks including CIFAR-10, SVHN, and ILSVRC-2012. Notably, we achieve\n72.1% top-1 accuracy, surpassing the previous best result by 3.3%, on the\nchallenging ImageNet benchmark with only $10\\%$ labeled data.",
          "link": "http://arxiv.org/abs/2108.06070",
          "publishedOn": "2021-08-16T00:47:33.219Z",
          "wordCount": 594,
          "title": "Progressive Representative Labeling for Deep Semi-Supervised Learning. (arXiv:2108.06070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuefan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Shinjae Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuewei Lin</a>",
          "description": "While deep neural networks have shown impressive performance in many tasks,\nthey are fragile to carefully designed adversarial attacks. We propose a novel\nadversarial training-based model by Attention Guided Knowledge Distillation and\nBi-directional Metric Learning (AGKD-BML). The attention knowledge is obtained\nfrom a weight-fixed model trained on a clean dataset, referred to as a teacher\nmodel, and transferred to a model that is under training on adversarial\nexamples (AEs), referred to as a student model. In this way, the student model\nis able to focus on the correct region, as well as correcting the intermediate\nfeatures corrupted by AEs to eventually improve the model accuracy. Moreover,\nto efficiently regularize the representation in feature space, we propose a\nbidirectional metric learning. Specifically, given a clean image, it is first\nattacked to its most confusing class to get the forward AE. A clean image in\nthe most confusing class is then randomly picked and attacked back to the\noriginal class to get the backward AE. A triplet loss is then used to shorten\nthe representation distance between original image and its AE, while enlarge\nthat between the forward and backward AEs. We conduct extensive adversarial\nrobustness experiments on two widely used datasets with different attacks. Our\nproposed AGKD-BML model consistently outperforms the state-of-the-art\napproaches. The code of AGKD-BML will be available at:\nhttps://github.com/hongw579/AGKD-BML.",
          "link": "http://arxiv.org/abs/2108.06017",
          "publishedOn": "2021-08-16T00:47:33.206Z",
          "wordCount": 676,
          "title": "AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning. (arXiv:2108.06017v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haitao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>",
          "description": "Semantic change detection (SCD) extends the change detection (CD) task to\nprovide not only the change locations but also the detailed semantic categories\n(before and after the observation intervals). This fine-grained change\ninformation is more useful in land-cover/land-use (LC/LU) applications. Recent\nstudies indicate that the SCD can be modeled through a triple-branch\nConvolutional Neural Network (CNN), which contains two temporal branches and a\nchange branch. However, in this architecture, the connections between the\ntemporal branches and the change branch are weak. To overcome these\nlimitations, we propose a novel CNN architecture for the SCD, where the\ntemporal features are re-used and are deeply merged in the temporal branch.\nFurthermore, we elaborate on this architecture to model the bi-temporal\nsemantic correlations. The resulting Bi-temporal Semantic Reasoning Network\n(Bi-SRNet) contains two types of semantic reasoning blocks to reason both\nsingle-temporal and cross-temporal semantic correlations, as well as a novel\nloss function to improve the semantic consistency of change detection results.\nExperimental results on a benchmark dataset show that the proposed architecture\nobtains significant accuracy improvements over the existing approaches, while\nthe added designs in the Bi-SRNet further improves the segmentation of both\nsemantic categories and the changed areas. The codes in this paper are\naccessible at: https://github.com/ggsDing/Bi-SRNet",
          "link": "http://arxiv.org/abs/2108.06103",
          "publishedOn": "2021-08-16T00:47:33.200Z",
          "wordCount": 671,
          "title": "Bi-Temporal Semantic Reasoning for the Semantic Change Detection of HR Remote Sensing Images. (arXiv:2108.06103v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Real world traffic sign recognition is an important step towards building\nautonomous vehicles, most of which highly dependent on Deep Neural Networks\n(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to\nadversarial examples. Many attack methods have been proposed to understand and\ngenerate adversarial examples, such as gradient based attack, score based\nattack, decision based attack, and transfer based attacks. However, most of\nthese algorithms are ineffective in real-world road sign attack, because (1)\niteratively learning perturbations for each frame is not realistic for a fast\nmoving car and (2) most optimization algorithms traverse all pixels equally\nwithout considering their diverse contribution. To alleviate these problems,\nthis paper proposes the targeted attention attack (TAA) method for real world\nroad sign attack. Specifically, we have made the following contributions: (1)\nwe leverage the soft attention map to highlight those important pixels and skip\nthose zero-contributed areas - this also helps to generate natural\nperturbations, (2) we design an efficient universal attack that optimizes a\nsingle perturbation/noise based on a set of training images under the guidance\nof the pre-trained attention map, (3) we design a simple objective function\nthat can be easily optimized, (4) we evaluate the effectiveness of TAA on real\nworld data sets. Experimental results validate that the TAA method improves the\nattack successful rate (nearly 10%) and reduces the perturbation loss (about a\nquarter) compared with the popular RP2 method. Additionally, our TAA also\nprovides good properties, e.g., transferability and generalization capability.\nWe provide code and data to ensure the reproducibility:\nhttps://github.com/AdvAttack/RoadSignAttack.",
          "link": "http://arxiv.org/abs/2010.04331",
          "publishedOn": "2021-08-16T00:47:33.193Z",
          "wordCount": 756,
          "title": "Targeted Physical-World Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_F/0/1/0/all/0/1\">Feng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiu Yu</a>",
          "description": "The multi-modal salient object detection model based on RGB-D information has\nbetter robustness in the real world. However, it remains nontrivial to better\nadaptively balance effective multi-modal information in the feature fusion\nphase. In this letter, we propose a novel gated recoding network (GRNet) to\nevaluate the information validity of the two modes, and balance their\ninfluence. Our framework is divided into three phases: perception phase,\nrecoding mixing phase and feature integration phase. First, A perception\nencoder is adopted to extract multi-level single-modal features, which lays the\nfoundation for multi-modal semantic comparative analysis. Then, a\nmodal-adaptive gate unit (MGU) is proposed to suppress the invalid information\nand transfer the effective modal features to the recoding mixer and the hybrid\nbranch decoder. The recoding mixer is responsible for recoding and mixing the\nbalanced multi-modal information. Finally, the hybrid branch decoder completes\nthe multi-level feature integration under the guidance of an optional edge\nguidance stream (OEGS). Experiments and analysis on eight popular benchmarks\nverify that our framework performs favorably against 9 state-of-art methods.",
          "link": "http://arxiv.org/abs/2108.06281",
          "publishedOn": "2021-08-16T00:47:33.174Z",
          "wordCount": 609,
          "title": "Modal-Adaptive Gated Recoding Network for RGB-D Salient Object Detection. (arXiv:2108.06281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pissas_T/0/1/0/all/0/1\">Theodoros Pissas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravasio_C/0/1/0/all/0/1\">Claudio Ravasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1\">Lyndon Da Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeles_C/0/1/0/all/0/1\">Christos Bergeles</a>",
          "description": "Our work proposes neural network design choices that set the state-of-the-art\non a challenging public benchmark on cataract surgery, CaDIS. Our methodology\nachieves strong performance across three semantic segmentation tasks with\nincreasingly granular surgical tool class sets by effectively handling class\nimbalance, an inherent challenge in any surgical video. We consider and\nevaluate two conceptually simple data oversampling methods as well as different\nloss functions. We show significant performance gains across network\narchitectures and tasks especially on the rarest tool classes, thereby\npresenting an approach for achieving high performance when imbalanced granular\ndatasets are considered. Our code and trained models are available at\nhttps://github.com/RViMLab/MICCAI2021_Cataract_semantic_segmentation and\nqualitative results on unseen surgical video can be found at\nhttps://youtu.be/twVIPUj1WZM.",
          "link": "http://arxiv.org/abs/2108.06119",
          "publishedOn": "2021-08-16T00:47:33.168Z",
          "wordCount": 561,
          "title": "Effective semantic segmentation in Cataract Surgery: What matters most?. (arXiv:2108.06119v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1\">Berkan Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>",
          "description": "Image caption generation is one of the most challenging problems at the\nintersection of visual recognition and natural language modeling domains. In\nthis work, we propose and study a practically important variant of this problem\nwhere test images may contain visual objects with no corresponding visual or\ntextual training examples. For this problem, we propose a detection-driven\napproach based on a generalized zero-shot detection model and a template-based\nsentence generation model. In order to improve the detection component, we\njointly define a class-to-class similarity based class representation and a\npractical score calibration mechanism. We also propose a novel evaluation\nmetric that provides complimentary insights to the captioning outputs, by\nseparately handling the visual and non-visual components of the captions. Our\nexperiments show that the proposed zero-shot detection model obtains\nstate-of-the-art performance on the MS-COCO dataset and the zero-shot\ncaptioning approach yields promising results.",
          "link": "http://arxiv.org/abs/2108.06165",
          "publishedOn": "2021-08-16T00:47:33.161Z",
          "wordCount": 574,
          "title": "Detection and Captioning with Unseen Object Classes. (arXiv:2108.06165v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Renshu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuozhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>",
          "description": "Vehicle tracking is an essential task in the multi-object tracking (MOT)\nfield. A distinct characteristic in vehicle tracking is that the trajectories\nof vehicles are fairly smooth in both the world coordinate and the image\ncoordinate. Hence, models that capture motion consistencies are of high\nnecessity. However, tracking with the standalone motion-based trackers is quite\nchallenging because targets could get lost easily due to limited information,\ndetection error and occlusion. Leveraging appearance information to assist\nobject re-identification could resolve this challenge to some extent. However,\ndoing so requires extra computation while appearance information is sensitive\nto occlusion as well. In this paper, we try to explore the significance of\nmotion patterns for vehicle tracking without appearance information. We propose\na novel approach that tackles the association issue for long-term tracking with\nthe exclusive fully-exploited motion information. We address the tracklet\nembedding issue with the proposed reconstruct-to-embed strategy based on deep\ngraph convolutional neural networks (GCN). Comprehensive experiments on the\nKITTI-car tracking dataset and UA-Detrac dataset show that the proposed method,\nthough without appearance information, could achieve competitive performance\nwith the state-of-the-art (SOTA) trackers. The source code will be available at\nhttps://github.com/GaoangW/LGMTracker.",
          "link": "http://arxiv.org/abs/2108.06029",
          "publishedOn": "2021-08-16T00:47:33.124Z",
          "wordCount": 650,
          "title": "Track without Appearance: Learn Box and Tracklet Embedding with Local and Global Motion Patterns for Vehicle Tracking. (arXiv:2108.06029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Keke Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1\">Dingruibo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Weilong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianpeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yawen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhaoquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhihong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "Overconfident predictions on out-of-distribution (OOD) samples is a thorny\nissue for deep neural networks. The key to resolve the OOD overconfidence issue\ninherently is to build a subset of OOD samples and then suppress predictions on\nthem. This paper proposes the Chamfer OOD examples (CODEs), whose distribution\nis close to that of in-distribution samples, and thus could be utilized to\nalleviate the OOD overconfidence issue effectively by suppressing predictions\non them. To obtain CODEs, we first generate seed OOD examples via\nslicing&splicing operations on in-distribution samples from different\ncategories, and then feed them to the Chamfer generative adversarial network\nfor distribution transformation, without accessing to any extra data. Training\nwith suppressing predictions on CODEs is validated to alleviate the OOD\noverconfidence issue largely without hurting classification accuracy, and\noutperform the state-of-the-art methods. Besides, we demonstrate CODEs are\nuseful for improving OOD detection and classification.",
          "link": "http://arxiv.org/abs/2108.06024",
          "publishedOn": "2021-08-16T00:47:33.101Z",
          "wordCount": 588,
          "title": "CODEs: Chamfer Out-of-Distribution Examples against Overconfidence Issue. (arXiv:2108.06024v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt\nfrom a labeled source domain to an unlabeled target domain. Previous work is\nmainly built upon convolutional neural networks (CNNs) to learn\ndomain-invariant representations. With the recent exponential increase in\napplying Vision Transformer (ViT) to vision tasks, the capability of ViT in\nadapting cross-domain knowledge, however, remains unexplored in the literature.\nTo fill this gap, this paper first comprehensively investigates the\ntransferability of ViT on a variety of domain adaptation tasks. Surprisingly,\nViT demonstrates superior transferability over its CNNs-based counterparts with\na large margin, while the performance can be further improved by incorporating\nadversarial adaptation. Notwithstanding, directly using CNNs-based adaptation\nstrategies fails to take the advantage of ViT's intrinsic merits (e.g.,\nattention mechanism and sequential image representation) which play an\nimportant role in knowledge transfer. To remedy this, we propose an unified\nframework, namely Transferable Vision Transformer (TVT), to fully exploit the\ntransferability of ViT for domain adaptation. Specifically, we delicately\ndevise a novel and effective unit, which we term Transferability Adaption\nModule (TAM). By injecting learned transferabilities into attention blocks, TAM\ncompels ViT focus on both transferable and discriminative features. Besides, we\nleverage discriminative clustering to enhance feature diversity and separation\nwhich are undermined during adversarial domain alignment. To verify its\nversatility, we perform extensive studies of TVT on four benchmarks and the\nexperimental results demonstrate that TVT attains significant improvements\ncompared to existing state-of-the-art UDA methods.",
          "link": "http://arxiv.org/abs/2108.05988",
          "publishedOn": "2021-08-16T00:47:33.091Z",
          "wordCount": 677,
          "title": "TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation. (arXiv:2108.05988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuqian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chunlei Song</a>",
          "description": "Image matting and image harmonization are two important tasks in image\ncomposition. Image matting, aiming to achieve foreground boundary details, and\nimage harmonization, aiming to make the background compatible with the\nforeground, are both promising yet challenging tasks. Previous works consider\noptimizing these two tasks separately, which may lead to a sub-optimal\nsolution. We propose to optimize matting and harmonization simultaneously to\nget better performance on both the two tasks and achieve more natural results.\nWe propose a new Generative Adversarial (GAN) framework which optimizing the\nmatting network and the harmonization network based on a self-attention\ndiscriminator. The discriminator is required to distinguish the natural images\nfrom different types of fake synthesis images. Extensive experiments on our\nconstructed dataset demonstrate the effectiveness of our proposed method. Our\ndataset and dataset generating pipeline can be found in\n\\url{https://git.io/HaMaGAN}",
          "link": "http://arxiv.org/abs/2108.06087",
          "publishedOn": "2021-08-16T00:47:33.081Z",
          "wordCount": 588,
          "title": "A Generative Adversarial Framework for Optimizing Image Matting and Harmonization Simultaneously. (arXiv:2108.06087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Garcia_C/0/1/0/all/0/1\">Carlos Gonzalez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_J/0/1/0/all/0/1\">Juan Carlos Ruiz-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Tapiador_S/0/1/0/all/0/1\">Sergio Romero-Tapiador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rengifo_S/0/1/0/all/0/1\">Santiago Rengifo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caruana_M/0/1/0/all/0/1\">Miguel Caruana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiajia Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yecheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1\">Javier Galbally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Moises Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_M/0/1/0/all/0/1\">Miguel Angel Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Barrero_M/0/1/0/all/0/1\">Marta Gomez-Barrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodashinsky_I/0/1/0/all/0/1\">Ilya Hodashinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarin_K/0/1/0/all/0/1\">Konstantin Sarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slezkin_A/0/1/0/all/0/1\">Artem Slezkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardamova_M/0/1/0/all/0/1\">Marina Bardamova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svetlakov_M/0/1/0/all/0/1\">Mikhail Svetlakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_M/0/1/0/all/0/1\">Mohammad Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szucs_C/0/1/0/all/0/1\">Cintia Lia Szucs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovari_B/0/1/0/all/0/1\">Bence Kovari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulsmeyer_F/0/1/0/all/0/1\">Falk Pulsmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1\">Mohamad Wehbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1\">Dario Zanca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1\">Sumaiya Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sarthak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabin_S/0/1/0/all/0/1\">Suraiya Jabin</a>",
          "description": "This article presents SVC-onGoing, an on-going competition for on-line\nsignature verification where researchers can easily benchmark their systems\nagainst the state of the art in an open common platform using large-scale\npublic databases, such as DeepSignDB and SVC2021_EvalDB, and standard\nexperimental protocols. SVC-onGoing is based on the ICDAR 2021 Competition on\nOn-Line Signature Verification (SVC 2021), which has been extended to allow\nparticipants anytime. The goal of SVC-onGoing is to evaluate the limits of\non-line signature verification systems on popular scenarios (office/mobile) and\nwriting inputs (stylus/finger) through large-scale public databases. Three\ndifferent tasks are considered in the competition, simulating realistic\nscenarios as both random and skilled forgeries are simultaneously considered on\neach task. The results obtained in SVC-onGoing prove the high potential of deep\nlearning methods in comparison with traditional methods. In particular, the\nbest signature verification system has obtained Equal Error Rate (EER) values\nof 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3). Future studies in the\nfield should be oriented to improve the performance of signature verification\nsystems on the challenging mobile scenarios of SVC-onGoing in which several\nmobile devices and the finger are used during the signature acquisition.",
          "link": "http://arxiv.org/abs/2108.06090",
          "publishedOn": "2021-08-16T00:47:33.068Z",
          "wordCount": 683,
          "title": "SVC-onGoing: Signature Verification Competition. (arXiv:2108.06090v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ronge_R/0/1/0/all/0/1\">Raphael Ronge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nho_K/0/1/0/all/0/1\">Kwangsik Nho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>",
          "description": "The current state-of-the-art deep neural networks (DNNs) for Alzheimer's\nDisease diagnosis use different biomarker combinations to classify patients,\nbut do not allow extracting knowledge about the interactions of biomarkers.\nHowever, to improve our understanding of the disease, it is paramount to\nextract such knowledge from the learned model. In this paper, we propose a Deep\nFactorization Machine model that combines the ability of DNNs to learn complex\nrelationships and the ease of interpretability of a linear model. The proposed\nmodel has three parts: (i) an embedding layer to deal with sparse categorical\ndata, (ii) a Factorization Machine to efficiently learn pairwise interactions,\nand (iii) a DNN to implicitly model higher order interactions. In our\nexperiments on data from the Alzheimer's Disease Neuroimaging Initiative, we\ndemonstrate that our proposed model classifies cognitive normal, mild cognitive\nimpaired, and demented patients more accurately than competing models. In\naddition, we show that valuable knowledge about the interactions among\nbiomarkers can be obtained.",
          "link": "http://arxiv.org/abs/2108.05916",
          "publishedOn": "2021-08-16T00:47:33.031Z",
          "wordCount": 608,
          "title": "Alzheimer's Disease Diagnosis via Deep Factorization Machine Models. (arXiv:2108.05916v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">D.Y. Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">X.J. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">H. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">J. Kittler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">T.Y. Xu</a>",
          "description": "In this paper, we propose a photorealistic style transfer network to\nemphasize the natural effect of photorealistic image stylization. In general,\ndistortion of the image content and lacking of details are two typical issues\nin the style transfer field. To this end, we design a novel framework employing\nthe U-Net structure to maintain the rich spatial clues, with a multi-layer\nfeature aggregation (MFA) method to simultaneously provide the details obtained\nby the shallow layers in the stylization processing. In particular, an encoder\nbased on the dense block and a decoder form a symmetrical structure of U-Net\nare jointly staked to realize an effective feature extraction and image\nreconstruction. Besides, a transfer module based on MFA and \"adaptive instance\nnormalization\" (AdaIN) is inserted in the skip connection positions to achieve\nthe stylization. Accordingly, the stylized image possesses the texture of a\nreal photo and preserves rich content details without introducing any mask or\npost-processing steps. The experimental results on public datasets demonstrate\nthat our method achieves a more faithful structural similarity with a lower\nstyle loss, reflecting the effectiveness and merit of our approach.",
          "link": "http://arxiv.org/abs/2108.06113",
          "publishedOn": "2021-08-16T00:47:33.014Z",
          "wordCount": 630,
          "title": "UMFA: A photorealistic style transfer method based on U-Net and multi-layer feature aggregation. (arXiv:2108.06113v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dongki Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaehoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yonghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Deokhwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghwan Lee</a>",
          "description": "We present a novel approach for estimating depth from a monocular camera as\nit moves through complex and crowded indoor environments, e.g., a department\nstore or a metro station. Our approach predicts absolute scale depth maps over\nthe entire scene consisting of a static background and multiple moving people,\nby training on dynamic scenes. Since it is difficult to collect dense depth\nmaps from crowded indoor environments, we design our training framework without\nrequiring depths produced from depth sensing devices. Our network leverages RGB\nimages and sparse depth maps generated from traditional 3D reconstruction\nmethods to estimate dense depth maps. We use two constraints to handle depth\nfor non-rigidly moving people without tracking their motion explicitly. We\ndemonstrate that our approach offers consistent improvements over recent depth\nestimation methods on the NAVERLABS dataset, which includes complex and crowded\nscenes.",
          "link": "http://arxiv.org/abs/2108.05615",
          "publishedOn": "2021-08-13T01:56:56.743Z",
          "wordCount": 583,
          "title": "DnD: Dense Depth Estimation in Crowded Dynamic Indoor Scenes. (arXiv:2108.05615v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Ling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denoeux_T/0/1/0/all/0/1\">Thierry Denoeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonnelet_D/0/1/0/all/0/1\">David Tonnelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decazes_P/0/1/0/all/0/1\">Pierre Decazes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>",
          "description": "Lymphoma detection and segmentation from whole-body Positron Emission\nTomography/Computed Tomography (PET/CT) volumes are crucial for surgical\nindication and radiotherapy. Designing automatic segmentation methods capable\nof effectively exploiting the information from PET and CT as well as resolving\ntheir uncertainty remain a challenge. In this paper, we propose an lymphoma\nsegmentation model using an UNet with an evidential PET/CT fusion layer.\nSingle-modality volumes are trained separately to get initial segmentation maps\nand an evidential fusion layer is proposed to fuse the two pieces of evidence\nusing Dempster-Shafer theory (DST). Moreover, a multi-task loss function is\nproposed: in addition to the use of the Dice loss for PET and CT segmentation,\na loss function based on the concordance between the two segmentation is added\nto constrain the final segmentation. We evaluate our proposal on a database of\npolycentric PET/CT volumes of patients treated for lymphoma, delineated by the\nexperts. Our method get accurate segmentation results with Dice score of 0.726,\nwithout any user interaction. Quantitative results show that our method is\nsuperior to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.05422",
          "publishedOn": "2021-08-13T01:56:56.720Z",
          "wordCount": 619,
          "title": "Deep PET/CT fusion with Dempster-Shafer theory for lymphoma segmentation. (arXiv:2108.05422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and show excellent\nresults, in particular for rare answers. Furthermore, we demonstrate our method\nto significantly outperform the state of the art on MSRVTT-QA, MSVD-QA,\nActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce\niVQA, a new VideoQA dataset with reduced language biases and high-quality\nredundant manual annotations. Our code, datasets and trained models are\navailable at https://antoyang.github.io/just-ask.html.",
          "link": "http://arxiv.org/abs/2012.00451",
          "publishedOn": "2021-08-13T01:56:56.504Z",
          "wordCount": 684,
          "title": "Just Ask: Learning to Answer Questions from Millions of Narrated Videos. (arXiv:2012.00451v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Cat P. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mohammadreza Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravier_R/0/1/0/all/0/1\">Robert Ravier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>",
          "description": "In this paper, we propose a neural architecture search framework based on a\nsimilarity measure between the baseline tasks and the incoming target task. We\nfirst define the notion of task similarity based on the log-determinant of the\nFisher Information Matrices. Next, we compute the task similarity from each of\nthe baseline tasks to the incoming target task. By utilizing the relation\nbetween a target and a set of learned baseline tasks, the search space of\narchitectures for the incoming target task can be significantly reduced, making\nthe discovery of the best candidates in the set of possible architectures\ntractable and efficient, in terms of GPU days. This method eliminates the\nrequirement for training the networks from scratch for the incoming target task\nas well as introducing the bias in the initialization of the search space from\nthe human domain. Experimental results with 8 classification tasks in MNIST and\nCIFAR-10 datasets illustrate the efficacy of our proposed approach and its\ncompetitiveness with other state-of-art methods in terms of the classification\nperformance, the number of parameters, and the search time.",
          "link": "http://arxiv.org/abs/2103.00241",
          "publishedOn": "2021-08-13T01:56:56.498Z",
          "wordCount": 661,
          "title": "Neural Architecture Search From Task Similarity Measure. (arXiv:2103.00241v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chang-Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1\">Feng Mao</a>",
          "description": "Semantic segmentation models trained on public datasets have achieved great\nsuccess in recent years. However, these models didn't consider the\npersonalization issue of segmentation though it is important in practice. In\nthis paper, we address the problem of personalized image segmentation. The\nobjective is to generate more accurate segmentation results on unlabeled\npersonalized images by investigating the data's personalized traits. To open up\nfuture research in this area, we collect a large dataset containing various\nusers' personalized images called PIS (Personalized Image Semantic\nSegmentation). We also survey some recent researches related to this problem\nand report their performance on our dataset. Furthermore, by observing the\ncorrelation among a user's personalized images, we propose a baseline method\nthat incorporates the inter-image context when segmenting certain images.\nExtensive experiments show that our method outperforms the existing methods on\nthe proposed dataset. The code and the PIS dataset will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2107.13978",
          "publishedOn": "2021-08-13T01:56:56.493Z",
          "wordCount": 600,
          "title": "Personalized Image Semantic Segmentation. (arXiv:2107.13978v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zike Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuxin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xuesong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Ping Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongbin Zha</a>",
          "description": "Recent advances have enabled a single neural network to serve as an implicit\nscene representation, establishing the mapping function between spatial\ncoordinates and scene properties. In this paper, we make a further step towards\ncontinual learning of the implicit scene representation directly from\nsequential observations, namely Continual Neural Mapping. The proposed problem\nsetting bridges the gap between batch-trained implicit neural representations\nand commonly used streaming data in robotics and vision communities. We\nintroduce an experience replay approach to tackle an exemplary task of\ncontinual neural mapping: approximating a continuous signed distance function\n(SDF) from sequential depth images as a scene geometry representation. We show\nfor the first time that a single network can represent scene geometry over time\ncontinually without catastrophic forgetting, while achieving promising\ntrade-offs between accuracy and efficiency.",
          "link": "http://arxiv.org/abs/2108.05851",
          "publishedOn": "2021-08-13T01:56:56.466Z",
          "wordCount": 578,
          "title": "Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations. (arXiv:2108.05851v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xingxing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xiwen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>",
          "description": "Current state-of-the-art two-stage detectors generate oriented proposals\nthrough time-consuming schemes. This diminishes the detectors' speed, thereby\nbecoming the computational bottleneck in advanced oriented object detection\nsystems. This work proposes an effective and simple oriented object detection\nframework, termed Oriented R-CNN, which is a general two-stage oriented\ndetector with promising accuracy and efficiency. To be specific, in the first\nstage, we propose an oriented Region Proposal Network (oriented RPN) that\ndirectly generates high-quality oriented proposals in a nearly cost-free\nmanner. The second stage is oriented R-CNN head for refining oriented Regions\nof Interest (oriented RoIs) and recognizing them. Without tricks, oriented\nR-CNN with ResNet50 achieves state-of-the-art detection accuracy on two\ncommonly-used datasets for oriented object detection including DOTA (75.87%\nmAP) and HRSC2016 (96.50% mAP), while having a speed of 15.1 FPS with the image\nsize of 1024$\\times$1024 on a single RTX 2080Ti. We hope our work could inspire\nrethinking the design of oriented detectors and serve as a baseline for\noriented object detection. Code is available at\nhttps://github.com/jbwang1997/OBBDetection.",
          "link": "http://arxiv.org/abs/2108.05699",
          "publishedOn": "2021-08-13T01:56:56.429Z",
          "wordCount": 604,
          "title": "Oriented R-CNN for Object Detection. (arXiv:2108.05699v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1912.00215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Resnick_C/0/1/0/all/0/1\">Cinjon Resnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zeping Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Self-supervised research improved greatly over the past half decade, with\nmuch of the growth being driven by objectives that are hard to quantitatively\ncompare. These techniques include colorization, cyclical consistency, and\nnoise-contrastive estimation from image patches. Consequently, the field has\nsettled on a handful of measurements that depend on linear probes to adjudicate\nwhich approaches are the best. Our first contribution is to show that this test\nis insufficient and that models which perform poorly (strongly) on linear\nclassification can perform strongly (weakly) on more involved tasks like\ntemporal activity localization. Our second contribution is to analyze the\ncapabilities of five different representations. And our third contribution is a\nmuch needed new dataset for temporal activity localization.",
          "link": "http://arxiv.org/abs/1912.00215",
          "publishedOn": "2021-08-13T01:56:56.407Z",
          "wordCount": 597,
          "title": "Probing the State of the Art: A Critical Look at Visual Representation Evaluation. (arXiv:1912.00215v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yunzhong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Multiview detection incorporates multiple camera views to deal with\nocclusions, and its central problem is multiview aggregation. Given feature map\nprojections from multiple views onto a common ground plane, the\nstate-of-the-art method addresses this problem via convolution, which applies\nthe same calculation regardless of object locations. However, such\ntranslation-invariant behaviors might not be the best choice, as object\nfeatures undergo various projection distortions according to their positions\nand cameras. In this paper, we propose a novel multiview detector, MVDeTr, that\nadopts a newly introduced shadow transformer to aggregate multiview\ninformation. Unlike convolutions, shadow transformer attends differently at\ndifferent positions and cameras to deal with various shadow-like distortions.\nWe propose an effective training scheme that includes a new view-coherent data\naugmentation method, which applies random augmentations while maintaining\nmultiview consistency. On two multiview detection benchmarks, we report new\nstate-of-the-art accuracy with the proposed system. Code is available at\nhttps://github.com/hou-yz/MVDeTr.",
          "link": "http://arxiv.org/abs/2108.05888",
          "publishedOn": "2021-08-13T01:56:56.392Z",
          "wordCount": 588,
          "title": "Multiview Detection with Shadow Transformer (and View-Coherent Data Augmentation). (arXiv:2108.05888v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xubo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheyang Li</a>",
          "description": "Accuracy predictor is trained to predict the validation accuracy of an\nnetwork from its architecture encoding. It can effectively assist in designing\nnetworks and improving Neural Architecture Search(NAS) efficiency. However, a\nhigh-performance predictor depends on adequate trainning samples, which\nrequires unaffordable computation overhead. To alleviate this problem, we\npropose a novel framework to train an accuracy predictor under few training\nsamples. The framework consists ofdata augmentation methods and an ensemble\nlearning algorithm. The data augmentation methods calibrate weak labels and\ninject noise to feature space. The ensemble learning algorithm, termed cascade\nbagging, trains two-level models by sampling data and features. In the end, the\nadvantages of above methods are proved in the Performance Prediciton Track of\nCVPR2021 1st Lightweight NAS Challenge. Our code is made public at:\nhttps://github.com/dlongry/Solutionto-CVPR2021-NAS-Track2.",
          "link": "http://arxiv.org/abs/2108.05613",
          "publishedOn": "2021-08-13T01:56:56.379Z",
          "wordCount": 572,
          "title": "Cascade Bagging for Accuracy Prediction with Few Training Samples. (arXiv:2108.05613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Liqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiwen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongfang Liu</a>",
          "description": "Video objection detection is a challenging task because isolated video frames\nmay encounter appearance deterioration, which introduces great confusion for\ndetection. One of the popular solutions is to exploit the temporal information\nand enhance per-frame representation through aggregating features from\nneighboring frames. Despite achieving improvements in detection, existing\nmethods focus on the selection of higher-level video frames for aggregation\nrather than modeling lower-level temporal relations to increase the feature\nrepresentation. To address this limitation, we propose a novel solution named\nTF-Blender,which includes three modules: 1) Temporal relation mod-els the\nrelations between the current frame and its neighboring frames to preserve\nspatial information. 2). Feature adjustment enriches the representation of\nevery neigh-boring feature map; 3) Feature blender combines outputs from the\nfirst two modules and produces stronger features for the later detection tasks.\nFor its simplicity, TF-Blender can be effortlessly plugged into any detection\nnetwork to improve detection behavior. Extensive evaluations on ImageNet VID\nand YouTube-VIS benchmarks indicate the performance guarantees of using\nTF-Blender on recent state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.05821",
          "publishedOn": "2021-08-13T01:56:56.347Z",
          "wordCount": 603,
          "title": "TF-Blender: Temporal Feature Blender for Video Object Detection. (arXiv:2108.05821v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Altaf_F/0/1/0/all/0/1\">Fouzia Altaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Syed M.S. Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>",
          "description": "Deep learning is gaining instant popularity in computer aided diagnosis of\nCOVID-19. Due to the high sensitivity of Computed Tomography (CT) to this\ndisease, CT-based COVID-19 detection with visual models is currently at the\nforefront of medical imaging research. Outcomes published in this direction are\nfrequently claiming highly accurate detection under deep transfer learning.\nThis is leading medical technologists to believe that deep transfer learning is\nthe mainstream solution for the problem. However, our critical analysis of the\nliterature reveals an alarming performance disparity between different\npublished results. Hence, we conduct a systematic thorough investigation to\nanalyze the effectiveness of deep transfer learning for COVID-19 detection with\nCT images. Exploring 14 state-of-the-art visual models with over 200 model\ntraining sessions, we conclusively establish that the published literature is\nfrequently overestimating transfer learning performance for the problem, even\nin the prestigious scientific sources. The roots of overestimation trace back\nto inappropriate data curation. We also provide case studies that consider more\nrealistic scenarios, and establish transparent baselines for the problem. We\nhope that our reproducible investigation will help in curbing hype-driven\nclaims for the critical problem of COVID-19 diagnosis, and pave the way for a\nmore transparent performance evaluation of techniques for CT-based COVID-19\ndetection.",
          "link": "http://arxiv.org/abs/2108.05649",
          "publishedOn": "2021-08-13T01:56:56.332Z",
          "wordCount": 714,
          "title": "Resetting the baseline: CT-based COVID-19 diagnosis with Deep Transfer Learning is not as accurate as widely thought. (arXiv:2108.05649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sifan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yubo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>",
          "description": "3D single object tracking is a key issue for autonomous following robot,\nwhere the robot should robustly track and accurately localize the target for\nefficient following. In this paper, we propose a 3D tracking method called\n3D-SiamRPN Network to track a single target object by using raw 3D point cloud\ndata. The proposed network consists of two subnetworks. The first subnetwork is\nfeature embedding subnetwork which is used for point cloud feature extraction\nand fusion. In this subnetwork, we first use PointNet++ to extract features of\npoint cloud from template and search branches. Then, to fuse the information of\nfeatures in the two branches and obtain their similarity, we propose two cross\ncorrelation modules, named Pointcloud-wise and Point-wise respectively. The\nsecond subnetwork is region proposal network(RPN), which is used to get the\nfinal 3D bounding box of the target object based on the fusion feature from\ncross correlation modules. In this subnetwork, we utilize the regression and\nclassification branches of a region proposal subnetwork to obtain proposals and\nscores, thus get the final 3D bounding box of the target object. Experimental\nresults on KITTI dataset show that our method has a competitive performance in\nboth Success and Precision compared to the state-of-the-art methods, and could\nrun in real-time at 20.8 FPS. Additionally, experimental results on H3D dataset\ndemonstrate that our method also has good generalization ability and could\nachieve good tracking performance in a new scene without re-training.",
          "link": "http://arxiv.org/abs/2108.05630",
          "publishedOn": "2021-08-13T01:56:56.298Z",
          "wordCount": 697,
          "title": "3D-SiamRPN: An End-to-End Learning Method for Real-Time 3D Single Object Tracking Using Raw Point Cloud. (arXiv:2108.05630v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anctil_Robitaille_B/0/1/0/all/0/1\">Benoit Anctil-Robitaille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theberge_A/0/1/0/all/0/1\">Antoine Th&#xe9;berge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Descoteaux_M/0/1/0/all/0/1\">Maxime Descoteaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1\">Herv&#xe9; Lombaert</a>",
          "description": "The physical and clinical constraints surrounding diffusion-weighted imaging\n(DWI) often limit the spatial resolution of the produced images to voxels up to\n8 times larger than those of T1w images. Thus, the detailed information\ncontained in T1w imagescould help in the synthesis of diffusion images in\nhigher resolution. However, the non-Euclidean nature of diffusion imaging\nhinders current deep generative models from synthesizing physically plausible\nimages. In this work, we propose the first Riemannian network architecture for\nthe direct generation of diffusion tensors (DT) and diffusion orientation\ndistribution functions (dODFs) from high-resolution T1w images. Our integration\nof the Log-Euclidean Metric into a learning objective guarantees, unlike\nstandard Euclidean networks, the mathematically-valid synthesis of diffusion.\nFurthermore, our approach improves the fractional anisotropy mean squared error\n(FA MSE) between the synthesized diffusion and the ground-truth by more than\n23% and the cosine similarity between principal directions by almost 5% when\ncompared to our baselines. We validate our generated diffusion by comparing the\nresulting tractograms to our expected real data. We observe similar fiber\nbundles with streamlines having less than 3% difference in length, less than 1%\ndifference in volume, and a visually close shape. While our method is able to\ngenerate high-resolution diffusion images from structural inputs in less than\n15 seconds, we acknowledge and discuss the limits of diffusion inference solely\nrelying on T1w images. Our results nonetheless suggest a relationship between\nthe high-level geometry of the brain and the overall white matter architecture.",
          "link": "http://arxiv.org/abs/2108.04135",
          "publishedOn": "2021-08-13T01:56:56.044Z",
          "wordCount": 707,
          "title": "Manifold-aware Synthesis of High-resolution Diffusion from Structural Imaging. (arXiv:2108.04135v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yixing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Transformer, first applied to the field of natural language processing, is a\ntype of deep neural network mainly based on the self-attention mechanism.\nThanks to its strong representation capabilities, researchers are looking at\nways to apply transformer to computer vision tasks. In a variety of visual\nbenchmarks, transformer-based models perform similar to or better than other\ntypes of networks such as convolutional and recurrent networks. Given its high\nperformance and less need for vision-specific inductive bias, transformer is\nreceiving more and more attention from the computer vision community. In this\npaper, we review these vision transformer models by categorizing them in\ndifferent tasks and analyzing their advantages and disadvantages. The main\ncategories we explore include the backbone network, high/mid-level vision,\nlow-level vision, and video processing. We also include efficient transformer\nmethods for pushing transformer into real device-based applications.\nFurthermore, we also take a brief look at the self-attention mechanism in\ncomputer vision, as it is the base component in transformer. Toward the end of\nthis paper, we discuss the challenges and provide several further research\ndirections for vision transformers.",
          "link": "http://arxiv.org/abs/2012.12556",
          "publishedOn": "2021-08-13T01:56:56.038Z",
          "wordCount": 680,
          "title": "A Survey on Vision Transformer. (arXiv:2012.12556v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02110",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1\">Minyi Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>",
          "description": "A number of deep learning based algorithms have been proposed to recover\nhigh-quality videos from low-quality compressed ones. Among them, some restore\nthe missing details of each frame via exploring the spatiotemporal information\nof neighboring frames. However, these methods usually suffer from a narrow\ntemporal scope, thus may miss some useful details from some frames outside the\nneighboring ones. In this paper, to boost artifact removal, on the one hand, we\npropose a Recursive Fusion (RF) module to model the temporal dependency within\na long temporal range. Specifically, RF utilizes both the current reference\nframes and the preceding hidden state to conduct better spatiotemporal\ncompensation. On the other hand, we design an efficient and effective\nDeformable Spatiotemporal Attention (DSTA) module such that the model can pay\nmore effort on restoring the artifact-rich areas like the boundary area of a\nmoving object. Extensive experiments show that our method outperforms the\nexisting ones on the MFQE 2.0 dataset in terms of both fidelity and perceptual\neffect. Code is available at https://github.com/zhaominyiz/RFDA-PyTorch.",
          "link": "http://arxiv.org/abs/2108.02110",
          "publishedOn": "2021-08-13T01:56:55.985Z",
          "wordCount": 646,
          "title": "Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction. (arXiv:2108.02110v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chaoyou Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yibo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>",
          "description": "Visible-Infrared person re-identification (VI-ReID) aims to match\ncross-modality pedestrian images, breaking through the limitation of\nsingle-modality person ReID in dark environment. In order to mitigate the\nimpact of large modality discrepancy, existing works manually design various\ntwo-stream architectures to separately learn modality-specific and\nmodality-sharable representations. Such a manual design routine, however,\nhighly depends on massive experiments and empirical practice, which is time\nconsuming and labor intensive. In this paper, we systematically study the\nmanually designed architectures, and identify that appropriately separating\nBatch Normalization (BN) layers is the key to bring a great boost towards\ncross-modality matching. Based on this observation, the essential objective is\nto find the optimal separation scheme for each BN layer. To this end, we\npropose a novel method, named Cross-Modality Neural Architecture Search\n(CM-NAS). It consists of a BN-oriented search space in which the standard\noptimization can be fulfilled subject to the cross-modality task. Equipped with\nthe searched architecture, our method outperforms state-of-the-art counterparts\nin both two benchmarks, improving the Rank-1/mAP by 6.70%/6.13% on SYSU-MM01\nand by 12.17%/11.23% on RegDB. Code is released at\nhttps://github.com/JDAI-CV/CM-NAS.",
          "link": "http://arxiv.org/abs/2101.08467",
          "publishedOn": "2021-08-13T01:56:55.973Z",
          "wordCount": 668,
          "title": "CM-NAS: Cross-Modality Neural Architecture Search for Visible-Infrared Person Re-Identification. (arXiv:2101.08467v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yunkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yuyang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guozheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peiyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_W/0/1/0/all/0/1\">Wenjing Shan</a>",
          "description": "Useful information (UI) is an elusive concept in neural networks. A\nquantitative measurement of UI is absent, despite the variations of UI can be\nrecognized by prior knowledge. The communication bandwidth of feature maps\ndecreases after downscaling operations, but UI flows smoothly after training\ndue to lower Nyquist frequency. Inspired by the low-Nyqusit-frequency nature of\nUI, we propose the use of spectral roll-off points (SROPs) to estimate UI on\nvariations. The computation of an SROP is extended from a 1-D signal to a 2-D\nimage by the required rotation invariance in image classification tasks. SROP\nstatistics across feature maps are implemented as layer-wise useful information\nestimates. We design sanity checks to explore SROP variations when UI\nvariations are produced by variations in model input, model architecture and\ntraining stages. The variations of SROP is synchronizes with UI variations in\nvarious randomized and sufficiently trained model structures. Therefore, SROP\nvariations is an accurate and convenient sign of UI variations, which promotes\nthe explainability of data representations with respect to frequency-domain\nknowledge.",
          "link": "http://arxiv.org/abs/2102.00369",
          "publishedOn": "2021-08-13T01:56:55.937Z",
          "wordCount": 683,
          "title": "Spectral Roll-off Points Variations: Exploring Useful Information in Feature Maps by Its Variations. (arXiv:2102.00369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salvador_T/0/1/0/all/0/1\">Tiago Salvador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cairns_S/0/1/0/all/0/1\">Stephanie Cairns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_N/0/1/0/all/0/1\">Noah Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1\">Adam Oberman</a>",
          "description": "Face recognition models suffer from bias: for example, the probability of a\nfalse positive (incorrect face match) strongly depends on sensitive attributes\nlike ethnicity. As a result, these models may disproportionately and negatively\nimpact minority groups when used in law enforcement. In this work, we introduce\nthe Bias Mitigation Calibration (BMC) method, which (i) increases model\naccuracy (improving the state-of-the-art), (ii) produces fairly-calibrated\nprobabilities, (iii) significantly reduces the gap in the false positive rates,\nand (iv) does not require knowledge of the sensitive attribute.",
          "link": "http://arxiv.org/abs/2106.03761",
          "publishedOn": "2021-08-13T01:56:55.931Z",
          "wordCount": 562,
          "title": "Bias Mitigation of Face Recognition Models Through Calibration. (arXiv:2106.03761v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Random field and random cluster theory are used to describe certain\nmathematical results concerning the probability distribution of image pixel\nintensities characterized as generic $2D$ integer arrays. The size of the\nsmallest bounded region within an image is estimated for segmenting an image,\nfrom which, the equilibrium distribution of intensities can be recovered. From\nthe estimated bounded regions, properties of the sub-optimal and equilibrium\ndistributions of intensities are derived, which leads to an image compression\nmethodology whereby only slightly more than half of all pixels are required for\na worst-case reconstruction of the original image. A custom deep belief network\nand heuristic allows for the unsupervised segmentation, detection and\nlocalization of objects in an image. An example illustrates the mathematical\nresults.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-08-13T01:56:55.925Z",
          "wordCount": 679,
          "title": "Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v11 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yongjian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huiying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youfu Li</a>",
          "description": "Event cameras report sparse intensity changes and hold noticeable advantages\nof low power consumption, high dynamic range, and high response speed for\nvisual perception and understanding on portable devices. Event-based learning\nmethods have recently achieved massive success on object recognition by\nintegrating events into dense frame-based representations to apply traditional\n2D learning algorithms. However, these approaches introduce much redundant\ninformation during the sparse-to-dense conversion and necessitate models with\nheavy-weight and large capacities, limiting the potential of event cameras on\nreal-life applications. To address the core problem of balancing accuracy and\nmodel complexity for event-based classification models, we (1) construct graph\nrepresentations for event data to utilize their sparsity nature better and\ndesign a lightweight end-to-end graph neural network (EV-VGCNN) for\nclassification; (2) use voxel-wise vertices rather than traditional point-wise\nmethods to incorporate the information from more points; (3) introduce a\nmulti-scale feature relational layer (MFRL) to extract semantic and motion cues\nfrom each vertex adaptively concerning its distances to neighbors.\nComprehensive experiments show that our approach advances state-of-the-art\nclassification accuracy while achieving nearly 20 times parameter reduction\n(merely 0.84M parameters).",
          "link": "http://arxiv.org/abs/2106.00216",
          "publishedOn": "2021-08-13T01:56:55.909Z",
          "wordCount": 650,
          "title": "EV-VGCNN: A Voxel Graph CNN for Event-based Object Classification. (arXiv:2106.00216v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sanghun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwak_D/0/1/0/all/0/1\">Daehoon Gwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungha Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Identifying unexpected objects on roads in semantic segmentation (e.g.,\nidentifying dogs on roads) is crucial in safety-critical applications. Existing\napproaches use images of unexpected objects from external datasets or require\nadditional training (e.g., retraining segmentation networks or training an\nextra network), which necessitate a non-trivial amount of labor intensity or\nlengthy inference time. One possible alternative is to use prediction scores of\na pre-trained network such as the max logits (i.e., maximum values among\nclasses before the final softmax layer) for detecting such objects. However,\nthe distribution of max logits of each predicted class is significantly\ndifferent from each other, which degrades the performance of identifying\nunexpected objects in urban-scene segmentation. To address this issue, we\npropose a simple yet effective approach that standardizes the max logits in\norder to align the different distributions and reflect the relative meanings of\nmax logits within each predicted class. Moreover, we consider the local regions\nfrom two different perspectives based on the intuition that neighboring pixels\nshare similar semantic information. In contrast to previous approaches, our\nmethod does not utilize any external datasets or require additional training,\nwhich makes our method widely applicable to existing pre-trained segmentation\nmodels. Such a straightforward approach achieves a new state-of-the-art\nperformance on the publicly available Fishyscapes Lost & Found leaderboard with\na large margin.",
          "link": "http://arxiv.org/abs/2107.11264",
          "publishedOn": "2021-08-13T01:56:55.889Z",
          "wordCount": 704,
          "title": "Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation. (arXiv:2107.11264v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "In this paper, we present a neat yet effective transformer-based framework\nfor visual grounding, namely TransVG, to address the task of grounding a\nlanguage query to the corresponding region onto an image. The state-of-the-art\nmethods, including two-stage or one-stage ones, rely on a complex module with\nmanually-designed mechanisms to perform the query reasoning and multi-modal\nfusion. However, the involvement of certain mechanisms in fusion module design,\nsuch as query decomposition and image scene graph, makes the models easily\noverfit to datasets with specific scenarios, and limits the plenitudinous\ninteraction between the visual-linguistic context. To avoid this caveat, we\npropose to establish the multi-modal correspondence by leveraging transformers,\nand empirically show that the complex fusion modules (\\eg, modular attention\nnetwork, dynamic graph, and multi-modal tree) can be replaced by a simple stack\nof transformer encoder layers with higher performance. Moreover, we\nre-formulate the visual grounding as a direct coordinates regression problem\nand avoid making predictions out of a set of candidates (\\emph{i.e.}, region\nproposals or anchor boxes). Extensive experiments are conducted on five widely\nused datasets, and a series of state-of-the-art records are set by our TransVG.\nWe build the benchmark of transformer-based visual grounding framework and make\nthe code available at \\url{https://github.com/djiajunustc/TransVG}.",
          "link": "http://arxiv.org/abs/2104.08541",
          "publishedOn": "2021-08-13T01:56:55.865Z",
          "wordCount": 674,
          "title": "TransVG: End-to-End Visual Grounding with Transformers. (arXiv:2104.08541v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zirui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting-Hsiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaixiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Human-designed data augmentation strategies have been replaced by\nautomatically learned augmentation policy in the past two years. Specifically,\nrecent work has empirically shown that the superior performance of the\nautomated data augmentation methods stems from increasing the diversity of\naugmented data \\cite{autoaug, randaug}. However, two factors regarding the\ndiversity of augmented data are still missing: 1) the explicit definition (and\nthus measurement) of diversity and 2) the quantifiable relationship between\ndiversity and its regularization effects. To bridge this gap, we propose a\ndiversity measure called Variance Diversity and theoretically show that the\nregularization effect of data augmentation is promised by Variance Diversity.\nWe validate in experiments that the relative gain from automated data\naugmentation in test accuracy is highly correlated to Variance Diversity. An\nunsupervised sampling-based framework, \\textbf{DivAug}, is designed to directly\nmaximize Variance Diversity and hence strengthen the regularization effect.\nWithout requiring a separate search process, the performance gain from DivAug\nis comparable with the state-of-the-art method with better efficiency.\nMoreover, under the semi-supervised setting, our framework can further improve\nthe performance of semi-supervised learning algorithms compared to RandAugment,\nmaking it highly applicable to real-world problems, where labeled data is\nscarce. The code is available at\n\\texttt{\\url{https://github.com/warai-0toko/DivAug}}.",
          "link": "http://arxiv.org/abs/2103.14545",
          "publishedOn": "2021-08-13T01:56:55.843Z",
          "wordCount": 669,
          "title": "DivAug: Plug-in Automated Data Augmentation with Explicit Diversity Maximization. (arXiv:2103.14545v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doran_C/0/1/0/all/0/1\">Chris Doran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1\">Jean-Jacques Slotine</a>",
          "description": "We study the problem of aligning two sets of 3D geometric primitives given\nknown correspondences. Our first contribution is to show that this primitive\nalignment framework unifies five perception problems including point cloud\nregistration, primitive (mesh) registration, category-level 3D registration,\nabsolution pose estimation (APE), and category-level APE. Our second\ncontribution is to propose DynAMical Pose estimation (DAMP), the first general\nand practical algorithm to solve primitive alignment problem by simulating\nrigid body dynamics arising from virtual springs and damping, where the springs\nspan the shortest distances between corresponding primitives. We evaluate DAMP\nin simulated and real datasets across all five problems, and demonstrate (i)\nDAMP always converges to the globally optimal solution in the first three\nproblems with 3D-3D correspondences; (ii) although DAMP sometimes converges to\nsuboptimal solutions in the last two problems with 2D-3D correspondences, using\na scheme for escaping local minima, DAMP always succeeds. Our third\ncontribution is to demystify the surprising empirical performance of DAMP and\nformally prove a global convergence result in the case of point cloud\nregistration by charactering local stability of the equilibrium points of the\nunderlying dynamical system.",
          "link": "http://arxiv.org/abs/2103.06182",
          "publishedOn": "2021-08-13T01:56:55.837Z",
          "wordCount": 671,
          "title": "Dynamical Pose Estimation. (arXiv:2103.06182v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>",
          "description": "This paper aims at addressing the problem of substantial performance\ndegradation at extremely low computational cost (e.g. 5M FLOPs on ImageNet\nclassification). We found that two factors, sparse connectivity and dynamic\nactivation function, are effective to improve the accuracy. The former avoids\nthe significant reduction of network width, while the latter mitigates the\ndetriment of reduction in network depth. Technically, we propose\nmicro-factorized convolution, which factorizes a convolution matrix into low\nrank matrices, to integrate sparse connectivity into convolution. We also\npresent a new dynamic activation function, named Dynamic Shift Max, to improve\nthe non-linearity via maxing out multiple dynamic fusions between an input\nfeature map and its circular channel shift. Building upon these two new\noperators, we arrive at a family of networks, named MicroNet, that achieves\nsignificant performance gains over the state of the art in the low FLOP regime.\nFor instance, under the constraint of 12M FLOPs, MicroNet achieves 59.4\\% top-1\naccuracy on ImageNet classification, outperforming MobileNetV3 by 9.6\\%. Source\ncode is at\n\\href{https://github.com/liyunsheng13/micronet}{https://github.com/liyunsheng13/micronet}.",
          "link": "http://arxiv.org/abs/2108.05894",
          "publishedOn": "2021-08-13T01:56:55.831Z",
          "wordCount": 633,
          "title": "MicroNet: Improving Image Recognition with Extremely Low FLOPs. (arXiv:2108.05894v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mixue Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1\">Fangrui Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>",
          "description": "Domain adaptation (DA) paves the way for label annotation and dataset bias\nissues by the knowledge transfer from a label-rich source domain to a related\nbut unlabeled target domain. A mainstream of DA methods is to align the feature\ndistributions of the two domains. However, the majority of them focus on the\nentire image features where irrelevant semantic information, e.g., the messy\nbackground, is inevitably embedded. Enforcing feature alignments in such case\nwill negatively influence the correct matching of objects and consequently lead\nto the semantically negative transfer due to the confusion of irrelevant\nsemantics. To tackle this issue, we propose Semantic Concentration for Domain\nAdaptation (SCDA), which encourages the model to concentrate on the most\nprincipal features via the pair-wise adversarial alignment of prediction\ndistributions. Specifically, we train the classifier to class-wisely maximize\nthe prediction distribution divergence of each sample pair, which enables the\nmodel to find the region with large differences among the same class of\nsamples. Meanwhile, the feature extractor attempts to minimize that\ndiscrepancy, which suppresses the features of dissimilar regions among the same\nclass of samples and accentuates the features of principal parts. As a general\nmethod, SCDA can be easily integrated into various DA methods as a regularizer\nto further boost their performance. Extensive experiments on the cross-domain\nbenchmarks show the efficacy of SCDA.",
          "link": "http://arxiv.org/abs/2108.05720",
          "publishedOn": "2021-08-13T01:56:55.825Z",
          "wordCount": 662,
          "title": "Semantic Concentration for Domain Adaptation. (arXiv:2108.05720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang-Hua Gao</a>",
          "description": "In recent years, the connections between deep residual networks and\nfirst-order Ordinary Differential Equations (ODEs) have been disclosed. In this\nwork, we further bridge the deep neural architecture design with the\nsecond-order ODEs and propose a novel reversible neural network, termed as\nm-RevNet, that is characterized by inserting momentum update to residual\nblocks. The reversible property allows us to perform backward pass without\naccess to activation values of the forward pass, greatly relieving the storage\nburden during training. Furthermore, the theoretical foundation based on\nsecond-order ODEs grants m-RevNet with stronger representational power than\nvanilla residual networks, which potentially explains its performance gains.\nFor certain learning scenarios, we analytically and empirically reveal that our\nm-RevNet succeeds while standard ResNet fails. Comprehensive experiments on\nvarious image classification and semantic segmentation benchmarks demonstrate\nthe superiority of our m-RevNet over ResNet, concerning both memory efficiency\nand recognition performance.",
          "link": "http://arxiv.org/abs/2108.05862",
          "publishedOn": "2021-08-13T01:56:55.810Z",
          "wordCount": 582,
          "title": "m-RevNet: Deep Reversible Neural Networks with Momentum. (arXiv:2108.05862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.01642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tursun_O/0/1/0/all/0/1\">Osman Tursun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>",
          "description": "Off-the-shelf convolutional neural network features achieve outstanding\nresults in many image retrieval tasks. However, their invariance to target data\nis pre-defined by the network architecture and training data. Existing image\nretrieval approaches require fine-tuning or modification of pre-trained\nnetworks to adapt to variations unique to the target data. In contrast, our\nmethod enhances the invariance of off-the-shelf features by aggregating\nfeatures extracted from images augmented at test-time, with augmentations\nguided by a policy learned through reinforcement learning. The learned policy\nassigns different magnitudes and weights to the selected transformations, which\nare selected from a list of image transformations. Policies are evaluated using\na metric learning protocol to learn the optimal policy. The model converges\nquickly and the cost of each policy iteration is minimal as we propose an\noff-line caching technique to greatly reduce the computational cost of\nextracting features from augmented images. Experimental results on large\ntrademark retrieval (METU trademark dataset) and landmark retrieval (ROxford5k\nand RParis6k scene datasets) tasks show that the learned ensemble of\ntransformations is highly effective for improving performance, and is\npractical, and transferable.",
          "link": "http://arxiv.org/abs/2002.01642",
          "publishedOn": "2021-08-13T01:56:55.802Z",
          "wordCount": 660,
          "title": "Learning Test-time Augmentation for Content-based Image Retrieval. (arXiv:2002.01642v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "How do the neural networks distinguish two images? It is of critical\nimportance to understand the matching mechanism of deep models for developing\nreliable intelligent systems for many risky visual applications such as\nsurveillance and access control. However, most existing deep metric learning\nmethods match the images by comparing feature vectors, which ignores the\nspatial structure of images and thus lacks interpretability. In this paper, we\npresent a deep interpretable metric learning (DIML) method for more transparent\nembedding learning. Unlike conventional metric learning methods based on\nfeature vector comparison, we propose a structural matching strategy that\nexplicitly aligns the spatial embeddings by computing an optimal matching flow\nbetween feature maps of the two images. Our method enables deep models to learn\nmetrics in a more human-friendly way, where the similarity of two images can be\ndecomposed to several part-wise similarities and their contributions to the\noverall similarity. Our method is model-agnostic, which can be applied to\noff-the-shelf backbone networks and metric learning methods. We evaluate our\nmethod on three major benchmarks of deep metric learning including CUB200-2011,\nCars196, and Stanford Online Products, and achieve substantial improvements\nover popular metric learning methods with better interpretability. Code is\navailable at https://github.com/wl-zhao/DIML",
          "link": "http://arxiv.org/abs/2108.05889",
          "publishedOn": "2021-08-13T01:56:55.733Z",
          "wordCount": 652,
          "title": "Towards Interpretable Deep Metric Learning with Structural Matching. (arXiv:2108.05889v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.00348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1\">Kuniaki Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A Plummer</a>",
          "description": "Many self-supervised learning (SSL) methods have been successful in learning\nsemantically meaningful visual representations by solving pretext tasks.\nHowever, prior work in SSL focuses on tasks like object recognition or\ndetection, which aim to learn object shapes and assume that the features should\nbe invariant to concepts like colors and textures. Thus, these SSL methods\nperform poorly on downstream tasks where these concepts provide critical\ninformation. In this paper, we present an SSL framework that enables us to\nlearn color and texture-aware features without requiring any labels during\ntraining. Our approach consists of three self-supervised tasks designed to\ncapture different concepts that are neglected in prior work that we can select\nfrom depending on the needs of our downstream tasks. Our tasks include learning\nto predict color histograms and discriminate shapeless local patches and\ntextures from each instance. We evaluate our approach on fashion compatibility\nusing Polyvore Outfits and In-Shop Clothing Retrieval using Deepfashion,\nimproving upon prior SSL methods by 9.5-16%, and even outperforming some\nsupervised approaches on Polyvore Outfits despite using no labels. We also show\nthat our approach can be used for transfer learning, demonstrating that we can\ntrain on one dataset while achieving high performance on a different dataset.",
          "link": "http://arxiv.org/abs/2008.00348",
          "publishedOn": "2021-08-13T01:56:55.711Z",
          "wordCount": 679,
          "title": "Self-supervised Visual Attribute Learning for Fashion Compatibility. (arXiv:2008.00348v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiss_T/0/1/0/all/0/1\">Tal Reiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Niv Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergman_L/0/1/0/all/0/1\">Liron Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Anomaly detection methods require high-quality features. In recent years, the\nanomaly detection community has attempted to obtain better features using\nadvances in deep self-supervised feature learning. Surprisingly, a very\npromising direction, using pretrained deep features, has been mostly\noverlooked. In this paper, we first empirically establish the perhaps expected,\nbut unreported result, that combining pretrained features with simple anomaly\ndetection and segmentation methods convincingly outperforms, much more complex,\nstate-of-the-art methods.\n\nIn order to obtain further performance gains in anomaly detection, we adapt\npretrained features to the target distribution. Although transfer learning\nmethods are well established in multi-class classification problems, the\none-class classification (OCC) setting is not as well explored. It turns out\nthat naive adaptation methods, which typically work well in supervised\nlearning, often result in catastrophic collapse (feature deterioration) and\nreduce performance in OCC settings. A popular OCC method, DeepSVDD, advocates\nusing specialized architectures, but this limits the adaptation performance\ngain. We propose two methods for combating collapse: i) a variant of early\nstopping that dynamically learns the stopping iteration ii) elastic\nregularization inspired by continual learning. Our method, PANDA, outperforms\nthe state-of-the-art in the OCC, outlier exposure and anomaly segmentation\nsettings by large margins.",
          "link": "http://arxiv.org/abs/2010.05903",
          "publishedOn": "2021-08-13T01:56:55.697Z",
          "wordCount": 676,
          "title": "PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation. (arXiv:2010.05903v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_P/0/1/0/all/0/1\">Panhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ZHANG_L/0/1/0/all/0/1\">Lin ZHANG</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zijian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xuejing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_A/0/1/0/all/0/1\">Anlong Ming</a>",
          "description": "Retrieving occlusion relation among objects in a single image is challenging\ndue to sparsity of boundaries in image. We observe two key issues in existing\nworks: firstly, lack of an architecture which can exploit the limited amount of\ncoupling in the decoder stage between the two subtasks, namely occlusion\nboundary extraction and occlusion orientation prediction, and secondly,\nimproper representation of occlusion orientation. In this paper, we propose a\nnovel architecture called Occlusion-shared and Path-separated Network (OPNet),\nwhich solves the first issue by exploiting rich occlusion cues in shared\nhigh-level features and structured spatial information in task-specific\nlow-level features. We then design a simple but effective orthogonal occlusion\nrepresentation (OOR) to tackle the second issue. Our method surpasses the\nstate-of-the-art methods by 6.1%/8.3% Boundary-AP and 6.5%/10% Orientation-AP\non standard PIOD/BSDS ownership datasets. Code is available at\nhttps://github.com/fengpanhe/MT-ORL.",
          "link": "http://arxiv.org/abs/2108.05722",
          "publishedOn": "2021-08-13T01:56:55.682Z",
          "wordCount": 589,
          "title": "MT-ORL: Multi-Task Occlusion Relationship Learning. (arXiv:2108.05722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.11245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongcheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingchao Zhang</a>",
          "description": "We propose a general learning based framework for solving nonsmooth and\nnonconvex image reconstruction problems. We model the regularization function\nas the composition of the $l_{2,1}$ norm and a smooth but nonconvex feature\nmapping parametrized as a deep convolutional neural network. We develop a\nprovably convergent descent-type algorithm to solve the nonsmooth nonconvex\nminimization problem by leveraging the Nesterov's smoothing technique and the\nidea of residual learning, and learn the network parameters such that the\noutputs of the algorithm match the references in training data. Our method is\nversatile as one can employ various modern network structures into the\nregularization, and the resulting network inherits the guaranteed convergence\nof the algorithm. We also show that the proposed network is parameter-efficient\nand its performance compares favorably to the state-of-the-art methods in a\nvariety of image reconstruction problems in practice.",
          "link": "http://arxiv.org/abs/2007.11245",
          "publishedOn": "2021-08-13T01:56:55.675Z",
          "wordCount": 620,
          "title": "Learnable Descent Algorithm for Nonsmooth Nonconvex Image Reconstruction. (arXiv:2007.11245v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gilmour_E/0/1/0/all/0/1\">Elizabeth Gilmour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plotkin_N/0/1/0/all/0/1\">Noah Plotkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Leslie Smith</a>",
          "description": "Reinforcement learning (RL) is successful at learning to play games where the\nentire environment is visible. However, RL approaches are challenged in complex\ngames like Starcraft II and in real-world environments where the entire\nenvironment is not visible. In these more complex games with more limited\nvisual information, agents must choose where to look and how to optimally use\ntheir limited visual information in order to succeed at the game. We verify\nthat with a relatively simple model the agent can learn where to look in\nscenarios with a limited visual bandwidth. We develop a method for masking part\nof the environment in Atari games to force the RL agent to learn both where to\nlook and how to play the game in order to study where the RL agent learns to\nlook. In addition, we develop a neural network architecture and method for\nallowing the agent to choose where to look and what action to take in the Pong\ngame. Further, we analyze the strategies the agent learns to better understand\nhow the RL agent learns to play the game.",
          "link": "http://arxiv.org/abs/2108.05701",
          "publishedOn": "2021-08-13T01:56:55.668Z",
          "wordCount": 653,
          "title": "An Approach to Partial Observability in Games: Learning to Both Act and Observe. (arXiv:2108.05701v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beal_J/0/1/0/all/0/1\">Josh Beal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao-Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dong Huk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_A/0/1/0/all/0/1\">Andrew Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kislyuk_D/0/1/0/all/0/1\">Dmitry Kislyuk</a>",
          "description": "Large-scale pretraining of visual representations has led to state-of-the-art\nperformance on a range of benchmark computer vision tasks, yet the benefits of\nthese techniques at extreme scale in complex production systems has been\nrelatively unexplored. We consider the case of a popular visual discovery\nproduct, where these representations are trained with multi-task learning, from\nuse-case specific visual understanding (e.g. skin tone classification) to\ngeneral representation learning for all visual content (e.g. embeddings for\nretrieval). In this work, we describe how we (1) generate a dataset with over a\nbillion images via large weakly-supervised pretraining to improve the\nperformance of these visual representations, and (2) leverage Transformers to\nreplace the traditional convolutional backbone, with insights into both system\nand performance improvements, especially at 1B+ image scale. To support this\nbackbone model, we detail a systematic approach to deriving weakly-supervised\nimage annotations from heterogenous text signals, demonstrating the benefits of\nclustering techniques to handle the long-tail distribution of image labels.\nThrough a comprehensive study of offline and online evaluation, we show that\nlarge-scale Transformer-based pretraining provides significant benefits to\nindustry computer vision applications. The model is deployed in a production\nvisual shopping system, with 36% improvement in top-1 relevance and 23%\nimprovement in click-through volume. We conduct extensive experiments to better\nunderstand the empirical relationships between Transformer-based architectures,\ndataset scale, and the performance of production vision systems.",
          "link": "http://arxiv.org/abs/2108.05887",
          "publishedOn": "2021-08-13T01:56:55.651Z",
          "wordCount": 679,
          "title": "Billion-Scale Pretraining with Vision Transformers for Multi-Task Visual Representations. (arXiv:2108.05887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhihao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>",
          "description": "The combination of the traditional convolutional network (i.e., an\nauto-encoder) and the graph convolutional network has attracted much attention\nin clustering, in which the auto-encoder extracts the node attribute feature\nand the graph convolutional network captures the topological graph feature.\nHowever, the existing works (i) lack a flexible combination mechanism to\nadaptively fuse those two kinds of features for learning the discriminative\nrepresentation and (ii) overlook the multi-scale information embedded at\ndifferent layers for subsequent cluster assignment, leading to inferior\nclustering results. To this end, we propose a novel deep clustering method\nnamed Attention-driven Graph Clustering Network (AGCN). Specifically, AGCN\nexploits a heterogeneity-wise fusion module to dynamically fuse the node\nattribute feature and the topological graph feature. Moreover, AGCN develops a\nscale-wise fusion module to adaptively aggregate the multi-scale features\nembedded at different layers. Based on a unified optimization framework, AGCN\ncan jointly perform feature learning and cluster assignment in an unsupervised\nfashion. Compared with the existing deep clustering methods, our method is more\nflexible and effective since it comprehensively considers the numerous and\ndiscriminative information embedded in the network and directly produces the\nclustering results. Extensive quantitative and qualitative results on commonly\nused benchmark datasets validate that our AGCN consistently outperforms\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.05499",
          "publishedOn": "2021-08-13T01:56:55.644Z",
          "wordCount": 636,
          "title": "Attention-driven Graph Clustering Network. (arXiv:2108.05499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yucheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jiajun Bu</a>",
          "description": "Knowledge Distillation (KD) aims at transferring knowledge from a larger\nwell-optimized teacher network to a smaller learnable student network.Existing\nKD methods have mainly considered two types of knowledge, namely the individual\nknowledge and the relational knowledge. However, these two types of knowledge\nare usually modeled independently while the inherent correlations between them\nare largely ignored. It is critical for sufficient student network learning to\nintegrate both individual knowledge and relational knowledge while reserving\ntheir inherent correlation. In this paper, we propose to distill the novel\nholistic knowledge based on an attributed graph constructed among instances.\nThe holistic knowledge is represented as a unified graph-based embedding by\naggregating individual knowledge from relational neighborhood samples with\ngraph neural networks, the student network is learned by distilling the\nholistic knowledge in a contrastive manner. Extensive experiments and ablation\nstudies are conducted on benchmark datasets, the results demonstrate the\neffectiveness of the proposed method. The code has been published in\nhttps://github.com/wyc-ruiker/HKD",
          "link": "http://arxiv.org/abs/2108.05507",
          "publishedOn": "2021-08-13T01:56:55.638Z",
          "wordCount": 604,
          "title": "Distilling Holistic Knowledge with Graph Neural Networks. (arXiv:2108.05507v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueh-Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaowei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hanwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "While we have made significant progress on understanding hand-object\ninteractions in computer vision, it is still very challenging for robots to\nperform complex dexterous manipulation. In this paper, we propose a new\nplatform and pipeline, DexMV (Dex Manipulation from Videos), for imitation\nlearning to bridge the gap between computer vision and robot learning. We\ndesign a platform with: (i) a simulation system for complex dexterous\nmanipulation tasks with a multi-finger robot hand and (ii) a computer vision\nsystem to record large-scale demonstrations of a human hand conducting the same\ntasks. In our new pipeline, we extract 3D hand and object poses from the\nvideos, and convert them to robot demonstrations via motion retargeting. We\nthen apply and compare multiple imitation learning algorithms with the\ndemonstrations. We show that the demonstrations can indeed improve robot\nlearning by a large margin and solve the complex tasks which reinforcement\nlearning alone cannot solve. Project page with video:\nhttps://yzqin.github.io/dexmv/",
          "link": "http://arxiv.org/abs/2108.05877",
          "publishedOn": "2021-08-13T01:56:55.632Z",
          "wordCount": 608,
          "title": "DexMV: Imitation Learning for Dexterous Manipulation from Human Videos. (arXiv:2108.05877v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_A/0/1/0/all/0/1\">Aditya Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouganis_C/0/1/0/all/0/1\">Christos-Savvas Bouganis</a>",
          "description": "The increased memory and processing capabilities of today's edge devices\ncreate opportunities for greater edge intelligence. In the domain of vision,\nthe ability to adapt a Convolutional Neural Network's (CNN) structure and\nparameters to the input data distribution leads to systems with lower memory\nfootprint, latency and power consumption. However, due to the limited compute\nresources and memory budget on edge devices, it is necessary for the system to\nbe able to predict the latency and memory footprint of the training process in\norder to identify favourable training configurations of the network topology\nand device combination for efficient network adaptation. This work proposes\nperf4sight, an automated methodology for developing accurate models that\npredict CNN training memory footprint and latency given a target device and\nnetwork. This enables rapid identification of network topologies that can be\nretrained on the edge device with low resource consumption. With PyTorch as the\nframework and NVIDIA Jetson TX2 as the target device, the developed models\npredict training memory footprint and latency with 95% and 91% accuracy\nrespectively for a wide range of networks, opening the path towards efficient\nnetwork adaptation on edge GPUs.",
          "link": "http://arxiv.org/abs/2108.05580",
          "publishedOn": "2021-08-13T01:56:55.626Z",
          "wordCount": 647,
          "title": "perf4sight: A toolflow to model CNN training performance on Edge GPUs. (arXiv:2108.05580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1\">Inkyu Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sanghyun Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kwanyong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "Unsupervised Domain Adaptation (UDA) for semantic segmentation has been\nactively studied to mitigate the domain gap between label-rich source data and\nunlabeled target data. Despite these efforts, UDA still has a long way to go to\nreach the fully supervised performance. To this end, we propose a Labeling Only\nif Required strategy, LabOR, where we introduce a human-in-the-loop approach to\nadaptively give scarce labels to points that a UDA model is uncertain about. In\norder to find the uncertain points, we generate an inconsistency mask using the\nproposed adaptive pixel selector and we label these segment-based regions to\nachieve near supervised performance with only a small fraction (about 2.2%)\nground truth points, which we call \"Segment based Pixel-Labeling (SPL)\". To\nfurther reduce the efforts of the human annotator, we also propose \"Point-based\nPixel-Labeling (PPL)\", which finds the most representative points for labeling\nwithin the generated inconsistency mask. This reduces efforts from 2.2% segment\nlabel to 40 points label while minimizing performance degradation. Through\nextensive experimentation, we show the advantages of this new framework for\ndomain adaptive semantic segmentation while minimizing human labor costs.",
          "link": "http://arxiv.org/abs/2108.05570",
          "publishedOn": "2021-08-13T01:56:55.610Z",
          "wordCount": 634,
          "title": "LabOR: Labeling Only if Required for Domain Adaptive Semantic Segmentation. (arXiv:2108.05570v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoshi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>",
          "description": "The abundance and richness of Internet photos of landmarks and cities has led\nto significant progress in 3D vision over the past two decades, including\nautomated 3D reconstructions of the world's landmarks from tourist photos.\nHowever, a major source of information available for these 3D-augmented\ncollections---namely language, e.g., from image captions---has been virtually\nuntapped. In this work, we present WikiScenes, a new, large-scale dataset of\nlandmark photo collections that contains descriptive text in the form of\ncaptions and hierarchical category names. WikiScenes forms a new testbed for\nmultimodal reasoning involving images, text, and 3D geometry. We demonstrate\nthe utility of WikiScenes for learning semantic concepts over images and 3D\nmodels. Our weakly-supervised framework connects images, 3D structure, and\nsemantics---utilizing the strong constraints provided by 3D geometry---to\nassociate semantic concepts to image pixels and 3D points.",
          "link": "http://arxiv.org/abs/2108.05863",
          "publishedOn": "2021-08-13T01:56:55.604Z",
          "wordCount": 590,
          "title": "Towers of Babel: Combining Images, Language, and 3D Geometry for Learning Multimodal Vision. (arXiv:2108.05863v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.06294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhewei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_W/0/1/0/all/0/1\">Wen Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>",
          "description": "We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video\nFrame Interpolation (VFI). Many recent flow-based VFI methods first estimate\nthe bi-directional optical flows, then scale and reverse them to approximate\nintermediate flows, leading to artifacts on motion boundaries. RIFE uses a\nneural network named IFNet that can directly estimate the intermediate flows\nfrom coarse-to-fine with much better speed. We design a privileged distillation\nscheme for training intermediate flow model, which leads to a large performance\nimprovement. Experiments demonstrate that RIFE is flexible and can achieve\nstate-of-the-art performance on several public benchmarks. The code is\navailable at \\url{https://github.com/hzwer/arXiv2020-RIFE}",
          "link": "http://arxiv.org/abs/2011.06294",
          "publishedOn": "2021-08-13T01:56:55.598Z",
          "wordCount": 623,
          "title": "RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation. (arXiv:2011.06294v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shreya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1\">Abhinav Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knibbe_J/0/1/0/all/0/1\">Jarrod Knibbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1\">Qiang Ji</a>",
          "description": "Eye gaze analysis is an important research problem in the field of computer\nvision and Human-Computer Interaction (HCI). Even with significant progress in\nthe last few years, automatic gaze analysis still remains challenging due to\nthe individuality of eyes, eye-head interplay, occlusion, image quality, and\nillumination conditions. There are several open questions including what are\nthe important cues to interpret gaze direction in an unconstrained environment\nwithout prior knowledge and how to encode them in real-time. We review the\nprogress across a range of gaze analysis tasks and applications to shed light\non these fundamental questions; identify effective methods in gaze analysis and\nprovide possible future directions. We analyze recent gaze estimation and\nsegmentation methods, especially in the unsupervised and weakly supervised\ndomain, based on their advantages and reported evaluation metrics. Our analysis\nshows that the development of a robust and generic gaze analysis method still\nneeds to address real-world challenges such as unconstrained setup and learning\nwith less supervision. We conclude by discussing future research directions for\ndesigning a real-world gaze analysis system that can propagate to other domains\nincluding computer vision, AR (Augmented Reality), VR (Virtual Reality), and\nHCI (Human Computer Interaction).",
          "link": "http://arxiv.org/abs/2108.05479",
          "publishedOn": "2021-08-13T01:56:55.591Z",
          "wordCount": 634,
          "title": "Automatic Gaze Analysis: A Survey of DeepLearning based Approaches. (arXiv:2108.05479v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_T/0/1/0/all/0/1\">Tianrui Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_X/0/1/0/all/0/1\">Xinyu Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Annan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>",
          "description": "Gait recognition under multiple views is an important computer vision and\npattern recognition task. In the emerging convolutional neural network based\napproaches, the information of view angle is ignored to some extent. Instead of\ndirect view estimation and training view-specific recognition models, we\npropose a compatible framework that can embed view information into existing\narchitectures of gait recognition. The embedding is simply achieved by a\nselective projection layer. Experimental results on two large public datasets\nshow that the proposed framework is very effective.",
          "link": "http://arxiv.org/abs/2108.05524",
          "publishedOn": "2021-08-13T01:56:55.586Z",
          "wordCount": 526,
          "title": "Silhouette based View embeddings for Gait Recognition under Multiple Views. (arXiv:2108.05524v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaogang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "To synthesize a realistic action sequence based on a single human image, it\nis crucial to model both motion patterns and diversity in the action video.\nThis paper proposes an Action Conditional Temporal Variational AutoEncoder\n(ACT-VAE) to improve motion prediction accuracy and capture movement diversity.\nACT-VAE predicts pose sequences for an action clips from a single input image.\nIt is implemented as a deep generative model that maintains temporal coherence\naccording to the action category with a novel temporal modeling on latent\nspace. Further, ACT-VAE is a general action sequence prediction framework. When\nconnected with a plug-and-play Pose-to-Image (P2I) network, ACT-VAE can\nsynthesize image sequences. Extensive experiments bear out our approach can\npredict accurate pose and synthesize realistic image sequences, surpassing\nstate-of-the-art approaches. Compared to existing methods, ACT-VAE improves\nmodel accuracy and preserves diversity.",
          "link": "http://arxiv.org/abs/2108.05658",
          "publishedOn": "2021-08-13T01:56:55.570Z",
          "wordCount": 576,
          "title": "Conditional Temporal Variational AutoEncoder for Action Video Prediction. (arXiv:2108.05658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "We present Mobile-Former, a parallel design of MobileNet and Transformer with\na two-way bridge in between. This structure leverages the advantage of\nMobileNet at local processing and transformer at global interaction. And the\nbridge enables bidirectional fusion of local and global features. Different\nwith recent works on vision transformer, the transformer in Mobile-Former\ncontains very few tokens (e.g. less than 6 tokens) that are randomly\ninitialized, resulting in low computational cost. Combining with the proposed\nlight-weight cross attention to model the bridge, Mobile-Former is not only\ncomputationally efficient, but also has more representation power,\noutperforming MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet\nclassification. For instance, it achieves 77.9\\% top-1 accuracy at 294M FLOPs,\ngaining 1.3\\% over MobileNetV3 but saving 17\\% of computations. When\ntransferring to object detection, Mobile-Former outperforms MobileNetV3 by 8.6\nAP.",
          "link": "http://arxiv.org/abs/2108.05895",
          "publishedOn": "2021-08-13T01:56:55.548Z",
          "wordCount": 578,
          "title": "Mobile-Former: Bridging MobileNet and Transformer. (arXiv:2108.05895v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rego_J/0/1/0/all/0/1\">Joshua D. Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaijin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinwei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasuriya_S/0/1/0/all/0/1\">Suren Jayasuriya</a>",
          "description": "The lensless pinhole camera is perhaps the earliest and simplest form of an\nimaging system using only a pinhole-sized aperture in place of a lens. They can\ncapture an infinite depth-of-field and offer greater freedom from optical\ndistortion over their lens-based counterparts. However, the inherent\nlimitations of a pinhole system result in lower sharpness from blur caused by\noptical diffraction and higher noise levels due to low light throughput of the\nsmall aperture, requiring very long exposure times to capture well-exposed\nimages. In this paper, we explore an image restoration pipeline using deep\nlearning and domain-knowledge of the pinhole system to enhance the pinhole\nimage quality through a joint denoise and deblur approach. Our approach allows\nfor more practical exposure times for hand-held photography and provides higher\nimage quality, making it more suitable for daily photography compared to other\nlensless cameras while keeping size and cost low. This opens up the potential\nof pinhole cameras to be used in smaller devices, such as smartphones.",
          "link": "http://arxiv.org/abs/2108.05563",
          "publishedOn": "2021-08-13T01:56:55.542Z",
          "wordCount": 621,
          "title": "Deep Camera Obscura: An Image Restoration Pipeline for Lensless Pinhole Photography. (arXiv:2108.05563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Binici_K/0/1/0/all/0/1\">Kuluhan Binici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nam Trung Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1\">Tulika Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leman_K/0/1/0/all/0/1\">Karianto Leman</a>",
          "description": "With the increasing popularity of deep learning on edge devices, compressing\nlarge neural networks to meet the hardware requirements of resource-constrained\ndevices became a significant research direction. Numerous compression\nmethodologies are currently being used to reduce the memory sizes and energy\nconsumption of neural networks. Knowledge distillation (KD) is among such\nmethodologies and it functions by using data samples to transfer the knowledge\ncaptured by a large model (teacher) to a smaller one(student). However, due to\nvarious reasons, the original training data might not be accessible at the\ncompression stage. Therefore, data-free model compression is an ongoing\nresearch problem that has been addressed by various works. In this paper, we\npoint out that catastrophic forgetting is a problem that can potentially be\nobserved in existing data-free distillation methods. Moreover, the sample\ngeneration strategies in some of these methods could result in a mismatch\nbetween the synthetic and real data distributions. To prevent such problems, we\npropose a data-free KD framework that maintains a dynamic collection of\ngenerated samples over time. Additionally, we add the constraint of matching\nthe real data distribution in sample generation strategies that target maximum\ninformation gain. Our experiments demonstrate that we can improve the accuracy\nof the student models obtained via KD when compared with state-of-the-art\napproaches on the SVHN, Fashion MNIST and CIFAR100 datasets.",
          "link": "http://arxiv.org/abs/2108.05698",
          "publishedOn": "2021-08-13T01:56:55.536Z",
          "wordCount": 665,
          "title": "Preventing Catastrophic Forgetting and Distribution Mismatch in Knowledge Distillation via Synthetic Data. (arXiv:2108.05698v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_M/0/1/0/all/0/1\">Michael A. Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen-Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidinosti_C/0/1/0/all/0/1\">Christopher P. Bidinosti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_C/0/1/0/all/0/1\">Christopher J. Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godee_C/0/1/0/all/0/1\">Cara M. Godee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajmani_M/0/1/0/all/0/1\">Manisha Ajmani</a>",
          "description": "We present two large datasets of labelled plant-images that are suited\ntowards the training of machine learning and computer vision models. The first\ndataset encompasses as the day of writing over 1.2 million images of\nindoor-grown crops and weeds common to the Canadian Prairies and many US\nstates. The second dataset consists of over 540,000 images of plants imaged in\nfarmland. All indoor plant images are labelled by species and we provide rich\netadata on the level of individual images. This comprehensive database allows\nto filter the datasets under user-defined specifications such as for example\nthe crop-type or the age of the plant. Furthermore, the indoor dataset contains\nimages of plants taken from a wide variety of angles, including profile shots,\ntop-down shots, and angled perspectives. The images taken from plants in fields\nare all from a top-down perspective and contain usually multiple plants per\nimage. For these images metadata is also available. In this paper we describe\nboth datasets' characteristics with respect to plant variety, plant age, and\nnumber of images. We further introduce an open-access sample of the\nindoor-dataset that contains 1,000 images of each species covered in our\ndataset. These, in total 14,000 images, had been selected, such that they form\na representative sample with respect to plant age and ndividual plants per\nspecies. This sample serves as a quick entry point for new users to the\ndataset, allowing them to explore the data on a small scale and find the\nparameters of data most useful for their application without having to deal\nwith hundreds of thousands of individual images.",
          "link": "http://arxiv.org/abs/2108.05789",
          "publishedOn": "2021-08-13T01:56:55.529Z",
          "wordCount": 732,
          "title": "Presenting an extensive lab- and field-image dataset of crops and weeds for computer vision tasks in agriculture. (arXiv:2108.05789v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sarthak Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamo_H/0/1/0/all/0/1\">Helisa Dhamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farshad_A/0/1/0/all/0/1\">Azade Farshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musatian_S/0/1/0/all/0/1\">Sabrina Musatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Despite recent advancements in single-domain or single-object image\ngeneration, it is still challenging to generate complex scenes containing\ndiverse, multiple objects and their interactions. Scene graphs, composed of\nnodes as objects and directed-edges as relationships among objects, offer an\nalternative representation of a scene that is more semantically grounded than\nimages. We hypothesize that a generative model for scene graphs might be able\nto learn the underlying semantic structure of real-world scenes more\neffectively than images, and hence, generate realistic novel scenes in the form\nof scene graphs. In this work, we explore a new task for the unconditional\ngeneration of semantic scene graphs. We develop a deep auto-regressive model\ncalled SceneGraphGen which can directly learn the probability distribution over\nlabelled and directed graphs using a hierarchical recurrent architecture. The\nmodel takes a seed object as input and generates a scene graph in a sequence of\nsteps, each step generating an object node, followed by a sequence of\nrelationship edges connecting to the previous nodes. We show that the scene\ngraphs generated by SceneGraphGen are diverse and follow the semantic patterns\nof real-world scenes. Additionally, we demonstrate the application of the\ngenerated graphs in image synthesis, anomaly detection and scene graph\ncompletion.",
          "link": "http://arxiv.org/abs/2108.05884",
          "publishedOn": "2021-08-13T01:56:55.513Z",
          "wordCount": 641,
          "title": "Unconditional Scene Graph Generation. (arXiv:2108.05884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rockwell_C/0/1/0/all/0/1\">Chris Rockwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1\">David F. Fouhey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>",
          "description": "Recent advancements in differentiable rendering and 3D reasoning have driven\nexciting results in novel view synthesis from a single image. Despite realistic\nresults, methods are limited to relatively small view change. In order to\nsynthesize immersive scenes, models must also be able to extrapolate. We\npresent an approach that fuses 3D reasoning with autoregressive modeling to\noutpaint large view changes in a 3D-consistent manner, enabling scene\nsynthesis. We demonstrate considerable improvement in single image large-angle\nview synthesis results compared to a variety of methods and possible variants\nacross simulated and real datasets. In addition, we show increased 3D\nconsistency compared to alternative accumulation methods. Project website:\nhttps://crockwell.github.io/pixelsynth/",
          "link": "http://arxiv.org/abs/2108.05892",
          "publishedOn": "2021-08-13T01:56:55.496Z",
          "wordCount": 550,
          "title": "PixelSynth: Generating a 3D-Consistent Experience from a Single Image. (arXiv:2108.05892v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Recent research has witnessed advances in facial image editing tasks\nincluding face swapping and face reenactment. However, these methods are\nconfined to dealing with one specific task at a time. In addition, for video\nfacial editing, previous methods either simply apply transformations frame by\nframe or utilize multiple frames in a concatenated or iterative fashion, which\nleads to noticeable visual flickers. In this paper, we propose a unified\ntemporally consistent facial video editing framework termed UniFaceGAN. Based\non a 3D reconstruction model and a simple yet efficient dynamic training sample\nselection mechanism, our framework is designed to handle face swapping and face\nreenactment simultaneously. To enforce the temporal consistency, a novel 3D\ntemporal loss constraint is introduced based on the barycentric coordinate\ninterpolation. Besides, we propose a region-aware conditional normalization\nlayer to replace the traditional AdaIN or SPADE to synthesize more\ncontext-harmonious results. Compared with the state-of-the-art facial image\nediting methods, our framework generates video portraits that are more\nphoto-realistic and temporally smooth.",
          "link": "http://arxiv.org/abs/2108.05650",
          "publishedOn": "2021-08-13T01:56:55.483Z",
          "wordCount": 621,
          "title": "UniFaceGAN: A Unified Framework for Temporally Consistent Facial Video Editing. (arXiv:2108.05650v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishida_S/0/1/0/all/0/1\">Shu Ishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jo&#xe3;o F. Henriques</a>",
          "description": "We train embodied neural networks to plan and navigate unseen complex 3D\nenvironments, emphasising real-world deployment. Rather than requiring prior\nknowledge of the agent or environment, the planner learns to model the state\ntransitions and rewards. To avoid the potentially hazardous trial-and-error of\nreinforcement learning, we focus on differentiable planners such as Value\nIteration Networks (VIN), which are trained offline from safe expert\ndemonstrations. Although they work well in small simulations, we address two\nmajor limitations that hinder their deployment. First, we observed that current\ndifferentiable planners struggle to plan long-term in environments with a high\nbranching complexity. While they should ideally learn to assign low rewards to\nobstacles to avoid collisions, we posit that the constraints imposed on the\nnetwork are not strong enough to guarantee the network to learn sufficiently\nlarge penalties for every possible collision. We thus impose a structural\nconstraint on the value iteration, which explicitly learns to model any\nimpossible actions. Secondly, we extend the model to work with a limited\nperspective camera under translation and rotation, which is crucial for real\nrobot deployment. Many VIN-like planners assume a 360 degrees or overhead view\nwithout rotation. In contrast, our method uses a memory-efficient lattice map\nto aggregate CNN embeddings of partial observations, and models the rotational\ndynamics explicitly using a 3D state-space grid (translation and rotation). Our\nproposals significantly improve semantic navigation and exploration on several\n2D and 3D environments, succeeding in settings that are otherwise challenging\nfor this class of methods. As far as we know, we are the first to successfully\nperform differentiable planning on the difficult Active Vision Dataset,\nconsisting of real images captured from a robot.",
          "link": "http://arxiv.org/abs/2108.05713",
          "publishedOn": "2021-08-13T01:56:55.475Z",
          "wordCount": 713,
          "title": "Towards real-world navigation with deep differentiable planners. (arXiv:2108.05713v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Runsong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tengping Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bisheng Yang</a>",
          "description": "This paper presents a neural network for robust normal estimation on point\nclouds, named AdaFit, that can deal with point clouds with noise and density\nvariations. Existing works use a network to learn point-wise weights for\nweighted least squares surface fitting to estimate the normals, which has\ndifficulty in finding accurate normals in complex regions or containing noisy\npoints. By analyzing the step of weighted least squares surface fitting, we\nfind that it is hard to determine the polynomial order of the fitting surface\nand the fitting surface is sensitive to outliers. To address these problems, we\npropose a simple yet effective solution that adds an additional offset\nprediction to improve the quality of normal estimation. Furthermore, in order\nto take advantage of points from different neighborhood sizes, a novel Cascaded\nScale Aggregation layer is proposed to help the network predict more accurate\npoint-wise offsets and weights. Extensive experiments demonstrate that AdaFit\nachieves state-of-the-art performance on both the synthetic PCPNet dataset and\nthe real-word SceneNN dataset.",
          "link": "http://arxiv.org/abs/2108.05836",
          "publishedOn": "2021-08-13T01:56:55.470Z",
          "wordCount": 614,
          "title": "AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds. (arXiv:2108.05836v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmuck_P/0/1/0/all/0/1\">Patrik Schmuck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_T/0/1/0/all/0/1\">Thomas Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karrer_M/0/1/0/all/0/1\">Marco Karrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perraudin_J/0/1/0/all/0/1\">Jonathan Perraudin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chli_M/0/1/0/all/0/1\">Margarita Chli</a>",
          "description": "Collaborative SLAM enables a group of agents to simultaneously co-localize\nand jointly map an environment, thus paving the way to wide-ranging\napplications of multi-robot perception and multi-user AR experiences by\neliminating the need for external infrastructure or pre-built maps. This\narticle presents COVINS, a novel collaborative SLAM system, that enables\nmulti-agent, scalable SLAM in large environments and for large teams of more\nthan 10 agents. The paradigm here is that each agent runs visual-inertial\nodomety independently onboard in order to ensure its autonomy, while sharing\nmap information with the COVINS server back-end running on a powerful local PC\nor a remote cloud server. The server back-end establishes an accurate\ncollaborative global estimate from the contributed data, refining the joint\nestimate by means of place recognition, global optimization and removal of\nredundant data, in order to ensure an accurate, but also efficient SLAM\nprocess. A thorough evaluation of COVINS reveals increased accuracy of the\ncollaborative SLAM estimates, as well as efficiency in both removing redundant\ninformation and reducing the coordination overhead, and demonstrates successful\noperation in a large-scale mission with 12 agents jointly performing SLAM.",
          "link": "http://arxiv.org/abs/2108.05756",
          "publishedOn": "2021-08-13T01:56:55.445Z",
          "wordCount": 621,
          "title": "COVINS: Visual-Inertial SLAM for Centralized Collaboration. (arXiv:2108.05756v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xubo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheyang Li</a>",
          "description": "One-shot neural architecture search (NAS) applies weight-sharing supernet to\nreduce the unaffordable computation overhead of automated architecture\ndesigning. However, the weight-sharing technique worsens the ranking\nconsistency of performance due to the interferences between different candidate\nnetworks. To address this issue, we propose a candidates enhancement method and\nprogressive training pipeline to improve the ranking correlation of supernet.\nSpecifically, we carefully redesign the sub-networks in the supernet and map\nthe original supernet to a new one of high capacity. In addition, we gradually\nadd narrow branches of supernet to reduce the degree of weight sharing which\neffectively alleviates the mutual interference between sub-networks. Finally,\nour method ranks the 1st place in the Supernet Track of CVPR2021 1st\nLightweight NAS Challenge.",
          "link": "http://arxiv.org/abs/2108.05866",
          "publishedOn": "2021-08-13T01:56:55.439Z",
          "wordCount": 573,
          "title": "Improving Ranking Correlation of Supernet with Candidates Enhancement and Progressive Training. (arXiv:2108.05866v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatziagapi_A/0/1/0/all/0/1\">Aggelina Chatziagapi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athar_S/0/1/0/all/0/1\">ShahRukh Athar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>",
          "description": "We present SIDER(Single-Image neural optimization for facial geometric DEtail\nRecovery), a novel photometric optimization method that recovers detailed\nfacial geometry from a single image in an unsupervised manner. Inspired by\nclassical techniques of coarse-to-fine optimization and recent advances in\nimplicit neural representations of 3D shape, SIDER combines a geometry prior\nbased on statistical models and Signed Distance Functions (SDFs) to recover\nfacial details from single images. First, it estimates a coarse geometry using\na morphable model represented as an SDF. Next, it reconstructs facial geometry\ndetails by optimizing a photometric loss with respect to the ground truth\nimage. In contrast to prior work, SIDER does not rely on any dataset priors and\ndoes not require additional supervision from multiple views, lighting changes\nor ground truth 3D shape. Extensive qualitative and quantitative evaluation\ndemonstrates that our method achieves state-of-the-art on facial geometric\ndetail recovery, using only a single in-the-wild image.",
          "link": "http://arxiv.org/abs/2108.05465",
          "publishedOn": "2021-08-13T01:56:55.433Z",
          "wordCount": 589,
          "title": "SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery. (arXiv:2108.05465v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_J/0/1/0/all/0/1\">Jovita Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>",
          "description": "Differentiable architecture search (DARTS) is a widely researched tool for\nneural architecture search, due to its promising results for image\nclassification. The main benefit of DARTS is the effectiveness achieved through\nthe weight-sharing one-shot paradigm, which allows efficient architecture\nsearch. In this work, we investigate DARTS in a systematic case study of\ninverse problems, which allows us to analyze these potential benefits in a\ncontrolled manner. Although we demonstrate that the success of DARTS can be\nextended from image classification to reconstruction, our experiments yield\nthree fundamental difficulties in the evaluation of DARTS-based methods: First,\nthe results show a large variance in all test cases. Second, the final\nperformance is highly dependent on the hyperparameters of the optimizer. And\nthird, the performance of the weight-sharing architecture used during training\ndoes not reflect the final performance of the found architecture well. Thus, we\nconclude the necessity to 1) report the results of any DARTS-based methods from\nseveral runs along with its underlying performance statistics, 2) show the\ncorrelation of the training and final architecture performance, and 3)\ncarefully consider if the computational efficiency of DARTS outweighs the costs\nof hyperparameter optimization and multiple runs.",
          "link": "http://arxiv.org/abs/2108.05647",
          "publishedOn": "2021-08-13T01:56:55.427Z",
          "wordCount": 637,
          "title": "DARTS for Inverse Problems: a Study on Hyperparameter Sensitivity. (arXiv:2108.05647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bangunharcana_A/0/1/0/all/0/1\">Antyanta Bangunharcana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seokju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soohyun Kim</a>",
          "description": "Volumetric deep learning approach towards stereo matching aggregates a cost\nvolume computed from input left and right images using 3D convolutions. Recent\nworks showed that utilization of extracted image features and a spatially\nvarying cost volume aggregation complements 3D convolutions. However, existing\nmethods with spatially varying operations are complex, cost considerable\ncomputation time, and cause memory consumption to increase. In this work, we\nconstruct Guided Cost volume Excitation (GCE) and show that simple channel\nexcitation of cost volume guided by image can improve performance considerably.\nMoreover, we propose a novel method of using top-k selection prior to\nsoft-argmin disparity regression for computing the final disparity estimate.\nCombining our novel contributions, we present an end-to-end network that we\ncall Correlate-and-Excite (CoEx). Extensive experiments of our model on the\nSceneFlow, KITTI 2012, and KITTI 2015 datasets demonstrate the effectiveness\nand efficiency of our model and show that our model outperforms other\nspeed-based algorithms while also being competitive to other state-of-the-art\nalgorithms. Codes will be made available at https://github.com/antabangun/coex.",
          "link": "http://arxiv.org/abs/2108.05773",
          "publishedOn": "2021-08-13T01:56:55.421Z",
          "wordCount": 633,
          "title": "Correlate-and-Excite: Real-Time Stereo Matching via Guided Cost Volume Excitation. (arXiv:2108.05773v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingfu Zhang</a>",
          "description": "This paper investigates the problem of recovering hyperspectral (HS) images\nfrom single RGB images. To tackle such a severely ill-posed problem, we propose\na physically-interpretable, compact, efficient, and end-to-end learning-based\nframework, namely AGD-Net. Precisely, by taking advantage of the imaging\nprocess, we first formulate the problem explicitly based on the classic\ngradient descent algorithm. Then, we design a lightweight neural network with a\nmulti-stage architecture to mimic the formed amended gradient descent process,\nin which efficient convolution and novel spectral zero-mean normalization are\nproposed to effectively extract spatial-spectral features for regressing an\ninitialization, a basic gradient, and an incremental gradient. Besides, based\non the approximate low-rank property of HS images, we propose a novel rank loss\nto promote the similarity between the global structures of reconstructed and\nground-truth HS images, which is optimized with our singular value weighting\nstrategy during training. Moreover, AGD-Net, a single network after one-time\ntraining, is flexible to handle the reconstruction with various spectral\nresponse functions. Extensive experiments over three commonly-used benchmark\ndatasets demonstrate that AGD-Net can improve the reconstruction quality by\nmore than 1.0 dB on average while saving 67$\\times$ parameters and 32$\\times$\nFLOPs, compared with state-of-the-art methods. The code will be publicly\navailable at https://github.com/zbzhzhy/GD-Net.",
          "link": "http://arxiv.org/abs/2108.05547",
          "publishedOn": "2021-08-13T01:56:55.404Z",
          "wordCount": 649,
          "title": "Deep Amended Gradient Descent for Efficient Spectral Reconstruction from Single RGB Images. (arXiv:2108.05547v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_N/0/1/0/all/0/1\">Noa Barzilay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_T/0/1/0/all/0/1\">Tal Berkovitz Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>",
          "description": "Unsupervised style transfer that supports diverse input styles using only one\ntrained generator is a challenging and interesting task in computer vision.\nThis paper proposes a Multi-IlluStrator Style Generative Adversarial Network\n(MISS GAN) that is a multi-style framework for unsupervised\nimage-to-illustration translation, which can generate styled yet content\npreserving images. The illustrations dataset is a challenging one since it is\ncomprised of illustrations of seven different illustrators, hence contains\ndiverse styles. Existing methods require to train several generators (as the\nnumber of illustrators) to handle the different illustrators' styles, which\nlimits their practical usage, or require to train an image specific network,\nwhich ignores the style information provided in other images of the\nillustrator. MISS GAN is both input image specific and uses the information of\nother images using only one trained model.",
          "link": "http://arxiv.org/abs/2108.05693",
          "publishedOn": "2021-08-13T01:56:55.391Z",
          "wordCount": 591,
          "title": "MISS GAN: A Multi-IlluStrator Style Generative Adversarial Network for image to illustration translation. (arXiv:2108.05693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ajinkya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giguere_S/0/1/0/all/0/1\">Stephen Giguere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1\">Rudolf Lioutikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>",
          "description": "We propose a method that efficiently learns distributions over articulation\nmodel parameters directly from depth images without the need to know\narticulation model categories a priori. By contrast, existing methods that\nlearn articulation models from raw observations typically only predict point\nestimates of the model parameters, which are insufficient to guarantee the safe\nmanipulation of articulated objects. Our core contributions include a novel\nrepresentation for distributions over rigid body transformations and\narticulation model parameters based on screw theory, von Mises-Fisher\ndistributions, and Stiefel manifolds. Combining these concepts allows for an\nefficient, mathematically sound representation that implicitly satisfies the\nconstraints that rigid body transformations and articulations must adhere to.\nLeveraging this representation, we introduce a novel deep learning based\napproach, DUST-net, that performs category-independent articulation model\nestimation while also providing model uncertainties. We evaluate our approach\non several benchmarking datasets and real-world objects and compare its\nperformance with two current state-of-the-art methods. Our results demonstrate\nthat DUST-net can successfully learn distributions over articulation models for\nnovel objects across articulation model categories, which generate point\nestimates with better accuracy than state-of-the-art methods and effectively\ncapture the uncertainty over predicted model parameters due to noisy inputs.",
          "link": "http://arxiv.org/abs/2108.05875",
          "publishedOn": "2021-08-13T01:56:55.381Z",
          "wordCount": 647,
          "title": "Distributional Depth-Based Estimation of Object Articulation Models. (arXiv:2108.05875v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andrade_Loarca_H/0/1/0/all/0/1\">H&#xe9;ctor Andrade-Loarca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1\">Gitta Kutyniok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktem_O/0/1/0/all/0/1\">Ozan &#xd6;ktem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_P/0/1/0/all/0/1\">Philipp Petersen</a>",
          "description": "We present a deep learning-based algorithm to jointly solve a reconstruction\nproblem and a wavefront set extraction problem in tomographic imaging. The\nalgorithm is based on a recently developed digital wavefront set extractor as\nwell as the well-known microlocal canonical relation for the Radon transform.\nWe use the wavefront set information about x-ray data to improve the\nreconstruction by requiring that the underlying neural networks simultaneously\nextract the correct ground truth wavefront set and ground truth image. As a\nnecessary theoretical step, we identify the digital microlocal canonical\nrelations for deep convolutional residual neural networks. We find strong\nnumerical evidence for the effectiveness of this approach.",
          "link": "http://arxiv.org/abs/2108.05732",
          "publishedOn": "2021-08-13T01:56:55.376Z",
          "wordCount": 558,
          "title": "Deep Microlocal Reconstruction for Limited-Angle Tomography. (arXiv:2108.05732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aman Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1\">Rohan Ramanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1\">Anika Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sirou Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingzhou Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keerthi_S/0/1/0/all/0/1\">S. Sathiya Keerthi</a>",
          "description": "Over-parameterized deep networks trained using gradient-based optimizers are\na popular choice for solving classification and ranking problems. Without\nappropriately tuned $\\ell_2$ regularization or weight decay, such networks have\nthe tendency to make output scores (logits) and network weights large, causing\ntraining loss to become too small and the network to lose its adaptivity\n(ability to move around) in the parameter space. Although regularization is\ntypically understood from an overfitting perspective, we highlight its role in\nmaking the network more adaptive and enabling it to escape more easily from\nweights that generalize poorly. To provide such a capability, we propose a\nmethod called Logit Attenuating Weight Normalization (LAWN), that can be\nstacked onto any gradient-based optimizer. LAWN controls the logits by\nconstraining the weight norms of layers in the final homogeneous sub-network.\nEmpirically, we show that the resulting LAWN variant of the optimizer makes a\ndeep network more adaptive to finding minimas with superior generalization\nperformance on large-scale image classification and recommender systems. While\nLAWN is particularly impressive in improving Adam, it greatly improves all\noptimizers when used with large batch sizes",
          "link": "http://arxiv.org/abs/2108.05839",
          "publishedOn": "2021-08-13T01:56:55.357Z",
          "wordCount": 629,
          "title": "Logit Attenuating Weight Normalization. (arXiv:2108.05839v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize\nactions in untrimmed videos with only video-level labels. Currently, most\nstate-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline:\nproducing snippet-level predictions first and then aggregating to the\nvideo-level prediction. However, we argue that existing methods have overlooked\ntwo important drawbacks: 1) inadequate use of motion information and 2) the\nincompatibility of prevailing cross-entropy training loss. In this paper, we\nanalyze that the motion cues behind the optical flow features are complementary\ninformative. Inspired by this, we propose to build a context-dependent motion\nprior, termed as motionness. Specifically, a motion graph is introduced to\nmodel motionness based on the local motion carrier (e.g., optical flow). In\naddition, to highlight more informative video snippets, a motion-guided loss is\nproposed to modulate the network training conditioned on motionness scores.\nExtensive ablation studies confirm that motionness efficaciously models\naction-of-interest, and the motion-guided loss leads to more accurate results.\nBesides, our motion-guided loss is a plug-and-play loss function and is\napplicable with existing WSTAL methods. Without loss of generality, based on\nthe standard MIL pipeline, our method achieves new state-of-the-art performance\non three challenging benchmarks, including THUMOS'14, ActivityNet v1.2 and\nv1.3.",
          "link": "http://arxiv.org/abs/2108.05607",
          "publishedOn": "2021-08-13T01:56:55.240Z",
          "wordCount": 633,
          "title": "Deep Motion Prior for Weakly-Supervised Temporal Action Localization. (arXiv:2108.05607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Visual affordance grounding aims to segment all possible interaction regions\nbetween people and objects from an image/video, which is beneficial for many\napplications, such as robot grasping and action recognition. However, existing\nmethods mainly rely on the appearance feature of the objects to segment each\nregion of the image, which face the following two problems: (i) there are\nmultiple possible regions in an object that people interact with; and (ii)\nthere are multiple possible human interactions in the same object region. To\naddress these problems, we propose a Hand-aided Affordance Grounding Network\n(HAGNet) that leverages the aided clues provided by the position and action of\nthe hand in demonstration videos to eliminate the multiple possibilities and\nbetter locate the interaction regions in the object. Specifically, HAG-Net has\na dual-branch structure to process the demonstration video and object image.\nFor the video branch, we introduce hand-aided attention to enhance the region\naround the hand in each video frame and then use the LSTM network to aggregate\nthe action features. For the object branch, we introduce a semantic enhancement\nmodule (SEM) to make the network focus on different parts of the object\naccording to the action classes and utilize a distillation loss to align the\noutput features of the object branch with that of the video branch and transfer\nthe knowledge in the video branch to the object branch. Quantitative and\nqualitative evaluations on two challenging datasets show that our method has\nachieved stateof-the-art results for affordance grounding. The source code will\nbe made available to the public.",
          "link": "http://arxiv.org/abs/2108.05675",
          "publishedOn": "2021-08-13T01:56:55.222Z",
          "wordCount": 691,
          "title": "Learning Visual Affordance Grounding from Demonstration Videos. (arXiv:2108.05675v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>",
          "description": "Recognizing and localizing objects in the 3D space is a crucial ability for\nan AI agent to perceive its surrounding environment. While significant progress\nhas been achieved with expensive LiDAR point clouds, it poses a great challenge\nfor 3D object detection given only a monocular image. While there exist\ndifferent alternatives for tackling this problem, it is found that they are\neither equipped with heavy networks to fuse RGB and depth information or\nempirically ineffective to process millions of pseudo-LiDAR points. With\nin-depth examination, we realize that these limitations are rooted in\ninaccurate object localization. In this paper, we propose a novel and\nlightweight approach, dubbed Progressive Coordinate Transforms (PCT) to\nfacilitate learning coordinate representations. Specifically, a localization\nboosting mechanism with confidence-aware loss is introduced to progressively\nrefine the localization prediction. In addition, semantic image repre-\nsentation is also exploited to compensate for the usage of patch proposals.\nDespite being lightweight and simple, our strategy leads to superior\nimprovements on the KITTI and Waymo Open Dataset monocular 3D detection\nbenchmarks. At the same time, our proposed PCT shows great generalization to\nmost coordinate- based 3D detection frameworks. The code is available at:\nhttps://github.com/ amazon-research/progressive-coordinate-transforms.",
          "link": "http://arxiv.org/abs/2108.05793",
          "publishedOn": "2021-08-13T01:56:55.216Z",
          "wordCount": 637,
          "title": "Progressive Coordinate Transforms for Monocular 3D Object Detection. (arXiv:2108.05793v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Tae Ha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosse_J/0/1/0/all/0/1\">Juergen Bosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmico_S/0/1/0/all/0/1\">Simone D&#x27;Amico</a>",
          "description": "This work presents the most recent advances of the Robotic Testbed for\nRendezvous and Optical Navigation (TRON) at Stanford University - the first\nrobotic testbed capable of validating machine learning algorithms for\nspaceborne optical navigation. The TRON facility consists of two 6\ndegrees-of-freedom KUKA robot arms and a set of Vicon motion track cameras to\nreconfigure an arbitrary relative pose between a camera and a target mockup\nmodel. The facility includes multiple Earth albedo light boxes and a sun lamp\nto recreate the high-fidelity spaceborne illumination conditions. After the\noverview of the facility, this work details the multi-source calibration\nprocedure which enables the estimation of the relative pose between the object\nand the camera with millimeter-level position and millidegree-level orientation\naccuracies. Finally, a comparative analysis of the synthetic and TRON simulated\nimageries is performed using a Convolutional Neural Network (CNN) pre-trained\non the synthetic images. The result shows a considerable gap in the CNN's\nperformance, suggesting the TRON simulated images can be used to validate the\nrobustness of any machine learning algorithms trained on more easily accessible\nsynthetic imagery from computer graphics.",
          "link": "http://arxiv.org/abs/2108.05529",
          "publishedOn": "2021-08-13T01:56:55.206Z",
          "wordCount": 650,
          "title": "Robotic Testbed for Rendezvous and Optical Navigation: Multi-Source Calibration and Machine Learning Use Cases. (arXiv:2108.05529v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eulig_E/0/1/0/all/0/1\">Elias Eulig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saranrittichai_P/0/1/0/all/0/1\">Piyapat Saranrittichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1\">Chaithanya Kumar Mummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_K/0/1/0/all/0/1\">Kilian Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beluch_W/0/1/0/all/0/1\">William Beluch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiahan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_V/0/1/0/all/0/1\">Volker Fischer</a>",
          "description": "Common deep neural networks (DNNs) for image classification have been shown\nto rely on shortcut opportunities (SO) in the form of predictive and\neasy-to-represent visual factors. This is known as shortcut learning and leads\nto impaired generalization. In this work, we show that common DNNs also suffer\nfrom shortcut learning when predicting only basic visual object factors of\nvariation (FoV) such as shape, color, or texture. We argue that besides\nshortcut opportunities, generalization opportunities (GO) are also an inherent\npart of real-world vision data and arise from partial independence between\npredicted classes and FoVs. We also argue that it is necessary for DNNs to\nexploit GO to overcome shortcut learning. Our core contribution is to introduce\nthe Diagnostic Vision Benchmark suite DiagViB-6, which includes datasets and\nmetrics to study a network's shortcut vulnerability and generalization\ncapability for six independent FoV. In particular, DiagViB-6 allows controlling\nthe type and degree of SO and GO in a dataset. We benchmark a wide range of\npopular vision architectures and show that they can exploit GO only to a\nlimited extent.",
          "link": "http://arxiv.org/abs/2108.05779",
          "publishedOn": "2021-08-13T01:56:55.193Z",
          "wordCount": 648,
          "title": "DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities. (arXiv:2108.05779v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Can Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksoud_S/0/1/0/all/0/1\">Sam Maksoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovell_B/0/1/0/all/0/1\">Brian C. Lovell</a>",
          "description": "To accommodate rapid changes in the real world, the cognition system of\nhumans is capable of continually learning concepts. On the contrary,\nconventional deep learning models lack this capability of preserving previously\nlearned knowledge. When a neural network is fine-tuned to learn new tasks, its\nperformance on previously trained tasks will significantly deteriorate. Many\nrecent works on incremental object detection tackle this problem by introducing\nadvanced regularization. Although these methods have shown promising results,\nthe benefits are often short-lived after the first incremental step. Under\nmulti-step incremental learning, the trade-off between old knowledge preserving\nand new task learning becomes progressively more severe. Thus, the performance\nof regularization-based incremental object detectors gradually decays for\nsubsequent learning steps. In this paper, we aim to alleviate this performance\ndecay on multi-step incremental detection tasks by proposing a dilatable\nincremental object detector (DIODE). For the task-shared parameters, our method\nadaptively penalizes the changes of important weights for previous tasks. At\nthe same time, the structure of the model is dilated or expanded by a limited\nnumber of task-specific parameters to promote new task learning. Extensive\nexperiments on PASCAL VOC and COCO datasets demonstrate substantial\nimprovements over the state-of-the-art methods. Notably, compared with the\nstate-of-the-art methods, our method achieves up to 6.0% performance\nimprovement by increasing the number of parameters by just 1.2% for each newly\nlearned task.",
          "link": "http://arxiv.org/abs/2108.05627",
          "publishedOn": "2021-08-13T01:56:55.171Z",
          "wordCount": 656,
          "title": "DIODE: Dilatable Incremental Object Detection. (arXiv:2108.05627v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Deep learning algorithms mine knowledge from the training data and thus would\nlikely inherit the dataset's bias information. As a result, the obtained model\nwould generalize poorly and even mislead the decision process in real-life\napplications. We propose to remove the bias information misused by the target\ntask with a cross-sample adversarial debiasing (CSAD) method. CSAD explicitly\nextracts target and bias features disentangled from the latent representation\ngenerated by a feature extractor and then learns to discover and remove the\ncorrelation between the target and bias features. The correlation measurement\nplays a critical role in adversarial debiasing and is conducted by a\ncross-sample neural mutual information estimator. Moreover, we propose joint\ncontent and local structural representation learning to boost mutual\ninformation estimation for better performance. We conduct thorough experiments\non publicly available datasets to validate the advantages of the proposed\nmethod over state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.05449",
          "publishedOn": "2021-08-13T01:56:55.164Z",
          "wordCount": 586,
          "title": "Learning Bias-Invariant Representation by Cross-Sample Mutual Information Minimization. (arXiv:2108.05449v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuke/0/1/0/all/0/1\">Yuke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1\">Yang</a>",
          "description": "Human action recognition is a well-known computer vision and pattern\nrecognition task of identifying which action a man is actually doing.\nExtracting the keypoint information of a single human with both spatial and\ntemporal features of action sequences plays an essential role to accomplish the\ntask.In this paper, we propose a human action system for Red-Green-Blue(RGB)\ninput video with our own designed module. Based on the efficient Gated\nRecurrent Unit(GRU) for spatio-temporal feature extraction, we add another\nsampling module and normalization module to improve the performance of the\nmodel in order to recognize the human actions. Furthermore, we build a novel\ndataset with a similar background and discriminative actions for both human\nkeypoint prediction and behavior recognition. To get a better result, we\nretrain the pose model with our new dataset to get better performance.\nExperimental results demonstrate the effectiveness of the proposed model on our\nown human behavior recognition dataset and some public datasets.",
          "link": "http://arxiv.org/abs/2108.05633",
          "publishedOn": "2021-08-13T01:56:55.160Z",
          "wordCount": 588,
          "title": "Spatio-Temporal Human Action Recognition Modelwith Flexible-interval Sampling and Normalization. (arXiv:2108.05633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gama_P/0/1/0/all/0/1\">Pedro H. T. Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "In this paper, we propose a novel approach for few-shot semantic segmentation\nwith sparse labeled images. We investigate the effectiveness of our method,\nwhich is based on the Model-Agnostic Meta-Learning (MAML) algorithm, in the\nmedical scenario, where the use of sparse labeling and few-shot can alleviate\nthe cost of producing new annotated datasets. Our method uses sparse labels in\nthe meta-training and dense labels in the meta-test, thus making the model\nlearn to predict dense labels from sparse ones. We conducted experiments with\nfour Chest X-Ray datasets to evaluate two types of annotations (grid and\npoints). The results show that our method is the most suitable when the target\ndomain highly differs from source domains, achieving Jaccard scores comparable\nto dense labels, using less than 2% of the pixels of an image with labels in\nfew-shot scenarios.",
          "link": "http://arxiv.org/abs/2108.05476",
          "publishedOn": "2021-08-13T01:56:55.154Z",
          "wordCount": 573,
          "title": "Weakly Supervised Medical Image Segmentation. (arXiv:2108.05476v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngsaeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">David K. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>",
          "description": "With the availability of many datasets tailored for autonomous driving in\nreal-world urban scenes, semantic segmentation for urban driving scenes\nachieves significant progress. However, semantic segmentation for off-road,\nunstructured environments is not widely studied. Directly applying existing\nsegmentation networks often results in performance degradation as they cannot\novercome intrinsic problems in such environments, such as illumination changes.\nIn this paper, a built-in memory module for semantic segmentation is proposed\nto overcome these problems. The memory module stores significant\nrepresentations of training images as memory items. In addition to the encoder\nembedding like items together, the proposed memory module is specifically\ndesigned to cluster together instances of the same class even when there are\nsignificant variances in embedded features. Therefore, it makes segmentation\nnetworks better deal with unexpected illumination changes. A triplet loss is\nused in training to minimize redundancy in storing discriminative\nrepresentations of the memory module. The proposed memory module is general so\nthat it can be adopted in a variety of networks. We conduct experiments on the\nRobot Unstructured Ground Driving (RUGD) dataset and RELLIS dataset, which are\ncollected from off-road, unstructured natural environments. Experimental\nresults show that the proposed memory module improves the performance of\nexisting segmentation networks and contributes to capturing unclear objects\nover various off-road, unstructured natural scenes with equivalent\ncomputational cost and network parameters. As the proposed method can be\nintegrated into compact networks, it presents a viable approach for\nresource-limited small autonomous platforms.",
          "link": "http://arxiv.org/abs/2108.05635",
          "publishedOn": "2021-08-13T01:56:55.148Z",
          "wordCount": 693,
          "title": "Memory-based Semantic Segmentation for Off-road Unstructured Natural Environments. (arXiv:2108.05635v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Pei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suo_X/0/1/0/all/0/1\">Xin Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "Generating ``bullet-time'' effects of human free-viewpoint videos is critical\nfor immersive visual effects and VR/AR experience. Recent neural advances still\nlack the controllable and interactive bullet-time design ability for human\nfree-viewpoint rendering, especially under the real-time, dynamic and general\nsetting for our trajectory-aware task. To fill this gap, in this paper we\npropose a neural interactive bullet-time generator (iButter) for\nphoto-realistic human free-viewpoint rendering from dense RGB streams, which\nenables flexible and interactive design for human bullet-time visual effects.\nOur iButter approach consists of a real-time preview and design stage as well\nas a trajectory-aware refinement stage. During preview, we propose an\ninteractive bullet-time design approach by extending the NeRF rendering to a\nreal-time and dynamic setting and getting rid of the tedious per-scene\ntraining. To this end, our bullet-time design stage utilizes a hybrid training\nset, light-weight network design and an efficient silhouette-based sampling\nstrategy. During refinement, we introduce an efficient trajectory-aware scheme\nwithin 20 minutes, which jointly encodes the spatial, temporal consistency and\nsemantic cues along the designed trajectory, achieving photo-realistic\nbullet-time viewing experience of human activities. Extensive experiments\ndemonstrate the effectiveness of our approach for convenient interactive\nbullet-time design and photo-realistic human free-viewpoint video generation.",
          "link": "http://arxiv.org/abs/2108.05577",
          "publishedOn": "2021-08-13T01:56:55.129Z",
          "wordCount": 654,
          "title": "iButter: Neural Interactive Bullet Time Generator for Human Free-viewpoint Rendering. (arXiv:2108.05577v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xuan_K/0/1/0/all/0/1\">Kai Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>",
          "description": "In clinical practice, magnetic resonance imaging (MRI) with multiple\ncontrasts is usually acquired in a single study to assess different properties\nof the same region of interest in human body. The whole acquisition process can\nbe accelerated by having one or more modalities under-sampled in the k-space.\nRecent researches demonstrate that, considering the redundancy between\ndifferent contrasts or modalities, a target MRI modality under-sampled in the\nk-space can be better reconstructed with the helps from a fully-sampled\nsequence (i.e., the reference modality). It implies that, in the same study of\nthe same subject, multiple sequences can be utilized together toward the\npurpose of highly efficient multi-modal reconstruction. However, we find that\nmulti-modal reconstruction can be negatively affected by subtle spatial\nmisalignment between different sequences, which is actually common in clinical\npractice. In this paper, we integrate the spatial alignment network with\nreconstruction, to improve the quality of the reconstructed target modality.\nSpecifically, the spatial alignment network estimates the spatial misalignment\nbetween the fully-sampled reference and the under-sampled target images, and\nwarps the reference image accordingly. Then, the aligned fully-sampled\nreference image joins the under-sampled target image in the reconstruction\nnetwork, to produce the high-quality target image. Considering the contrast\ndifference between the target and the reference, we particularly design the\ncross-modality-synthesis-based registration loss, in combination with the\nreconstruction loss, to jointly train the spatial alignment network and the\nreconstruction network. Our experiments on both clinical MRI and multi-coil\nk-space raw data demonstrate the superiority and robustness of our spatial\nalignment network. Code is publicly available at\nhttps://github.com/woxuankai/SpatialAlignmentNetwork.",
          "link": "http://arxiv.org/abs/2108.05603",
          "publishedOn": "2021-08-13T01:56:55.124Z",
          "wordCount": 699,
          "title": "Multi-Modal MRI Reconstruction with Spatial Alignment Network. (arXiv:2108.05603v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agastya_C/0/1/0/all/0/1\">Chitra Agastya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghebremusse_S/0/1/0/all/0/1\">Sirak Ghebremusse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_I/0/1/0/all/0/1\">Ian Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Colorado Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahabi_H/0/1/0/all/0/1\">Hossein Vahabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todeschini_A/0/1/0/all/0/1\">Alberto Todeschini</a>",
          "description": "Climate change has caused reductions in river runoffs and aquifer recharge\nresulting in an increasingly unsustainable crop water demand from reduced\nfreshwater availability. Achieving food security while deploying water in a\nsustainable manner will continue to be a major challenge necessitating careful\nmonitoring and tracking of agricultural water usage. Historically, monitoring\nwater usage has been a slow and expensive manual process with many\nimperfections and abuses. Ma-chine learning and remote sensing developments\nhave increased the ability to automatically monitor irrigation patterns, but\nexisting techniques often require curated and labelled irrigation data, which\nare expensive and time consuming to obtain and may not exist for impactful\nareas such as developing countries. In this paper, we explore an end-to-end\nreal world application of irrigation detection with uncurated and unlabeled\nsatellite imagery. We apply state-of-the-art self-supervised deep learning\ntechniques to optical remote sensing data, and find that we are able to detect\nirrigation with up to nine times better precision, 90% better recall and 40%\nmore generalization ability than the traditional supervised learning methods.",
          "link": "http://arxiv.org/abs/2108.05484",
          "publishedOn": "2021-08-13T01:56:55.118Z",
          "wordCount": 616,
          "title": "Self-supervised Contrastive Learning for Irrigation Detection in Satellite Imagery. (arXiv:2108.05484v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1\">Kyriaki-Margarita Bintsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1\">Vasileios Baltatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammers_A/0/1/0/all/0/1\">Alexander Hammers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "Brain aging, and more specifically the difference between the chronological\nand the biological age of a person, may be a promising biomarker for\nidentifying neurodegenerative diseases. For this purpose accurate prediction is\nimportant but the localisation of the areas that play a significant role in the\nprediction is also crucial, in order to gain clinicians' trust and reassurance\nabout the performance of a prediction model. Most interpretability methods are\nfocused on classification tasks and cannot be directly transferred to\nregression tasks. In this study, we focus on the task of brain age regression\nfrom 3D brain Magnetic Resonance (MR) images using a Convolutional Neural\nNetwork, termed prediction model. We interpret its predictions by extracting\nimportance maps, which discover the parts of the brain that are the most\nimportant for brain age. In order to do so, we assume that voxels that are not\nuseful for the regression are resilient to noise addition. We implement a noise\nmodel which aims to add as much noise as possible to the input without harming\nthe performance of the prediction model. We average the importance maps of the\nsubjects and end up with a population-based importance map, which displays the\nregions of the brain that are influential for the task. We test our method on\n13,750 3D brain MR images from the UK Biobank, and our findings are consistent\nwith the existing neuropathology literature, highlighting that the hippocampus\nand the ventricles are the most relevant regions for brain aging.",
          "link": "http://arxiv.org/abs/2108.05388",
          "publishedOn": "2021-08-13T01:56:55.113Z",
          "wordCount": 687,
          "title": "Voxel-level Importance Maps for Interpretable Brain Age Estimation. (arXiv:2108.05388v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wencan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Hyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jong Hwan Ko</a>",
          "description": "With increasing applications of 3D hand pose estimation in various\nhuman-computer interaction applications, convolution neural networks (CNNs)\nbased estimation models have been actively explored. However, the existing\nmodels require complex architectures or redundant computational resources to\ntrade with the acceptable accuracy. To tackle this limitation, this paper\nproposes HandFoldingNet, an accurate and efficient hand pose estimator that\nregresses the hand joint locations from the normalized 3D hand point cloud\ninput. The proposed model utilizes a folding-based decoder that folds a given\n2D hand skeleton into the corresponding joint coordinates. For higher\nestimation accuracy, folding is guided by multi-scale features, which include\nboth global and joint-wise local features. Experimental results show that the\nproposed model outperforms the existing methods on three hand pose benchmark\ndatasets with the lowest model parameter requirement. Code is available at\nhttps://github.com/cwc1260/HandFold.",
          "link": "http://arxiv.org/abs/2108.05545",
          "publishedOn": "2021-08-13T01:56:55.108Z",
          "wordCount": 603,
          "title": "HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton. (arXiv:2108.05545v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xudong Jiang</a>",
          "description": "In this work, we address the challenging task of referring segmentation. The\nquery expression in referring segmentation typically indicates the target\nobject by describing its relationship with others. Therefore, to find the\ntarget one among all instances in the image, the model must have a holistic\nunderstanding of the whole image. To achieve this, we reformulate referring\nsegmentation as a direct attention problem: finding the region in the image\nwhere the query language expression is most attended to. We introduce\ntransformer and multi-head attention to build a network with an encoder-decoder\nattention mechanism architecture that \"queries\" the given image with the\nlanguage expression. Furthermore, we propose a Query Generation Module, which\nproduces multiple sets of queries with different attention weights that\nrepresent the diversified comprehensions of the language expression from\ndifferent aspects. At the same time, to find the best way from these\ndiversified comprehensions based on visual clues, we further propose a Query\nBalance Module to adaptively select the output features of these queries for a\nbetter mask generation. Without bells and whistles, our approach is\nlight-weight and achieves new state-of-the-art performance consistently on\nthree referring segmentation datasets, RefCOCO, RefCOCO+, and G-Ref. Our code\nis available at https://github.com/henghuiding/Vision-Language-Transformer.",
          "link": "http://arxiv.org/abs/2108.05565",
          "publishedOn": "2021-08-13T01:56:55.103Z",
          "wordCount": 640,
          "title": "Vision-Language Transformer and Query Generation for Referring Segmentation. (arXiv:2108.05565v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1\">Vasileios Baltatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1\">Kyriaki-Margarita Bintsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1\">Loic Le Folgoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1\">Octavio E. Martinez Manzanera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1\">Sam Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Arjun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sujal Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>",
          "description": "Using publicly available data to determine the performance of methodological\ncontributions is important as it facilitates reproducibility and allows\nscrutiny of the published results. In lung nodule classification, for example,\nmany works report results on the publicly available LIDC dataset. In theory,\nthis should allow a direct comparison of the performance of proposed methods\nand assess the impact of individual contributions. When analyzing seven recent\nworks, however, we find that each employs a different data selection process,\nleading to largely varying total number of samples and ratios between benign\nand malignant cases. As each subset will have different characteristics with\nvarying difficulty for classification, a direct comparison between the proposed\nmethods is thus not always possible, nor fair. We study the particular effect\nof truthing when aggregating labels from multiple experts. We show that\nspecific choices can have severe impact on the data distribution where it may\nbe possible to achieve superior performance on one sample distribution but not\non another. While we show that we can further improve on the state-of-the-art\non one sample selection, we also find that on a more challenging sample\nselection, on the same database, the more advanced models underperform with\nrespect to very simple baseline methods, highlighting that the selected data\ndistribution may play an even more important role than the model architecture.\nThis raises concerns about the validity of claimed methodological\ncontributions. We believe the community should be aware of these pitfalls and\nmake recommendations on how these can be avoided in future work.",
          "link": "http://arxiv.org/abs/2108.05386",
          "publishedOn": "2021-08-13T01:56:55.073Z",
          "wordCount": 715,
          "title": "The Pitfalls of Sample Selection: A Case Study on Lung Nodule Classification. (arXiv:2108.05386v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junkai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weikai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zhenhua Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengxu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>",
          "description": "Open-set semi-supervised learning (open-set SSL) investigates a challenging\nbut practical scenario where out-of-distribution (OOD) samples are contained in\nthe unlabeled data. While the mainstream technique seeks to completely filter\nout the OOD samples for semi-supervised learning (SSL), we propose a novel\ntraining mechanism that could effectively exploit the presence of OOD data for\nenhanced feature learning while avoiding its adverse impact on the SSL. We\nachieve this goal by first introducing a warm-up training that leverages all\nthe unlabeled data, including both the in-distribution (ID) and OOD samples.\nSpecifically, we perform a pretext task that enforces our feature extractor to\nobtain a high-level semantic understanding of the training images, leading to\nmore discriminative features that can benefit the downstream tasks. Since the\nOOD samples are inevitably detrimental to SSL, we propose a novel cross-modal\nmatching strategy to detect OOD samples. Instead of directly applying binary\nclassification, we train the network to predict whether the data sample is\nmatched to an assigned one-hot class label. The appeal of the proposed\ncross-modal matching over binary classification is the ability to generate a\ncompatible feature space that aligns with the core classification task.\nExtensive experiments show that our approach substantially lifts the\nperformance on open-set SSL and outperforms the state-of-the-art by a large\nmargin.",
          "link": "http://arxiv.org/abs/2108.05617",
          "publishedOn": "2021-08-13T01:56:55.067Z",
          "wordCount": 669,
          "title": "Trash to Treasure: Harvesting OOD Data with Cross-Modal Matching for Open-Set Semi-Supervised Learning. (arXiv:2108.05617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Minho Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>",
          "description": "Ground segmentation is crucial for terrestrial mobile platforms to perform\nnavigation or neighboring object recognition. Unfortunately, the ground is not\nflat, as it features steep slopes; bumpy roads; or objects, such as curbs,\nflower beds, and so forth. To tackle the problem, this paper presents a novel\nground segmentation method called \\textit{Patchwork}, which is robust for\naddressing the under-segmentation problem and operates at more than 40 Hz. In\nthis paper, a point cloud is encoded into a Concentric Zone Model-based\nrepresentation to assign an appropriate density of cloud points among bins in a\nway that is not computationally complex. This is followed by Region-wise Ground\nPlane Fitting, which is performed to estimate the partial ground for each bin.\nFinally, Ground Likelihood Estimation is introduced to dramatically reduce\nfalse positives. As experimentally verified on SemanticKITTI and rough terrain\ndatasets, our proposed method yields promising performance compared with the\nstate-of-the-art methods, showing faster speed compared with existing plane\nfitting--based methods. Code is available:\nhttps://github.com/LimHyungTae/patchwork",
          "link": "http://arxiv.org/abs/2108.05560",
          "publishedOn": "2021-08-13T01:56:55.057Z",
          "wordCount": 610,
          "title": "Patchwork: Concentric Zone-based Region-wise Ground Segmentation with Ground Likelihood Estimation Using a 3D LiDAR Sensor. (arXiv:2108.05560v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aixi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yue Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Miao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>",
          "description": "Two-stage methods have dominated Human-Object Interaction (HOI) detection for\nseveral years. Recently, one-stage HOI detection methods have become popular.\nIn this paper, we aim to explore the essential pros and cons of two-stage and\none-stage methods. With this as the goal, we find that conventional two-stage\nmethods mainly suffer from positioning positive interactive human-object pairs,\nwhile one-stage methods are challenging to make an appropriate trade-off on\nmulti-task learning, i.e., object detection, and interaction classification.\nTherefore, a core problem is how to take the essence and discard the dregs from\nthe conventional two types of methods. To this end, we propose a novel\none-stage framework with disentangling human-object detection and interaction\nclassification in a cascade manner. In detail, we first design a human-object\npair generator based on a state-of-the-art one-stage HOI detector by removing\nthe interaction classification module or head and then design a relatively\nisolated interaction classifier to classify each human-object pair. Two cascade\ndecoders in our proposed framework can focus on one specific task, detection or\ninteraction classification. In terms of the specific implementation, we adopt a\ntransformer-based HOI detector as our base model. The newly introduced\ndisentangling paradigm outperforms existing methods by a large margin, with a\nsignificant relative mAP gain of 9.32% on HICO-Det.",
          "link": "http://arxiv.org/abs/2108.05077",
          "publishedOn": "2021-08-12T01:56:24.355Z",
          "wordCount": 654,
          "title": "Mining the Benefits of Two-stage and One-stage HOI Detection. (arXiv:2108.05077v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Swope_A/0/1/0/all/0/1\">Aidan M. Swope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudelis_X/0/1/0/all/0/1\">Xander H. Rudelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Story_K/0/1/0/all/0/1\">Kyle T. Story</a>",
          "description": "In the application of machine learning to remote sensing, labeled data is\noften scarce or expensive, which impedes the training of powerful models like\ndeep convolutional neural networks. Although unlabeled data is abundant, recent\nself-supervised learning approaches are ill-suited to the remote sensing\ndomain. In addition, most remote sensing applications currently use only a\nsmall subset of the multi-sensor, multi-channel information available,\nmotivating the need for fused multi-sensor representations. We propose a new\nself-supervised training objective, Contrastive Sensor Fusion, which exploits\ncoterminous data from multiple sources to learn useful representations of every\npossible combination of those sources. This method uses information common\nacross multiple sensors and bands by training a single model to produce a\nrepresentation that remains similar when any subset of its input channels is\nused. Using a dataset of 47 million unlabeled coterminous image triplets, we\ntrain an encoder to produce semantically meaningful representations from any\npossible combination of channels from the input sensors. These representations\noutperform fully supervised ImageNet weights on a remote sensing classification\ntask and improve as more sensors are fused. Our code is available at\nhttps://storage.cloud.google.com/public-published-datasets/csf_code.zip.",
          "link": "http://arxiv.org/abs/2108.05094",
          "publishedOn": "2021-08-12T01:56:24.011Z",
          "wordCount": 653,
          "title": "Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach. (arXiv:2108.05094v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Charig Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamdouar_H/0/1/0/all/0/1\">Hala Lamdouar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_E/0/1/0/all/0/1\">Erika Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>",
          "description": "Animals have evolved highly functional visual systems to understand motion,\nassisting perception even under complex environments. In this paper, we work\ntowards developing a computer vision system able to segment objects by\nexploiting motion cues, i.e. motion segmentation. We make the following\ncontributions: First, we introduce a simple variant of the Transformer to\nsegment optical flow frames into primary objects and the background. Second, we\ntrain the architecture in a self-supervised manner, i.e. without using any\nmanual annotations. Third, we analyze several critical components of our method\nand conduct thorough ablation studies to validate their necessity. Fourth, we\nevaluate the proposed architecture on public benchmarks (DAVIS2016, SegTrackv2,\nand FBMS59). Despite using only optical flow as input, our approach achieves\nsuperior or comparable results to previous state-of-the-art self-supervised\nmethods, while being an order of magnitude faster. We additionally evaluate on\na challenging camouflage dataset (MoCA), significantly outperforming the other\nself-supervised approaches, and comparing favourably to the top supervised\napproach, highlighting the importance of motion cues, and the potential bias\ntowards visual appearance in existing video segmentation models.",
          "link": "http://arxiv.org/abs/2104.07658",
          "publishedOn": "2021-08-12T01:56:23.479Z",
          "wordCount": 653,
          "title": "Self-supervised Video Object Segmentation by Motion Grouping. (arXiv:2104.07658v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kyathanahally_S/0/1/0/all/0/1\">S. P. Kyathanahally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardeman_T/0/1/0/all/0/1\">T. Hardeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merz_E/0/1/0/all/0/1\">E. Merz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozakiewicz_T/0/1/0/all/0/1\">T. Kozakiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1\">M. Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isles_P/0/1/0/all/0/1\">P. Isles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomati_F/0/1/0/all/0/1\">F. Pomati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baity_Jesi_M/0/1/0/all/0/1\">M. Baity-Jesi</a>",
          "description": "Plankton are effective indicators of environmental change and ecosystem\nhealth in freshwater habitats, but collection of plankton data using manual\nmicroscopic methods is extremely labor-intensive and expensive. Automated\nplankton imaging offers a promising way forward to monitor plankton communities\nwith high frequency and accuracy in real-time. Yet, manual annotation of\nmillions of images proposes a serious challenge to taxonomists. Deep learning\nclassifiers have been successfully applied in various fields and provided\nencouraging results when used to categorize marine plankton images. Here, we\npresent a set of deep learning models developed for the identification of lake\nplankton, and study several strategies to obtain optimal performances,which\nlead to operational prescriptions for users. To this aim, we annotated into 35\nclasses over 17900 images of zooplankton and large phytoplankton colonies,\ndetected in Lake Greifensee (Switzerland) with the Dual Scripps Plankton\nCamera. Our best models were based on transfer learning and ensembling, which\nclassified plankton images with 98% accuracy and 93% F1 score. When tested on\nfreely available plankton datasets produced by other automated imaging tools\n(ZooScan, FlowCytobot and ISIIS), our models performed better than previously\nused models. Our annotated data, code and classification models are freely\navailable online.",
          "link": "http://arxiv.org/abs/2108.05258",
          "publishedOn": "2021-08-12T01:56:23.427Z",
          "wordCount": 656,
          "title": "Deep Learning Classification of Lake Zooplankton. (arXiv:2108.05258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrami_E/0/1/0/all/0/1\">Emad Bahrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diba_A/0/1/0/all/0/1\">Ali Diba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noroozi_M/0/1/0/all/0/1\">Mehdi Noroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>",
          "description": "While state-of-the-art 3D Convolutional Neural Networks (CNN) achieve very\ngood results on action recognition datasets, they are computationally very\nexpensive and require many GFLOPs. While the GFLOPs of a 3D CNN can be\ndecreased by reducing the temporal feature resolution within the network, there\nis no setting that is optimal for all input clips. In this work, we therefore\nintroduce a differentiable Similarity Guided Sampling (SGS) module, which can\nbe plugged into any existing 3D CNN architecture. SGS empowers 3D CNNs by\nlearning the similarity of temporal features and grouping similar features\ntogether. As a result, the temporal feature resolution is not anymore static\nbut it varies for each input video clip. By integrating SGS as an additional\nlayer within current 3D CNNs, we can convert them into much more efficient 3D\nCNNs with adaptive temporal feature resolutions (ATFR). Our evaluations show\nthat the proposed module improves the state-of-the-art by reducing the\ncomputational cost (GFLOPs) by half while preserving or even improving the\naccuracy. We evaluate our module by adding it to multiple state-of-the-art 3D\nCNNs on various datasets such as Kinetics-600, Kinetics-400, mini-Kinetics,\nSomething-Something V2, UCF101, and HMDB51.",
          "link": "http://arxiv.org/abs/2011.08652",
          "publishedOn": "2021-08-12T01:56:23.371Z",
          "wordCount": 683,
          "title": "3D CNNs with Adaptive Temporal Feature Resolutions. (arXiv:2011.08652v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects using\npoint cloud data. We present anchor design along with custom loss functions\nused in this work. A combination of spatial and channel wise attention module\nis used in this work. We use the Kitti 3D Birds Eye View dataset for\nbenchmarking and validating our results. Our method surpasses previous state of\nthe art in this domain both in terms of average precision and speed running at\n> 30 FPS. Finally, we present the ablation study to demonstrate that the\nperformance of our network is generalizable. This makes it a feasible option to\nbe deployed in real time applications like self driving cars.",
          "link": "http://arxiv.org/abs/2107.12137",
          "publishedOn": "2021-08-12T01:56:23.362Z",
          "wordCount": 633,
          "title": "AA3DNet: Attention Augmented Real Time 3D Object Detection. (arXiv:2107.12137v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifei Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fufu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Pai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>",
          "description": "Text-based image retrieval has seen considerable progress in recent years.\nHowever, the performance of existing methods suffers in real life since the\nuser is likely to provide an incomplete description of an image, which often\nleads to results filled with false positives that fit the incomplete\ndescription. In this work, we introduce the partial-query problem and\nextensively analyze its influence on text-based image retrieval. Previous\ninteractive methods tackle the problem by passively receiving users' feedback\nto supplement the incomplete query iteratively, which is time-consuming and\nrequires heavy user effort. Instead, we propose a novel retrieval framework\nthat conducts the interactive process in an Ask-and-Confirm fashion, where AI\nactively searches for discriminative details missing in the current query, and\nusers only need to confirm AI's proposal. Specifically, we propose an\nobject-based interaction to make the interactive retrieval more user-friendly\nand present a reinforcement-learning-based policy to search for discriminative\nobjects. Furthermore, since fully-supervised training is often infeasible due\nto the difficulty of obtaining human-machine dialog data, we present a\nweakly-supervised training strategy that needs no human-annotated dialogs other\nthan a text-image dataset. Experiments show that our framework significantly\nimproves the performance of text-based image retrieval. Code is avaiable at\nhttps://github.com/CuthbertCai/Ask-Confirm.",
          "link": "http://arxiv.org/abs/2103.01654",
          "publishedOn": "2021-08-12T01:56:23.330Z",
          "wordCount": 687,
          "title": "Ask&Confirm: Active Detail Enriching for Cross-Modal Retrieval with Partial Query. (arXiv:2103.01654v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Changhong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "Successful continual learning of new knowledge would enable intelligent\nsystems to recognize more and more classes of objects. However, current\nintelligent systems often fail to correctly recognize previously learned\nclasses of objects when updated to learn new classes. It is widely believed\nthat such downgraded performance is solely due to the catastrophic forgetting\nof previously learned knowledge. In this study, we argue that the class\nconfusion phenomena may also play a role in downgrading the classification\nperformance during continual learning, i.e., the high similarity between new\nclasses and any previously learned classes would also cause the classifier to\nmake mistakes in recognizing these old classes, even if the knowledge of these\nold classes is not forgotten. To alleviate the class confusion issue, we\npropose a discriminative distillation strategy to help the classify well learn\nthe discriminative features between confusing classes during continual\nlearning. Experiments on multiple natural image classification tasks support\nthat the proposed distillation strategy, when combined with existing methods,\nis effective in further improving continual learning.",
          "link": "http://arxiv.org/abs/2108.05187",
          "publishedOn": "2021-08-12T01:56:23.306Z",
          "wordCount": 606,
          "title": "Discriminative Distillation to Reduce Class Confusion in Continual Learning. (arXiv:2108.05187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>",
          "description": "The class activation mapping, or CAM, has been the cornerstone of feature\nattribution methods for multiple vision tasks. Its simplicity and effectiveness\nhave led to wide applications in the explanation of visual predictions and\nweakly-supervised localization tasks. However, CAM has its own shortcomings.\nThe computation of attribution maps relies on ad-hoc calibration steps that are\nnot part of the training computational graph, making it difficult for us to\nunderstand the real meaning of the attribution values. In this paper, we\nimprove CAM by explicitly incorporating a latent variable encoding the location\nof the cue for recognition in the formulation, thereby subsuming the\nattribution map into the training computational graph. The resulting model,\nclass activation latent mapping, or CALM, is trained with the\nexpectation-maximization algorithm. Our experiments show that CALM identifies\ndiscriminative attributes for image classifiers more accurately than CAM and\nother visual attribution baselines. CALM also shows performance improvements\nover prior arts on the weakly-supervised object localization benchmarks. Our\ncode is available at\nhttps://github.com/naver-ai/calm}{https://github.com/naver-ai/calm.",
          "link": "http://arxiv.org/abs/2106.07861",
          "publishedOn": "2021-08-12T01:56:22.934Z",
          "wordCount": 643,
          "title": "Keep CALM and Improve Visual Feature Attribution. (arXiv:2106.07861v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Although using convolutional neural networks (CNNs) as backbones achieves\ngreat successes in computer vision, this work investigates a simple backbone\nnetwork useful for many dense prediction tasks without convolutions. Unlike the\nrecently-proposed Transformer model (e.g., ViT) that is specially designed for\nimage classification, we propose Pyramid Vision Transformer~(PVT), which\novercomes the difficulties of porting Transformer to various dense prediction\ntasks. PVT has several merits compared to prior arts. (1) Different from ViT\nthat typically has low-resolution outputs and high computational and memory\ncost, PVT can be not only trained on dense partitions of the image to achieve\nhigh output resolution, which is important for dense predictions but also using\na progressive shrinking pyramid to reduce computations of large feature maps.\n(2) PVT inherits the advantages from both CNN and Transformer, making it a\nunified backbone in various vision tasks without convolutions by simply\nreplacing CNN backbones. (3) We validate PVT by conducting extensive\nexperiments, showing that it boosts the performance of many downstream tasks,\ne.g., object detection, semantic, and instance segmentation. For example, with\na comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO\ndataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT\ncould serve as an alternative and useful backbone for pixel-level predictions\nand facilitate future researches. Code is available at\nhttps://github.com/whai362/PVT.",
          "link": "http://arxiv.org/abs/2102.12122",
          "publishedOn": "2021-08-12T01:56:22.927Z",
          "wordCount": 707,
          "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. (arXiv:2102.12122v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1\">Li Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Junxiang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_K/0/1/0/all/0/1\">Ke Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Generative Adversarial Networks (GAN) have many potential medical imaging\napplications, including data augmentation, domain adaptation, and model\nexplanation. Due to the limited memory of Graphical Processing Units (GPUs),\nmost current 3D GAN models are trained on low-resolution medical images, these\nmodels either cannot scale to high-resolution or are prone to patchy artifacts.\nIn this work, we propose a novel end-to-end GAN architecture that can generate\nhigh-resolution 3D images. We achieve this goal by separating training and\ninference. During training, we adopt a hierarchical structure that\nsimultaneously generates a low-resolution version of the image and a randomly\nselected sub-volume of the high-resolution image. The hierarchical design has\ntwo advantages: First, the memory demand for training on high-resolution images\nis amortized among subvolumes. Furthermore, anchoring the high-resolution\nsubvolumes to a single low-resolution image ensures anatomical consistency\nbetween subvolumes. During inference, our model can directly generate full\nhigh-resolution images. We also incorporate an encoder with a similar\nhierarchical structure into the model to extract features from the images.\nExperiments on 3D thorax CT and brain MRI demonstrate that our approach\noutperforms state of the art in image generation. We also demonstrate clinical\napplications of the proposed model in data augmentation, image super-resolution\nand clinical-relevant feature extraction.",
          "link": "http://arxiv.org/abs/2008.01910",
          "publishedOn": "2021-08-12T01:56:22.909Z",
          "wordCount": 683,
          "title": "Hierarchical Amortized Training for Memory-efficient High Resolution 3D GAN. (arXiv:2008.01910v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saralajew_S/0/1/0/all/0/1\">Sascha Saralajew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohnemus_L/0/1/0/all/0/1\">Lars Ohnemus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewecker_L/0/1/0/all/0/1\">Lukas Ewecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asan_E/0/1/0/all/0/1\">Ebubekir Asan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isele_S/0/1/0/all/0/1\">Simon Isele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roos_S/0/1/0/all/0/1\">Stefan Roos</a>",
          "description": "In current object detection, algorithms require the object to be directly\nvisible in order to be detected. As humans, however, we intuitively use visual\ncues caused by the respective object to already make assumptions about its\nappearance. In the context of driving, such cues can be shadows during the day\nand often light reflections at night. In this paper, we study the problem of\nhow to map this intuitive human behavior to computer vision algorithms to\ndetect oncoming vehicles at night just from the light reflections they cause by\ntheir headlights. For that, we present an extensive open-source dataset\ncontaining 59746 annotated grayscale images out of 346 different scenes in a\nrural environment at night. In these images, all oncoming vehicles, their\ncorresponding light objects (e.g., headlamps), and their respective light\nreflections (e.g., light reflections on guardrails) are labeled. In this\ncontext, we discuss the characteristics of the dataset and the challenges in\nobjectively describing visual cues such as light reflections. We provide\ndifferent metrics for different ways to approach the task and report the\nresults we achieved using state-of-the-art and custom object detection models\nas a first benchmark. With that, we want to bring attention to a new and so far\nneglected field in computer vision research, encourage more researchers to\ntackle the problem, and thereby further close the gap between human performance\nand computer vision systems.",
          "link": "http://arxiv.org/abs/2105.13236",
          "publishedOn": "2021-08-12T01:56:22.871Z",
          "wordCount": 717,
          "title": "A Dataset for Provident Vehicle Detection at Night. (arXiv:2105.13236v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giudice_O/0/1/0/all/0/1\">Oliver Giudice</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Guarnera_L/0/1/0/all/0/1\">Luca Guarnera</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1\">Sebastiano Battiato</a> (1 and 2) ((1) University of Catania, (2) iCTLab s.r.l. - Spin-off of University of Catania)",
          "description": "To properly contrast the Deepfake phenomenon the need to design new Deepfake\ndetection algorithms arises; the misuse of this formidable A.I. technology\nbrings serious consequences in the private life of every involved person.\nState-of-the-art proliferates with solutions using deep neural networks to\ndetect a fake multimedia content but unfortunately these algorithms appear to\nbe neither generalizable nor explainable. However, traces left by Generative\nAdversarial Network (GAN) engines during the creation of the Deepfakes can be\ndetected by analyzing ad-hoc frequencies. For this reason, in this paper we\npropose a new pipeline able to detect the so-called GAN Specific Frequencies\n(GSF) representing a unique fingerprint of the different generative\narchitectures. By employing Discrete Cosine Transform (DCT), anomalous\nfrequencies were detected. The \\BETA statistics inferred by the AC coefficients\ndistribution have been the key to recognize GAN-engine generated data.\nRobustness tests were also carried out in order to demonstrate the\neffectiveness of the technique using different attacks on images such as JPEG\nCompression, mirroring, rotation, scaling, addition of random sized rectangles.\nExperiments demonstrated that the method is innovative, exceeds the state of\nthe art and also give many insights in terms of explainability.",
          "link": "http://arxiv.org/abs/2101.09781",
          "publishedOn": "2021-08-12T01:56:22.848Z",
          "wordCount": 698,
          "title": "Fighting deepfakes by detecting GAN DCT anomalies. (arXiv:2101.09781v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1\">Tianpei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jin-An Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose an attention pyramid method for person\nre-identification. Unlike conventional attention-based methods which only learn\na global attention map, our attention pyramid exploits the attention regions in\na multi-scale manner because human attention varies with different scales. Our\nattention pyramid imitates the process of human visual perception which tends\nto notice the foreground person over the cluttered background, and further\nfocus on the specific color of the shirt with close observation. Specifically,\nwe describe our attention pyramid by a \"split-attend-merge-stack\" principle. We\nfirst split the features into multiple local parts and learn the corresponding\nattentions. Then, we merge local attentions and stack these merged attentions\nwith the residual connection as an attention pyramid. The proposed attention\npyramid is a lightweight plug-and-play module that can be applied to\noff-the-shelf models. We implement our attention pyramid method in two\ndifferent attention mechanisms including channel-wise attention and spatial\nattention. We evaluate our method on four largescale person re-identification\nbenchmarks including Market-1501, DukeMTMC, CUHK03, and MSMT17. Experimental\nresults demonstrate the superiority of our method, which outperforms the\nstate-of-the-art methods by a large margin with limited computational cost.",
          "link": "http://arxiv.org/abs/2108.05340",
          "publishedOn": "2021-08-12T01:56:22.841Z",
          "wordCount": 637,
          "title": "Person Re-identification via Attention Pyramid. (arXiv:2108.05340v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.02692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1\">Mustafa Mukadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1\">Shubham Tulsiani</a>",
          "description": "One of the fundamental goals of visual perception is to allow agents to\nmeaningfully interact with their environment. In this paper, we take a step\ntowards that long-term goal -- we extract highly localized actionable\ninformation related to elementary actions such as pushing or pulling for\narticulated objects with movable parts. For example, given a drawer, our\nnetwork predicts that applying a pulling force on the handle opens the drawer.\nWe propose, discuss, and evaluate novel network architectures that given image\nand depth data, predict the set of actions possible at each pixel, and the\nregions over articulated parts that are likely to move under the force. We\npropose a learning-from-interaction framework with an online data sampling\nstrategy that allows us to train the network in simulation (SAPIEN) and\ngeneralizes across categories. Check the website for code and data release:\nhttps://cs.stanford.edu/~kaichun/where2act/",
          "link": "http://arxiv.org/abs/2101.02692",
          "publishedOn": "2021-08-12T01:56:22.836Z",
          "wordCount": 622,
          "title": "Where2Act: From Pixels to Actions for Articulated 3D Objects. (arXiv:2101.02692v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zunzhi You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Hsuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>",
          "description": "Deep networks for Monocular Depth Estimation (MDE) have achieved promising\nperformance recently and it is of great importance to further understand the\ninterpretability of these networks. Existing methods attempt to provide posthoc\nexplanations by investigating visual cues, which may not explore the internal\nrepresentations learned by deep networks. In this paper, we find that some\nhidden units of the network are selective to certain ranges of depth, and thus\nsuch behavior can be served as a way to interpret the internal representations.\nBased on our observations, we quantify the interpretability of a deep MDE\nnetwork by the depth selectivity of its hidden units. Moreover, we then propose\na method to train interpretable MDE deep networks without changing their\noriginal architectures, by assigning a depth range for each unit to select.\nExperimental results demonstrate that our method is able to enhance the\ninterpretability of deep MDE networks by largely improving the depth\nselectivity of their units, while not harming or even improving the depth\nestimation accuracy. We further provide a comprehensive analysis to show the\nreliability of selective units, the applicability of our method on different\nlayers, models, and datasets, and a demonstration on analysis of model error.\nSource code and models are available at\nhttps://github.com/youzunzhi/InterpretableMDE .",
          "link": "http://arxiv.org/abs/2108.05312",
          "publishedOn": "2021-08-12T01:56:22.814Z",
          "wordCount": 648,
          "title": "Towards Interpretable Deep Networks for Monocular Depth Estimation. (arXiv:2108.05312v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moskalev_A/0/1/0/all/0/1\">Artem Moskalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>",
          "description": "Tracking multiple objects individually differs from tracking groups of\nrelated objects. When an object is a part of the group, its trajectory depends\non the trajectories of the other group members. Most of the current\nstate-of-the-art trackers follow the approach of tracking each object\nindependently, with the mechanism to handle the overlapping trajectories where\nnecessary. Such an approach does not take inter-object relations into account,\nwhich may cause unreliable tracking for the members of the groups, especially\nin crowded scenarios, where individual cues become unreliable due to\nocclusions. To overcome these limitations and to extend such trackers to\ncrowded scenes, we propose a plug-in Relation Encoding Module (REM). REM\nencodes relations between tracked objects by running a message passing over a\ncorresponding spatio-temporal graph, computing relation embeddings for the\ntracked objects. Our experiments on MOT17 and MOT20 demonstrate that the\nbaseline tracker improves its results after a simple extension with REM. The\nproposed module allows for tracking severely or even fully occluded objects by\nutilizing relational cues.",
          "link": "http://arxiv.org/abs/2108.05331",
          "publishedOn": "2021-08-12T01:56:22.802Z",
          "wordCount": 601,
          "title": "Two is a crowd: tracking relations in videos. (arXiv:2108.05331v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungsoo Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Abstract reasoning, i.e., inferring complicated patterns from given\nobservations, is a central building block of artificial general intelligence.\nWhile humans find the answer by either eliminating wrong candidates or first\nconstructing the answer, prior deep neural network (DNN)-based methods focus on\nthe former discriminative approach. This paper aims to design a framework for\nthe latter approach and bridge the gap between artificial and human\nintelligence. To this end, we propose logic-guided generation (LoGe), a novel\ngenerative DNN framework that reduces abstract reasoning as an optimization\nproblem in propositional logic. LoGe is composed of three steps: extract\npropositional variables from images, reason the answer variables with a logic\nlayer, and reconstruct the answer image from the variables. We demonstrate that\nLoGe outperforms the black box DNN frameworks for generative abstract reasoning\nunder the RAVEN benchmark, i.e., reconstructing answers based on capturing\ncorrect rules of various attributes from observations.",
          "link": "http://arxiv.org/abs/2107.10493",
          "publishedOn": "2021-08-12T01:56:22.789Z",
          "wordCount": 629,
          "title": "Abstract Reasoning via Logic-guided Generation. (arXiv:2107.10493v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.13126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1\">Mohammad Rasool Izadi</a>",
          "description": "We introduce a deep convolutional neural networks (CNN) architecture to\nclassify facial attributes and recognize face images simultaneously via a\nshared learning paradigm to improve the accuracy for facial attribute\nprediction and face recognition performance. In this method, we use facial\nattributes as an auxiliary source of information to assist CNN features\nextracted from the face images to improve the face recognition performance.\nSpecifically, we use a shared CNN architecture that jointly predicts facial\nattributes and recognize face images simultaneously via a shared learning\nparameters, and then we use facial attribute features an an auxiliary source of\ninformation concatenated by face features to increase the discrimination of the\nCNN for face recognition. This process assists the CNN classifier to better\nrecognize face images. The experimental results show that our model increases\nboth the face recognition and facial attribute prediction performance,\nespecially for the identity attributes such as gender and race. We evaluated\nour method on several standard datasets labeled by identities and face\nattributes and the results show that the proposed method outperforms\nstate-of-the-art face recognition models.",
          "link": "http://arxiv.org/abs/1909.13126",
          "publishedOn": "2021-08-12T01:56:22.782Z",
          "wordCount": 634,
          "title": "Feature Level Fusion from Facial Attributes for Face Recognition. (arXiv:1909.13126v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>",
          "description": "The aim of re-identification is to match objects in surveillance cameras with\ndifferent viewpoints. Although ReID is developing at a considerably rapid pace,\nthere is currently no processing method for the ReID task in multiple\nscenarios. However, such processing method is required in real life scenarios,\nsuch as those involving security. In the present study, a new ReID scenario was\nexplored, which differs in terms of perspective, background, and pose(walking\nor cycling). Obviously, ordinary ReID processing methods cannot effectively\nhandle such a scenario, with the introduction of image datasets being the\noptimal solution, in addition to being considerably expensive.\n\nTo solve the aforementioned problem, a simple and effective method to\ngenerate images in several new scenarios was proposed, which is names the Copy\nand Paste method based on Pose(CPP). The CPP method is based on key point\ndetection, using copy as paste, to composite a new semantic image dataset in\ntwo different semantic image datasets. As an example, pedestrains and bicycles\ncan be used to generate several images that show the same person riding on\ndifferent bicycles. The CPP method is suitable for ReID tasks in new scenarios\nand outperforms the traditional methods when applied to the original datasets\nin original ReID tasks. To be specific, the CPP method can also perform better\nin terms of generalization for third-party public dataset. The Code and\ndatasets composited by the CPP method will be available in the future.",
          "link": "http://arxiv.org/abs/2107.10479",
          "publishedOn": "2021-08-12T01:56:22.761Z",
          "wordCount": 707,
          "title": "Copy and Paste method based on Pose for Re-identification. (arXiv:2107.10479v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sohail A. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>",
          "description": "Face forgery by deepfake is widely spread over the internet and this raises\nsevere societal concerns. In this paper, we propose a novel video transformer\nwith incremental learning for detecting deepfake videos. To better align the\ninput face images, we use a 3D face reconstruction method to generate UV\ntexture from a single input face image. The aligned face image can also provide\npose, eyes blink and mouth movement information that cannot be perceived in the\nUV texture image, so we use both face images and their UV texture maps to\nextract the image features. We present an incremental learning strategy to\nfine-tune the proposed model on a smaller amount of data and achieve better\ndeepfake detection performance. The comprehensive experiments on various public\ndeepfake datasets demonstrate that the proposed video transformer model with\nincremental learning achieves state-of-the-art performance in the deepfake\nvideo detection task with enhanced feature learning from the sequenced data.",
          "link": "http://arxiv.org/abs/2108.05307",
          "publishedOn": "2021-08-12T01:56:22.755Z",
          "wordCount": 602,
          "title": "Video Transformer for Deepfake Detection with Incremental Learning. (arXiv:2108.05307v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Attention mechanism of late has been quite popular in the computer vision\ncommunity. A lot of work has been done to improve the performance of the\nnetwork, although almost always it results in increased computational\ncomplexity. In this paper, we propose a new attention module that not only\nachieves the best performance but also has lesser parameters compared to most\nexisting models. Our attention module can easily be integrated with other\nconvolutional neural networks because of its lightweight nature. The proposed\nnetwork named Dual Multi Scale Attention Network (DMSANet) is comprised of two\nparts: the first part is used to extract features at various scales and\naggregate them, the second part uses spatial and channel attention modules in\nparallel to adaptively integrate local features with their global dependencies.\nWe benchmark our network performance for Image Classification on ImageNet\ndataset, Object Detection and Instance Segmentation both on MS COCO dataset.",
          "link": "http://arxiv.org/abs/2106.08382",
          "publishedOn": "2021-08-12T01:56:22.750Z",
          "wordCount": 613,
          "title": "DMSANet: Dual Multi Scale Attention Network. (arXiv:2106.08382v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.04868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Joya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shiwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yifei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>",
          "description": "To train accurate deep object detectors under the extreme\nforeground-background imbalance, heuristic sampling methods are always\nnecessary, which either re-sample a subset of all training samples (hard\nsampling methods, \\eg biased sampling, OHEM), or use all training samples but\nre-weight them discriminatively (soft sampling methods, \\eg Focal Loss, GHM).\nIn this paper, we challenge the necessity of such hard/soft sampling methods\nfor training accurate deep object detectors. While previous studies have shown\nthat training detectors without heuristic sampling methods would significantly\ndegrade accuracy, we reveal that this degradation comes from an unreasonable\nclassification gradient magnitude caused by the imbalance, rather than a lack\nof re-sampling/re-weighting. Motivated by our discovery, we propose a simple\nyet effective \\emph{Sampling-Free} mechanism to achieve a reasonable\nclassification gradient magnitude by initialization and loss scaling. Unlike\nheuristic sampling methods with multiple hyperparameters, our Sampling-Free\nmechanism is fully data diagnostic, without laborious hyperparameters\nsearching. We verify the effectiveness of our method in training anchor-based\nand anchor-free object detectors, where our method always achieves higher\ndetection accuracy than heuristic sampling methods on COCO and PASCAL VOC\ndatasets. Our Sampling-Free mechanism provides a new perspective to address the\nforeground-background imbalance. Our code is released at\n\\url{https://github.com/ChenJoya/sampling-free}.",
          "link": "http://arxiv.org/abs/1909.04868",
          "publishedOn": "2021-08-12T01:56:22.744Z",
          "wordCount": 729,
          "title": "Is Heuristic Sampling Necessary in Training Deep Object Detectors?. (arXiv:1909.04868v8 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bomatter_P/0/1/0/all/0/1\">Philipp Bomatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karev_D/0/1/0/all/0/1\">Dimitar Karev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1\">Claire Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>",
          "description": "Context is of fundamental importance to both human and machine vision; e.g.,\nan object in the air is more likely to be an airplane than a pig. The rich\nnotion of context incorporates several aspects including physics rules,\nstatistical co-occurrences, and relative object sizes, among others. While\nprevious work has focused on crowd-sourced out-of-context photographs from the\nweb to study scene context, controlling the nature and extent of contextual\nviolations has been a daunting task. Here we introduce a diverse, synthetic\nOut-of-Context Dataset (OCD) with fine-grained control over scene context. By\nleveraging a 3D simulation engine, we systematically control the gravity,\nobject co-occurrences and relative sizes across 36 object categories in a\nvirtual household environment. We conducted a series of experiments to gain\ninsights into the impact of contextual cues on both human and machine vision\nusing OCD. We conducted psychophysics experiments to establish a human\nbenchmark for out-of-context recognition, and then compared it with\nstate-of-the-art computer vision models to quantify the gap between the two. We\npropose a context-aware recognition transformer model, fusing object and\ncontextual information via multi-head attention. Our model captures useful\ninformation for contextual reasoning, enabling human-level performance and\nbetter robustness in out-of-context conditions compared to baseline models\nacross OCD and other out-of-context datasets. All source code and data are\npublicly available at https://github.com/kreimanlab/WhenPigsFlyContext",
          "link": "http://arxiv.org/abs/2104.02215",
          "publishedOn": "2021-08-12T01:56:22.728Z",
          "wordCount": 704,
          "title": "When Pigs Fly: Contextual Reasoning in Synthetic and Natural Scenes. (arXiv:2104.02215v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trevithick_A/0/1/0/all/0/1\">Alex Trevithick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>",
          "description": "We present a simple yet powerful neural network that implicitly represents\nand renders 3D objects and scenes only from 2D observations. The network models\n3D geometries as a general radiance field, which takes a set of 2D images with\ncamera poses and intrinsics as input, constructs an internal representation for\neach point of the 3D space, and then renders the corresponding appearance and\ngeometry of that point viewed from an arbitrary position. The key to our\napproach is to learn local features for each pixel in 2D images and to then\nproject these features to 3D points, thus yielding general and rich point\nrepresentations. We additionally integrate an attention mechanism to aggregate\npixel features from multiple 2D views, such that visual occlusions are\nimplicitly taken into account. Extensive experiments demonstrate that our\nmethod can generate high-quality and realistic novel views for novel objects,\nunseen categories and challenging real-world scenes.",
          "link": "http://arxiv.org/abs/2010.04595",
          "publishedOn": "2021-08-12T01:56:22.701Z",
          "wordCount": 648,
          "title": "GRF: Learning a General Radiance Field for 3D Representation and Rendering. (arXiv:2010.04595v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ewecker_L/0/1/0/all/0/1\">Lukas Ewecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asan_E/0/1/0/all/0/1\">Ebubekir Asan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohnemus_L/0/1/0/all/0/1\">Lars Ohnemus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saralajew_S/0/1/0/all/0/1\">Sascha Saralajew</a>",
          "description": "In recent years, computer vision algorithms have become more and more\npowerful, which enabled technologies such as autonomous driving to evolve with\nrapid pace. However, current algorithms mainly share one limitation: They rely\non directly visible objects. This is a major drawback compared to human\nbehavior, where indirect visual cues caused by the actual object (e.g.,\nshadows) are already used intuitively to retrieve information or anticipate\noccurring objects. While driving at night, this performance deficit becomes\neven more obvious: Humans already process the light artifacts caused by\noncoming vehicles to assume their future appearance, whereas current object\ndetection systems rely on the oncoming vehicle's direct visibility. Based on\nprevious work in this subject, we present with this paper a complete system\ncapable of solving the task to providently detect oncoming vehicles at\nnighttime based on their caused light artifacts. For that, we outline the full\nalgorithm architecture ranging from the detection of light artifacts in the\nimage space, localizing the objects in the three-dimensional space, and\nverifying the objects over time. To demonstrate the applicability, we deploy\nthe system in a test vehicle and use the information of providently detected\nvehicles to control the glare-free high beam system proactively. Using this\nexperimental setting, we quantify the time benefit that the provident vehicle\ndetection system provides compared to an in-production computer vision system.\nAdditionally, the glare-free high beam use case provides a real-time and\nreal-world visualization interface of the detection results. With this\ncontribution, we want to put awareness on the unconventional sensing task of\nprovident object detection and further close the performance gap between human\nbehavior and computer vision algorithms in order to bring autonomous and\nautomated driving a step forward.",
          "link": "http://arxiv.org/abs/2107.11302",
          "publishedOn": "2021-08-12T01:56:22.673Z",
          "wordCount": 750,
          "title": "Provident Vehicle Detection at Night for Advanced Driver Assistance Systems. (arXiv:2107.11302v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06887",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_Q/0/1/0/all/0/1\">Qing Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koh_J/0/1/0/all/0/1\">Jae Chul Koh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_W/0/1/0/all/0/1\">WonSook Lee</a>",
          "description": "Synthetic X-ray images are simulated X-ray images projected from CT data.\nHigh-quality synthetic X-ray images can facilitate various applications such as\nsurgical image guidance systems and VR training simulations. However, it is\ndifficult to produce high-quality arbitrary view synthetic X-ray images in\nreal-time due to different CT slice thickness, high computational cost, and the\ncomplexity of algorithms. Our goal is to generate high-resolution synthetic\nX-ray images in real-time by upsampling low-resolution images with deep\nlearning-based super-resolution methods. Reference-based Super Resolution\n(RefSR) has been well studied in recent years and has shown higher performance\nthan traditional Single Image Super-Resolution (SISR). It can produce fine\ndetails by utilizing the reference image but still inevitably generates some\nartifacts and noise. In this paper, we introduce frequency domain loss as a\nconstraint to further improve the quality of the RefSR results with fine\ndetails and without obvious artifacts. To the best of our knowledge, this is\nthe first paper utilizing the frequency domain for the loss functions in the\nfield of super-resolution. We achieved good results in evaluating our method on\nboth synthetic and real X-ray image datasets.",
          "link": "http://arxiv.org/abs/2105.06887",
          "publishedOn": "2021-08-12T01:56:22.647Z",
          "wordCount": 656,
          "title": "A Frequency Domain Constraint for Synthetic and Real X-ray Image Super Resolution. (arXiv:2105.06887v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guolei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "Existing blind image super-resolution (SR) methods mostly assume blur kernels\nare spatially invariant across the whole image. However, such an assumption is\nrarely applicable for real images whose blur kernels are usually spatially\nvariant due to factors such as object motion and out-of-focus. Hence, existing\nblind SR methods would inevitably give rise to poor performance in real\napplications. To address this issue, this paper proposes a mutual affine\nnetwork (MANet) for spatially variant kernel estimation. Specifically, MANet\nhas two distinctive features. First, it has a moderate receptive field so as to\nkeep the locality of degradation. Second, it involves a new mutual affine\nconvolution (MAConv) layer that enhances feature expressiveness without\nincreasing receptive field, model size and computation burden. This is made\npossible through exploiting channel interdependence, which applies each channel\nsplit with an affine transformation module whose input are the rest channel\nsplits. Extensive experiments on synthetic and real images show that the\nproposed MANet not only performs favorably for both spatially variant and\ninvariant kernel estimation, but also leads to state-of-the-art blind SR\nperformance when combined with non-blind SR methods.",
          "link": "http://arxiv.org/abs/2108.05302",
          "publishedOn": "2021-08-12T01:56:22.642Z",
          "wordCount": 636,
          "title": "Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution. (arXiv:2108.05302v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chixiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sibei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Vision transformers have attracted much attention from computer vision\nresearchers as they are not restricted to the spatial inductive bias of\nConvNets. However, although Transformer-based backbones have achieved much\nprogress on ImageNet classification, it is still unclear whether the learned\nrepresentations are as transferable as or even more transferable than ConvNets'\nfeatures. To address this point, we systematically investigate the transfer\nlearning ability of ConvNets and vision transformers in 15 single-task and\nmulti-task performance evaluations. Given the strong correlation between the\nperformance of pre-trained models and transfer learning, we include 2 residual\nConvNets (i.e., R-101x3 and R-152x4) and 3 Transformer-based visual backbones\n(i.e., ViT-B, ViT-L and Swin-B), which have close error rates on ImageNet, that\nindicate similar transfer learning performance on downstream datasets.\n\nWe observe consistent advantages of Transformer-based backbones on 13\ndownstream tasks (out of 15), including but not limited to fine-grained\nclassification, scene recognition (classification, segmentation and depth\nestimation), open-domain classification, face recognition, etc. More\nspecifically, we find that two ViT models heavily rely on whole network\nfine-tuning to achieve performance gains while Swin Transformer does not have\nsuch a requirement. Moreover, vision transformers behave more robustly in\nmulti-task learning, i.e., bringing more improvements when managing mutually\nbeneficial tasks and reducing performance losses when tackling irrelevant\ntasks. We hope our discoveries can facilitate the exploration and exploitation\nof vision transformers in the future.",
          "link": "http://arxiv.org/abs/2108.05305",
          "publishedOn": "2021-08-12T01:56:22.616Z",
          "wordCount": 675,
          "title": "ConvNets vs. Transformers: Whose Visual Representations are More Transferable?. (arXiv:2108.05305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rigoni_D/0/1/0/all/0/1\">Davide Rigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serafini_L/0/1/0/all/0/1\">Luciano Serafini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1\">Alessandro Sperduti</a>",
          "description": "Given a textual phrase and an image, the visual grounding problem is defined\nas the task of locating the content of the image referenced by the sentence. It\nis a challenging task that has several real-world applications in\nhuman-computer interaction, image-text reference resolution, and video-text\nreference resolution. In the last years, several works have addressed this\nproblem with heavy and complex models that try to capture visual-textual\ndependencies better than before. These models are typically constituted by two\nmain components that focus on how to learn useful multi-modal features for\ngrounding and how to improve the predicted bounding box of the visual mention,\nrespectively. Finding the right learning balance between these two sub-tasks is\nnot easy, and the current models are not necessarily optimal with respect to\nthis issue. In this work, we propose a model that, although using a simple\nmulti-modal feature fusion component, is able to achieve a higher accuracy than\nstate-of-the-art models thanks to the adoption of a more effective loss\nfunction, based on the classes probabilities, that reach, in the considered\ndatasets, a better learning balance between the two sub-tasks mentioned above.",
          "link": "http://arxiv.org/abs/2108.05308",
          "publishedOn": "2021-08-12T01:56:22.593Z",
          "wordCount": 620,
          "title": "A Better Loss for Visual-Textual Grounding. (arXiv:2108.05308v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lugmayr_A/0/1/0/all/0/1\">Andreas Lugmayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "Normalizing flows have recently demonstrated promising results for low-level\nvision tasks. For image super-resolution (SR), it learns to predict diverse\nphoto-realistic high-resolution (HR) images from the low-resolution (LR) image\nrather than learning a deterministic mapping. For image rescaling, it achieves\nhigh accuracy by jointly modelling the downscaling and upscaling processes.\nWhile existing approaches employ specialized techniques for these two tasks, we\nset out to unify them in a single formulation. In this paper, we propose the\nhierarchical conditional flow (HCFlow) as a unified framework for image SR and\nimage rescaling. More specifically, HCFlow learns a bijective mapping between\nHR and LR image pairs by modelling the distribution of the LR image and the\nrest high-frequency component simultaneously. In particular, the high-frequency\ncomponent is conditional on the LR image in a hierarchical manner. To further\nenhance the performance, other losses such as perceptual loss and GAN loss are\ncombined with the commonly used negative log-likelihood loss in training.\nExtensive experiments on general image SR, face image SR and image rescaling\nhave demonstrated that the proposed HCFlow achieves state-of-the-art\nperformance in terms of both quantitative metrics and visual quality.",
          "link": "http://arxiv.org/abs/2108.05301",
          "publishedOn": "2021-08-12T01:56:22.574Z",
          "wordCount": 643,
          "title": "Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling. (arXiv:2108.05301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>",
          "description": "Medical images, especially volumetric images, are of high resolution and\noften exceed the capacity of standard desktop GPUs. As a result, most deep\nlearning-based medical image analysis tasks require the input images to be\ndownsampled, often substantially, before these can be fed to a neural network.\nHowever, downsampling can lead to a loss of image quality, which is undesirable\nespecially in reconstruction tasks, where the fine geometric details need to be\npreserved. In this paper, we propose that high-resolution images can be\nreconstructed in a coarse-to-fine fashion, where a deep learning algorithm is\nonly responsible for generating a coarse representation of the image, which\nconsumes moderate GPU memory. For producing the high-resolution outcome, we\npropose two novel methods: learned voxel rearrangement of the coarse output and\nhierarchical image synthesis. Compared to the coarse output, the\nhigh-resolution counterpart allows for smooth surface triangulation, which can\nbe 3D-printed in the highest possible quality. Experiments of this paper are\ncarried out on the dataset of AutoImplant 2021\n(https://autoimplant2021.grand-challenge.org/), a MICCAI challenge on cranial\nimplant design. The dataset contains high-resolution skulls that can be viewed\nas 2D manifolds embedded in a 3D space. Codes associated with this study can be\naccessed at https://github.com/Jianningli/voxel_rearrangement.",
          "link": "http://arxiv.org/abs/2108.05269",
          "publishedOn": "2021-08-12T01:56:22.454Z",
          "wordCount": 670,
          "title": "Learning to Rearrange Voxels in Binary Segmentation Masks for Smooth Manifold Triangulation. (arXiv:2108.05269v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahner_M/0/1/0/all/0/1\">Martin Hahner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "This work addresses the challenging task of LiDAR-based 3D object detection\nin foggy weather. Collecting and annotating data in such a scenario is very\ntime, labor and cost intensive. In this paper, we tackle this problem by\nsimulating physically accurate fog into clear-weather scenes, so that the\nabundant existing real datasets captured in clear weather can be repurposed for\nour task. Our contributions are twofold: 1) We develop a physically valid fog\nsimulation method that is applicable to any LiDAR dataset. This unleashes the\nacquisition of large-scale foggy training data at no extra cost. These\npartially synthetic data can be used to improve the robustness of several\nperception methods, such as 3D object detection and tracking or simultaneous\nlocalization and mapping, on real foggy data. 2) Through extensive experiments\nwith several state-of-the-art detection approaches, we show that our fog\nsimulation can be leveraged to significantly improve the performance for 3D\nobject detection in the presence of fog. Thus, we are the first to provide\nstrong 3D object detection baselines on the Seeing Through Fog dataset. Our\ncode is available at www.trace.ethz.ch/lidar_fog_simulation.",
          "link": "http://arxiv.org/abs/2108.05249",
          "publishedOn": "2021-08-12T01:56:22.448Z",
          "wordCount": 642,
          "title": "Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather. (arXiv:2108.05249v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>",
          "description": "In this work, we address the challenging task of few-shot segmentation.\nPrevious few-shot segmentation methods mainly employ the information of support\nimages as guidance for query image segmentation. Although some works propose to\nbuild cross-reference between support and query images, their extraction of\nquery information still depends on the support images. We here propose to\nextract the information from the query itself independently to benefit the\nfew-shot segmentation task. To this end, we first propose a prior extractor to\nlearn the query information from the unlabeled images with our proposed\nglobal-local contrastive learning. Then, we extract a set of predetermined\npriors via this prior extractor. With the obtained priors, we generate the\nprior region maps for query images, which locate the objects, as guidance to\nperform cross interaction with support features. In such a way, the extraction\nof query information is detached from the support branch, overcoming the\nlimitation by support, and could obtain more informative query clues to achieve\nbetter interaction. Without bells and whistles, the proposed approach achieves\nnew state-of-the-art performance for the few-shot segmentation task on\nPASCAL-5$^{i}$ and COCO datasets.",
          "link": "http://arxiv.org/abs/2108.05293",
          "publishedOn": "2021-08-12T01:56:22.424Z",
          "wordCount": 623,
          "title": "Few-Shot Segmentation with Global and Local Contrastive Learning. (arXiv:2108.05293v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menini_D/0/1/0/all/0/1\">Davide Menini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandstrom_E/0/1/0/all/0/1\">Erik Sandstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "This paper presents a real-time online vision framework to jointly recover an\nindoor scene's 3D structure and semantic label. Given noisy depth maps, a\ncamera trajectory, and 2D semantic labels at train time, the proposed neural\nnetwork learns to fuse the depth over frames with suitable semantic labels in\nthe scene space. Our approach exploits the joint volumetric representation of\nthe depth and semantics in the scene feature space to solve this task. For a\ncompelling online fusion of the semantic labels and geometry in real-time, we\nintroduce an efficient vortex pooling block while dropping the routing network\nin online depth fusion to preserve high-frequency surface details. We show that\nthe context information provided by the semantics of the scene helps the depth\nfusion network learn noise-resistant features. Not only that, it helps overcome\nthe shortcomings of the current online depth fusion method in dealing with thin\nobject structures, thickening artifacts, and false surfaces. Experimental\nevaluation on the Replica dataset shows that our approach can perform depth\nfusion at 37, 10 frames per second with an average reconstruction F-score of\n88%, and 91%, respectively, depending on the depth map resolution. Moreover,\nour model shows an average IoU score of 0.515 on the ScanNet 3D semantic\nbenchmark leaderboard.",
          "link": "http://arxiv.org/abs/2108.05246",
          "publishedOn": "2021-08-12T01:56:22.371Z",
          "wordCount": 671,
          "title": "A Real-Time Online Learning Framework for Joint 3D Reconstruction and Semantic Segmentation of Indoor Scenes. (arXiv:2108.05246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mantowsky_S/0/1/0/all/0/1\">Sven Mantowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuer_F/0/1/0/all/0/1\">Falk Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhari_S/0/1/0/all/0/1\">Syed Saqib Bukhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keckeisen_M/0/1/0/all/0/1\">Michael Keckeisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Georg Schneider</a>",
          "description": "Development in the field of Single Board Computers (SBC) have been increasing\nfor several years. They provide a good balance between computing performance\nand power consumption which is usually required for mobile platforms, like\napplication in vehicles for Advanced Driver Assistance Systems (ADAS) and\nAutonomous Driving (AD). However, there is an ever-increasing need of more\npowerful and efficient SBCs which can run power intensive Deep Neural Networks\n(DNNs) in real-time and can also satisfy necessary functional safety\nrequirements such as Automotive Safety Integrity Level (ASIL). ProAI is being\ndeveloped by ZF mainly to run powerful and efficient applications such as\nmultitask DNNs and on top of that it also has the required safety certification\nfor AD. In this work, we compare and discuss state of the art SBC on the basis\nof power intensive multitask DNN architecture called Multitask-CenterNet with\nrespect to performance measures such as, FPS and power efficiency. As an\nautomotive supercomputer, ProAI delivers an excellent combination of\nperformance and efficiency, managing nearly twice the number of FPS per watt\nthan a modern workstation laptop and almost four times compared to the Jetson\nNano. Furthermore, it was also shown that there is still power in reserve for\nfurther and more complex tasks on the ProAI, based on the CPU and GPU\nutilization during the benchmark.",
          "link": "http://arxiv.org/abs/2108.05170",
          "publishedOn": "2021-08-12T01:56:22.342Z",
          "wordCount": 684,
          "title": "ProAI: An Efficient Embedded AI Hardware for Automotive Applications - a Benchmark Study. (arXiv:2108.05170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_L/0/1/0/all/0/1\">Louis Gallagher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John B. McDonald</a>",
          "description": "We present a new technique that achieves a significant reduction in the\nquantity of measurements required for a fusion based dense 3D mapping system to\nconverge to an accurate, de-noised surface reconstruction. This is achieved\nthrough the use of a Normalised Information Distance metric, that computes the\nnovelty of the information contained in each incoming frame with respect to the\nreconstruction, and avoids fusing those frames that exceed a redundancy\nthreshold. This provides a principled approach for opitmising the trade-off\nbetween surface reconstruction accuracy and the computational cost of\nprocessing frames. The technique builds upon the ElasticFusion (EF) algorithm\nwhere we report results of the technique's scalability and the accuracy of the\nresultant maps by applying it to both the ICL-NUIM and TUM RGB-D datasets.\nThese results demonstrate the capabilities of the approach in performing\naccurate surface reconstructions whilst utilising a fraction of the frames when\ncompared to the original EF algorithm.",
          "link": "http://arxiv.org/abs/2108.05163",
          "publishedOn": "2021-08-12T01:56:22.335Z",
          "wordCount": 605,
          "title": "Efficient Surfel Fusion Using Normalised Information Distance. (arXiv:2108.05163v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoye Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Songfang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianxiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "Virtual 3D try-on can provide an intuitive and realistic view for online\nshopping and has a huge potential commercial value. However, existing 3D\nvirtual try-on methods mainly rely on annotated 3D human shapes and garment\ntemplates, which hinders their applications in practical scenarios. 2D virtual\ntry-on approaches provide a faster alternative to manipulate clothed humans,\nbut lack the rich and realistic 3D representation. In this paper, we propose a\nnovel Monocular-to-3D Virtual Try-On Network (M3D-VTON) that builds on the\nmerits of both 2D and 3D approaches. By integrating 2D information efficiently\nand learning a mapping that lifts the 2D representation to 3D, we make the\nfirst attempt to reconstruct a 3D try-on mesh only taking the target clothing\nand a person image as inputs. The proposed M3D-VTON includes three modules: 1)\nThe Monocular Prediction Module (MPM) that estimates an initial full-body depth\nmap and accomplishes 2D clothes-person alignment through a novel two-stage\nwarping procedure; 2) The Depth Refinement Module (DRM) that refines the\ninitial body depth to produce more detailed pleat and face characteristics; 3)\nThe Texture Fusion Module (TFM) that fuses the warped clothing with the\nnon-target body part to refine the results. We also construct a high-quality\nsynthesized Monocular-to-3D virtual try-on dataset, in which each person image\nis associated with a front and a back depth map. Extensive experiments\ndemonstrate that the proposed M3D-VTON can manipulate and reconstruct the 3D\nhuman body wearing the given clothing with compelling details and is more\nefficient than other 3D approaches.",
          "link": "http://arxiv.org/abs/2108.05126",
          "publishedOn": "2021-08-12T01:56:22.320Z",
          "wordCount": 694,
          "title": "M3D-VTON: A Monocular-to-3D Virtual Try-On Network. (arXiv:2108.05126v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Medical imaging technologies, including computed tomography (CT) or chest\nX-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.\nSince manual report writing is usually too time-consuming, a more intelligent\nauxiliary medical system that could generate medical reports automatically and\nimmediately is urgently needed. In this article, we propose to use the medical\nvisual language BERT (Medical-VLBERT) model to identify the abnormality on the\nCOVID-19 scans and generate the medical report automatically based on the\ndetected lesion regions. To produce more accurate medical reports and minimize\nthe visual-and-linguistic differences, this model adopts an alternate learning\nstrategy with two procedures that are knowledge pretraining and transferring.\nTo be more precise, the knowledge pretraining procedure is to memorize the\nknowledge from medical texts, while the transferring procedure is to utilize\nthe acquired knowledge for professional medical sentences generations through\nobservations of medical images. In practice, for automatic medical report\ngeneration on the COVID-19 cases, we constructed a dataset of 368 medical\nfindings in Chinese and 1104 chest CT scans from The First Affiliated Hospital\nof Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun\nYat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of\nthe COVID-19 training samples, our model was first trained on the large-scale\nChinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for\nfurther fine-tuning. The experimental results showed that Medical-VLBERT\nachieved state-of-the-art performances on terminology prediction and report\ngeneration with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The\nChinese COVID-19 CT dataset is available at https://covid19ct.github.io/.",
          "link": "http://arxiv.org/abs/2108.05067",
          "publishedOn": "2021-08-12T01:56:22.315Z",
          "wordCount": 782,
          "title": "Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yutao Ma</a>",
          "description": "Background: Cervical cancer seriously affects the health of the female\nreproductive system. Optical coherence tomography (OCT) emerges as a\nnon-invasive, high-resolution imaging technology for cervical disease\ndetection. However, OCT image annotation is knowledge-intensive and\ntime-consuming, which impedes the training process of deep-learning-based\nclassification models. Objective: This study aims to develop a computer-aided\ndiagnosis (CADx) approach to classifying in-vivo cervical OCT images based on\nself-supervised learning. Methods: Besides high-level semantic features\nextracted by a convolutional neural network (CNN), the proposed CADx approach\nleverages unlabeled cervical OCT images' texture features learned by\ncontrastive texture learning. We conducted ten-fold cross-validation on the OCT\nimage dataset from a multi-center clinical study on 733 patients from China.\nResults: In a binary classification task for detecting high-risk diseases,\nincluding high-grade squamous intraepithelial lesion (HSIL) and cervical\ncancer, our method achieved an area-under-the-curve (AUC) value of 0.9798 Plus\nor Minus 0.0157 with a sensitivity of 91.17 Plus or Minus 4.99% and a\nspecificity of 93.96 Plus or Minus 4.72% for OCT image patches; also, it\noutperformed two out of four medical experts on the test set. Furthermore, our\nmethod achieved a 91.53% sensitivity and 97.37% specificity on an external\nvalidation dataset containing 287 3D OCT volumes from 118 Chinese patients in a\nnew hospital using a cross-shaped threshold voting strategy. Conclusion: The\nproposed contrastive-learning-based CADx method outperformed the end-to-end CNN\nmodels and provided better interpretability based on texture features, which\nholds great potential to be used in the clinical protocol of \"see-and-treat.\"",
          "link": "http://arxiv.org/abs/2108.05081",
          "publishedOn": "2021-08-12T01:56:22.285Z",
          "wordCount": 701,
          "title": "Cervical Optical Coherence Tomography Image Classification Based on Contrastive Self-Supervised Texture Learning. (arXiv:2108.05081v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>",
          "description": "Many camera sensors use a dual-pixel (DP) design that operates as a\nrudimentary light field providing two sub-aperture views of a scene in a single\ncapture. The DP sensor was developed to improve how cameras perform autofocus.\nSince the DP sensor's introduction, researchers have found additional uses for\nthe DP data, such as depth estimation, reflection removal, and defocus\ndeblurring. We are interested in the latter task of defocus deblurring. In\nparticular, we propose a single-image deblurring network that incorporates the\ntwo sub-aperture views into a multi-task framework. Specifically, we show that\njointly learning to predict the two DP views from a single blurry input image\nimproves the network's ability to learn to deblur the image. Our experiments\nshow this multi-task strategy achieves +1dB PSNR improvement over\nstate-of-the-art defocus deblurring methods. In addition, our multi-task\nframework allows accurate DP-view synthesis (e.g., ~ 39dB PSNR) from the single\ninput image. These high-quality DP views can be used for other DP-based\napplications, such as reflection removal. As part of this effort, we have\ncaptured a new dataset of 7,059 high-quality images to support our training for\nthe DP-view synthesis task. Our dataset, code, and trained models will be made\npublicly available at\nhttps://github.com/Abdullah-Abuolaim/multi-task-defocus-deblurring-dual-pixel-nimat",
          "link": "http://arxiv.org/abs/2108.05251",
          "publishedOn": "2021-08-12T01:56:22.272Z",
          "wordCount": 644,
          "title": "Improving Single-Image Defocus Deblurring: How Dual-Pixel Images Help Through Multi-Task Learning. (arXiv:2108.05251v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hanyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "Deep hashing has been widely applied to large-scale image retrieval by\nencoding high-dimensional data points into binary codes for efficient\nretrieval. Compared with pairwise/triplet similarity based hash learning,\ncentral similarity based hashing can more efficiently capture the global data\ndistribution. For multi-label image retrieval, however, previous methods only\nuse multiple hash centers with equal weights to generate one centroid as the\nlearning target, which ignores the relationship between the weights of hash\ncenters and the proportion of instance regions in the image. To address the\nabove issue, we propose a two-step alternative optimization approach,\nInstance-weighted Central Similarity (ICS), to automatically learn the center\nweight corresponding to a hash code. Firstly, we apply the maximum entropy\nregularizer to prevent one hash center from dominating the loss function, and\ncompute the center weights via projection gradient descent. Secondly, we update\nneural network parameters by standard back-propagation with fixed center\nweights. More importantly, the learned center weights can well reflect the\nproportion of foreground instances in the image. Our method achieves the\nstate-of-the-art performance on the image retrieval benchmarks, and especially\nimproves the mAP by 1.6%-6.4% on the MS COCO dataset.",
          "link": "http://arxiv.org/abs/2108.05274",
          "publishedOn": "2021-08-12T01:56:22.267Z",
          "wordCount": 625,
          "title": "Instance-weighted Central Similarity for Multi-label Image Retrieval. (arXiv:2108.05274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donggeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seongho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Youwon Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>",
          "description": "Video question answering has recently received a lot of attention from\nmultimodal video researchers. Most video question answering datasets are\nusually in the form of multiple-choice. But, the model for the multiple-choice\ntask does not infer the answer. Rather it compares the answer candidates for\npicking the correct answer. Furthermore, it makes it difficult to extend to\nother tasks. In this paper, we challenge the existing multiple-choice video\nquestion answering by changing it to open-ended video question answering. To\ntackle open-ended question answering, we use the pretrained GPT2 model. The\nmodel is fine-tuned with video inputs and subtitles. An ablation study is\nperformed by changing the existing DramaQA dataset to an open-ended question\nanswering, and it shows that performance can be improved using video metadata.",
          "link": "http://arxiv.org/abs/2108.05158",
          "publishedOn": "2021-08-12T01:56:22.242Z",
          "wordCount": 578,
          "title": "Mounting Video Metadata on Transformer-based Language Model for Open-ended Video Question Answering. (arXiv:2108.05158v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>",
          "description": "More than 90\\% of colorectal cancer is gradually transformed from colorectal\npolyps. In clinical practice, precise polyp segmentation provides important\ninformation in the early detection of colorectal cancer. Therefore, automatic\npolyp segmentation techniques are of great importance for both patients and\ndoctors. Most existing methods are based on U-shape structure and use\nelement-wise addition or concatenation to fuse different level features\nprogressively in decoder. However, both the two operations easily generate\nplenty of redundant information, which will weaken the complementarity between\ndifferent level features, resulting in inaccurate localization and blurred\nedges of polyps. To address this challenge, we propose a multi-scale\nsubtraction network (MSNet) to segment polyp from colonoscopy image.\nSpecifically, we first design a subtraction unit (SU) to produce the difference\nfeatures between adjacent levels in encoder. Then, we pyramidally equip the SUs\nat different levels with varying receptive fields, thereby obtaining rich\nmulti-scale difference information. In addition, we build a training-free\nnetwork \"LossNet\" to comprehensively supervise the polyp-aware features from\nbottom layer to top layer, which drives the MSNet to capture the detailed and\nstructural cues simultaneously. Extensive experiments on five benchmark\ndatasets demonstrate that our MSNet performs favorably against most\nstate-of-the-art methods under different evaluation metrics. Furthermore, MSNet\nruns at a real-time speed of $\\sim$70fps when processing a $352 \\times 352$\nimage. The source code will be publicly available at\n\\url{https://github.com/Xiaoqi-Zhao-DLUT/MSNet}. \\keywords{Colorectal Cancer\n\\and Automatic Polyp Segmentation \\and Subtraction \\and LossNet.}",
          "link": "http://arxiv.org/abs/2108.05082",
          "publishedOn": "2021-08-12T01:56:22.224Z",
          "wordCount": 676,
          "title": "Automatic Polyp Segmentation via Multi-scale Subtraction Network. (arXiv:2108.05082v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_H/0/1/0/all/0/1\">Hasam Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "With the significant advancements made in generation of forged video and\naudio, commonly known as deepfakes, using deep learning technologies, the\nproblem of its misuse is a well-known issue now. Recently, a new problem of\ngenerating cloned or synthesized human voice of a person is emerging. AI-based\ndeep learning models can synthesize any person's voice requiring just a few\nseconds of audio. With the emerging threat of impersonation attacks using\ndeepfake videos and audios, new deepfake detectors are need that focuses on\nboth, video and audio. Detecting deepfakes is a challenging task and\nresearchers have made numerous attempts and proposed several deepfake detection\nmethods. To develop a good deepfake detector, a handsome amount of good quality\ndataset is needed that captures the real world scenarios. Many researchers have\ncontributed in this cause and provided several deepfake dataset, self generated\nand in-the-wild. However, almost all of these datasets either contains deepfake\nvideos or audio. Moreover, the recent deepfake datasets proposed by researchers\nhave racial bias issues. Hence, there is a crucial need of a good deepfake\nvideo and audio deepfake dataset. To fill this gap, we propose a novel\nAudio-Video Deepfake dataset (FakeAVCeleb) that not only contains deepfake\nvideos but respective synthesized cloned audios as well. We generated our\ndataset using recent most popular deepfake generation methods and the videos\nand audios are perfectly lip-synced with each other. To generate a more\nrealistic dataset, we selected real YouTube videos of celebrities having four\nracial backgrounds (Caucasian, Black, East Asian and South Asian) to counter\nthe racial bias issue. Lastly, we propose a novel multimodal detection method\nthat detects deepfake videos and audios based on our multimodal Audio-Video\ndeepfake dataset.",
          "link": "http://arxiv.org/abs/2108.05080",
          "publishedOn": "2021-08-12T01:56:22.218Z",
          "wordCount": 714,
          "title": "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset. (arXiv:2108.05080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lengyel_A/0/1/0/all/0/1\">Attila Lengyel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>",
          "description": "We explore the zero-shot setting for day-night domain adaptation. The\ntraditional domain adaptation setting is to train on one domain and adapt to\nthe target domain by exploiting unlabeled data samples from the test set. As\ngathering relevant test data is expensive and sometimes even impossible, we\nremove any reliance on test data imagery and instead exploit a visual inductive\nprior derived from physics-based reflection models for domain adaptation. We\ncast a number of color invariant edge detectors as trainable layers in a\nconvolutional neural network and evaluate their robustness to illumination\nchanges. We show that the color invariant layer reduces the day-night\ndistribution shift in feature map activations throughout the network. We\ndemonstrate improved performance for zero-shot day to night domain adaptation\non both synthetic as well as natural datasets in various tasks, including\nclassification, segmentation and place recognition.",
          "link": "http://arxiv.org/abs/2108.05137",
          "publishedOn": "2021-08-12T01:56:22.209Z",
          "wordCount": 589,
          "title": "Zero-Shot Domain Adaptation with a Physics Prior. (arXiv:2108.05137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heuer_F/0/1/0/all/0/1\">Falk Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantowsky_S/0/1/0/all/0/1\">Sven Mantowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhari_S/0/1/0/all/0/1\">Syed Saqib Bukhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Georg Schneider</a>",
          "description": "Multitask learning is a common approach in machine learning, which allows to\ntrain multiple objectives with a shared architecture. It has been shown that by\ntraining multiple tasks together inference time and compute resources can be\nsaved, while the objectives performance remains on a similar or even higher\nlevel. However, in perception related multitask networks only closely related\ntasks can be found, such as object detection, instance and semantic\nsegmentation or depth estimation. Multitask networks with diverse tasks and\ntheir effects with respect to efficiency on one another are not well studied.\nIn this paper we augment the CenterNet anchor-free approach for training\nmultiple diverse perception related tasks together, including the task of\nobject detection and semantic segmentation as well as human pose estimation. We\nrefer to this DNN as Multitask-CenterNet (MCN). Additionally, we study\ndifferent MCN settings for efficiency. The MCN can perform several tasks at\nonce while maintaining, and in some cases even exceeding, the performance\nvalues of its corresponding single task networks. More importantly, the MCN\narchitecture decreases inference time and reduces network size when compared to\na composition of single task networks.",
          "link": "http://arxiv.org/abs/2108.05060",
          "publishedOn": "2021-08-12T01:56:22.171Z",
          "wordCount": 644,
          "title": "MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach. (arXiv:2108.05060v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sung-Jin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Seo-Won Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jun-Pyo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sung-Jea Ko</a>",
          "description": "Coarse-to-fine strategies have been extensively used for the architecture\ndesign of single image deblurring networks. Conventional methods typically\nstack sub-networks with multi-scale input images and gradually improve\nsharpness of images from the bottom sub-network to the top sub-network,\nyielding inevitably high computational costs. Toward a fast and accurate\ndeblurring network design, we revisit the coarse-to-fine strategy and present a\nmulti-input multi-output U-net (MIMO-UNet). The MIMO-UNet has three distinct\nfeatures. First, the single encoder of the MIMO-UNet takes multi-scale input\nimages to ease the difficulty of training. Second, the single decoder of the\nMIMO-UNet outputs multiple deblurred images with different scales to mimic\nmulti-cascaded U-nets using a single U-shaped network. Last, asymmetric feature\nfusion is introduced to merge multi-scale features in an efficient manner.\nExtensive experiments on the GoPro and RealBlur datasets demonstrate that the\nproposed network outperforms the state-of-the-art methods in terms of both\naccuracy and computational complexity. Source code is available for research\npurposes at https://github.com/chosj95/MIMO-UNet.",
          "link": "http://arxiv.org/abs/2108.05054",
          "publishedOn": "2021-08-12T01:56:22.155Z",
          "wordCount": 609,
          "title": "Rethinking Coarse-to-Fine Approach in Single Image Deblurring. (arXiv:2108.05054v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuangchi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_X/0/1/0/all/0/1\">Xue Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Ziwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiduo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruobing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "Standard plane recognition plays an important role in prenatal ultrasound\n(US) screening. Automatically recognizing the standard plane along with the\ncorresponding anatomical structures in US image can not only facilitate US\nimage interpretation but also improve diagnostic efficiency. In this study, we\nbuild a novel multi-label learning (MLL) scheme to identify multiple standard\nplanes and corresponding anatomical structures of fetus simultaneously. Our\ncontribution is three-fold. First, we represent the class correlation by word\nembeddings to capture the fine-grained semantic and latent statistical\nconcurrency. Second, we equip the MLL with a graph convolutional network to\nexplore the inner and outer relationship among categories. Third, we propose a\nnovel cluster relabel-based contrastive learning algorithm to encourage the\ndivergence among ambiguous classes. Extensive validation was performed on our\nlarge in-house dataset. Our approach reports the highest accuracy as 90.25% for\nstandard planes labeling, 85.59% for planes and structures labeling and mAP as\n94.63%. The proposed MLL scheme provides a novel perspective for standard plane\nrecognition and can be easily extended to other medical image classification\ntasks.",
          "link": "http://arxiv.org/abs/2108.05055",
          "publishedOn": "2021-08-12T01:56:22.135Z",
          "wordCount": 644,
          "title": "Statistical Dependency Guided Contrastive Learning for Multiple Labeling in Prenatal Ultrasound. (arXiv:2108.05055v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Youwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>",
          "description": "Location and appearance are the key cues for video object segmentation. Many\nsources such as RGB, depth, optical flow and static saliency can provide useful\ninformation about the objects. However, existing approaches only utilize the\nRGB or RGB and optical flow. In this paper, we propose a novel multi-source\nfusion network for zero-shot video object segmentation. With the help of\ninteroceptive spatial attention module (ISAM), spatial importance of each\nsource is highlighted. Furthermore, we design a feature purification module\n(FPM) to filter the inter-source incompatible features. By the ISAM and FPM,\nthe multi-source features are effectively fused. In addition, we put forward an\nautomatic predictor selection network (APS) to select the better prediction of\neither the static saliency predictor or the moving object predictor in order to\nprevent over-reliance on the failed results caused by low-quality optical flow\nmaps. Extensive experiments on three challenging public benchmarks (i.e.\nDAVIS$_{16}$, Youtube-Objects and FBMS) show that the proposed model achieves\ncompelling performance against the state-of-the-arts. The source code will be\npublicly available at\n\\textcolor{red}{\\url{https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS}}.",
          "link": "http://arxiv.org/abs/2108.05076",
          "publishedOn": "2021-08-12T01:56:22.063Z",
          "wordCount": 627,
          "title": "Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation. (arXiv:2108.05076v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiuping Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1\">Feng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>",
          "description": "Existing efforts on Just noticeable difference (JND) estimation mainly\ndedicate to modeling the visibility masking effects of different factors in\nspatial and frequency domains, and then fusing them into an overall JND\nestimate. However, the overall visibility masking effect can be related with\nmore contributing factors beyond those have been considered in the literature\nand it is also insufficiently accurate to formulate the masking effect even for\nan individual factor. Moreover, the potential interactions among different\nmasking effects are also difficult to be characterized with a simple fusion\nmodel. In this work, we turn to a dramatically different way to address these\nproblems with a top-down design philosophy. Instead of formulating and fusing\nmultiple masking effects in a bottom-up way, the proposed JND estimation model\ndirectly generates a critical perceptual lossless (CPL) image from a top-down\nperspective and calculates the difference map between the original image and\nthe CPL image as the final JND map. Given an input image, an adaptively\ncritical point (perceptual lossless threshold), defined as the minimum number\nof spectral components in Karhunen-Lo\\'{e}ve Transform (KLT) used for\nperceptual lossless image reconstruction, is derived by exploiting the\nconvergence characteristics of KLT coefficient energy. Then, the CPL image can\nbe reconstructed via inverse KLT according to the derived critical point.\nFinally, the difference map between the original image and the CPL image is\ncalculated as the JND map. The performance of the proposed JND model is\nevaluated with two applications including JND-guided noise injection and\nJND-guided image compression. Experimental results have demonstrated that our\nproposed JND model can achieve better performance than several latest JND\nmodels.",
          "link": "http://arxiv.org/abs/2108.05058",
          "publishedOn": "2021-08-12T01:56:22.045Z",
          "wordCount": 711,
          "title": "Towards Top-Down Just Noticeable Difference Estimation of Natural Images. (arXiv:2108.05058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baoquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shanshan Feng</a>",
          "description": "Few-shot learning aims to recognize novel classes with few examples.\nPre-training based methods effectively tackle the problem by pre-training a\nfeature extractor and then fine-tuning it through the nearest centroid based\nmeta-learning. However, results show that the fine-tuning step makes marginal\nimprovements. In this paper, 1) we figure out the reason, i.e., in the\npre-trained feature space, the base classes already form compact clusters while\nnovel classes spread as groups with large variances, which implies that\nfine-tuning feature extractor is less meaningful; 2) instead of fine-tuning\nfeature extractor, we focus on estimating more representative prototypes.\nConsequently, we propose a novel prototype completion based meta-learning\nframework. This framework first introduces primitive knowledge (i.e.,\nclass-level part or attribute annotations) and extracts representative features\nfor seen attributes as priors. Second, a part/attribute transfer network is\ndesigned to learn to infer the representative features for unseen attributes as\nsupplementary priors. Finally, a prototype completion network is devised to\nlearn to complete prototypes with these priors. Moreover, to avoid the\nprototype completion error, we further develop a Gaussian based prototype\nfusion strategy that fuses the mean-based and completed prototypes by\nexploiting the unlabeled samples. Extensive experiments show that our method:\n(i) obtains more accurate prototypes; (ii) achieves superior performance on\nboth inductive and transductive FSL settings.",
          "link": "http://arxiv.org/abs/2108.05010",
          "publishedOn": "2021-08-12T01:56:21.943Z",
          "wordCount": 655,
          "title": "Prototype Completion for Few-Shot Learning. (arXiv:2108.05010v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xingyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Existing person re-identification (re-id) methods are stuck when deployed to\na new unseen scenario despite the success in cross-camera person matching.\nRecent efforts have been substantially devoted to domain adaptive person re-id\nwhere extensive unlabeled data in the new scenario are utilized in a\ntransductive learning manner. However, for each scenario, it is required to\nfirst collect enough data and then train such a domain adaptive re-id model,\nthus restricting their practical application. Instead, we aim to explore\nmultiple labeled datasets to learn generalized domain-invariant representations\nfor person re-id, which is expected universally effective for each new-coming\nre-id scenario. To pursue practicability in real-world systems, we collect all\nthe person re-id datasets (20 datasets) in this field and select the three most\nfrequently used datasets (i.e., Market1501, DukeMTMC, and MSMT17) as unseen\ntarget domains. In addition, we develop DataHunter that collects over 300K+\nweak annotated images named YouTube-Human from YouTube street-view videos,\nwhich joins 17 remaining full labeled datasets to form multiple source domains.\nOn such a large and challenging benchmark called FastHuman (~440K+ labeled\nimages), we further propose a simple yet effective Semi-Supervised Knowledge\nDistillation (SSKD) framework. SSKD effectively exploits the weakly annotated\ndata by assigning soft pseudo labels to YouTube-Human to improve models'\ngeneralization ability. Experiments on several protocols verify the\neffectiveness of the proposed SSKD framework on domain generalizable person\nre-id, which is even comparable to supervised learning on the target domains.\nLastly, but most importantly, we hope the proposed benchmark FastHuman could\nbring the next development of domain generalizable person re-id algorithms.",
          "link": "http://arxiv.org/abs/2108.05045",
          "publishedOn": "2021-08-12T01:56:21.937Z",
          "wordCount": 692,
          "title": "Semi-Supervised Domain Generalizable Person Re-Identification. (arXiv:2108.05045v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangyi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Weiwei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jingzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiguo Gong</a>",
          "description": "We propose a new general Graph Adversarial Domain Adaptation (GADA) based on\nsemantic knowledge reasoning of class structure for solving the problem of\nunsupervised domain adaptation (UDA) from the big data with non-shared and\nimbalanced classes to specified small and imbalanced applications (NI-UDA),\nwhere non-shared classes mean the label space out of the target domain. Our\ngoal is to leverage priori hierarchy knowledge to enhance domain adversarial\naligned feature representation with graph reasoning. In this paper, to address\ntwo challenges in NI-UDA, we equip adversarial domain adaptation with Hierarchy\nGraph Reasoning (HGR) layer and the Source Classifier Filter (SCF). For sparse\nclasses transfer challenge, our HGR layer can aggregate local feature to\nhierarchy graph nodes by node prediction and enhance domain adversarial aligned\nfeature with hierarchy graph reasoning for sparse classes. Our HGR contributes\nto learn direct semantic patterns for sparse classes by hierarchy attention in\nself-attention, non-linear mapping and graph normalization. our SCF is proposed\nfor the challenge of knowledge sharing from non-shared data without negative\ntransfer effect by filtering low-confidence non-shared data in HGR layer.\nExperiments on two benchmark datasets show our GADA methods consistently\nimprove the state-of-the-art adversarial UDA algorithms, e.g. GADA(HGR) can\ngreatly improve f1 of the MDD by \\textbf{7.19\\%} and GVB-GD by \\textbf{7.89\\%}\nrespectively on imbalanced source task in Meal300 dataset. The code is\navailable at https://gadatransfer.wixsite.com/gada.",
          "link": "http://arxiv.org/abs/2108.05061",
          "publishedOn": "2021-08-12T01:56:21.911Z",
          "wordCount": 688,
          "title": "NI-UDA: Graph Adversarial Domain Adaptation from Non-shared-and-Imbalanced Big Data to Small Imbalanced Applications. (arXiv:2108.05061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Juwei Lu</a>",
          "description": "State of the art (SOTA) few-shot learning (FSL) methods suffer significant\nperformance drop in the presence of domain differences between source and\ntarget datasets. The strong discrimination ability on the source dataset does\nnot necessarily translate to high classification accuracy on the target\ndataset. In this work, we address this cross-domain few-shot learning (CDFSL)\nproblem by boosting the generalization capability of the model. Specifically,\nwe teach the model to capture broader variations of the feature distributions\nwith a novel noise-enhanced supervised autoencoder (NSAE). NSAE trains the\nmodel by jointly reconstructing inputs and predicting the labels of inputs as\nwell as their reconstructed pairs. Theoretical analysis based on intra-class\ncorrelation (ICC) shows that the feature embeddings learned from NSAE have\nstronger discrimination and generalization abilities in the target domain. We\nalso take advantage of NSAE structure and propose a two-step fine-tuning\nprocedure that achieves better adaption and improves classification performance\nin the target domain. Extensive experiments and ablation studies are conducted\nto demonstrate the effectiveness of the proposed method. Experimental results\nshow that our proposed method consistently outperforms SOTA methods under\nvarious conditions.",
          "link": "http://arxiv.org/abs/2108.05028",
          "publishedOn": "2021-08-12T01:56:21.866Z",
          "wordCount": 634,
          "title": "Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder. (arXiv:2108.05028v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Beibin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuechterlein_N/0/1/0/all/0/1\">Nicholas Nuechterlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barney_E/0/1/0/all/0/1\">Erin Barney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_C/0/1/0/all/0/1\">Claire Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minah Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahony_M/0/1/0/all/0/1\">Monique Mahony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atyabi_A/0/1/0/all/0/1\">Adham Atyabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Li Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_P/0/1/0/all/0/1\">Pamela Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapiro_L/0/1/0/all/0/1\">Linda Shapiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shic_F/0/1/0/all/0/1\">Frederick Shic</a>",
          "description": "Identifying oculomotor behaviors relevant for eye-tracking applications is a\ncritical but often challenging task. Aiming to automatically learn and extract\nknowledge from existing eye-tracking data, we develop a novel method that\ncreates rich representations of oculomotor scanpaths to facilitate the learning\nof downstream tasks. The proposed stimulus-agnostic Oculomotor Behavior\nFramework (OBF) model learns human oculomotor behaviors from unsupervised and\nsemi-supervised tasks, including reconstruction, predictive coding, fixation\nidentification, and contrastive learning tasks. The resultant pre-trained OBF\nmodel can be used in a variety of applications. Our pre-trained model\noutperforms baseline approaches and traditional scanpath methods in autism\nspectrum disorder and viewed-stimulus classification tasks. Ablation\nexperiments further show our proposed method could achieve even better results\nwith larger model sizes and more diverse eye-tracking training datasets,\nsupporting the model's potential for future eye-tracking applications. Open\nsource code: this http URL",
          "link": "http://arxiv.org/abs/2108.05025",
          "publishedOn": "2021-08-12T01:56:21.796Z",
          "wordCount": 599,
          "title": "Learning Oculomotor Behaviors from Scanpath. (arXiv:2108.05025v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05021",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1\">Yuanhao Gong</a>",
          "description": "Image smoothing is a fundamental task in signal processing. For such task,\nbox filter is well-known. However, box filter can not keep some features of the\nsignal, such as edges, corners and the jump in the step function. In this\npaper, we present a one-sided box filter that can smooth the signal but keep\nthe discontinuous features in the signal. More specifically, we perform box\nfilter on eight one-sided windows, leading to a one-sided box filter that can\npreserve corners and edges. Our filter inherits the constant $O(1)$\ncomputational complexity of the original box filter with respect to the window\nsize and also the linear $O(N)$ computational complexity with respect to the\ntotal number of samples. We performance several experiments to show the\nefficiency and effectiveness of this filter. We further compare our filter with\nother the-state-of-the-art edge preserving methods. Our filter can be deployed\nin a large range of applications where the classical box filter is adopted.",
          "link": "http://arxiv.org/abs/2108.05021",
          "publishedOn": "2021-08-12T01:56:21.785Z",
          "wordCount": 606,
          "title": "One-Sided Box Filter for Edge Preserving Image Smoothing. (arXiv:2108.05021v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Different from visible cameras which record intensity images frame by frame,\nthe biologically inspired event camera produces a stream of asynchronous and\nsparse events with much lower latency. In practice, the visible cameras can\nbetter perceive texture details and slow motion, while event cameras can be\nfree from motion blurs and have a larger dynamic range which enables them to\nwork well under fast motion and low illumination. Therefore, the two sensors\ncan cooperate with each other to achieve more reliable object tracking. In this\nwork, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to\nthe lack of a realistic and scaled dataset for this task. Our dataset consists\nof 820 video pairs captured under low illumination, high speed, and background\nclutter scenarios, and it is divided into a training and a testing subset, each\nof which contains 500 and 320 videos, respectively. Based on VisEvent, we\ntransform the event flows into event images and construct more than 30 baseline\nmethods by extending current single-modality trackers into dual-modality\nversions. More importantly, we further build a simple but effective tracking\nalgorithm by proposing a cross-modality transformer, to achieve more effective\nfeature fusion between visible and event data. Extensive experiments on the\nproposed VisEvent dataset, and two simulated datasets (i.e., OTB-DVS and\nVOT-DVS), validated the effectiveness of our model. The dataset and source code\nwill be available at our project page:\n\\url{https://sites.google.com/view/viseventtrack/}.",
          "link": "http://arxiv.org/abs/2108.05015",
          "publishedOn": "2021-08-12T01:56:21.778Z",
          "wordCount": 693,
          "title": "VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koga_K/0/1/0/all/0/1\">Kazuki Koga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takemoto_K/0/1/0/all/0/1\">Kazuhiro Takemoto</a>",
          "description": "Universal adversarial attacks, which hinder most deep neural network (DNN)\ntasks using only a small single perturbation called a universal adversarial\nperturbation (UAP), is a realistic security threat to the practical application\nof a DNN. In particular, such attacks cause serious problems in medical\nimaging. Given that computer-based systems are generally operated under a\nblack-box condition in which only queries on inputs are allowed and outputs are\naccessible, the impact of UAPs seems to be limited because well-used algorithms\nfor generating UAPs are limited to a white-box condition in which adversaries\ncan access the model weights and loss gradients. Nevertheless, we demonstrate\nthat UAPs are easily generatable using a relatively small dataset under\nblack-box conditions. In particular, we propose a method for generating UAPs\nusing a simple hill-climbing search based only on DNN outputs and demonstrate\nthe validity of the proposed method using representative DNN-based medical\nimage classifications. Black-box UAPs can be used to conduct both non-targeted\nand targeted attacks. Overall, the black-box UAPs showed high attack success\nrates (40% to 90%), although some of them had relatively low success rates\nbecause the method only utilizes limited information to generate UAPs. The\nvulnerability of black-box UAPs was observed in several model architectures.\nThe results indicate that adversaries can also generate UAPs through a simple\nprocedure under the black-box condition to foil or control DNN-based medical\nimage diagnoses, and that UAPs are a more realistic security threat.",
          "link": "http://arxiv.org/abs/2108.04979",
          "publishedOn": "2021-08-12T01:56:21.758Z",
          "wordCount": 694,
          "title": "Simple black-box universal adversarial attacks on medical image classification based on deep neural networks. (arXiv:2108.04979v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Pilhyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>",
          "description": "We tackle the problem of localizing temporal intervals of actions with only a\nsingle frame label for each action instance for training. Owing to label\nsparsity, existing work fails to learn action completeness, resulting in\nfragmentary action predictions. In this paper, we propose a novel framework,\nwhere dense pseudo-labels are generated to provide completeness guidance for\nthe model. Concretely, we first select pseudo background points to supplement\npoint-level action labels. Then, by taking the points as seeds, we search for\nthe optimal sequence that is likely to contain complete action instances while\nagreeing with the seeds. To learn completeness from the obtained sequence, we\nintroduce two novel losses that contrast action instances with background ones\nin terms of action score and feature similarity, respectively. Experimental\nresults demonstrate that our completeness guidance indeed helps the model to\nlocate complete action instances, leading to large performance gains especially\nunder high IoU thresholds. Moreover, we demonstrate the superiority of our\nmethod over existing state-of-the-art methods on four benchmarks: THUMOS'14,\nGTEA, BEOID, and ActivityNet. Notably, our method even performs comparably to\nrecent fully-supervised methods, at the 6 times cheaper annotation cost. Our\ncode is available at https://github.com/Pilhyeon.",
          "link": "http://arxiv.org/abs/2108.05029",
          "publishedOn": "2021-08-12T01:56:21.744Z",
          "wordCount": 644,
          "title": "Learning Action Completeness from Points for Weakly-supervised Temporal Action Localization. (arXiv:2108.05029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Anbang Yao</a>",
          "description": "We propose a compact and effective framework to fuse multimodal features at\nmultiple layers in a single network. The framework consists of two innovative\nfusion schemes. Firstly, unlike existing multimodal methods that necessitate\nindividual encoders for different modalities, we verify that multimodal\nfeatures can be learnt within a shared single network by merely maintaining\nmodality-specific batch normalization layers in the encoder, which also enables\nimplicit fusion via joint feature representation learning. Secondly, we propose\na bidirectional multi-layer fusion scheme, where multimodal features can be\nexploited progressively. To take advantage of such scheme, we introduce two\nasymmetric fusion operations including channel shuffle and pixel shift, which\nlearn different fused features with respect to different fusion directions.\nThese two operations are parameter-free and strengthen the multimodal feature\ninteractions across channels as well as enhance the spatial feature\ndiscrimination within channels. We conduct extensive experiments on semantic\nsegmentation and image translation tasks, based on three publicly available\ndatasets covering diverse modalities. Results indicate that our proposed\nframework is general, compact and is superior to state-of-the-art fusion\nframeworks.",
          "link": "http://arxiv.org/abs/2108.05009",
          "publishedOn": "2021-08-12T01:56:21.736Z",
          "wordCount": 616,
          "title": "Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion. (arXiv:2108.05009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang Li</a>",
          "description": "Tactile sensing plays an important role in robotic perception and\nmanipulation tasks. To overcome the real-world limitations of data collection,\nsimulating tactile response in a virtual environment comes as a desirable\ndirection of robotic research. In this paper, we propose Elastic Interaction of\nParticles (EIP) for tactile simulation. Most existing works model the tactile\nsensor as a rigid multi-body, which is incapable of reflecting the elastic\nproperty of the tactile sensor as well as characterizing the fine-grained\nphysical interaction between the two objects. By contrast, EIP models the\ntactile sensor as a group of coordinated particles, and the elastic property is\napplied to regulate the deformation of particles during contact. With the\ntactile simulation by EIP, we further propose a tactile-visual perception\nnetwork that enables information fusion between tactile data and visual images.\nThe perception network is based on a global-to-local fusion mechanism where\nmulti-scale tactile features are aggregated to the corresponding local region\nof the visual modality with the guidance of tactile positions and directions.\nThe fusion method exhibits superiority regarding the 3D geometric\nreconstruction task.",
          "link": "http://arxiv.org/abs/2108.05013",
          "publishedOn": "2021-08-12T01:56:21.706Z",
          "wordCount": 614,
          "title": "Elastic Tactile Simulation Towards Tactile-Visual Perception. (arXiv:2108.05013v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasser_A/0/1/0/all/0/1\">Alex Nasser</a>",
          "description": "Proposed are alternative generator architectures for Boundary Equilibrium\nGenerative Adversarial Networks, motivated by Learning from Simulated and\nUnsupervised Images through Adversarial Training. It disentangles the need for\na noise-based latent space. The generator will operate mainly as a refiner\nnetwork to gain a photo-realistic presentation of the given synthetic images.\nIt also attempts to resolve the latent space's poorly understood properties by\neliminating the need for noise injection and replacing it with an image-based\nconcept. The new flexible and simple generator architecture will also give the\npower to control the trade-off between restrictive refinement and\nexpressiveness ability. Contrary to other available methods, this architecture\nwill not require a paired or unpaired dataset of real and synthetic images for\nthe training phase. Only a relatively small set of real images would suffice.",
          "link": "http://arxiv.org/abs/2108.04957",
          "publishedOn": "2021-08-12T01:56:21.684Z",
          "wordCount": 564,
          "title": "An Image-based Generator Architecture for Synthetic Image Refinement. (arXiv:2108.04957v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1\">Huy Quang Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Nghia Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>",
          "description": "Handwritten mathematical expressions (HMEs) contain ambiguities in their\ninterpretations, even for humans sometimes. Several math symbols are very\nsimilar in the writing style, such as dot and comma or 0, O, and o, which is a\nchallenge for HME recognition systems to handle without using contextual\ninformation. To address this problem, this paper presents a Transformer-based\nMath Language Model (TMLM). Based on the self-attention mechanism, the\nhigh-level representation of an input token in a sequence of tokens is computed\nby how it is related to the previous tokens. Thus, TMLM can capture long\ndependencies and correlations among symbols and relations in a mathematical\nexpression (ME). We trained the proposed language model using a corpus of\napproximately 70,000 LaTeX sequences provided in CROHME 2016. TMLM achieved the\nperplexity of 4.42, which outperformed the previous math language models, i.e.,\nthe N-gram and recurrent neural network-based language models. In addition, we\ncombine TMLM into a stochastic context-free grammar-based HME recognition\nsystem using a weighting parameter to re-rank the top-10 best candidates. The\nexpression rates on the testing sets of CROHME 2016 and CROHME 2019 were\nimproved by 2.97 and 0.83 percentage points, respectively.",
          "link": "http://arxiv.org/abs/2108.05002",
          "publishedOn": "2021-08-12T01:56:21.678Z",
          "wordCount": 645,
          "title": "A Transformer-based Math Language Model for Handwritten Math Expression Recognition. (arXiv:2108.05002v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhsedaghat_M/0/1/0/all/0/1\">Mozhdeh Rouhsedaghat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_A/0/1/0/all/0/1\">Aichi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scalzo_F/0/1/0/all/0/1\">Fabien Scalzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>",
          "description": "Vision-and-language(V&L) models take image and text as input and learn to\ncapture the associations between them. Prior studies show that pre-trained V&L\nmodels can significantly improve the model performance for downstream tasks\nsuch as Visual Question Answering (VQA). However, V&L models are less effective\nwhen applied in the medical domain (e.g., on X-ray images and clinical notes)\ndue to the domain gap. In this paper, we investigate the challenges of applying\npre-trained V&L models in medical applications. In particular, we identify that\nthe visual representation in general V&L models is not suitable for processing\nmedical data. To overcome this limitation, we propose BERTHop, a\ntransformer-based model based on PixelHop++ and VisualBERT, for better\ncapturing the associations between the two modalities. Experiments on the OpenI\ndataset, a commonly used thoracic disease diagnosis benchmark, show that\nBERTHop achieves an average Area Under the Curve (AUC) of 98.12% which is 1.62%\nhigher than state-of-the-art (SOTA) while it is trained on a 9 times smaller\ndataset.",
          "link": "http://arxiv.org/abs/2108.04938",
          "publishedOn": "2021-08-12T01:56:21.660Z",
          "wordCount": 628,
          "title": "BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease Diagnosis. (arXiv:2108.04938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yufei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Face recognition (FR) has made extraordinary progress owing to the\nadvancement of deep convolutional neural networks. However, demographic bias\namong different racial cohorts still challenges the practical face recognition\nsystem. The race factor has been proven to be a dilemma for fair FR (FFR) as\nthe subject-related specific attributes induce the classification bias whilst\ncarrying some useful cues for FR. To mitigate racial bias and meantime preserve\nrobust FR, we abstract face identity-related representation as a signal\ndenoising problem and propose a progressive cross transformer (PCT) method for\nfair face recognition. Originating from the signal decomposition theory, we\nattempt to decouple face representation into i) identity-related components and\nii) noisy/identity-unrelated components induced by race. As an extension of\nsignal subspace decomposition, we formulate face decoupling as a generalized\nfunctional expression model to cross-predict face identity and race\ninformation. The face expression model is further concretized by designing dual\ncross-transformers to distill identity-related components and suppress racial\nnoises. In order to refine face representation, we take a progressive face\ndecoupling way to learn identity/race-specific transformations, so that\nidentity-unrelated components induced by race could be better disentangled. We\nevaluate the proposed PCT on the public fair face recognition benchmarks (BFW,\nRFW) and verify that PCT is capable of mitigating bias in face recognition\nwhile achieving state-of-the-art FR performance. Besides, visualization results\nalso show that the attention maps in PCT can well reveal the\nrace-related/biased facial regions.",
          "link": "http://arxiv.org/abs/2108.04983",
          "publishedOn": "2021-08-12T01:56:21.654Z",
          "wordCount": 671,
          "title": "Learning Fair Face Representation With Progressive Cross Transformer. (arXiv:2108.04983v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+pourmirzaei_M/0/1/0/all/0/1\">Mahdi pourmirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+montazer_g/0/1/0/all/0/1\">gholam ali montazer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+esmaili_f/0/1/0/all/0/1\">farzaneh esmaili</a>",
          "description": "Recent progress of Self-Supervised Learning (SSL) demonstrates the capability\nof these methods in computer vision field. However, this progress could not\nshow any promises for fine-grained tasks such as Head Pose estimation. In this\narticle, we have tried to answer a question: How SSL can be used for Head Pose\nestimation? In general, there are two main approaches to use SSL: 1. Using\npre-trained weights which can be done via weights pre-training on ImageNet or\nvia SSL tasks. 2. Leveraging SSL as an auxiliary co-training task besides of\nSupervised Learning (SL) tasks at the same time. In this study, modified\nversions of jigsaw puzzling and rotation as SSL pre-text tasks are used and the\nbest architecture for our proposed Hybrid Multi-Task Learning (HMTL) is found.\nFinally, the HopeNet method as a baseline is selected and the impact of SSL\npre-training and ImageNet pre-training on both HMTL and SL are compared. The\nerror rate reduced by the HTML method up to 11% compare to the SL. Moreover,\nHMTL method showed that it was good with all kinds of initial weights: random,\nImageNet and SSL pre-training weights. Also, it was observed, when puzzled\nimages are used for SL alone, the average error rate placed between SL and HMTL\nwhich showed the importance of local spatial features compare to global spatial\nfeatures.",
          "link": "http://arxiv.org/abs/2108.04893",
          "publishedOn": "2021-08-12T01:56:21.647Z",
          "wordCount": 662,
          "title": "How Self-Supervised Learning Can be Used for Fine-Grained Head Pose Estimation?. (arXiv:2108.04893v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tascon_J/0/1/0/all/0/1\">Jose D Tascon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauze_F/0/1/0/all/0/1\">Francois Lauze</a>",
          "description": "First Order Locally Orderless Registration (FLOR) is a scale-space framework\nfor image density estimation used for defining image similarity, mainly for\nImage Registration. The Locally Orderless Registration framework was designed\nin principle to use zeroth-order information, providing image density estimates\nover three scales: image scale, intensity scale, and integration scale. We\nextend it to take first-order information into account and hint at higher-order\ninformation. We show how standard similarity measures extend into the\nframework. We study especially Sum of Squared Differences (SSD) and Normalized\nCross-Correlation (NCC) but present the theory of how Normalised Mutual\nInformation (NMI) can be included.",
          "link": "http://arxiv.org/abs/2108.04926",
          "publishedOn": "2021-08-12T01:56:21.642Z",
          "wordCount": 528,
          "title": "First Order Locally Orderless Registration. (arXiv:2108.04926v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1\">Gaurav Sukhatme</a>",
          "description": "Language-guided robots performing home and office tasks must navigate in and\ninteract with the world. Grounding language instructions against visual\nobservations and actions to take in an environment is an open challenge. We\npresent Embodied BERT (EmBERT), a transformer-based model which can attend to\nhigh-dimensional, multi-modal inputs across long temporal horizons for\nlanguage-conditioned task completion. Additionally, we bridge the gap between\nsuccessful object-centric navigation models used for non-interactive agents and\nthe language-guided visual task completion benchmark, ALFRED, by introducing\nobject navigation targets for EmBERT training. We achieve competitive\nperformance on the ALFRED benchmark, and EmBERT marks the first\ntransformer-based model to successfully handle the long-horizon, dense,\nmulti-modal histories of ALFRED, and the first ALFRED model to utilize\nobject-centric navigation targets.",
          "link": "http://arxiv.org/abs/2108.04927",
          "publishedOn": "2021-08-12T01:56:21.628Z",
          "wordCount": 581,
          "title": "Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion. (arXiv:2108.04927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Razumov_A/0/1/0/all/0/1\">Artem Razumov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogov_O/0/1/0/all/0/1\">Oleg Y. Rogov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>",
          "description": "To accelerate MRI, the field of compressed sensing is traditionally concerned\nwith optimizing the image quality after a partial undersampling of the\nmeasurable $\\textit{k}$-space. In our work, we propose to change the focus from\nthe quality of the reconstructed image to the quality of the downstream image\nanalysis outcome. Specifically, we propose to optimize the patterns according\nto how well a sought-after pathology could be detected or localized in the\nreconstructed images. We find the optimal undersampling patterns in\n$\\textit{k}$-space that maximize target value functions of interest in\ncommonplace medical vision problems (reconstruction, segmentation, and\nclassification) and propose a new iterative gradient sampling routine\nuniversally suitable for these tasks. We validate the proposed MRI acceleration\nparadigm on three classical medical datasets, demonstrating a noticeable\nimprovement of the target metrics at the high acceleration factors (for the\nsegmentation problem at $\\times$16 acceleration, we report up to 12%\nimprovement in Dice score over the other undersampling patterns).",
          "link": "http://arxiv.org/abs/2108.04914",
          "publishedOn": "2021-08-12T01:56:21.621Z",
          "wordCount": 597,
          "title": "Optimal MRI Undersampling Patterns for Ultimate Benefit of Medical Vision Tasks. (arXiv:2108.04914v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoran Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simmons_Edler_R/0/1/0/all/0/1\">Riley Simmons-Edler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daewon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackel_L/0/1/0/all/0/1\">Larry Jackel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_R/0/1/0/all/0/1\">Richard Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daniel Lee</a>",
          "description": "Perceiving obstacles and avoiding collisions is fundamental to the safe\noperation of a robot system, particularly when the robot must operate in highly\ndynamic human environments. Proximity detection using on-robot sensors can be\nused to avoid or mitigate impending collisions. However, existing proximity\nsensing methods are orientation and placement dependent, resulting in blind\nspots even with large numbers of sensors. In this paper, we introduce the\nphenomenon of the Leaky Surface Wave (LSW), a novel sensing modality, and\npresent AuraSense, a proximity detection system using the LSW. AuraSense is the\nfirst system to realize no-dead-spot proximity sensing for robot arms. It\nrequires only a single pair of piezoelectric transducers, and can easily be\napplied to off-the-shelf robots with minimal modifications. We further\nintroduce a set of signal processing techniques and a lightweight neural\nnetwork to address the unique challenges in using the LSW for proximity\nsensing. Finally, we demonstrate a prototype system consisting of a single\npiezoelectric element pair on a robot manipulator, which validates our design.\nWe conducted several micro benchmark experiments and performed more than 2000\non-robot proximity detection trials with various potential robot arm materials,\ncolliding objects, approach patterns, and robot movement patterns. AuraSense\nachieves 100% and 95.3% true positive proximity detection rates when the arm\napproaches static and mobile obstacles respectively, with a true negative rate\nover 99%, showing the real-world viability of this system.",
          "link": "http://arxiv.org/abs/2108.04867",
          "publishedOn": "2021-08-12T01:56:21.571Z",
          "wordCount": 686,
          "title": "AuraSense: Robot Collision Avoidance by Full Surface Proximity Detection. (arXiv:2108.04867v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parida_K/0/1/0/all/0/1\">Kranti Kumar Parida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Siddharth Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matiyali_N/0/1/0/all/0/1\">Neeraj Matiyali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gaurav Sharma</a>",
          "description": "Binaural audio gives the listener the feeling of being in the recording place\nand enhances the immersive experience if coupled with AR/VR. But the problem\nwith binaural audio recording is that it requires a specialized setup which is\nnot possible to fabricate within handheld devices as compared to traditional\nmono audio that can be recorded with a single microphone. In order to overcome\nthis drawback, prior works have tried to uplift the mono recorded audio to\nbinaural audio as a post processing step conditioning on the visual input. But\nall the prior approaches missed other most important information required for\nthe task, i.e. distance of different sound producing objects from the recording\nsetup. In this work, we argue that the depth map of the scene can act as a\nproxy for encoding distance information of objects in the scene and show that\nadding depth features along with image features improves the performance both\nqualitatively and quantitatively. We propose a novel encoder-decoder\narchitecture, where we use a hierarchical attention mechanism to encode the\nimage and depth feature extracted from individual transformer backbone, with\naudio features at each layer of the decoder.",
          "link": "http://arxiv.org/abs/2108.04906",
          "publishedOn": "2021-08-12T01:56:21.540Z",
          "wordCount": 640,
          "title": "Depth Infused Binaural Audio Generation using Hierarchical Cross-Modal Attention. (arXiv:2108.04906v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>",
          "description": "Great progress has been made by the advances in Generative Adversarial\nNetworks (GANs) for image generation. However, there lacks enough understanding\non how a realistic image can be generated by the deep representations of GANs\nfrom a random vector. This chapter will give a summary of recent works on\ninterpreting deep generative models. We will see how the human-understandable\nconcepts that emerge in the learned representation can be identified and used\nfor interactive image generation and editing.",
          "link": "http://arxiv.org/abs/2108.04896",
          "publishedOn": "2021-08-12T01:56:21.415Z",
          "wordCount": 515,
          "title": "Interpreting Generative Adversarial Networks for Interactive Image Generation. (arXiv:2108.04896v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Athar_S/0/1/0/all/0/1\">ShahRukh Athar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>",
          "description": "This paper presents a neural rendering method for controllable portrait video\nsynthesis. Recent advances in volumetric neural rendering, such as neural\nradiance fields (NeRF), has enabled the photorealistic novel view synthesis of\nstatic scenes with impressive results. However, modeling dynamic and\ncontrollable objects as part of a scene with such scene representations is\nstill challenging. In this work, we design a system that enables both novel\nview synthesis for portrait video, including the human subject and the scene\nbackground, and explicit control of the facial expressions through a\nlow-dimensional expression representation. We leverage the expression space of\na 3D morphable face model (3DMM) to represent the distribution of human facial\nexpressions, and use it to condition the NeRF volumetric function. Furthermore,\nwe impose a spatial prior brought by 3DMM fitting to guide the network to learn\ndisentangled control for scene appearance and facial actions. We demonstrate\nthe effectiveness of our method on free view synthesis of portrait videos with\nexpression controls. To train a scene, our method only requires a short video\nof a subject captured by a mobile device.",
          "link": "http://arxiv.org/abs/2108.04913",
          "publishedOn": "2021-08-12T01:56:21.395Z",
          "wordCount": 624,
          "title": "FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face Animation. (arXiv:2108.04913v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1\">Forrester Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genova_K/0/1/0/all/0/1\">Kyle Genova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sud_A/0/1/0/all/0/1\">Avneesh Sud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlasic_D/0/1/0/all/0/1\">Daniel Vlasic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhoutong Zhang</a>",
          "description": "We present a method for differentiable rendering of 3D surfaces that supports\nboth explicit and implicit representations, provides derivatives at occlusion\nboundaries, and is fast and simple to implement. The method first samples the\nsurface using non-differentiable rasterization, then applies differentiable,\ndepth-aware point splatting to produce the final image. Our approach requires\nno differentiable meshing or rasterization steps, making it efficient for large\n3D models and applicable to isosurfaces extracted from implicit surface\ndefinitions. We demonstrate the effectiveness of our method for implicit-,\nmesh-, and parametric-surface-based inverse rendering and neural-network\ntraining applications. In particular, we show for the first time efficient,\ndifferentiable rendering of an isosurface extracted from a neural radiance\nfield (NeRF), and demonstrate surface-based, rather than volume-based,\nrendering of a NeRF.",
          "link": "http://arxiv.org/abs/2108.04886",
          "publishedOn": "2021-08-12T01:56:21.362Z",
          "wordCount": 560,
          "title": "Differentiable Surface Rendering via Non-Differentiable Sampling. (arXiv:2108.04886v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jordao_A/0/1/0/all/0/1\">Artur Jordao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrini_H/0/1/0/all/0/1\">Helio Pedrini</a>",
          "description": "Pruning is a well-known mechanism for reducing the computational cost of deep\nconvolutional networks. However, studies have shown the potential of pruning as\na form of regularization, which reduces overfitting and improves\ngeneralization. We demonstrate that this family of strategies provides\nadditional benefits beyond computational performance and generalization. Our\nanalyses reveal that pruning structures (filters and/or layers) from\nconvolutional networks increase not only generalization but also robustness to\nadversarial images (natural images with content modified). Such achievements\nare possible since pruning reduces network capacity and provides\nregularization, which have been proven effective tools against adversarial\nimages. In contrast to promising defense mechanisms that require training with\nadversarial images and careful regularization, we show that pruning obtains\ncompetitive results considering only natural images (e.g., the standard and\nlow-cost training). We confirm these findings on several adversarial attacks\nand architectures; thus suggesting the potential of pruning as a novel defense\nmechanism against adversarial images.",
          "link": "http://arxiv.org/abs/2108.04890",
          "publishedOn": "2021-08-12T01:56:21.295Z",
          "wordCount": 583,
          "title": "On the Effect of Pruning on Adversarial Robustness. (arXiv:2108.04890v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Usman_B/0/1/0/all/0/1\">Ben Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sud_A/0/1/0/all/0/1\">Avneesh Sud</a>",
          "description": "Recently, huge strides were made in monocular and multi-view pose estimation\nwith known camera parameters, whereas pose estimation from multiple cameras\nwith unknown positions and orientations received much less attention. In this\npaper, we show how to train a neural model that can perform accurate 3D pose\nand camera estimation, takes into account joint location uncertainty due\nocclusion from multiple views, and requires only 2D keypoint data for training.\nOur method outperforms both classical bundle adjustment and weakly-supervised\nmonocular 3D baselines on the well-established Human3.6M dataset, as well as\nthe more challenging in-the-wild Ski-Pose PTZ dataset with moving cameras. We\nprovide an extensive ablation study separating the error due to the camera\nmodel, number of cameras, initialization, and image-space joint localization\nfrom the additional error introduced by our model.",
          "link": "http://arxiv.org/abs/2108.04869",
          "publishedOn": "2021-08-12T01:56:21.235Z",
          "wordCount": 572,
          "title": "MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision. (arXiv:2108.04869v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changshu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guangchun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Attempting to fully exploit the rich information of topological structure and\nnode features for attributed graph, we introduce self-supervised learning\nmechanism to graph representation learning and propose a novel Self-supervised\nConsensus Representation Learning (SCRL) framework. In contrast to most\nexisting works that only explore one graph, our proposed SCRL method treats\ngraph from two perspectives: topology graph and feature graph. We argue that\ntheir embeddings should share some common information, which could serve as a\nsupervisory signal. Specifically, we construct the feature graph of node\nfeatures via k-nearest neighbor algorithm. Then graph convolutional network\n(GCN) encoders extract features from two graphs respectively. Self-supervised\nloss is designed to maximize the agreement of the embeddings of the same node\nin the topology graph and the feature graph. Extensive experiments on real\ncitation networks and social networks demonstrate the superiority of our\nproposed SCRL over the state-of-the-art methods on semi-supervised node\nclassification task. Meanwhile, compared with its main competitors, SCRL is\nrather efficient.",
          "link": "http://arxiv.org/abs/2108.04822",
          "publishedOn": "2021-08-12T01:56:21.216Z",
          "wordCount": 617,
          "title": "Self-supervised Consensus Representation Learning for Attributed Graph. (arXiv:2108.04822v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>",
          "description": "Point cloud completion aims to predict a complete shape in high accuracy from\nits partial observation. However, previous methods usually suffered from\ndiscrete nature of point cloud and unstructured prediction of points in local\nregions, which makes it hard to reveal fine local geometric details on the\ncomplete shape. To resolve this issue, we propose SnowflakeNet with Snowflake\nPoint Deconvolution (SPD) to generate the complete point clouds. The\nSnowflakeNet models the generation of complete point clouds as the\nsnowflake-like growth of points in 3D space, where the child points are\nprogressively generated by splitting their parent points after each SPD. Our\ninsight of revealing detailed geometry is to introduce skip-transformer in SPD\nto learn point splitting patterns which can fit local regions the best.\nSkip-transformer leverages attention mechanism to summarize the splitting\npatterns used in the previous SPD layer to produce the splitting in the current\nSPD layer. The locally compact and structured point cloud generated by SPD is\nable to precisely capture the structure characteristic of 3D shape in local\npatches, which enables the network to predict highly detailed geometries, such\nas smooth regions, sharp edges and corners. Our experimental results outperform\nthe state-of-the-art point cloud completion methods under widely used\nbenchmarks. Code will be available at\nhttps://github.com/AllenXiangX/SnowflakeNet.",
          "link": "http://arxiv.org/abs/2108.04444",
          "publishedOn": "2021-08-11T01:55:24.416Z",
          "wordCount": 675,
          "title": "SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer. (arXiv:2108.04444v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingkai Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>",
          "description": "Jointly exploiting multiple different yet complementary domain information\nhas been proven to be an effective way to perform robust object tracking. This\npaper focuses on effectively representing and utilizing complementary features\nfrom the frame domain and event domain for boosting object tracking performance\nin challenge scenarios. Specifically, we propose Common Features Extractor\n(CFE) to learn potential common representations from the RGB domain and event\ndomain. For learning the unique features of the two domains, we utilize a\nUnique Extractor for Event (UEE) based on Spiking Neural Networks to extract\nedge cues in the event domain which may be missed in RGB in some challenging\nconditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional\nNeural Networks to extract texture and semantic information in RGB domain.\nExtensive experiments on standard RGB benchmark and real event tracking dataset\ndemonstrate the effectiveness of the proposed approach. We show our approach\noutperforms all compared state-of-the-art tracking algorithms and verify\nevent-based data is a powerful cue for tracking in challenging scenes.",
          "link": "http://arxiv.org/abs/2108.04521",
          "publishedOn": "2021-08-11T01:55:24.411Z",
          "wordCount": 612,
          "title": "Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking. (arXiv:2108.04521v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joung_S/0/1/0/all/0/1\">Sunghun Joung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Ig-Jae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>",
          "description": "We propose a novel framework for fine-grained object recognition that learns\nto recover object variation in 3D space from a single image, trained on an\nimage collection without using any ground-truth 3D annotation. We accomplish\nthis by representing an object as a composition of 3D shape and its appearance,\nwhile eliminating the effect of camera viewpoint, in a canonical configuration.\nUnlike conventional methods modeling spatial variation in 2D images only, our\nmethod is capable of reconfiguring the appearance feature in a canonical 3D\nspace, thus enabling the subsequent object classifier to be invariant under 3D\ngeometric variation. Our representation also allows us to go beyond existing\nmethods, by incorporating 3D shape variation as an additional cue for object\nrecognition. To learn the model without ground-truth 3D annotation, we deploy a\ndifferentiable renderer in an analysis-by-synthesis framework. By incorporating\n3D shape and appearance jointly in a deep representation, our method learns the\ndiscriminative representation of the object and achieves competitive\nperformance on fine-grained image recognition and vehicle re-identification. We\nalso demonstrate that the performance of 3D shape reconstruction is improved by\nlearning fine-grained shape deformation in a boosting manner.",
          "link": "http://arxiv.org/abs/2108.04628",
          "publishedOn": "2021-08-11T01:55:24.406Z",
          "wordCount": 628,
          "title": "Learning Canonical 3D Object Representation for Fine-Grained Recognition. (arXiv:2108.04628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Styles_O/0/1/0/all/0/1\">Olly Styles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>",
          "description": "We introduce the problem of multi-camera trajectory forecasting (MCTF), which\ninvolves predicting the trajectory of a moving object across a network of\ncameras. While multi-camera setups are widespread for applications such as\nsurveillance and traffic monitoring, existing trajectory forecasting methods\ntypically focus on single-camera trajectory forecasting (SCTF), limiting their\nuse for such applications. Furthermore, using a single camera limits the\nfield-of-view available, making long-term trajectory forecasting impossible. We\naddress these shortcomings of SCTF by developing an MCTF framework that\nsimultaneously uses all estimated relative object locations from several\nviewpoints and predicts the object's future location in all possible\nviewpoints. Our framework follows a Which-When-Where approach that predicts in\nwhich camera(s) the objects appear and when and where within the camera views\nthey appear. To this end, we propose the concept of trajectory tensors: a new\ntechnique to encode trajectories across multiple camera views and the\nassociated uncertainties. We develop several encoder-decoder MCTF models for\ntrajectory tensors and present extensive experiments on our own database\n(comprising 600 hours of video data from 15 camera views) created particularly\nfor the MCTF task. Results show that our trajectory tensor models outperform\ncoordinate trajectory-based MCTF models and existing SCTF methods adapted for\nMCTF. Code is available from: https://github.com/olly-styles/Trajectory-Tensors",
          "link": "http://arxiv.org/abs/2108.04694",
          "publishedOn": "2021-08-11T01:55:24.395Z",
          "wordCount": 639,
          "title": "Multi-Camera Trajectory Forecasting with Trajectory Tensors. (arXiv:2108.04694v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>",
          "description": "This paper proposes a novel model for recognizing images with composite\nattribute-object concepts, notably for composite concepts that are unseen\nduring model training. We aim to explore the three key properties required by\nthe task --- relation-aware, consistent, and decoupled --- to learn rich and\nrobust features for primitive concepts that compose attribute-object pairs. To\nthis end, we propose the Blocked Message Passing Network (BMP-Net). The model\nconsists of two modules. The concept module generates semantically meaningful\nfeatures for primitive concepts, whereas the visual module extracts visual\nfeatures for attributes and objects from input images. A message passing\nmechanism is used in the concept module to capture the relations between\nprimitive concepts. Furthermore, to prevent the model from being biased towards\nseen composite concepts and reduce the entanglement between attributes and\nobjects, we propose a blocking mechanism that equalizes the information\navailable to the model for both seen and unseen concepts. Extensive experiments\nand ablation studies on two benchmarks show the efficacy of the proposed model.",
          "link": "http://arxiv.org/abs/2108.04603",
          "publishedOn": "2021-08-11T01:55:24.388Z",
          "wordCount": 612,
          "title": "Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition. (arXiv:2108.04603v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1\">Ashild Kummen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1\">Ali Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1\">Teodora Ganeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qianying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1\">Robert Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1\">Chenuka Ratwatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1\">Emil Almazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1\">Sheena Visram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Andrew Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1\">Neil J Sebire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1\">Lee Stott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1\">Yvonne Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1\">Graham Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1\">Dean Mohamedally</a>",
          "description": "Touchless computer interaction has become an important consideration during\nthe COVID-19 pandemic period. Despite progress in machine learning and computer\nvision that allows for advanced gesture recognition, an integrated collection\nof such open-source methods and a user-customisable approach to utilising them\nin a low-cost solution for touchless interaction in existing software is still\nmissing. In this paper, we introduce the MotionInput v2.0 application. This\napplication utilises published open-source libraries and additional gesture\ndefinitions developed to take the video stream from a standard RGB webcam as\ninput. It then maps human motion gestures to input operations for existing\napplications and games. The user can choose their own preferred way of\ninteracting from a series of motion types, including single and bi-modal hand\ngesturing, full-body repetitive or extremities-based exercises, head and facial\nmovements, eye tracking, and combinations of the above. We also introduce a\nseries of bespoke gesture recognition classifications as DirectInput triggers,\nincluding gestures for idle states, auto calibration, depth capture from a 2D\nRGB webcam stream and tracking of facial motions such as mouth motions,\nwinking, and head direction with rotation. Three use case areas assisted the\ndevelopment of the modules: creativity software, office and clinical software,\nand gaming software. A collection of open-source libraries has been integrated\nand provide a layer of modular gesture mapping on top of existing mouse and\nkeyboard controls in Windows via DirectX. With ease of access to webcams\nintegrated into most laptops and desktop computers, touchless computing becomes\nmore available with MotionInput v2.0, in a federated and locally processed\nmethod.",
          "link": "http://arxiv.org/abs/2108.04357",
          "publishedOn": "2021-08-11T01:55:24.373Z",
          "wordCount": 818,
          "title": "MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam. (arXiv:2108.04357v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1\">Jun Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_P/0/1/0/all/0/1\">Peng Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Junhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Classical close-set semantic segmentation networks have limited ability to\ndetect out-of-distribution (OOD) objects, which is important for\nsafety-critical applications such as autonomous driving. Incrementally learning\nthese OOD objects with few annotations is an ideal way to enlarge the knowledge\nbase of the deep learning models. In this paper, we propose an open world\nsemantic segmentation system that includes two modules: (1) an open-set\nsemantic segmentation module to detect both in-distribution and OOD objects.\n(2) an incremental few-shot learning module to gradually incorporate those OOD\nobjects into its existing knowledge base. This open world semantic segmentation\nsystem behaves like a human being, which is able to identify OOD objects and\ngradually learn them with corresponding supervision. We adopt the Deep Metric\nLearning Network (DMLNet) with contrastive clustering to implement open-set\nsemantic segmentation. Compared to other open-set semantic segmentation\nmethods, our DMLNet achieves state-of-the-art performance on three challenging\nopen-set semantic segmentation datasets without using additional data or\ngenerative models. On this basis, two incremental few-shot learning methods are\nfurther proposed to progressively improve the DMLNet with the annotations of\nOOD objects.",
          "link": "http://arxiv.org/abs/2108.04562",
          "publishedOn": "2021-08-11T01:55:24.366Z",
          "wordCount": 621,
          "title": "Deep Metric Learning for Open World Semantic Segmentation. (arXiv:2108.04562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiaopeng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dehao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ran Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "This report describes Megvii-3D team's approach towards CVPR 2021 Image\nMatching Workshop.",
          "link": "http://arxiv.org/abs/2108.04453",
          "publishedOn": "2021-08-11T01:55:24.361Z",
          "wordCount": 458,
          "title": "Method Towards CVPR 2021 Image Matching Challenge. (arXiv:2108.04453v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaoda Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiantao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weibing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Current 3D single object tracking approaches track the target based on a\nfeature comparison between the target template and the search area. However,\ndue to the common occlusion in LiDAR scans, it is non-trivial to conduct\naccurate feature comparisons on severe sparse and incomplete shapes. In this\nwork, we exploit the ground truth bounding box given in the first frame as a\nstrong cue to enhance the feature description of the target object, enabling a\nmore accurate feature comparison in a simple yet effective way. In particular,\nwe first propose the BoxCloud, an informative and robust representation, to\ndepict an object using the point-to-box relation. We further design an\nefficient box-aware feature fusion module, which leverages the aforementioned\nBoxCloud for reliable feature matching and embedding. Integrating the proposed\ngeneral components into an existing model P2B, we construct a superior\nbox-aware tracker (BAT). Experiments confirm that our proposed BAT outperforms\nthe previous state-of-the-art by a large margin on both KITTI and NuScenes\nbenchmarks, achieving a 12.8% improvement in terms of precision while running\n~20% faster.",
          "link": "http://arxiv.org/abs/2108.04728",
          "publishedOn": "2021-08-11T01:55:24.352Z",
          "wordCount": 625,
          "title": "Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds. (arXiv:2108.04728v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Botach_A/0/1/0/all/0/1\">Adam Botach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_Y/0/1/0/all/0/1\">Yuri Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_Y/0/1/0/all/0/1\">Yakov Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapiro_Y/0/1/0/all/0/1\">Yoel Shapiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>",
          "description": "We introduce BIDCD - the Bosch Industrial Depth Completion Dataset. BIDCD is\na new RGBD dataset of metallic industrial objects, collected with a depth\ncamera mounted on a robotic manipulator. The main purpose of this dataset is to\nfacilitate the training of domain-specific depth completion models, to be used\nin logistics and manufacturing tasks. We trained a State-of-the-Art depth\ncompletion model on this dataset, and report the results, setting an initial\nbenchmark.",
          "link": "http://arxiv.org/abs/2108.04706",
          "publishedOn": "2021-08-11T01:55:24.347Z",
          "wordCount": 511,
          "title": "BIDCD - Bosch Industrial Depth Completion Dataset. (arXiv:2108.04706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1\">Harald K&#xf6;stler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1\">Marco Heisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1\">Patrick Krauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seung Hee Yang</a>",
          "description": "In this article, we perform a review of the state-of-the-art of hybrid\nmachine learning in medical imaging. We start with a short summary of the\ngeneral developments of the past in machine learning and how general and\nspecialized approaches have been in competition in the past decades. A\nparticular focus will be the theoretical and experimental evidence pro and\ncontra hybrid modelling. Next, we inspect several new developments regarding\nhybrid machine learning with a particular focus on so-called known operator\nlearning and how hybrid approaches gain more and more momentum across\nessentially all applications in medical imaging and medical image analysis. As\nwe will point out by numerous examples, hybrid models are taking over in image\nreconstruction and analysis. Even domains such as physical simulation and\nscanner and acquisition design are being addressed using machine learning grey\nbox modelling approaches. Towards the end of the article, we will investigate a\nfew future directions and point out relevant areas in which hybrid modelling,\nmeta learning, and other domains will likely be able to drive the\nstate-of-the-art ahead.",
          "link": "http://arxiv.org/abs/2108.04543",
          "publishedOn": "2021-08-11T01:55:24.342Z",
          "wordCount": 657,
          "title": "Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future. (arXiv:2108.04543v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04663",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thapa_A/0/1/0/all/0/1\">Ashu Thapa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alsadoon_A/0/1/0/all/0/1\">Abeer Alsadoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_P/0/1/0/all/0/1\">P.W.C. Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajaj_S/0/1/0/all/0/1\">Simi Bajaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alsadoon_O/0/1/0/all/0/1\">Omar Hisham Alsadoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1\">Tarik A. Rashid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_R/0/1/0/all/0/1\">Rasha S. Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jerew_O/0/1/0/all/0/1\">Oday D. Jerew</a>",
          "description": "Background and Aim: Recently, deep learning using convolutional neural\nnetwork has been used successfully to classify the images of breast cells\naccurately. However, the accuracy of manual classification of those\nhistopathological images is comparatively low. This research aims to increase\nthe accuracy of the classification of breast cancer images by utilizing a\nPatch-Based Classifier (PBC) along with deep learning architecture.\nMethodology: The proposed system consists of a Deep Convolutional Neural\nNetwork (DCNN) that helps in enhancing and increasing the accuracy of the\nclassification process. This is done by the use of the Patch-based Classifier\n(PBC). CNN has completely different layers where images are first fed through\nconvolutional layers using hyperbolic tangent function together with the\nmax-pooling layer, drop out layers, and SoftMax function for classification.\nFurther, the output obtained is fed to a patch-based classifier that consists\nof patch-wise classification output followed by majority voting. Results: The\nresults are obtained throughout the classification stage for breast cancer\nimages that are collected from breast-histology datasets. The proposed solution\nimproves the accuracy of classification whether or not the images had normal,\nbenign, in-situ, or invasive carcinoma from 87% to 94% with a decrease in\nprocessing time from 0.45 s to 0.2s on average. Conclusion: The proposed\nsolution focused on increasing the accuracy of classifying cancer in the breast\nby enhancing the image contrast and reducing the vanishing gradient. Finally,\nthis solution for the implementation of the Contrast Limited Adaptive Histogram\nEqualization (CLAHE) technique and modified tangent function helps in\nincreasing the accuracy.",
          "link": "http://arxiv.org/abs/2108.04663",
          "publishedOn": "2021-08-11T01:55:24.326Z",
          "wordCount": 723,
          "title": "Deep Learning for Breast Cancer Classification: Enhanced Tangent Function. (arXiv:2108.04663v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Po-Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_B/0/1/0/all/0/1\">Bing-Chen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wen-Feng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke-Jyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Understanding and comprehending video content is crucial for many real-world\napplications such as search and recommendation systems. While recent progress\nof deep learning has boosted performance on various tasks using visual cues,\ndeep cognition to reason intentions, motivation, or causality remains\nchallenging. Existing datasets that aim to examine video reasoning capability\nfocus on visual signals such as actions, objects, relations, or could be\nanswered utilizing text bias. Observing this, we propose a novel task, along\nwith a new dataset: Trope Understanding in Movies and Animations (TrUMAn),\nintending to evaluate and develop learning systems beyond visual signals.\nTropes are frequently used storytelling devices for creative works. By coping\nwith the trope understanding task and enabling the deep cognition skills of\nmachines, we are optimistic that data mining applications and algorithms could\nbe taken to the next level. To tackle the challenging TrUMAn dataset, we\npresent a Trope Understanding and Storytelling (TrUSt) with a new Conceptual\nStoryteller module, which guides the video encoder by performing video\nstorytelling on a latent space. The generated story embedding is then fed into\nthe trope understanding model to provide further signals. Experimental results\ndemonstrate that state-of-the-art learning systems on existing tasks reach only\n12.01% of accuracy with raw input signals. Also, even in the oracle case with\nhuman-annotated descriptions, BERT contextual embedding achieves at most 28% of\naccuracy. Our proposed TrUSt boosts the model performance and reaches 13.94%\nperformance. We also provide detailed analysis topave the way for future\nresearch. TrUMAn is publicly available\nat:https://www.cmlab.csie.ntu.edu.tw/project/trope",
          "link": "http://arxiv.org/abs/2108.04542",
          "publishedOn": "2021-08-11T01:55:24.283Z",
          "wordCount": 702,
          "title": "TrUMAn: Trope Understanding in Movies and Animations. (arXiv:2108.04542v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>",
          "description": "In this work, deep learning models are applied to a segment of a robust\nhand-washing dataset that has been created with the help of 30 volunteers. This\nwork demonstrates the classification of presence of one hand, two hands and no\nhand in the scene based on transfer learning. The pre-trained model; simplest\nNN from Keras library is utilized to train the network with 704 images of hand\ngestures and the predictions are carried out for the input image. Due to the\ncontrolled and restricted dataset, 100% accuracy is achieved during the\ntraining with correct predictions for the input image. Complete handwashing\ndataset with dense models such as AlexNet for video classification for hand\nhygiene stages will be used in the future work.",
          "link": "http://arxiv.org/abs/2108.04529",
          "publishedOn": "2021-08-11T01:55:24.277Z",
          "wordCount": 548,
          "title": "Hand Pose Classification Based on Neural Networks. (arXiv:2108.04529v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kemiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1\">Qi Hao</a>",
          "description": "Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results\nof object detection, affinity computation and data association in real time.\nThis paper presents an efficient multi-modal MOT framework with online joint\ndetection and tracking schemes and robust data association for autonomous\ndriving applications. The novelty of this work includes: (1) development of an\nend-to-end deep neural network for joint object detection and correlation using\n2D and 3D measurements; (2) development of a robust affinity computation module\nto compute occlusion-aware appearance and motion affinities in 3D space; (3)\ndevelopment of a comprehensive data association module for joint optimization\namong detection confidences, affinities and start-end probabilities. The\nexperiment results on the KITTI tracking benchmark demonstrate the superior\nperformance of the proposed method in terms of both tracking accuracy and\nprocessing speed.",
          "link": "http://arxiv.org/abs/2108.04602",
          "publishedOn": "2021-08-11T01:55:24.272Z",
          "wordCount": 571,
          "title": "Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for Autonomous Driving. (arXiv:2108.04602v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_B/0/1/0/all/0/1\">Boseung Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jicheol Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>",
          "description": "Attribute-based person search is the task of finding person images that are\nbest matched with a set of text attributes given as query. The main challenge\nof this task is the large modality gap between attributes and images. To reduce\nthe gap, we present a new loss for learning cross-modal embeddings in the\ncontext of attribute-based person search. We regard a set of attributes as a\ncategory of people sharing the same traits. In a joint embedding space of the\ntwo modalities, our loss pulls images close to their person categories for\nmodality alignment. More importantly, it pushes apart a pair of person\ncategories by a margin determined adaptively by their semantic distance, where\nthe distance metric is learned end-to-end so that the loss considers importance\nof each attribute when relating person categories. Our loss guided by the\nadaptive semantic margin leads to more discriminative and semantically\nwell-arranged distributions of person images. As a consequence, it enables a\nsimple embedding model to achieve state-of-the-art records on public benchmarks\nwithout bells and whistles.",
          "link": "http://arxiv.org/abs/2108.04533",
          "publishedOn": "2021-08-11T01:55:24.256Z",
          "wordCount": 613,
          "title": "ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer. (arXiv:2108.04533v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Praveen_N/0/1/0/all/0/1\">Nalla Praveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "Machine learning has endless applications in the health care industry. White\nblood cell classification is one of the interesting and promising area of\nresearch. The classification of the white blood cells plays an important part\nin the medical diagnosis. In practise white blood cell classification is\nperformed by the haematologist by taking a small smear of blood and careful\nexamination under the microscope. The current procedures to identify the white\nblood cell subtype is more time taking and error-prone. The computer aided\ndetection and diagnosis of the white blood cells tend to avoid the human error\nand reduce the time taken to classify the white blood cells. In the recent\nyears several deep learning approaches have been developed in the context of\nclassification of the white blood cells that are able to identify but are\nunable to localize the positions of white blood cells in the blood cell image.\nFollowing this, the present research proposes to utilize YOLOv3 object\ndetection technique to localize and classify the white blood cells with\nbounding boxes. With exhaustive experimental analysis, the proposed work is\nfound to detect the white blood cell with 99.2% accuracy and classify with 90%\naccuracy.",
          "link": "http://arxiv.org/abs/2108.04614",
          "publishedOn": "2021-08-11T01:55:24.249Z",
          "wordCount": 631,
          "title": "White blood cell subtype detection and classification. (arXiv:2108.04614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sodani_A/0/1/0/all/0/1\">Abhigya Sodani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1\">Michael Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Researchers often spend weeks sifting through decades of unlabeled satellite\nimagery(on NASA Worldview) in order to develop datasets on which they can start\nconducting research. We developed an interactive, scalable and fast image\nsimilarity search engine (which can take one or more images as the query image)\nthat automatically sifts through the unlabeled dataset reducing dataset\ngeneration time from weeks to minutes. In this work, we describe key components\nof the end to end pipeline. Our similarity search system was created to be able\nto identify similar images from a potentially petabyte scale database that are\nsimilar to an input image, and for this we had to break down each query image\ninto its features, which were generated by a classification layer stripped CNN\ntrained in a supervised manner. To store and search these features efficiently,\nwe had to make several scalability improvements. To improve the speed, reduce\nthe storage, and shrink memory requirements for embedding search, we add a\nfully connected layer to our CNN make all images into a 128 length vector\nbefore entering the classification layers. This helped us compress the size of\nour image features from 2048 (for ResNet, which was initially tried as our\nfeaturizer) to 128 for our new custom model. Additionally, we utilize existing\napproximate nearest neighbor search libraries to significantly speed up\nembedding search. Our system currently searches over our entire database of\nimages at 5 seconds per query on a single virtual machine in the cloud. In the\nfuture, we would like to incorporate a SimCLR based featurizing model which\ncould be trained without any labelling by a human (since the classification\naspect of the model is irrelevant to this use case).",
          "link": "http://arxiv.org/abs/2108.04479",
          "publishedOn": "2021-08-11T01:55:24.244Z",
          "wordCount": 730,
          "title": "Scalable Reverse Image Search Engine for NASAWorldview. (arXiv:2108.04479v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngsaeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">David Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>",
          "description": "Anomaly detection in video streams is a challengingproblem because of the\nscarcity of abnormal events andthe difficulty of accurately annotating them.To\nallevi-ate these issues, unsupervised learning-based predictionmethods have\nbeen previously applied. These approachestrain the model with only normal\nevents and predict a fu-ture frame from a sequence of preceding frames by use\nofencoder-decoder architectures so that they result in smallprediction errors\non normal events but large errors on ab-normal events. The architecture,\nhowever, comes with thecomputational burden as some anomaly detection tasks\nre-quire low computational cost without sacrificing perfor-mance. In this\npaper, Cross-Parallel Network (CPNet) forefficient anomaly detection is\nproposed here to minimizecomputations without performance drops. It consists\nofNsmaller parallel U-Net, each of which is designed to handlea single input\nframe, to make the calculations significantlymore efficient. Additionally, an\ninter-network shift moduleis incorporated to capture temporal relationships\namong se-quential frames to enable more accurate future predictions.The\nquantitative results show that our model requires lesscomputational cost than\nthe baseline U-Net while deliver-ing equivalent performance in anomaly\ndetection.",
          "link": "http://arxiv.org/abs/2108.04454",
          "publishedOn": "2021-08-11T01:55:24.238Z",
          "wordCount": 617,
          "title": "CPNet: Cross-Parallel Network for Efficient Anomaly Detection. (arXiv:2108.04454v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Ting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jizhong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>",
          "description": "Iterative self-consistent parallel imaging reconstruction (SPIRiT) is an\neffective self-calibrated reconstruction model for parallel magnetic resonance\nimaging (PMRI). The joint L1 norm of wavelet coefficients and joint total\nvariation (TV) regularization terms are incorporated into the SPIRiT model to\nimprove the reconstruction performance. The simultaneous two-directional\nlow-rankness (STDLR) in k-space data is incorporated into SPIRiT to realize\nimproved reconstruction. Recent methods have exploited the nonlocal\nself-similarity (NSS) of images by imposing nonlocal low-rankness of similar\npatches to achieve a superior performance. To fully utilize both the NSS in\nMagnetic resonance (MR) images and calibration consistency in the k-space\ndomain, we propose a nonlocal low-rank (NLR)-SPIRiT model by incorporating NLR\nregularization into the SPIRiT model. We apply the weighted nuclear norm (WNN)\nas a surrogate of the rank and employ the Nash equilibrium (NE) formulation and\nalternating direction method of multipliers (ADMM) to efficiently solve the\nNLR-SPIRiT model. The experimental results demonstrate the superior performance\nof NLR-SPIRiT over the state-of-the-art methods via three objective metrics and\nvisual comparison.",
          "link": "http://arxiv.org/abs/2108.04517",
          "publishedOn": "2021-08-11T01:55:24.233Z",
          "wordCount": 616,
          "title": "Iterative Self-consistent Parallel Magnetic Resonance Imaging Reconstruction based on Nonlocal Low-Rank Regularization. (arXiv:2108.04517v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1\">Balagopal Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1\">Shafa Balaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan Sheng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1\">Pavitra Krishnaswamy</a>",
          "description": "Deep learning models achieve strong performance for radiology image\nclassification, but their practical application is bottlenecked by the need for\nlarge labeled training datasets. Semi-supervised learning (SSL) approaches\nleverage small labeled datasets alongside larger unlabeled datasets and offer\npotential for reducing labeling cost. In this work, we introduce NoTeacher, a\nnovel consistency-based SSL framework which incorporates probabilistic\ngraphical models. Unlike Mean Teacher which maintains a teacher network updated\nvia a temporal ensemble, NoTeacher employs two independent networks, thereby\neliminating the need for a teacher network. We demonstrate how NoTeacher can be\ncustomized to handle a range of challenges in radiology image classification.\nSpecifically, we describe adaptations for scenarios with 2D and 3D inputs, uni\nand multi-label classification, and class distribution mismatch between labeled\nand unlabeled portions of the training data. In realistic empirical evaluations\non three public benchmark datasets spanning the workhorse modalities of\nradiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the\nfully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher\noutperforms established SSL methods with minimal hyperparameter tuning, and has\nimplications as a principled and practical option for semisupervised learning\nin radiology applications.",
          "link": "http://arxiv.org/abs/2108.04423",
          "publishedOn": "2021-08-11T01:55:24.226Z",
          "wordCount": 669,
          "title": "Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean. (arXiv:2108.04423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Sujuan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>",
          "description": "Food logo detection plays an important role in the multimedia for its wide\nreal-world applications, such as food recommendation of the self-service shop\nand infringement detection on e-commerce platforms. A large-scale food logo\ndataset is urgently needed for developing advanced food logo detection\nalgorithms. However, there are no available food logo datasets with food brand\ninformation. To support efforts towards food logo detection, we introduce the\ndataset FoodLogoDet-1500, a new large-scale publicly available food logo\ndataset, which has 1,500 categories, about 100,000 images and about 150,000\nmanually annotated food logo objects. We describe the collection and annotation\nprocess of FoodLogoDet-1500, analyze its scale and diversity, and compare it\nwith other logo datasets. To the best of our knowledge, FoodLogoDet-1500 is the\nfirst largest publicly available high-quality dataset for food logo detection.\nThe challenge of food logo detection lies in the large-scale categories and\nsimilarities between food logo categories. For that, we propose a novel food\nlogo detection method Multi-scale Feature Decoupling Network (MFDNet), which\ndecouples classification and regression into two branches and focuses on the\nclassification branch to solve the problem of distinguishing multiple food logo\ncategories. Specifically, we introduce the feature offset module, which\nutilizes the deformation-learning for optimal classification offset and can\neffectively obtain the most representative features of classification in\ndetection. In addition, we adopt a balanced feature pyramid in MFDNet, which\npays attention to global information, balances the multi-scale feature maps,\nand enhances feature extraction capability. Comprehensive experiments on\nFoodLogoDet-1500 and other two benchmark logo datasets demonstrate the\neffectiveness of the proposed method. The FoodLogoDet-1500 can be found at this\nhttps URL.",
          "link": "http://arxiv.org/abs/2108.04644",
          "publishedOn": "2021-08-11T01:55:24.211Z",
          "wordCount": 733,
          "title": "FoodLogoDet-1500: A Dataset for Large-Scale Food Logo Detection via Multi-Scale Feature Decoupling Network. (arXiv:2108.04644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.06056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thummel_M/0/1/0/all/0/1\">Martin Th&#xfc;mmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sickert_S/0/1/0/all/0/1\">Sven Sickert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "The human face has a high potential for biometric identification due to its\nmany individual traits. At the same time, such identification is vulnerable to\nbiometric copies. These presentation attacks pose a great challenge in\nunsupervised authentication settings. As a countermeasure, we propose a method\nthat automatically analyzes the plausibility of facial behavior based on a\nsequence of 3D face scans. A compact feature representation measures facial\nbehavior using the temporal curvature change. Finally, we train our method only\non genuine faces in an anomaly detection scenario. Our method can detect\npresentation attacks using elastic 3D masks, bent photographs with eye holes,\nand monitor replay-attacks. For evaluation, we recorded a challenging database\ncontaining such cases using a high-quality 3D sensor. It features 109 4D face\nscans including eleven different types of presentation attacks. We achieve\nerror rates of 11% and 6% for APCER and BPCER, respectively.",
          "link": "http://arxiv.org/abs/1910.06056",
          "publishedOn": "2021-08-11T01:55:24.204Z",
          "wordCount": 644,
          "title": "Facial Behavior Analysis using 4D Curvature Statistics for Presentation Attack Detection. (arXiv:1910.06056v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tailin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "The task of skeleton-based action recognition remains a core challenge in\nhuman-centred scene understanding due to the multiple granularities and large\nvariation in human motion. Existing approaches typically employ a single neural\nrepresentation for different motion patterns, which has difficulty in capturing\nfine-grained action classes given limited training data. To address the\naforementioned problems, we propose a novel multi-granular spatio-temporal\ngraph network for skeleton-based action classification that jointly models the\ncoarse- and fine-grained skeleton motion patterns. To this end, we develop a\ndual-head graph network consisting of two interleaved branches, which enables\nus to extract features at two spatio-temporal resolutions in an effective and\nefficient manner. Moreover, our network utilises a cross-head communication\nstrategy to mutually enhance the representations of both heads. We conducted\nextensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU\nRGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance\non all the benchmarks, which validates the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.04536",
          "publishedOn": "2021-08-11T01:55:24.199Z",
          "wordCount": 607,
          "title": "Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition. (arXiv:2108.04536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.14442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">William B. Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1\">Priya Kasimbeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1\">Alexander Toshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present Interactive Gibson Benchmark, the first comprehensive benchmark\nfor training and evaluating Interactive Navigation: robot navigation strategies\nwhere physical interaction with objects is allowed and even encouraged to\naccomplish a task. For example, the robot can move objects if needed in order\nto clear a path leading to the goal location. Our benchmark comprises two novel\nelements: 1) a new experimental setup, the Interactive Gibson Environment\n(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high\nfidelity physical dynamics of the robot and common objects found in these\nscenes; 2) a set of Interactive Navigation metrics which allows one to study\nthe interplay between navigation and physical interaction. We present and\nevaluate multiple learning-based baselines in Interactive Gibson, and provide\ninsights into regimes of navigation with different trade-offs between\nnavigation path efficiency and disturbance of surrounding objects. We make our\nbenchmark publicly\navailable(https://sites.google.com/view/interactivegibsonenv) and encourage\nresearchers from all disciplines in robotics (e.g. planning, learning, control)\nto propose, evaluate, and compare their Interactive Navigation solutions in\nInteractive Gibson.",
          "link": "http://arxiv.org/abs/1910.14442",
          "publishedOn": "2021-08-11T01:55:24.187Z",
          "wordCount": 707,
          "title": "Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments. (arXiv:1910.14442v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1\">Aishwarza Panday</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Muhammad Ashad Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1\">Nihad Karim Chowdhury</a>",
          "description": "Due to the limited availability and high cost of the reverse\ntranscription-polymerase chain reaction (RT-PCR) test, many studies have\nproposed machine learning techniques for detecting COVID-19 from medical\nimaging. The purpose of this study is to systematically review, assess, and\nsynthesize research articles that have used different machine learning\ntechniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.\nA structured literature search was conducted in the relevant bibliographic\ndatabases to ensure that the survey solely centered on reproducible and\nhigh-quality research. We selected papers based on our inclusion criteria. In\nthis survey, we reviewed $98$ articles that fulfilled our inclusion criteria.\nWe have surveyed a complete pipeline of chest imaging analysis techniques\nrelated to COVID-19, including data collection, pre-processing, feature\nextraction, classification, and visualization. We have considered CT scans and\nX-rays as both are widely used to describe the latest developments in medical\nimaging to detect COVID-19. This survey provides researchers with valuable\ninsights into different machine learning techniques and their performance in\nthe detection and diagnosis of COVID-19 from chest imaging. At the end, the\nchallenges and limitations in detecting COVID-19 using machine learning\ntechniques and the future direction of research are discussed.",
          "link": "http://arxiv.org/abs/2108.04344",
          "publishedOn": "2021-08-11T01:55:24.170Z",
          "wordCount": 714,
          "title": "A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging. (arXiv:2108.04344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1\">Benjamin Stadnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1\">Jan Witowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1\">Vishwaesh Rajiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>",
          "description": "Artificial intelligence (AI) is transforming medicine and showing promise in\nimproving clinical diagnosis. In breast cancer screening, several recent\nstudies show that AI has the potential to improve radiologists' accuracy,\nsubsequently helping in early cancer diagnosis and reducing unnecessary workup.\nAs the number of proposed models and their complexity grows, it is becoming\nincreasingly difficult to re-implement them in order to reproduce the results\nand to compare different approaches. To enable reproducibility of research in\nthis application area and to enable comparison between different methods, we\nrelease a meta-repository containing deep learning models for classification of\nscreening mammograms. This meta-repository creates a framework that enables the\nevaluation of machine learning models on any private or public screening\nmammography data set. At its inception, our meta-repository contains five\nstate-of-the-art models with open-source implementations and cross-platform\ncompatibility. We compare their performance on five international data sets:\ntwo private New York University breast cancer screening data sets as well as\nthree public (DDSM, INbreast and Chinese Mammography Database) data sets. Our\nframework has a flexible design that can be generalized to other medical image\nanalysis tasks. The meta-repository is available at\nhttps://www.github.com/nyukat/mammography_metarepository.",
          "link": "http://arxiv.org/abs/2108.04800",
          "publishedOn": "2021-08-11T01:55:24.165Z",
          "wordCount": 641,
          "title": "Meta-repository of screening mammography classifiers. (arXiv:2108.04800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1\">Jesper Kers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1\">Clarissa A. Cassol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1\">Joris J. Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1\">Najia Idrees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1\">Alik Farber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1\">Samir Haroon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1\">Kevin P. Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Suvranu Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1\">Vipul C. Chitalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1\">Vijaya B. Kolachalama</a>",
          "description": "Development of deep learning systems for biomedical segmentation often\nrequires access to expert-driven, manually annotated datasets. If more than a\nsingle expert is involved in the annotation of the same images, then the\ninter-expert agreement is not necessarily perfect, and no single expert\nannotation can precisely capture the so-called ground truth of the regions of\ninterest on all images. Also, it is not trivial to generate a reference\nestimate using annotations from multiple experts. Here we present a deep neural\nnetwork, defined as U-Net-and-a-half, which can simultaneously learn from\nannotations performed by multiple experts on the same set of images.\nU-Net-and-a-half contains a convolutional encoder to generate features from the\ninput images, multiple decoders that allow simultaneous learning from image\nmasks obtained from annotations that were independently generated by multiple\nexperts, and a shared low-dimensional feature space. To demonstrate the\napplicability of our framework, we used two distinct datasets from digital\npathology and radiology, respectively. Specifically, we trained two separate\nmodels using pathologist-driven annotations of glomeruli on whole slide images\nof human kidney biopsies (10 patients), and radiologist-driven annotations of\nlumen cross-sections of human arteriovenous fistulae obtained from\nintravascular ultrasound images (10 patients), respectively. The models based\non U-Net-and-a-half exceeded the performance of the traditional U-Net models\ntrained on single expert annotations alone, thus expanding the scope of\nmultitask learning in the context of biomedical image segmentation.",
          "link": "http://arxiv.org/abs/2108.04658",
          "publishedOn": "2021-08-11T01:55:24.159Z",
          "wordCount": 697,
          "title": "U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations. (arXiv:2108.04658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04327",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1\">Karol Mikula</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1\">Michal Kollar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1\">Aneta A. Ozvat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1\">Martin Ambroz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1\">Lucia Cahojova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1\">Ivan Jarolimek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1\">Jozef Sibik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1\">Maria Sibikova</a>",
          "description": "Natural numerical networks are introduced as a new classification algorithm\nbased on the numerical solution of nonlinear partial differential equations of\nforward-backward diffusion type on complete graphs. The proposed natural\nnumerical network is applied to open important environmental and nature\nconservation task, the automated identification of protected habitats by using\nsatellite images. In the natural numerical network, the forward diffusion\ncauses the movement of points in a feature space toward each other. The\nopposite effect, keeping the points away from each other, is caused by backward\ndiffusion. This yields the desired classification. The natural numerical\nnetwork contains a few parameters that are optimized in the learning phase of\nthe method. After learning parameters and optimizing the topology of the\nnetwork graph, classification necessary for habitat identification is\nperformed. A relevancy map for each habitat is introduced as a tool for\nvalidating the classification and finding new Natura 2000 habitat appearances.",
          "link": "http://arxiv.org/abs/2108.04327",
          "publishedOn": "2021-08-11T01:55:24.154Z",
          "wordCount": 615,
          "title": "Natural Numerical Networks for Natura 2000 habitats classification by satellite images. (arXiv:2108.04327v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1\">Vasileios Baltatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1\">Loic Le Folgoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1\">Sam Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1\">Octavio E. Martinez Manzanera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1\">Kyriaki-Margarita Bintsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Arjun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sujal Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>",
          "description": "Convolutional Neural Networks (CNNs) are widely used for image classification\nin a variety of fields, including medical imaging. While most studies deploy\ncross-entropy as the loss function in such tasks, a growing number of\napproaches have turned to a family of contrastive learning-based losses. Even\nthough performance metrics such as accuracy, sensitivity and specificity are\nregularly used for the evaluation of CNN classifiers, the features that these\nclassifiers actually learn are rarely identified and their effect on the\nclassification performance on out-of-distribution test samples is\ninsufficiently explored. In this paper, motivated by the real-world task of\nlung nodule classification, we investigate the features that a CNN learns when\ntrained and tested on different distributions of a synthetic dataset with\ncontrolled modes of variation. We show that different loss functions lead to\ndifferent features being learned and consequently affect the generalization\nability of the classifier on unseen data. This study provides some important\ninsights into the design of deep learning solutions for medical imaging tasks.",
          "link": "http://arxiv.org/abs/2108.04815",
          "publishedOn": "2021-08-11T01:55:24.133Z",
          "wordCount": 632,
          "title": "The Effect of the Loss on Generalization: Empirical Study on Synthetic Lung Nodule Data. (arXiv:2108.04815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1\">Stefano Gasperini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Patrick Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1\">Vinzenz Dallabetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "While self-supervised monocular depth estimation in driving scenarios has\nachieved comparable performance to supervised approaches, violations of the\nstatic world assumption can still lead to erroneous depth predictions of\ntraffic participants, posing a potential safety issue. In this paper, we\npresent R4Dyn, a novel set of techniques to use cost-efficient radar data on\ntop of a self-supervised depth estimation framework. In particular, we show how\nradar can be used during training as weak supervision signal, as well as an\nextra input to enhance the estimation robustness at inference time. Since\nautomotive radars are readily available, this allows to collect training data\nfrom a variety of existing vehicles. Moreover, by filtering and expanding the\nsignal to make it compatible with learning-based approaches, we address radar\ninherent issues, such as noise and sparsity. With R4Dyn we are able to overcome\na major limitation of self-supervised depth estimation, i.e. the prediction of\ntraffic participants. We substantially improve the estimation on dynamic\nobjects, such as cars by 37% on the challenging nuScenes dataset, hence\ndemonstrating that radar is a valuable additional sensor for monocular depth\nestimation in autonomous vehicles. Additionally, we plan on making the code\npublicly available.",
          "link": "http://arxiv.org/abs/2108.04814",
          "publishedOn": "2021-08-11T01:55:24.124Z",
          "wordCount": 651,
          "title": "R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes. (arXiv:2108.04814v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.09674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Gait recognition is widely used in social security applications due to its\nadvantages in long-distance human identification. Recently, sequence-based\nmethods have achieved high accuracy by learning abundant temporal and spatial\ninformation. However, their robustness under adversarial attacks has not been\nclearly explored. In this paper, we demonstrate that the state-of-the-art gait\nrecognition model is vulnerable to such attacks. To this end, we propose a\nnovel temporal sparse adversarial attack method. Different from previous\nadditive noise models which add perturbations on original samples, we employ a\ngenerative adversarial network based architecture to semantically generate\nadversarial high-quality gait silhouettes or video frames. Moreover, by\nsparsely substituting or inserting a few adversarial gait silhouettes, the\nproposed method ensures its imperceptibility and achieves a high attack success\nrate. The experimental results show that if only one-fortieth of the frames are\nattacked, the accuracy of the target model drops dramatically.",
          "link": "http://arxiv.org/abs/2002.09674",
          "publishedOn": "2021-08-11T01:55:24.113Z",
          "wordCount": 618,
          "title": "Temporal Sparse Adversarial Attack on Sequence-based Gait Recognition. (arXiv:2002.09674v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rabbi_M/0/1/0/all/0/1\">Md Fazle Rabbi</a>",
          "description": "Derivative compressive sampling (DCS) is a signal reconstruction method from\nmeasurements of the spatial gradient with sub-Nyquist sampling rate.\nApplications of DCS include optical image reconstruction, photometric stereo,\nand shape-from-shading. In this work, we study the sensitivity of DCS with\nrespect to algorithmic hyperparameters using a brute-force search algorithm. We\nperform experiments on a dataset of surface images and deduce guidelines for\nthe user to setup values for the hyperparameters for improved signal recovery\nperformance.",
          "link": "http://arxiv.org/abs/2108.04355",
          "publishedOn": "2021-08-11T01:55:24.107Z",
          "wordCount": 502,
          "title": "Hyperparameter Analysis for Derivative Compressive Sampling. (arXiv:2108.04355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this paper, we present a new network named Attention Aware Network (AASeg)\nfor real time semantic image segmentation. Our network incorporates spatial and\nchannel information using Spatial Attention (SA) and Channel Attention (CA)\nmodules respectively. It also uses dense local multi-scale context information\nusing Multi Scale Context (MSC) module. The feature maps are concatenated\nindividually to produce the final segmentation map. We demonstrate the\neffectiveness of our method using a comprehensive analysis, quantitative\nexperimental results and ablation study using Cityscapes, ADE20K and Camvid\ndatasets. Our network performs better than most previous architectures with a\n74.4\\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.",
          "link": "http://arxiv.org/abs/2108.04349",
          "publishedOn": "2021-08-11T01:55:24.089Z",
          "wordCount": 556,
          "title": "AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1\">Bin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingyi He</a>",
          "description": "The vast majority of modern consumer-grade cameras employ a rolling shutter\nmechanism, leading to image distortions if the camera moves during image\nacquisition. In this paper, we present a novel deep network to solve the\ngeneric rolling shutter correction problem with two consecutive frames. Our\npipeline is symmetrically designed to predict the global shutter image\ncorresponding to the intermediate time of these two frames, which is difficult\nfor existing methods because it corresponds to a camera pose that differs most\nfrom the two frames. First, two time-symmetric dense undistortion flows are\nestimated by using well-established principles: pyramidal construction,\nwarping, and cost volume processing. Then, both rolling shutter images are\nwarped into a common global shutter one in the feature space, respectively.\nFinally, a symmetric consistency constraint is constructed in the image decoder\nto effectively aggregate the contextual cues of two rolling shutter images,\nthereby recovering the high-quality global shutter image. Extensive experiments\nwith both synthetic and real data from public benchmarks demonstrate the\nsuperiority of our proposed approach over the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.04775",
          "publishedOn": "2021-08-11T01:55:24.084Z",
          "wordCount": 620,
          "title": "SUNet: Symmetric Undistortion Network for Rolling Shutter Correction. (arXiv:2108.04775v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.",
          "link": "http://arxiv.org/abs/2108.04351",
          "publishedOn": "2021-08-11T01:55:24.027Z",
          "wordCount": 633,
          "title": "Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>",
          "description": "In this demo, we present VirtualConductor, a system that can generate\nconducting video from any given music and a single user's image. First, a\nlarge-scale conductor motion dataset is collected and constructed. Then, we\npropose Audio Motion Correspondence Network (AMCNet) and adversarial-perceptual\nlearning to learn the cross-modal relationship and generate diverse, plausible,\nmusic-synchronized motion. Finally, we combine 3D animation rendering and a\npose transfer model to synthesize conducting video from a single given user's\nimage. Therefore, any user can become a virtual conductor through the system.",
          "link": "http://arxiv.org/abs/2108.04350",
          "publishedOn": "2021-08-11T01:55:24.016Z",
          "wordCount": 537,
          "title": "VirtualConductor: Music-driven Conducting Video Generation System. (arXiv:2108.04350v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qicong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg/0/1/0/all/0/1\">Scharenborg</a>",
          "description": "Automatically generating videos in which synthesized speech is synchronized\nwith lip movements in a talking head has great potential in many human-computer\ninteraction scenarios. In this paper, we present an automatic method to\ngenerate synchronized speech and talking-head videos on the basis of text and a\nsingle face image of an arbitrary person as input. In contrast to previous\ntext-driven talking head generation methods, which can only synthesize the\nvoice of a specific person, the proposed method is capable of synthesizing\nspeech for any person that is inaccessible in the training stage. Specifically,\nthe proposed method decomposes the generation of synchronized speech and\ntalking head videos into two stages, i.e., a text-to-speech (TTS) stage and a\nspeech-driven talking head generation stage. The proposed TTS module is a\nface-conditioned multi-speaker TTS model that gets the speaker identity\ninformation from face images instead of speech, which allows us to synthesize a\npersonalized voice on the basis of the input face image. To generate the\ntalking head videos from the face images, a facial landmark-based method that\ncan predict both lip movements and head rotations is proposed. Extensive\nexperiments demonstrate that the proposed method is able to generate\nsynchronized speech and talking head videos for arbitrary persons and\nnon-persons. Synthesized speech shows consistency with the given face regarding\nto the synthesized voice's timbre and one's appearance in the image, and the\nproposed landmark-based talking head method outperforms the state-of-the-art\nlandmark-based method on generating natural talking head videos.",
          "link": "http://arxiv.org/abs/2108.04325",
          "publishedOn": "2021-08-11T01:55:24.001Z",
          "wordCount": 689,
          "title": "AnyoneNet: Synchronized Speech and Talking Head Generation for arbitrary person. (arXiv:2108.04325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1\">Santiago Estrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1\">Kersten Diers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Weiyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1\">Philipp Ehses</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1\">Tony St&#xf6;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1\">Monique M.B Breteler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>",
          "description": "The neuroimage analysis community has neglected the automated segmentation of\nthe olfactory bulb (OB) despite its crucial role in olfactory function. The\nlack of an automatic processing method for the OB can be explained by its\nchallenging properties. Nonetheless, recent advances in MRI acquisition\ntechniques and resolution have allowed raters to generate more reliable manual\nannotations. Furthermore, the high accuracy of deep learning methods for\nsolving semantic segmentation problems provides us with an option to reliably\nassess even small structures. In this work, we introduce a novel, fast, and\nfully automated deep learning pipeline to accurately segment OB tissue on\nsub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we\ndesigned a three-stage pipeline: (1) Localization of a region containing both\nOBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized\nregion through four independent AttFastSurferCNN - a novel deep learning\narchitecture with a self-attention mechanism to improve modeling of contextual\ninformation, and (3) Ensemble of the predicted label maps. The OB pipeline\nexhibits high performance in terms of boundary delineation, OB localization,\nand volume estimation across a wide range of ages in 203 participants of the\nRhineland Study. Moreover, it also generalizes to scans of an independent\ndataset never encountered during training, the Human Connectome Project (HCP),\nwith different acquisition parameters and demographics, evaluated in 30 cases\nat the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.\nWe extensively validated our pipeline not only with respect to segmentation\naccuracy but also to known OB volume effects, where it can sensitively\nreplicate age effects.",
          "link": "http://arxiv.org/abs/2108.04267",
          "publishedOn": "2021-08-11T01:55:23.966Z",
          "wordCount": 707,
          "title": "Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI. (arXiv:2108.04267v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1\">Fangwen Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yaxu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagani_A/0/1/0/all/0/1\">Alain Pagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>",
          "description": "This paper presents a semantic planar SLAM system that improves pose\nestimation and mapping using cues from an instance planar segmentation network.\nWhile the mainstream approaches are using RGB-D sensors, employing a monocular\ncamera with such a system still faces challenges such as robust data\nassociation and precise geometric model fitting. In the majority of existing\nwork, geometric model estimation problems such as homography estimation and\npiece-wise planar reconstruction (PPR) are usually solved by standard (greedy)\nRANSAC separately and sequentially. However, setting the inlier-outlier\nthreshold is difficult in absence of information about the scene (i.e. the\nscale). In this work, we revisit these problems and argue that two mentioned\ngeometric models (homographies/3D planes) can be solved by minimizing an energy\nfunction that exploits the spatial coherence, i.e. with graph-cut optimization,\nwhich also tackles the practical issue when the output of a trained CNN is\ninaccurate. Moreover, we propose an adaptive parameter setting strategy based\non our experiments, and report a comprehensive evaluation on various\nopen-source datasets.",
          "link": "http://arxiv.org/abs/2108.04281",
          "publishedOn": "2021-08-11T01:55:23.961Z",
          "wordCount": 609,
          "title": "Visual SLAM with Graph-Cut Optimized Multi-Plane Reconstruction. (arXiv:2108.04281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1\">Eden Bensaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1\">Mauro Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>",
          "description": "Storytelling is an open-ended task that entails creative thinking and\nrequires a constant flow of ideas. Natural language generation (NLG) for\nstorytelling is especially challenging because it requires the generated text\nto follow an overall theme while remaining creative and diverse to engage the\nreader. In this work, we introduce a system and a web-based demo, FairyTailor,\nfor human-in-the-loop visual story co-creation. Users can create a cohesive\nchildren's fairytale by weaving generated texts and retrieved images with their\ninput. FairyTailor adds another modality and modifies the text generation\nprocess to produce a coherent and creative sequence of text and images. To our\nknowledge, this is the first dynamic tool for multimodal story generation that\nallows interactive co-formation of both texts and images. It allows users to\ngive feedback on co-created stories and share their results.",
          "link": "http://arxiv.org/abs/2108.04324",
          "publishedOn": "2021-08-11T01:55:23.955Z",
          "wordCount": 592,
          "title": "FairyTailor: A Multimodal Generative Framework for Storytelling. (arXiv:2108.04324v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1907.12975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1\">Florian Kromp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1\">Lukas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1\">Eva Bozsaky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1\">Inge Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1\">Wolfgang Doerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1\">Sabine Taschner-Mandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1\">Peter Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "Separating and labeling each instance of a nucleus (instance-aware\nsegmentation) is the key challenge in segmenting single cell nuclei on\nfluorescence microscopy images. Deep Neural Networks can learn the implicit\ntransformation of a nuclear image into a probability map indicating the class\nmembership of each pixel (nucleus or background), but the use of\npost-processing steps to turn the probability map into a labeled object mask is\nerror-prone. This especially accounts for nuclear images of tissue sections and\nnuclear images across varying tissue preparations. In this work, we aim to\nevaluate the performance of state-of-the-art deep learning architectures to\nsegment nuclei in fluorescence images of various tissue origins and sample\npreparation types without post-processing. We compare architectures that\noperate on pixel to pixel translation and an architecture that operates on\nobject detection and subsequent locally applied segmentation. In addition, we\npropose a novel strategy to create artificial images to extend the training\nset. We evaluate the influence of ground truth annotation quality, image scale\nand segmentation complexity on segmentation performance. Results show that\nthree out of four deep learning architectures (U-Net, U-Net with ResNet34\nbackbone, Mask R-CNN) can segment fluorescent nuclear images on most of the\nsample preparation types and tissue origins with satisfactory segmentation\nperformance. Mask R-CNN, an architecture designed to address instance aware\nsegmentation tasks, outperforms other architectures. Equal nuclear mean size,\nconsistent nuclear annotations and the use of artificially generated images\nresult in overall acceptable precision and recall across different tissues and\nsample preparation types.",
          "link": "http://arxiv.org/abs/1907.12975",
          "publishedOn": "2021-08-11T01:55:22.683Z",
          "wordCount": 725,
          "title": "Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation. (arXiv:1907.12975v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07770",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1\">Pankaj Topiwala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1\">Jiangfeng Pian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1\">Katalina Biondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1\">Arvind Krovvidi</a>",
          "description": "Video quality assessment (VQA) is now a fastgrowing subject, beginning to\nmature in the full reference (FR) case, while the burgeoning no reference (NR)\ncase remains challenging. We investigate variants of the popular VMAF video\nquality assessment algorithm for the FR case, using support vector regression\nand feedforward neural networks, and extend it to the NR case, using the same\nlearning architectures, to develop a partially unified framework for VQA. When\nheavily trained, algorithms such as VMAF perform well on test datasets, with\n90%+ match; but predicting performance in the wild is better done by\ntraining/testing from scratch, as we do. Even from scratch, we achieve 90%+\nperformance in FR, with gains over VMAF. And we greatly reduce complexity vs.\nleading recent NR algorithms, VIDEVAL, RAPIQUE, yet exceed 80% in SRCC. In our\npreliminary testing, we find the improvements in trainability, while also\nconstraining computational complexity, as quite encouraging, suggesting further\nstudy and analysis.",
          "link": "http://arxiv.org/abs/2103.07770",
          "publishedOn": "2021-08-11T01:55:22.677Z",
          "wordCount": 634,
          "title": "VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_Y/0/1/0/all/0/1\">Yawar Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fangchang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1\">Qi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>",
          "description": "3D reconstruction of large scenes is a challenging problem due to the\nhigh-complexity nature of the solution space, in particular for generative\nneural networks. In contrast to traditional generative learned models which\nencode the full generative process into a neural network and can struggle with\nmaintaining local details at the scene level, we introduce a new method that\ndirectly leverages scene geometry from the training database. First, we learn\nto synthesize an initial estimate for a 3D scene, constructed by retrieving a\ntop-k set of volumetric chunks from the scene database. These candidates are\nthen refined to a final scene generation with an attention-based refinement\nthat can effectively select the most consistent set of geometry from the\ncandidates and combine them together to create an output scene, facilitating\ntransfer of coherent structures and local detail from train scene geometry. We\ndemonstrate our neural scene reconstruction with a database for the tasks of 3D\nsuper resolution and surface reconstruction from sparse point clouds, showing\nthat our approach enables generation of more coherent, accurate 3D scenes,\nimproving on average by over 8% in IoU over state-of-the-art scene\nreconstruction.",
          "link": "http://arxiv.org/abs/2104.00024",
          "publishedOn": "2021-08-11T01:55:22.672Z",
          "wordCount": 663,
          "title": "RetrievalFuse: Neural 3D Scene Reconstruction with a Database. (arXiv:2104.00024v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zhenyu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuesheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruixin Liu</a>",
          "description": "Weighted Hamming distance, as a similarity measure between binary codes and\nbinary queries, provides superior accuracy in search tasks than Hamming\ndistance. However, how to efficiently and accurately find $K$ binary codes that\nhave the smallest weighted Hamming distance to the query remains an open issue.\nIn this paper, a fast search algorithm is proposed to perform the\nnon-exhaustive search for $K$ nearest binary codes by weighted Hamming\ndistance. By using binary codes as direct bucket indices in a hash table, the\nsearch algorithm generates a sequence to probe the buckets based on the\nindependence characteristic of the weights for each bit. Furthermore, a fast\nsearch framework based on the proposed search algorithm is designed to solve\nthe problem of long binary codes. Specifically, long binary codes are split\ninto substrings and multiple hash tables are built on them. Then, the search\nalgorithm probes the buckets to obtain candidates according to the generated\nsubstring indices, and a merging algorithm is proposed to find the nearest\nbinary codes by merging the candidates. Theoretical analysis and experimental\nresults demonstrate that the search algorithm improves the search accuracy\ncompared to other non-exhaustive algorithms and provides orders-of-magnitude\nfaster search than the linear scan baseline.",
          "link": "http://arxiv.org/abs/2009.08591",
          "publishedOn": "2021-08-11T01:55:22.666Z",
          "wordCount": 666,
          "title": "Fast Search on Binary Codes by Weighted Hamming Distance. (arXiv:2009.08591v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xianbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.",
          "link": "http://arxiv.org/abs/2103.13689",
          "publishedOn": "2021-08-11T01:55:22.652Z",
          "wordCount": 670,
          "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Contrastive learning shows great potential in unpaired image-to-image\ntranslation, but sometimes the translated results are in poor quality and the\ncontents are not preserved consistently. In this paper, we uncover that the\nnegative examples play a critical role in the performance of contrastive\nlearning for image translation. The negative examples in previous methods are\nrandomly sampled from the patches of different positions in the source image,\nwhich are not effective to push the positive examples close to the query\nexamples. To address this issue, we present instance-wise hard Negative Example\nGeneration for Contrastive learning in Unpaired image-to-image\nTranslation~(NEGCUT). Specifically, we train a generator to produce negative\nexamples online. The generator is novel from two perspectives: 1) it is\ninstance-wise which means that the generated examples are based on the input\nimage, and 2) it can generate hard negative examples since it is trained with\nan adversarial loss. With the generator, the performance of unpaired\nimage-to-image translation is significantly improved. Experiments on three\nbenchmark datasets demonstrate that the proposed NEGCUT framework achieves\nstate-of-the-art performance compared to previous methods.",
          "link": "http://arxiv.org/abs/2108.04547",
          "publishedOn": "2021-08-11T01:55:22.646Z",
          "wordCount": 623,
          "title": "Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation. (arXiv:2108.04547v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>",
          "description": "Arbitrary shape text detection is a challenging task due to the high\ncomplexity and variety of scene texts. In this work, we propose a novel\nadaptive boundary proposal network for arbitrary shape text detection, which\ncan learn to directly produce accurate boundary for arbitrary shape text\nwithout any post-processing. Our method mainly consists of a boundary proposal\nmodel and an innovative adaptive boundary deformation model. The boundary\nproposal model constructed by multi-layer dilated convolutions is adopted to\nproduce prior information (including classification map, distance field, and\ndirection field) and coarse boundary proposals. The adaptive boundary\ndeformation model is an encoder-decoder network, in which the encoder mainly\nconsists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network\n(RNN). It aims to perform boundary deformation in an iterative way for\nobtaining text instance shape guided by prior information from the boundary\nproposal model. In this way, our method can directly and efficiently generate\naccurate text boundaries without complex post-processing. Extensive experiments\non publicly available datasets demonstrate the state-of-the-art performance of\nour method.",
          "link": "http://arxiv.org/abs/2107.12664",
          "publishedOn": "2021-08-11T01:55:22.641Z",
          "wordCount": 669,
          "title": "Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection. (arXiv:2107.12664v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1\">Weili Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Finding tampered regions in images is a hot research topic in machine\nlearning and computer vision. Although many image manipulation location\nalgorithms have been proposed, most of them only focus on the RGB images with\ndifferent color spaces, and the frequency information that contains the\npotential tampering clues is often ignored. In this work, a novel end-to-end\ntwo-stream boundary-aware network (abbreviated as TBNet) is proposed for\ngeneric image manipulation localization in which the RGB stream, the frequency\nstream, and the boundary artifact location are explored in a unified framework.\nSpecifically, we first design an adaptive frequency selection module (AFS) to\nadaptively select the appropriate frequency to mine inconsistent statistics and\neliminate the interference of redundant statistics. Then, an adaptive\ncross-attention fusion module (ACF) is proposed to adaptively fuse the RGB\nfeature and the frequency feature. Finally, the boundary artifact location\nnetwork (BAL) is designed to locate the boundary artifacts for which the\nparameters are jointly updated by the outputs of the ACF, and its results are\nfurther fed into the decoder. Thus, the parameters of the RGB stream, the\nfrequency stream, and the boundary artifact location network are jointly\noptimized, and their latent complementary relationships are fully mined. The\nresults of extensive experiments performed on four public benchmarks of the\nimage manipulation localization task, namely, CASIA1.0, COVER, Carvalho, and\nIn-The-Wild, demonstrate that the proposed TBNet can significantly outperform\nstate-of-the-art generic image manipulation localization methods in terms of\nboth MCC and F1.",
          "link": "http://arxiv.org/abs/2108.04508",
          "publishedOn": "2021-08-11T01:55:22.635Z",
          "wordCount": 680,
          "title": "TBNet:Two-Stream Boundary-aware Network for Generic Image Manipulation Localization. (arXiv:2108.04508v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+li_Y/0/1/0/all/0/1\">Yuan li</a>",
          "description": "Among 2D convolutional networks on point clouds, point-based approaches\nconsume point clouds of fixed size directly. By analysis of PointNet, a pioneer\nin introducing deep learning into point sets, we reveal that current\npoint-based methods are essentially spatial relationship processing networks.\nIn this paper, we take a different approach. Our architecture, named PE-Net,\nlearns the representation of point clouds in high-dimensional space, and\nencodes the unordered input points to feature vectors, which standard 2D CNNs\ncan be applied to. The recommended network can adapt to changes in the number\nof input points which is the limit of current methods. Experiments show that in\nthe tasks of classification and part segmentation, PE-Net achieves the\nstate-of-the-art performance in multiple challenging datasets, such as ModelNet\nand ShapeNetPart.",
          "link": "http://arxiv.org/abs/2107.08565",
          "publishedOn": "2021-08-11T01:55:22.630Z",
          "wordCount": 578,
          "title": "Learning point embedding for 3D data processing. (arXiv:2107.08565v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaoqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Junbo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_Z/0/1/0/all/0/1\">Zhou Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "Accurate detection of obstacles in 3D is an essential task for autonomous\ndriving and intelligent transportation. In this work, we propose a general\nmultimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D\npoint clouds at a semantic level for boosting the 3D object detection task.\nEspecially, the FusionPainting framework consists of three main modules: a\nmulti-modal semantic segmentation module, an adaptive attention-based semantic\nfusion module, and a 3D object detector. First, semantic information is\nobtained for 2D images and 3D Lidar point clouds based on 2D and 3D\nsegmentation approaches. Then the segmentation results from different sensors\nare adaptively fused based on the proposed attention-based semantic fusion\nmodule. Finally, the point clouds painted with the fused semantic label are\nsent to the 3D detector for obtaining the 3D objection results. The\neffectiveness of the proposed framework has been verified on the large-scale\nnuScenes detection benchmark by comparing it with three different baselines.\nThe experimental results show that the fusion strategy can significantly\nimprove the detection performance compared to the methods using only point\nclouds, and the methods using point clouds only painted with 2D segmentation\ninformation. Furthermore, the proposed approach outperforms other\nstate-of-the-art methods on the nuScenes testing benchmark.",
          "link": "http://arxiv.org/abs/2106.12449",
          "publishedOn": "2021-08-11T01:55:22.624Z",
          "wordCount": 678,
          "title": "FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection. (arXiv:2106.12449v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07588",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Deep learning motivated by convolutional neural networks has been highly\nsuccessful in a range of medical imaging problems like image classification,\nimage segmentation, image synthesis etc. However for validation and\ninterpretability, not only do we need the predictions made by the model but\nalso how confident it is while making those predictions. This is important in\nsafety critical applications for the people to accept it. In this work, we used\nan encoder decoder architecture based on variational inference techniques for\nsegmenting brain tumour images. We evaluate our work on the publicly available\nBRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over\nUnion (IOU) as the evaluation metrics. Our model is able to segment brain\ntumours while taking into account both aleatoric uncertainty and epistemic\nuncertainty in a principled bayesian manner.",
          "link": "http://arxiv.org/abs/2008.07588",
          "publishedOn": "2021-08-11T01:55:22.618Z",
          "wordCount": 602,
          "title": "Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation. (arXiv:2008.07588v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "Recent vision-language (VL) studies have shown remarkable progress by\nlearning generic representations from massive image-text pairs with transformer\nmodels and then fine-tuning on downstream VL tasks. While existing research has\nbeen focused on achieving high accuracy with large pre-trained models, building\na lightweight model is of great value in practice but is less explored. In this\npaper, we propose a smaller and faster VL model, MiniVLM, which can be\nfinetuned with good performance on various downstream tasks like its larger\ncounterpart. MiniVLM consists of two modules, a vision feature extractor and a\ntransformer-based vision-language fusion module. We design a Two-stage\nEfficient feature Extractor (TEE), inspired by the one-stage EfficientDet\nnetwork, to significantly reduce the time cost of visual feature extraction by\n$95\\%$, compared to a baseline model. We adopt the MiniLM structure to reduce\nthe computation cost of the transformer module after comparing different\ncompact BERT models. In addition, we improve the MiniVLM pre-training by adding\n$7M$ Open Images data, which are pseudo-labeled by a state-of-the-art\ncaptioning model. We also pre-train with high-quality image tags obtained from\na strong tagging model to enhance cross-modality alignment. The large models\nare used offline without adding any overhead in fine-tuning and inference. With\nthe above design choices, our MiniVLM reduces the model size by $73\\%$ and the\ninference time cost by $94\\%$ while being able to retain $94-97\\%$ of the\naccuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the\nstate-of-the-art VL research for on-the-edge applications.",
          "link": "http://arxiv.org/abs/2012.06946",
          "publishedOn": "2021-08-11T01:55:22.612Z",
          "wordCount": 721,
          "title": "MiniVLM: A Smaller and Faster Vision-Language Model. (arXiv:2012.06946v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1\">Chloe LeGendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Ta Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleibel_F/0/1/0/all/0/1\">Francois Bleibel</a>",
          "description": "We present \"Cross-Camera Convolutional Color Constancy\" (C5), a\nlearning-based method, trained on images from multiple cameras, that accurately\nestimates a scene's illuminant color from raw images captured by a new camera\npreviously unseen during training. C5 is a hypernetwork-like extension of the\nconvolutional color constancy (CCC) approach: C5 learns to generate the weights\nof a CCC model that is then evaluated on the input image, with the CCC weights\ndynamically adapted to different input content. Unlike prior cross-camera color\nconstancy models, which are usually designed to be agnostic to the spectral\nproperties of test-set images from unobserved cameras, C5 approaches this\nproblem through the lens of transductive inference: additional unlabeled images\nare provided as input to the model at test time, which allows the model to\ncalibrate itself to the spectral properties of the test-set camera during\ninference. C5 achieves state-of-the-art accuracy for cross-camera color\nconstancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on\na GPU or CPU, respectively), and requires little memory (~2 MB), and, thus, is\na practical solution to the problem of calibration-free automatic white balance\nfor mobile photography.",
          "link": "http://arxiv.org/abs/2011.11890",
          "publishedOn": "2021-08-11T01:55:22.587Z",
          "wordCount": 653,
          "title": "Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.04264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>",
          "description": "Key for solving fine-grained image categorization is finding discriminate and\nlocal regions that correspond to subtle visual traits. Great strides have been\nmade, with complex networks designed specifically to learn part-level\ndiscriminate feature representations. In this paper, we show it is possible to\ncultivate subtle details without the need for overly complicated network\ndesigns or training mechanisms -- a single loss is all it takes. The main trick\nlies with how we delve into individual feature channels early on, as opposed to\nthe convention of starting from a consolidated feature map. The proposed loss\nfunction, termed as mutual-channel loss (MC-Loss), consists of two\nchannel-specific components: a discriminality component and a diversity\ncomponent. The discriminality component forces all feature channels belonging\nto the same class to be discriminative, through a novel channel-wise attention\nmechanism. The diversity component additionally constraints channels so that\nthey become mutually exclusive on spatial-wise. The end result is therefore a\nset of feature channels that each reflects different locally discriminative\nregions for a specific class. The MC-Loss can be trained end-to-end, without\nthe need for any bounding-box/part annotations, and yields highly\ndiscriminative regions during inference. Experimental results show our MC-Loss\nwhen implemented on top of common base networks can achieve state-of-the-art\nperformance on all four fine-grained categorization datasets (CUB-Birds,\nFGVC-Aircraft, Flowers-102, and Stanford-Cars). Ablative studies further\ndemonstrate the superiority of MC-Loss when compared with other recently\nproposed general-purpose losses for visual classification, on two different\nbase networks. Code available at\nhttps://github.com/dongliangchang/Mutual-Channel-Loss",
          "link": "http://arxiv.org/abs/2002.04264",
          "publishedOn": "2021-08-11T01:55:22.580Z",
          "wordCount": 752,
          "title": "The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification. (arXiv:2002.04264v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>",
          "description": "Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.",
          "link": "http://arxiv.org/abs/2107.11170",
          "publishedOn": "2021-08-11T01:55:22.557Z",
          "wordCount": 744,
          "title": "Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1\">Tianze Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongxiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yichao Xiong</a>",
          "description": "We proposed a new modeling method to promote the performance of prohibited\nitems recognition via X-ray image. We analyzed the characteristics of\nprohibited items and X-ray images. We found the fact that the scales of some\nitems are too small to be recognized which encumber the model performance. Then\nwe adopted a set of data augmentation and modified the model to adapt the field\nof prohibited items recognition. The Convolutional Block Attention Module(CBAM)\nand rescoring mechanism has been assembled into the model. By the modification,\nour model achieved a mAP of 89.9% on SIXray10, mAP of 74.8%.",
          "link": "http://arxiv.org/abs/2102.12256",
          "publishedOn": "2021-08-11T01:55:22.550Z",
          "wordCount": 553,
          "title": "An Enhanced Prohibited Items Recognition Model. (arXiv:2102.12256v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1\">Tianze Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanjia Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongxiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yichao Xiong</a>",
          "description": "This work is a solution to densely packed scenes dataset SKU-110k. Our work\nis modified from Cascade R-CNN. To solve the problem, we proposed a random crop\nstrategy to ensure both the sampling rate and input scale is relatively\nsufficient as a contrast to the regular random crop. And we adopted some of\ntrick and optimized the hyper-parameters. To grasp the essential feature of the\ndensely packed scenes, we analysis the stages of a detector and investigate the\nbottleneck which limits the performance. As a result, our method obtains 58.7\nmAP on test set of SKU-110k.",
          "link": "http://arxiv.org/abs/2007.11946",
          "publishedOn": "2021-08-11T01:55:22.539Z",
          "wordCount": 572,
          "title": "A Solution to Product detection in Densely Packed Scenes. (arXiv:2007.11946v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuanzhouhan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>",
          "description": "Data hiding is the procedure of encoding desired information into an image to\nresist potential noises while ensuring the embedded image has little perceptual\nperturbations from the original image. Recently, with the tremendous successes\ngained by deep neural networks in various fields, data hiding areas have\nattracted increasing number of attentions. The neglect of considering the pixel\nsensitivity within the cover image of deep neural methods will inevitably\naffect the model robustness for information hiding. Targeting at the problem,\nin this paper, we propose a novel deep data hiding scheme with Inverse Gradient\nAttention (IGA), combing the ideas of adversarial learning and attention\nmechanism to endow different sensitivity to different pixels. With the proposed\ncomponent, the model can spotlight pixels with more robustness for embedding\ndata. Empirically, extensive experiments show that the proposed model\noutperforms the state-of-the-art methods on two prevalent datasets under\nmultiple settings. Besides, we further identify and discuss the connections\nbetween the proposed inverse gradient attention and high-frequency regions\nwithin images.",
          "link": "http://arxiv.org/abs/2011.10850",
          "publishedOn": "2021-08-11T01:55:22.524Z",
          "wordCount": 649,
          "title": "Robust Data Hiding Using Inverse Gradient Attention. (arXiv:2011.10850v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1\">Muhammad Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Semantic segmentation is a crucial step in many Earth observation tasks.\nLarge quantity of pixel-level annotation is required to train deep networks for\nsemantic segmentation. Earth observation techniques are applied to varieties of\napplications and since classes vary widely depending on the applications,\ntherefore, domain knowledge is often required to label Earth observation\nimages, impeding availability of labeled training data in many Earth\nobservation applications. To tackle these challenges, in this paper we propose\nan unsupervised semantic segmentation method that can be trained using just a\nsingle unlabeled scene. Remote sensing scenes are generally large. The proposed\nmethod exploits this property to sample smaller patches from the larger scene\nand uses deep clustering and contrastive learning to refine the weights of a\nlightweight deep model composed of a series of the convolution layers along\nwith an embedded channel attention. After unsupervised training on the target\nimage/scene, the model automatically segregates the major classes present in\nthe scene and produces the segmentation map. Experimental results on the\nVaihingen dataset demonstrate the efficacy of the proposed method.",
          "link": "http://arxiv.org/abs/2108.04222",
          "publishedOn": "2021-08-11T01:55:22.513Z",
          "wordCount": 629,
          "title": "Segmentation of VHR EO Images using Unsupervised Learning. (arXiv:2108.04222v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhu Li</a>",
          "description": "In this paper, we propose a no-reference (NR) image quality assessment (IQA)\nmethod via feature level pseudo-reference (PR) hallucination. The proposed\nquality assessment framework is grounded on the prior models of natural image\nstatistical behaviors and rooted in the view that the perceptually meaningful\nfeatures could be well exploited to characterize the visual quality. Herein,\nthe PR features from the distorted images are learned by a mutual learning\nscheme with the pristine reference as the supervision, and the discriminative\ncharacteristics of PR features are further ensured with the triplet\nconstraints. Given a distorted image for quality inference, the feature level\ndisentanglement is performed with an invertible neural layer for final quality\nprediction, leading to the PR and the corresponding distortion features for\ncomparison. The effectiveness of our proposed method is demonstrated on four\npopular IQA databases, and superior performance on cross-database evaluation\nalso reveals the high generalization capability of our method. The\nimplementation of our method is publicly available on\nhttps://github.com/Baoliang93/FPR.",
          "link": "http://arxiv.org/abs/2108.04165",
          "publishedOn": "2021-08-11T01:55:22.499Z",
          "wordCount": 619,
          "title": "No-Reference Image Quality Assessment by Hallucinating Pristine Features. (arXiv:2108.04165v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaemin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Geol Choi</a>",
          "description": "Deep convolutional neural networks (CNNs) have shown state-of-the-art\nperformances in various computer vision tasks. Advances on CNN architectures\nhave focused mainly on designing convolutional blocks of the feature\nextractors, but less on the classifiers that exploit extracted features. In\nthis work, we propose Split-and-Share Module (SSM),a classifier that splits a\ngiven feature into parts, which are partially shared by multiple\nsub-classifiers. Our intuition is that the more the features are shared, the\nmore common they will become, and SSM can encourage such structural\ncharacteristics in the split features. SSM can be easily integrated into any\narchitecture without bells and whistles. We have extensively validated the\nefficacy of SSM on ImageNet-1K classification task, andSSM has shown consistent\nand significant improvements over baseline architectures. In addition, we\nanalyze the effect of SSM using the Grad-CAM visualization.",
          "link": "http://arxiv.org/abs/2108.04500",
          "publishedOn": "2021-08-11T01:55:22.493Z",
          "wordCount": 561,
          "title": "Exploiting Featureswith Split-and-Share Module. (arXiv:2108.04500v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "Visual perception of a person is easily influenced by many factors such as\ncamera parameters, pose and viewpoint variations. These variations make person\nRe-Identification (ReID) a challenging problem. Nevertheless, human attributes\nusually stand as robust visual properties to such variations. In this paper, we\npropose a new method to leverage features from human attributes for person\nReID. Our model uses a tensor to non-linearly fuse identity and attribute\nfeatures, and then forces the parameters of the tensor in the loss function to\ngenerate discriminative fused features for ReID. Since tensor-based methods\nusually contain a large number of parameters, training all of these parameters\nbecomes very slow, and the chance of overfitting increases as well. To address\nthis issue, we propose two new techniques based on Structural Sparsity Learning\n(SSL) and Tensor Decomposition (TD) methods to create an accurate and stable\nlearning problem. We conducted experiments on several standard pedestrian\ndatasets, and experimental results indicate that our tensor-based approach\nsignificantly improves person ReID baselines and also outperforms state of the\nart methods.",
          "link": "http://arxiv.org/abs/2108.04352",
          "publishedOn": "2021-08-11T01:55:22.482Z",
          "wordCount": 609,
          "title": "Attribute Guided Sparse Tensor-Based Model for Person Re-Identification. (arXiv:2108.04352v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sirui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anmoll Kumar Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Mohammad Qazim Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kwei-Herng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1\">Na Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Action recognition is a crucial task for video understanding. In this paper,\nwe present AutoVideo, a Python system for automated video action recognition.\nIt currently supports seven action recognition algorithms and various\npre-processing modules. Unlike the existing libraries that only provide model\nzoos, AutoVideo is built with the standard pipeline language. The basic\nbuilding block is primitive, which wraps a pre-processing module or an\nalgorithm with some hyperparameters. AutoVideo is highly modular and\nextendable. It can be easily combined with AutoML searchers. The pipeline\nlanguage is quite general so that we can easily enrich AutoVideo with\nalgorithms for various other video-related tasks in the future. AutoVideo is\nreleased under MIT license at https://github.com/datamllab/autovideo",
          "link": "http://arxiv.org/abs/2108.04212",
          "publishedOn": "2021-08-11T01:55:22.464Z",
          "wordCount": 598,
          "title": "AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "Semi-supervised machine learning models learn from a (small) set of labeled\ntraining examples, and a (large) set of unlabeled training examples.\nState-of-the-art models can reach within a few percentage points of\nfully-supervised training, while requiring 100x less labeled data.\n\nWe study a new class of vulnerabilities: poisoning attacks that modify the\nunlabeled dataset. In order to be useful, unlabeled datasets are given strictly\nless review than labeled datasets, and adversaries can therefore poison them\neasily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%\nof the dataset size, we can manipulate a model trained on this poisoned dataset\nto misclassify arbitrary examples at test time (as any desired label). Our\nattacks are highly effective across datasets and semi-supervised learning\nmethods.\n\nWe find that more accurate methods (thus more likely to be used) are\nsignificantly more vulnerable to poisoning attacks, and as such better training\nmethods are unlikely to prevent this attack. To counter this we explore the\nspace of defenses, and propose two methods that mitigate our attack.",
          "link": "http://arxiv.org/abs/2105.01622",
          "publishedOn": "2021-08-11T01:55:22.458Z",
          "wordCount": 627,
          "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning. (arXiv:2105.01622v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1\">Claudia P&#xe9;rez-D&#x27;Arpino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1\">Lyne P. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael E. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Josiah Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present iGibson 1.0, a novel simulation environment to develop robotic\nsolutions for interactive tasks in large-scale realistic scenes. Our\nenvironment contains 15 fully interactive home-sized scenes with 108 rooms\npopulated with rigid and articulated objects. The scenes are replicas of\nreal-world homes, with distribution and the layout of objects aligned to those\nof the real world. iGibson 1.0 integrates several key features to facilitate\nthe study of interactive tasks: i) generation of high-quality virtual sensor\nsignals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain\nrandomization to change the materials of the objects (both visual and physical)\nand/or their shapes, iii) integrated sampling-based motion planners to generate\ncollision-free trajectories for robot bases and arms, and iv) intuitive\nhuman-iGibson interface that enables efficient collection of human\ndemonstrations. Through experiments, we show that the full interactivity of the\nscenes enables agents to learn useful visual representations that accelerate\nthe training of downstream manipulation tasks. We also show that iGibson 1.0\nfeatures enable the generalization of navigation agents, and that the\nhuman-iGibson interface and integrated motion planners facilitate efficient\nimitation learning of human demonstrated (mobile) manipulation behaviors.\niGibson 1.0 is open-source, equipped with comprehensive examples and\ndocumentation. For more information, visit our project website:\nthis http URL",
          "link": "http://arxiv.org/abs/2012.02924",
          "publishedOn": "2021-08-11T01:55:22.440Z",
          "wordCount": 757,
          "title": "IGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v6 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "The great success of Convolutional Neural Networks (CNN) for facial attribute\nprediction relies on a large amount of labeled images. Facial image datasets\nare usually annotated by some commonly used attributes (e.g., gender), while\nlabels for the other attributes (e.g., big nose) are limited which causes their\nprediction challenging. To address this problem, we use a new Multi-Task\nLearning (MTL) paradigm in which a facial attribute predictor uses the\nknowledge of other related attributes to obtain a better generalization\nperformance. Here, we leverage MLT paradigm in two problem settings. First, it\nis assumed that the structure of the tasks (e.g., grouping pattern of facial\nattributes) is known as a prior knowledge, and parameters of the tasks (i.e.,\npredictors) within the same group are represented by a linear combination of a\nlimited number of underlying basis tasks. Here, a sparsity constraint on the\ncoefficients of this linear combination is also considered such that each task\nis represented in a more structured and simpler manner. Second, it is assumed\nthat the structure of the tasks is unknown, and then structure and parameters\nof the tasks are learned jointly by using a Laplacian regularization framework.\nOur MTL methods are compared with competing methods for facial attribute\nprediction to show its effectiveness.",
          "link": "http://arxiv.org/abs/2108.04353",
          "publishedOn": "2021-08-11T01:55:22.424Z",
          "wordCount": 652,
          "title": "Tasks Structure Regularization in Multi-Task Learning for Improving Facial Attribute Prediction. (arXiv:2108.04353v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besler_B/0/1/0/all/0/1\">Bryce A Besler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_T/0/1/0/all/0/1\">Tannis D. Kemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalski_A/0/1/0/all/0/1\">Andrew S. Michalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forkert_N/0/1/0/all/0/1\">Nils D. Forkert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_S/0/1/0/all/0/1\">Steven K. Boyd</a>",
          "description": "Anatomical structures such as the hippocampus, liver, and bones can be\nanalyzed as orientable, closed surfaces. This permits the computation of\nvolume, surface area, mean curvature, Gaussian curvature, and the\nEuler-Poincar\\'e characteristic as well as comparison of these morphometrics\nbetween structures of different topology. The structures are commonly\nrepresented implicitly in curve evolution problems as the zero level set of an\nembedding. Practically, binary images of anatomical structures are embedded\nusing a signed distance transform. However, quantization prevents the accurate\ncomputation of curvatures, leading to considerable errors in morphometry. This\npaper presents a fast, simple embedding procedure for accurate local\nmorphometry as the zero crossing of the Gaussian blurred binary image. The\nproposed method was validated based on the femur and fourth lumbar vertebrae of\n50 clinical computed tomography datasets. The results show that the signed\ndistance transform leads to large quantization errors in the computed local\ncurvature. Global validation of morphometry using regression and Bland-Altman\nanalysis revealed that the coefficient of determination for the average mean\ncurvature is improved from 93.8% with the signed distance transform to 100%\nwith the proposed method. For the surface area, the proportional bias is\nimproved from -5.0% for the signed distance transform to +0.6% for the proposed\nmethod. The Euler-Poincar\\'e characteristic is improved from unusable in the\nsigned distance transform to 98% accuracy for the proposed method. The proposed\nmethod enables an improved local and global evaluation of curvature for\npurposes of morphometry on closed, implicit surfaces.",
          "link": "http://arxiv.org/abs/2108.04354",
          "publishedOn": "2021-08-11T01:55:22.418Z",
          "wordCount": 682,
          "title": "Local Morphometry of Closed, Implicit Surfaces. (arXiv:2108.04354v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1\">NareshKumar Gurulingan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Scene understanding is crucial for autonomous systems which intend to operate\nin the real world. Single task vision networks extract information only based\non some aspects of the scene. In multi-task learning (MTL), on the other hand,\nthese single tasks are jointly learned, thereby providing an opportunity for\ntasks to share information and obtain a more comprehensive understanding. To\nthis end, we develop UniNet, a unified scene understanding network that\naccurately and efficiently infers vital vision tasks including object\ndetection, semantic segmentation, instance segmentation, monocular depth\nestimation, and monocular instance depth prediction. As these tasks look at\ndifferent semantic and geometric information, they can either complement or\nconflict with each other. Therefore, understanding inter-task relationships can\nprovide useful cues to enable complementary information sharing. We evaluate\nthe task relationships in UniNet through the lens of adversarial attacks based\non the notion that they can exploit learned biases and task interactions in the\nneural network. Extensive experiments on the Cityscapes dataset, using\nuntargeted and targeted attacks reveal that semantic tasks strongly interact\namongst themselves, and the same holds for geometric tasks. Additionally, we\nshow that the relationship between semantic and geometric tasks is asymmetric\nand their interaction becomes weaker as we move towards higher-level\nrepresentations.",
          "link": "http://arxiv.org/abs/2108.04584",
          "publishedOn": "2021-08-11T01:55:22.413Z",
          "wordCount": 668,
          "title": "UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks. (arXiv:2108.04584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aman Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1\">Mayank Kothyari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>",
          "description": "Multimodal IR, spanning text corpus, knowledge graph and images, called\noutside knowledge visual question answering (OKVQA), is of much recent\ninterest. However, the popular data set has serious limitations. A surprisingly\nlarge fraction of queries do not assess the ability to integrate cross-modal\ninformation. Instead, some are independent of the image, some depend on\nspeculation, some require OCR or are otherwise answerable from the image alone.\nTo add to the above limitations, frequency-based guessing is very effective\nbecause of (unintended) widespread answer overlaps between the train and test\nfolds. Overall, it is hard to determine when state-of-the-art systems exploit\nthese weaknesses rather than really infer the answers, because they are opaque\nand their 'reasoning' process is uninterpretable. An equally important\nlimitation is that the dataset is designed for the quantitative assessment only\nof the end-to-end answer retrieval task, with no provision for assessing the\ncorrect(semantic) interpretation of the input query. In response, we identify a\nkey structural idiom in OKVQA ,viz., S3 (select, substitute and search), and\nbuild a new data set and challenge around it. Specifically, the questioner\nidentifies an entity in the image and asks a question involving that entity\nwhich can be answered only by consulting a knowledge graph or corpus passage\nmentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA\nannotated based on the structural idiom and (ii)S3VQA, a new dataset built from\nscratch. We also present a neural but structurally transparent OKVQA system,\nS3, that explicitly addresses our challenge dataset, and outperforms recent\ncompetitive baselines.",
          "link": "http://arxiv.org/abs/2103.05568",
          "publishedOn": "2021-08-11T01:55:22.405Z",
          "wordCount": 749,
          "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. (arXiv:2103.05568v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Ting-Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>",
          "description": "In real applications, different computation-resource devices need\ndifferent-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,\nexisting methods either design multiple networks and train them independently,\nor construct depth-level/width-level dynamic neural networks which is hard to\nprove the accuracy of each sub-net. In this article, we propose an elegant\nDepth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets\nof similar architectures. To improve the generalization of sub-nets, we design\nthe Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to\nimplement knowledge transfer from the teacher (full-net) to multiple students\n(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to\nconstrain the posterior class probability consistency between full-net and\nsub-nets, and self-attention distillation on the same resolution feature of\ndifferent depth is addressed to drive more abundant feature representations of\nsub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in\na DDNN via the online knowledge distillation in each training iteration without\nextra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet\ndatasets demonstrate that sub-nets in DDNN with EKD training achieve better\nperformance than individually training networks while preserving the original\nperformance of full-nets.",
          "link": "http://arxiv.org/abs/2103.00793",
          "publishedOn": "2021-08-11T01:55:22.389Z",
          "wordCount": 674,
          "title": "Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network. (arXiv:2103.00793v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shimei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shangyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1\">Miao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiaofang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chuangxue Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kunyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuxin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yuer Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Ting Zhong</a>",
          "description": "We presented an optical system to perform imaging interested objects in\ncomplex scenes, like the creature easy see the interested prey in the hunt for\ncomplex environments. It utilized Deep-learning network to learn the interested\nobjects's vision features and designed the corresponding \"imaging matrices\",\nfurthermore the learned matrixes act as the measurement matrix to complete\ncompressive imaging with a single-pixel camera, finally we can using the\ncompressed image data to only image the interested objects without the rest\nobjects and backgrounds of the scenes with the previous Deep-learning network.\nOur results demonstrate that no matter interested object is single feature or\nrich details, the interference can be successfully filtered out and this idea\ncan be applied in some common applications that effectively improve the\nperformance. This bio-inspired optical system can act as the creature eye to\nachieve success on interested-based object imaging, object detection, object\nrecognition and object tracking, etc.",
          "link": "http://arxiv.org/abs/2108.04236",
          "publishedOn": "2021-08-11T01:55:22.361Z",
          "wordCount": 607,
          "title": "An optical biomimetic eyes with interested object imaging. (arXiv:2108.04236v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Chongyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1\">Srinivas Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>",
          "description": "To improve the accessibility of smart devices and to simplify their usage,\nbuilding models which understand user interfaces (UIs) and assist users to\ncomplete their tasks is critical. However, unique challenges are proposed by\nUI-specific characteristics, such as how to effectively leverage multimodal UI\nfeatures that involve image, text, and structural metadata and how to achieve\ngood performance when high-quality labeled data is unavailable. To address such\nchallenges we introduce UIBert, a transformer-based joint image-text model\ntrained through novel pre-training tasks on large-scale unlabeled UI data to\nlearn generic feature representations for a UI and its components. Our key\nintuition is that the heterogeneous features in a UI are self-aligned, i.e.,\nthe image and text features of UI components, are predictive of each other. We\npropose five pretraining tasks utilizing this self-alignment among different\nfeatures of a UI component and across various components in the same UI. We\nevaluate our method on nine real-world downstream UI tasks where UIBert\noutperforms strong multimodal baselines by up to 9.26% accuracy.",
          "link": "http://arxiv.org/abs/2107.13731",
          "publishedOn": "2021-08-11T01:55:22.350Z",
          "wordCount": 637,
          "title": "UIBert: Learning Generic Multimodal Representations for UI Understanding. (arXiv:2107.13731v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1\">Hamza Rasaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>",
          "description": "Ultrasound is a non-invasive imaging modality that can be conveniently used\nto classify suspicious breast nodules and potentially detect the onset of\nbreast cancer. Recently, Convolutional Neural Networks (CNN) techniques have\nshown promising results in classifying ultrasound images of the breast into\nbenign or malignant. However, CNN inference acts as a black-box model, and as\nsuch, its decision-making is not interpretable. Therefore, increasing effort\nhas been dedicated to explaining this process, most notably through GRAD-CAM\nand other techniques that provide visual explanations into inner workings of\nCNNs. In addition to interpretation, these methods provide clinically important\ninformation, such as identifying the location for biopsy or treatment. In this\nwork, we analyze how adversarial assaults that are practically undetectable may\nbe devised to alter these importance maps dramatically. Furthermore, we will\nshow that this change in the importance maps can come with or without altering\nthe classification result, rendering them even harder to detect. As such, care\nmust be taken when using these importance maps to shed light on the inner\nworkings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and\npropose a new network based on ResNet-50 to improve the classification\naccuracies. Our sensitivity and specificity is comparable to the state of the\nart results.",
          "link": "http://arxiv.org/abs/2108.04345",
          "publishedOn": "2021-08-11T01:55:22.345Z",
          "wordCount": 667,
          "title": "Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images. (arXiv:2108.04345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wencong_C/0/1/0/all/0/1\">CHENG Wencong</a> (1) ((1) Beijing Aviation Meteorological Institute)",
          "description": "Meteorology satellite visible light images is critical for meteorology\nsupport and forecast. However, there is no such kind of data during night time.\nTo overcome this, we propose a method based on deep learning to create\nsynthetic satellite visible light images during night. Specifically, to produce\nmore realistic products, we train a Generative Adversarial Networks (GAN) model\nto generate visible light images given the corresponding satellite infrared\nimages and numerical weather prediction(NWP) products. To better model the\nnonlinear relationship from infrared data and NWP products to visible light\nimages, we propose to use the channel-wise attention mechanics, e.g., SEBlock\nto quantitative weight the input channels. The experiments based on the ECMWF\nNWP products and FY-4A meteorology satellite visible light and infrared\nchannels date show that the proposed methods can be effective to create\nrealistic synthetic satellite visible light images during night.",
          "link": "http://arxiv.org/abs/2108.04330",
          "publishedOn": "2021-08-11T01:55:22.330Z",
          "wordCount": 586,
          "title": "Creating synthetic meteorology satellite visible light images during night based on GAN method. (arXiv:2108.04330v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any\nuser-information. We optimize these faces, by using an evolutionary algorithm\nin the latent embedding space of the StyleGAN face generator. Multiple\nevolutionary strategies are compared, and we propose a novel approach that\nemploys a neural network in order to direct the search in the direction of\npromising samples, without adding fitness evaluations. The results we present\ndemonstrate that it is possible to obtain a high coverage of the LFW identities\n(over 40%) with less than 10 master faces, for three leading deep face\nrecognition systems.",
          "link": "http://arxiv.org/abs/2108.01077",
          "publishedOn": "2021-08-11T01:55:22.324Z",
          "wordCount": 607,
          "title": "Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yunpei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>",
          "description": "In recent years, research on adversarial attacks has become a hot spot.\nAlthough current literature on the transfer-based adversarial attack has\nachieved promising results for improving the transferability to unseen\nblack-box models, it still leaves a long way to go. Inspired by the idea of\nmeta-learning, this paper proposes a novel architecture called Meta Gradient\nAdversarial Attack (MGAA), which is plug-and-play and can be integrated with\nany existing gradient-based attack method for improving the cross-model\ntransferability. Specifically, we randomly sample multiple models from a model\nzoo to compose different tasks and iteratively simulate a white-box attack and\na black-box attack in each task. By narrowing the gap between the gradient\ndirections in white-box and black-box attacks, the transferability of\nadversarial examples on the black-box setting can be improved. Extensive\nexperiments on the CIFAR10 and ImageNet datasets show that our architecture\noutperforms the state-of-the-art methods for both black-box and white-box\nattack settings.",
          "link": "http://arxiv.org/abs/2108.04204",
          "publishedOn": "2021-08-11T01:55:22.318Z",
          "wordCount": 610,
          "title": "Meta Gradient Adversarial Attack. (arXiv:2108.04204v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1\">Lin Yen-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Alberto Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>",
          "description": "We present iNeRF, a framework that performs mesh-free pose estimation by\n\"inverting\" a Neural RadianceField (NeRF). NeRFs have been shown to be\nremarkably effective for the task of view synthesis - synthesizing\nphotorealistic novel views of real-world scenes or objects. In this work, we\ninvestigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,\nRGB-only 6DoF pose estimation - given an image, find the translation and\nrotation of a camera relative to a 3D object or scene. Our method assumes that\nno object mesh models are available during either training or test time.\nStarting from an initial pose estimate, we use gradient descent to minimize the\nresidual between pixels rendered from a NeRF and pixels in an observed image.\nIn our experiments, we first study 1) how to sample rays during pose refinement\nfor iNeRF to collect informative gradients and 2) how different batch sizes of\nrays affect iNeRF on a synthetic dataset. We then show that for complex\nreal-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating\nthe camera poses of novel images and using these images as additional training\ndata for NeRF. Finally, we show iNeRF can perform category-level object pose\nestimation, including object instances not seen during training, with RGB\nimages by inverting a NeRF model inferred from a single view.",
          "link": "http://arxiv.org/abs/2012.05877",
          "publishedOn": "2021-08-11T01:55:22.312Z",
          "wordCount": 708,
          "title": "INeRF: Inverting Neural Radiance Fields for Pose Estimation. (arXiv:2012.05877v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11962",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1\">Alex Ling Yu Hung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1\">John Galeotti</a>",
          "description": "Ultrasound 3D compounding is important for volumetric reconstruction, but as\nof yet there is no consensus on best practices for compounding. Ultrasound\nimages depend on probe direction and the path sound waves pass through, so when\nmultiple intersecting B-scans of the same spot from different perspectives\nyield different pixel values, there is not a single, ideal representation for\ncompounding (i.e. combining) the overlapping pixel values. Current popular\nmethods inevitably suppress or altogether leave out bright or dark regions that\nare useful, and potentially introduce new artifacts. In this work, we establish\na new algorithm to compound the overlapping pixels from different view points\nin ultrasound. We uniquely leverage Laplacian and Gaussian Pyramids to preserve\nthe maximum boundary contrast without overemphasizing noise and speckle. We\nevaluate our algorithm by comparing ours with previous algorithms, and we show\nthat our approach not only preserves both light and dark details, but also\nsomewhat suppresses artifacts, rather than amplifying them.",
          "link": "http://arxiv.org/abs/2011.11962",
          "publishedOn": "2021-08-11T01:55:22.307Z",
          "wordCount": 662,
          "title": "Good and Bad Boundaries in Ultrasound Compounding: Preserving Anatomic Boundaries While Suppressing Artifacts. (arXiv:2011.11962v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Video content creation keeps growing at an incredible pace; yet, creating\nengaging stories remains challenging and requires non-trivial video editing\nexpertise. Many video editing components are astonishingly hard to automate\nprimarily due to the lack of raw video materials. This paper focuses on a new\ntask for computational video editing, namely the task of raking cut\nplausibility. Our key idea is to leverage content that has already been edited\nto learn fine-grained audiovisual patterns that trigger cuts. To do this, we\nfirst collected a data source of more than 10K videos, from which we extract\nmore than 255K cuts. We devise a model that learns to discriminate between real\nand artificial cuts via contrastive learning. We set up a new task and a set of\nbaselines to benchmark video cut generation. We observe that our proposed model\noutperforms the baselines by large margins. To demonstrate our model in\nreal-world applications, we conduct human studies in a collection of unedited\nvideos. The results show that our model does a better job at cutting than\nrandom and alternative baselines.",
          "link": "http://arxiv.org/abs/2108.04294",
          "publishedOn": "2021-08-11T01:55:22.267Z",
          "wordCount": 624,
          "title": "Learning to Cut by Watching Movies. (arXiv:2108.04294v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheuerman_M/0/1/0/all/0/1\">Morgan Klaus Scheuerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1\">Emily Denton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanna_A/0/1/0/all/0/1\">Alex Hanna</a>",
          "description": "Data is a crucial component of machine learning. The field is reliant on data\nto train, validate, and test models. With increased technical capabilities,\nmachine learning research has boomed in both academic and industry settings,\nand one major focus has been on computer vision. Computer vision is a popular\ndomain of machine learning increasingly pertinent to real-world applications,\nfrom facial recognition in policing to object detection for autonomous\nvehicles. Given computer vision's propensity to shape machine learning research\nand impact human life, we seek to understand disciplinary practices around\ndataset documentation - how data is collected, curated, annotated, and packaged\ninto datasets for computer vision researchers and practitioners to use for\nmodel tuning and development. Specifically, we examine what dataset\ndocumentation communicates about the underlying values of vision data and the\nlarger practices and goals of computer vision as a field. To conduct this\nstudy, we collected a corpus of about 500 computer vision datasets, from which\nwe sampled 114 dataset publications across different vision tasks. Through both\na structured and thematic content analysis, we document a number of values\naround accepted data practices, what makes desirable data, and the treatment of\nhumans in the dataset construction process. We discuss how computer vision\ndatasets authors value efficiency at the expense of care; universality at the\nexpense of contextuality; impartiality at the expense of positionality; and\nmodel work at the expense of data work. Many of the silenced values we identify\nsit in opposition with social computing practices. We conclude with suggestions\non how to better incorporate silenced values into the dataset creation and\ncuration process.",
          "link": "http://arxiv.org/abs/2108.04308",
          "publishedOn": "2021-08-11T01:55:22.254Z",
          "wordCount": 747,
          "title": "Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. (arXiv:2108.04308v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Haocheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caleb Chen Cao</a>",
          "description": "Explanation of AI, as well as fairness of algorithms' decisions and the\ntransparency of the decision model, are becoming more and more important. And\nit is crucial to design effective and human-friendly techniques when opening\nthe black-box model. Counterfactual conforms to the human way of thinking and\nprovides a human-friendly explanation, and its corresponding explanation\nalgorithm refers to a strategic alternation of a given data point so that its\nmodel output is \"counter-facted\", i.e. the prediction is reverted. In this\npaper, we adapt counterfactual explanation over fine-grained image\nclassification problem. We demonstrated an adaptive method that could give a\ncounterfactual explanation by showing the composed counterfactual feature map\nusing top-down layer searching algorithm (TDLS). We have proved that our TDLS\nalgorithm could provide more flexible counterfactual visual explanation in an\nefficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,\nwe discussed several applicable scenarios of counterfactual visual\nexplanations.",
          "link": "http://arxiv.org/abs/2108.04238",
          "publishedOn": "2021-08-11T01:55:22.232Z",
          "wordCount": 608,
          "title": "TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Gang Mei</a>",
          "description": "Geohazards such as landslides have caused great losses to the safety of\npeople's lives and property, which is often accompanied with surface cracks. If\nsuch surface cracks could be identified in time, it is of great significance\nfor the monitoring and early warning of geohazards. Currently, the most common\nmethod for crack identification is manual detection, which is with low\nefficiency and accuracy. In this paper, a deep transfer learning framework is\nproposed to effectively and efficiently identify slope surface cracks for the\nsake of fast monitoring and early warning of geohazards such as landslides. The\nessential idea is to employ transfer learning by training (a) the large sample\ndataset of concrete cracks and (b) the small sample dataset of soil and rock\nmasses cracks. In the proposed framework, (1) pretrained cracks identification\nmodels are constructed based on the large sample dataset of concrete cracks;\n(2) refined cracks identification models are further constructed based on the\nsmall sample dataset of soil and rock masses cracks. The proposed framework\ncould be applied to conduct UAV surveys on high-steep slopes to realize the\nmonitoring and early warning of landslides to ensure the safety of people's\nlives and property.",
          "link": "http://arxiv.org/abs/2108.04235",
          "publishedOn": "2021-08-11T01:55:22.146Z",
          "wordCount": 632,
          "title": "Deep Transfer Learning for Identifications of Slope Surface Cracks. (arXiv:2108.04235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiaopeng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ran Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "This report describes Megvii-3D team's approach to-wards SimLocMatch\nChallenge @ CVPR 2021 Image Matching Workshop.",
          "link": "http://arxiv.org/abs/2108.04466",
          "publishedOn": "2021-08-11T01:55:22.125Z",
          "wordCount": 450,
          "title": "Method Towards CVPR 2021 SimLocMatch Challenge. (arXiv:2108.04466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otte_M/0/1/0/all/0/1\">Maximilian Otte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delfosse_Q/0/1/0/all/0/1\">Quentin Delfosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czech_J/0/1/0/all/0/1\">Johannes Czech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Motivated by the interaction between cells, the recently introduced concept\nof Neural Cellular Automata shows promising results in a variety of tasks. So\nfar, this concept was mostly used to generate images for a single scenario. As\neach scenario requires a new model, this type of generation seems contradictory\nto the adaptability of cells in nature. To address this contradiction, we\nintroduce a concept using different initial environments as input while using a\nsingle Neural Cellular Automata to produce several outputs. Additionally, we\nintroduce GANCA, a novel algorithm that combines Neural Cellular Automata with\nGenerative Adversarial Networks, allowing for more generalization through\nadversarial training. The experiments show that a single model is capable of\nlearning several images when presented with different inputs, and that the\nadversarially trained model improves drastically on out-of-distribution data\ncompared to a supervised trained model.",
          "link": "http://arxiv.org/abs/2108.04328",
          "publishedOn": "2021-08-11T01:55:22.120Z",
          "wordCount": 582,
          "title": "Generative Adversarial Neural Cellular Automata. (arXiv:2108.04328v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rainbow_B/0/1/0/all/0/1\">Ben A. Rainbow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1\">Qianhui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>",
          "description": "Predicting the movement trajectories of multiple classes of road users in\nreal-world scenarios is a challenging task due to the diverse trajectory\npatterns. While recent works of pedestrian trajectory prediction successfully\nmodelled the influence of surrounding neighbours based on the relative\ndistances, they are ineffective on multi-class trajectory prediction. This is\nbecause they ignore the impact of the implicit correlations between different\ntypes of road users on the trajectory to be predicted - for example, a nearby\npedestrian has a different level of influence from a nearby car. In this paper,\nwe propose to introduce class information into a graph convolutional neural\nnetwork to better predict the trajectory of an individual. We embed the class\nlabels of the surrounding objects into the label adjacency matrix (LAM), which\nis combined with the velocity-based adjacency matrix (VAM) comprised of the\nobjects' velocity, thereby generating a semantics-guided graph adjacency (SAM).\nSAM effectively models semantic information with trainable parameters to\nautomatically learn the embedded label features that will contribute to the\nfixed velocity-based trajectory. Such information of spatial and temporal\ndependencies is passed to a graph convolutional and temporal convolutional\nnetwork to estimate the predicted trajectory distributions. We further propose\nnew metrics, known as Average2 Displacement Error (aADE) and Average Final\nDisplacement Error (aFDE), that assess network accuracy more accurately. We\ncall our framework Semantics-STGCNN. It consistently shows superior performance\nto the state-of-the-arts in existing and the newly proposed metrics.",
          "link": "http://arxiv.org/abs/2108.04740",
          "publishedOn": "2021-08-11T01:55:22.084Z",
          "wordCount": 681,
          "title": "Semantics-STGCNN: A Semantics-guided Spatial-Temporal Graph Convolutional Network for Multi-class Trajectory Prediction. (arXiv:2108.04740v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1\">Weili Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weizhi Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Person reidentification (ReID) is a very hot research topic in machine\nlearning and computer vision, and many person ReID approaches have been\nproposed; however, most of these methods assume that the same person has the\nsame clothes within a short time interval, and thus their visual appearance\nmust be similar. However, in an actual surveillance environment, a given person\nhas a great probability of changing clothes after a long time span, and they\nalso often take different personal belongings with them. When the existing\nperson ReID methods are applied in this type of case, almost all of them fail.\nTo date, only a few works have focused on the cloth-changing person ReID task,\nbut since it is very difficult to extract generalized and robust features for\nrepresenting people with different clothes, their performances need to be\nimproved. Moreover, visual-semantic information is often ignored. To solve\nthese issues, in this work, a novel multigranular visual-semantic embedding\nalgorithm (MVSE) is proposed for cloth-changing person ReID, where visual\nsemantic information and human attributes are embedded into the network, and\nthe generalized features of human appearance can be well learned to effectively\nsolve the problem of clothing changes. Specifically, to fully represent a\nperson with clothing changes, a multigranular feature representation scheme\n(MGR) is employed to focus on the unchanged part of the human, and then a cloth\ndesensitization network (CDN) is designed to improve the feature robustness of\nthe approach for the person with different clothing, where different high-level\nhuman attributes are fully utilized. Moreover, to further solve the issue of\npose changes and occlusion under different camera perspectives, a partially\nsemantically aligned network (PSA) is proposed to obtain the visual-semantic\ninformation that is used to align the human attributes.",
          "link": "http://arxiv.org/abs/2108.04527",
          "publishedOn": "2021-08-11T01:55:22.079Z",
          "wordCount": 724,
          "title": "Multigranular Visual-Semantic Embedding for Cloth-Changing Person Re-identification. (arXiv:2108.04527v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhaoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>",
          "description": "The defect detection task can be regarded as a realistic scenario of object\ndetection in the computer vision field and it is widely used in the industrial\nfield. Directly applying vanilla object detector to defect detection task can\nachieve promising results, while there still exists challenging issues that\nhave not been solved. The first issue is the texture shift which means a\ntrained defect detector model will be easily affected by unseen texture, and\nthe second issue is partial visual confusion which indicates that a partial\ndefect box is visually similar with a complete box. To tackle these two\nproblems, we propose a Reference-based Defect Detection Network (RDDN).\nSpecifically, we introduce template reference and context reference to against\nthose two problems, respectively. Template reference can reduce the texture\nshift from image, feature or region levels, and encourage the detectors to\nfocus more on the defective area as a result. We can use either well-aligned\ntemplate images or the outputs of a pseudo template generator as template\nreferences in this work, and they are jointly trained with detectors by the\nsupervision of normal samples. To solve the partial visual confusion issue, we\npropose to leverage the carried context information of context reference, which\nis the concentric bigger box of each region proposal, to perform more accurate\nregion classification and regression. Experiments on two defect detection\ndatasets demonstrate the effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2108.04456",
          "publishedOn": "2021-08-11T01:55:22.073Z",
          "wordCount": 674,
          "title": "Reference-based Defect Detection Network. (arXiv:2108.04456v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1\">Shaheer U. Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunguan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1\">Vasilis Stavrinides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M. C. Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Richard E. Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey A. Sonn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>",
          "description": "The performance of many medical image analysis tasks are strongly associated\nwith image data quality. When developing modern deep learning algorithms,\nrather than relying on subjective (human-based) image quality assessment (IQA),\ntask amenability potentially provides an objective measure of task-specific\nimage quality. To predict task amenability, an IQA agent is trained using\nreinforcement learning (RL) with a simultaneously optimised task predictor,\nsuch as a classification or segmentation neural network. In this work, we\ndevelop transfer learning or adaptation strategies to increase the adaptability\nof both the IQA agent and the task predictor so that they are less dependent on\nhigh-quality, expert-labelled training data. The proposed transfer learning\nstrategy re-formulates the original RL problem for task amenability in a\nmeta-reinforcement learning (meta-RL) framework. The resulting algorithm\nfacilitates efficient adaptation of the agent to different definitions of image\nquality, each with its own Markov decision process environment including\ndifferent images, labels and an adaptable task predictor. Our work demonstrates\nthat the IQA agents pre-trained on non-expert task labels can be adapted to\npredict task amenability as defined by expert task labels, using only a small\nset of expert labels. Using 6644 clinical ultrasound images from 249 prostate\ncancer patients, our results for image classification and segmentation tasks\nshow that the proposed IQA method can be adapted using data with as few as\nrespective 19.7% and 29.6% expert-reviewed consensus labels and still achieve\ncomparable IQA and task performance, which would otherwise require a training\ndataset with 100% expert labels.",
          "link": "http://arxiv.org/abs/2108.04359",
          "publishedOn": "2021-08-11T01:55:22.067Z",
          "wordCount": 727,
          "title": "Adaptable image quality assessment using meta-reinforcement learning of task amenability. (arXiv:2108.04359v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "We present a new method to create spatial data using a generative adversarial\nnetwork (GAN). Our contribution uses coarse and widely available geospatial\ndata to create maps of less available features at the finer scale in the built\nenvironment, bypassing their traditional acquisition techniques (e.g. satellite\nimagery or land surveying). In the work, we employ land use data and road\nnetworks as input to generate building footprints, and conduct experiments in 9\ncities around the world. The method, which we implement in a tool we release\nopenly, enables generating approximate maps of the urban form, and it is\ngeneralisable to augment other types of geoinformation, enhancing the\ncompleteness and quality of spatial data infrastructure. It may be especially\nuseful in locations missing detailed and high-resolution data and those that\nare mapped with uncertain or heterogeneous quality, such as much of\nOpenStreetMap. The quality of the results is influenced by the urban form and\nscale. In most cases, experiments suggest promising performance as the method\ntends to truthfully indicate the locations, amount, and shape of buildings. The\nwork has the potential to support several applications, such as energy,\nclimate, and urban morphology studies in areas previously lacking required\ndata.",
          "link": "http://arxiv.org/abs/2108.04232",
          "publishedOn": "2021-08-11T01:55:22.034Z",
          "wordCount": 628,
          "title": "GANmapper: geographical content filling. (arXiv:2108.04232v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wenshan Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_K/0/1/0/all/0/1\">Kit Ian Kou</a>",
          "description": "In this article, a robust color-edge feature extraction method based on the\nQuaternion Hardy filter is proposed. The Quaternion Hardy filter is an emerging\nedge detection theory. It is along with the Poisson and conjugate Poisson\nsmoothing kernels to handle various types of noise. Combining with the\nQuaternion Hardy filter, Jin's color gradient operator and Hough transform, the\ncolor-edge feature detection algorithm is proposed and applied to the lane\nmarking detection. Experiments are presented to demonstrate the validity of the\nproposed algorithm. The results are accurate and robust with respect to the\ncomplex environment lane markings.",
          "link": "http://arxiv.org/abs/2108.04356",
          "publishedOn": "2021-08-11T01:55:22.029Z",
          "wordCount": 532,
          "title": "A Robust Lane Detection Associated with Quaternion Hardy Filter. (arXiv:2108.04356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdary_G/0/1/0/all/0/1\">G Jignesh Chowdary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1\">Suganya G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_P/0/1/0/all/0/1\">Premalatha M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karunamurthy K</a>",
          "description": "Tuberculosis is an infectious disease that is leading to the death of\nmillions of people across the world. The mortality rate of this disease is high\nin patients suffering from immuno-compromised disorders. The early diagnosis of\nthis disease can save lives and can avoid further complications. But the\ndiagnosis of TB is a very complex task. The standard diagnostic tests still\nrely on traditional procedures developed in the last century. These procedures\nare slow and expensive. So this paper presents an automatic approach for the\ndiagnosis of TB from posteroanterior chest x-rays. This is a two-step approach,\nwhere in the first step the lung regions are segmented from the chest x-rays\nusing the graph cut method, and then in the second step the transfer learning\nof VGG16 combined with Bi-directional LSTM is used for extracting high-level\ndiscriminative features from the segmented lung regions and then classification\nis performed using a fully connected layer. The proposed model is evaluated\nusing data from two publicly available databases namely Montgomery Country set\nand Schezien set. The proposed model achieved accuracy and sensitivity of\n97.76%, 97.01% and 96.42%, 94.11% on Schezien and Montgomery county datasets.\nThis model enhanced the diagnostic accuracy of TB by 0.7% and 11.68% on\nSchezien and Montgomery county datasets.",
          "link": "http://arxiv.org/abs/2108.04329",
          "publishedOn": "2021-08-11T01:55:22.022Z",
          "wordCount": 672,
          "title": "Class dependency based learning using Bi-LSTM coupled with the transfer learning of VGG16 for the diagnosis of Tuberculosis from chest x-rays. (arXiv:2108.04329v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kibeom Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Seogkyu Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>",
          "description": "Style transfer aims to reproduce content images with the styles from\nreference images. Existing universal style transfer methods successfully\ndeliver arbitrary styles to original images either in an artistic or a\nphoto-realistic way. However, the range of 'arbitrary style' defined by\nexisting works is bounded in the particular domain due to their structural\nlimitation. Specifically, the degrees of content preservation and stylization\nare established according to a predefined target domain. As a result, both\nphoto-realistic and artistic models have difficulty in performing the desired\nstyle transfer for the other domain. To overcome this limitation, we propose a\nunified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer\nnot only the style but also the property of domain (i.e., domainness) from a\ngiven reference image. To this end, we design a novel domainness indicator that\ncaptures the domainness value from the texture and structural features of\nreference images. Moreover, we introduce a unified framework with domain-aware\nskip connection to adaptively transfer the stroke and palette to the input\ncontents guided by the domainness indicator. Our extensive experiments validate\nthat our model produces better qualitative results and outperforms previous\nmethods in terms of proxy metrics on both artistic and photo-realistic\nstylizations.",
          "link": "http://arxiv.org/abs/2108.04441",
          "publishedOn": "2021-08-11T01:55:21.935Z",
          "wordCount": 633,
          "title": "Domain-Aware Universal Style Transfer. (arXiv:2108.04441v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Ka-Hei Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>",
          "description": "We present SP-GAN, a new unsupervised sphere-guided generative model for\ndirect synthesis of 3D shapes in the form of point clouds. Compared with\nexisting models, SP-GAN is able to synthesize diverse and high-quality shapes\nwith fine details and promote controllability for part-aware shape generation\nand manipulation, yet trainable without any parts annotations. In SP-GAN, we\nincorporate a global prior (uniform points on a sphere) to spatially guide the\ngenerative process and attach a local prior (a random latent code) to each\nsphere point to provide local details. The key insight in our design is to\ndisentangle the complex 3D shape generation task into a global shape modeling\nand a local structure adjustment, to ease the learning process and enhance the\nshape generation quality. Also, our model forms an implicit dense\ncorrespondence between the sphere points and points in every generated shape,\nenabling various forms of structure-aware shape manipulations such as part\nediting, part-wise shape interpolation, and multi-shape part composition, etc.,\nbeyond the existing generative models. Experimental results, which include both\nvisual and quantitative evaluations, demonstrate that our model is able to\nsynthesize diverse point clouds with fine details and less noise, as compared\nwith the state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.04476",
          "publishedOn": "2021-08-11T01:55:21.930Z",
          "wordCount": 653,
          "title": "SP-GAN: Sphere-Guided 3D Shape Generation and Manipulation. (arXiv:2108.04476v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1\">Yuki Tatsunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1\">Masato Taki</a>",
          "description": "For the past ten years, CNN has reigned supreme in the world of computer\nvision, but recently, Transformer is on the rise. However, the quadratic\ncomputational cost of self-attention has become a severe problem of practice.\nThere has been much research on architectures without CNN and self-attention in\nthis context. In particular, MLP-Mixer is a simple idea designed using MLPs and\nhit an accuracy comparable to the Vision Transformer. However, the only\ninductive bias in this architecture is the embedding of tokens. Thus, there is\nstill a possibility to build a non-convolutional inductive bias into the\narchitecture itself, and we built in an inductive bias using two simple ideas.\nA way is to divide the token-mixing block vertically and horizontally. Another\nway is to make spatial correlations denser among some channels of token-mixing.\nWith this approach, we were able to improve the accuracy of the MLP-Mixer while\nreducing its parameters and computational complexity. Compared to other\nMLP-based models, the proposed model, named RaftMLP has a good balance of\ncomputational complexity, the number of parameters, and actual memory usage. In\naddition, our work indicates that MLP-based models have the potential to\nreplace CNNs by adopting inductive bias. The source code in PyTorch version is\navailable at \\url{https://github.com/okojoalg/raft-mlp}.",
          "link": "http://arxiv.org/abs/2108.04384",
          "publishedOn": "2021-08-11T01:55:21.919Z",
          "wordCount": 651,
          "title": "RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?. (arXiv:2108.04384v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ralekar_C/0/1/0/all/0/1\">Chetan Ralekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Shubham Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan Kumar Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1\">Santanu Chaudhury</a>",
          "description": "Human observers engage in selective information uptake when classifying\nvisual patterns. The same is true of deep neural networks, which currently\nconstitute the best performing artificial vision systems. Our goal is to\nexamine the congruence, or lack thereof, in the information-gathering\nstrategies of the two systems. We have operationalized our investigation as a\ncharacter recognition task. We have used eye-tracking to assay the spatial\ndistribution of information hotspots for humans via fixation maps and an\nactivation mapping technique for obtaining analogous distributions for deep\nnetworks through visualization maps. Qualitative comparison between\nvisualization maps and fixation maps reveals an interesting correlate of\ncongruence. The deep learning model considered similar regions in character,\nwhich humans have fixated in the case of correctly classified characters. On\nthe other hand, when the focused regions are different for humans and deep\nnets, the characters are typically misclassified by the latter. Hence, we\npropose to use the visual fixation maps obtained from the eye-tracking\nexperiment as a supervisory input to align the model's focus on relevant\ncharacter regions. We find that such supervision improves the model's\nperformance significantly and does not require any additional parameters. This\napproach has the potential to find applications in diverse domains such as\nmedical analysis and surveillance in which explainability helps to determine\nsystem fidelity.",
          "link": "http://arxiv.org/abs/2108.04558",
          "publishedOn": "2021-08-11T01:55:21.913Z",
          "wordCount": 664,
          "title": "Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks. (arXiv:2108.04558v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wancheng Ge</a>",
          "description": "Deep Neural Networks (DNNs) are vulnerable to adversarial examples which\nwould inveigle neural networks to make prediction errors with small per-\nturbations on the input images. Researchers have been devoted to promoting the\nresearch on the universal adversarial perturbations (UAPs) which are\ngradient-free and have little prior knowledge on data distributions. Procedural\nadversarial noise at- tack is a data-free universal perturbation generation\nmethod. In this paper, we propose two universal adversarial perturbation (UAP)\ngeneration methods based on procedural noise functions: Simplex noise and\nWorley noise. In our framework, the shading which disturbs visual\nclassification is generated with rendering technology. Without changing the\nsemantic representations, the adversarial examples generated via our methods\nshow superior performance on the attack.",
          "link": "http://arxiv.org/abs/2108.04409",
          "publishedOn": "2021-08-11T01:55:21.899Z",
          "wordCount": 560,
          "title": "On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "Blind face inpainting refers to the task of reconstructing visual contents\nwithout explicitly indicating the corrupted regions in a face image.\nInherently, this task faces two challenges: (1) how to detect various mask\npatterns of different shapes and contents; (2) how to restore visually\nplausible and pleasing contents in the masked regions. In this paper, we\npropose a novel two-stage blind face inpainting method named Frequency-guided\nTransformer and Top-Down Refinement Network (FT-TDR) to tackle these\nchallenges. Specifically, we first use a transformer-based network to detect\nthe corrupted regions to be inpainted as masks by modeling the relation among\ndifferent patches. We also exploit the frequency modality as complementary\ninformation for improved detection results and capture the local contextual\nincoherence to enhance boundary consistency. Then a top-down refinement network\nis proposed to hierarchically restore features at different levels and generate\ncontents that are semantically consistent with the unmasked face regions.\nExtensive experiments demonstrate that our method outperforms current\nstate-of-the-art blind and non-blind face inpainting methods qualitatively and\nquantitatively.",
          "link": "http://arxiv.org/abs/2108.04424",
          "publishedOn": "2021-08-11T01:55:21.867Z",
          "wordCount": 608,
          "title": "FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face Inpainting. (arXiv:2108.04424v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Differentiable Neural Architecture Search is one of the most popular Neural\nArchitecture Search (NAS) methods for its search efficiency and simplicity,\naccomplished by jointly optimizing the model weight and architecture parameters\nin a weight-sharing supernet via gradient-based algorithms. At the end of the\nsearch phase, the operations with the largest architecture parameters will be\nselected to form the final architecture, with the implicit assumption that the\nvalues of architecture parameters reflect the operation strength. While much\nhas been discussed about the supernet's optimization, the architecture\nselection process has received little attention. We provide empirical and\ntheoretical analysis to show that the magnitude of architecture parameters does\nnot necessarily indicate how much the operation contributes to the supernet's\nperformance. We propose an alternative perturbation-based architecture\nselection that directly measures each operation's influence on the supernet. We\nre-evaluate several differentiable NAS methods with the proposed architecture\nselection and find that it is able to extract significantly improved\narchitectures from the underlying supernets consistently. Furthermore, we find\nthat several failure modes of DARTS can be greatly alleviated with the proposed\nselection method, indicating that much of the poor generalization observed in\nDARTS can be attributed to the failure of magnitude-based architecture\nselection rather than entirely the optimization of its supernet.",
          "link": "http://arxiv.org/abs/2108.04392",
          "publishedOn": "2021-08-11T01:55:21.840Z",
          "wordCount": 656,
          "title": "Rethinking Architecture Selection in Differentiable NAS. (arXiv:2108.04392v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1\">Ipsita Sutradhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mahziba Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mehedi Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1\">Malabika Sarker</a>",
          "description": "Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as\na result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye\nillness caused by diabetes, can lead to blindness if it is not identified and\ntreated in its early stages. Unfortunately, diagnosis of DR requires medically\ntrained professionals, but Bangladesh has limited specialists in comparison to\nits population. Moreover, the screening process is often expensive, prohibiting\nmany from receiving timely and proper diagnosis. To address the problem, we\nintroduce a deep learning algorithm which screens for different stages of DR.\nWe use a state-of-the-art CNN architecture to diagnose patients based on\nretinal fundus imagery. This paper is an experimental evaluation of the\nalgorithm we developed for DR diagnosis and screening specifically for\nBangladeshi patients. We perform this validation study using separate pools of\nretinal image data of real patients from a hospital and field studies in\nBangladesh. Our results show that the algorithm is effective at screening\nBangladeshi eyes even when trained on a public dataset which is out of domain,\nand can accurately determine the stage of DR as well, achieving an overall\naccuracy of 92.27\\% and 93.02\\% on two validation sets of Bangladeshi eyes. The\nresults confirm the ability of the algorithm to be used in real clinical\nsettings and applications due to its high accuracy and classwise metrics. Our\nalgorithm is implemented in the application Drishti, which is used to screen\nfor DR in patients living in rural areas in Bangladesh, where access to\nprofessional screening is limited.",
          "link": "http://arxiv.org/abs/2108.04358",
          "publishedOn": "2021-08-11T01:55:21.827Z",
          "wordCount": 706,
          "title": "Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients. (arXiv:2108.04358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miyauchi_R/0/1/0/all/0/1\">Ryoma Miyauchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukusato_T/0/1/0/all/0/1\">Tsukasa Fukusato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyata_K/0/1/0/all/0/1\">Kazunori Miyata</a>",
          "description": "Constructing stroke correspondences between keyframes is one of the most\nimportant processes in the production pipeline of hand-drawn inbetweening\nframes. This process requires time-consuming manual work imposing a tremendous\nburden on the animators. We propose a method to estimate stroke correspondences\nbetween raster character images (keyframes) without vectorization processes.\nFirst, the proposed system separates the closed areas in each keyframe and\nestimates the correspondences between closed areas by using the characteristics\nof shape, depth, and closed area connection. Second, the proposed system\nestimates stroke correspondences from the estimated closed area\ncorrespondences. We demonstrate the effectiveness of our method by performing a\nuser study and comparing the proposed system with conventional approaches.",
          "link": "http://arxiv.org/abs/2108.04393",
          "publishedOn": "2021-08-11T01:55:21.785Z",
          "wordCount": 551,
          "title": "Stroke Correspondence by Labeling Closed Areas. (arXiv:2108.04393v1 [cs.GR])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.07139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alaka_S/0/1/0/all/0/1\">Souridas Alaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreekumar_R/0/1/0/all/0/1\">Rishikesh Sreekumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>",
          "description": "Data analysis has become a necessity in the modern era of cricket. Everything\nfrom effective team management to match win predictions use some form of\nanalytics. Meaningful data representations are necessary for efficient analysis\nof data. In this study we investigate the use of adaptive (learnable)\nembeddings to represent inter-related features (such as players, teams, etc).\nThe data used for this study is collected from a classical T20 tournament IPL\n(Indian Premier League). To naturally facilitate the learning of meaningful\nrepresentations of features for accurate data analysis, we formulate a deep\nrepresentation learning framework which jointly learns a custom set of\nembeddings (which represents our features of interest) through the minimization\nof a contrastive loss. We base our objective on a set of classes obtained as a\nresult of hierarchical clustering on the overall run rate of an innings. It's\nbeen assessed that the framework ensures greater generality in the obtained\nembeddings, on top of which a task based analysis of overall run rate\nprediction was done to show the reliability of the framework.",
          "link": "http://arxiv.org/abs/2108.07139",
          "publishedOn": "2021-08-17T01:54:53.552Z",
          "wordCount": 618,
          "title": "Efficient Feature Representations for Cricket Data Analysis using Deep Learning based Multi-Modal Fusion Model. (arXiv:2108.07139v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangupantulu_R/0/1/0/all/0/1\">Rohit Gangupantulu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1\">Tyler Cody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_P/0/1/0/all/0/1\">Paul Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">Abdul Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenbeiser_L/0/1/0/all/0/1\">Logan Eisenbeiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radke_D/0/1/0/all/0/1\">Dan Radke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_R/0/1/0/all/0/1\">Ryan Clark</a>",
          "description": "Reinforcement learning (RL) has been applied to attack graphs for penetration\ntesting, however, trained agents do not reflect reality because the attack\ngraphs lack operational nuances typically captured within the intelligence\npreparation of the battlefield (IPB) that include notions of (cyber) terrain.\nIn particular, current practice constructs attack graphs exclusively using the\nCommon Vulnerability Scoring System (CVSS) and its components. We present\nmethods for constructing attack graphs using notions from IPB on cyber terrain\nanalysis of obstacles, avenues of approach, key terrain, observation and fields\nof fire, and cover and concealment. We demonstrate our methods on an example\nwhere firewalls are treated as obstacles and represented in (1) the reward\nspace and (2) the state dynamics. We show that terrain analysis can be used to\nbring realism to attack graphs for RL.",
          "link": "http://arxiv.org/abs/2108.07124",
          "publishedOn": "2021-08-17T01:54:53.463Z",
          "wordCount": 581,
          "title": "Using Cyber Terrain in Reinforcement Learning for Penetration Testing. (arXiv:2108.07124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1\">Mark Sellke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>",
          "description": "We consider incentivized exploration: a version of multi-armed bandits where\nthe choice of arms is controlled by self-interested agents, and the algorithm\ncan only issue recommendations. The algorithm controls the flow of information,\nand the information asymmetry can incentivize the agents to explore. Prior work\nachieves optimal regret rates up to multiplicative factors that become\narbitrarily large depending on the Bayesian priors, and scale exponentially in\nthe number of arms. A more basic problem of sampling each arm once runs into\nsimilar factors.\n\nWe focus on the price of incentives: the loss in performance, broadly\nconstrued, incurred for the sake of incentive-compatibility. We prove that\nThompson Sampling, a standard bandit algorithm, is incentive-compatible if\ninitialized with sufficiently many data points. The performance loss due to\nincentives is therefore limited to the initial rounds when these data points\nare collected. The problem is largely reduced to that of sample complexity: how\nmany rounds are needed? We address this question, providing matching upper and\nlower bounds and instantiating them in various corollaries. Typically, the\noptimal sample complexity is polynomial in the number of arms and exponential\nin the \"strength of beliefs\".",
          "link": "http://arxiv.org/abs/2002.00558",
          "publishedOn": "2021-08-17T01:54:53.457Z",
          "wordCount": 696,
          "title": "The Price of Incentivizing Exploration: A Characterization via Thompson Sampling and Sample Complexity. (arXiv:2002.00558v5 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.01722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>",
          "description": "Video-based person re-identification has drawn massive attention in recent\nyears due to its extensive applications in video surveillance. While deep\nlearning-based methods have led to significant progress, these methods are\nlimited by ineffectively using complementary information, which is blamed on\nnecessary data augmentation in the training process. Data augmentation has been\nwidely used to mitigate the over-fitting trap and improve the ability of\nnetwork representation. However, the previous methods adopt image-based data\naugmentation scheme to individually process the input frames, which corrupts\nthe complementary information between consecutive frames and causes performance\ndegradation. Extensive experiments on three benchmark datasets demonstrate that\nour framework outperforms the most recent state-of-the-art methods. We also\nperform cross-dataset validation to prove the generality of our method.",
          "link": "http://arxiv.org/abs/1905.01722",
          "publishedOn": "2021-08-17T01:54:53.375Z",
          "wordCount": 609,
          "title": "Intra-clip Aggregation for Video Person Re-identification. (arXiv:1905.01722v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>",
          "description": "Real-world recommender system needs to be regularly retrained to keep with\nthe new data. In this work, we consider how to efficiently retrain graph\nconvolution network (GCN) based recommender models, which are state-of-the-art\ntechniques for collaborative recommendation. To pursue high efficiency, we set\nthe target as using only new data for model updating, meanwhile not sacrificing\nthe recommendation accuracy compared with full model retraining. This is\nnon-trivial to achieve, since the interaction data participates in both the\ngraph structure for model construction and the loss function for model\nlearning, whereas the old graph structure is not allowed to use in model\nupdating. Towards the goal, we propose a \\textit{Causal Incremental Graph\nConvolution} approach, which consists of two new operators named\n\\textit{Incremental Graph Convolution} (IGC) and \\textit{Colliding Effect\nDistillation} (CED) to estimate the output of full graph convolution. In\nparticular, we devise simple and effective modules for IGC to ingeniously\ncombine the old representations and the incremental graph and effectively fuse\nthe long-term and short-term preference signals. CED aims to avoid the\nout-of-date issue of inactive nodes that are not in the incremental graph,\nwhich connects the new data with inactive nodes through causal inference. In\nparticular, CED estimates the causal effect of new data on the representation\nof inactive nodes through the control of their collider. Extensive experiments\non three real-world datasets demonstrate both accuracy gains and significant\nspeed-ups over the existing retraining mechanism.",
          "link": "http://arxiv.org/abs/2108.06889",
          "publishedOn": "2021-08-17T01:54:50.403Z",
          "wordCount": 676,
          "title": "Causal Incremental Graph Convolution for Recommender System Retraining. (arXiv:2108.06889v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yutong Li</a>",
          "description": "Building an independent misspelling detector and serve it before correction\ncan bring multiple benefits to speller and other search components, which is\nparticularly true for the most commonly deployed noisy-channel based speller\nsystems. With rapid development of deep learning and substantial advancement in\ncontextual representation learning such as BERTology, building a decent\nmisspelling detector without having to rely on hand-crafted features associated\nwith noisy-channel architecture becomes more-than-ever accessible. However\nBERTolgy models are trained with natural language corpus but Maps Search is\nhighly domain specific, would BERTology continue its success. In this paper we\ndesign 4 stages of models for misspeling detection ranging from the most basic\nLSTM to single-domain augmented fine-tuned BERT. We found for Maps Search in\nour case, other advanced BERTology family model such as RoBERTa does not\nnecessarily outperform BERT, and a classic cross-domain fine-tuned full BERT\neven underperforms a smaller single-domain fine-tuned BERT. We share more\nfindings through comprehensive modeling experiments and analysis, we also\nbriefly cover the data generation algorithm breakthrough.",
          "link": "http://arxiv.org/abs/2108.06842",
          "publishedOn": "2021-08-17T01:54:50.391Z",
          "wordCount": 596,
          "title": "Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations. (arXiv:2108.06842v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06847",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ha_W/0/1/0/all/0/1\">Wooseok Ha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>",
          "description": "Recent deep-learning models have achieved impressive predictive performance\nby learning complex functions of many variables, often at the cost of\ninterpretability. This chapter covers recent work aiming to interpret models by\nattributing importance to features and feature groups for a single prediction.\nImportantly, the proposed attributions assign importance to interactions\nbetween features, in addition to features in isolation. These attributions are\nshown to yield insights across real-world domains, including bio-imaging,\ncosmology image and natural-language processing. We then show how these\nattributions can be used to directly improve the generalization of a neural\nnetwork or to distill it into a simple model. Throughout the chapter, we\nemphasize the use of reality checks to scrutinize the proposed interpretation\ntechniques.",
          "link": "http://arxiv.org/abs/2108.06847",
          "publishedOn": "2021-08-17T01:54:50.306Z",
          "wordCount": 549,
          "title": "Interpreting and improving deep-learning models with reality checks. (arXiv:2108.06847v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lexing Ying</a>",
          "description": "We introduce a class of variational actor-critic algorithms based on a\nvariational formulation over both the value function and the policy. The\nobjective function of the variational formulation consists of two parts: one\nfor maximizing the value function and the other for minimizing the Bellman\nresidual. Besides the vanilla gradient descent with both the value function and\nthe policy updates, we propose two variants, the clipping method and the\nflipping method, in order to speed up the convergence. We also prove that, when\nthe prefactor of the Bellman residual is sufficiently large, the fixed point of\nthe algorithm is close to the optimal policy.",
          "link": "http://arxiv.org/abs/2108.01215",
          "publishedOn": "2021-08-17T01:54:50.255Z",
          "wordCount": 551,
          "title": "Variational Actor-Critic Algorithms. (arXiv:2108.01215v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "We propose Future Discriminators for Generation (FUDGE), a flexible and\nmodular method for controlled text generation. Given a pre-existing model G for\ngenerating text from a distribution of interest, FUDGE enables conditioning on\na desired attribute a (for example, formality) while requiring access only to\nG's output logits. FUDGE learns an attribute predictor operating on a partial\nsequence, and uses this predictor's outputs to adjust G's original\nprobabilities. We show that FUDGE models terms corresponding to a Bayesian\ndecomposition of the conditional distribution of G given attribute a. Moreover,\nFUDGE can easily compose predictors for multiple desired attributes. We\nevaluate FUDGE on three tasks -- couplet completion in poetry, topic control in\nlanguage generation, and formality change in machine translation -- and observe\ngains in all three tasks.",
          "link": "http://arxiv.org/abs/2104.05218",
          "publishedOn": "2021-08-17T01:54:50.230Z",
          "wordCount": 590,
          "title": "FUDGE: Controlled Text Generation With Future Discriminators. (arXiv:2104.05218v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nomura_M/0/1/0/all/0/1\">Masahiro Nomura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1\">Yuta Saito</a>",
          "description": "A typical assumption in supervised machine learning is that the train\n(source) and test (target) datasets follow completely the same distribution.\nThis assumption is, however, often violated in uncertain real-world\napplications, which motivates the study of learning under covariate shift. In\nthis setting, the naive use of adaptive hyperparameter optimization methods\nsuch as Bayesian optimization does not work as desired since it does not\naddress the distributional shift among different datasets. In this work, we\nconsider a novel hyperparameter optimization problem under the multi-source\ncovariate shift whose goal is to find the optimal hyperparameters for a target\ntask of interest using only unlabeled data in a target task and labeled data in\nmultiple source tasks. To conduct efficient hyperparameter optimization for the\ntarget task, it is essential to estimate the target objective using only the\navailable information. To this end, we construct the variance reduced estimator\nthat unbiasedly approximates the target objective with a desirable variance\nproperty. Building on the proposed estimator, we provide a general and\ntractable hyperparameter optimization procedure, which works preferably in our\nsetting with a no-regret guarantee. The experiments demonstrate that the\nproposed framework broadens the applications of automated hyperparameter\noptimization.",
          "link": "http://arxiv.org/abs/2006.10600",
          "publishedOn": "2021-08-17T01:54:50.109Z",
          "wordCount": 653,
          "title": "Efficient Hyperparameter Optimization under Multi-Source Covariate Shift. (arXiv:2006.10600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06552",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Boschini_M/0/1/0/all/0/1\">Matteo Boschini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Buzzega_P/0/1/0/all/0/1\">Pietro Buzzega</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bonicelli_L/0/1/0/all/0/1\">Lorenzo Bonicelli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Porrello_A/0/1/0/all/0/1\">Angelo Porrello</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>",
          "description": "Continual Learning (CL) investigates how to train Deep Networks on a stream\nof tasks without incurring catastrophic forgetting. CL settings proposed in the\nliterature assume that every incoming example is paired with ground-truth\nannotations. However, this clashes with many real-world applications: gathering\nlabeled data, which is in itself tedious and expensive, becomes indeed\ninfeasible when data flow as a stream and must be consumed in real-time. This\nwork explores Weakly Supervised Continual Learning (WSCL): here, only a small\nfraction of labeled input examples are shown to the learner. We assess how\ncurrent CL methods (e.g.: EWC, LwF, iCaRL, ER, GDumb, DER) perform in this\nnovel and challenging scenario, in which overfitting entangles forgetting.\nSubsequently, we design two novel WSCL methods which exploit metric learning\nand consistency regularization to leverage unsupervised data while learning. In\ndoing so, we show that not only our proposals exhibit higher flexibility when\nsupervised information is scarce, but also that less than 25% labels can be\nenough to reach or even outperform SOTA methods trained under full supervision.",
          "link": "http://arxiv.org/abs/2108.06552",
          "publishedOn": "2021-08-17T01:54:50.069Z",
          "wordCount": 605,
          "title": "Weakly Supervised Continual Learning. (arXiv:2108.06552v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kaiming He</a>",
          "description": "This paper does not describe a novel method. Instead, it studies a\nstraightforward, incremental, yet must-know baseline given the recent progress\nin computer vision: self-supervised learning for Vision Transformers (ViT).\nWhile the training recipes for standard convolutional networks have been highly\nmature and robust, the recipes for ViT are yet to be built, especially in the\nself-supervised scenarios where training becomes more challenging. In this\nwork, we go back to basics and investigate the effects of several fundamental\ncomponents for training self-supervised ViT. We observe that instability is a\nmajor issue that degrades accuracy, and it can be hidden by apparently good\nresults. We reveal that these results are indeed partial failure, and they can\nbe improved when training is made more stable. We benchmark ViT results in MoCo\nv3 and several other self-supervised frameworks, with ablations in various\naspects. We discuss the currently positive evidence as well as challenges and\nopen questions. We hope that this work will provide useful data points and\nexperience for future research.",
          "link": "http://arxiv.org/abs/2104.02057",
          "publishedOn": "2021-08-17T01:54:50.042Z",
          "wordCount": 660,
          "title": "An Empirical Study of Training Self-Supervised Vision Transformers. (arXiv:2104.02057v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1\">Ladislav Ramp&#xe1;&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>",
          "description": "Graph neural networks (GNNs) based on message passing between neighboring\nnodes are known to be insufficient for capturing long-range interactions in\ngraphs. In this project we study hierarchical message passing models that\nleverage a multi-resolution representation of a given graph. This facilitates\nlearning of features that span large receptive fields without loss of local\ninformation, an aspect not studied in preceding work on hierarchical GNNs. We\nintroduce Hierarchical Graph Net (HGNet), which for any two connected nodes\nguarantees existence of message-passing paths of at most logarithmic length\nw.r.t. the input graph size. Yet, under mild assumptions, its internal\nhierarchy maintains asymptotic size equivalent to that of the input graph. We\nobserve that our HGNet outperforms conventional stacking of GCN layers\nparticularly in molecular property prediction benchmarks. Finally, we propose\ntwo benchmarking tasks designed to elucidate capability of GNNs to leverage\nlong-range interactions in graphs.",
          "link": "http://arxiv.org/abs/2107.07432",
          "publishedOn": "2021-08-17T01:54:50.036Z",
          "wordCount": 602,
          "title": "Hierarchical graph neural nets can capture long-range interactions. (arXiv:2107.07432v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "The newly emerged transformer technology has a tremendous impact on NLP\nresearch. In the general English domain, transformer-based models have achieved\nstate-of-the-art performances on various NLP benchmarks. In the clinical\ndomain, researchers also have investigated transformer models for clinical\napplications. The goal of this study is to systematically explore three widely\nused transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical\nrelation extraction and develop an open-source package with clinical\npre-trained transformer-based models to facilitate information extraction in\nthe clinical domain. We developed a series of clinical RE models based on three\ntransformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these\nmodels using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2\nchallenges. We compared two classification strategies (binary vs. multi-class\nclassification) and investigated two approaches to generate candidate relations\nin different experimental settings. In this study, we compared three\ntransformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We\ndemonstrated that the RoBERTa-clinical RE model achieved the best performance\non the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2\ndataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our\nresults indicated that the binary classification strategy consistently\noutperformed the multi-class classification strategy for clinical relation\nextraction. Our methods and models are publicly available at\nhttps://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction.\nWe believe this work will improve current practice on clinical relation\nextraction and other related NLP tasks in the biomedical domain.",
          "link": "http://arxiv.org/abs/2107.08957",
          "publishedOn": "2021-08-17T01:54:50.023Z",
          "wordCount": 708,
          "title": "Clinical Relation Extraction Using Transformer-based Models. (arXiv:2107.08957v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1\">Ananth Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1\">Michael Mathioudakis</a>",
          "description": "Machine unlearning is the task of updating machine learning (ML) models after\na subset of the training data they were trained on is deleted. Methods for the\ntask are desired to combine effectiveness and efficiency, i.e., they should\neffectively \"unlearn\" deleted data, but in a way that does not require\nexcessive computation effort (e.g., a full retraining) for a small amount of\ndeletions. Such a combination is typically achieved by tolerating some amount\nof approximation in the unlearning. In addition, laws and regulations in the\nspirit of \"the right to be forgotten\" have given rise to requirements for\ncertifiability, i.e., the ability to demonstrate that the deleted data has\nindeed been unlearned by the ML model.\n\nIn this paper, we present an experimental study of the three state-of-the-art\napproximate unlearning methods for linear models and demonstrate the trade-offs\nbetween efficiency, effectiveness and certifiability offered by each method. In\nimplementing the study, we extend some of the existing works and describe a\ncommon ML pipeline to compare and evaluate the unlearning methods on six\nreal-world datasets and a variety of settings. We provide insights into the\neffect of the quantity and distribution of the deleted data on ML models and\nthe performance of each unlearning method in different settings. We also\npropose a practical online strategy to determine when the accumulated error\nfrom approximate unlearning is large enough to warrant a full retrain of the ML\nmodel.",
          "link": "http://arxiv.org/abs/2106.15093",
          "publishedOn": "2021-08-17T01:54:50.017Z",
          "wordCount": 701,
          "title": "Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menculini_L/0/1/0/all/0/1\">Lorenzo Menculini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marini_A/0/1/0/all/0/1\">Andrea Marini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proietti_M/0/1/0/all/0/1\">Massimiliano Proietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garinei_A/0/1/0/all/0/1\">Alberto Garinei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozza_A/0/1/0/all/0/1\">Alessio Bozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moretti_C/0/1/0/all/0/1\">Cecilia Moretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marconi_M/0/1/0/all/0/1\">Marcello Marconi</a>",
          "description": "Setting sale prices correctly is of great importance for firms, and the study\nand forecast of prices time series is therefore a relevant topic not only from\na data science perspective but also from an economic and applicative one. In\nthis paper we examine different techniques to forecast sale prices applied by\nan Italian food wholesaler, as a step towards the automation of pricing tasks\nusually taken care by human workforce. We consider ARIMA models and compare\nthem to Prophet, a scalable forecasting tool by Facebook based on a generalized\nadditive model, and to deep learning models exploiting Long Short--Term Memory\n(LSTM) and Convolutional Neural Networks (CNNs). ARIMA models are frequently\nused in econometric analyses, providing a good benchmark for the problem under\nstudy. Our results indicate that ARIMA models and LSTM neural networks perform\nsimilarly for the forecasting task under consideration, while the combination\nof CNNs and LSTMs attains the best overall accuracy, but requires more time to\nbe tuned. On the contrary, Prophet is quick and easy to use, but considerably\nless accurate.t overall accuracy, but requires more time to be tuned. On the\ncontrary, Prophet is quick and easy to use, but considerably less accurate.",
          "link": "http://arxiv.org/abs/2107.12770",
          "publishedOn": "2021-08-17T01:54:49.992Z",
          "wordCount": 694,
          "title": "Comparing Prophet and Deep Learning to ARIMA in Forecasting Wholesale Food Prices. (arXiv:2107.12770v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-08-17T01:54:49.957Z",
          "wordCount": 653,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Cross-entropy loss with softmax output is a standard choice to train neural\nnetwork classifiers. We give a new view of neural network classifiers with\nsoftmax and cross-entropy as mutual information evaluators. We show that when\nthe dataset is balanced, training a neural network with cross-entropy maximises\nthe mutual information between inputs and labels through a variational form of\nmutual information. Thereby, we develop a new form of softmax that also\nconverts a classifier to a mutual information evaluator when the dataset is\nimbalanced. Experimental results show that the new form leads to better\nclassification accuracy, in particular for imbalanced datasets.",
          "link": "http://arxiv.org/abs/2106.10471",
          "publishedOn": "2021-08-17T01:54:49.933Z",
          "wordCount": 564,
          "title": "Neural Network Classifier as Mutual Information Evaluator. (arXiv:2106.10471v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_T/0/1/0/all/0/1\">Takeshi Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unno_H/0/1/0/all/0/1\">Hiroshi Unno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekiyama_T/0/1/0/all/0/1\">Taro Sekiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>",
          "description": "Loop-invariant synthesis is the basis of every program verification\nprocedure. Due to its undecidability in general, a tool for invariant synthesis\nnecessarily uses heuristics. Despite the common belief that the design of\nheuristics is vital for the effective performance of a verifier, little work\nhas been performed toward obtaining the optimal heuristics for each\ninvariant-synthesis tool. Instead, developers have hand-tuned the heuristics of\ntools. This study demonstrates that we can effectively and automatically learn\na good heuristic via reinforcement learning for an invariant synthesizer PCSat.\nOur experiment shows that PCSat combined with the heuristic learned by\nreinforcement learning outperforms the state-of-the-art solvers for this task.\nTo the best of our knowledge, this is the first work that investigates learning\nthe heuristics of an invariant synthesis tool.",
          "link": "http://arxiv.org/abs/2107.09766",
          "publishedOn": "2021-08-17T01:54:49.910Z",
          "wordCount": 589,
          "title": "Enhancing Loop-Invariant Synthesis via Reinforcement Learning. (arXiv:2107.09766v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Ding Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_P/0/1/0/all/0/1\">Parth H. Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zizhan Zheng</a>",
          "description": "We consider a set of APs with unknown data rates that cooperatively serve a\nmobile client. The data rate of each link is i.i.d. sampled from a distribution\nthat is unknown a priori. In contrast to traditional link scheduling problems\nunder uncertainty, we assume that in each time step, the device can probe a\nsubset of links before deciding which one to use. We model this problem as a\ncontextual bandit problem with probing (CBwP) and present an efficient\nalgorithm. We further establish the regret of our algorithm for links with\nBernoulli data rates. Our CBwP model is a novel extension of the classic\ncontextual bandit model and can potentially be applied to a large class of\nsequential decision-making problems that involve joint probing and play under\nuncertainty.",
          "link": "http://arxiv.org/abs/2108.03297",
          "publishedOn": "2021-08-17T01:54:49.898Z",
          "wordCount": 587,
          "title": "Joint AP Probing and Scheduling: A Contextual Bandit Approach. (arXiv:2108.03297v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hacohen_G/0/1/0/all/0/1\">Guy Hacohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1\">Daphna Weinshall</a>",
          "description": "Recent work suggests that convolutional neural networks of different\narchitectures learn to classify images in the same order. To understand this\nphenomenon, we revisit the over-parametrized deep linear network model. Our\nasymptotic analysis, assuming that the hidden layers are wide enough, reveals\nthat the convergence rate of this model's parameters is exponentially faster\nalong directions corresponding to the larger principal components of the data,\nat a rate governed by the singular values. We term this convergence pattern the\nPrincipal Components bias (PC-bias). We show how the PC-bias streamlines the\norder of learning of both linear and non-linear networks, more prominently at\nearlier stages of learning. We then compare our results to the simplicity bias,\nshowing that both biases can be seen independently, and affect the order of\nlearning in different ways. Finally, we discuss how the PC-bias may explain\nsome benefits of early stopping and its connection to PCA, and why deep\nnetworks converge more slowly when given random labels.",
          "link": "http://arxiv.org/abs/2105.05553",
          "publishedOn": "2021-08-17T01:54:49.893Z",
          "wordCount": 628,
          "title": "Principal Components Bias in Deep Neural Networks. (arXiv:2105.05553v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiexia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Furong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Juanjuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kejiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>",
          "description": "Accurate traffic state prediction is the foundation of transportation control\nand guidance. It is very challenging due to the complex spatiotemporal\ndependencies in traffic data. Existing works cannot perform well for multi-step\ntraffic prediction that involves long future time period. The spatiotemporal\ninformation dilution becomes serve when the time gap between input step and\npredicted step is large, especially when traffic data is not sufficient or\nnoisy. To address this issue, we propose a multi-spatial graph convolution\nbased Seq2Seq model. Our main novelties are three aspects: (1) We enrich the\nspatiotemporal information of model inputs by fusing multi-view features (time,\nlocation and traffic states) (2) We build multiple kinds of spatial\ncorrelations based on both prior knowledge and data-driven knowledge to improve\nmodel performance especially in insufficient or noisy data cases. (3) A\nspatiotemporal attention mechanism based on reachability knowledge is novelly\ndesigned to produce high-level features fed into decoder of Seq2Seq directly to\nease information dilution. Our model is evaluated on two real world traffic\ndatasets and achieves better performance than other competitors.",
          "link": "http://arxiv.org/abs/2107.01528",
          "publishedOn": "2021-08-17T01:54:49.887Z",
          "wordCount": 653,
          "title": "Incorporating Reachability Knowledge into a Multi-Spatial Graph Convolution Based Seq2Seq Model for Traffic Forecasting. (arXiv:2107.01528v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>",
          "description": "Graph-based multi-view clustering has become an active topic due to the\nefficiency in characterizing both the complex structure and relationship\nbetween multimedia data. However, existing methods have the following\nshortcomings: (1) They are inefficient or even fail for graph learning in large\nscale due to the graph construction and eigen-decomposition. (2) They cannot\nwell exploit both the complementary information and spatial structure embedded\nin graphs of different views. To well exploit complementary information and\ntackle the scalability issue plaguing graph-based multi-view clustering, we\npropose an efficient multiple graph learning model via a small number of anchor\npoints and tensor Schatten p-norm minimization. Specifically, we construct a\nhidden and tractable large graph by anchor graph for each view and well exploit\ncomplementary information embedded in anchor graphs of different views by\ntensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,\nwhich scales linearly with the data size, to solve our proposed model.\nExtensive experimental results on several datasets indicate that our proposed\nmethod outperforms some state-of-the-art multi-view clustering algorithms.",
          "link": "http://arxiv.org/abs/2106.15382",
          "publishedOn": "2021-08-17T01:54:49.871Z",
          "wordCount": 629,
          "title": "Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+John_T/0/1/0/all/0/1\">Thrupthi Ann John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>",
          "description": "As Deep Neural Network models for face processing tasks approach human-like\nperformance, their deployment in critical applications such as law enforcement\nand access control has seen an upswing, where any failure may have far-reaching\nconsequences. We need methods to build trust in deployed systems by making\ntheir working as transparent as possible. Existing visualization algorithms are\ndesigned for object recognition and do not give insightful results when applied\nto the face domain. In this work, we present 'Canonical Saliency Maps', a new\nmethod that highlights relevant facial areas by projecting saliency maps onto a\ncanonical face model. We present two kinds of Canonical Saliency Maps:\nimage-level maps and model-level maps. Image-level maps highlight facial\nfeatures responsible for the decision made by a deep face model on a given\nimage, thus helping to understand how a DNN made a prediction on the image.\nModel-level maps provide an understanding of what the entire DNN model focuses\non in each task and thus can be used to detect biases in the model. Our\nqualitative and quantitative results show the usefulness of the proposed\ncanonical saliency maps, which can be used on any deep face model regardless of\nthe architecture.",
          "link": "http://arxiv.org/abs/2105.01386",
          "publishedOn": "2021-08-17T01:54:49.863Z",
          "wordCount": 682,
          "title": "Canonical Saliency Maps: Decoding Deep Face Models. (arXiv:2105.01386v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with a limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-08-17T01:54:49.858Z",
          "wordCount": 608,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_A/0/1/0/all/0/1\">Austin Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>",
          "description": "Visual engagement in social media platforms comprises interactions with photo\nposts including comments, shares, and likes. In this paper, we leverage such\nvisual engagement clues as supervisory signals for representation learning.\nHowever, learning from engagement signals is non-trivial as it is not clear how\nto bridge the gap between low-level visual information and high-level social\ninteractions. We present VisE, a weakly supervised learning approach, which\nmaps social images to pseudo labels derived by clustered engagement signals. We\nthen study how models trained in this way benefit subjective downstream\ncomputer vision tasks such as emotion recognition or political bias detection.\nThrough extensive studies, we empirically demonstrate the effectiveness of VisE\nacross a diverse set of classification tasks beyond the scope of conventional\nrecognition.",
          "link": "http://arxiv.org/abs/2104.07767",
          "publishedOn": "2021-08-17T01:54:49.852Z",
          "wordCount": 600,
          "title": "Exploring Visual Engagement Signals for Representation Learning. (arXiv:2104.07767v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sourjya Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mustafa Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Anand Raghunathan</a>",
          "description": "Deep Neural Networks (DNNs) have transformed the field of machine learning\nand are widely deployed in many applications involving image, video, speech and\nnatural language processing. The increasing compute demands of DNNs have been\nwidely addressed through Graphics Processing Units (GPUs) and specialized\naccelerators. However, as model sizes grow, these von Neumann architectures\nrequire very high memory bandwidth to keep the processing elements utilized as\na majority of the data resides in the main memory. Processing in memory has\nbeen proposed as a promising solution for the memory wall bottleneck for ML\nworkloads. In this work, we propose a new DRAM-based processing-in-memory (PIM)\nmultiplication primitive coupled with intra-bank accumulation to accelerate\nmatrix vector operations in ML workloads. The proposed multiplication primitive\nadds < 1% area overhead and does not require any change in the DRAM\nperipherals. Therefore, the proposed multiplication can be easily adopted in\ncommodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture,\ndata mapping scheme and dataflow for executing DNNs within DRAM. System\nevaluations performed on networks like AlexNet, VGG16 and ResNet18 show that\nthe proposed architecture, mapping, and data flow can provide up to 19.5x\nspeedup over an NVIDIA Titan Xp GPU highlighting the need to overcome the\nmemory bottleneck in future generations of DNN hardware.",
          "link": "http://arxiv.org/abs/2105.03736",
          "publishedOn": "2021-08-17T01:54:49.846Z",
          "wordCount": 685,
          "title": "PIM-DRAM: Accelerating Machine Learning Workloads using Processing in Commodity DRAM. (arXiv:2105.03736v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "We study how to evaluate the quantitative information content of a region\nwithin an image for a particular label. To this end, we bridge class activation\nmaps with information theory. We develop an informative class activation map\n(infoCAM). Given a classification task, infoCAM depict how to accumulate\ninformation of partial regions to that of the entire image toward a label.\nThus, we can utilise infoCAM to locate the most informative features for a\nlabel. When applied to an image classification task, infoCAM performs better\nthan the traditional classification map in the weakly supervised object\nlocalisation task. We achieve state-of-the-art results on Tiny-ImageNet.",
          "link": "http://arxiv.org/abs/2106.10472",
          "publishedOn": "2021-08-17T01:54:49.831Z",
          "wordCount": 564,
          "title": "Informative Class Activation Maps. (arXiv:2106.10472v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1\">Taeyeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1\">Grzegorz Cielniak</a>",
          "description": "In our previous work, we designed a systematic policy to prioritize sampling\nlocations to lead significant accuracy improvement in spatial interpolation by\nusing the prediction uncertainty of Gaussian Process Regression (GPR) as\n\"attraction force\" to deployed robots in path planning. Although the\nintegration with Traveling Salesman Problem (TSP) solvers was also shown to\nproduce relatively short travel distance, we here hypothesise several factors\nthat could decrease the overall prediction precision as well because\nsub-optimal locations may eventually be included in their paths. To address\nthis issue, in this paper, we first explore \"local planning\" approaches\nadopting various spatial ranges within which next sampling locations are\nprioritized to investigate their effects on the prediction performance as well\nas incurred travel distance. Also, Reinforcement Learning (RL)-based high-level\ncontrollers are trained to adaptively produce blended plans from a particular\nset of local planners to inherit unique strengths from that selection depending\non latest prediction states. Our experiments on use cases of temperature\nmonitoring robots demonstrate that the dynamic mixtures of planners can not\nonly generate sophisticated, informative plans that a single planner could not\ncreate alone but also ensure significantly reduced travel distances at no cost\nof prediction reliability without any assist of additional modules for shortest\npath calculation.",
          "link": "http://arxiv.org/abs/2108.06618",
          "publishedOn": "2021-08-17T01:54:49.825Z",
          "wordCount": 650,
          "title": "Adaptive Selection of Informative Path Planning Strategies via Reinforcement Learning. (arXiv:2108.06618v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03002",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">Hongtao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">Yajing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>",
          "description": "More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.",
          "link": "http://arxiv.org/abs/2108.03002",
          "publishedOn": "2021-08-17T01:54:49.820Z",
          "wordCount": 666,
          "title": "Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04150",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yilin Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+DeJong_J/0/1/0/all/0/1\">Jennifer DeJong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Halverson_T/0/1/0/all/0/1\">Tom Halverson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shuman_D/0/1/0/all/0/1\">David I Shuman</a>",
          "description": "Ranked data sets, where m judges/voters specify a preference ranking of n\nobjects/candidates, are increasingly prevalent in contexts such as political\nelections, computer vision, recommender systems, and bioinformatics. The vote\ncounts for each ranking can be viewed as an n! data vector lying on the\npermutahedron, which is a Cayley graph of the symmetric group with vertices\nlabeled by permutations and an edge when two permutations differ by an adjacent\ntransposition. Leveraging combinatorial representation theory and recent\nprogress in signal processing on graphs, we investigate a novel, scalable\ntransform method to interpret and exploit structure in ranked data. We\nrepresent data on the permutahedron using an overcomplete dictionary of atoms,\neach of which captures both smoothness information about the data (typically\nthe focus of spectral graph decomposition methods in graph signal processing)\nand structural information about the data (typically the focus of symmetry\ndecomposition methods from representation theory). These atoms have a more\nnaturally interpretable structure than any known basis for signals on the\npermutahedron, and they form a Parseval frame, ensuring beneficial numerical\nproperties such as energy preservation. We develop specialized algorithms and\nopen software that take advantage of the symmetry and structure of the\npermutahedron to improve the scalability of the proposed method, making it more\napplicable to the high-dimensional ranked data found in applications.",
          "link": "http://arxiv.org/abs/2103.04150",
          "publishedOn": "2021-08-17T01:54:49.814Z",
          "wordCount": 727,
          "title": "Signal Processing on the Permutahedron: Tight Spectral Frames for Ranked Data Analysis. (arXiv:2103.04150v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1\">Jeff Mentch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_J/0/1/0/all/0/1\">Jerry Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Measuring the acoustic characteristics of a space is often done by capturing\nits impulse response (IR), a representation of how a full-range stimulus sound\nexcites it. This work generates an IR from a single image, which can then be\napplied to other signals using convolution, simulating the reverberant\ncharacteristics of the space shown in the image. Recording these IRs is both\ntime-intensive and expensive, and often infeasible for inaccessible locations.\nWe use an end-to-end neural network architecture to generate plausible audio\nimpulse responses from single images of acoustic environments. We evaluate our\nmethod both by comparisons to ground truth data and by human expert evaluation.\nWe demonstrate our approach by generating plausible impulse responses from\ndiverse settings and formats including well known places, musical halls, rooms\nin paintings, images from animations and computer games, synthetic environments\ngenerated from text, panoramic images, and video conference backgrounds.",
          "link": "http://arxiv.org/abs/2103.14201",
          "publishedOn": "2021-08-17T01:54:49.807Z",
          "wordCount": 623,
          "title": "Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis. (arXiv:2103.14201v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kahyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas J. Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McInnes_B/0/1/0/all/0/1\">Bridget McInnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">&#xd6;zlem Uzuner</a>",
          "description": "Methods and Materials: We investigated transferability of neural\nnetwork-based de-identification sys-tems with and without domain\ngeneralization. We used two domain generalization approaches: a novel approach\nJoint-Domain Learning (JDL) as developed in this paper, and a state-of-the-art\ndomain general-ization approach Common-Specific Decomposition (CSD) from the\nliterature. First, we measured trans-ferability from a single external source.\nSecond, we used two external sources and evaluated whether domain\ngeneralization can improve transferability of de-identification models across\ndomains which rep-resent different note types from the same institution. Third,\nusing two external sources with in-domain training data, we studied whether\nexternal source data are useful even in cases where sufficient in-domain\ntraining data are available. Finally, we investigated transferability of the\nde-identification mod-els across institutions. Results and Conclusions: We\nfound transferability from a single external source gave inconsistent re-sults.\nUsing additional external sources consistently yielded an F1-score of\napproximately 80%, but domain generalization was not always helpful to improve\ntransferability. We also found that external sources were useful even in cases\nwhere in-domain training data were available by reducing the amount of needed\nin-domain training data or by improving performance. Transferability across\ninstitutions was differed by note type and annotation label. External sources\nfrom a different institution were also useful to further improve performance.",
          "link": "http://arxiv.org/abs/2102.08517",
          "publishedOn": "2021-08-17T01:54:49.784Z",
          "wordCount": 684,
          "title": "Transferability of Neural Network-based De-identification Systems. (arXiv:2102.08517v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1\">Qinghua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yipeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>",
          "description": "Deep neural networks (DNNs) usually contain massive parameters, but there is\nredundancy such that it is guessed that the DNNs could be trained in\nlow-dimensional subspaces. In this paper, we propose a Dynamic Linear\nDimensionality Reduction (DLDR) based on low-dimensional properties of the\ntraining trajectory. The reduction is efficient, which is supported by\ncomprehensive experiments: optimization in 40 dimensional spaces can achieve\ncomparable performance as regular training over thousands or even millions of\nparameters. Since there are only a few optimization variables, we develop a\nquasi-Newton-based algorithm and also obtain robustness against label noises,\nwhich are two follow-up experiments to show the advantages of finding\nlow-dimensional subspaces.",
          "link": "http://arxiv.org/abs/2103.11154",
          "publishedOn": "2021-08-17T01:54:49.778Z",
          "wordCount": 591,
          "title": "Low Dimensional Landscape Hypothesis is True: DNNs can be Trained in Tiny Subspaces. (arXiv:2103.11154v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "The existence of multiple datasets for sarcasm detection prompts us to apply\ntransfer learning to exploit their commonality. The adversarial neural transfer\n(ANT) framework utilizes multiple loss terms that encourage the source-domain\nand the target-domain feature distributions to be similar while optimizing for\ndomain-specific performance. However, these objectives may be in conflict,\nwhich can lead to optimization difficulties and sometimes diminished transfer.\nWe propose a generalized latent optimization strategy that allows different\nlosses to accommodate each other and improves training dynamics. The proposed\nmethod outperforms transfer learning and meta-learning baselines. In\nparticular, we achieve 10.02% absolute performance gain over the previous state\nof the art on the iSarcasm dataset.",
          "link": "http://arxiv.org/abs/2104.09261",
          "publishedOn": "2021-08-17T01:54:49.766Z",
          "wordCount": 594,
          "title": "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection. (arXiv:2104.09261v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_V/0/1/0/all/0/1\">Vincent Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>",
          "description": "Knowledge graph embedding plays an important role in knowledge\nrepresentation, reasoning, and data mining applications. However, for multiple\ncross-domain knowledge graphs, state-of-the-art embedding models cannot make\nfull use of the data from different knowledge domains while preserving the\nprivacy of exchanged data. In addition, the centralized embedding model may not\nscale to the extensive real-world knowledge graphs. Therefore, we propose a\nnovel decentralized scalable learning framework, \\emph{Federated Knowledge\nGraphs Embedding} (FKGE), where embeddings from different knowledge graphs can\nbe learnt in an asynchronous and peer-to-peer manner while being\nprivacy-preserving. FKGE exploits adversarial generation between pairs of\nknowledge graphs to translate identical entities and relations of different\ndomains into near embedding spaces. In order to protect the privacy of the\ntraining data, FKGE further implements a privacy-preserving neural network\nstructure to guarantee no raw data leakage. We conduct extensive experiments to\nevaluate FKGE on 11 knowledge graphs, demonstrating a significant and\nconsistent improvement in model quality with at most 17.85\\% and 7.90\\%\nincreases in performance on triple classification and link prediction tasks.",
          "link": "http://arxiv.org/abs/2105.07615",
          "publishedOn": "2021-08-17T01:54:49.728Z",
          "wordCount": 639,
          "title": "Differentially Private Federated Knowledge Graphs Embedding. (arXiv:2105.07615v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1\">Vinod Kumar Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sukhdeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>",
          "description": "Handwritten character recognition (HCR) is a challenging learning problem in\npattern recognition, mainly due to similarity in structure of characters,\ndifferent handwriting styles, noisy datasets and a large variety of languages\nand scripts. HCR problem is studied extensively for a few decades but there is\nvery limited research on script independent models. This is because of factors,\nlike, diversity of scripts, focus of the most of conventional research efforts\non handcrafted feature extraction techniques which are language/script specific\nand are not always available, and unavailability of public datasets and codes\nto reproduce the results. On the other hand, deep learning has witnessed huge\nsuccess in different areas of pattern recognition, including HCR, and provides\nend-to-end learning, i.e., automated feature extraction and recognition. In\nthis paper, we have proposed a novel deep learning architecture which exploits\ntransfer learning and image-augmentation for end-to-end learning for script\nindependent handwritten character recognition, called HCR-Net. The network is\nbased on a novel transfer learning approach for HCR, where some of lower layers\nof a pre-trained VGG16 network are utilised. Due to transfer learning and\nimage-augmentation, HCR-Net provides faster training, better performance and\nbetter generalisations. The experimental results on publicly available datasets\nof Bangla, Punjabi, Hindi, English, Swedish, Urdu, Farsi, Tibetan, Kannada,\nMalayalam, Telugu, Marathi, Nepali and Arabic languages prove the efficacy of\nHCR-Net and establishes several new benchmarks. For reproducibility of the\nresults and for the advancements of the HCR research, complete code is publicly\nreleased at \\href{https://github.com/jmdvinodjmd/HCR-Net}{GitHub}.",
          "link": "http://arxiv.org/abs/2108.06663",
          "publishedOn": "2021-08-17T01:54:49.723Z",
          "wordCount": 704,
          "title": "HCR-Net: A deep learning based script independent handwritten character recognition network. (arXiv:2108.06663v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "Data quantity and quality are crucial factors for data-driven learning\nmethods. In some target problem domains, there are not many data samples\navailable, which could significantly hinder the learning process. While data\nfrom similar domains may be leveraged to help through domain adaptation,\nobtaining high-quality labeled data for those source domains themselves could\nbe difficult or costly. To address such challenges on data insufficiency for\nclassification problem in a target domain, we propose a weak adaptation\nlearning (WAL) approach that leverages unlabeled data from a similar source\ndomain, a low-cost weak annotator that produces labels based on task-specific\nheuristics, labeling rules, or other methods (albeit with inaccuracy), and a\nsmall amount of labeled data in the target domain. Our approach first conducts\na theoretical analysis on the error bound of the trained classifier with\nrespect to the data quantity and the performance of the weak annotator, and\nthen introduces a multi-stage weak adaptation learning method to learn an\naccurate classifier by lowering the error bound. Our experiments demonstrate\nthe effectiveness of our approach in learning an accurate classifier with\nlimited labeled data in the target domain and unlabeled data in the source\ndomain.",
          "link": "http://arxiv.org/abs/2102.07358",
          "publishedOn": "2021-08-17T01:54:49.706Z",
          "wordCount": 668,
          "title": "Weak Adaptation Learning -- Addressing Cross-domain Data Insufficiency with Weak Annotator. (arXiv:2102.07358v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lybrand_E/0/1/0/all/0/1\">Eric Lybrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saab_R/0/1/0/all/0/1\">Rayan Saab</a>",
          "description": "We propose a new computationally efficient method for quantizing the weights\nof pre- trained neural networks that is general enough to handle both\nmulti-layer perceptrons and convolutional neural networks. Our method\ndeterministically quantizes layers in an iterative fashion with no complicated\nre-training required. Specifically, we quantize each neuron, or hidden unit,\nusing a greedy path-following algorithm. This simple algorithm is equivalent to\nrunning a dynamical system, which we prove is stable for quantizing a\nsingle-layer neural network (or, alternatively, for quantizing the first layer\nof a multi-layer network) when the training data are Gaussian. We show that\nunder these assumptions, the quantization error decays with the width of the\nlayer, i.e., its level of over-parametrization. We provide numerical\nexperiments, on multi-layer networks, to illustrate the performance of our\nmethods on MNIST and CIFAR10 data, as well as for quantizing the VGG16 network\nusing ImageNet data.",
          "link": "http://arxiv.org/abs/2010.15979",
          "publishedOn": "2021-08-17T01:54:49.694Z",
          "wordCount": 602,
          "title": "A Greedy Algorithm for Quantizing Neural Networks. (arXiv:2010.15979v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1\">Dirk Tasche</a>",
          "description": "For the binary prevalence quantification problem under prior probability\nshift, we determine the asymptotic variance of the maximum likelihood\nestimator. We find that it is a function of the Brier score for the regression\nof the class label against the features under the test data set distribution.\nThis observation suggests that optimising the accuracy of a base classifier on\nthe training data set helps to reduce the variance of the related quantifier on\nthe test data set. Therefore, we also point out training criteria for the base\nclassifier that imply optimisation of both of the Brier scores on the training\nand the test data sets.",
          "link": "http://arxiv.org/abs/2107.08209",
          "publishedOn": "2021-08-17T01:54:49.672Z",
          "wordCount": 576,
          "title": "Minimising quantifier variance under prior probability shift. (arXiv:2107.08209v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Seung Won Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sitao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidayetoglu_M/0/1/0/all/0/1\">Mert Hidayeto&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_E/0/1/0/all/0/1\">Eiman Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>",
          "description": "Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale\ngraph-based recommender systems. Training GCN requires the minibatch generator\ntraversing graphs and sampling the sparsely located neighboring nodes to obtain\ntheir features. Since real-world graphs often exceed the capacity of GPU\nmemory, current GCN training systems keep the feature table in host memory and\nrely on the CPU to collect sparse features before sending them to the GPUs.\nThis approach, however, puts tremendous pressure on host memory bandwidth and\nthe CPU. This is because the CPU needs to (1) read sparse features from memory,\n(2) write features into memory as a dense format, and (3) transfer the features\nfrom memory to the GPUs. In this work, we propose a novel GPU-oriented data\ncommunication approach for GCN training, where GPU threads directly access\nsparse features in host memory through zero-copy accesses without much CPU\nhelp. By removing the CPU gathering stage, our method significantly reduces the\nconsumption of the host resources and data access latency. We further present\ntwo important techniques to achieve high host memory access efficiency by the\nGPU: (1) automatic data access address alignment to maximize PCIe packet\nefficiency, and (2) asynchronous zero-copy access and kernel execution to fully\noverlap data transfer with training. We incorporate our method into PyTorch and\nevaluate its effectiveness using several graphs with sizes up to 111 million\nnodes and 1.6 billion edges. In a multi-GPU training setup, our method is\n65-92% faster than the conventional data transfer method, and can even match\nthe performance of all-in-GPU-memory training for some graphs that fit in GPU\nmemory.",
          "link": "http://arxiv.org/abs/2103.03330",
          "publishedOn": "2021-08-17T01:54:49.644Z",
          "wordCount": 753,
          "title": "Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture. (arXiv:2103.03330v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Sin Kit Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1\">Hye-Young Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>",
          "description": "Federated learning is an emerging privacy-preserving AI technique where\nclients (i.e., organisations or devices) train models locally and formulate a\nglobal model based on the local model updates without transferring local data\nexternally. However, federated learning systems struggle to achieve\ntrustworthiness and embody responsible AI principles. In particular, federated\nlearning systems face accountability and fairness challenges due to\nmulti-stakeholder involvement and heterogeneity in client data distribution. To\nenhance the accountability and fairness of federated learning systems, we\npresent a blockchain-based trustworthy federated learning architecture. We\nfirst design a smart contract-based data-model provenance registry to enable\naccountability. Additionally, we propose a weighted fair data sampler algorithm\nto enhance fairness in training data. We evaluate the proposed approach using a\nCOVID-19 X-ray detection use case. The evaluation results show that the\napproach is feasible to enable accountability and improve fairness. The\nproposed algorithm can achieve better performance than the default federated\nlearning setting in terms of the model's generalisation and accuracy.",
          "link": "http://arxiv.org/abs/2108.06912",
          "publishedOn": "2021-08-17T01:54:49.629Z",
          "wordCount": 640,
          "title": "Blockchain-based Trustworthy Federated Learning Architecture. (arXiv:2108.06912v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_X/0/1/0/all/0/1\">Xin Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>",
          "description": "Unsupervised representation learning has achieved outstanding performances\nusing centralized data available on the Internet. However, the increasing\nawareness of privacy protection limits sharing of decentralized unlabeled image\ndata that grows explosively in multiple parties (e.g., mobile phones and\ncameras). As such, a natural problem is how to leverage these data to learn\nvisual representations for downstream tasks while preserving data privacy. To\naddress this problem, we propose a novel federated unsupervised learning\nframework, FedU. In this framework, each party trains models from unlabeled\ndata independently using contrastive learning with an online network and a\ntarget network. Then, a central server aggregates trained models and updates\nclients' models with the aggregated model. It preserves data privacy as each\nparty only has access to its raw data. Decentralized data among multiple\nparties are normally non-independent and identically distributed (non-IID),\nleading to performance degradation. To tackle this challenge, we propose two\nsimple but effective methods: 1) We design the communication protocol to upload\nonly the encoders of online networks for server aggregation and update them\nwith the aggregated encoder; 2) We introduce a new module to dynamically decide\nhow to update predictors based on the divergence caused by non-IID. The\npredictor is the other component of the online network. Extensive experiments\nand ablations demonstrate the effectiveness and significance of FedU. It\noutperforms training with only one party by over 5% and other methods by over\n14% in linear and semi-supervised evaluation on non-IID data.",
          "link": "http://arxiv.org/abs/2108.06492",
          "publishedOn": "2021-08-17T01:54:49.623Z",
          "wordCount": 696,
          "title": "Collaborative Unsupervised Visual Representation Learning from Decentralized Data. (arXiv:2108.06492v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Charlie Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thekumparampil_K/0/1/0/all/0/1\">Kiran K. Thekumparampil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1\">Giulia Fanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>",
          "description": "A central question in federated learning (FL) is how to design optimization\nalgorithms that minimize the communication cost of training a model over\nheterogeneous data distributed across many clients. A popular technique for\nreducing communication is the use of local steps, where clients take multiple\noptimization steps over local data before communicating with the server (e.g.,\nFedAvg, SCAFFOLD). This contrasts with centralized methods, where clients take\none optimization step per communication round (e.g., Minibatch SGD). A recent\nlower bound on the communication complexity of first-order methods shows that\ncentralized methods are optimal over highly-heterogeneous data, whereas local\nmethods are optimal over purely homogeneous data [Woodworth et al., 2020]. For\nintermediate heterogeneity levels, no algorithm is known to match the lower\nbound. In this paper, we propose a multistage optimization scheme that nearly\nmatches the lower bound across all heterogeneity levels. The idea is to first\nrun a local method up to a heterogeneity-induced error floor; next, we switch\nto a centralized method for the remaining steps. Our analysis may help explain\nempirically-successful stepsize decay methods in FL [Charles et al., 2020;\nReddi et al., 2020]. We demonstrate the scheme's practical utility in image\nclassification tasks.",
          "link": "http://arxiv.org/abs/2108.06869",
          "publishedOn": "2021-08-17T01:54:49.617Z",
          "wordCount": 643,
          "title": "Reducing the Communication Cost of Federated Learning through Multistage Optimization. (arXiv:2108.06869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03932",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ayatollahi_F/0/1/0/all/0/1\">Fazael Ayatollahi</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Shokouhi_S/0/1/0/all/0/1\">Shahriar B. Shokouhi</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Mann_R/0/1/0/all/0/1\">Ritse M. Mann</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a> (2 and 3) ((1) Electrical Engineering Department, Iran University of Science and Technology (IUST), Tehran, Iran, (2) Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, the Netherlands, (3) Department of Radiation Oncology, Netherlands Cancer Institute, Amsterdam, the Netherlands)",
          "description": "Purpose: We propose a deep learning-based computer-aided detection (CADe)\nmethod to detect breast lesions in ultrafast DCE-MRI sequences. This method\nuses both the three-dimensional spatial information and temporal information\nobtained from the early-phase of the dynamic acquisition. Methods: The proposed\nCADe method, based on a modified 3D RetinaNet model, operates on ultrafast T1\nweighted sequences, which are preprocessed for motion compensation, temporal\nnormalization, and are cropped before passing into the model. The model is\noptimized to enable the detection of relatively small breast lesions in a\nscreening setting, focusing on detection of lesions that are harder to\ndifferentiate from confounding structures inside the breast. Results: The\nmethod was developed based on a dataset consisting of 489 ultrafast MRI studies\nobtained from 462 patients containing a total of 572 lesions (365 malignant,\n207 benign) and achieved a detection rate, sensitivity, and detection rate of\nbenign lesions of 0.90 (0.876-0.934), 0.95 (0.934-0.980), and 0.81\n(0.751-0.871) at 4 false positives per normal breast with 10-fold\ncross-testing, respectively. Conclusions: The deep learning architecture used\nfor the proposed CADe application can efficiently detect benign and malignant\nlesions on ultrafast DCE-MRI. Furthermore, utilizing the less visible hard-to\ndetect-lesions in training improves the learning process and, subsequently,\ndetection of malignant breast lesions.",
          "link": "http://arxiv.org/abs/2102.03932",
          "publishedOn": "2021-08-17T01:54:49.611Z",
          "wordCount": 723,
          "title": "Automatic Breast Lesion Detection in Ultrafast DCE-MRI Using Deep Learning. (arXiv:2102.03932v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huaizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shanshan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yong Luo</a>",
          "description": "AI engineering has emerged as a crucial discipline to democratize deep neural\nnetwork (DNN) models among software developers with a diverse background. In\nparticular, altering these DNN models in the deployment stage posits a\ntremendous challenge. In this research, we propose and develop a low-code\nsolution, ModelPS (an acronym for \"Model Photoshop\"), to enable and empower\ncollaborative DNN model editing and intelligent model serving. The ModelPS\nsolution embodies two transformative features: 1) a user-friendly web interface\nfor a developer team to share and edit DNN models pictorially, in a low-code\nfashion, and 2) a model genie engine in the backend to aid developers in\ncustomizing model editing configurations for given deployment requirements or\nconstraints. Our case studies with a wide range of deep learning (DL) models\nshow that the system can tremendously reduce both development and communication\noverheads with improved productivity.",
          "link": "http://arxiv.org/abs/2105.08275",
          "publishedOn": "2021-08-17T01:54:49.605Z",
          "wordCount": 637,
          "title": "ModelPS: An Interactive and Collaborative Platform for Editing Pre-trained Models at Scale. (arXiv:2105.08275v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pantazis_O/0/1/0/all/0/1\">Omiros Pantazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brostow_G/0/1/0/all/0/1\">Gabriel Brostow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_K/0/1/0/all/0/1\">Kate Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>",
          "description": "We address the problem of learning self-supervised representations from\nunlabeled image collections. Unlike existing approaches that attempt to learn\nuseful features by maximizing similarity between augmented versions of each\ninput image or by speculatively picking negative samples, we instead also make\nuse of the natural variation that occurs in image collections that are captured\nusing static monitoring cameras. To achieve this, we exploit readily available\ncontext data that encodes information such as the spatial and temporal\nrelationships between the input images. We are able to learn representations\nthat are surprisingly effective for downstream supervised classification, by\nfirst identifying high probability positive pairs at training time, i.e. those\nimages that are likely to depict the same visual concept. For the critical task\nof global biodiversity monitoring, this results in image features that can be\nadapted to challenging visual species classification tasks with limited human\nsupervision. We present results on four different camera trap image\ncollections, across three different families of self-supervised learning\nmethods, and show that careful image selection at training time results in\nsuperior performance compared to existing baselines such as conventional\nself-supervised training and transfer learning.",
          "link": "http://arxiv.org/abs/2108.06435",
          "publishedOn": "2021-08-17T01:54:49.599Z",
          "wordCount": 633,
          "title": "Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring. (arXiv:2108.06435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_M/0/1/0/all/0/1\">Megan Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jingjing Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acero_A/0/1/0/all/0/1\">Alex Acero</a>",
          "description": "Named entity recognition (NER) is usually developed and tested on text from\nwell-written sources. However, in intelligent voice assistants, where NER is an\nimportant component, input to NER may be noisy because of user or speech\nrecognition error. In applications, entity labels may change frequently, and\nnon-textual properties like topicality or popularity may be needed to choose\namong alternatives.\n\nWe describe a NER system intended to address these problems. We test and\ntrain this system on a proprietary user-derived dataset. We compare with a\nbaseline text-only NER system; the baseline enhanced with external gazetteers;\nand the baseline enhanced with the search and indirect labelling techniques we\ndescribe below. The final configuration gives around 6% reduction in NER error\nrate. We also show that this technique improves related tasks, such as semantic\nparsing, with an improvement of up to 5% in error rate.",
          "link": "http://arxiv.org/abs/2108.06633",
          "publishedOn": "2021-08-17T01:54:49.575Z",
          "wordCount": 609,
          "title": "DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants. (arXiv:2108.06633v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1\">Amartya Sanyal</a>",
          "description": "Deep learning research has recently witnessed an impressively fast-paced\nprogress in a wide range of tasks including computer vision, natural language\nprocessing, and reinforcement learning. The extraordinary performance of these\nsystems often gives the impression that they can be used to revolutionise our\nlives for the better. However, as recent works point out, these systems suffer\nfrom several issues that make them unreliable for use in the real world,\nincluding vulnerability to adversarial attacks (Szegedy et al. [248]), tendency\nto memorise noise (Zhang et al. [292]), being over-confident on incorrect\npredictions (miscalibration) (Guo et al. [99]), and unsuitability for handling\nprivate data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of\nthese issues in detail, investigate their causes, and propose computationally\ncheap algorithms for mitigating them in practice. To do this, we identify\nstructures in deep neural networks that can be exploited to mitigate the above\ncauses of unreliability of deep learning algorithms.",
          "link": "http://arxiv.org/abs/2108.07083",
          "publishedOn": "2021-08-17T01:54:49.570Z",
          "wordCount": 593,
          "title": "Identifying and Exploiting Structures for Reliable Deep Learning. (arXiv:2108.07083v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1\">Mengting Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yusan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>",
          "description": "Modeling inter-dependencies between time-series is the key to achieve high\nperformance in anomaly detection for multivariate time-series data. The\nde-facto solution to model the dependencies is to feed the data into a\nrecurrent neural network (RNN). However, the fully connected network structure\nunderneath the RNN (either GRU or LSTM) assumes a static and complete\ndependency graph between time-series, which may not hold in many real-world\napplications. To alleviate this assumption, we propose a dynamic bipartite\ngraph structure to encode the inter-dependencies between time-series. More\nconcretely, we model time series as one type of nodes, and the time series\nsegments (regarded as event) as another type of nodes, where the edge between\ntwo types of nodes describe a temporal pattern occurred on a specific time\nseries at a certain time. Based on this design, relations between time series\ncan be explicitly modelled via dynamic connections to event nodes, and the\nmultivariate time-series anomaly detection problem can be formulated as a\nself-supervised, edge stream prediction problem in dynamic graphs. We conducted\nextensive experiments to demonstrate the effectiveness of the design.",
          "link": "http://arxiv.org/abs/2108.06783",
          "publishedOn": "2021-08-17T01:54:49.553Z",
          "wordCount": 629,
          "title": "Event2Graph: Event-driven Bipartite Graph for Multivariate Time-series Anomaly Detection. (arXiv:2108.06783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "Since the Lipschitz properties of convolutional neural networks (CNNs) are\nwidely considered to be related to adversarial robustness, we theoretically\ncharacterize the $\\ell_1$ norm and $\\ell_\\infty$ norm of 2D multi-channel\nconvolutional layers and provide efficient methods to compute the exact\n$\\ell_1$ norm and $\\ell_\\infty$ norm. Based on our theorem, we propose a novel\nregularization method termed norm decay, which can effectively reduce the norms\nof convolutional layers and fully-connected layers. Experiments show that\nnorm-regularization methods, including norm decay, weight decay, and singular\nvalue clipping, can improve generalization of CNNs. However, they can slightly\nhurt adversarial robustness. Observing this unexpected phenomenon, we compute\nthe norms of layers in the CNNs trained with three different adversarial\ntraining frameworks and surprisingly find that adversarially robust CNNs have\ncomparable or even larger layer norms than their non-adversarially robust\ncounterparts. Furthermore, we prove that under a mild assumption, adversarially\nrobust classifiers can be achieved using neural networks, and an adversarially\nrobust neural network can have an arbitrarily large Lipschitz constant. For\nthis reason, enforcing small norms on CNN layers may be neither necessary nor\neffective in achieving adversarial robustness. The code is available at\nhttps://github.com/youweiliang/norm_robustness.",
          "link": "http://arxiv.org/abs/2009.08435",
          "publishedOn": "2021-08-17T01:54:49.547Z",
          "wordCount": 710,
          "title": "Large Norms of CNN Layers Do Not Hurt Adversarial Robustness. (arXiv:2009.08435v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.01699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karmakar_S/0/1/0/all/0/1\">Sayar Karmakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Anirbit Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthukumar_R/0/1/0/all/0/1\">Ramchandran Muthukumar</a>",
          "description": "In this work, we study the possibility of defending against \"data-poisoning\"\nattacks while learning a neural net. We focus on the supervised learning setup\nfor a class of finite-sized depth-2 nets - which include the standard single\nfilter convolutional nets. For this setup we attempt to learn the true label\ngenerating weights in the presence of a malicious oracle doing stochastic\nbounded and additive adversarial distortions on the true labels being accessed\nby the algorithm during training. For the non-gradient stochastic algorithm\nthat we instantiate we prove (worst case nearly optimal) trade-offs among the\nmagnitude of the adversarial attack, the accuracy, and the confidence achieved\nby the proposed algorithm. Additionally, our algorithm uses mini-batching and\nwe keep track of how the mini-batch size affects the convergence.",
          "link": "http://arxiv.org/abs/2005.01699",
          "publishedOn": "2021-08-17T01:54:49.531Z",
          "wordCount": 604,
          "title": "Guarantees on learning depth-2 neural networks under a data-poisoning attack. (arXiv:2005.01699v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_V/0/1/0/all/0/1\">Violet Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeNero_J/0/1/0/all/0/1\">John DeNero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "We propose an efficient batching strategy for variable-length decoding on GPU\narchitectures. During decoding, when candidates terminate or are pruned\naccording to heuristics, our streaming approach periodically \"refills\" the\nbatch before proceeding with a selected subset of candidates. We apply our\nmethod to variable-width beam search on a state-of-the-art machine translation\nmodel. Our method decreases runtime by up to 71% compared to a fixed-width beam\nsearch baseline and 17% compared to a variable-width baseline, while matching\nbaselines' BLEU. Finally, experiments show that our method can speed up\ndecoding in other domains, such as semantic and syntactic parsing.",
          "link": "http://arxiv.org/abs/2010.02164",
          "publishedOn": "2021-08-17T01:54:49.526Z",
          "wordCount": 589,
          "title": "A Streaming Approach For Efficient Batched Beam Search. (arXiv:2010.02164v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>",
          "description": "We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter\npruning), which slims down a CNN by reducing the width (number of output\nchannels) of convolutional layers. Inspired by the neurobiology research about\nthe independence of remembering and forgetting, we propose to re-parameterize a\nCNN into the remembering parts and forgetting parts, where the former learn to\nmaintain the performance and the latter learn to prune. Via training with\nregular SGD on the former but a novel update rule with penalty gradients on the\nlatter, we realize structured sparsity. Then we equivalently merge the\nremembering and forgetting parts into the original architecture with narrower\nlayers. In this sense, ResRep can be viewed as a successful application of\nStructural Re-parameterization. Such a methodology distinguishes ResRep from\nthe traditional learning-based pruning paradigm that applies a penalty on\nparameters to produce sparsity, which may suppress the parameters essential for\nthe remembering. ResRep slims down a standard ResNet-50 with 76.15% accuracy on\nImageNet to a narrower one with only 45% FLOPs and no accuracy drop, which is\nthe first to achieve lossless pruning with such a high compression ratio. The\ncode and models are at https://github.com/DingXiaoH/ResRep.",
          "link": "http://arxiv.org/abs/2007.03260",
          "publishedOn": "2021-08-17T01:54:49.520Z",
          "wordCount": 704,
          "title": "ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting. (arXiv:2007.03260v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tantawy_D/0/1/0/all/0/1\">Dina Tantawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahran_M/0/1/0/all/0/1\">Mohamed Zahran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wassal_A/0/1/0/all/0/1\">Amr Wassal</a>",
          "description": "Since its invention, Generative adversarial networks (GANs) have shown\noutstanding results in many applications. Generative Adversarial Networks are\npowerful yet, resource-hungry deep-learning models. Their main difference from\nordinary deep learning models is the nature of their output. For example, GAN\noutput can be a whole image versus other models detecting objects or\nclassifying images. Thus, the architecture and numeric precision of the network\naffect the quality and speed of the solution. Hence, accelerating GANs is\npivotal. Accelerating GANs can be classified into three main tracks: (1) Memory\ncompression, (2) Computation optimization, and (3) Data-flow optimization.\nBecause data transfer is the main source of energy usage, memory compression\nleads to the most savings. Thus, in this paper, we survey memory compression\ntechniques for CNN-Based GANs. Additionally, the paper summarizes opportunities\nand challenges in GANs acceleration and suggests open research problems to be\nfurther investigated.",
          "link": "http://arxiv.org/abs/2108.06626",
          "publishedOn": "2021-08-17T01:54:49.514Z",
          "wordCount": 590,
          "title": "A Survey on GAN Acceleration Using Memory Compression Technique. (arXiv:2108.06626v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1\">Anshul Nasery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_S/0/1/0/all/0/1\">Soumyadeep Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piratla_V/0/1/0/all/0/1\">Vihari Piratla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>",
          "description": "In several real world applications, machine learning models are deployed to\nmake predictions on data whose distribution changes gradually along time,\nleading to a drift between the train and test distributions. Such models are\noften re-trained on new data periodically, and they hence need to generalize to\ndata not too far into the future. In this context, there is much prior work on\nenhancing temporal generalization, e.g. continuous transportation of past data,\nkernel smoothed time-sensitive parameters and more recently, adversarial\nlearning of time-invariant features. However, these methods share several\nlimitations, e.g, poor scalability, training instability, and dependence on\nunlabeled data from the future. Responding to the above limitations, we propose\na simple method that starts with a model with time-sensitive parameters but\nregularizes its temporal complexity using a Gradient Interpolation (GI) loss.\nGI allows the decision boundary to change along time and can still prevent\noverfitting to the limited training time snapshots by allowing task-specific\ncontrol over changes along time. We compare our method to existing baselines on\nmultiple real-world datasets, which show that GI outperforms more complicated\ngenerative and adversarial approaches on the one hand, and simpler gradient\nregularization methods on the other.",
          "link": "http://arxiv.org/abs/2108.06721",
          "publishedOn": "2021-08-17T01:54:49.507Z",
          "wordCount": 641,
          "title": "Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time. (arXiv:2108.06721v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1810.08102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pierrot_T/0/1/0/all/0/1\">Thomas Pierrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrin_N/0/1/0/all/0/1\">Nicolas Perrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1\">Olivier Sigaud</a>",
          "description": "In this paper, we provide an overview of first-order and second-order\nvariants of the gradient descent method that are commonly used in machine\nlearning. We propose a general framework in which 6 of these variants can be\ninterpreted as different instances of the same approach. They are the vanilla\ngradient descent, the classical and generalized Gauss-Newton methods, the\nnatural gradient descent method, the gradient covariance matrix approach, and\nNewton's method. Besides interpreting these methods within a single framework,\nwe explain their specificities and show under which conditions some of them\ncoincide.",
          "link": "http://arxiv.org/abs/1810.08102",
          "publishedOn": "2021-08-17T01:54:49.482Z",
          "wordCount": 581,
          "title": "First-order and second-order variants of the gradient descent in a unified framework. (arXiv:1810.08102v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1\">Armin W. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poldrack_R/0/1/0/all/0/1\">Russell A. Poldrack</a>",
          "description": "In cognitive decoding, researchers aim to characterize a brain region's\nrepresentations by identifying the cognitive states (e.g., accepting/rejecting\na gamble) that can be identified from the region's activity. Deep learning (DL)\nmethods are highly promising for cognitive decoding, with their unmatched\nability to learn versatile representations of complex data. Yet, their\nwidespread application in cognitive decoding is hindered by their general lack\nof interpretability as well as difficulties in applying them to small datasets\nand in ensuring their reproducibility and robustness. We propose to approach\nthese challenges by leveraging recent advances in explainable artificial\nintelligence and transfer learning, while also providing specific\nrecommendations on how to improve the reproducibility and robustness of DL\nmodeling results.",
          "link": "http://arxiv.org/abs/2108.06896",
          "publishedOn": "2021-08-17T01:54:49.462Z",
          "wordCount": 547,
          "title": "Challenges for cognitive decoding using deep learning methods. (arXiv:2108.06896v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "Facial expression recognition (FER) has emerged as an important component of\nhuman-computer interaction systems. Despite recent advancements in FER,\nperformance often drops significantly for non-frontal facial images. We propose\nContrastive Learning of Multi-view facial Expressions (CL-MEx) to exploit\nfacial images captured simultaneously from different angles towards FER. CL-MEx\nis a two-step training framework. In the first step, an encoder network is\npre-trained with the proposed self-supervised contrastive loss, where it learns\nto generate view-invariant embeddings for different views of a subject. The\nmodel is then fine-tuned with labeled data in a supervised setting. We\ndemonstrate the performance of the proposed method on two multi-view FER\ndatasets, KDEF and DDCF, where state-of-the-art performances are achieved.\nFurther experiments show the robustness of our method in dealing with\nchallenging angles and reduced amounts of labeled data.",
          "link": "http://arxiv.org/abs/2108.06723",
          "publishedOn": "2021-08-17T01:54:49.456Z",
          "wordCount": 580,
          "title": "Self-supervised Contrastive Learning of Multi-view Facial Expressions. (arXiv:2108.06723v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piratla_V/0/1/0/all/0/1\">Vihari Piratla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_S/0/1/0/all/0/1\">Soumen Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>",
          "description": "Our goal is to evaluate the accuracy of a black-box classification model, not\nas a single aggregate on a given test data distribution, but as a surface over\na large number of combinations of attributes characterizing multiple test data\ndistributions. Such attributed accuracy measures become important as machine\nlearning models get deployed as a service, where the training data distribution\nis hidden from clients, and different clients may be interested in diverse\nregions of the data distribution. We present Attributed Accuracy Assay (AAA)--a\nGaussian Process (GP)--based probabilistic estimator for such an accuracy\nsurface. Each attribute combination, called an 'arm', is associated with a Beta\ndensity from which the service's accuracy is sampled. We expect the GP to\nsmooth the parameters of the Beta density over related arms to mitigate\nsparsity. We show that obvious application of GPs cannot address the challenge\nof heteroscedastic uncertainty over a huge attribute space that is sparsely and\nunevenly populated. In response, we present two enhancements: pooling sparse\nobservations, and regularizing the scale parameter of the Beta densities. After\nintroducing these innovations, we establish the effectiveness of AAA in terms\nof both its estimation accuracy and exploration efficiency, through extensive\nexperiments and analysis.",
          "link": "http://arxiv.org/abs/2108.06514",
          "publishedOn": "2021-08-17T01:54:49.451Z",
          "wordCount": 633,
          "title": "Active Assessment of Prediction Services as Accuracy Surface Over Attribute Combinations. (arXiv:2108.06514v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Ramneet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Anirban Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangdon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolsky_O/0/1/0/all/0/1\">Oleg Sokolsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>",
          "description": "Deep neural networks (DNNs) are known to produce incorrect predictions with\nvery high confidence on out-of-distribution inputs (OODs). This limitation is\none of the key challenges in the adoption of DNNs in high-assurance systems\nsuch as autonomous driving, air traffic management, and medical diagnosis. This\nchallenge has received significant attention recently, and several techniques\nhave been developed to detect inputs where the model's prediction cannot be\ntrusted. These techniques detect OODs as datapoints with either high epistemic\nuncertainty or high aleatoric uncertainty. We demonstrate the difference in the\ndetection ability of these techniques and propose an ensemble approach for\ndetection of OODs as datapoints with high uncertainty (epistemic or aleatoric).\nWe perform experiments on vision datasets with multiple DNN architectures,\nachieving state-of-the-art results in most cases.",
          "link": "http://arxiv.org/abs/2108.06380",
          "publishedOn": "2021-08-17T01:54:49.446Z",
          "wordCount": 572,
          "title": "Detecting OODs as datapoints with High Uncertainty. (arXiv:2108.06380v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08434",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kotsalis_G/0/1/0/all/0/1\">Georgios Kotsalis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1\">Guanghui Lan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>",
          "description": "The focus of this paper is on stochastic variational inequalities (VI) under\nMarkovian noise. A prominent application of our algorithmic developments is the\nstochastic policy evaluation problem in reinforcement learning. Prior\ninvestigations in the literature focused on temporal difference (TD) learning\nby employing nonsmooth finite time analysis motivated by stochastic subgradient\ndescent leading to certain limitations. These encompass the requirement of\nanalyzing a modified TD algorithm that involves projection to an a-priori\ndefined Euclidean ball, achieving a non-optimal convergence rate and no clear\nway of deriving the beneficial effects of parallel implementation. Our approach\nremedies these shortcomings in the broader context of stochastic VIs and in\nparticular when it comes to stochastic policy evaluation. We developed a\nvariety of simple TD learning type algorithms motivated by its original version\nthat maintain its simplicity, while offering distinct advantages from a\nnon-asymptotic analysis point of view. We first provide an improved analysis of\nthe standard TD algorithm that can benefit from parallel implementation. Then\nwe present versions of a conditional TD algorithm (CTD), that involves periodic\nupdates of the stochastic iterates, which reduce the bias and therefore exhibit\nimproved iteration complexity. This brings us to the fast TD (FTD) algorithm\nwhich combines elements of CTD and the stochastic operator extrapolation method\nof the companion paper. For a novel index resetting policy FTD exhibits the\nbest known convergence rate. We also devised a robust version of the algorithm\nthat is particularly suitable for discounting factors close to 1.",
          "link": "http://arxiv.org/abs/2011.08434",
          "publishedOn": "2021-08-17T01:54:49.418Z",
          "wordCount": 750,
          "title": "Simple and optimal methods for stochastic variational inequalities, II: Markovian noise and policy evaluation in reinforcement learning. (arXiv:2011.08434v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheibani_M/0/1/0/all/0/1\">Mohamadreza Sheibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_G/0/1/0/all/0/1\">Ge Ou</a>",
          "description": "The abundance of training data is not guaranteed in various supervised\nlearning applications. One of these situations is the post-earthquake regional\ndamage assessment of buildings. Querying the damage label of each building\nrequires a thorough inspection by experts, and thus, is an expensive task. A\npractical approach is to sample the most informative buildings in a sequential\nlearning scheme. Active learning methods recommend the most informative cases\nthat are able to maximally reduce the generalization error. The information\ntheoretic measure of mutual information (MI) is one of the most effective\ncriteria to evaluate the effectiveness of the samples in a pool-based sample\nselection scenario. However, the computational complexity of the standard MI\nalgorithm prevents the utilization of this method on large datasets. A local\nkernels strategy was proposed to reduce the computational costs, but the\nadaptability of the kernels to the observed labels was not considered in the\noriginal formulation of this strategy. In this article, an adaptive local\nkernels methodology is developed that allows for the conformability of the\nkernels to the observed output data while enhancing the computational\ncomplexity of the standard MI algorithm. The proposed algorithm is developed to\nwork on a Gaussian process regression (GPR) framework, where the kernel\nhyperparameters are updated after each label query using the maximum likelihood\nestimation. In the sequential learning procedure, the updated hyperparameters\ncan be used in the MI kernel matrices to improve the sample suggestion\nperformance. The advantages are demonstrated on a simulation of the 2018\nAnchorage, AK, earthquake. It is shown that while the proposed algorithm\nenables GPR to reach acceptable performance with fewer training data, the\ncomputational demands remain lower than the standard local kernels strategy.",
          "link": "http://arxiv.org/abs/2105.11492",
          "publishedOn": "2021-08-17T01:54:49.388Z",
          "wordCount": 776,
          "title": "Adaptive Local Kernels Formulation of Mutual Information with Application to Active Post-Seismic Building Damage Inference. (arXiv:2105.11492v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiqiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>",
          "description": "Traditional normalization techniques (e.g., Batch Normalization and Instance\nNormalization) generally and simplistically assume that training and test data\nfollow the same distribution. As distribution shifts are inevitable in\nreal-world applications, well-trained models with previous normalization\nmethods can perform badly in new environments. Can we develop new normalization\nmethods to improve generalization robustness under distribution shifts? In this\npaper, we answer the question by proposing CrossNorm and SelfNorm. CrossNorm\nexchanges channel-wise mean and variance between feature maps to enlarge\ntraining distribution, while SelfNorm uses attention to recalibrate the\nstatistics to bridge gaps between training and test distributions. CrossNorm\nand SelfNorm can complement each other, though exploring different directions\nin statistics usage. Extensive experiments on different fields (vision and\nlanguage), tasks (classification and segmentation), settings (supervised and\nsemi-supervised), and distribution shift types (synthetic and natural) show the\neffectiveness. Code is available at\nhttps://github.com/amazon-research/crossnorm-selfnorm",
          "link": "http://arxiv.org/abs/2102.02811",
          "publishedOn": "2021-08-17T01:54:49.365Z",
          "wordCount": 621,
          "title": "CrossNorm and SelfNorm for Generalization under Distribution Shifts. (arXiv:2102.02811v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Ashwin Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskar_A/0/1/0/all/0/1\">A Baskar</a>",
          "description": "We present a general technique for constructing Graph Neural Networks (GNNs)\ncapable of using multi-relational domain knowledge. The technique is based on\nmode-directed inverse entailment (MDIE) developed in Inductive Logic\nProgramming (ILP). Given a data instance $e$ and background knowledge $B$, MDIE\nidentifies a most-specific logical formula $\\bot_B(e)$ that contains all the\nrelational information in $B$ that is related to $e$. We represent $\\bot_B(e)$\nby a \"bottom-graph\" that can be converted into a form suitable for GNN\nimplementations. This transformation allows a principled way of incorporating\ngeneric background knowledge into GNNs: we use the term `BotGNN' for this form\nof graph neural networks. For several GNN variants, using real-world datasets\nwith substantial background knowledge, we show that BotGNNs perform\nsignificantly better than both GNNs without background knowledge and a recently\nproposed simplified technique for including domain knowledge into GNNs. We also\nprovide experimental evidence comparing BotGNNs favourably to multi-layer\nperceptrons (MLPs) that use features representing a \"propositionalised\" form of\nthe background knowledge; and BotGNNs to a standard ILP based on the use of\nmost-specific clauses. Taken together, these results point to BotGNNs as\ncapable of combining the computational efficacy of GNNs with the\nrepresentational versatility of ILP.",
          "link": "http://arxiv.org/abs/2105.10709",
          "publishedOn": "2021-08-17T01:54:49.359Z",
          "wordCount": 683,
          "title": "Inclusion of Domain-Knowledge into GNNs using Mode-Directed Inverse Entailment. (arXiv:2105.10709v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neu_G/0/1/0/all/0/1\">Gergely Neu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1\">Gintare Karolina Dziugaite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghifam_M/0/1/0/all/0/1\">Mahdi Haghifam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Daniel M. Roy</a>",
          "description": "We study the generalization properties of the popular stochastic optimization\nmethod known as stochastic gradient descent (SGD) for optimizing general\nnon-convex loss functions. Our main contribution is providing upper bounds on\nthe generalization error that depend on local statistics of the stochastic\ngradients evaluated along the path of iterates calculated by SGD. The key\nfactors our bounds depend on are the variance of the gradients (with respect to\nthe data distribution) and the local smoothness of the objective function along\nthe SGD path, and the sensitivity of the loss function to perturbations to the\nfinal output. Our key technical tool is combining the information-theoretic\ngeneralization bounds previously used for analyzing randomized variants of SGD\nwith a perturbation analysis of the iterates.",
          "link": "http://arxiv.org/abs/2102.00931",
          "publishedOn": "2021-08-17T01:54:49.285Z",
          "wordCount": 594,
          "title": "Information-Theoretic Generalization Bounds for Stochastic Gradient Descent. (arXiv:2102.00931v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhizhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1\">Pengfei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>",
          "description": "Community detection, a fundamental task for network analysis, aims to\npartition a network into multiple sub-structures to help reveal their latent\nfunctions. Community detection has been extensively studied in and broadly\napplied to many real-world network problems. Classical approaches to community\ndetection typically utilize probabilistic graphical models and adopt a variety\nof prior knowledge to infer community structures. As the problems that network\nmethods try to solve and the network data to be analyzed become increasingly\nmore sophisticated, new approaches have also been proposed and developed,\nparticularly those that utilize deep learning and convert networked data into\nlow dimensional representation. Despite all the recent advancement, there is\nstill a lack of insightful understanding of the theoretical and methodological\nunderpinning of community detection, which will be critically important for\nfuture development of the area of network analysis. In this paper, we develop\nand present a unified architecture of network community-finding methods to\ncharacterize the state-of-the-art of the field of community detection.\nSpecifically, we provide a comprehensive review of the existing community\ndetection methods and introduce a new taxonomy that divides the existing\nmethods into two categories, namely probabilistic graphical model and deep\nlearning. We then discuss in detail the main idea behind each method in the two\ncategories. Furthermore, to promote future development of community detection,\nwe release several benchmark datasets from several problem domains and\nhighlight their applications to various network analysis tasks. We conclude\nwith discussions of the challenges of the field and suggestions of possible\ndirections for future research.",
          "link": "http://arxiv.org/abs/2101.01669",
          "publishedOn": "2021-08-17T01:54:49.251Z",
          "wordCount": 757,
          "title": "A Survey of Community Detection Approaches: From Statistical Modeling to Deep Learning. (arXiv:2101.01669v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.11897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>",
          "description": "State-of-the-art methods in image-to-image translation are capable of\nlearning a mapping from a source domain to a target domain with unpaired image\ndata. Though the existing methods have achieved promising results, they still\nproduce visual artifacts, being able to translate low-level information but not\nhigh-level semantics of input images. One possible reason is that generators do\nnot have the ability to perceive the most discriminative parts between the\nsource and target domains, thus making the generated images low quality. In\nthis paper, we propose a new Attention-Guided Generative Adversarial Networks\n(AttentionGAN) for the unpaired image-to-image translation task. AttentionGAN\ncan identify the most discriminative foreground objects and minimize the change\nof the background. The attention-guided generators in AttentionGAN are able to\nproduce attention masks, and then fuse the generation output with the attention\nmasks to obtain high-quality target images. Accordingly, we also design a novel\nattention-guided discriminator which only considers attended regions. Extensive\nexperiments are conducted on several generative tasks with eight public\ndatasets, demonstrating that the proposed method is effective to generate\nsharper and more realistic images compared with existing competitive models.\nThe code is available at https://github.com/Ha0Tang/AttentionGAN.",
          "link": "http://arxiv.org/abs/1911.11897",
          "publishedOn": "2021-08-17T01:54:49.238Z",
          "wordCount": 722,
          "title": "AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks. (arXiv:1911.11897v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guo-Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yifan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Knowledge distillation (KD) is a popular method to train efficient networks\n(\"student\") with the help of high-capacity networks (\"teacher\"). Traditional\nmethods use the teacher's soft logits as extra supervision to train the student\nnetwork. In this paper, we argue that it is more advantageous to make the\nstudent mimic the teacher's features in the penultimate layer. Not only the\nstudent can directly learn more effective information from the teacher feature,\nfeature mimicking can also be applied for teachers trained without a softmax\nlayer. Experiments show that it can achieve higher accuracy than traditional\nKD. To further facilitate feature mimicking, we decompose a feature vector into\nthe magnitude and the direction. We argue that the teacher should give more\nfreedom to the student feature's magnitude, and let the student pay more\nattention on mimicking the feature direction. To meet this requirement, we\npropose a loss term based on locality-sensitive hashing (LSH). With the help of\nthis new loss, our method indeed mimics feature directions more accurately,\nrelaxes constraints on feature magnitudes, and achieves state-of-the-art\ndistillation accuracy. We provide theoretical analyses of how LSH facilitates\nfeature direction mimicking, and further extend feature mimicking to\nmulti-label recognition and object detection.",
          "link": "http://arxiv.org/abs/2011.01424",
          "publishedOn": "2021-08-17T01:54:49.222Z",
          "wordCount": 664,
          "title": "Distilling Knowledge by Mimicking Features. (arXiv:2011.01424v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanchirico_M/0/1/0/all/0/1\">Mauro J. Sanchirico III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_X/0/1/0/all/0/1\">Xun Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nataraj_C/0/1/0/all/0/1\">C. Nataraj</a>",
          "description": "Polynomial expansions are important in the analysis of neural network\nnonlinearities. They have been applied thereto addressing well-known\ndifficulties in verification, explainability, and security. Existing approaches\nspan classical Taylor and Chebyshev methods, asymptotics, and many numerical\napproaches. We find that while these individually have useful properties such\nas exact error formulas, adjustable domain, and robustness to undefined\nderivatives, there are no approaches that provide a consistent method yielding\nan expansion with all these properties. To address this, we develop an\nanalytically modified integral transform expansion (AMITE), a novel expansion\nvia integral transforms modified using derived criteria for convergence. We\nshow the general expansion and then demonstrate application for two popular\nactivation functions, hyperbolic tangent and rectified linear units. Compared\nwith existing expansions (i.e., Chebyshev, Taylor, and numerical) employed to\nthis end, AMITE is the first to provide six previously mutually exclusive\ndesired expansion properties such as exact formulas for the coefficients and\nexact expansion errors (Table II). We demonstrate the effectiveness of AMITE in\ntwo case studies. First, a multivariate polynomial form is efficiently\nextracted from a single hidden layer black-box MLP to facilitate equivalence\ntesting from noisy stimulus-response pairs. Second, a variety of FFNN\narchitectures having between 3 and 7 layers are range bounded using Taylor\nmodels improved by the AMITE polynomials and error formulas. AMITE presents a\nnew dimension of expansion methods suitable for analysis/approximation of\nnonlinearities in neural networks, opening new directions and opportunities for\nthe theoretical analysis and systematic testing of neural networks.",
          "link": "http://arxiv.org/abs/2007.06226",
          "publishedOn": "2021-08-17T01:54:49.159Z",
          "wordCount": 752,
          "title": "AMITE: A Novel Polynomial Expansion for Analyzing Neural Network Nonlinearities. (arXiv:2007.06226v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.06814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>",
          "description": "As billions of personal data being shared through social media and network,\nthe data privacy and security have drawn an increasing attention. Several\nattempts have been made to alleviate the leakage of identity information from\nface photos, with the aid of, e.g., image obfuscation techniques. However, most\nof the present results are either perceptually unsatisfactory or ineffective\nagainst face recognition systems. Our goal in this paper is to develop a\ntechnique that can encrypt the personal photos such that they can protect users\nfrom unauthorized face recognition systems but remain visually identical to the\noriginal version for human beings. To achieve this, we propose a targeted\nidentity-protection iterative method (TIP-IM) to generate adversarial identity\nmasks which can be overlaid on facial images, such that the original identities\ncan be concealed without sacrificing the visual quality. Extensive experiments\ndemonstrate that TIP-IM provides 95\\%+ protection success rate against various\nstate-of-the-art face recognition models under practical test scenarios.\nBesides, we also show the practical and effective applicability of our method\non a commercial API service.",
          "link": "http://arxiv.org/abs/2003.06814",
          "publishedOn": "2021-08-17T01:54:49.153Z",
          "wordCount": 659,
          "title": "Towards Face Encryption by Generating Adversarial Identity Masks. (arXiv:2003.06814v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jia Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1\">Andre Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jingxuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuli_M/0/1/0/all/0/1\">Mathieu Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "Neural Architecture Search (NAS) has shifted network design from using human\nintuition to leveraging search algorithms guided by evaluation metrics. We\nstudy channel size optimization in convolutional neural networks (CNN) and\nidentify the role it plays in model accuracy and complexity. Current channel\nsize selection methods are generally limited by discrete sample spaces while\nsuffering from manual iteration and simple heuristics. To solve this, we\nintroduce an efficient dynamic scaling algorithm -- CONet -- that automatically\noptimizes channel sizes across network layers for a given CNN. Two metrics --\n``\\textit{Rank}\" and \"\\textit{Rank Average Slope}\" -- are introduced to\nidentify the information accumulated in training. The algorithm dynamically\nscales channel sizes up or down over a fixed searching phase. We conduct\nexperiments on CIFAR10/100 and ImageNet datasets and show that CONet can find\nefficient and accurate architectures searched in ResNet, DARTS, and DARTS+\nspaces that outperform their baseline models.",
          "link": "http://arxiv.org/abs/2108.06822",
          "publishedOn": "2021-08-17T01:54:49.024Z",
          "wordCount": 609,
          "title": "CONet: Channel Optimization for Convolutional Neural Networks. (arXiv:2108.06822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>",
          "description": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.",
          "link": "http://arxiv.org/abs/2108.03857",
          "publishedOn": "2021-08-17T01:54:48.706Z",
          "wordCount": 613,
          "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04085",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bonneville_C/0/1/0/all/0/1\">Christophe Bonneville</a>, <a href=\"http://arxiv.org/find/math/1/au:+Earls_C/0/1/0/all/0/1\">Christopher J. Earls</a>",
          "description": "Scientific machine learning has been successfully applied to inverse problems\nand PDE discoveries in computational physics. One caveat of current methods\nhowever is the need for large amounts of (clean) data in order to recover full\nsystem responses or underlying physical models. Bayesian methods may be\nparticularly promising to overcome these challenges as they are naturally less\nsensitive to sparse and noisy data. In this paper, we propose to use Bayesian\nneural networks (BNN) in order to: 1) Recover the full system states from\nmeasurement data (e.g. temperature, velocity field, etc.). We use Hamiltonian\nMonte-Carlo to sample the posterior distribution of a deep and dense BNN, and\nshow that it is possible to accurately capture physics of varying complexity\nwithout overfitting. 2) Recover the parameters in the underlying partial\ndifferential equation (PDE) governing the physical system. Using the trained\nBNN as a surrogate of the system response, we generate datasets of derivatives\npotentially comprising the latent PDE of the observed system and perform a\nBayesian linear regression (BLR) between the successive derivatives in space\nand time to recover the original PDE parameters. We take advantage of the\nconfidence intervals on the BNN outputs and introduce the spatial derivative\nvariance into the BLR likelihood to discard the influence of highly uncertain\nsurrogate data points, which allows for more accurate parameter discovery. We\ndemonstrate our approach on a handful of example applied to physics and\nnon-linear dynamics.",
          "link": "http://arxiv.org/abs/2108.04085",
          "publishedOn": "2021-08-17T01:54:48.609Z",
          "wordCount": 696,
          "title": "Bayesian Deep Learning for Partial Differential Equation Parameter Discovery with Sparse and Noisy Data. (arXiv:2108.04085v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rundi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changxi Zheng</a>",
          "description": "Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation --- describing a shape\nas a sequence of computer-aided design (CAD) operations. Unlike meshes and\npoint clouds, CAD models encode the user creation process of 3D shapes, widely\nused in numerous industrial and engineering design tasks. However, the\nsequential and irregular structure of CAD operations poses significant\nchallenges for existing 3D generative models. Drawing an analogy between CAD\noperations and natural language, we propose a CAD generative network based on\nthe Transformer. We demonstrate the performance of our model for both shape\nautoencoding and random shape generation. To train our network, we create a new\nCAD dataset consisting of 178,238 models and their CAD construction sequences.\nWe have made this dataset publicly available to promote future research on this\ntopic.",
          "link": "http://arxiv.org/abs/2105.09492",
          "publishedOn": "2021-08-17T01:54:48.520Z",
          "wordCount": 649,
          "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models. (arXiv:2105.09492v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wycoff_N/0/1/0/all/0/1\">Nathan Wycoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getty_N/0/1/0/all/0/1\">Neil Getty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1\">Rick Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fangfang Xia</a>",
          "description": "The field of neuromorphic computing is in a period of active exploration.\nWhile many tools have been developed to simulate neuronal dynamics or convert\ndeep networks to spiking models, general software libraries for learning rules\nremain underexplored. This is partly due to the diverse, challenging nature of\nefforts to design new learning rules, which range from encoding methods to\ngradient approximations, from population approaches that mimic the Bayesian\nbrain to constrained learning algorithms deployed on memristor crossbars. To\naddress this gap, we present Neko, a modular, extensible library with a focus\non aiding the design of new learning algorithms. We demonstrate the utility of\nNeko in three exemplar cases: online local learning, probabilistic learning,\nand analog on-device learning. Our results show that Neko can replicate the\nstate-of-the-art algorithms and, in one case, lead to significant\noutperformance in accuracy and speed. Further, it offers tools including\ngradient comparison that can help develop new algorithmic variants. Neko is an\nopen source Python library that supports PyTorch and TensorFlow backends.",
          "link": "http://arxiv.org/abs/2105.00324",
          "publishedOn": "2021-08-17T01:54:48.510Z",
          "wordCount": 646,
          "title": "Neko: a Library for Exploring Neuromorphic Learning Rules. (arXiv:2105.00324v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melodia_L/0/1/0/all/0/1\">Luciano Melodia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_R/0/1/0/all/0/1\">Richard Lenz</a>",
          "description": "In this paper, we use topological data analysis techniques to construct a\nsuitable neural network classifier for the task of learning sensor signals of\nentire power plants according to their reference designation system. We use\nrepresentations of persistence diagrams to derive necessary preprocessing steps\nand visualize the large amounts of data. We derive deep architectures with\none-dimensional convolutional layers combined with stacked long short-term\nmemories as residual networks suitable for processing the persistence features.\nWe combine three separate sub-networks, obtaining as input the time series\nitself and a representation of the persistent homology for the zeroth and first\ndimension. We give a mathematical derivation for most of the used\nhyper-parameters. For validation, numerical experiments were performed with\nsensor data from four power plants of the same construction type.",
          "link": "http://arxiv.org/abs/2106.02493",
          "publishedOn": "2021-08-17T01:54:48.493Z",
          "wordCount": 601,
          "title": "Homological Time Series Analysis of Sensor Signals from Power Plants. (arXiv:2106.02493v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huafeng Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chonggang Lu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhimin Hu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaodong Yuan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingshu Zhang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanquan Liu</a> (3) ((1) School of Information, North China University of Technology,(2) Department of Neurology, Kailuan General Hospital, Tangshan,(3) School of Intelligent Systems Engineering, Sun Yat-sen University)",
          "description": "Sleep staging plays an important role on the diagnosis of sleep disorders. In\ngeneral, experts classify sleep stages manually based on polysomnography (PSG),\nwhich is quite time-consuming. Meanwhile, the acquisition process of multiple\nsignals is much complex, which can affect the subject's sleep. Therefore, the\nuse of single-channel electroencephalogram (EEG) for automatic sleep staging\nhas become a popular research topic. In the literature, a large number of sleep\nstaging methods based on single-channel EEG have been proposed with promising\nresults and achieve the preliminary automation of sleep staging. However, the\nperformance for most of these methods in the N1 stage do not satisfy the needs\nof the diagnosis. In this paper, we propose a deep learning model multi scale\ndual attention network(MSDAN) based on raw EEG, which utilizes multi-scale\nconvolution to extract features in different waveforms contained in the EEG\nsignal, connects channel attention and spatial attention mechanisms in series\nto filter and highlight key information, and uses soft thresholding to remove\nredundant information. Experiments were conducted using two datasets with\n5-fold cross-validation and hold-out validation method. The final average\naccuracy, overall accuracy, macro F1 score and Cohen's Kappa coefficient of the\nmodel reach 96.70%, 91.74%, 0.8231 and 0.8723 on the Sleep-EDF dataset, 96.14%,\n90.35%, 0.7945 and 0.8284 on the Sleep-EDFx dataset. Significantly, our model\nperformed superiorly in the N1 stage, with F1 scores of 54.41% and 52.79% on\nthe two datasets respectively. The results show the superiority of our network\nover the existing methods, reaching a new state-of-the-art. In particular, the\nproposed method achieves excellent results in the N1 sleep stage compared to\nother methods.",
          "link": "http://arxiv.org/abs/2107.08442",
          "publishedOn": "2021-08-17T01:54:48.475Z",
          "wordCount": 768,
          "title": "Sleep Staging Based on Multi Scale Dual Attention Network. (arXiv:2107.08442v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13020",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pena_J/0/1/0/all/0/1\">Jose M. Pe&#xf1;a</a>",
          "description": "We present a method for assessing the sensitivity of the true causal effect\nto unmeasured confounding. The method requires the analyst to set two intuitive\nparameters. Otherwise, the method is assumption-free. The method returns an\ninterval that contains the true causal effect, and whose bounds are sharp, i.e.\nattainable. We show experimentally that our bounds can be sharper than those\nobtained by the method of Ding and VanderWeele (2016a) which, moreover,\nrequires to set one more parameter than our method. Finally, we extend our\nmethod to bound the natural direct and indirect effects when there are measured\nmediators and unmeasured exposure-outcome confounding.",
          "link": "http://arxiv.org/abs/2104.13020",
          "publishedOn": "2021-08-17T01:54:48.469Z",
          "wordCount": 560,
          "title": "Simple yet Sharp Sensitivity Analysis for Unmeasured Confounding. (arXiv:2104.13020v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao-Hua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>",
          "description": "While deep reinforcement learning has achieved promising results in\nchallenging decision-making tasks, the main bones of its success --- deep\nneural networks are mostly black-boxes. A feasible way to gain insight into a\nblack-box model is to distill it into an interpretable model such as a decision\ntree, which consists of if-then rules and is easy to grasp and be verified.\nHowever, the traditional model distillation is usually a supervised learning\ntask under a stationary data distribution assumption, which is violated in\nreinforcement learning. Therefore, a typical policy distillation that clones\nmodel behaviors with even a small error could bring a data distribution shift,\nresulting in an unsatisfied distilled policy model with low fidelity or low\nperformance. In this paper, we propose to address this issue by changing the\ndistillation objective from behavior cloning to maximizing an advantage\nevaluation. The novel distillation objective maximizes an approximated\ncumulative reward and focuses more on disastrous behaviors in critical states,\nwhich controls the data shift effect. We evaluate our method on several Gym\ntasks, a commercial fight game, and a self-driving car simulator. The empirical\nresults show that the proposed method can preserve a higher cumulative reward\nthan behavior cloning and learn a more consistent policy to the original one.\nMoreover, by examining the extracted rules from the distilled decision trees,\nwe demonstrate that the proposed method delivers reasonable and robust\ndecisions.",
          "link": "http://arxiv.org/abs/2108.06898",
          "publishedOn": "2021-08-17T01:54:48.452Z",
          "wordCount": 660,
          "title": "Neural-to-Tree Policy Distillation with Policy Improvement Criterion. (arXiv:2108.06898v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08250",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Anderer_M/0/1/0/all/0/1\">Matthias Anderer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>",
          "description": "Hierarchical forecasting with intermittent time series is a challenge in both\nresearch and empirical studies. Vast research focuses on improving the accuracy\nof each hierarchy, especially the intermittent time series at bottom levels. It\nthen reconciles forecasts at each hierarchy to further improve the overall\nperformance. In this paper, we present a hierarchical forecasting approach that\ntreats the bottom level forecasts as mutable to ensure higher forecasting\naccuracy on the upper levels of the hierarchy. We employ a pure deep learning\nforecasting approach N-BEATS for continuous time series on top levels and a\nwidely used tree-based algorithm LightGBM for the bottom level intermittent\ntime series. The hierarchical forecasting with alignment approach is a simple\nyet effective variant of the bottom-up method, which accounts for biases that\nare difficult to observe at the bottom level. It allows suboptimal forecasts at\nthe lower level to retain a higher overall performance. The approach in this\nempirical study was developed by the first author during the M5 Forecasting\nAccuracy competition, ranking second place. The approach is also business\norientated and could be beneficial for business strategic planning.",
          "link": "http://arxiv.org/abs/2103.08250",
          "publishedOn": "2021-08-17T01:54:48.446Z",
          "wordCount": 637,
          "title": "Hierarchical forecasting with a top-down alignment of independent level forecasts. (arXiv:2103.08250v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07736",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jin_D/0/1/0/all/0/1\">Dian Jin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bing_X/0/1/0/all/0/1\">Xin Bing</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqian Zhang</a>",
          "description": "The problem of finding the unique low dimensional decomposition of a given\nmatrix has been a fundamental and recurrent problem in many areas. In this\npaper, we study the problem of seeking a unique decomposition of a low rank\nmatrix $Y\\in \\mathbb{R}^{p\\times n}$ that admits a sparse representation.\nSpecifically, we consider $Y = A X\\in \\mathbb{R}^{p\\times n}$ where the matrix\n$A\\in \\mathbb{R}^{p\\times r}$ has full column rank, with $r < \\min\\{n,p\\}$, and\nthe matrix $X\\in \\mathbb{R}^{r\\times n}$ is element-wise sparse. We prove that\nthis sparse decomposition of $Y$ can be uniquely identified, up to some\nintrinsic signed permutation. Our approach relies on solving a nonconvex\noptimization problem constrained over the unit sphere. Our geometric analysis\nfor the nonconvex optimization landscape shows that any {\\em strict} local\nsolution is close to the ground truth solution, and can be recovered by a\nsimple data-driven initialization followed with any second order descent\nalgorithm. At last, we corroborate these theoretical results with numerical\nexperiments.",
          "link": "http://arxiv.org/abs/2106.07736",
          "publishedOn": "2021-08-17T01:54:48.435Z",
          "wordCount": 620,
          "title": "Unique sparse decomposition of low rank matrices. (arXiv:2106.07736v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1\">Rajarshi Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldsmith_A/0/1/0/all/0/1\">Andrea J. Goldsmith</a>",
          "description": "The communication cost of distributed optimization algorithms is a major\nbottleneck in their scalability. This work considers a parameter-server setting\nin which the worker is constrained to communicate information to the server\nusing only $R$ bits per dimension. We show that $\\mathbf{democratic}$\n$\\mathbf{embeddings}$ from random matrix theory are significantly useful for\ndesigning efficient and optimal vector quantizers that respect this bit budget.\nThe resulting polynomial complexity source coding schemes are used to design\ndistributed optimization algorithms with convergence rates matching the minimax\noptimal lower bounds for (i) Smooth and Strongly-Convex objectives with access\nto an Exact Gradient oracle, as well as (ii) General Convex and Non-Smooth\nobjectives with access to a Noisy Subgradient oracle. We further propose a\nrelaxation of this coding scheme which is nearly minimax optimal. Numerical\nsimulations validate our theoretical claims.",
          "link": "http://arxiv.org/abs/2103.07578",
          "publishedOn": "2021-08-17T01:54:48.424Z",
          "wordCount": 618,
          "title": "Democratic Source Coding: An Optimal Fixed-Length Quantization Scheme for Distributed Optimization Under Communication Constraints. (arXiv:2103.07578v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armenta_M/0/1/0/all/0/1\">Marco Armenta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judge_T/0/1/0/all/0/1\">Thierry Judge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Painchaud_N/0/1/0/all/0/1\">Nathan Painchaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemaire_C/0/1/0/all/0/1\">Carl Lemaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_G/0/1/0/all/0/1\">Gabriel Gibeau Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spino_P/0/1/0/all/0/1\">Philippe Spino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>",
          "description": "In this paper, we explore a process called neural teleportation, a\nmathematical consequence of applying quiver representation theory to neural\nnetworks. Neural teleportation \"teleports\" a network to a new position in the\nweight space and preserves its function. This phenomenon comes directly from\nthe definitions of representation theory applied to neural networks and it\nturns out to be a very simple operation that has remarkable properties. We shed\nlight on surprising and counter-intuitive consequences neural teleportation has\non the loss landscape. In particular, we show that teleportation can be used to\nexplore loss level curves, that it changes the local loss landscape, sharpens\nglobal minima and boosts back-propagated gradients at any moment during the\nlearning process. Our results can be reproduced with the code available here:\nhttps://github.com/vitalab/neuralteleportation",
          "link": "http://arxiv.org/abs/2012.01118",
          "publishedOn": "2021-08-17T01:54:48.406Z",
          "wordCount": 602,
          "title": "Neural Teleportation. (arXiv:2012.01118v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12066",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yanming Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chunyan Wang</a>",
          "description": "The work presented in this paper is to propose a reliable high-quality system\nof Convolutional Neural Network (CNN) for brain tumor segmentation with a low\ncomputation requirement. The system consists of a CNN for the main processing\nfor the segmentation, a pre-CNN block for data reduction and post-CNN\nrefinement block. The unique CNN consists of 7 convolution layers involving\nonly 108 kernels and 20308 trainable parameters. It is custom-designed,\nfollowing the proposed paradigm of ASCNN (application specific CNN), to perform\nmono-modality and cross-modality feature extraction, tumor localization and\npixel classification. Each layer fits the task assigned to it, by means of (i)\nappropriate normalization applied to its input data, (ii) correct convolution\nmodes for the assigned task, and (iii) suitable nonlinear transformation to\noptimize the convolution results. In this specific design context, the number\nof kernels in each of the 7 layers is made to be just-sufficient for its task,\ninstead of exponentially growing over the layers, to increase information\ndensity and to reduce randomness in the processing. The proposed activation\nfunction Full-ReLU helps to halve the number of kernels in convolution layers\nof high-pass filtering without degrading processing quality. A large number of\nexperiments with BRATS2018 dataset have been conducted to measure the\nprocessing quality and reproducibility of the proposed system. The results\ndemonstrate that the system reproduces reliably almost the same output to the\nsame input after retraining. The mean dice scores for enhancing tumor, whole\ntumor and tumor core are 77.2%, 89.2% and 76.3%, respectively. The simple\nstructure and reliable high processing quality of the proposed system will\nfacilitate its implementation and medical applications.",
          "link": "http://arxiv.org/abs/2007.12066",
          "publishedOn": "2021-08-17T01:54:48.400Z",
          "wordCount": 739,
          "title": "A Computation-Efficient CNN System for High-Quality Brain Tumor Segmentation. (arXiv:2007.12066v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11434",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gammelli_D/0/1/0/all/0/1\">Daniele Gammelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_K/0/1/0/all/0/1\">Kaidi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harrison_J/0/1/0/all/0/1\">James Harrison</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_F/0/1/0/all/0/1\">Filipe Rodrigues</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_F/0/1/0/all/0/1\">Francisco C. Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>",
          "description": "Autonomous mobility-on-demand (AMoD) systems represent a rapidly developing\nmode of transportation wherein travel requests are dynamically handled by a\ncoordinated fleet of robotic, self-driving vehicles. Given a graph\nrepresentation of the transportation network - one where, for example, nodes\nrepresent areas of the city, and edges the connectivity between them - we argue\nthat the AMoD control problem is naturally cast as a node-wise decision-making\nproblem. In this paper, we propose a deep reinforcement learning framework to\ncontrol the rebalancing of AMoD systems through graph neural networks.\nCrucially, we demonstrate that graph neural networks enable reinforcement\nlearning agents to recover behavior policies that are significantly more\ntransferable, generalizable, and scalable than policies learned through other\napproaches. Empirically, we show how the learned policies exhibit promising\nzero-shot transfer capabilities when faced with critical portability tasks such\nas inter-city generalization, service area expansion, and adaptation to\npotentially complex urban topologies.",
          "link": "http://arxiv.org/abs/2104.11434",
          "publishedOn": "2021-08-17T01:54:48.394Z",
          "wordCount": 616,
          "title": "Graph Neural Network Reinforcement Learning for Autonomous Mobility-on-Demand Systems. (arXiv:2104.11434v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yantian Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1\">Siddhant Bhambri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1\">Lin Guan</a>",
          "description": "Conventional works that learn grasping affordance from demonstrations need to\nexplicitly predict grasping configurations, such as gripper approaching angles\nor grasping preshapes. Classic motion planners could then sample trajectories\nby using such predicted configurations. In this work, our goal is instead to\nfill the gap between affordance discovery and affordance-based policy learning\nby integrating the two objectives in an end-to-end imitation learning framework\nbased on deep neural networks. From a psychological perspective, there is a\nclose association between attention and affordance. Therefore, with an\nend-to-end neural network, we propose to learn affordance cues as visual\nattention that serves as a useful indicating signal of how a demonstrator\naccomplishes tasks, instead of explicitly modeling affordances. To achieve\nthis, we propose a contrastive learning framework that consists of a Siamese\nencoder and a trajectory decoder. We further introduce a coupled triplet loss\nto encourage the discovered affordance cues to be more affordance-relevant. Our\nexperimental results demonstrate that our model with the coupled triplet loss\nachieves the highest grasping success rate in a simulated robot environment.\nOur project website can be accessed at\nhttps://sites.google.com/asu.edu/affordance-aware-imitation/project.",
          "link": "http://arxiv.org/abs/2104.00878",
          "publishedOn": "2021-08-17T01:54:48.347Z",
          "wordCount": 660,
          "title": "Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping. (arXiv:2104.00878v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jurewicz_M/0/1/0/all/0/1\">Mateusz Jurewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromberg_Derczynski_L/0/1/0/all/0/1\">Leon Str&#xf8;mberg-Derczynski</a>",
          "description": "Machine learning on sets towards sequential output is an important and\nubiquitous task, with applications ranging from language modeling and\nmeta-learning to multi-agent strategy games and power grid optimization.\nCombining elements of representation learning and structured prediction, its\ntwo primary challenges include obtaining a meaningful, permutation invariant\nset representation and subsequently utilizing this representation to output a\ncomplex target permutation. This paper provides a comprehensive introduction to\nthe field as well as an overview of important machine learning methods tackling\nboth of these key challenges, with a detailed qualitative comparison of\nselected model architectures.",
          "link": "http://arxiv.org/abs/2103.09656",
          "publishedOn": "2021-08-17T01:54:48.332Z",
          "wordCount": 593,
          "title": "Set-to-Sequence Methods in Machine Learning: a Review. (arXiv:2103.09656v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1\">Marco Scutari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panero_F/0/1/0/all/0/1\">Francesca Panero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proissl_M/0/1/0/all/0/1\">Manuel Proissl</a>",
          "description": "In this paper we present a general framework for estimating regression models\nsubject to a user-defined level of fairness. We enforce fairness as a model\nselection step in which we choose the value of a ridge penalty to control the\neffect of sensitive attributes. We then estimate the parameters of the model\nconditional on the chosen penalty value. Our proposal is mathematically simple,\nwith a solution that is partly in closed form, and produces estimates of the\nregression coefficients that are intuitive to interpret as a function of the\nlevel of fairness. Furthermore, it is easily extended to generalised linear\nmodels, kernelised regression models and other penalties; and it can\naccommodate multiple definitions of fairness.\n\nWe compare our approach with the regression model from Komiyama et al.\n(2018), which implements a provably-optimal linear regression model; and with\nthe fair models from Zafar et al. (2019). We evaluate these approaches\nempirically on six different data sets, and we find that our proposal provides\nbetter goodness of fit and better predictive accuracy for the same level of\nfairness. In addition, we highlight a source of bias in the original\nexperimental evaluation in Komiyama et al. (2018).",
          "link": "http://arxiv.org/abs/2105.13817",
          "publishedOn": "2021-08-17T01:54:48.302Z",
          "wordCount": 655,
          "title": "Achieving Fairness with a Simple Ridge Penalty. (arXiv:2105.13817v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kataria_A/0/1/0/all/0/1\">Aman Kataria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Smarajit Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karar_V/0/1/0/all/0/1\">Vinod Karar</a>",
          "description": "A head tracker is a crucial part of the head mounted display systems, as it\ntracks the head of the pilot in the plane/cockpit simulator. The operational\nflaws of head trackers are also dependent on different environmental conditions\nlike different lighting conditions and stray light interference. In this\nletter, an optical tracker has been employed to gather the 6-DoF data of head\nmovements under different environmental conditions. Also, the effect of\ndifferent environmental conditions and variation in distance between the\nreceiver and optical transmitter on the 6-DoF data was analyzed.",
          "link": "http://arxiv.org/abs/2108.06606",
          "publishedOn": "2021-08-17T01:54:48.275Z",
          "wordCount": 562,
          "title": "Prediction Analysis of Optical Tracker Parameters using Machine Learning Approaches for efficient Head Tracking. (arXiv:2108.06606v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karpov_N/0/1/0/all/0/1\">Nikolai Karpov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>",
          "description": "Motivated by real-world applications such as fast fashion retailing and\nonline advertising, the Multinomial Logit Bandit (MNL-bandit) is a popular\nmodel in online learning and operations research, and has attracted much\nattention in the past decade. However, it is a bit surprising that pure\nexploration, a basic problem in bandit theory, has not been well studied in\nMNL-bandit so far. In this paper we give efficient algorithms for pure\nexploration in MNL-bandit. Our algorithms achieve instance-sensitive pull\ncomplexities. We also complement the upper bounds by an almost matching lower\nbound.",
          "link": "http://arxiv.org/abs/2012.01499",
          "publishedOn": "2021-08-17T01:54:48.268Z",
          "wordCount": 554,
          "title": "Instance-Sensitive Algorithms for Pure Exploration in Multinomial Logit Bandit. (arXiv:2012.01499v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1\">Doseok Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_L/0/1/0/all/0/1\">Lucas Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattar_M/0/1/0/all/0/1\">Manan Khattar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agwan_U/0/1/0/all/0/1\">Utkarsha Agwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadarajah_S/0/1/0/all/0/1\">Selvaprabuh Nadarajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanos_C/0/1/0/all/0/1\">Costas Spanos</a>",
          "description": "Our team is proposing to run a full-scale energy demand response experiment\nin an office building. Although this is an exciting endeavor which will provide\nvalue to the community, collecting training data for the reinforcement learning\nagent is costly and will be limited. In this work, we examine how offline\ntraining can be leveraged to minimize data costs (accelerate convergence) and\nprogram implementation costs. We present two approaches to doing so:\npretraining our model to warm start the experiment with simulated tasks, and\nusing a planning model trained to simulate the real world's rewards to the\nagent. We present results that demonstrate the utility of offline reinforcement\nlearning to efficient price-setting in the energy demand response problem.",
          "link": "http://arxiv.org/abs/2108.06594",
          "publishedOn": "2021-08-17T01:54:48.263Z",
          "wordCount": 568,
          "title": "Offline-Online Reinforcement Learning for Energy Pricing in Office Demand Response: Lowering Energy and Data Costs. (arXiv:2108.06594v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.01354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kenny Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pogue_A/0/1/0/all/0/1\">Alexandra Pogue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_B/0/1/0/all/0/1\">Brett T. Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agha_mohammadi_A/0/1/0/all/0/1\">Ali-akbar Agha-mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1\">Ankur Mehta</a>",
          "description": "Monocular depth inference has gained tremendous attention from researchers in\nrecent years and remains as a promising replacement for expensive\ntime-of-flight sensors, but issues with scale acquisition and implementation\noverhead still plague these systems. To this end, this work presents an\nunsupervised learning framework that is able to predict at-scale depth maps and\negomotion, in addition to camera intrinsics, from a sequence of monocular\nimages via a single network. Our method incorporates both spatial and temporal\ngeometric constraints to resolve depth and pose scale factors, which are\nenforced within the supervisory reconstruction loss functions at training time.\nOnly unlabeled stereo sequences are required for training the weights of our\nsingle-network architecture, which reduces overall implementation overhead as\ncompared to previous methods. Our results demonstrate strong performance when\ncompared to the current state-of-the-art on multiple sequences of the KITTI\ndriving dataset and can provide faster training times with its reduced network\ncomplexity.",
          "link": "http://arxiv.org/abs/2011.01354",
          "publishedOn": "2021-08-17T01:54:48.240Z",
          "wordCount": 656,
          "title": "Unsupervised Monocular Depth Learning with Integrated Intrinsics and Spatio-Temporal Constraints. (arXiv:2011.01354v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1\">Cheng Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>",
          "description": "This paper tackles the task of category-level pose estimation for garments.\nWith a near infinite degree of freedom, a garment's full configuration (i.e.,\nposes) is often described by the per-vertex 3D locations of its entire 3D\nsurface. However, garments are also commonly subject to extreme cases of\nself-occlusion, especially when folded or crumpled, making it challenging to\nperceive their full 3D surface. To address these challenges, we propose\nGarmentNets, where the key idea is to formulate the deformable object pose\nestimation problem as a shape completion task in the canonical space. This\ncanonical space is defined across garments instances within a category,\ntherefore, specifies the shared category-level pose. By mapping the observed\npartial surface to the canonical space and completing it in this space, the\noutput representation describes the garment's full configuration using a\ncomplete 3D mesh with the per-vertex canonical coordinate label. To properly\nhandle the thin 3D structure presented on garments, we proposed a novel 3D\nshape representation using the generalized winding number field. Experiments\ndemonstrate that GarmentNets is able to generalize to unseen garment instances\nand achieve significantly better performance compared to alternative\napproaches.",
          "link": "http://arxiv.org/abs/2104.05177",
          "publishedOn": "2021-08-17T01:54:48.234Z",
          "wordCount": 663,
          "title": "GarmentNets: Category-Level Pose Estimation for Garments via Canonical Space Shape Completion. (arXiv:2104.05177v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Learning from a limited number of samples is challenging since the learned\nmodel can easily become overfitted based on the biased distribution formed by\nonly a few training examples. In this paper, we calibrate the distribution of\nthese few-sample classes by transferring statistics from the classes with\nsufficient examples, then an adequate number of examples can be sampled from\nthe calibrated distribution to expand the inputs to the classifier. We assume\nevery dimension in the feature representation follows a Gaussian distribution\nso that the mean and the variance of the distribution can borrow from that of\nsimilar classes whose statistics are better estimated with an adequate number\nof samples. Our method can be built on top of off-the-shelf pretrained feature\nextractors and classification models without extra parameters. We show that a\nsimple logistic regression classifier trained using the features sampled from\nour calibrated distribution can outperform the state-of-the-art accuracy on two\ndatasets (~5% improvement on miniImageNet compared to the next best). The\nvisualization of these generated features demonstrates that our calibrated\ndistribution is an accurate estimation.",
          "link": "http://arxiv.org/abs/2101.06395",
          "publishedOn": "2021-08-17T01:54:48.222Z",
          "wordCount": 659,
          "title": "Free Lunch for Few-shot Learning: Distribution Calibration. (arXiv:2101.06395v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1\">Christian Anti&#x107;</a>",
          "description": "Analogy-making is at the core of human and artificial intelligence and\ncreativity. This paper introduces from first principles an abstract algebraic\nframework of analogical proportions of the form `$a$ is to $b$ what $c$ is to\n$d$' in the general setting of universal algebra. This enables us to compare\nmathematical objects possibly across different domains in a uniform way which\nis crucial for AI-systems. The main idea is to define solutions to analogical\nequations in terms of maximal sets of algebraic justifications, which amounts\nto deriving abstract terms of concrete elements from a `known' source domain\nwhich can then be instantiated in an `unknown' target domain to obtain\nanalogous elements. It turns out that our notion of analogical proportions has\nappealing mathematical properties. We compare our framework with two recently\nintroduced frameworks of analogical proportions from the literature in the\nconcrete domains of sets and numbers, and we show that in each case we either\ndisagree with the notion from the literature justified by some counter-example\nor we can show that our model yields strictly more solutions. As we construct\nour model from first principles using only elementary concepts of universal\nalgebra, and since our model questions some basic properties of analogical\nproportions presupposed in the literature, to convince the reader of the\nplausibility of our model we show that it can be naturally embedded into\nfirst-order logic via model-theoretic types, and prove that analogical\nproportions are compatible with structure-preserving mappings from that\nperspective. This provides strong evidence for its applicability. In a broader\nsense, this paper is a first step towards a theory of analogical reasoning and\nlearning systems with potential applications to fundamental AI-problems like\ncommonsense reasoning and computational learning and creativity.",
          "link": "http://arxiv.org/abs/2006.02854",
          "publishedOn": "2021-08-17T01:54:48.215Z",
          "wordCount": 784,
          "title": "Analogical Proportions. (arXiv:2006.02854v7 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Nanyang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qianxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao-Yun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhanxing Zhu</a>",
          "description": "Despite the empirical success in various domains, it has been revealed that\ndeep neural networks are vulnerable to maliciously perturbed input data that\nmuch degrade their performance. This is known as adversarial attacks. To\ncounter adversarial attacks, adversarial training formulated as a form of\nrobust optimization has been demonstrated to be effective. However, conducting\nadversarial training brings much computational overhead compared with standard\ntraining. In order to reduce the computational cost, we propose an annealing\nmechanism, Amata, to reduce the overhead associated with adversarial training.\nThe proposed Amata is provably convergent, well-motivated from the lens of\noptimal control theory and can be combined with existing acceleration methods\nto further enhance performance. It is demonstrated that on standard datasets,\nAmata can achieve similar or better robustness with around 1/3 to 1/2 the\ncomputational time compared with traditional methods. In addition, Amata can be\nincorporated into other adversarial training acceleration algorithms (e.g.\nYOPO, Free, Fast, and ATTA), which leads to further reduction in computational\ntime on large-scale problems.",
          "link": "http://arxiv.org/abs/2012.08112",
          "publishedOn": "2021-08-17T01:54:48.177Z",
          "wordCount": 634,
          "title": "Amata: An Annealing Mechanism for Adversarial Training Acceleration. (arXiv:2012.08112v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_B/0/1/0/all/0/1\">Bissan Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nathwani_J/0/1/0/all/0/1\">Jatin Nathwani</a>",
          "description": "The past decade has seen a rapid penetration of electric vehicles (EV) in the\nmarket, more and more logistics and transportation companies start to deploy\nEVs for service provision. In order to model the operations of a commercial EV\nfleet, we utilize the EV routing problem with time windows (EVRPTW). In this\nresearch, we propose an end-to-end deep reinforcement learning framework to\nsolve the EVRPTW. In particular, we develop an attention model incorporating\nthe pointer network and a graph embedding technique to parameterize a\nstochastic policy for solving the EVRPTW. The model is then trained using\npolicy gradient with rollout baseline. Our numerical studies show that the\nproposed model is able to efficiently solve EVRPTW instances of large sizes\nthat are not solvable with any existing approaches.",
          "link": "http://arxiv.org/abs/2010.02068",
          "publishedOn": "2021-08-17T01:54:48.164Z",
          "wordCount": 620,
          "title": "Deep Reinforcement Learning for Electric Vehicle Routing Problem with Time Windows. (arXiv:2010.02068v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_K/0/1/0/all/0/1\">Kaan Gokcesu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_H/0/1/0/all/0/1\">Hakan Gokcesu</a>",
          "description": "We investigate the problem of online learning, which has gained significant\nattention in recent years due to its applicability in a wide range of fields\nfrom machine learning to game theory. Specifically, we study the online\noptimization of mixable loss functions in a dynamic environment. We introduce\nonline mixture schemes that asymptotically achieves the performance of the best\ndynamic estimation sequence of the switching oracle with optimal regret\nredundancies. The best dynamic estimation sequence that we compete against is\nselected in hindsight with full observation of the loss functions and is\nallowed to select different optimal estimations in different time intervals\n(segments). We propose two mixtures in our work. Firstly, we propose a\ntractable polynomial time complexity algorithm that can achieve the optimal\nredundancy of the intractable brute force approach. Secondly, we propose an\nefficient logarithmic time complexity algorithm that can achieve the optimal\nredundancy up to a constant multiplicity gap. Our results are guaranteed to\nhold in a strong deterministic sense in an individual sequence manner.",
          "link": "http://arxiv.org/abs/2108.06411",
          "publishedOn": "2021-08-17T01:54:48.140Z",
          "wordCount": 605,
          "title": "Optimal and Efficient Algorithms for General Mixable Losses against Switching Oracles. (arXiv:2108.06411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziwei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "In order to model the evolution of user preference, we should learn user/item\nembeddings based on time-ordered item purchasing sequences, which is defined as\nSequential Recommendation (SR) problem. Existing methods leverage sequential\npatterns to model item transitions. However, most of them ignore crucial\ntemporal collaborative signals, which are latent in evolving user-item\ninteractions and coexist with sequential patterns. Therefore, we propose to\nunify sequential patterns and temporal collaborative signals to improve the\nquality of recommendation, which is rather challenging. Firstly, it is hard to\nsimultaneously encode sequential patterns and collaborative signals. Secondly,\nit is non-trivial to express the temporal effects of collaborative signals.\n\nHence, we design a new framework Temporal Graph Sequential Recommender\n(TGSRec) upon our defined continuous-time bi-partite graph. We propose a novel\nTemporal Collaborative Trans-former (TCT) layer in TGSRec, which advances the\nself-attention mechanism by adopting a novel collaborative attention. TCT layer\ncan simultaneously capture collaborative signals from both users and items, as\nwell as considering temporal dynamics inside sequential patterns. We propagate\nthe information learned fromTCTlayerover the temporal graph to unify sequential\npatterns and temporal collaborative signals. Empirical results on five datasets\nshow that TGSRec significantly outperforms other baselines, in average up to\n22.5% and 22.1%absolute improvements in Recall@10and MRR, respectively.",
          "link": "http://arxiv.org/abs/2108.06625",
          "publishedOn": "2021-08-17T01:54:48.134Z",
          "wordCount": 650,
          "title": "Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer. (arXiv:2108.06625v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Boxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Adversarial training is a powerful type of defense against adversarial\nexamples. Previous empirical results suggest that adversarial training requires\nwider networks for better performances. However, it remains elusive how neural\nnetwork width affects model robustness. In this paper, we carefully examine the\nrelationship between network width and model robustness. Specifically, we show\nthat the model robustness is closely related to the tradeoff between natural\naccuracy and perturbation stability, which is controlled by the robust\nregularization parameter $\\lambda$. With the same $\\lambda$, wider networks can\nachieve better natural accuracy but worse perturbation stability, leading to a\npotentially worse overall model robustness. To understand the origin of this\nphenomenon, we further relate the perturbation stability with the network's\nlocal Lipschitzness. By leveraging recent results on neural tangent kernels, we\ntheoretically show that wider networks tend to have worse perturbation\nstability. Our analyses suggest that: 1) the common strategy of first\nfine-tuning $\\lambda$ on small networks and then directly use it for wide model\ntraining could lead to deteriorated model robustness; 2) one needs to properly\nenlarge $\\lambda$ to unleash the robustness potential of wider models fully.\nFinally, we propose a new Width Adjusted Regularization (WAR) method that\nadaptively enlarges $\\lambda$ on wide models and significantly saves the tuning\ntime.",
          "link": "http://arxiv.org/abs/2010.01279",
          "publishedOn": "2021-08-17T01:54:48.128Z",
          "wordCount": 694,
          "title": "Do Wider Neural Networks Really Help Adversarial Robustness?. (arXiv:2010.01279v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_Y/0/1/0/all/0/1\">Yunseok Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1\">Won Joon Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Kook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>",
          "description": "The emergence of quantum computing enables for researchers to apply quantum\ncircuit on many existing studies. Utilizing quantum circuit and quantum\ndifferential programming, many research are conducted such as \\textit{Quantum\nMachine Learning} (QML). In particular, quantum reinforcement learning is a\ngood field to test the possibility of quantum machine learning, and a lot of\nresearch is being done. This work will introduce the concept of quantum\nreinforcement learning using a variational quantum circuit, and confirm its\npossibility through implementation and experimentation. We will first present\nthe background knowledge and working principle of quantum reinforcement\nlearning, and then guide the implementation method using the PennyLane library.\nWe will also discuss the power and possibility of quantum reinforcement\nlearning from the experimental results obtained through this work.",
          "link": "http://arxiv.org/abs/2108.06849",
          "publishedOn": "2021-08-17T01:54:47.822Z",
          "wordCount": 560,
          "title": "Introduction to Quantum Reinforcement Learning: Theory and PennyLane-based Implementation. (arXiv:2108.06849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shao-Qun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Gao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhi-Hua Zhou</a>",
          "description": "Complex-valued neural networks have attracted increasing attention in recent\nyears, while it remains open on the advantages of complex-valued neural\nnetworks in comparison with real-valued networks. This work takes one step on\nthis direction by introducing the \\emph{complex-reaction network} with\nfully-connected feed-forward architecture. We prove the universal approximation\nproperty for complex-reaction networks, and show that a class of radial\nfunctions can be approximated by a complex-reaction network using the\npolynomial number of parameters, whereas real-valued networks need at least\nexponential parameters to reach the same approximation level. For empirical\nrisk minimization, our theoretical result shows that the critical point set of\ncomplex-reaction networks is a proper subset of that of real-valued networks,\nwhich may show some insights on finding the optimal solutions more easily for\ncomplex-reaction networks.",
          "link": "http://arxiv.org/abs/2108.06711",
          "publishedOn": "2021-08-17T01:54:47.750Z",
          "wordCount": 558,
          "title": "Towards Understanding Theoretical Advantages of Complex-Reaction Networks. (arXiv:2108.06711v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karlbauer_M/0/1/0/all/0/1\">Matthias Karlbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menge_T/0/1/0/all/0/1\">Tobias Menge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otte_S/0/1/0/all/0/1\">Sebastian Otte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P.A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholten_T/0/1/0/all/0/1\">Thomas Scholten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wulfmeyer_V/0/1/0/all/0/1\">Volker Wulfmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butz_M/0/1/0/all/0/1\">Martin V. Butz</a>",
          "description": "Knowledge about the hidden factors that determine particular system dynamics\nis crucial for both explaining them and pursuing goal-directed interventions.\nInferring these factors from time series data without supervision remains an\nopen challenge. Here, we focus on spatiotemporal processes, including wave\npropagation and weather dynamics, for which we assume that universal causes\n(e.g. physics) apply throughout space and time. A recently introduced\nDIstributed SpatioTemporal graph Artificial Neural network Architecture\n(DISTANA) is used and enhanced to learn such processes, requiring fewer\nparameters and achieving significantly more accurate predictions compared to\ntemporal convolutional neural networks and other related approaches. We show\nthat DISTANA, when combined with a retrospective latent state inference\nprinciple called active tuning, can reliably derive location-respective hidden\ncausal factors. In a current weather prediction benchmark, DISTANA infers our\nplanet's land-sea mask solely by observing temperature dynamics and, meanwhile,\nuses the self inferred information to improve its own future temperature\npredictions.",
          "link": "http://arxiv.org/abs/2009.09823",
          "publishedOn": "2021-08-17T01:54:47.638Z",
          "wordCount": 641,
          "title": "Latent State Inference in a Spatiotemporal Generative Model. (arXiv:2009.09823v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>",
          "description": "Session-based recommendation systems suggest relevant items to users by\nmodeling user behavior and preferences using short-term anonymous sessions.\nExisting methods leverage Graph Neural Networks (GNNs) that propagate and\naggregate information from neighboring nodes i.e., local message passing. Such\ngraph-based architectures have representational limits, as a single sub-graph\nis susceptible to overfit the sequential dependencies instead of accounting for\ncomplex transitions between items in different sessions. We propose using a\nTransformer in combination with a target attentive GNN, which allows richer\nRepresentation Learning. Our experimental results and ablation show that our\nproposed method is competitive with the existing methods on real-world\nbenchmark datasets, improving on graph-based hypotheses.",
          "link": "http://arxiv.org/abs/2107.01516",
          "publishedOn": "2021-08-17T01:54:47.620Z",
          "wordCount": 570,
          "title": "Improved Representation Learning for Session-based Recommendation. (arXiv:2107.01516v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.09526",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Oglic_D/0/1/0/all/0/1\">Dino Oglic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cvetkovic_Z/0/1/0/all/0/1\">Zoran Cvetkovic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sollich_P/0/1/0/all/0/1\">Peter Sollich</a>",
          "description": "We investigate the potential of stochastic neural networks for learning\neffective waveform-based acoustic models. The waveform-based setting, inherent\nto fully end-to-end speech recognition systems, is motivated by several\ncomparative studies of automatic and human speech recognition that associate\nstandard non-adaptive feature extraction techniques with information loss which\ncan adversely affect robustness. Stochastic neural networks, on the other hand,\nare a class of models capable of incorporating rich regularization mechanisms\ninto the learning process. We consider a deep convolutional neural network that\nfirst decomposes speech into frequency sub-bands via an adaptive parametric\nconvolutional block where filters are specified by cosine modulations of\ncompactly supported windows. The network then employs standard non-parametric\n1D convolutions to extract relevant spectro-temporal patterns while gradually\ncompressing the structured high dimensional representation generated by the\nparametric block. We rely on a probabilistic parametrization of the proposed\nneural architecture and learn the model using stochastic variational inference.\nThis requires evaluation of an analytically intractable integral defining the\nKullback-Leibler divergence term responsible for regularization, for which we\npropose an effective approximation based on the Gauss-Hermite quadrature. Our\nempirical results demonstrate a superior performance of the proposed approach\nover comparable waveform-based baselines and indicate that it could lead to\nrobustness. Moreover, the approach outperforms a recently proposed deep\nconvolutional neural network for learning of robust acoustic models with\nstandard FBANK features.",
          "link": "http://arxiv.org/abs/1906.09526",
          "publishedOn": "2021-08-17T01:54:47.608Z",
          "wordCount": 705,
          "title": "Learning Waveform-Based Acoustic Models using Deep Variational Convolutional Neural Networks. (arXiv:1906.09526v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geissler_F/0/1/0/all/0/1\">Florian Geissler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qutub_S/0/1/0/all/0/1\">Syed Qutub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sayanta Roychowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_A/0/1/0/all/0/1\">Ali Asgari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamasia_A/0/1/0/all/0/1\">Akash Dhamasia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graefe_R/0/1/0/all/0/1\">Ralf Graefe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattabiraman_K/0/1/0/all/0/1\">Karthik Pattabiraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulitsch_M/0/1/0/all/0/1\">Michael Paulitsch</a>",
          "description": "Convolutional neural networks (CNNs) have become an established part of\nnumerous safety-critical computer vision applications, including human robot\ninteractions and automated driving. Real-world implementations will need to\nguarantee their robustness against hardware soft errors corrupting the\nunderlying platform memory. Based on the previously observed efficacy of\nactivation clipping techniques, we build a prototypical safety case for\nclassifier CNNs by demonstrating that range supervision represents a highly\nreliable fault detector and mitigator with respect to relevant bit flips,\nadopting an eight-exponent floating point data representation. We further\nexplore novel, non-uniform range restriction methods that effectively suppress\nthe probability of silent data corruptions and uncorrectable errors. As a\nsafety-relevant end-to-end use case, we showcase the benefit of our approach in\na vehicle classification scenario, using ResNet-50 and the traffic camera data\nset MIOVision. The quantitative evidence provided in this work can be leveraged\nto inspire further and possibly more complex CNN safety arguments.",
          "link": "http://arxiv.org/abs/2108.07019",
          "publishedOn": "2021-08-17T01:54:47.602Z",
          "wordCount": 611,
          "title": "Towards a Safety Case for Hardware Fault Tolerance in Convolutional Neural Networks Using Activation Range Supervision. (arXiv:2108.07019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.13446",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sharma_A/0/1/0/all/0/1\">Ankit Sharma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gupta_G/0/1/0/all/0/1\">Garima Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Prasad_R/0/1/0/all/0/1\">Ranjitha Prasad</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1\">Arnab Chatterjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vig_L/0/1/0/all/0/1\">Lovekesh Vig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shroff_G/0/1/0/all/0/1\">Gautam Shroff</a>",
          "description": "Causal inference (CI) in observational studies has received a lot of\nattention in healthcare, education, ad attribution, policy evaluation, etc.\nConfounding is a typical hazard, where the context affects both, the treatment\nassignment and response. In a multiple treatment scenario, we propose the\nneural network based MultiMBNN, where we overcome confounding by employing\ngeneralized propensity score based matching, and learning balanced\nrepresentations. We benchmark the performance on synthetic and real-world\ndatasets using PEHE, and mean absolute percentage error over ATE as metrics.\nMultiMBNN outperforms the state-of-the-art algorithms for CI such as TARNet and\nPerfect Match (PM).",
          "link": "http://arxiv.org/abs/2004.13446",
          "publishedOn": "2021-08-17T01:54:47.581Z",
          "wordCount": 583,
          "title": "MultiMBNN: Matched and Balanced Causal Inference with Neural Networks. (arXiv:2004.13446v4 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aykas_D/0/1/0/all/0/1\">Dogan Aykas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrkanoon_S/0/1/0/all/0/1\">Siamak Mehrkanoon</a>",
          "description": "Reliable and accurate wind speed prediction has significant impact in many\nindustrial sectors such as economic, business and management among others. This\npaper presents a new model for wind speed prediction based on Graph Attention\nNetworks (GAT). In particular, the proposed model extends GAT architecture by\nequipping it with a learnable adjacency matrix as well as incorporating a new\nattention mechanism with the aim of obtaining attention scores per weather\nvariable. The output of the GAT based model is combined with the LSTM layer in\norder to exploit both the spatial and temporal characteristics of the\nmultivariate multidimensional historical weather data. Real weather data\ncollected from several cities in Denmark and Netherlands are used to conduct\nthe experiments and evaluate the performance of the proposed model. We show\nthat in comparison to previous architectures used for wind speed prediction,\nthe proposed model is able to better learn the complex input-output\nrelationships of the weather data. Furthermore, thanks to the learned attention\nweights, the model provides an additional insights on the most important\nweather variables and cities for the studied prediction task.",
          "link": "http://arxiv.org/abs/2108.07063",
          "publishedOn": "2021-08-17T01:54:47.568Z",
          "wordCount": 615,
          "title": "Multistream Graph Attention Networks for Wind Speed Forecasting. (arXiv:2108.07063v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.11304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faez Ahmed</a>",
          "description": "Deep generative models are proven to be a useful tool for automatic design\nsynthesis and design space exploration. When applied in engineering design,\nexisting generative models face three challenges: 1) generated designs lack\ndiversity and do not cover all areas of the design space, 2) it is difficult to\nexplicitly improve the overall performance or quality of generated designs, and\n3) existing models generally do not generate novel designs, outside the domain\nof the training data. In this paper, we simultaneously address these challenges\nby proposing a new Determinantal Point Processes based loss function for\nprobabilistic modeling of diversity and quality. With this new loss function,\nwe develop a variant of the Generative Adversarial Network, named \"Performance\nAugmented Diverse Generative Adversarial Network\" or PaDGAN, which can generate\nnovel high-quality designs with good coverage of the design space. Using three\nsynthetic examples and one real-world airfoil design example, we demonstrate\nthat PaDGAN can generate diverse and high-quality designs. In comparison to a\nvanilla Generative Adversarial Network, on average, it generates samples with a\n28% higher mean quality score with larger diversity and without the mode\ncollapse issue. Unlike typical generative models that usually generate new\ndesigns by interpolating within the boundary of training data, we show that\nPaDGAN expands the design space boundary outside the training data towards\nhigh-quality regions. The proposed method is broadly applicable to many tasks\nincluding design space exploration, design optimization, and creative solution\nrecommendation.",
          "link": "http://arxiv.org/abs/2002.11304",
          "publishedOn": "2021-08-17T01:54:47.563Z",
          "wordCount": 734,
          "title": "PaDGAN: A Generative Adversarial Network for Performance Augmented Diverse Designs. (arXiv:2002.11304v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1\">Houcemeddine Turki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taieb_M/0/1/0/all/0/1\">Mohamed Ali Hadj Taieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouicha_M/0/1/0/all/0/1\">Mohamed Ben Aouicha</a>",
          "description": "So far, multi-label classification algorithms have been evaluated using\nstatistical methods that do not consider the semantics of the considered\nclasses and that fully depend on abstract computations such as Bayesian\nReasoning. Currently, there are several attempts to develop ontology-based\nmethods for a better assessment of supervised classification algorithms. In\nthis research paper, we define a novel approach that aligns expected labels\nwith predicted labels in multi-label classification using ontology-driven\nfeature-based semantic similarity measures and we use it to develop a method\nfor creating precise confusion matrices for a more effective evaluation of\nmulti-label classification algorithms.",
          "link": "http://arxiv.org/abs/2011.00109",
          "publishedOn": "2021-08-17T01:54:47.556Z",
          "wordCount": 579,
          "title": "Knowledge-Based Construction of Confusion Matrices for Multi-Label Classification Algorithms using Semantic Similarity Measures. (arXiv:2011.00109v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.06022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xudong Li</a>",
          "description": "To discover powerful yet compact models is an important goal of neural\narchitecture search. Previous two-stage one-shot approaches are limited by\nsearch space with a fixed depth. It seems handy to include an additional skip\nconnection in the search space to make depths variable. However, it creates a\nlarge range of perturbation during supernet training and it has difficulty\ngiving a confident ranking for subnetworks. In this paper, we discover that\nskip connections bring about significant feature inconsistency compared with\nother operations, which potentially degrades the supernet performance. Based on\nthis observation, we tackle the problem by imposing an equivariant learnable\nstabilizer to homogenize such disparities. Experiments show that our proposed\nstabilizer helps to improve the supernet's convergence as well as ranking\nperformance. With an evolutionary search backend that incorporates the\nstabilized supernet as an evaluator, we derive a family of state-of-the-art\narchitectures, the SCARLET series of several depths, especially SCARLET-A\nobtains 76.9% top-1 accuracy on ImageNet. Code is available at\nhttps://github.com/xiaomi-automl/ScarletNAS.",
          "link": "http://arxiv.org/abs/1908.06022",
          "publishedOn": "2021-08-17T01:54:47.550Z",
          "wordCount": 688,
          "title": "SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search. (arXiv:1908.06022v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiawei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jian Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yanru Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jieli Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiange Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wei-Wei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1\">Yufang Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_G/0/1/0/all/0/1\">Guang Ning</a>",
          "description": "Diabetes prediction is an important data science application in the social\nhealthcare domain. There exist two main challenges in the diabetes prediction\ntask: data heterogeneity since demographic and metabolic data are of different\ntypes, data insufficiency since the number of diabetes cases in a single\nmedical center is usually limited. To tackle the above challenges, we employ\ngradient boosting decision trees (GBDT) to handle data heterogeneity and\nintroduce multi-task learning (MTL) to solve data insufficiency. To this end,\nTask-wise Split Gradient Boosting Trees (TSGB) is proposed for the multi-center\ndiabetes prediction task. Specifically, we firstly introduce task gain to\nevaluate each task separately during tree construction, with a theoretical\nanalysis of GBDT's learning objective. Secondly, we reveal a problem when\ndirectly applying GBDT in MTL, i.e., the negative task gain problem. Finally,\nwe propose a novel split method for GBDT in MTL based on the task gain\nstatistics, named task-wise split, as an alternative to standard feature-wise\nsplit to overcome the mentioned negative task gain problem. Extensive\nexperiments on a large-scale real-world diabetes dataset and a commonly used\nbenchmark dataset demonstrate TSGB achieves superior performance against\nseveral state-of-the-art methods. Detailed case studies further support our\nanalysis of negative task gain problems and provide insightful findings. The\nproposed TSGB method has been deployed as an online diabetes risk assessment\nsoftware for early diagnosis.",
          "link": "http://arxiv.org/abs/2108.07107",
          "publishedOn": "2021-08-17T01:54:47.545Z",
          "wordCount": 705,
          "title": "Task-wise Split Gradient Boosting Trees for Multi-center Diabetes Prediction. (arXiv:2108.07107v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasu_M/0/1/0/all/0/1\">Malolan Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abreu_N/0/1/0/all/0/1\">Nelson Abreu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasquez_R/0/1/0/all/0/1\">Raysa V&#xe1;squez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">Christian L&#xf3;pez</a>",
          "description": "Vehicle counting systems can help with vehicle analysis and traffic incident\ndetection. Unfortunately, most existing methods require some level of human\ninput to identify the Region of interest (ROI), movements of interest, or to\nestablish a reference point or line to count vehicles from traffic cameras.\nThis work introduces a method to count vehicles from traffic videos that\nautomatically identifies the ROI for the camera, as well as the driving\ntrajectories of the vehicles. This makes the method feasible to use with\nPan-Tilt-Zoom cameras, which are frequently used in developing countries.\nPreliminary results indicate that the proposed method achieves an average\nintersection over the union of 57.05% for the ROI and a mean absolute error of\njust 17.44% at counting vehicles of the traffic video cameras tested.",
          "link": "http://arxiv.org/abs/2108.07135",
          "publishedOn": "2021-08-17T01:54:47.522Z",
          "wordCount": 578,
          "title": "Vehicle-counting with Automatic Region-of-Interest and Driving-Trajectory detection. (arXiv:2108.07135v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09264",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Wen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hung_K/0/1/0/all/0/1\">Kuo-Hsuan Hung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">You-Jin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_A/0/1/0/all/0/1\">Alexander Chao-Fu Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_Y/0/1/0/all/0/1\">Ya-Hsin Lai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kai-Chun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_S/0/1/0/all/0/1\">Sze-Wei Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Syu-Siang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>",
          "description": "In this paper, we present a deep learning-based speech signal-processing\nmobile application, CITISEN, which can perform three functions: speech\nenhancement (SE), acoustic scene conversion (ASC), and model adaptation (MA).\nFor SE, CITISEN can effectively reduce noise components from speech signals and\naccordingly enhance their clarity and intelligibility. For ASC, CITISEN can\nconvert the current background sound to a different background sound. Finally,\nfor MA, CITISEN can effectively adapt an SE model, with a few audio files, when\nit encounters unknown speakers or noise types; the adapted SE model is used to\nenhance the upcoming noisy utterances. Experimental results confirmed the\neffectiveness of CITISEN in performing these three functions via objective\nevaluation and subjective listening tests. The promising results reveal that\nthe developed CITISEN mobile application can potentially be used as a front-end\nprocessor for various speech-related services such as voice communication,\nassistive hearing devices, and virtual reality headsets.",
          "link": "http://arxiv.org/abs/2008.09264",
          "publishedOn": "2021-08-17T01:54:47.510Z",
          "wordCount": 621,
          "title": "CITISEN: A Deep Learning-Based Speech Signal-Processing Mobile Application. (arXiv:2008.09264v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06890",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Ji-Hoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Hoon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Hyun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_H/0/1/0/all/0/1\">Hong-Gyu Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Few-shot speaker adaptation is a specific Text-to-Speech (TTS) system that\naims to reproduce a novel speaker's voice with a few training data. While\nnumerous attempts have been made to the few-shot speaker adaptation system,\nthere is still a gap in terms of speaker similarity to the target speaker\ndepending on the amount of data. To bridge the gap, we propose GC-TTS which\nachieves high-quality speaker adaptation with significantly improved speaker\nsimilarity. Specifically, we leverage two geometric constraints to learn\ndiscriminative speaker representations. Here, a TTS model is pre-trained for\nbase speakers with a sufficient amount of data, and then fine-tuned for novel\nspeakers on a few minutes of data with two geometric constraints. Two geometric\nconstraints enable the model to extract discriminative speaker embeddings from\nlimited data, which leads to the synthesis of intelligible speech. We discuss\nand verify the effectiveness of GC-TTS by comparing it with popular and\nessential methods. The experimental results demonstrate that GC-TTS generates\nhigh-quality speech from only a few minutes of training data, outperforming\nstandard techniques in terms of speaker similarity to the target speaker.",
          "link": "http://arxiv.org/abs/2108.06890",
          "publishedOn": "2021-08-17T01:54:47.501Z",
          "wordCount": 639,
          "title": "GC-TTS: Few-shot Speaker Adaptation with Geometric Constraints. (arXiv:2108.06890v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/1911.09721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Avishek Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maity_R/0/1/0/all/0/1\">Raj Kumar Maity</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadhe_S/0/1/0/all/0/1\">Swanand Kadhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1\">Arya Mazumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>",
          "description": "We develop a communication-efficient distributed learning algorithm that is\nrobust against Byzantine worker machines. We propose and analyze a distributed\ngradient-descent algorithm that performs a simple thresholding based on\ngradient norms to mitigate Byzantine failures. We show the (statistical)\nerror-rate of our algorithm matches that of Yin et al.~\\cite{dong}, which uses\nmore complicated schemes (coordinate-wise median, trimmed mean). Furthermore,\nfor communication efficiency, we consider a generic class of\n$\\delta$-approximate compressors from Karimireddi et al.~\\cite{errorfeed} that\nencompasses sign-based compressors and top-$k$ sparsification. Our algorithm\nuses compressed gradients and gradient norms for aggregation and Byzantine\nremoval respectively. We establish the statistical error rate for non-convex\nsmooth loss functions. We show that, in certain range of the compression factor\n$\\delta$, the (order-wise) rate of convergence is not affected by the\ncompression operation. Moreover, we analyze the compressed gradient descent\nalgorithm with error feedback (proposed in \\cite{errorfeed}) in a distributed\nsetting and in the presence of Byzantine worker machines. We show that\nexploiting error feedback improves the statistical error rate. Finally, we\nexperimentally validate our results and show good performance in convergence\nfor convex (least-square regression) and non-convex (neural network training)\nproblems.",
          "link": "http://arxiv.org/abs/1911.09721",
          "publishedOn": "2021-08-17T01:54:47.485Z",
          "wordCount": 692,
          "title": "Communication-Efficient and Byzantine-Robust Distributed Learning with Error Feedback. (arXiv:1911.09721v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Alvaro Almeida Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_A/0/1/0/all/0/1\">Ant&#xf4;nio J. Silva Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubelli_J/0/1/0/all/0/1\">Jorge P. Zubelli</a>",
          "description": "We recover the gradient of a given function defined on interior points of a\nsubmanifold with boundary of the Euclidean space based on a (normally\ndistributed) random sample of function evaluations at points in the manifold.\nThis approach is based on the estimates of the Laplace-Beltrami operator\nproposed in the theory of Diffusion-Maps. Analytical convergence results of the\nresulting expansion are proved, and an efficient algorithm is proposed to deal\nwith non-convex optimization problems defined on Euclidean submanifolds. We\ntest and validate our methodology as a post-processing tool in Cryogenic\nelectron microscopy (Cryo-EM). We also apply the method to the classical sphere\npacking problem.",
          "link": "http://arxiv.org/abs/2108.06988",
          "publishedOn": "2021-08-17T01:54:47.466Z",
          "wordCount": 549,
          "title": "A diffusion-map-based algorithm for gradient computation on manifolds and applications. (arXiv:2108.06988v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vieillard_N/0/1/0/all/0/1\">Nino Vieillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrychowicz_M/0/1/0/all/0/1\">Marcin Andrychowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raichuk_A/0/1/0/all/0/1\">Anton Raichuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1\">Olivier Pietquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1\">Matthieu Geist</a>",
          "description": "The $Q$-function is a central quantity in many Reinforcement Learning (RL)\nalgorithms for which RL agents behave following a (soft)-greedy policy w.r.t.\nto $Q$. It is a powerful tool that allows action selection without a model of\nthe environment and even without explicitly modeling the policy. Yet, this\nscheme can only be used in discrete action tasks, with small numbers of\nactions, as the softmax cannot be computed exactly otherwise. Especially the\nusage of function approximation, to deal with continuous action spaces in\nmodern actor-critic architectures, intrinsically prevents the exact computation\nof a softmax. We propose to alleviate this issue by parametrizing the\n$Q$-function implicitly, as the sum of a log-policy and of a value function. We\nuse the resulting parametrization to derive a practical off-policy deep RL\nalgorithm, suitable for large action spaces, and that enforces the softmax\nrelation between the policy and the $Q$-value. We provide a theoretical\nanalysis of our algorithm: from an Approximate Dynamic Programming perspective,\nwe show its equivalence to a regularized version of value iteration, accounting\nfor both entropy and Kullback-Leibler regularization, and that enjoys\nbeneficial error propagation results. We then evaluate our algorithm on classic\ncontrol tasks, where its results compete with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.07041",
          "publishedOn": "2021-08-17T01:54:47.442Z",
          "wordCount": 628,
          "title": "Implicitly Regularized RL with Implicit Q-Values. (arXiv:2108.07041v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1\">Shubham Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pahwa_K/0/1/0/all/0/1\">Khushbu Pahwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>",
          "description": "Structure learning offers an expressive, versatile and explainable approach\nto causal and mechanistic modeling of complex biological data. We present\nwiseR, an open source application for learning, evaluating and deploying robust\ncausal graphical models using graph neural networks and Bayesian networks. We\ndemonstrate the utility of this application through application on for\nbiomarker discovery in a COVID-19 clinical dataset.",
          "link": "http://arxiv.org/abs/2108.07046",
          "publishedOn": "2021-08-17T01:54:47.436Z",
          "wordCount": 538,
          "title": "WiseR: An end-to-end structure learning and deployment framework for causal graphical models. (arXiv:2108.07046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.01777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dat Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Stephen S. Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>",
          "description": "We propose a novel interpretable deep neural network for text classification,\ncalled ProtoryNet, based on a new concept of prototype trajectories. Motivated\nby the prototype theory in modern linguistics, ProtoryNet makes a prediction by\nfinding the most similar prototype for each sentence in a text sequence and\nfeeding an RNN backbone with the proximity of each sentence to the\ncorresponding active prototype. The RNN backbone then captures the temporal\npattern of the prototypes, which we refer to as prototype trajectories.\nPrototype trajectories enable intuitive and fine-grained interpretation of the\nreasoning process of the RNN model, in resemblance to how humans analyze texts.\nWe also design a prototype pruning procedure to reduce the total number of\nprototypes used by the model for better interpretability. Experiments on\nmultiple public data sets show that ProtoryNet is more accurate than the\nbaseline prototype-based deep neural net and reduces the performance gap\ncompared to state-of-the-art black-box models. In addition, after prototype\npruning, the resulting ProtoryNet models only need less than or around 20\nprototypes for all datasets, which significantly benefits interpretability.\nFurthermore, we report a survey result indicating that human users find\nProtoryNet more intuitive and easier to understand than other prototype-based\nmethods.",
          "link": "http://arxiv.org/abs/2007.01777",
          "publishedOn": "2021-08-17T01:54:47.430Z",
          "wordCount": 658,
          "title": "Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jihoon Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuu_C/0/1/0/all/0/1\">Cheng-hsin Wuu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsuan-ru Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>",
          "description": "We contribute HAA500, a manually annotated human-centric atomic action\ndataset for action recognition on 500 classes with over 591K labeled frames. To\nminimize ambiguities in action classification, HAA500 consists of highly\ndiversified classes of fine-grained atomic actions, where only consistent\nactions fall under the same label, e.g., \"Baseball Pitching\" vs \"Free Throw in\nBasketball\". Thus HAA500 is different from existing atomic action datasets,\nwhere coarse-grained atomic actions were labeled with coarse action-verbs such\nas \"Throw\". HAA500 has been carefully curated to capture the precise movement\nof human figures with little class-irrelevant motions or spatio-temporal label\nnoises. The advantages of HAA500 are fourfold: 1) human-centric actions with a\nhigh average of 69.7% detectable joints for the relevant human poses; 2) high\nscalability since adding a new class can be done under 20-60 minutes; 3)\ncurated videos capturing essential elements of an atomic action without\nirrelevant frames; 4) fine-grained atomic action classes. Our extensive\nexperiments including cross-data validation using datasets collected in the\nwild demonstrate the clear benefits of human-centric and atomic characteristics\nof HAA500, which enable training even a baseline deep learning model to improve\nprediction by attending to atomic human poses. We detail the HAA500 dataset\nstatistics and collection methodology and compare quantitatively with existing\naction recognition datasets.",
          "link": "http://arxiv.org/abs/2009.05224",
          "publishedOn": "2021-08-17T01:54:47.423Z",
          "wordCount": 685,
          "title": "HAA500: Human-Centric Atomic Action Dataset with Curated Videos. (arXiv:2009.05224v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goswami_S/0/1/0/all/0/1\">Somdatta Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Minglang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Karniadakis</a>",
          "description": "Failure trajectories, identifying the probable failure zones, and damage\nstatistics are some of the key quantities of relevance in brittle fracture\napplications. High-fidelity numerical solvers that reliably estimate these\nrelevant quantities exist but they are computationally demanding requiring a\nhigh resolution of the crack. Moreover, independent intensive simulations need\nto be carried out even for a small change in domain parameters and/or material\nproperties. Therefore, fast and generalizable surrogate models are needed to\nalleviate the computational burden but the discontinuous nature of fracture\nmechanics presents a major challenge to developing such models. We propose a\nphysics-informed variational formulation of DeepONet (V-DeepONet) for brittle\nfracture analysis. V-DeepONet is trained to map the initial configuration of\nthe defect to the relevant fields of interests (e.g., damage and displacement\nfields). Once the network is trained, the entire global solution can be rapidly\nobtained for any initial crack configuration and loading steps on that domain.\nWhile the original DeepONet is solely data-driven, we take a different path to\ntrain the V-DeepONet by imposing the governing equations in variational form\nand we also use some labelled data. We demonstrate the effectiveness of\nV-DeepOnet through two benchmarks of brittle fracture, and we verify its\naccuracy using results from high-fidelity solvers. Encoding the physical laws\nand also some data to train the network renders the surrogate model capable of\naccurately performing both interpolation and extrapolation tasks, considering\nthat fracture modeling is very sensitive to fluctuations. The proposed hybrid\ntraining of V-DeepONet is superior to state-of-the-art methods and can be\napplied to a wide array of dynamical systems with complex responses.",
          "link": "http://arxiv.org/abs/2108.06905",
          "publishedOn": "2021-08-17T01:54:47.416Z",
          "wordCount": 706,
          "title": "A physics-informed variational DeepONet for predicting the crack path in brittle materials. (arXiv:2108.06905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.04720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wengong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swanson_K/0/1/0/all/0/1\">Kyle Swanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>",
          "description": "Generative models in molecular design tend to be richly parameterized,\ndata-hungry neural models, as they must create complex structured objects as\noutputs. Estimating such models from data may be challenging due to the lack of\nsufficient training data. In this paper, we propose a surprisingly effective\nself-training approach for iteratively creating additional molecular targets.\nWe first pre-train the generative model together with a simple property\npredictor. The property predictor is then used as a likelihood model for\nfiltering candidate structures from the generative model. Additional targets\nare iteratively produced and used in the course of stochastic EM iterations to\nmaximize the log-likelihood that the candidate structures are accepted. A\nsimple rejection (re-weighting) sampler suffices to draw posterior samples\nsince the generative model is already reasonable after pre-training. We\ndemonstrate significant gains over strong baselines for both unconditional and\nconditional molecular design. In particular, our approach outperforms the\nprevious state-of-the-art in conditional molecular design by over 10% in\nabsolute gain. Finally, we show that our approach is useful in other domains as\nwell, such as program synthesis.",
          "link": "http://arxiv.org/abs/2002.04720",
          "publishedOn": "2021-08-17T01:54:47.399Z",
          "wordCount": 666,
          "title": "Improving Molecular Design by Stochastic Iterative Target Augmentation. (arXiv:2002.04720v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cairoli_F/0/1/0/all/0/1\">Francesca Cairoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1\">Luca Bortolussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paoletti_N/0/1/0/all/0/1\">Nicola Paoletti</a>",
          "description": "We consider the problem of predictive monitoring (PM), i.e., predicting at\nruntime future violations of a system from the current state. We work under the\nmost realistic settings where only partial and noisy observations of the state\nare available at runtime. Such settings directly affect the accuracy and\nreliability of the reachability predictions, jeopardizing the safety of the\nsystem. In this work, we present a learning-based method for PM that produces\naccurate and reliable reachability predictions despite partial observability\n(PO). We build on Neural Predictive Monitoring (NPM), a PM method that uses\ndeep neural networks for approximating hybrid systems reachability, and extend\nit to the PO case. We propose and compare two solutions, an end-to-end\napproach, which directly operates on the rough observations, and a two-step\napproach, which introduces an intermediate state estimation step. Both\nsolutions rely on conformal prediction to provide 1) probabilistic guarantees\nin the form of prediction regions and 2) sound estimates of predictive\nuncertainty. We use the latter to identify unreliable (and likely erroneous)\npredictions and to retrain and improve the monitors on these uncertain inputs\n(i.e., active learning). Our method results in highly accurate reachability\npredictions and error detection, as well as tight prediction regions with\nguaranteed coverage.",
          "link": "http://arxiv.org/abs/2108.07134",
          "publishedOn": "2021-08-17T01:54:47.393Z",
          "wordCount": 627,
          "title": "Neural Predictive Monitoring under Partial Observabilit. (arXiv:2108.07134v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samiei_S/0/1/0/all/0/1\">Samaneh Samiei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_N/0/1/0/all/0/1\">Nasser Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansari_B/0/1/0/all/0/1\">Behnaz Ansari</a>",
          "description": "Electromyography (EMG) refers to a biomedical signal indicating neuromuscular\nactivity and muscle morphology. Experts accurately diagnose neuromuscular\ndisorders using this time series. Modern data analysis techniques have recently\nled to introducing novel approaches for mapping time series data to graphs and\ncomplex networks with applications in diverse fields, including medicine. The\nresulting networks develop a completely different visual acuity that can be\nused to complement physician findings of time series. This can lead to a more\nenriched analysis, reduced error, more accurate diagnosis of the disease, and\nincreased accuracy and speed of the treatment process. The mapping process may\ncause the loss of essential data from the time series and not retain all the\ntime series features. As a result, achieving an approach that can provide a\ngood representation of the time series while maintaining essential features is\ncrucial. This paper proposes a new approach to network development named\nGraphTS to overcome the limited accuracy of existing methods through EMG time\nseries using the visibility graph method. For this purpose, EMG signals are\npre-processed and mapped to a complex network by a standard visibility graph\nalgorithm. The resulting networks can differentiate between healthy and patient\nsamples. In the next step, the properties of the developed networks are given\nin the form of a feature matrix as input to classifiers after extracting\noptimal features. Performance evaluation of the proposed approach with deep\nneural network shows 99.30% accuracy for training data and 99.18% for test\ndata. Therefore, in addition to enriched network representation and covering\nthe features of time series for healthy, myopathy, and neuropathy EMG, the\nproposed technique improves accuracy, precision, recall, and F-score.",
          "link": "http://arxiv.org/abs/2108.06920",
          "publishedOn": "2021-08-17T01:54:47.386Z",
          "wordCount": 737,
          "title": "A complex network approach to time series analysis with application in diagnosis of neuromuscular disorders. (arXiv:2108.06920v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1\">Niamh Belton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1\">Aonghus Lawlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1\">Kathleen M. Curran</a>",
          "description": "Noisy data present in medical imaging datasets can often aid the development\nof robust models that are equipped to handle real-world data. However, if the\nbad data contains insufficient anatomical information, it can have a severe\nnegative effect on the model's performance. We propose a novel methodology\nusing a semi-supervised Siamese network to identify bad data. This method\nrequires only a small pool of 'reference' medical images to be reviewed by a\nnon-expert human to ensure the major anatomical structures are present in the\nField of View. The model trains on this reference set and identifies bad data\nby using the Siamese network to compute the distance between the reference set\nand all other medical images in the dataset. This methodology achieves an Area\nUnder the Curve (AUC) of 0.989 for identifying bad data. Code will be available\nat https://git.io/JYFuV.",
          "link": "http://arxiv.org/abs/2108.07130",
          "publishedOn": "2021-08-17T01:54:47.379Z",
          "wordCount": 594,
          "title": "Semi-Supervised Siamese Network for Identifying Bad Data in Medical Imaging Datasets. (arXiv:2108.07130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eikeland_O/0/1/0/all/0/1\">Odin Foldvik Eikeland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmstrand_I/0/1/0/all/0/1\">Inga Sets&#xe5; Holmstrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakkejord_S/0/1/0/all/0/1\">Sigurd Bakkejord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiesa_M/0/1/0/all/0/1\">Matteo Chiesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Filippo Maria Bianchi</a>",
          "description": "Unscheduled power disturbances cause severe consequences both for customers\nand grid operators. To defend against such events, it is necessary to identify\nthe causes of interruptions in the power distribution network. In this work, we\nfocus on the power grid of a Norwegian community in the Arctic that experiences\nseveral faults whose sources are unknown. First, we construct a data set\nconsisting of relevant meteorological data and information about the current\npower quality logged by power-quality meters. Then, we adopt machine-learning\ntechniques to predict the occurrence of faults. Experimental results show that\nboth linear and non-linear classifiers achieve good classification performance.\nThis indicates that the considered power-quality and weather variables explain\nwell the power disturbances. Interpreting the decision process of the\nclassifiers provides valuable insights to understand the main causes of\ndisturbances. Traditional features selection methods can only indicate which\nare the variables that, on average, mostly explain the fault occurrences in the\ndataset. Besides providing such a global interpretation, it is also important\nto identify the specific set of variables that explain each individual fault.\nTo address this challenge, we adopt a recent technique to interpret the\ndecision process of a deep learning model, called Integrated Gradients. The\nproposed approach allows to gain detailed insights on the occurrence of a\nspecific fault, which are valuable for the distribution system operators to\nimplement strategies to prevent and mitigate power disturbances.",
          "link": "http://arxiv.org/abs/2108.07060",
          "publishedOn": "2021-08-17T01:54:47.373Z",
          "wordCount": 672,
          "title": "Detecting and interpreting faults in vulnerable power grids with machine learning. (arXiv:2108.07060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_Y/0/1/0/all/0/1\">Yuya Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_K/0/1/0/all/0/1\">Kei Harada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamasaki_S/0/1/0/all/0/1\">Shohei Yamasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onizuka_M/0/1/0/all/0/1\">Makoto Onizuka</a>",
          "description": "Urban air pollution is a major environmental problem affecting human health\nand quality of life. Monitoring stations have been established to continuously\nobtain air quality information, but they do not cover all areas. Thus, there\nare numerous methods for spatially fine-grained air quality inference. Since\nexisting methods aim to infer air quality of locations only in monitored\ncities, they do not assume inferring air quality in unmonitored cities. In this\npaper, we first study the air quality inference in unmonitored cities. To\naccurately infer air quality in unmonitored cities, we propose a neural\nnetwork-based approach AIREX. The novelty of AIREX is employing a\nmixture-of-experts approach, which is a machine learning technique based on the\ndivide-and-conquer principle, to learn correlations of air quality between\nmultiple cities. To further boost the performance, it employs attention\nmechanisms to compute impacts of air quality inference from the monitored\ncities to the locations in the unmonitored city. We show, through experiments\non a real-world air quality dataset, that AIREX achieves higher accuracy than\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.07120",
          "publishedOn": "2021-08-17T01:54:47.354Z",
          "wordCount": 609,
          "title": "AIREX: Neural Network-based Approach for Air Quality Inference in Unmonitored Cities. (arXiv:2108.07120v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1\">Taeyeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyenson_B/0/1/0/all/0/1\">Benjamin Pyenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebig_J/0/1/0/all/0/1\">Juergen Liebig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>",
          "description": "Biology is both an important application area and a source of motivation for\ndevelopment of advanced machine learning techniques. Although much attention\nhas been paid to large and complex data sets resulting from high-throughput\nsequencing, advances in high-quality video recording technology have begun to\ngenerate similarly rich data sets requiring sophisticated techniques from both\ncomputer vision and time-series analysis. Moreover, just as studying gene\nexpression patterns in one organism can reveal general principles that apply to\nother organisms, the study of complex social interactions in an experimentally\ntractable model system, such as a laboratory ant colony, can provide general\nprinciples about the dynamics of other social groups. Here, we focus on one\nsuch example from the study of reproductive regulation in small laboratory\ncolonies of more than 50 Harpegnathos ants. These ants can be artificially\ninduced to begin a ~20 day process of hierarchy reformation. Although the\nconclusion of this process is conspicuous to a human observer, it remains\nunclear which behaviors during the transient period are contributing to the\nprocess. To address this issue, we explore the potential application of\nOne-class Classification (OC) to the detection of abnormal states in ant\ncolonies for which behavioral data is only available for the normal societal\nconditions during training. Specifically, we build upon the Deep Support Vector\nData Description (DSVDD) and introduce the Inner-Outlier Generator (IO-GEN)\nthat synthesizes fake \"inner outlier\" observations during training that are\nnear the center of the DSVDD data description. We show that IO-GEN increases\nthe reliability of the final OC classifier relative to other DSVDD baselines.\nThis method can be used to screen video frames for which additional human\nobservation is needed.",
          "link": "http://arxiv.org/abs/2009.08626",
          "publishedOn": "2021-08-17T01:54:47.345Z",
          "wordCount": 801,
          "title": "Identification of Abnormal States in Videos of Ants Undergoing Social Phase Change. (arXiv:2009.08626v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.14220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mostaani_A/0/1/0/all/0/1\">Arsham Mostaani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thang X. Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzinotas_S/0/1/0/all/0/1\">Symeon Chatzinotas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottersten_B/0/1/0/all/0/1\">Bj&#xf6;rn Ottersten</a>",
          "description": "A collaborative task is assigned to a multiagent system (MAS) in which agents\nare allowed to communicate. The MAS runs over an underlying Markov decision\nprocess and its task is to maximize the averaged sum of discounted one-stage\nrewards. Although knowing the global state of the environment is necessary for\nthe optimal action selection of the MAS, agents are limited to individual\nobservations. Inter-agent communication can tackle the issue of local\nobservability, however, the limited rate of inter-agent communication prevents\nthe agents from acquiring the precise global state information. To overcome\nthis challenge, agents need to communicate an abstract version of their\nobservations to each other such that the MAS compromises the minimum possible\nsum of rewards. We show that this problem is equivalent to a form of\nrate-distortion problem, which we call task-based information compression\n(TBIC). We introduce state aggregation for information compression (SAIC) to\nsolve the TBIC problem. SAIC is shown to achieve near-optimal performance in\nterms of the achieved sum of discounted rewards. The proposed algorithm is\napplied to a rendezvous problem and its performance is compared with several\nbenchmarks. Numerical experiments confirm the superiority of the proposed\nalgorithm.",
          "link": "http://arxiv.org/abs/2005.14220",
          "publishedOn": "2021-08-17T01:54:47.328Z",
          "wordCount": 691,
          "title": "Task-Based Information Compression for Multi-Agent Communication Problems with Channel Rate Constraints. (arXiv:2005.14220v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1801.04062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belghazi_M/0/1/0/all/0/1\">Mohamed Ishmael Belghazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1\">Aristide Baratin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1\">Sai Rajeswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozair_S/0/1/0/all/0/1\">Sherjil Ozair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1\">R Devon Hjelm</a>",
          "description": "We argue that the estimation of mutual information between high dimensional\ncontinuous random variables can be achieved by gradient descent over neural\nnetworks. We present a Mutual Information Neural Estimator (MINE) that is\nlinearly scalable in dimensionality as well as in sample size, trainable\nthrough back-prop, and strongly consistent. We present a handful of\napplications on which MINE can be used to minimize or maximize mutual\ninformation. We apply MINE to improve adversarially trained generative models.\nWe also use MINE to implement Information Bottleneck, applying it to supervised\nclassification; our results demonstrate substantial improvement in flexibility\nand performance in these settings.",
          "link": "http://arxiv.org/abs/1801.04062",
          "publishedOn": "2021-08-17T01:54:47.320Z",
          "wordCount": 607,
          "title": "MINE: Mutual Information Neural Estimation. (arXiv:1801.04062v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "With the tremendous advances in the architecture and scale of convolutional\nneural networks (CNNs) over the past few decades, they can easily reach or even\nexceed the performance of humans in certain tasks. However, a recently\ndiscovered shortcoming of CNNs is that they are vulnerable to adversarial\nattacks. Although the adversarial robustness of CNNs can be improved by\nadversarial training, there is a trade-off between standard accuracy and\nadversarial robustness. From the neural architecture perspective, this paper\naims to improve the adversarial robustness of the backbone CNNs that have a\nsatisfactory accuracy. Under a minimal computational overhead, the introduction\nof a dilation architecture is expected to be friendly with the standard\nperformance of the backbone CNN while pursuing adversarial robustness.\nTheoretical analyses on the standard and adversarial error bounds naturally\nmotivate the proposed neural architecture dilation algorithm. Experimental\nresults on real-world datasets and benchmark neural networks demonstrate the\neffectiveness of the proposed algorithm to balance the accuracy and adversarial\nrobustness.",
          "link": "http://arxiv.org/abs/2108.06885",
          "publishedOn": "2021-08-17T01:54:47.312Z",
          "wordCount": 614,
          "title": "Neural Architecture Dilation for Adversarial Robustness. (arXiv:2108.06885v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07028",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rahmani_M/0/1/0/all/0/1\">Mostafa Rahmani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shafipour_R/0/1/0/all/0/1\">Rasoul Shafipour</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "In contrast to image/text data whose order can be used to perform non-local\nfeature aggregation in a straightforward way using the pooling layers, graphs\nlack the tensor representation and mostly the element-wise max/mean function is\nutilized to aggregate the locally extracted feature vectors. In this paper, we\npresent a novel approach for global feature aggregation in Graph Neural\nNetworks (GNNs) which utilizes a Latent Fixed Data Structure (LFDS) to\naggregate the extracted feature vectors. The locally extracted feature vectors\nare sorted/distributed on the LFDS and a latent neural network (CNN/GNN) is\nutilized to perform feature aggregation on the LFDS. The proposed approach is\nused to design several novel global feature aggregation methods based on the\nchoice of the LFDS. We introduce multiple LFDSs including loop, 3D tensor\n(image), sequence, data driven graphs and an algorithm which sorts/distributes\nthe extracted local feature vectors on the LFDS. While the computational\ncomplexity of the proposed methods are linear with the order of input graphs,\nthey achieve competitive or better results.",
          "link": "http://arxiv.org/abs/2108.07028",
          "publishedOn": "2021-08-17T01:54:47.295Z",
          "wordCount": 615,
          "title": "Non-Local Feature Aggregation on Graphs via Latent Fixed Data Structures. (arXiv:2108.07028v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Freitas_S/0/1/0/all/0/1\">Scott Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Srijan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>",
          "description": "Network robustness plays a crucial role in our understanding of complex\ninterconnected systems such as transportation, communication, and computer\nnetworks. While significant research has been conducted in the area of network\nrobustness, no comprehensive open-source toolbox currently exists to assist\nresearchers and practitioners in this important topic. This lack of available\ntools hinders reproducibility and examination of existing work, development of\nnew research, and dissemination of new ideas. We contribute TIGER, an\nopen-sourced Python toolbox to address these challenges. TIGER contains 22\ngraph robustness measures with both original and fast approximate versions; 17\nfailure and attack strategies; 15 heuristic and optimization-based defense\ntechniques; and 4 simulation tools. By democratizing the tools required to\nstudy network robustness, our goal is to assist researchers and practitioners\nin analyzing their own networks; and facilitate the development of new research\nin the field. TIGER has been integrated into the Nvidia Data Science Teaching\nKit available to educators across the world; and Georgia Tech's Data and Visual\nAnalytics class with over 1,000 students. TIGER is open sourced at:\nhttps://github.com/safreita1/TIGER",
          "link": "http://arxiv.org/abs/2006.05648",
          "publishedOn": "2021-08-17T01:54:47.289Z",
          "wordCount": 655,
          "title": "Evaluating Graph Vulnerability and Robustness using TIGER. (arXiv:2006.05648v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellani_A/0/1/0/all/0/1\">Andrea Castellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1\">Sebastian Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Detecting drifts in data is essential for machine learning applications, as\nchanges in the statistics of processed data typically has a profound influence\non the performance of trained models. Most of the available drift detection\nmethods require access to true labels during inference time. In a real-world\nscenario, true labels usually available only during model training. In this\nwork, we propose a novel task-sensitive drift detection framework, which is\nable to detect drifts without access to true labels during inference. It\nutilizes metric learning of a constrained low-dimensional embedding\nrepresentation of the input data, which is best suited for the classification\ntask. It is able to detect real drift, where the drift affects the\nclassification performance, while it properly ignores virtual drift, where the\nclassification performance is not affected by the drift. In the proposed\nframework, the actual method to detect a change in the statistics of incoming\ndata samples can be chosen freely. We also propose the two change detection\nmethods, which are based on the exponential moving average and a modified\n$z$-score, respectively. We evaluate the performance of the proposed framework\nwith a novel metric, which accumulates the standard metrics of detection\naccuracy, false positive rate and detection delay into one value. Experimental\nevaluation on nine benchmarks datasets, with different types of drift,\ndemonstrates that the proposed framework can reliably detect drifts, and\noutperforms state-of-the-art unsupervised drift detection approaches.",
          "link": "http://arxiv.org/abs/2108.06980",
          "publishedOn": "2021-08-17T01:54:47.282Z",
          "wordCount": 673,
          "title": "Task-Sensitive Concept Drift Detector with Metric Learning. (arXiv:2108.06980v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.09843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Weijun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaohu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Rongxing Lu</a>",
          "description": "Although federated learning improves privacy of training data by exchanging\nlocal gradients or parameters rather than raw data, the adversary still can\nleverage local gradients and parameters to obtain local training data by\nlaunching reconstruction and membership inference attacks. To defend such\nprivacy attacks, many noises perturbation methods (like differential privacy or\nCountSketch matrix) have been widely designed. However, the strong defence\nability and high learning accuracy of these schemes cannot be ensured at the\nsame time, which will impede the wide application of FL in practice (especially\nfor medical or financial institutions that require both high accuracy and\nstrong privacy guarantee). To overcome this issue, in this paper, we propose\n\\emph{an efficient model perturbation method for federated learning} to defend\nreconstruction and membership inference attacks launched by curious clients. On\nthe one hand, similar to the differential privacy, our method also selects\nrandom numbers as perturbed noises added to the global model parameters, and\nthus it is very efficient and easy to be integrated in practice. Meanwhile, the\nrandom selected noises are positive real numbers and the corresponding value\ncan be arbitrarily large, and thus the strong defence ability can be ensured.\nOn the other hand, unlike differential privacy or other perturbation methods\nthat cannot eliminate the added noises, our method allows the server to recover\nthe true gradients by eliminating the added noises. Therefore, our method does\nnot hinder learning accuracy at all.",
          "link": "http://arxiv.org/abs/2002.09843",
          "publishedOn": "2021-08-17T01:54:47.276Z",
          "wordCount": 750,
          "title": "An Accuracy-Lossless Perturbation Method for Defending Privacy Attacks in Federated Learning. (arXiv:2002.09843v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cengguang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_D/0/1/0/all/0/1\">Di Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>",
          "description": "Vertical federated learning (VFL) leverages various privacy-preserving\nalgorithms, e.g., homomorphic encryption or secret sharing based SecureBoost,\nto ensure data privacy. However, these algorithms all require a semi-honest\nsecure definition, which raises concerns in real-world applications. In this\npaper, we present Aegis, a trusted, automatic, and accurate verification\nframework to verify the security of VFL jobs. Aegis is separated from local\nparties to ensure the security of the framework. Furthermore, it automatically\nadapts to evolving VFL algorithms by defining the VFL job as a finite state\nmachine to uniformly verify different algorithms and reproduce the entire job\nto provide more accurate verification. We implement and evaluate Aegis with\ndifferent threat models on financial and medical datasets. Evaluation results\nshow that: 1) Aegis can detect 95% threat models, and 2) it provides\nfine-grained verification results within 84% of the total VFL job time.",
          "link": "http://arxiv.org/abs/2108.06958",
          "publishedOn": "2021-08-17T01:54:47.269Z",
          "wordCount": 616,
          "title": "Aegis: A Trusted, Automatic and Accurate Verification Framework for Vertical Federated Learning. (arXiv:2108.06958v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06430",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bradford_E/0/1/0/all/0/1\">E. Bradford</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Imsland_L/0/1/0/all/0/1\">L. Imsland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reble_M/0/1/0/all/0/1\">M. Reble</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rio_Chanona_E/0/1/0/all/0/1\">E.A. del Rio-Chanona</a>",
          "description": "Nonlinear model predictive control (NMPC) is an efficient approach for the\ncontrol of nonlinear multivariable dynamic systems with constraints, which\nhowever requires an accurate plant model. Plant models can often be determined\nfrom first principles, parts of the model are however difficult to derive using\nphysical laws alone. In this paper a hybrid Gaussian process (GP) first\nprinciples modeling scheme is proposed to overcome this issue, which exploits\nGPs to model the parts of the dynamic system that are difficult to describe\nusing first principles. GPs not only give accurate predictions, but also\nquantify the residual uncertainty of this model. It is vital to account for\nthis uncertainty in the control algorithm, to prevent constraint violations and\nperformance deterioration. Monte Carlo samples of the GPs are generated offline\nto tighten constraints of the NMPC to ensure joint probabilistic constraint\nsatisfaction online. Advantages of our method include fast online evaluation\ntimes, possibility to account for online learning alleviating conservativeness,\nand exploiting the flexibility of GPs and the data efficiency of first\nprinciple models. The algorithm is verified on a case study involving a\nchallenging semi-batch bioreactor.",
          "link": "http://arxiv.org/abs/2108.06430",
          "publishedOn": "2021-08-17T01:54:47.251Z",
          "wordCount": 673,
          "title": "Hybrid Gaussian Process Modeling Applied to Economic Stochastic Model Predictive Control of Batch Processes. (arXiv:2108.06430v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>",
          "description": "Federated learning (FL) emerged as a promising learning paradigm to enable a\nmultitude of participants to construct a joint ML model without exposing their\nprivate training data. Existing FL designs have been shown to exhibit\nvulnerabilities which can be exploited by adversaries both within and outside\nof the system to compromise data privacy. However, most current works conduct\nattacks by leveraging gradients on a small batch of data, which is less\npractical in FL. In this work, we consider a more practical and interesting\nscenario in which participants share their epoch-averaged gradients (share\ngradients after at least 1 epoch of local training) rather than per-example or\nsmall batch-averaged gradients as in previous works. We perform the first\nsystematic evaluation of attribute reconstruction attack (ARA) launched by the\nmalicious server in the FL system, and empirically demonstrate that the shared\nepoch-averaged local model gradients can reveal sensitive attributes of local\ntraining data of any victim participant. To achieve this goal, we develop a\nmore effective and efficient gradient matching based method called cos-matching\nto reconstruct the training data attributes. We evaluate our attacks on a\nvariety of real-world datasets, scenarios, assumptions. Our experiments show\nthat our proposed method achieves better attribute attack performance than most\nexisting baselines.",
          "link": "http://arxiv.org/abs/2108.06910",
          "publishedOn": "2021-08-17T01:54:47.245Z",
          "wordCount": 643,
          "title": "A Novel Attribute Reconstruction Attack in Federated Learning. (arXiv:2108.06910v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changliu Liu</a>",
          "description": "Neural networks (NNs) are widely used for classification tasks for their\nremarkable performance. However, the robustness and accuracy of NNs heavily\ndepend on the training data. In many applications, massive training data is\nusually not available. To address the challenge, this paper proposes an\niterative adversarial data augmentation (IADA) framework to learn neural\nnetwork models from an insufficient amount of training data. The method uses\nformal verification to identify the most \"confusing\" input samples, and\nleverages human guidance to safely and iteratively augment the training data\nwith these samples. The proposed framework is applied to an artificial 2D\ndataset, the MNIST dataset, and a human motion dataset. By applying IADA to\nfully-connected NN classifiers, we show that our training method can improve\nthe robustness and accuracy of the learned model. By comparing to regular\nsupervised training, on the MNIST dataset, the average perturbation bound\nimproved 107.4%. The classification accuracy improved 1.77%, 3.76%, 10.85% on\nthe 2D dataset, the MNIST dataset, and the human motion dataset respectively.",
          "link": "http://arxiv.org/abs/2108.06871",
          "publishedOn": "2021-08-17T01:54:47.239Z",
          "wordCount": 619,
          "title": "IADA: Iterative Adversarial Data Augmentation Using Formal Verification and Expert Guidance. (arXiv:2108.06871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1\">Constantinos Daskalakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishelson_M/0/1/0/all/0/1\">Maxwell Fishelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1\">Noah Golowich</a>",
          "description": "We show that Optimistic Hedge -- a common variant of\nmultiplicative-weights-updates with recency bias -- attains ${\\rm poly}(\\log\nT)$ regret in multi-player general-sum games. In particular, when every player\nof the game uses Optimistic Hedge to iteratively update her strategy in\nresponse to the history of play so far, then after $T$ rounds of interaction,\neach player experiences total regret that is ${\\rm poly}(\\log T)$. Our bound\nimproves, exponentially, the $O({T}^{1/2})$ regret attainable by standard\nno-regret learners in games, the $O(T^{1/4})$ regret attainable by no-regret\nlearners with recency bias (Syrgkanis et al., 2015), and the ${O}(T^{1/6})$\nbound that was recently shown for Optimistic Hedge in the special case of\ntwo-player games (Chen & Pen, 2020). A corollary of our bound is that\nOptimistic Hedge converges to coarse correlated equilibrium in general games at\na rate of $\\tilde{O}\\left(\\frac 1T\\right)$.",
          "link": "http://arxiv.org/abs/2108.06924",
          "publishedOn": "2021-08-17T01:54:47.233Z",
          "wordCount": 565,
          "title": "Near-Optimal No-Regret Learning in General Games. (arXiv:2108.06924v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "We introduce a novel dataset for architectural style classification,\nconsisting of 9,485 images of church buildings. Both images and style labels\nwere sourced from Wikipedia. The dataset can serve as a benchmark for various\nresearch fields, as it combines numerous real-world challenges: fine-grained\ndistinctions between classes based on subtle visual features, a comparatively\nsmall sample size, a highly imbalanced class distribution, a high variance of\nviewpoints, and a hierarchical organization of labels, where only some images\nare labeled at the most precise level. In addition, we provide 631 bounding box\nannotations of characteristic visual features for 139 churches from four major\ncategories. These annotations can, for example, be useful for research on\nfine-grained classification, where additional expert knowledge about\ndistinctive object parts is often available. Images and annotations are\navailable at: https://doi.org/10.5281/zenodo.5166987",
          "link": "http://arxiv.org/abs/2108.06959",
          "publishedOn": "2021-08-17T01:54:47.227Z",
          "wordCount": 580,
          "title": "WikiChurches: A Fine-Grained Dataset of Architectural Styles with Real-World Challenges. (arXiv:2108.06959v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06717",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Oh_Y/0/1/0/all/0/1\">YongKyung Oh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kwak_J/0/1/0/all/0/1\">JiIn Kwak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1\">JuYoung Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kim_S/0/1/0/all/0/1\">Sungil Kim</a>",
          "description": "Considering how congestion will propagate in the near future, understanding\ntraffic congestion propagation has become crucial in GPS navigation systems for\nproviding users with a more accurate estimated time of arrival (ETA). However,\nproviding the exact ETA during congestion is a challenge owing to the complex\npropagation process between roads and high uncertainty regarding the future\nbehavior of the process. Recent studies have focused on finding frequent\ncongestion propagation patterns and determining the propagation probabilities.\nBy contrast, this study proposes a novel time delay estimation method for\ntraffic congestion propagation between roads using lag-specific transfer\nentropy (TE). Nonlinear normalization with a sliding window is used to\neffectively reveal the causal relationship between the source and target time\nseries in calculating the TE. Moreover, Markov bootstrap techniques were\nadopted to quantify the uncertainty in the time delay estimator. To the best of\nour knowledge, the time delay estimation method presented in this article is\nthe first to determine the time delay between roads for any congestion\npropagation pattern. The proposed method was validated using simulated data as\nwell as real user trajectory data obtained from a major GPS navigation system\napplied in South Korea.",
          "link": "http://arxiv.org/abs/2108.06717",
          "publishedOn": "2021-08-17T01:54:47.209Z",
          "wordCount": 638,
          "title": "Time Delay Estimation of Traffic Congestion Propagation based on Transfer Entropy. (arXiv:2108.06717v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuyun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yufei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "This paper aims to explain adversarial attacks in terms of how adversarial\nperturbations contribute to the attacking task. We estimate attributions of\ndifferent image regions to the decrease of the attacking cost based on the\nShapley value. We define and quantify interactions among adversarial\nperturbation pixels, and decompose the entire perturbation map into relatively\nindependent perturbation components. The decomposition of the perturbation map\nshows that adversarially-trained DNNs have more perturbation components in the\nforeground than normally-trained DNNs. Moreover, compared to the\nnormally-trained DNN, the adversarially-trained DNN have more components which\nmainly decrease the score of the true category. Above analyses provide new\ninsights into the understanding of adversarial attacks.",
          "link": "http://arxiv.org/abs/2108.06895",
          "publishedOn": "2021-08-17T01:54:47.203Z",
          "wordCount": 555,
          "title": "Interpreting Attributions and Interactions of Adversarial Attacks. (arXiv:2108.06895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karpov_N/0/1/0/all/0/1\">Nikolai Karpov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>",
          "description": "We study Thompson Sampling algorithms for stochastic multi-armed bandits in\nthe batched setting, in which we want to minimize the regret over a sequence of\narm pulls using a small number of policy changes (or, batches). We propose two\nalgorithms and demonstrate their effectiveness by experiments on both synthetic\nand real datasets. We also analyze the proposed algorithms from the theoretical\naspect and obtain almost tight regret-batches tradeoffs for the two-arm case.",
          "link": "http://arxiv.org/abs/2108.06812",
          "publishedOn": "2021-08-17T01:54:47.176Z",
          "wordCount": 496,
          "title": "Batched Thompson Sampling for Multi-Armed Bandits. (arXiv:2108.06812v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_C/0/1/0/all/0/1\">Chayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noman_N/0/1/0/all/0/1\">Nasimul Noman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_M/0/1/0/all/0/1\">Mohsen Zamani</a>",
          "description": "Actor-critic (AC) algorithms are known for their efficacy and high\nperformance in solving reinforcement learning problems, but they also suffer\nfrom low sampling efficiency. An AC based policy optimization process is\niterative and needs to frequently access the agent-environment system to\nevaluate and update the policy by rolling out the policy, collecting rewards\nand states (i.e. samples), and learning from them. It ultimately requires a\nhuge number of samples to learn an optimal policy. To improve sampling\nefficiency, we propose a strategy to optimize the training dataset that\ncontains significantly less samples collected from the AC process. The dataset\noptimization is made of a best episode only operation, a policy\nparameter-fitness model, and a genetic algorithm module. The optimal policy\nnetwork trained by the optimized training dataset exhibits superior performance\ncompared to many contemporary AC algorithms in controlling autonomous dynamical\nsystems. Evaluation on standard benchmarks show that the method improves\nsampling efficiency, ensures faster convergence to optima, and is more\ndata-efficient than its counterparts.",
          "link": "http://arxiv.org/abs/2108.06911",
          "publishedOn": "2021-08-17T01:54:47.138Z",
          "wordCount": 599,
          "title": "Optimal Actor-Critic Policy with Optimized Training Datasets. (arXiv:2108.06911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zeyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_P/0/1/0/all/0/1\">Pierce KH Chow</a>",
          "description": "Accurate automatic liver and tumor segmentation plays a vital role in\ntreatment planning and disease monitoring. Recently, deep convolutional neural\nnetwork (DCNNs) has obtained tremendous success in 2D and 3D medical image\nsegmentation. However, 2D DCNNs cannot fully leverage the inter-slice\ninformation, while 3D DCNNs are computationally expensive and memory intensive.\nTo address these issues, we first propose a novel dense-sparse training flow\nfrom a data perspective, in which, densely adjacent slices and sparsely\nadjacent slices are extracted as inputs for regularizing DCNNs, thereby\nimproving the model performance. Moreover, we design a 2.5D light-weight\nnnU-Net from a network perspective, in which, depthwise separable convolutions\nare adopted to improve the efficiency. Extensive experiments on the LiTS\ndataset have demonstrated the superiority of the proposed method.",
          "link": "http://arxiv.org/abs/2108.06761",
          "publishedOn": "2021-08-17T01:54:47.128Z",
          "wordCount": 599,
          "title": "Multi-Slice Dense-Sparse Learning for Efficient Liver and Tumor Segmentation. (arXiv:2108.06761v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barba_O/0/1/0/all/0/1\">Ocean M. Barba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calbay_F/0/1/0/all/0/1\">Franz Arvin T. Calbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francisco_A/0/1/0/all/0/1\">Angelica Jane S. Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1\">Angel Luis D. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponay_C/0/1/0/all/0/1\">Charmaine S. Ponay</a>",
          "description": "Social media has played a huge part on how people get informed and\ncommunicate with one another. It has helped people express their needs due to\ndistress especially during disasters. Because posts made through it are\npublicly accessible by default, Twitter is among the most helpful social media\nsites in times of disaster. With this, the study aims to assess the needs\nexpressed during calamities by Filipinos on Twitter. Data were gathered and\nclassified as either disaster-related or unrelated with the use of Na\\\"ive\nBayes classifier. After this, the disaster-related tweets were clustered per\ndisaster type using Incremental Clustering Algorithm, and then sub-clustered\nbased on the location and time of the tweet using Density-based Spatiotemporal\nClustering Algorithm. Lastly, using Support Vector Machines, the tweets were\nclassified according to the expressed need, such as shelter, rescue, relief,\ncash, prayer, and others. After conducting the study, results showed that the\nIncremental Clustering Algorithm and Density-Based Spatiotemporal Clustering\nAlgorithm were able to cluster the tweets with f-measure scores of 47.20% and\n82.28% respectively. Also, the Na\\\"ive Bayes and Support Vector Machines were\nable to classify with an average f-measure score of 97% and an average accuracy\nof 77.57% respectively.",
          "link": "http://arxiv.org/abs/2108.06853",
          "publishedOn": "2021-08-17T01:54:47.103Z",
          "wordCount": 661,
          "title": "Clustering Filipino Disaster-Related Tweets Using Incremental and Density-Based Spatiotemporal Algorithm with Support Vector Machines for Needs Assessment 2. (arXiv:2108.06853v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saini_A/0/1/0/all/0/1\">Aditya Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1\">Ranjitha Prasad</a>",
          "description": "Owing to tremendous performance improvements in data-intensive domains,\nmachine learning (ML) has garnered immense interest in the research community.\nHowever, these ML models turn out to be black boxes, which are tough to\ninterpret, resulting in a direct decrease in productivity. Local Interpretable\nModel-Agnostic Explanations (LIME) is a popular technique for explaining the\nprediction of a single instance. Although LIME is simple and versatile, it\nsuffers from instability in the generated explanations. In this paper, we\npropose a Gaussian Process (GP) based variation of locally interpretable\nmodels. We employ a smart sampling strategy based on the acquisition functions\nin Bayesian optimization. Further, we employ the automatic relevance\ndetermination based covariance function in GP, with separate length-scale\nparameters for each feature, where the reciprocal of lengthscale parameters\nserve as feature explanations. We illustrate the performance of the proposed\ntechnique on two real-world datasets, and demonstrate the superior stability of\nthe proposed technique. Furthermore, we demonstrate that the proposed technique\nis able to generate faithful explanations using much fewer samples as compared\nto LIME.",
          "link": "http://arxiv.org/abs/2108.06907",
          "publishedOn": "2021-08-17T01:54:47.094Z",
          "wordCount": 598,
          "title": "Locally Interpretable Model Agnostic Explanations using Gaussian Processes. (arXiv:2108.06907v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lima_M/0/1/0/all/0/1\">Markus V. S. Lima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaves_G/0/1/0/all/0/1\">Gabriel S. Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_T/0/1/0/all/0/1\">Tadeu N. Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diniz_P/0/1/0/all/0/1\">Paulo S. R. Diniz</a>",
          "description": "Adaptive filters exploiting sparsity have been a very active research field,\namong which the algorithms that follow the \"proportional-update principle\", the\nso-called proportionate-type algorithms, are very popular. Indeed, there are\nhundreds of works on proportionate-type algorithms and, therefore, their\nadvantages are widely known. This paper addresses the unexplored drawbacks and\nlimitations of using proportional updates and their practical impacts. Our\nfindings include the theoretical justification for the poor performance of\nthese algorithms in several sparse scenarios, and also when dealing with\nnon-stationary and compressible systems. Simulation results corroborating the\ntheory are presented.",
          "link": "http://arxiv.org/abs/2108.06846",
          "publishedOn": "2021-08-17T01:54:47.087Z",
          "wordCount": 529,
          "title": "Do Proportionate Algorithms Exploit Sparsity?. (arXiv:2108.06846v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>",
          "description": "In recent years, Neural Machine Translator (NMT) has shown promise in\nautomatically editing source code. Typical NMT based code editor only considers\nthe code that needs to be changed as input and suggests developers with a\nranked list of patched code to choose from - where the correct one may not\nalways be at the top of the list. While NMT based code editing systems generate\na broad spectrum of plausible patches, the correct one depends on the\ndevelopers' requirement and often on the context where the patch is applied.\nThus, if developers provide some hints, using natural language, or providing\npatch context, NMT models can benefit from them. As a proof of concept, in this\nresearch, we leverage three modalities of information: edit location, edit code\ncontext, commit messages (as a proxy of developers' hint in natural language)\nto automatically generate edits with NMT models. To that end, we build MODIT, a\nmulti-modal NMT based code editing engine. With in-depth investigation and\nanalysis, we show that developers' hint as an input modality can narrow the\nsearch space for patches and outperform state-of-the-art models to generate\ncorrectly patched code in top-1 position.",
          "link": "http://arxiv.org/abs/2108.06645",
          "publishedOn": "2021-08-17T01:54:47.080Z",
          "wordCount": 636,
          "title": "On Multi-Modal Learning of Editing Source Code. (arXiv:2108.06645v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagus_B/0/1/0/all/0/1\">Benedikt Bagus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gepperth_A/0/1/0/all/0/1\">Alexander Gepperth</a>",
          "description": "Continual learning (CL) is a major challenge of machine learning (ML) and\ndescribes the ability to learn several tasks sequentially without catastrophic\nforgetting (CF). Recent works indicate that CL is a complex topic, even more so\nwhen real-world scenarios with multiple constraints are involved. Several\nsolution classes have been proposed, of which so-called replay-based approaches\nseem very promising due to their simplicity and robustness. Such approaches\nstore a subset of past samples in a dedicated memory for later processing:\nwhile this does not solve all problems, good results have been obtained. In\nthis article, we empirically investigate replay-based approaches of continual\nlearning and assess their potential for applications. Selected recent\napproaches as well as own proposals are compared on a common set of benchmarks,\nwith a particular focus on assessing the performance of different sample\nselection strategies. We find that the impact of sample selection increases\nwhen a smaller number of samples is stored. Nevertheless, performance varies\nstrongly between different replay approaches. Surprisingly, we find that the\nmost naive rehearsal-based approaches that we propose here can outperform\nrecent state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.06758",
          "publishedOn": "2021-08-17T01:54:47.073Z",
          "wordCount": 615,
          "title": "An Investigation of Replay-based Approaches for Continual Learning. (arXiv:2108.06758v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_J/0/1/0/all/0/1\">Jessica Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narisetty_C/0/1/0/all/0/1\">Chaitanya Narisetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>",
          "description": "We motivate and propose a suite of simple but effective improvements for\nconcept-to-text generation called SAPPHIRE: Set Augmentation and Post-hoc\nPHrase Infilling and REcombination. We demonstrate their effectiveness on\ngenerative commonsense reasoning, a.k.a. the CommonGen task, through\nexperiments using both BART and T5 models. Through extensive automatic and\nhuman evaluation, we show that SAPPHIRE noticeably improves model performance.\nAn in-depth qualitative analysis illustrates that SAPPHIRE effectively\naddresses many issues of the baseline model generations, including lack of\ncommonsense, insufficient specificity, and poor fluency.",
          "link": "http://arxiv.org/abs/2108.06643",
          "publishedOn": "2021-08-17T01:54:47.068Z",
          "wordCount": 532,
          "title": "SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation. (arXiv:2108.06643v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cahall_D/0/1/0/all/0/1\">Daniel E. Cahall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal C. Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathallah_Shaykh_H/0/1/0/all/0/1\">Hassan M. Fathallah-Shaykh</a>",
          "description": "Magnetic resonance imaging (MRI) is routinely used for brain tumor diagnosis,\ntreatment planning, and post-treatment surveillance. Recently, various models\nbased on deep neural networks have been proposed for the pixel-level\nsegmentation of tumors in brain MRIs. However, the structural variations,\nspatial dissimilarities, and intensity inhomogeneity in MRIs make segmentation\na challenging task. We propose a new end-to-end brain tumor segmentation\narchitecture based on U-Net that integrates Inception modules and dilated\nconvolutions into its contracting and expanding paths. This allows us to\nextract local structural as well as global contextual information. We performed\nsegmentation of glioma sub-regions, including tumor core, enhancing tumor, and\nwhole tumor using Brain Tumor Segmentation (BraTS) 2018 dataset. Our proposed\nmodel performed significantly better than the state-of-the-art U-Net-based\nmodel ($p<0.05$) for tumor core and whole tumor segmentation.",
          "link": "http://arxiv.org/abs/2108.06772",
          "publishedOn": "2021-08-17T01:54:47.050Z",
          "wordCount": 572,
          "title": "Dilated Inception U-Net (DIU-Net) for Brain Tumor Segmentation. (arXiv:2108.06772v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>",
          "description": "Recent works have theoretically and empirically shown that deep neural\nnetworks (DNNs) have an inherent vulnerability to small perturbations. Applying\nthe Deep k-Nearest Neighbors (DkNN) classifier, we observe a dramatically\nincreasing robustness-accuracy trade-off as the layer goes deeper. In this\nwork, we propose a Deep Adversarially-Enhanced k-Nearest Neighbors (DAEkNN)\nmethod which achieves higher robustness than DkNN and mitigates the\nrobustness-accuracy trade-off in deep layers through two key elements. First,\nDAEkNN is based on an adversarially trained model. Second, DAEkNN makes\npredictions by leveraging a weighted combination of benign and adversarial\ntraining data. Empirically, we find that DAEkNN improves both the robustness\nand the robustness-accuracy trade-off on MNIST and CIFAR-10 datasets.",
          "link": "http://arxiv.org/abs/2108.06797",
          "publishedOn": "2021-08-17T01:54:47.042Z",
          "wordCount": 539,
          "title": "Deep Adversarially-Enhanced k-Nearest Neighbors. (arXiv:2108.06797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yunhui Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Graph structured data have enabled several successful applications such as\nrecommendation systems and traffic prediction, given the rich node features and\nedges information. However, these high-dimensional features and high-order\nadjacency information are usually heterogeneous and held by different data\nholders in practice. Given such vertical data partition (e.g., one data holder\nwill only own either the node features or edge information), different data\nholders have to develop efficient joint training protocols rather than directly\ntransfer data to each other due to privacy concerns. In this paper, we focus on\nthe edge privacy, and consider a training scenario where Bob with node features\nwill first send training node features to Alice who owns the adjacency\ninformation. Alice will then train a graph neural network (GNN) with the joint\ninformation and release an inference API. During inference, Bob is able to\nprovide test node features and query the API to obtain the predictions for test\nnodes. Under this setting, we first propose a privacy attack LinkTeller via\ninfluence analysis to infer the private edge information held by Alice via\ndesigning adversarial queries for Bob. We then empirically show that LinkTeller\nis able to recover a significant amount of private edges, outperforming\nexisting baselines. To further evaluate the privacy leakage, we adapt an\nexisting algorithm for differentially private graph convolutional network (DP\nGCN) training and propose a new DP GCN mechanism LapGraph. We show that these\nDP GCN mechanisms are not always resilient against LinkTeller empirically under\nmild privacy guarantees ($\\varepsilon>5$). Our studies will shed light on\nfuture research towards designing more resilient privacy-preserving GCN models;\nin the meantime, provide an in-depth understanding of the tradeoff between GCN\nmodel utility and robustness against potential privacy attacks.",
          "link": "http://arxiv.org/abs/2108.06504",
          "publishedOn": "2021-08-17T01:54:47.034Z",
          "wordCount": 719,
          "title": "LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis. (arXiv:2108.06504v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Haitao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Initialization plays a critical role in the training of deep neural networks\n(DNN). Existing initialization strategies mainly focus on stabilizing the\ntraining process to mitigate gradient vanish/explosion problems. However, these\ninitialization methods are lacking in consideration about how to enhance\ngeneralization ability. The Information Bottleneck (IB) theory is a well-known\nunderstanding framework to provide an explanation about the generalization of\nDNN. Guided by the insights provided by IB theory, we design two criteria for\nbetter initializing DNN. And we further design a neuron campaign initialization\nalgorithm to efficiently select a good initialization for a neural network on a\ngiven dataset. The experiments on MNIST dataset show that our method can lead\nto a better generalization performance with faster convergence.",
          "link": "http://arxiv.org/abs/2108.06530",
          "publishedOn": "2021-08-17T01:54:47.028Z",
          "wordCount": 568,
          "title": "Neuron Campaign for Initialization Guided by Information Bottleneck Theory. (arXiv:2108.06530v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Despite the impressive clustering performance and efficiency in\ncharacterizing both the relationship between data and cluster structure,\nexisting graph-based multi-view clustering methods still have the following\ndrawbacks. They suffer from the expensive time burden due to both the\nconstruction of graphs and eigen-decomposition of Laplacian matrix, and fail to\nexplore the cluster structure of large-scale data. Moreover, they require a\npost-processing to get the final clustering, resulting in suboptimal\nperformance. Furthermore, rank of the learned view-consensus graph cannot\napproximate the target rank. In this paper, drawing the inspiration from the\nbipartite graph, we propose an effective and efficient graph learning model for\nmulti-view clustering. Specifically, our method exploits the view-similar\nbetween graphs of different views by the minimization of tensor Schatten\np-norm, which well characterizes both the spatial structure and complementary\ninformation embedded in graphs of different views. We learn view-consensus\ngraph with adaptively weighted strategy and connectivity constraint such that\nthe connected components indicates clusters directly. Our proposed algorithm is\ntime-economical and obtains the stable results and scales well with the data\nsize. Extensive experimental results indicate that our method is superior to\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.06734",
          "publishedOn": "2021-08-17T01:54:47.021Z",
          "wordCount": 616,
          "title": "Effective and Efficient Graph Learning for Multi-view Clustering. (arXiv:2108.06734v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yanwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xun Yu Zhou</a>",
          "description": "We propose a unified framework to study policy evaluation (PE) and the\nassociated temporal difference (TD) methods for reinforcement learning in\ncontinuous time and space. We show that PE is equivalent to maintaining the\nmartingale condition of a process. From this perspective, we find that the\nmean--square TD error approximates the quadratic variation of the martingale\nand thus is not a suitable objective for PE. We present two methods to use the\nmartingale characterization for designing PE algorithms. The first one\nminimizes a \"martingale loss function\", whose solution is proved to be the best\napproximation of the true value function in the mean--square sense. This method\ninterprets the classical gradient Monte-Carlo algorithm. The second method is\nbased on a system of equations called the \"martingale orthogonality conditions\"\nwith \"test functions\". Solving these equations in different ways recovers\nvarious classical TD algorithms, such as TD($\\lambda$), LSTD, and GTD.\nDifferent choices of test functions determine in what sense the resulting\nsolutions approximate the true value function. Moreover, we prove that any\nconvergent time-discretized algorithm converges to its continuous-time\ncounterpart as the mesh size goes to zero. We demonstrate the theoretical\nresults and corresponding algorithms with numerical experiments and\napplications.",
          "link": "http://arxiv.org/abs/2108.06655",
          "publishedOn": "2021-08-17T01:54:47.001Z",
          "wordCount": 644,
          "title": "Policy Evaluation and Temporal-Difference Learning in Continuous Time and Space: A Martingale Approach. (arXiv:2108.06655v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yitian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kunlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kunjin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>",
          "description": "We took part in the city brain challenge competition and achieved the 8th\nplace. In this competition, the players are provided with a real-world\ncity-scale road network and its traffic demand derived from real traffic data.\nThe players are asked to coordinate the traffic signals with a self-designed\nagent to maximize the number of vehicles served while maintaining an acceptable\ndelay. In this abstract paper, we present an overall analysis and our detailed\nsolution to this competition. Our approach is mainly based on the adaptation of\nthe deep Q-network (DQN) for real-time traffic signal control. From our\nperspective, the major challenge of this competition is how to extend the\nclassical DQN framework to traffic signals control in real-world complex road\nnetwork and traffic flow situation. After trying and implementing several\nclassical reward functions, we finally chose to apply our newly-designed reward\nin our agent. By applying our newly-proposed reward function and carefully\ntuning the control scheme, an agent based on a single DQN model can rank among\nthe top 15 teams. We hope this paper could serve, to some extent, as a baseline\nsolution to traffic signal control of real-world road network and inspire\nfurther attempts and researches.",
          "link": "http://arxiv.org/abs/2108.06491",
          "publishedOn": "2021-08-17T01:54:46.995Z",
          "wordCount": 644,
          "title": "DQN Control Solution for KDD Cup 2021 City Brain Challenge. (arXiv:2108.06491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_M/0/1/0/all/0/1\">Mostafa Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "This paper studies the subspace clustering problem in which data points\ncollected from high-dimensional ambient space lie in a union of linear\nsubspaces. Subspace clustering becomes challenging when the dimension of\nintersection between subspaces is large and most of the self-representation\nbased methods are sensitive to the intersection between the span of clusters.\nIn sharp contrast to the self-representation based methods, a recently proposed\nclustering method termed Innovation Pursuit, computed a set of optimal\ndirections (directions of innovation) to build the adjacency matrix. This paper\nfocuses on the Innovation Pursuit Algorithm to shed light on its impressive\nperformance when the subspaces are heavily intersected. It is shown that in\ncontrast to most of the existing methods which require the subspaces to be\nsufficiently incoherent with each other, Innovation Pursuit only requires the\ninnovative components of the subspaces to be sufficiently incoherent with each\nother. These new sufficient conditions allow the clusters to be strongly close\nto each other. Motivated by the presented theoretical analysis, a simple yet\neffective projection based technique is proposed which we show with both\nnumerical and theoretical results that it can boost the performance of\nInnovation Pursuit.",
          "link": "http://arxiv.org/abs/2108.06888",
          "publishedOn": "2021-08-17T01:54:46.988Z",
          "wordCount": 615,
          "title": "Provable Data Clustering via Innovation Search. (arXiv:2108.06888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratik Kumar</a>",
          "description": "With surge in online platforms, there has been an upsurge in the user\nengagement on these platforms via comments and reactions. A large portion of\nsuch textual comments are abusive, rude and offensive to the audience. With\nmachine learning systems in-place to check such comments coming onto platform,\nbiases present in the training data gets passed onto the classifier leading to\ndiscrimination against a set of classes, religion and gender. In this work, we\nevaluate different classifiers and feature to estimate the bias in these\nclassifiers along with their performance on downstream task of toxicity\nclassification. Results show that improvement in performance of automatic toxic\ncomment detection models is positively correlated to mitigating biases in these\nmodels. In our work, LSTM with attention mechanism proved to be a better\nmodelling strategy than a CNN model. Further analysis shows that fasttext\nembeddings is marginally preferable than glove embeddings on training models\nfor toxicity comment detection. Deeper analysis reveals the findings that such\nautomatic models are particularly biased to specific identity groups even\nthough the model has a high AUC score. Finally, in effort to mitigate bias in\ntoxicity detection models, a multi-task setup trained with auxiliary task of\ntoxicity sub-types proved to be useful leading to upto 0.26% (6% relative) gain\nin AUC scores.",
          "link": "http://arxiv.org/abs/2108.06487",
          "publishedOn": "2021-08-17T01:54:46.981Z",
          "wordCount": 659,
          "title": "Investigating Bias In Automatic Toxic Comment Detection: An Empirical Study. (arXiv:2108.06487v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossen_M/0/1/0/all/0/1\">Md Imran Hossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Ashraful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anowar_F/0/1/0/all/0/1\">Farzana Anowar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_E/0/1/0/all/0/1\">Eshtiak Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammad Masudur Rahman</a>",
          "description": "Due to the variety of cyber-attacks or threats, the cybersecurity community\nhas been enhancing the traditional security control mechanisms to an advanced\nlevel so that automated tools can encounter potential security threats. Very\nrecently a term, Cyber Threat Intelligence (CTI) has been represented as one of\nthe proactive and robust mechanisms because of its automated cybersecurity\nthreat prediction based on data. In general, CTI collects and analyses data\nfrom various sources e.g. online security forums, social media where cyber\nenthusiasts, analysts, even cybercriminals discuss cyber or computer security\nrelated topics and discovers potential threats based on the analysis. As the\nmanual analysis of every such discussion i.e. posts on online platforms is\ntime-consuming, inefficient, and susceptible to errors, CTI as an automated\ntool can perform uniquely to detect cyber threats. In this paper, our goal is\nto identify and explore relevant CTI from hacker forums by using different\nsupervised and unsupervised learning techniques. To this end, we collect data\nfrom a real hacker forum and constructed two datasets: a binary dataset and a\nmulti-class dataset. Our binary dataset contains two classes one containing\ncybersecurity-relevant posts and another one containing posts that are not\nrelated to security. This dataset is constructed using simple keyword search\ntechnique. Using a similar approach, we further categorize posts from\nsecurity-relevant posts into five different threat categories. We then applied\nseveral machine learning classifiers along with deep neural network-based\nclassifiers and use them on the datasets to compare their performances. We also\ntested the classifiers on a leaked dataset with labels named nulled.io as our\nground truth. We further explore the datasets using unsupervised techniques\ni.e. Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization\n(NMF).",
          "link": "http://arxiv.org/abs/2108.06862",
          "publishedOn": "2021-08-17T01:54:46.973Z",
          "wordCount": 730,
          "title": "Generating Cyber Threat Intelligence to Discover Potential Security Threats Using Classification and Topic Modeling. (arXiv:2108.06862v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_M/0/1/0/all/0/1\">Mohammad Reza Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarei_A/0/1/0/all/0/1\">Ariyan Zarei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Hoshin V. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnard_K/0/1/0/all/0/1\">Kobus Barnard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrangi_A/0/1/0/all/0/1\">Ali Behrangi</a>",
          "description": "Accurate and timely estimation of precipitation is critical for issuing\nhazard warnings (e.g., for flash floods or landslides). Current remotely sensed\nprecipitation products have a few hours of latency, associated with the\nacquisition and processing of satellite data. By applying a robust nowcasting\nsystem to these products, it is (in principle) possible to reduce this latency\nand improve their applicability, value, and impact. However, the development of\nsuch a system is complicated by the chaotic nature of the atmosphere, and the\nconsequent rapid changes that can occur in the structures of precipitation\nsystems In this work, we develop two approaches (hereafter referred to as\nNowcasting-Nets) that use Recurrent and Convolutional deep neural network\nstructures to address the challenge of precipitation nowcasting. A total of\nfive models are trained using Global Precipitation Measurement (GPM) Integrated\nMulti-satellitE Retrievals for GPM (IMERG) precipitation data over the Eastern\nContiguous United States (CONUS) and then tested against independent data for\nthe Eastern and Western CONUS. The models were designed to provide forecasts\nwith a lead time of up to 1.5 hours and, by using a feedback loop approach, the\nability of the models to extend the forecast time to 4.5 hours was also\ninvestigated. Model performance was compared against the Random Forest (RF) and\nLinear Regression (LR) machine learning methods, and also against a persistence\nbenchmark (BM) that used the most recent observation as the forecast.\nIndependent IMERG observations were used as a reference, and experiments were\nconducted to examine both overall statistics and case studies involving\nspecific precipitation events. Overall, the forecasts provided by the\nNowcasting-Net models are superior, with the Convolutional Nowcasting Network\nwith Residual Head (CNC-R) achieving 25%, 28%, and 46% improvement in the test\n...",
          "link": "http://arxiv.org/abs/2108.06868",
          "publishedOn": "2021-08-17T01:54:46.954Z",
          "wordCount": 746,
          "title": "Nowcasting-Nets: Deep Neural Network Structures for Precipitation Nowcasting Using IMERG. (arXiv:2108.06868v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Khay Boon Hong</a>",
          "description": "We use deep sparsely connected neural networks to measure the complexity of a\nfunction class in $L^2(\\mathbb R^d)$ by restricting connectivity and memory\nrequirement for storing the neural networks. We also introduce representation\nsystem - a countable collection of functions to guide neural networks, since\napproximation theory with representation system has been well developed in\nMathematics. We then prove the fundamental bound theorem, implying a quantity\nintrinsic to the function class itself can give information about the\napproximation ability of neural networks and representation system. We also\nprovides a method for transferring existing theories about approximation by\nrepresentation systems to that of neural networks, greatly amplifying the\npractical values of neural networks. Finally, we use neural networks to\napproximate B-spline functions, which are used to generate the B-spline curves.\nThen, we analyse the complexity of a class called $\\beta$ cartoon-like\nfunctions using rate-distortion theory and wedgelets construction.",
          "link": "http://arxiv.org/abs/2108.06467",
          "publishedOn": "2021-08-17T01:54:46.947Z",
          "wordCount": 592,
          "title": "Optimal Approximation with Sparse Neural Networks and Applications. (arXiv:2108.06467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenggang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kai Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Liting Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changliu Liu</a>",
          "description": "Microscopic epidemic models are powerful tools for government policy makers\nto predict and simulate epidemic outbreaks, which can capture the impact of\nindividual behaviors on the macroscopic phenomenon. However, existing models\nonly consider simple rule-based individual behaviors, limiting their\napplicability. This paper proposes a deep-reinforcement-learning-powered\nmicroscopic model named Microscopic Pandemic Simulator (MPS). By replacing\nrule-based agents with rational agents whose behaviors are driven to maximize\nrewards, the MPS provides a better approximation of real world dynamics. To\nefficiently simulate with massive amounts of agents in MPS, we propose Scalable\nMillion-Agent DQN (SMADQN). The MPS allows us to efficiently evaluate the\nimpact of different government strategies. This paper first calibrates the MPS\nagainst real-world data in Allegheny, US, then demonstratively evaluates two\ngovernment strategies: information disclosure and quarantine. The results\nvalidate the effectiveness of the proposed method. As a broad impact, this\npaper provides novel insights for the application of DRL in large scale\nagent-based networks such as economic and social networks.",
          "link": "http://arxiv.org/abs/2108.06589",
          "publishedOn": "2021-08-17T01:54:46.936Z",
          "wordCount": 611,
          "title": "A Microscopic Pandemic Simulator for Pandemic Prediction Using Scalable Million-Agent Reinforcement Learning. (arXiv:2108.06589v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarna_A/0/1/0/all/0/1\">Aaron Sarna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1\">Dilip Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maschinot_A/0/1/0/all/0/1\">Aaron Maschinot</a>",
          "description": "Disentangled visual representations have largely been studied with generative\nmodels such as Variational AutoEncoders (VAEs). While prior work has focused on\ngenerative methods for disentangled representation learning, these approaches\ndo not scale to large datasets due to current limitations of generative models.\nInstead, we explore regularization methods with contrastive learning, which\ncould result in disentangled representations that are powerful enough for large\nscale datasets and downstream applications. However, we find that unsupervised\ndisentanglement is difficult to achieve due to optimization and initialization\nsensitivity, with trade-offs in task performance. We evaluate disentanglement\nwith downstream tasks, analyze the benefits and disadvantages of each\nregularization used, and discuss future directions.",
          "link": "http://arxiv.org/abs/2108.06613",
          "publishedOn": "2021-08-17T01:54:46.925Z",
          "wordCount": 559,
          "title": "Unsupervised Disentanglement without Autoencoding: Pitfalls and Future Directions. (arXiv:2108.06613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1\">Sara Abdali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilescu_M/0/1/0/all/0/1\">M. Alex O. Vasilescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1\">Evangelos E. Papalexakis</a>",
          "description": "Generative neural network architectures such as GANs, may be used to generate\nsynthetic instances to compensate for the lack of real data. However, they may\nbe employed to create media that may cause social, political or economical\nupheaval. One emerging media is \"Deepfake\".Techniques that can discriminate\nbetween such media is indispensable. In this paper, we propose a modified\nmultilinear (tensor) method, a combination of linear and multilinear\nregressions for representing fake and real data. We test our approach by\nrepresenting Deepfakes with our modified multilinear (tensor) approach and\nperform SVM classification with encouraging results.",
          "link": "http://arxiv.org/abs/2108.06702",
          "publishedOn": "2021-08-17T01:54:46.919Z",
          "wordCount": 532,
          "title": "Deepfake Representation with Multilinear Regression. (arXiv:2108.06702v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Christopher Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jai Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_M/0/1/0/all/0/1\">Milind Maiti</a>",
          "description": "Dropout Regularization, serving to reduce variance, is nearly ubiquitous in\nDeep Learning models. We explore the relationship between the dropout rate and\nmodel complexity by training 2,000 neural networks configured with random\ncombinations of the dropout rate and the number of hidden units in each dense\nlayer, on each of the three data sets we selected. The generated figures, with\nbinary cross entropy loss and binary accuracy on the z-axis, question the\ncommon assumption that adding depth to a dense layer while increasing the\ndropout rate will certainly enhance performance. We also discover a complex\ncorrelation between the two hyperparameters that we proceed to quantify by\nbuilding additional machine learning and Deep Learning models which predict the\noptimal dropout rate given some hidden units in each dense layer. Linear\nregression and polynomial logistic regression require the use of arbitrary\nthresholds to select the cost data points included in the regression and to\nassign the cost data points a binary classification, respectively. These\nmachine learning models have mediocre performance because their naive nature\nprevented the modeling of complex decision boundaries. Turning to Deep Learning\nmodels, we build neural networks that predict the optimal dropout rate given\nthe number of hidden units in each dense layer, the desired cost, and the\ndesired accuracy of the model. Though, this attempt encounters a mathematical\nerror that can be attributed to the failure of the vertical line test. The\nultimate Deep Learning model is a neural network whose decision boundary\nrepresents the 2,000 previously generated data points. This final model leads\nus to devise a promising method for tuning hyperparameters to minimize\ncomputational expense yet maximize performance. The strategy can be applied to\nany model hyperparameters, with the prospect of more efficient tuning in\nindustrial models.",
          "link": "http://arxiv.org/abs/2108.06628",
          "publishedOn": "2021-08-17T01:54:46.813Z",
          "wordCount": 726,
          "title": "Investigating the Relationship Between Dropout Regularization and Model Complexity in Neural Networks. (arXiv:2108.06628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sehun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_H/0/1/0/all/0/1\">Hyunjun Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>",
          "description": "Most recent studies on detecting and localizing temporal anomalies have\nmainly employed deep neural networks to learn the normal patterns of temporal\ndata in an unsupervised manner. Unlike them, the goal of our work is to fully\nutilize instance-level (or weak) anomaly labels, which only indicate whether\nany anomalous events occurred or not in each instance of temporal data. In this\npaper, we present WETAS, a novel framework that effectively identifies\nanomalous temporal segments (i.e., consecutive time points) in an input\ninstance. WETAS learns discriminative features from the instance-level labels\nso that it infers the sequential order of normal and anomalous segments within\neach instance, which can be used as a rough segmentation mask. Based on the\ndynamic time warping (DTW) alignment between the input instance and its\nsegmentation mask, WETAS obtains the result of temporal segmentation, and\nsimultaneously, it further enhances itself by using the mask as additional\nsupervision. Our experiments show that WETAS considerably outperforms other\nbaselines in terms of the localization of temporal anomalies, and also it\nprovides more informative results than point-level detection methods.",
          "link": "http://arxiv.org/abs/2108.06816",
          "publishedOn": "2021-08-17T01:54:46.791Z",
          "wordCount": 636,
          "title": "Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping. (arXiv:2108.06816v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06460",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hong_K/0/1/0/all/0/1\">Kai Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Cailian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yancheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>",
          "description": "This work presents an unsupervised deep learning scheme that exploiting\nhigh-dimensional assisted score-based generative model for color image\nrestoration tasks. Considering that the sample number and internal dimension in\nscore-based generative model have key influence on estimating the gradients of\ndata distribution, two different high-dimensional ways are proposed: The\nchannel-copy transformation increases the sample number and the pixel-scale\ntransformation decreases feasible space dimension. Subsequently, a set of\nhigh-dimensional tensors represented by these transformations are used to train\nthe network through denoising score matching. Then, sampling is performed by\nannealing Langevin dynamics and alternative data-consistency update.\nFurthermore, to alleviate the difficulty of learning high-dimensional\nrepresentation, a progressive strategy is proposed to leverage the performance.\nThe proposed unsupervised learning and iterative restoration algo-rithm, which\ninvolves a pre-trained generative network to obtain prior, has transparent and\nclear interpretation compared to other data-driven approaches. Experimental\nresults on demosaicking and inpainting conveyed the remarkable performance and\ndiversity of our proposed method.",
          "link": "http://arxiv.org/abs/2108.06460",
          "publishedOn": "2021-08-17T01:54:46.784Z",
          "wordCount": 614,
          "title": "High-dimensional Assisted Generative Model for Color Image Restoration. (arXiv:2108.06460v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06624",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bhat_H/0/1/0/all/0/1\">Harish S. Bhat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reeves_M/0/1/0/all/0/1\">Majerle E. Reeves</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldman_Mellor_S/0/1/0/all/0/1\">Sidra Goldman-Mellor</a>",
          "description": "When faced with severely imbalanced binary classification problems, we often\ntrain models on bootstrapped data in which the number of instances of each\nclass occur in a more favorable ratio, e.g., one. We view algorithmic inequity\nthrough the lens of imbalanced classification: in order to balance the\nperformance of a classifier across groups, we can bootstrap to achieve training\nsets that are balanced with respect to both labels and group identity. For an\nexample problem with severe class imbalance---prediction of suicide death from\nadministrative patient records---we illustrate how an equity-directed bootstrap\ncan bring test set sensitivities and specificities much closer to satisfying\nthe equal odds criterion. In the context of na\\\"ive Bayes and logistic\nregression, we analyze the equity-directed bootstrap, demonstrating that it\nworks by bringing odds ratios close to one, and linking it to methods involving\nintercept adjustment, thresholding, and weighting.",
          "link": "http://arxiv.org/abs/2108.06624",
          "publishedOn": "2021-08-17T01:54:46.778Z",
          "wordCount": 576,
          "title": "Equity-Directed Bootstrapping: Examples and Analysis. (arXiv:2108.06624v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wanye_F/0/1/0/all/0/1\">Frank Wanye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleyzer_V/0/1/0/all/0/1\">Vitaliy Gleyzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_E/0/1/0/all/0/1\">Edward Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wu-chun Feng</a>",
          "description": "Community detection is a well-studied problem with applications in domains\nranging from computer networking to bioinformatics. While there are many\nalgorithms that perform community detection, the more accurate and\nstatistically robust algorithms tend to be slow and hard to parallelize. One\nway to speed up such algorithms is through data reduction. However, this\napproach has not been thoroughly studied, and the quality of results obtained\nwith this approach varies with the graph it is applied to. In this manuscript,\nwe present an approach based on topology-guided sampling for accelerating\nstochastic block partitioning - a community detection algorithm that works well\non graphs with complex and heterogeneous community structure. We also introduce\na degree-based thresholding scheme that improves the efficacy of our approach\nat the expense of speedup. Finally, we perform a series of experiments on\nsynthetically generated graphs to determine how various graph parameters affect\nthe quality of results and speedup obtained with our approach, and we validate\nour approach on real-world data. Our results show that our approach can lead to\na speedup of up to 15X over stochastic block partitioning without sampling\nwhile maintaining result quality and can even lead to improvements of over 150%\nin result quality in terms of F1 score on certain kinds of graphs.",
          "link": "http://arxiv.org/abs/2108.06651",
          "publishedOn": "2021-08-17T01:54:46.771Z",
          "wordCount": 648,
          "title": "Topology-Guided Sampling for Fast and Accurate Community Detection. (arXiv:2108.06651v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qinyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yiou Xiao</a>",
          "description": "As a special field in deep learning, Graph Neural Networks (GNNs) focus on\nextracting intrinsic network features and have drawn unprecedented popularity\nin both academia and industry. Most of the state-of-the-art GNN models offer\nexpressive, robust, scalable and inductive solutions empowering social network\nrecommender systems with rich network features that are computationally\ndifficult to leverage with graph traversal based methods.\n\nMost recent GNNs follow an encoder-decoder paradigm to encode high\ndimensional heterogeneous information from a subgraph onto one low dimensional\nembedding space. However, one single embedding space usually fails to capture\nall aspects of graph signals. In this work, we propose boosting-based meta\nlearner for GNNs, which automatically learns multiple projections and the\ncorresponding embedding spaces that captures different aspects of the graph\nsignals. As a result, similarities between sub-graphs are quantified by\nembedding proximity on multiple embedding spaces. AdaGNN performs exceptionally\nwell for applications with rich and diverse node neighborhood information.\nMoreover, AdaGNN is compatible with any inductive GNNs for both node-level and\nedge-level tasks.",
          "link": "http://arxiv.org/abs/2108.06452",
          "publishedOn": "2021-08-17T01:54:46.765Z",
          "wordCount": 604,
          "title": "AdaGNN: A multi-modal latent representation meta-learner for GNNs based on AdaBoosting. (arXiv:2108.06452v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06764",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Ruinong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>",
          "description": "In order to reduce the negative impact of the uncertainty of load and\nrenewable energies outputs on microgrid operation, an optimal scheduling model\nis proposed for isolated microgrids by using automated reinforcement\nlearning-based multi-period forecasting of renewable power generations and\nloads. Firstly, a prioritized experience replay automated reinforcement\nlearning (PER-AutoRL) is designed to simplify the deployment of deep\nreinforcement learning (DRL)-based forecasting model in a customized manner,\nthe single-step multi-period forecasting method based on PER-AutoRL is proposed\nfor the first time to address the error accumulation issue suffered by existing\nmulti-step forecasting methods, then the prediction values obtained by the\nproposed forecasting method are revised via the error distribution to improve\nthe prediction accuracy; secondly, a scheduling model considering demand\nresponse is constructed to minimize the total microgrid operating costs, where\nthe revised forecasting values are used as the dispatch basis, and a spinning\nreserve chance constraint is set according to the error distribution; finally,\nby transforming the original scheduling model into a readily solvable mixed\ninteger linear programming via the sequence operation theory (SOT), the\ntransformed model is solved by using CPLEX solver. The simulation results show\nthat compared with the traditional scheduling model without forecasting, this\napproach manages to significantly reduce the system operating costs by\nimproving the prediction accuracy.",
          "link": "http://arxiv.org/abs/2108.06764",
          "publishedOn": "2021-08-17T01:54:46.748Z",
          "wordCount": 670,
          "title": "Optimal Scheduling of Isolated Microgrids Using Automated Reinforcement Learning-based Multi-period Forecasting. (arXiv:2108.06764v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06558",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Papapicco_D/0/1/0/all/0/1\">Davide Papapicco</a>, <a href=\"http://arxiv.org/find/math/1/au:+Demo_N/0/1/0/all/0/1\">Nicola Demo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Girfoglio_M/0/1/0/all/0/1\">Michele Girfoglio</a>, <a href=\"http://arxiv.org/find/math/1/au:+Stabile_G/0/1/0/all/0/1\">Giovanni Stabile</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rozza_G/0/1/0/all/0/1\">Gianluigi Rozza</a>",
          "description": "Models with dominant advection always posed a difficult challenge for\nprojection-based reduced order modelling. Many methodologies that have recently\nbeen proposed are based on the pre-processing of the full-order solutions to\naccelerate the Kolmogorov N-width decay thereby obtaining smaller linear\nsubspaces with improved accuracy. These methods however must rely on the\nknowledge of the characteristic speeds in phase space of the solution, limiting\ntheir range of applicability to problems with explicit functional form for the\nadvection field. In this work we approach the problem of automatically\ndetecting the correct pre-processing transformation in a statistical learning\nframework by implementing a deep-learning architecture. The purely data-driven\nmethod allowed us to generalise the existing approaches of linear subspace\nmanipulation to non-linear hyperbolic problems with unknown advection fields.\nThe proposed algorithm has been validated against simple test cases to\nbenchmark its performances and later successfully applied to a multiphase\nsimulation.",
          "link": "http://arxiv.org/abs/2108.06558",
          "publishedOn": "2021-08-17T01:54:46.742Z",
          "wordCount": 598,
          "title": "The Neural Network shifted-Proper Orthogonal Decomposition: a Machine Learning Approach for Non-linear Reduction of Hyperbolic Equations. (arXiv:2108.06558v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sasso_R/0/1/0/all/0/1\">Remo Sasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatelli_M/0/1/0/all/0/1\">Matthia Sabatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1\">Marco A. Wiering</a>",
          "description": "Reinforcement learning (RL) is well known for requiring large amounts of data\nin order for RL agents to learn to perform complex tasks. Recent progress in\nmodel-based RL allows agents to be much more data-efficient, as it enables them\nto learn behaviors of visual environments in imagination by leveraging an\ninternal World Model of the environment. Improved sample efficiency can also be\nachieved by reusing knowledge from previously learned tasks, but transfer\nlearning is still a challenging topic in RL. Parameter-based transfer learning\nis generally done using an all-or-nothing approach, where the network's\nparameters are either fully transferred or randomly initialized. In this work\nwe present a simple alternative approach: fractional transfer learning. The\nidea is to transfer fractions of knowledge, opposed to discarding potentially\nuseful knowledge as is commonly done with random initialization. Using the\nWorld Model-based Dreamer algorithm, we identify which type of components this\napproach is applicable to, and perform experiments in a new multi-source\ntransfer learning setting. The results show that fractional transfer learning\noften leads to substantially improved performance and faster learning compared\nto learning from scratch and random initialization.",
          "link": "http://arxiv.org/abs/2108.06526",
          "publishedOn": "2021-08-17T01:54:46.735Z",
          "wordCount": 628,
          "title": "Fractional Transfer Learning for Deep Model-Based Reinforcement Learning. (arXiv:2108.06526v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06394",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Shneider_C/0/1/0/all/0/1\">Carl Shneider</a> (1), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hu_A/0/1/0/all/0/1\">Andong Hu</a> (1), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tiwari_A/0/1/0/all/0/1\">Ajay K. Tiwari</a> (1), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bobra_M/0/1/0/all/0/1\">Monica G. Bobra</a> (2), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Battams_K/0/1/0/all/0/1\">Karl Battams</a> (5), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Teunissen_J/0/1/0/all/0/1\">Jannis Teunissen</a> (1), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Camporeale_E/0/1/0/all/0/1\">Enrico Camporeale</a> (3 and 4) ((1) Multiscale Dynamics Group, Center for Mathematics and Computer Science (CWI), Amsterdam, The Netherlands, (2) W.W. Hansen Experimental Physics Laboratory, Stanford University, Stanford, CA, USA, (3) CIRES, University of Colorado, Boulder, CO, USA, (4) NOAA, Space Weather Prediction Center, Boulder, CO, USA, (5) US Naval Research Laboratory, Washington DC, USA)",
          "description": "We present a Python tool to generate a standard dataset from solar images\nthat allows for user-defined selection criteria and a range of pre-processing\nsteps. Our Python tool works with all image products from both the Solar and\nHeliospheric Observatory (SoHO) and Solar Dynamics Observatory (SDO) missions.\nWe discuss a dataset produced from the SoHO mission's multi-spectral images\nwhich is free of missing or corrupt data as well as planetary transits in\ncoronagraph images, and is temporally synced making it ready for input to a\nmachine learning system. Machine-learning-ready images are a valuable resource\nfor the community because they can be used, for example, for forecasting space\nweather parameters. We illustrate the use of this data with a 3-5 day-ahead\nforecast of the north-south component of the interplanetary magnetic field\n(IMF) observed at Lagrange point one (L1). For this use case, we apply a deep\nconvolutional neural network (CNN) to a subset of the full SoHO dataset and\ncompare with baseline results from a Gaussian Naive Bayes classifier.",
          "link": "http://arxiv.org/abs/2108.06394",
          "publishedOn": "2021-08-17T01:54:46.729Z",
          "wordCount": 698,
          "title": "A Machine-Learning-Ready Dataset Prepared from the Solar and Heliospheric Observatory Mission. (arXiv:2108.06394v1 [astro-ph.SR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Caleb Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_E/0/1/0/all/0/1\">Ethan X. Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Bregman proximal point algorithm (BPPA), as one of the centerpieces in the\noptimization toolbox, has been witnessing emerging applications. With simple\nand easy to implement update rule, the algorithm bears several compelling\nintuitions for empirical successes, yet rigorous justifications are still\nlargely unexplored. We study the computational properties of BPPA through\nclassification tasks with separable data, and demonstrate provable algorithmic\nregularization effects associated with BPPA. We show that BPPA attains\nnon-trivial margin, which closely depends on the condition number of the\ndistance generating function inducing the Bregman divergence. We further\ndemonstrate that the dependence on the condition number is tight for a class of\nproblems, thus showing the importance of divergence in affecting the quality of\nthe obtained solutions. In addition, we extend our findings to mirror descent\n(MD), for which we establish similar connections between the margin and Bregman\ndivergence. We demonstrate through a concrete example, and show BPPA/MD\nconverges in direction to the maximal margin solution with respect to the\nMahalanobis distance. Our theoretical findings are among the first to\ndemonstrate the benign learning properties BPPA/MD, and also provide\ncorroborations for a careful choice of divergence in the algorithmic design.",
          "link": "http://arxiv.org/abs/2108.06808",
          "publishedOn": "2021-08-17T01:54:46.722Z",
          "wordCount": 633,
          "title": "Implicit Regularization of Bregman Proximal Point Algorithm and Mirror Descent on Separable Data. (arXiv:2108.06808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_N/0/1/0/all/0/1\">Nanda K. Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parhi_K/0/1/0/all/0/1\">Keshab K. Parhi</a>",
          "description": "The time required for training the neural networks increases with size,\ncomplexity, and depth. Training model parameters by backpropagation inherently\ncreates feedback loops. These loops hinder efficient pipelining and scheduling\nof the tasks within the layer and between consecutive layers. Prior approaches,\nsuch as PipeDream, have exploited the use of delayed gradient to achieve\ninter-layer pipelining. However, these approaches treat the entire\nbackpropagation as a single task; this leads to an increase in computation time\nand processor underutilization. This paper presents novel optimization\napproaches where the gradient computations with respect to the weights and the\nactivation functions are considered independently; therefore, these can be\ncomputed in parallel. This is referred to as intra-layer optimization.\nAdditionally, the gradient computation with respect to the activation function\nis further divided into two parts and distributed to two consecutive layers.\nThis leads to balanced scheduling where the computation time of each layer is\nthe same. This is referred to as inter-layer optimization. The proposed system,\nreferred to as LayerPipe, reduces the number of clock cycles required for\ntraining while maximizing processor utilization with minimal inter-processor\ncommunication overhead. LayerPipe achieves an average speedup of 25% and\nupwards of 80% with 7 to 9 processors with less communication overhead when\ncompared to PipeDream.",
          "link": "http://arxiv.org/abs/2108.06629",
          "publishedOn": "2021-08-17T01:54:46.715Z",
          "wordCount": 681,
          "title": "LayerPipe: Accelerating Deep Neural Network Training by Intra-Layer and Inter-Layer Gradient Pipelining and Multiprocessor Scheduling. (arXiv:2108.06629v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Suyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1\">Luis Nunes Vicente</a>",
          "description": "In machine learning (ML) applications, unfair predictions may discriminate\nagainst a minority group. Most existing approaches for fair machine learning\n(FML) treat fairness as a constraint or a penalization term in the optimization\nof a ML model, which does not lead to the discovery of the complete landscape\nof the trade-offs among learning accuracy and fairness metrics, and does not\nintegrate fairness in a meaningful way.\n\nRecently, we have introduced a new paradigm for FML based on Stochastic\nMulti-Objective Optimization (SMOO), where accuracy and fairness metrics stand\nas conflicting objectives to be optimized simultaneously. The entire trade-offs\nrange is defined as the Pareto front of the SMOO problem, which can then be\nefficiently computed using stochastic-gradient type algorithms. SMOO also\nallows defining and computing new meaningful predictors for FML, a novel one\nbeing the Sharpe predictor that we introduce and explore in this paper, and\nwhich gives the highest ratio of accuracy-to-unfairness. Inspired from SMOO in\nfinance, the Sharpe predictor for FML provides the highest prediction return\n(accuracy) per unit of prediction risk (unfairness).",
          "link": "http://arxiv.org/abs/2108.06415",
          "publishedOn": "2021-08-17T01:54:46.698Z",
          "wordCount": 601,
          "title": "The Sharpe predictor for fairness in machine learning. (arXiv:2108.06415v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_S/0/1/0/all/0/1\">Sumit Kumar Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1\">Jeetu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Aditya Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishabh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunturi_V/0/1/0/all/0/1\">Venkata M. V. Gunturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C. Krishnan</a>",
          "description": "Interpolation in Spatio-temporal data has applications in various domains\nsuch as climate, transportation, and mining. Spatio-Temporal interpolation is\nhighly challenging due to the complex spatial and temporal relationships.\nHowever, traditional techniques such as Kriging suffer from high running time\nand poor performance on data that exhibit high variance across space and time\ndimensions. To this end, we propose a novel deep neural network called as Deep\nGeospatial Interpolation Network(DGIN), which incorporates both spatial and\ntemporal relationships and has significantly lower training time. DGIN consists\nof three major components: Spatial Encoder to capture the spatial dependencies,\nSequential module to incorporate the temporal dynamics, and an Attention block\nto learn the importance of the temporal neighborhood around the gap. We\nevaluate DGIN on the MODIS reflectance dataset from two different regions. Our\nexperimental results indicate that DGIN has two advantages: (a) it outperforms\nalternative approaches (has lower MSE with p-value < 0.01) and, (b) it has\nsignificantly low execution time than Kriging.",
          "link": "http://arxiv.org/abs/2108.06670",
          "publishedOn": "2021-08-17T01:54:46.692Z",
          "wordCount": 593,
          "title": "Deep Geospatial Interpolation Networks. (arXiv:2108.06670v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1\">Reza Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhier_L/0/1/0/all/0/1\">Lucas Rouhier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>",
          "description": "Labeling vertebral discs from MRI scans is important for the proper diagnosis\nof spinal related diseases, including multiple sclerosis, amyotrophic lateral\nsclerosis, degenerative cervical myelopathy and cancer. Automatic labeling of\nthe vertebral discs in MRI data is a difficult task because of the similarity\nbetween discs and bone area, the variability in the geometry of the spine and\nsurrounding tissues across individuals, and the variability across scans\n(manufacturers, pulse sequence, image contrast, resolution and artefacts). In\nprevious studies, vertebral disc labeling is often done after a disc detection\nstep and mostly fails when the localization algorithm misses discs or has false\npositive detection. In this work, we aim to mitigate this problem by\nreformulating the semantic vertebral disc labeling using the pose estimation\ntechnique. To do so, we propose a stacked hourglass network with multi-level\nattention mechanism to jointly learn intervertebral disc position and their\nskeleton structure. The proposed deep learning model takes into account the\nstrength of semantic segmentation and pose estimation technique to handle the\nmissing area and false positive detection. To further improve the performance\nof the proposed method, we propose a skeleton-based search space to reduce\nfalse positive detection. The proposed method evaluated on spine generic public\nmulti-center dataset and demonstrated better performance comparing to previous\nwork, on both T1w and T2w contrasts. The method is implemented in ivadomed\n(https://ivadomed.org).",
          "link": "http://arxiv.org/abs/2108.06554",
          "publishedOn": "2021-08-17T01:54:46.686Z",
          "wordCount": 685,
          "title": "Stacked Hourglass Network with a Multi-level Attention Mechanism: Where to Look for Intervertebral Disc Labeling. (arXiv:2108.06554v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>",
          "description": "Person re-identification (ReID) aims to re-identify a person from\nnon-overlapping camera views. Since person ReID data contains sensitive\npersonal information, researchers have adopted federated learning, an emerging\ndistributed training method, to mitigate the privacy leakage risks. However,\nexisting studies rely on data labels that are laborious and time-consuming to\nobtain. We present FedUReID, a federated unsupervised person ReID system to\nlearn person ReID models without any labels while preserving privacy. FedUReID\nenables in-situ model training on edges with unlabeled data. A cloud server\naggregates models from edges instead of centralizing raw data to preserve data\nprivacy. Moreover, to tackle the problem that edges vary in data volumes and\ndistributions, we personalize training in edges with joint optimization of\ncloud and edge. Specifically, we propose personalized epoch to reassign\ncomputation throughout training, personalized clustering to iteratively predict\nsuitable labels for unlabeled data, and personalized update to adapt the server\naggregated model to each edge. Extensive experiments on eight person ReID\ndatasets demonstrate that FedUReID not only achieves higher accuracy but also\nreduces computation cost by 29%. Our FedUReID system with the joint\noptimization will shed light on implementing federated learning to more\nmultimedia tasks without data labels.",
          "link": "http://arxiv.org/abs/2108.06493",
          "publishedOn": "2021-08-17T01:54:46.653Z",
          "wordCount": 653,
          "title": "Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised Person Re-identification. (arXiv:2108.06493v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Sheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Ju Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jiang Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Deyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weihua Zhuang</a>",
          "description": "Federated meta-learning (FML) has emerged as a promising paradigm to cope\nwith the data limitation and heterogeneity challenges in today's edge learning\narena. However, its performance is often limited by slow convergence and\ncorresponding low communication efficiency. Besides, since the wireless\nbandwidth and IoT devices' energy capacity are usually insufficient, it is\ncrucial to control the resource allocation and energy consumption when\ndeploying FML in realistic wireless networks. To overcome these challenges, in\nthis paper, we first rigorously analyze each device's contribution to the\nglobal loss reduction in each round and develop an FML algorithm (called NUFM)\nwith a non-uniform device selection scheme to accelerate the convergence. After\nthat, we formulate a resource allocation problem integrating NUFM in\nmulti-access wireless systems to jointly improve the convergence rate and\nminimize the wall-clock time along with energy cost. By deconstructing the\noriginal problem step by step, we devise a joint device selection and resource\nallocation strategy (called URAL) to solve the problem and provide theoretical\nguarantees. Further, we show that the computational complexity of NUFM can be\nreduced from $O(d^2)$ to $O(d)$ (with $d$ being the model dimension) via\ncombining two first-order approximation techniques. Extensive simulation\nresults demonstrate the effectiveness and superiority of the proposed methods\nby comparing with the existing baselines.",
          "link": "http://arxiv.org/abs/2108.06453",
          "publishedOn": "2021-08-17T01:54:46.631Z",
          "wordCount": 642,
          "title": "Efficient Federated Meta-Learning over Multi-Access Wireless Networks. (arXiv:2108.06453v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Runzhe Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Lin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>",
          "description": "How to explore efficiently is a central problem in multi-armed bandits. In\nthis paper, we introduce the metadata-based multi-task bandit problem, where\nthe agent needs to solve a large number of related multi-armed bandit tasks and\ncan leverage some task-specific features (i.e., metadata) to share knowledge\nacross tasks. As a general framework, we propose to capture task relations\nthrough the lens of Bayesian hierarchical models, upon which a Thompson\nsampling algorithm is designed to efficiently learn task relations, share\ninformation, and minimize the cumulative regrets. Two concrete examples for\nGaussian bandits and Bernoulli bandits are carefully analyzed. The Bayes regret\nfor Gaussian bandits clearly demonstrates the benefits of information sharing\nwith our algorithm. The proposed method is further supported by extensive\nexperiments.",
          "link": "http://arxiv.org/abs/2108.06422",
          "publishedOn": "2021-08-17T01:54:46.603Z",
          "wordCount": 552,
          "title": "Metadata-based Multi-Task Bandits with Bayesian Hierarchical Models. (arXiv:2108.06422v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutin_M/0/1/0/all/0/1\">Mireille Boutin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coupkova_E/0/1/0/all/0/1\">Evzenie Coupkova</a>",
          "description": "The generalization error of a classifier is related to the complexity of the\nset of functions among which the classifier is chosen. Roughly speaking, the\nmore complex the family, the greater the potential disparity between the\ntraining error and the population error of the classifier. This principle is\nembodied in layman's terms by Occam's razor principle, which suggests favoring\nlow-complexity hypotheses over complex ones. We study a family of\nlow-complexity classifiers consisting of thresholding the one-dimensional\nfeature obtained by projecting the data on a random line after embedding it\ninto a higher dimensional space parametrized by monomials of order up to k.\nMore specifically, the extended data is projected n-times and the best\nclassifier among those n (based on its performance on training data) is chosen.\nWe obtain a bound on the generalization error of these low-complexity\nclassifiers. The bound is less than that of any classifier with a non-trivial\nVC dimension, and thus less than that of a linear classifier. We also show\nthat, given full knowledge of the class conditional densities, the error of the\nclassifiers would converge to the optimal (Bayes) error as k and n go to\ninfinity; if only a training dataset is given, we show that the classifiers\nwill perfectly classify all the training points as k and n go to infinity.",
          "link": "http://arxiv.org/abs/2108.06339",
          "publishedOn": "2021-08-17T01:54:46.593Z",
          "wordCount": 657,
          "title": "Asymptotic optimality and minimal complexity of classification by random projection. (arXiv:2108.06339v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_Cardenas_C/0/1/0/all/0/1\">Carlos H. Mendoza-Cardenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockmeier_A/0/1/0/all/0/1\">Austin J. Brockmeier</a>",
          "description": "Seizure detection algorithms must discriminate abnormal neuronal activity\nassociated with a seizure from normal neural activity in a variety of\nconditions. Our approach is to seek spatiotemporal waveforms with distinct\nmorphology in electrocorticographic (ECoG) recordings of epileptic patients\nthat are indicative of a subsequent seizure (preictal) versus non-seizure\nsegments (interictal). To find these waveforms we apply a shift-invariant\nk-means algorithm to segments of spatially filtered signals to learn codebooks\nof prototypical waveforms. The frequency of the cluster labels from the\ncodebooks is then used to train a binary classifier that predicts the class\n(preictal or interictal) of a test ECoG segment. We use the Matthews\ncorrelation coefficient to evaluate the performance of the classifier and the\nquality of the codebooks. We found that our method finds recurrent\nnon-sinusoidal waveforms that could be used to build interpretable features for\nseizure prediction and that are also physiologically meaningful.",
          "link": "http://arxiv.org/abs/2108.03177",
          "publishedOn": "2021-08-16T01:57:42.793Z",
          "wordCount": 621,
          "title": "Shift-invariant waveform learning on epileptic ECoG. (arXiv:2108.03177v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tailor_S/0/1/0/all/0/1\">Shyam A. Tailor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_R/0/1/0/all/0/1\">Ren&#xe9; de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azevedo_T/0/1/0/all/0/1\">Tiago Azevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1\">Matthew Mattina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_P/0/1/0/all/0/1\">Partha Maji</a>",
          "description": "In recent years graph neural network (GNN)-based approaches have become a\npopular strategy for processing point cloud data, regularly achieving\nstate-of-the-art performance on a variety of tasks. To date, the research\ncommunity has primarily focused on improving model expressiveness, with\nsecondary thought given to how to design models that can run efficiently on\nresource constrained mobile devices including smartphones or mixed reality\nheadsets. In this work we make a step towards improving the efficiency of these\nmodels by making the observation that these GNN models are heavily limited by\nthe representational power of their first, feature extracting, layer. We find\nthat it is possible to radically simplify these models so long as the feature\nextraction layer is retained with minimal degradation to model performance;\nfurther, we discover that it is possible to improve performance overall on\nModelNet40 and S3DIS by improving the design of the feature extractor. Our\napproach reduces memory consumption by 20$\\times$ and latency by up to\n9.9$\\times$ for graph layers in models such as DGCNN; overall, we achieve\nspeed-ups of up to 4.5$\\times$ and peak memory reductions of 72.5%.",
          "link": "http://arxiv.org/abs/2108.06317",
          "publishedOn": "2021-08-16T00:47:33.903Z",
          "wordCount": 645,
          "title": "Towards Efficient Point Cloud Graph Neural Networks Through Architectural Simplification. (arXiv:2108.06317v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berghoff_C/0/1/0/all/0/1\">Christian Berghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielik_P/0/1/0/all/0/1\">Pavol Bielik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neu_M/0/1/0/all/0/1\">Matthias Neu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsankov_P/0/1/0/all/0/1\">Petar Tsankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twickel_A/0/1/0/all/0/1\">Arndt von Twickel</a>",
          "description": "In the last years, AI systems, in particular neural networks, have seen a\ntremendous increase in performance, and they are now used in a broad range of\napplications. Unlike classical symbolic AI systems, neural networks are trained\nusing large data sets and their inner structure containing possibly billions of\nparameters does not lend itself to human interpretation. As a consequence, it\nis so far not feasible to provide broad guarantees for the correct behaviour of\nneural networks during operation if they process input data that significantly\ndiffer from those seen during training. However, many applications of AI\nsystems are security- or safety-critical, and hence require obtaining\nstatements on the robustness of the systems when facing unexpected events,\nwhether they occur naturally or are induced by an attacker in a targeted way.\nAs a step towards developing robust AI systems for such applications, this\npaper presents how the robustness of AI systems can be practically examined and\nwhich methods and metrics can be used to do so. The robustness testing\nmethodology is described and analysed for the example use case of traffic sign\nrecognition in autonomous driving.",
          "link": "http://arxiv.org/abs/2108.06159",
          "publishedOn": "2021-08-16T00:47:33.897Z",
          "wordCount": 686,
          "title": "Robustness testing of AI systems: A case study for traffic sign recognition. (arXiv:2108.06159v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jasmine Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_V/0/1/0/all/0/1\">Viranga Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magana_A/0/1/0/all/0/1\">Alejandra J. Magana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newell_B/0/1/0/all/0/1\">Brittany Newell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Kocsis_J/0/1/0/all/0/1\">Jin Wei-Kocsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seah_Y/0/1/0/all/0/1\">Ying Ying Seah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Greg J. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Charles Xie</a>",
          "description": "Computer-aided design (CAD) programs are essential to engineering as they\nallow for better designs through low-cost iterations. While CAD programs are\ntypically taught to undergraduate students as a job skill, such software can\nalso help students learn engineering concepts. A current limitation of CAD\nprograms (even those that are specifically designed for educational purposes)\nis that they are not capable of providing automated real-time help to students.\nTo encourage CAD programs to build in assistance to students, we used data\ngenerated from students using a free, open source CAD software called Aladdin\nto demonstrate how student data combined with machine learning techniques can\npredict how well a particular student will perform in a design task. We\nchallenged students to design a house that consumed zero net energy as part of\nan introductory engineering technology undergraduate course. Using data from\n128 students, along with the scikit-learn Python machine learning library, we\ntested our models using both total counts of design actions and sequences of\ndesign actions as inputs. We found that our models using early design sequence\nactions are particularly valuable for prediction. Our logistic regression model\nachieved a >60% chance of predicting if a student would succeed in designing a\nzero net energy house. Our results suggest that it would be feasible for\nAladdin to provide useful feedback to students when they are approximately\nhalfway through their design. Further improvements to these models could lead\nto earlier predictions and thus provide students feedback sooner to enhance\ntheir learning.",
          "link": "http://arxiv.org/abs/2108.05955",
          "publishedOn": "2021-08-16T00:47:33.487Z",
          "wordCount": 703,
          "title": "Using Machine Learning to Predict Engineering Technology Students' Success with Computer Aided Design. (arXiv:2108.05955v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06338",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_L/0/1/0/all/0/1\">Lei Deng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_Y/0/1/0/all/0/1\">Yibiao Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_X/0/1/0/all/0/1\">Xuejun Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>",
          "description": "Accumulated clinical studies show that microbes living in humans interact\nclosely with human hosts, and get involved in modulating drug efficacy and drug\ntoxicity. Microbes have become novel targets for the development of\nantibacterial agents. Therefore, screening of microbe-drug associations can\nbenefit greatly drug research and development. With the increase of microbial\ngenomic and pharmacological datasets, we are greatly motivated to develop an\neffective computational method to identify new microbe-drug associations. In\nthis paper, we proposed a novel method, Graph2MDA, to predict microbe-drug\nassociations by using variational graph autoencoder (VGAE). We constructed\nmulti-modal attributed graphs based on multiple features of microbes and drugs,\nsuch as molecular structures, microbe genetic sequences, and function\nannotations. Taking as input the multi-modal attribute graphs, VGAE was trained\nto learn the informative and interpretable latent representations of each node\nand the whole graph, and then a deep neural network classifier was used to\npredict microbe-drug associations. The hyperparameter analysis and model\nablation studies showed the sensitivity and robustness of our model. We\nevaluated our method on three independent datasets and the experimental results\nshowed that our proposed method outperformed six existing state-of-the-art\nmethods. We also explored the meaningness of the learned latent representations\nof drugs and found that the drugs show obvious clustering patterns that are\nsignificantly consistent with drug ATC classification. Moreover, we conducted\ncase studies on two microbes and two drugs and found 75\\%-95\\% predicted\nassociations have been reported in PubMed literature. Our extensive performance\nevaluations validated the effectiveness of our proposed method.\\",
          "link": "http://arxiv.org/abs/2108.06338",
          "publishedOn": "2021-08-16T00:47:33.474Z",
          "wordCount": 690,
          "title": "Graph2MDA: a multi-modal variational graph embedding model for predicting microbe-drug associations. (arXiv:2108.06338v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caled_D/0/1/0/all/0/1\">Danielle Caled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_P/0/1/0/all/0/1\">Paula Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">M&#xe1;rio J. Silva</a>",
          "description": "This paper presents and characterizes MIND, a new Portuguese corpus comprised\nof different types of articles collected from online mainstream and alternative\nmedia sources, over a 10-month period. The articles in the corpus are organized\ninto five collections: facts, opinions, entertainment, satires, and conspiracy\ntheories. Throughout this paper, we explain how the data collection process was\nconducted, and present a set of linguistic metrics that allow us to perform a\npreliminary characterization of the texts included in the corpus. Also, we\ndeliver an analysis of the most frequent topics in the corpus, and discuss the\nmain differences and similarities among the collections considered. Finally, we\nenumerate some tasks and applications that could benefit from this corpus, in\nparticular the ones (in)directly related to misinformation detection. Overall,\nour contribution of a corpus and initial analysis are designed to support\nfuture exploratory news studies, and provide a better insight into\nmisinformation.",
          "link": "http://arxiv.org/abs/2108.06249",
          "publishedOn": "2021-08-16T00:47:33.469Z",
          "wordCount": 577,
          "title": "MIND - Mainstream and Independent News Documents Corpus. (arXiv:2108.06249v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.15658",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Behboodi_A/0/1/0/all/0/1\">Arash Behboodi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schnoor_E/0/1/0/all/0/1\">Ekkehard Schnoor</a>",
          "description": "Various iterative reconstruction algorithms for inverse problems can be\nunfolded as neural networks. Empirically, this approach has often led to\nimproved results, but theoretical guarantees are still scarce. While some\nprogress on generalization properties of neural networks have been made, great\nchallenges remain. In this chapter, we discuss and combine these topics to\npresent a generalization error analysis for a class of neural networks suitable\nfor sparse reconstruction from few linear measurements. The hypothesis class\nconsidered is inspired by the classical iterative soft-thresholding algorithm\n(ISTA). The neural networks in this class are obtained by unfolding iterations\nof ISTA and learning some of the weights. Based on training samples, we aim at\nlearning the optimal network parameters via empirical risk minimization and\nthereby the optimal network that reconstructs signals from their compressive\nlinear measurements. In particular, we may learn a sparsity basis that is\nshared by all of the iterations/layers and thereby obtain a new approach for\ndictionary learning. For this class of networks, we present a generalization\nbound, which is based on bounding the Rademacher complexity of hypothesis\nclasses consisting of such deep networks via Dudley's integral. Remarkably,\nunder realistic conditions, the generalization error scales only\nlogarithmically in the number of layers, and at most linear in number of\nmeasurements.",
          "link": "http://arxiv.org/abs/2010.15658",
          "publishedOn": "2021-08-16T00:47:33.435Z",
          "wordCount": 698,
          "title": "Compressive Sensing and Neural Networks from a Statistical Learning Perspective. (arXiv:2010.15658v4 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amir_G/0/1/0/all/0/1\">Guy Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schapira_M/0/1/0/all/0/1\">Michael Schapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Guy Katz</a>",
          "description": "Deep neural networks (DNNs) have gained significant popularity in recent\nyears, becoming the state of the art in a variety of domains. In particular,\ndeep reinforcement learning (DRL) has recently been employed to train DNNs that\nrealize control policies for various types of real-world systems. In this work,\nwe present the whiRL 2.0 tool, which implements a new approach for verifying\ncomplex properties of interest for DRL systems. To demonstrate the benefits of\nwhiRL 2.0, we apply it to case studies from the communication networks domain\nthat have recently been used to motivate formal verification of DRL systems,\nand which exhibit characteristics that are conducive for scalable verification.\nWe propose techniques for performing k-induction and semi-automated invariant\ninference on such systems, and leverage these techniques for proving safety and\nliveness properties that were previously impossible to verify due to the\nscalability barriers of prior approaches. Furthermore, we show how our proposed\ntechniques provide insights into the inner workings and the generalizability of\nDRL systems. whiRL 2.0 is publicly available online.",
          "link": "http://arxiv.org/abs/2105.11931",
          "publishedOn": "2021-08-16T00:47:33.423Z",
          "wordCount": 646,
          "title": "Towards Scalable Verification of Deep Reinforcement Learning. (arXiv:2105.11931v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.15050",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_J/0/1/0/all/0/1\">Junhua Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Traverso_A/0/1/0/all/0/1\">Alberto Traverso</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhovannik_I/0/1/0/all/0/1\">Ivan Zhovannik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dekker_A/0/1/0/all/0/1\">Andre Dekker</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wee_L/0/1/0/all/0/1\">Leonard Wee</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bermejo_I/0/1/0/all/0/1\">Inigo Bermejo</a>",
          "description": "Radiomics is an active area of research in medical image analysis, the low\nreproducibility of radiomics has limited its applicability to clinical\npractice. This issue is especially prominent when radiomic features are\ncalculated from noisy images, such as low dose computed tomography (CT) scans.\nIn this article, we investigate the possibility of improving the\nreproducibility of radiomic features calculated on noisy CTs by using\ngenerative models for denoising.One traditional denoising method - non-local\nmeans - and two generative models - encoder-decoder networks (EDN) and\nconditional generative adversarial networks (CGANs) - were selected as the test\nmodels. We added noise to the sinograms of full dose CTs to mimic low dose CTs\nwith two different levels of noise: low-noise CT and high-noise CT. Models were\ntrained on high-noise CTs and used to denoise low-noise CTs without\nre-training. We also test the performance of our model in real data, using\ndataset of same-day repeat low dose CTs to assess the reproducibility of\nradiomic features in denoised images. The EDN and the CGAN improved the\nconcordance correlation coefficients (CCC) of radiomic features for low-noise\nimages from 0.87 to 0.92 and for high-noise images from 0.68 to 0.92\nrespectively. Moreover, the EDN and the CGAN improved the test-retest\nreliability of radiomic features (mean CCC increased from 0.89 to 0.94) based\non real low dose CTs. The results show that denoising using EDN and CGANs can\nimprove the reproducibility of radiomic features calculated on noisy CTs.\nMoreover, images with different noise levels can be denoised to improve the\nreproducibility using these models without re-training, as long as the noise\nintensity is equal or lower than that in high-noise CTs. To the authors'\nknowledge, this is the first effort to improve the reproducibility of radiomic\nfeatures calculated on low dose CT scans.",
          "link": "http://arxiv.org/abs/2104.15050",
          "publishedOn": "2021-08-16T00:47:33.418Z",
          "wordCount": 779,
          "title": "Generative Models Improve Radiomics Reproducibility in Low Dose CTs: A Simulation Study. (arXiv:2104.15050v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hyeon_Woo_N/0/1/0/all/0/1\">Nam Hyeon-Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Bin_M/0/1/0/all/0/1\">Moon Ye-Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>",
          "description": "To overcome the burdens on frequent model uploads and downloads during\nfederated learning (FL), we propose a communication-efficient\nre-parameterization, FedPara. Our method re-parameterizes the model's layers\nusing low-rank matrices or tensors followed by the Hadamard product. Different\nfrom the conventional low-rank parameterization, our method is not limited to\nlow-rank constraints. Thereby, our FedPara has a larger capacity than the\nlow-rank one, even with the same number of parameters. It can achieve\ncomparable performance to the original models while requiring 2.8 to 10.1 times\nlower communication costs than the original models, which is not achievable by\nthe traditional low-rank parameterization. Moreover, the efficiency can be\nfurther improved by combining our method and other efficient FL techniques\nbecause our method is compatible with others. We also extend our method to a\npersonalized FL application, pFedPara, which separates parameters into global\nand local ones. We show that pFedPara outperforms competing personalized FL\nmethods with more than three times fewer parameters.",
          "link": "http://arxiv.org/abs/2108.06098",
          "publishedOn": "2021-08-16T00:47:33.384Z",
          "wordCount": 594,
          "title": "FedPara: Low-rank Hadamard Product Parameterization for Efficient Federated Learning. (arXiv:2108.06098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Seehwan Yoo</a>",
          "description": "This paper proposes a novel split learning framework with multiple\nend-systems in order to realize privacypreserving deep neural network\ncomputation. In conventional split learning frameworks, deep neural network\ncomputation is separated into multiple computing systems for hiding entire\nnetwork architectures. In our proposed framework, multiple computing\nend-systems are sharing one centralized server in split learning computation,\nwhere the multiple end-systems are with input and first hidden layers and the\ncentralized server is with the other hidden layers and output layer. This\nframework, which is called as spatio-temporal split learning, is spatially\nseparated for gathering data from multiple end-systems and also temporally\nseparated due to the nature of split learning. Our performance evaluation\nverifies that our proposed framework shows nearoptimal accuracy while\npreserving data privacy.",
          "link": "http://arxiv.org/abs/2108.06309",
          "publishedOn": "2021-08-16T00:47:33.370Z",
          "wordCount": 557,
          "title": "Spatio-Temporal Split Learning. (arXiv:2108.06309v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.13472",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vowels_M/0/1/0/all/0/1\">Matthew James Vowels</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Camgoz_N/0/1/0/all/0/1\">Necati Cihan Camgoz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "Undertaking causal inference with observational data is incredibly useful\nacross a wide range of tasks including the development of medical treatments,\nadvertisements and marketing, and policy making. There are two significant\nchallenges associated with undertaking causal inference using observational\ndata: treatment assignment heterogeneity (i.e., differences between the treated\nand untreated groups), and an absence of counterfactual data (i.e., not knowing\nwhat would have happened if an individual who did get treatment, were instead\nto have not been treated). We address these two challenges by combining\nstructured inference and targeted learning. In terms of structure, we factorize\nthe joint distribution into risk, confounding, instrumental, and miscellaneous\nfactors, and in terms of targeted learning, we apply a regularizer derived from\nthe influence curve in order to reduce residual bias. An ablation study is\nundertaken, and an evaluation on benchmark datasets demonstrates that TVAE has\ncompetitive and state of the art performance.",
          "link": "http://arxiv.org/abs/2009.13472",
          "publishedOn": "2021-08-16T00:47:33.364Z",
          "wordCount": 615,
          "title": "Targeted VAE: Variational and Targeted Learning for Causal Inference. (arXiv:2009.13472v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Termritthikun_C/0/1/0/all/0/1\">Chakkrit Termritthikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamtsho_Y/0/1/0/all/0/1\">Yeshi Jamtsho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ieamsaard_J/0/1/0/all/0/1\">Jirarat Ieamsaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muneesawang_P/0/1/0/all/0/1\">Paisarn Muneesawang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Ivan Lee</a>",
          "description": "The goals of this research were to search for Convolutional Neural Network\n(CNN) architectures, suitable for an on-device processor with limited computing\nresources, performing at substantially lower Network Architecture Search (NAS)\ncosts. A new algorithm entitled an Early Exit Population Initialisation (EE-PI)\nfor Evolutionary Algorithm (EA) was developed to achieve both goals. The EE-PI\nreduces the total number of parameters in the search process by filtering the\nmodels with fewer parameters than the maximum threshold. It will look for a new\nmodel to replace those models with parameters more than the threshold. Thereby,\nreducing the number of parameters, memory usage for model storage and\nprocessing time while maintaining the same performance or accuracy. The search\ntime was reduced to 0.52 GPU day. This is a huge and significant achievement\ncompared to the NAS of 4 GPU days achieved using NSGA-Net, 3,150 GPU days by\nthe AmoebaNet model, and the 2,000 GPU days by the NASNet model. As well, Early\nExit Evolutionary Algorithm networks (EEEA-Nets) yield network architectures\nwith minimal error and computational cost suitable for a given dataset as a\nclass of network algorithms. Using EEEA-Net on CIFAR-10, CIFAR-100, and\nImageNet datasets, our experiments showed that EEEA-Net achieved the lowest\nerror rate among state-of-the-art NAS models, with 2.46% for CIFAR-10, 15.02%\nfor CIFAR-100, and 23.8% for ImageNet dataset. Further, we implemented this\nimage recognition architecture for other tasks, such as object detection,\nsemantic segmentation, and keypoint detection tasks, and, in our experiments,\nEEEA-Net-C2 outperformed MobileNet-V3 on all of these various tasks. (The\nalgorithm code is available at https://github.com/chakkritte/EEEA-Net).",
          "link": "http://arxiv.org/abs/2108.06156",
          "publishedOn": "2021-08-16T00:47:33.357Z",
          "wordCount": 754,
          "title": "EEEA-Net: An Early Exit Evolutionary Neural Architecture Search. (arXiv:2108.06156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolova_M/0/1/0/all/0/1\">Marina Sokolova</a>",
          "description": "Reddit.com is a popular social media platform among young people. Reddit\nusers share their stories to seek support from other users, especially during\nthe Covid-19 pandemic. Messages posted on Reddit and their content have\nprovided researchers with opportunity to analyze public concerns. In this\nstudy, we analyzed sentiments of COVID-related messages posted on r/Depression.\nOur study poses the following questions: a) What are the common topics that the\nReddit users discuss? b) Can we use these topics to classify sentiments of the\nposts? c) What matters concern people more during the pandemic?\n\nKey Words: Sentiment Classification, Depression, COVID-19, Reddit, LDA, BERT",
          "link": "http://arxiv.org/abs/2108.06215",
          "publishedOn": "2021-08-16T00:47:33.351Z",
          "wordCount": 598,
          "title": "Sentiment Analysis of the COVID-related r/Depression Posts. (arXiv:2108.06215v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yicheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1\">Bingchen Fan</a>",
          "description": "A combinatorial cost function for hierarchical clustering was introduced by\nDasgupta \\cite{dasgupta2016cost}. It has been generalized by Cohen-Addad et al.\n\\cite{cohen2019hierarchical} to a general form named admissible function. In\nthis paper, we investigate hierarchical clustering from the\n\\emph{information-theoretic} perspective and formulate a new objective\nfunction. We also establish the relationship between these two perspectives. In\nalgorithmic aspect, we get rid of the traditional top-down and bottom-up\nframeworks, and propose a new one to stratify the \\emph{sparsest} level of a\ncluster tree recursively in guide with our objective function. For practical\nuse, our resulting cluster tree is not binary. Our algorithm called HCSE\noutputs a $k$-level cluster tree by a novel and interpretable mechanism to\nchoose $k$ automatically without any hyper-parameter. Our experimental results\non synthetic datasets show that HCSE has a great advantage in finding the\nintrinsic number of hierarchies, and the results on real datasets show that\nHCSE also achieves competitive costs over the popular algorithms LOUVAIN and\nHLP.",
          "link": "http://arxiv.org/abs/2108.06036",
          "publishedOn": "2021-08-16T00:47:33.345Z",
          "wordCount": 583,
          "title": "An Information-theoretic Perspective of Hierarchical Clustering. (arXiv:2108.06036v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tizikara_D/0/1/0/all/0/1\">Dativa K. Tizikara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serugunda_J/0/1/0/all/0/1\">Jonathan Serugunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katumba_A/0/1/0/all/0/1\">Andrew Katumba</a>",
          "description": "Future communication systems are faced with increased demand for high\ncapacity, dynamic bandwidth, reliability and heterogeneous traffic. To meet\nthese requirements, networks have become more complex and thus require new\ndesign methods and monitoring techniques, as they evolve towards becoming\nautonomous. Machine learning has come to the forefront in recent years as a\npromising technology to aid in this evolution. Optical fiber communications can\nalready provide the high capacity required for most applications, however,\nthere is a need for increased scalability and adaptability to changing user\ndemands and link conditions. Accurate performance monitoring is an integral\npart of this transformation. In this paper we review optical performance\nmonitoring techniques where machine learning algorithms have been applied.\nMoreover, since alot of OPM depends on knowledge of the signal type, we also\nreview work for modulation format recognition and bitrate identification. We\nadditionally briefly introduce a neuromorphic approach to OPM as an emerging\ntechnique that has only recently been applied to this domain.",
          "link": "http://arxiv.org/abs/2107.07338",
          "publishedOn": "2021-08-16T00:47:33.154Z",
          "wordCount": 622,
          "title": "An Overview of Machine Learning-aided Optical Performance Monitoring Techniques. (arXiv:2107.07338v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>",
          "description": "How to discriminatively vectorize graphs is a fundamental challenge that\nattracts increasing attentions in recent years. Inspired by the recent success\nof unsupervised contrastive learning, we aim to learn graph-level\nrepresentation in an unsupervised manner. Specifically, we propose a novel\nunsupervised graph learning paradigm called Iterative Graph Self-Distillation\n(IGSD) which iteratively performs the teacher-student distillation with graph\naugmentations. Different from conventional knowledge distillation, IGSD\nconstructs the teacher with an exponential moving average of the student model\nand distills the knowledge of itself. The intuition behind IGSD is to predict\nthe teacher network representation of the graph pairs under different augmented\nviews. As a natural extension, we also apply IGSD to semi-supervised scenarios\nby jointly regularizing the network with both supervised and unsupervised\ncontrastive loss. Finally, we show that finetuning the IGSD-trained models with\nself-training can further improve the graph representation power. Empirically,\nwe achieve significant and consistent performance gain on various graph\ndatasets in both unsupervised and semi-supervised settings, which well\nvalidates the superiority of IGSD.",
          "link": "http://arxiv.org/abs/2010.12609",
          "publishedOn": "2021-08-16T00:47:33.131Z",
          "wordCount": 633,
          "title": "Iterative Graph Self-Distillation. (arXiv:2010.12609v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16336",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1\">Emily M. Goren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.",
          "link": "http://arxiv.org/abs/2103.16336",
          "publishedOn": "2021-08-16T00:47:33.051Z",
          "wordCount": 680,
          "title": "Fast model-based clustering of partial records. (arXiv:2103.16336v5 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "Motivated by the success of masked language modeling~(MLM) in pre-training\nnatural language processing models, we propose w2v-BERT that explores MLM for\nself-supervised speech representation learning. w2v-BERT is a framework that\ncombines contrastive learning and MLM, where the former trains the model to\ndiscretize input continuous speech signals into a finite set of discriminative\nspeech tokens, and the latter trains the model to learn contextualized speech\nrepresentations via solving a masked prediction task consuming the discretized\ntokens. In contrast to existing MLM-based speech pre-training frameworks such\nas HuBERT, which relies on an iterative re-clustering and re-training process,\nor vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can\nbe optimized in an end-to-end fashion by solving the two self-supervised\ntasks~(the contrastive task and MLM) simultaneously. Our experiments show that\nw2v-BERT achieves competitive results compared to current state-of-the-art\npre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k\ncorpus as the unsupervised data. In particular, when compared to published\nmodels such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\\%\nto~10\\% relative WER reduction on the test-clean and test-other subsets. When\napplied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our\ninternal conformer-based wav2vec~2.0 by more than~30\\% relatively.",
          "link": "http://arxiv.org/abs/2108.06209",
          "publishedOn": "2021-08-16T00:47:33.004Z",
          "wordCount": 649,
          "title": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training. (arXiv:2108.06209v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dietrich_A/0/1/0/all/0/1\">Anastasia Dietrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gressmann_F/0/1/0/all/0/1\">Frithjof Gressmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_D/0/1/0/all/0/1\">Douglas Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelombiev_I/0/1/0/all/0/1\">Ivan Chelombiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justus_D/0/1/0/all/0/1\">Daniel Justus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>",
          "description": "Identifying algorithms for computational efficient unsupervised training of\nlarge language models is an important and active area of research. In this\nwork, we develop and study a straightforward, dynamic always-sparse\npre-training approach for BERT language modeling task, which leverages periodic\ncompression steps based on magnitude pruning followed by random parameter\nre-allocation. This approach enables us to achieve Pareto improvements in terms\nof the number of floating-point operations (FLOPs) over statically sparse and\ndense models across a broad spectrum of network sizes. Furthermore, we\ndemonstrate that training remains FLOP-efficient when using coarse-grained\nblock sparsity, making it particularly promising for efficient execution on\nmodern hardware accelerators.",
          "link": "http://arxiv.org/abs/2108.06277",
          "publishedOn": "2021-08-16T00:47:32.983Z",
          "wordCount": 541,
          "title": "Towards Structured Dynamic Sparse Pre-Training of BERT. (arXiv:2108.06277v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_P/0/1/0/all/0/1\">Prithiviraj Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhkaran Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achankuju_J/0/1/0/all/0/1\">Josemon Achankuju</a>",
          "description": "We present VESPA, an intentionally simple yet novel zero-shot system for\nlayout, locale, and domain agnostic document extraction. In spite of the\navailability of large corpora of documents, the lack of labeled and validated\ndatasets makes it a challenge to discriminatively train document extraction\nmodels for enterprises. We show that this problem can be addressed by simply\ntransferring the information extraction (IE) task to a natural language\nQuestion-Answering (QA) task without engineering task-specific architectures.\nWe demonstrate the effectiveness of our system by evaluating on a closed corpus\nof real-world retail and tax invoices with multiple complex layouts, domains,\nand geographies. The empirical evaluation shows that our system outperforms 4\nprominent commercial invoice solutions that use discriminatively trained models\nwith architectures specifically crafted for invoice extraction. We extracted 6\nfields with zero upfront human annotation or training with an Avg. F1 of 87.50.",
          "link": "http://arxiv.org/abs/2108.06069",
          "publishedOn": "2021-08-16T00:47:32.977Z",
          "wordCount": 585,
          "title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA Ensemble. (arXiv:2108.06069v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gnecco_G/0/1/0/all/0/1\">Giorgio Gnecco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacigalupo_A/0/1/0/all/0/1\">Andrea Bacigalupo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fantoni_F/0/1/0/all/0/1\">Francesca Fantoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvi_D/0/1/0/all/0/1\">Daniela Selvi</a>",
          "description": "A promising technique for the spectral design of acoustic metamaterials is\nbased on the formulation of suitable constrained nonlinear optimization\nproblems. Unfortunately, the straightforward application of classical\ngradient-based iterative optimization algorithms to the numerical solution of\nsuch problems is typically highly demanding, due to the complexity of the\nunderlying physical models. Nevertheless, supervised machine learning\ntechniques can reduce such a computational effort, e.g., by replacing the\noriginal objective functions of such optimization problems with more-easily\ncomputable approximations. In this framework, the present article describes the\napplication of a related unsupervised machine learning technique, namely,\nprincipal component analysis, to approximate the gradient of the objective\nfunction of a band gap optimization problem for an acoustic metamaterial, with\nthe aim of making the successive application of a gradient-based iterative\noptimization algorithm faster. Numerical results show the effectiveness of the\nproposed method.",
          "link": "http://arxiv.org/abs/2104.02588",
          "publishedOn": "2021-08-16T00:47:32.962Z",
          "wordCount": 662,
          "title": "Principal Component Analysis Applied to Gradient Fields in Band Gap Optimization Problems for Metamaterials. (arXiv:2104.02588v6 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets, often with better accuracy and fewer\nparameters. Our model eliminates the requirement for class token and positional\nembeddings through a novel sequence pooling strategy and the use of\nconvolution/s. It is flexible in terms of model size, and can have as little as\n0.28M parameters while achieving good results. Our model can reach 98.00%\naccuracy when training from scratch on CIFAR-10, which is a significant\nimprovement over previous Transformer based models. It also outperforms many\nmodern CNN based approaches, such as ResNet, and even some recent NAS-based\napproaches, such as Proxyless-NAS. Our simple and compact design democratizes\ntransformers by making them accessible to those with limited computing\nresources and/or dealing with small datasets. Our method also works on larger\ndatasets, such as ImageNet (82.71% accuracy with 29% parameters of ViT), and\nNLP tasks as well. Our code and pre-trained models are publicly available at\nhttps://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-08-16T00:47:32.955Z",
          "wordCount": 760,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1\">Hassan Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_W/0/1/0/all/0/1\">Wei-Hong Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>",
          "description": "We present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Specifically, our\nVideo-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts\nmultimodal representations that are rich enough to benefit a variety of\ndownstream tasks. We train VATT end-to-end from scratch using multimodal\ncontrastive losses and evaluate its performance by the downstream tasks of\nvideo action recognition, audio event classification, image classification, and\ntext-to-video retrieval. Furthermore, we study a modality-agnostic\nsingle-backbone Transformer by sharing weights among the three modalities. We\nshow that the convolution-free VATT outperforms state-of-the-art ConvNet-based\narchitectures in the downstream tasks. Especially, VATT's vision Transformer\nachieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and\n41.1% on Moments in Time, new records while avoiding supervised pre-training.\nTransferring to image classification leads to 78.7% top-1 accuracy on ImageNet\ncompared to 64.7% by training the same Transformer from scratch, showing the\ngeneralizability of our model despite the domain gap between videos and images.\nVATT's audio Transformer also sets a new record on waveform-based audio event\nrecognition by achieving the mAP of 39.4% on AudioSet without any supervised\npre-training. VATT's source code is publicly available.",
          "link": "http://arxiv.org/abs/2104.11178",
          "publishedOn": "2021-08-16T00:47:32.949Z",
          "wordCount": 689,
          "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text. (arXiv:2104.11178v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Faccio_F/0/1/0/all/0/1\">Francesco Faccio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_L/0/1/0/all/0/1\">Louis Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "Traditional off-policy actor-critic Reinforcement Learning (RL) algorithms\nlearn value functions of a single target policy. However, when value functions\nare updated to track the learned policy, they forget potentially useful\ninformation about old policies. We introduce a class of value functions called\nParameter-Based Value Functions (PBVFs) whose inputs include the policy\nparameters. They can generalize across different policies. PBVFs can evaluate\nthe performance of any policy given a state, a state-action pair, or a\ndistribution over the RL agent's initial states. First we show how PBVFs yield\nnovel off-policy policy gradient theorems. Then we derive off-policy\nactor-critic algorithms based on PBVFs trained by Monte Carlo or Temporal\nDifference methods. We show how learned PBVFs can zero-shot learn new policies\nthat outperform any policy seen during training. Finally our algorithms are\nevaluated on a selection of discrete and continuous control tasks using shallow\npolicies and deep neural networks. Their performance is comparable to\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2006.09226",
          "publishedOn": "2021-08-16T00:47:32.942Z",
          "wordCount": 635,
          "title": "Parameter-Based Value Functions. (arXiv:2006.09226v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12756",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Beck_E/0/1/0/all/0/1\">Edgar Beck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bockelmann_C/0/1/0/all/0/1\">Carsten Bockelmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dekorsy_A/0/1/0/all/0/1\">Armin Dekorsy</a>",
          "description": "Following the great success of Machine Learning (ML), especially Deep Neural\nNetworks (DNNs), in many research domains in 2010s, several ML-based approaches\nwere proposed for detection in large inverse linear problems, e.g., massive\nMIMO systems. The main motivation behind is that the complexity of Maximum\nA-Posteriori (MAP) detection grows exponentially with system dimensions.\nInstead of using DNNs, essentially being a black-box, we take a slightly\ndifferent approach and introduce a probabilistic Continuous relaxation of\ndisCrete variables to MAP detection. Enabling close approximation and\ncontinuous optimization, we derive an iterative detection algorithm: Concrete\nMAP Detection (CMD). Furthermore, extending CMD by the idea of deep unfolding\ninto CMDNet, we allow for (online) optimization of a small number of parameters\nto different working points while limiting complexity. In contrast to recent\nDNN-based approaches, we select the optimization criterion and output of CMDNet\nbased on information theory and are thus able to learn approximate\nprobabilities of the individual optimal detector. This is crucial for soft\ndecoding in today's communication systems. Numerical simulation results in MIMO\nsystems reveal CMDNet to feature a promising accuracy complexity trade-off\ncompared to State of the Art. Notably, we demonstrate CMDNet's soft outputs to\nbe reliable for decoders.",
          "link": "http://arxiv.org/abs/2102.12756",
          "publishedOn": "2021-08-16T00:47:32.934Z",
          "wordCount": 692,
          "title": "CMDNet: Learning a Probabilistic Relaxation of Discrete Variables for Soft Detection with Low Complexity. (arXiv:2102.12756v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brunke_L/0/1/0/all/0/1\">Lukas Brunke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greeff_M/0/1/0/all/0/1\">Melissa Greeff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_A/0/1/0/all/0/1\">Adam W. Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhaocong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Siqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panerati_J/0/1/0/all/0/1\">Jacopo Panerati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoellig_A/0/1/0/all/0/1\">Angela P. Schoellig</a> (University of Toronto Institute for Aerospace Studies, University of Toronto Robotics Institute, Vector Institute for Artificial Intelligence)",
          "description": "The last half-decade has seen a steep rise in the number of contributions on\nsafe learning methods for real-world robotic deployments from both the control\nand reinforcement learning communities. This article provides a concise but\nholistic review of the recent advances made in using machine learning to\nachieve safe decision making under uncertainties, with a focus on unifying the\nlanguage and frameworks used in control theory and reinforcement learning\nresearch. Our review includes: learning-based control approaches that safely\nimprove performance by learning the uncertain dynamics, reinforcement learning\napproaches that encourage safety or robustness, and methods that can formally\ncertify the safety of a learned control policy. As data- and learning-based\nrobot control methods continue to gain traction, researchers must understand\nwhen and how to best leverage them in real-world scenarios where safety is\nimperative, such as when operating in close proximity to humans. We highlight\nsome of the open challenges that will drive the field of robot learning in the\ncoming years, and emphasize the need for realistic physics-based benchmarks to\nfacilitate fair comparisons between control and reinforcement learning\napproaches.",
          "link": "http://arxiv.org/abs/2108.06266",
          "publishedOn": "2021-08-16T00:47:32.898Z",
          "wordCount": 655,
          "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning. (arXiv:2108.06266v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Savkli_C/0/1/0/all/0/1\">Cetin Savkli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_C/0/1/0/all/0/1\">Catherine Schwartz</a>",
          "description": "We present a new subspace-based method to construct probabilistic models for\nhigh-dimensional data and highlight its use in anomaly detection. The approach\nis based on a statistical estimation of probability density using densities of\nrandom subspaces combined with geometric averaging. In selecting random\nsubspaces, equal representation of each attribute is used to ensure correct\nstatistical limits. Gaussian mixture models (GMMs) are used to create the\nprobability densities for each subspace with techniques included to mitigate\nsingularities allowing for the ability to handle both numerical and categorial\nattributes. The number of components for each GMM is determined automatically\nthrough Bayesian information criterion to prevent overfitting. The proposed\nalgorithm attains competitive AUC scores compared with prominent algorithms\nagainst benchmark anomaly detection datasets with the added benefits of being\nsimple, scalable, and interpretable.",
          "link": "http://arxiv.org/abs/2108.06283",
          "publishedOn": "2021-08-16T00:47:32.888Z",
          "wordCount": 581,
          "title": "Random Subspace Mixture Models for Interpretable Anomaly Detection. (arXiv:2108.06283v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_E/0/1/0/all/0/1\">Erzhuo Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Fine-grained population distribution data is of great importance for many\napplications, e.g., urban planning, traffic scheduling, epidemic modeling, and\nrisk control. However, due to the limitations of data collection, including\ninfrastructure density, user privacy, and business security, such fine-grained\ndata is hard to collect and usually, only coarse-grained data is available.\nThus, obtaining fine-grained population distribution from coarse-grained\ndistribution becomes an important problem. To complete this task, existing\nmethods mainly rely on sufficient fine-grained ground truth for training, which\nis not often available. This limits the applications of these methods and\nbrings the necessity to transfer knowledge from data-sufficient cities to\ndata-scarce cities.\n\nIn knowledge transfer scenario, we employ single reference fine-grained\nground truth in the target city as the ground truth to inform the large-scale\nurban structure and support the knowledge transfer in the target city. By this\napproach, we transform the fine-grained population mapping problem into a\none-shot transfer learning problem for population mapping task.\n\nIn this paper, we propose a one-shot transfer learning framework, PSRNet, to\ntransfer spatial-temporal knowledge across cities in fine-grained population\nmapping task from the view of network structure, data, and optimization.\nExperiments on real-life datasets of 4 cities demonstrate that PSRNet has\nsignificant advantages over 8 baselines by reducing RMSE and MAE for more than\n25%. Our code and datasets are released in Github.",
          "link": "http://arxiv.org/abs/2108.06228",
          "publishedOn": "2021-08-16T00:47:32.879Z",
          "wordCount": 653,
          "title": "One-shot Transfer Learning for Population Mapping. (arXiv:2108.06228v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malekmohammadi_S/0/1/0/all/0/1\">Saber Malekmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaloudegi_K/0/1/0/all/0/1\">Kiarash Shaloudegi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zeou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaoliang Yu</a>",
          "description": "Over the past few years, the federated learning ($\\texttt{FL}$) community has\nwitnessed a proliferation of new $\\texttt{FL}$ algorithms. However, our\nunderstating of the theory of $\\texttt{FL}$ is still fragmented, and a\nthorough, formal comparison of these algorithms remains elusive. Motivated by\nthis gap, we show that many of the existing $\\texttt{FL}$ algorithms can be\nunderstood from an operator splitting point of view. This unification allows us\nto compare different algorithms with ease, to refine previous convergence\nresults and to uncover new algorithmic variants. In particular, our analysis\nreveals the vital role played by the step size in $\\texttt{FL}$ algorithms. The\nunification also leads to a streamlined and economic way to accelerate\n$\\texttt{FL}$ algorithms, without incurring any communication overhead. We\nperform numerical experiments on both convex and nonconvex models to validate\nour findings.",
          "link": "http://arxiv.org/abs/2108.05974",
          "publishedOn": "2021-08-16T00:47:32.874Z",
          "wordCount": 565,
          "title": "An Operator Splitting View of Federated Learning. (arXiv:2108.05974v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.13268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjal Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatziafratis_V/0/1/0/all/0/1\">Vaggos Chatziafratis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayaraghavan_A/0/1/0/all/0/1\">Aravindan Vijayaraghavan</a>",
          "description": "Many machine learning systems are vulnerable to small perturbations made to\ninputs either at test time or at training time. This has received much recent\ninterest on the empirical front due to applications where reliability and\nsecurity are critical. However, theoretical understanding of algorithms that\nare robust to adversarial perturbations is limited.\n\nIn this work we focus on Principal Component Analysis (PCA), a ubiquitous\nalgorithmic primitive in machine learning. We formulate a natural robust\nvariant of PCA where the goal is to find a low dimensional subspace to\nrepresent the given data with minimum projection error, that is in addition\nrobust to small perturbations measured in $\\ell_q$ norm (say $q=\\infty$).\nUnlike PCA which is solvable in polynomial time, our formulation is\ncomputationally intractable to optimize as it captures a variant of the\nwell-studied sparse PCA objective as a special case. We show the following\nresults:\n\n-Polynomial time algorithm that is constant factor competitive in the\nworst-case with respect to the best subspace, in terms of the projection error\nand the robustness criterion.\n\n-We show that our algorithmic techniques can also be made robust to\nadversarial training-time perturbations, in addition to yielding\nrepresentations that are robust to adversarial perturbations at test time.\nSpecifically, we design algorithms for a strong notion of training-time\nperturbations, where every point is adversarially perturbed up to a specified\namount.\n\n-We illustrate the broad applicability of our algorithmic techniques in\naddressing robustness to adversarial perturbations, both at training time and\ntest time. In particular, our adversarially robust PCA primitive leads to\ncomputationally efficient and robust algorithms for both unsupervised and\nsupervised learning problems such as clustering and learning adversarially\nrobust classifiers.",
          "link": "http://arxiv.org/abs/1911.13268",
          "publishedOn": "2021-08-16T00:47:32.869Z",
          "wordCount": 760,
          "title": "Adversarially Robust Low Dimensional Representations. (arXiv:1911.13268v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_V/0/1/0/all/0/1\">Vasileios Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "Data-driven learning algorithms are employed in many online applications, in\nwhich data become available over time, like network monitoring, stock price\nprediction, job applications, etc. The underlying data distribution might\nevolve over time calling for model adaptation as new instances arrive and old\ninstances become obsolete. In such dynamic environments, the so-called data\nstreams, fairness-aware learning cannot be considered as a one-off requirement,\nbut rather it should comprise a continual requirement over the stream. Recent\nfairness-aware stream classifiers ignore the problem of class imbalance, which\nmanifests in many real-life applications, and mitigate discrimination mainly\nbecause they \"reject\" minority instances at large due to their inability to\neffectively learn all classes.\n\nIn this work, we propose \\ours, an online fairness-aware approach that\nmaintains a valid and fair classifier over the stream. \\ours~is an online\nboosting approach that changes the training distribution in an online fashion\nby monitoring stream's class imbalance and tweaks its decision boundary to\nmitigate discriminatory outcomes over the stream. Experiments on 8 real-world\nand 1 synthetic datasets from different domains with varying class imbalance\ndemonstrate the superiority of our method over state-of-the-art fairness-aware\nstream approaches with a range (relative) increase [11.2\\%-14.2\\%] in balanced\naccuracy, [22.6\\%-31.8\\%] in gmean, [42.5\\%-49.6\\%] in recall, [14.3\\%-25.7\\%]\nin kappa and [89.4\\%-96.6\\%] in statistical parity (fairness).",
          "link": "http://arxiv.org/abs/2108.06231",
          "publishedOn": "2021-08-16T00:47:32.851Z",
          "wordCount": 648,
          "title": "Online Fairness-Aware Learning with Imbalanced Data Streams. (arXiv:2108.06231v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parasrampuria_R/0/1/0/all/0/1\">Rohan Parasrampuria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Suchandra Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Dhrubasish Sarkar</a>",
          "description": "In today's tech-savvy world every industry is trying to formulate methods for\nrecommending products by combining several techniques and algorithms to form a\npool that would bring forward the most enhanced models for making the\npredictions. Building on these lines is our paper focused on the application of\nsentiment analysis for recommendation in the insurance domain. We tried\nbuilding the following Machine Learning models namely, Logistic Regression,\nMultinomial Naive Bayes, and the mighty Random Forest for analyzing the\npolarity of a given feedback line given by a customer. Then we used this\npolarity along with other attributes like Age, Gender, Locality, Income, and\nthe list of other products already purchased by our existing customers as input\nfor our recommendation model. Then we matched the polarity score along with the\nuser's profiles and generated the list of insurance products to be recommended\nin descending order. Despite our model's simplicity and the lack of the key\ndata sets, the results seemed very logical and realistic. So, by developing the\nmodel with more enhanced methods and with access to better and true data\ngathered from an insurance industry may be the sector could be very well\nbenefitted from the amalgamation of sentiment analysis with a recommendation.",
          "link": "http://arxiv.org/abs/2108.06210",
          "publishedOn": "2021-08-16T00:47:32.845Z",
          "wordCount": 642,
          "title": "Recommending Insurance products by using Users' Sentiments. (arXiv:2108.06210v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2005.08859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Filom_K/0/1/0/all/0/1\">Khashayar Filom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kording_K/0/1/0/all/0/1\">Konrad Paul Kording</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhoodi_R/0/1/0/all/0/1\">Roozbeh Farhoodi</a>",
          "description": "Neural networks are versatile tools for computation, having the ability to\napproximate a broad range of functions. An important problem in the theory of\ndeep neural networks is expressivity; that is, we want to understand the\nfunctions that are computable by a given network. We study real infinitely\ndifferentiable (smooth) hierarchical functions implemented by feedforward\nneural networks via composing simpler functions in two cases:\n\n1) each constituent function of the composition has fewer inputs than the\nresulting function;\n\n2) constituent functions are in the more specific yet prevalent form of a\nnon-linear univariate function (e.g. tanh) applied to a linear multivariate\nfunction.\n\nWe establish that in each of these regimes there exist non-trivial algebraic\npartial differential equations (PDEs), which are satisfied by the computed\nfunctions. These PDEs are purely in terms of the partial derivatives and are\ndependent only on the topology of the network. For compositions of polynomial\nfunctions, the algebraic PDEs yield non-trivial equations (of degrees dependent\nonly on the architecture) in the ambient polynomial space that are satisfied on\nthe associated functional varieties. Conversely, we conjecture that such PDE\nconstraints, once accompanied by appropriate non-singularity conditions and\nperhaps certain inequalities involving partial derivatives, guarantee that the\nsmooth function under consideration can be represented by the network. The\nconjecture is verified in numerous examples including the case of tree\narchitectures which are of neuroscientific interest. Our approach is a step\ntoward formulating an algebraic description of functional spaces associated\nwith specific neural networks, and may provide new, useful tools for\nconstructing neural networks.",
          "link": "http://arxiv.org/abs/2005.08859",
          "publishedOn": "2021-08-16T00:47:32.836Z",
          "wordCount": 737,
          "title": "PDE constraints on smooth hierarchical functions computed by neural networks. (arXiv:2005.08859v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12758",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Dhadphale_J/0/1/0/all/0/1\">Jayesh Dhadphale</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Unni_V/0/1/0/all/0/1\">Vishnu R. Unni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saha_A/0/1/0/all/0/1\">Abhishek Saha</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sujith_R/0/1/0/all/0/1\">R. I. Sujith</a>",
          "description": "In reacting flow systems, thermoacoustic instability characterized by high\namplitude pressure fluctuations, is driven by a positive coupling between the\nunsteady heat release rate and the acoustic field of the combustor. When the\nunderlying flow is turbulent, as a control parameter of the system is varied\nand the system approach thermoacoustic instability, the acoustic pressure\noscillations synchronize with heat release rate oscillations. Consequently,\nduring the onset of thermoacoustic instability in turbulent combustors, the\nsystem dynamics transition from chaotic oscillations to periodic oscillations\nvia a state of intermittency. Thermoacoustic systems are traditionally modeled\nby coupling the model for the unsteady heat source and the acoustic subsystem,\neach estimated independently. The response of the unsteady heat source, the\nflame, to acoustic fluctuations are characterized by introducing external\nunsteady forcing. This necessitates a powerful excitation module to obtain the\nnonlinear response of the flame to acoustic perturbations. Instead of\ncharacterizing individual subsystems, we introduce a neural ordinary\ndifferential equation (neural ODE) framework to model the thermoacoustic system\nas a whole. The neural ODE model for the thermoacoustic system uses time series\nof the heat release rate and the pressure fluctuations, measured simultaneously\nwithout introducing any external perturbations, to model their coupled\ninteraction. Further, we use the parameters of neural ODE to define an anomaly\nmeasure that represents the proximity of system dynamics to limit cycle\noscillations and thus provide an early warning signal for the onset of\nthermoacoustic instability.",
          "link": "http://arxiv.org/abs/2106.12758",
          "publishedOn": "2021-08-16T00:47:32.821Z",
          "wordCount": 699,
          "title": "Neural ODE to model and prognose thermoacoustic instability. (arXiv:2106.12758v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01779",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Lu_Y/0/1/0/all/0/1\">Ying Lu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_Y/0/1/0/all/0/1\">Yue-Min Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhou_P/0/1/0/all/0/1\">Peng-Fei Zhou</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1\">Shi-Ju Ran</a>",
          "description": "State preparation is of fundamental importance in quantum physics, which can\nbe realized by constructing the quantum circuit as a unitary that transforms\nthe initial state to the target, or implementing a quantum control protocol to\nevolve to the target state with a designed Hamiltonian. In this work, we study\nthe latter on quantum many-body systems by the time evolution with fixed\ncouplings and variational magnetic fields. In specific, we consider to prepare\nthe ground states of the Hamiltonians containing certain interactions that are\nmissing in the Hamiltonians for the time evolution. An optimization method is\nproposed to optimize the magnetic fields by \"fine-graining\" the discretization\nof time, in order to gain high precision and stability. The back propagation\ntechnique is utilized to obtain the gradients of the fields against the\nlogarithmic fidelity. Our method is tested on preparing the ground state of\nHeisenberg chain with the time evolution by the XY and Ising interactions, and\nits performance surpasses two baseline methods that use local and global\noptimization strategies, respectively. Our work can be applied and generalized\nto other quantum models such as those defined on higher dimensional lattices.\nIt enlightens to reduce the complexity of the required interactions for\nimplementing quantum control or other tasks in quantum information and\ncomputation by means of optimizing the magnetic fields.",
          "link": "http://arxiv.org/abs/2106.01779",
          "publishedOn": "2021-08-16T00:47:32.816Z",
          "wordCount": 690,
          "title": "Preparation of Many-body Ground States by Time Evolution with Variational Microscopic Magnetic Fields and Incomplete Interactions. (arXiv:2106.01779v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.04840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjal Awasthi</a>",
          "description": "We study active learning of homogeneous $s$-sparse halfspaces in\n$\\mathbb{R}^d$ under the setting where the unlabeled data distribution is\nisotropic log-concave and each label is flipped with probability at most $\\eta$\nfor a parameter $\\eta \\in \\big[0, \\frac12\\big)$, known as the bounded noise.\nEven in the presence of mild label noise, i.e. $\\eta$ is a small constant, this\nis a challenging problem and only recently have label complexity bounds of the\nform $\\tilde{O}\\big(s \\cdot \\mathrm{polylog}(d, \\frac{1}{\\epsilon})\\big)$ been\nestablished in [Zhang, 2018] for computationally efficient algorithms. In\ncontrast, under high levels of label noise, the label complexity bounds\nachieved by computationally efficient algorithms are much worse: the best known\nresult of [Awasthi et al., 2016] provides a computationally efficient algorithm\nwith label complexity $\\tilde{O}\\big((\\frac{s \\ln\nd}{\\epsilon})^{2^{\\mathrm{poly}(1/(1-2\\eta))}} \\big)$, which is label-efficient\nonly when the noise rate $\\eta$ is a fixed constant. In this work, we\nsubstantially improve on it by designing a polynomial time algorithm for active\nlearning of $s$-sparse halfspaces, with a label complexity of\n$\\tilde{O}\\big(\\frac{s}{(1-2\\eta)^4} \\mathrm{polylog} (d, \\frac 1 \\epsilon)\n\\big)$. This is the first efficient algorithm with label complexity polynomial\nin $\\frac{1}{1-2\\eta}$ in this setting, which is label-efficient even for\n$\\eta$ arbitrarily close to $\\frac12$. Our active learning algorithm and its\ntheoretical guarantees also immediately translate to new state-of-the-art label\nand sample complexity results for full-dimensional active and passive halfspace\nlearning under arbitrary bounded noise. The key insight of our algorithm and\nanalysis is a new interpretation of online learning regret inequalities, which\nmay be of independent interest.",
          "link": "http://arxiv.org/abs/2002.04840",
          "publishedOn": "2021-08-16T00:47:32.801Z",
          "wordCount": 730,
          "title": "Efficient active learning of sparse halfspaces with arbitrary bounded noise. (arXiv:2002.04840v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kucherenko_T/0/1/0/all/0/1\">Taras Kucherenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_R/0/1/0/all/0/1\">Rajmund Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonell_P/0/1/0/all/0/1\">Patrik Jonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_M/0/1/0/all/0/1\">Michael Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>",
          "description": "We propose a new framework for gesture generation, aiming to allow\ndata-driven approaches to produce more semantically rich gestures. Our approach\nfirst predicts whether to gesture, followed by a prediction of the gesture\nproperties. Those properties are then used as conditioning for a modern\nprobabilistic gesture-generation model capable of high-quality output. This\nempowers the approach to generate gestures that are both diverse and\nrepresentational. Follow-ups and more information can be found on the project\npage: https://svito-zar.github.io/speech2properties2gestures/ .",
          "link": "http://arxiv.org/abs/2106.14736",
          "publishedOn": "2021-08-16T00:47:32.776Z",
          "wordCount": 587,
          "title": "Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for Generating Representational Gestures from Speech. (arXiv:2106.14736v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1\">Pavan Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1\">Alexander Lerch</a>",
          "description": "Several automatic approaches for objective music performance assessment (MPA)\nhave been proposed in the past, however, existing systems are not yet capable\nof reliably predicting ratings with the same accuracy as professional judges.\nThis study investigates contrastive learning as a potential method to improve\nexisting MPA systems. Contrastive learning is a widely used technique in\nrepresentation learning to learn a structured latent space capable of\nseparately clustering multiple classes. It has been shown to produce state of\nthe art results for image-based classification problems. We introduce a\nweighted contrastive loss suitable for regression tasks applied to a\nconvolutional neural network and show that contrastive loss results in\nperformance gains in regression tasks for MPA. Our results show that\ncontrastive-based methods are able to match and exceed SoTA performance for MPA\nregression tasks by creating better class clusters within the latent space of\nthe neural networks.",
          "link": "http://arxiv.org/abs/2108.01711",
          "publishedOn": "2021-08-16T00:47:32.771Z",
          "wordCount": 595,
          "title": "Improving Music Performance Assessment with Contrastive Learning. (arXiv:2108.01711v1 [cs.SD] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">Jan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhulai_S/0/1/0/all/0/1\">Sandjai Bhulai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1\">Mark Hoogendoorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_R/0/1/0/all/0/1\">Rob van der Mei</a>",
          "description": "Over the past decade, the advent of cybercrime has accelarated the research\non cybersecurity. However, the deployment of intrusion detection methods falls\nshort. One of the reasons for this is the lack of realistic evaluation\ndatasets, which makes it a challenge to develop techniques and compare them.\nThis is caused by the large amounts of effort it takes for a cyber analyst to\nclassify network connections. This has raised the need for methods (i) that can\nlearn from small sets of labeled data, (ii) that can make predictions on large\nsets of unlabeled data, and (iii) that request the label of only specially\nselected unlabeled data instances. Hence, Active Learning (AL) methods are of\ninterest. These approaches choose speci?fic unlabeled instances by a query\nfunction that are expected to improve overall classi?cation performance. The\nresulting query observations are labeled by a human expert and added to the\nlabeled set.\n\nIn this paper, we propose a new hybrid AL method called Jasmine. Firstly, it\ndetermines how suitable each observation is for querying, i.e., how likely it\nis to enhance classi?cation. These properties are the uncertainty score and\nanomaly score. Secondly, Jasmine introduces dynamic updating. This allows the\nmodel to adjust the balance between querying uncertain, anomalous and randomly\nselected observations. To this end, Jasmine is able to learn the best query\nstrategy during the labeling process. This is in contrast to the other AL\nmethods in cybersecurity that all have static, predetermined query functions.\nWe show that dynamic updating, and therefore Jasmine, is able to consistently\nobtain good and more robust results than querying only uncertainties, only\nanomalies or a ?fixed combination of the two.",
          "link": "http://arxiv.org/abs/2108.06238",
          "publishedOn": "2021-08-16T00:47:32.766Z",
          "wordCount": 718,
          "title": "Jasmine: A New Active Learning Approach to Combat Cybercrime. (arXiv:2108.06238v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06201",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Loecher_M/0/1/0/all/0/1\">Markus Loecher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>",
          "description": "Tree-based algorithms such as random forests and gradient boosted trees\ncontinue to be among the most popular and powerful machine learning models used\nacross multiple disciplines. The conventional wisdom of estimating the impact\nof a feature in tree based models is to measure the \\textit{node-wise reduction\nof a loss function}, which (i) yields only global importance measures and (ii)\nis known to suffer from severe biases. Conditional feature contributions (CFCs)\nprovide \\textit{local}, case-by-case explanations of a prediction by following\nthe decision path and attributing changes in the expected output of the model\nto each feature along the path. However, Lundberg et al. pointed out a\npotential bias of CFCs which depends on the distance from the root of a tree.\nThe by now immensely popular alternative, SHapley Additive exPlanation (SHAP)\nvalues appear to mitigate this bias but are computationally much more\nexpensive. Here we contribute a thorough comparison of the explanations\ncomputed by both methods on a set of 164 publicly available classification\nproblems in order to provide data-driven algorithm recommendations to current\nresearchers. For random forests, we find extremely high similarities and\ncorrelations of both local and global SHAP values and CFC scores, leading to\nvery similar rankings and interpretations. Analogous conclusions hold for the\nfidelity of using global feature importance scores as a proxy for the\npredictive power associated with each feature.",
          "link": "http://arxiv.org/abs/2108.06201",
          "publishedOn": "2021-08-16T00:47:32.747Z",
          "wordCount": 664,
          "title": "Data-driven advice for interpreting local and global model predictions in bioinformatics problems. (arXiv:2108.06201v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_R/0/1/0/all/0/1\">Rohan Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Maheep Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurya_C/0/1/0/all/0/1\">Chandresh Kumar Maurya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1\">Shitala Prasad</a>",
          "description": "Intelligent recommendation and reminder systems are the need of the\nfast-pacing life. Current intelligent systems such as Siri, Google Assistant,\nMicrosoft Cortona, etc., have limited capability. For example, if you want to\nwake up at 6 am because you have an upcoming trip, you have to set the alarm\nmanually. Besides, these systems do not recommend or remind what else to carry,\nsuch as carrying an umbrella during a likely rain. The present work proposes a\nsystem that takes an email as input and returns a recommendation-cumreminder\nlist. As a first step, we parse the emails, recognize the entities using named\nentity recognition (NER). In the second step, information retrieval over the\nweb is done to identify nearby places, climatic conditions, etc. Imperative\nsentences from the reviews of all places are extracted and passed to the object\nextraction module. The main challenge lies in extracting the objects (items) of\ninterest from the review. To solve it, a modified Machine Reading\nComprehension-NER (MRC-NER) model is trained to tag objects of interest by\nformulating annotation rules as a query. The objects so found are recommended\nto the user one day in advance. The final reminder list of objects is pruned by\nour proposed model for tracking objects kept during the \"packing activity.\"\nEventually, when the user leaves for the event/trip, an alert is sent\ncontaining the reminding list items. Our approach achieves superior performance\ncompared to several baselines by as much as 30% on recall and 10% on precision.",
          "link": "http://arxiv.org/abs/2108.06206",
          "publishedOn": "2021-08-16T00:47:32.726Z",
          "wordCount": 676,
          "title": "An Intelligent Recommendation-cum-Reminder System. (arXiv:2108.06206v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sherman_U/0/1/0/all/0/1\">Uri Sherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>",
          "description": "We study a variant of online convex optimization where the player is\npermitted to switch decisions at most $S$ times in expectation throughout $T$\nrounds. Similar problems have been addressed in prior work for the discrete\ndecision set setting, and more recently in the continuous setting but only with\nan adaptive adversary. In this work, we aim to fill the gap and present\ncomputationally efficient algorithms in the more prevalent oblivious setting,\nestablishing a regret bound of $O(T/S)$ for general convex losses and\n$\\widetilde O(T/S^2)$ for strongly convex losses. In addition, for stochastic\ni.i.d.~losses, we present a simple algorithm that performs $\\log T$ switches\nwith only a multiplicative $\\log T$ factor overhead in its regret in both the\ngeneral and strongly convex settings. Finally, we complement our algorithms\nwith lower bounds that match our upper bounds in some of the cases we consider.",
          "link": "http://arxiv.org/abs/2102.03803",
          "publishedOn": "2021-08-16T00:47:32.714Z",
          "wordCount": 617,
          "title": "Lazy OCO: Online Convex Optimization on a Switching Budget. (arXiv:2102.03803v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gerhard_Young_G/0/1/0/all/0/1\">Greyson Gerhard-Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantha_R/0/1/0/all/0/1\">Raviteja Anantha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chappidi_S/0/1/0/all/0/1\">Srinivas Chappidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmeister_B/0/1/0/all/0/1\">Bj&#xf6;rn Hoffmeister</a>",
          "description": "Recent work building open-domain chatbots has demonstrated that increasing\nmodel size improves performance. On the other hand, latency and connectivity\nconsiderations dictate the move of digital assistants on the device. Giving a\ndigital assistant like Siri, Alexa, or Google Assistant the ability to discuss\njust about anything leads to the need for reducing the chatbot model size such\nthat it fits on the user's device. We demonstrate that low parameter models can\nsimultaneously retain their general knowledge conversational abilities while\nimproving in a specific domain. Additionally, we propose a generic framework\nthat accounts for variety in question types, tracks reference throughout\nmulti-turn conversations, and removes inconsistent and potentially toxic\nresponses. Our framework seamlessly transitions between chatting and performing\ntransactional tasks, which will ultimately make interactions with digital\nassistants more human-like. We evaluate our framework on 1 internal and 4\npublic benchmark datasets using both automatic (Perplexity) and human (SSA -\nSensibleness and Specificity Average) evaluation metrics and establish\ncomparable performance while reducing model parameters by 90%.",
          "link": "http://arxiv.org/abs/2108.06329",
          "publishedOn": "2021-08-16T00:47:32.703Z",
          "wordCount": 601,
          "title": "Low-Resource Adaptation of Open-Domain Generative Chatbots. (arXiv:2108.06329v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Real world traffic sign recognition is an important step towards building\nautonomous vehicles, most of which highly dependent on Deep Neural Networks\n(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to\nadversarial examples. Many attack methods have been proposed to understand and\ngenerate adversarial examples, such as gradient based attack, score based\nattack, decision based attack, and transfer based attacks. However, most of\nthese algorithms are ineffective in real-world road sign attack, because (1)\niteratively learning perturbations for each frame is not realistic for a fast\nmoving car and (2) most optimization algorithms traverse all pixels equally\nwithout considering their diverse contribution. To alleviate these problems,\nthis paper proposes the targeted attention attack (TAA) method for real world\nroad sign attack. Specifically, we have made the following contributions: (1)\nwe leverage the soft attention map to highlight those important pixels and skip\nthose zero-contributed areas - this also helps to generate natural\nperturbations, (2) we design an efficient universal attack that optimizes a\nsingle perturbation/noise based on a set of training images under the guidance\nof the pre-trained attention map, (3) we design a simple objective function\nthat can be easily optimized, (4) we evaluate the effectiveness of TAA on real\nworld data sets. Experimental results validate that the TAA method improves the\nattack successful rate (nearly 10%) and reduces the perturbation loss (about a\nquarter) compared with the popular RP2 method. Additionally, our TAA also\nprovides good properties, e.g., transferability and generalization capability.\nWe provide code and data to ensure the reproducibility:\nhttps://github.com/AdvAttack/RoadSignAttack.",
          "link": "http://arxiv.org/abs/2010.04331",
          "publishedOn": "2021-08-16T00:47:32.693Z",
          "wordCount": 756,
          "title": "Targeted Physical-World Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12698",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Moreno_E/0/1/0/all/0/1\">Eric A. Moreno</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Vlimant_J/0/1/0/all/0/1\">Jean-Roch Vlimant</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Spiropulu_M/0/1/0/all/0/1\">Maria Spiropulu</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Borzyszkowski_B/0/1/0/all/0/1\">Bartlomiej Borzyszkowski</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>",
          "description": "We present an application of anomaly detection techniques based on deep\nrecurrent autoencoders to the problem of detecting gravitational wave signals\nin laser interferometers. Trained on noise data, this class of algorithms could\ndetect signals using an unsupervised strategy, i.e., without targeting a\nspecific kind of source. We develop a custom architecture to analyze the data\nfrom two interferometers. We compare the obtained performance to that obtained\nwith other autoencoder architectures and with a convolutional classifier. The\nunsupervised nature of the proposed strategy comes with a cost in terms of\naccuracy, when compared to more traditional supervised techniques. On the other\nhand, there is a qualitative gain in generalizing the experimental sensitivity\nbeyond the ensemble of pre-computed signal templates. The recurrent autoencoder\noutperforms other autoencoders based on different architectures. The class of\nrecurrent autoencoders presented in this paper could complement the search\nstrategy employed for gravitational wave detection and extend the reach of the\nongoing detection campaigns.",
          "link": "http://arxiv.org/abs/2107.12698",
          "publishedOn": "2021-08-16T00:47:32.675Z",
          "wordCount": 638,
          "title": "Source-Agnostic Gravitational-Wave Detection with Recurrent Autoencoders. (arXiv:2107.12698v2 [gr-qc] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1\">Hrishav Bakul Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mg_T/0/1/0/all/0/1\">Theint Haythi Mg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_P/0/1/0/all/0/1\">Pradip Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_C/0/1/0/all/0/1\">Chayan Sarkar</a>",
          "description": "Robots in our daily surroundings are increasing day by day. Their usability\nand acceptability largely depend on their explicit and implicit interaction\ncapability with fellow human beings. As a result, social behavior is one of the\nmost sought-after qualities that a robot can possess. However, there is no\nspecific aspect and/or feature that defines socially acceptable behavior and it\nlargely depends on the situation, application, and society. In this article, we\ninvestigate one such social behavior for collocated robots. Imagine a group of\npeople is interacting with each other and we want to join the group. We as\nhuman beings do it in a socially acceptable manner, i.e., within the group, we\ndo position ourselves in such a way that we can participate in the group\nactivity without disturbing/obstructing anybody. To possess such a quality,\nfirst, a robot needs to determine the formation of the group and then determine\na position for itself, which we humans do implicitly. The theory of f-formation\ncan be utilized for this purpose. As the types of formations can be very\ndiverse, detecting the social groups is not a trivial task. In this article, we\nprovide a comprehensive survey of the existing work on social interaction and\ngroup detection using f-formation for robotics and other applications. We also\nput forward a novel holistic survey framework combining all the possible\nconcerns and modules relevant to this problem. We define taxonomies based on\nmethods, camera views, datasets, detection capabilities and scale, evaluation\napproaches, and application areas. We discuss certain open challenges and\nlimitations in current literature along with possible future research\ndirections based on this framework. In particular, we discuss the existing\nmethods/techniques and their relative merits and demerits, applications, and\nprovide a set of unsolved but relevant problems in this domain.",
          "link": "http://arxiv.org/abs/2108.06181",
          "publishedOn": "2021-08-16T00:47:32.668Z",
          "wordCount": 790,
          "title": "Detecting socially interacting groups using f-formation: A survey of taxonomy, methods, datasets, applications, challenges, and future research directions. (arXiv:2108.06181v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08871",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Haan_K/0/1/0/all/0/1\">Kevin de Haan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yijie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuckerman_J/0/1/0/all/0/1\">Jonathan E. Zuckerman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisk_A/0/1/0/all/0/1\">Anthony E. Sisk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_M/0/1/0/all/0/1\">Miguel F. P. Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jen_K/0/1/0/all/0/1\">Kuang-Yu Jen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nobori_A/0/1/0/all/0/1\">Alexander Nobori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liou_S/0/1/0/all/0/1\">Sofia Liou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Sarah Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riahi_R/0/1/0/all/0/1\">Rana Riahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivenson_Y/0/1/0/all/0/1\">Yair Rivenson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wallace_W/0/1/0/all/0/1\">W. Dean Wallace</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>",
          "description": "Pathology is practiced by visual inspection of histochemically stained\nslides. Most commonly, the hematoxylin and eosin (H&E) stain is used in the\ndiagnostic workflow and it is the gold standard for cancer diagnosis. However,\nin many cases, especially for non-neoplastic diseases, additional \"special\nstains\" are used to provide different levels of contrast and color to tissue\ncomponents and allow pathologists to get a clearer diagnostic picture. In this\nstudy, we demonstrate the utility of supervised learning-based computational\nstain transformation from H&E to different special stains (Masson's Trichrome,\nperiodic acid-Schiff and Jones silver stain) using tissue sections from kidney\nneedle core biopsies. Based on evaluation by three renal pathologists, followed\nby adjudication by a fourth renal pathologist, we show that the generation of\nvirtual special stains from existing H&E images improves the diagnosis in\nseveral non-neoplastic kidney diseases sampled from 58 unique subjects. A\nsecond study performed by three pathologists found that the quality of the\nspecial stains generated by the stain transformation network was statistically\nequivalent to those generated through standard histochemical staining. As the\ntransformation of H&E images into special stains can be achieved within 1 min\nor less per patient core specimen slide, this stain-to-stain transformation\nframework can improve the quality of the preliminary diagnosis when additional\nspecial stains are needed, along with significant savings in time and cost,\nreducing the burden on healthcare system and patients.",
          "link": "http://arxiv.org/abs/2008.08871",
          "publishedOn": "2021-08-16T00:47:32.660Z",
          "wordCount": 763,
          "title": "Deep learning-based transformation of the H&E stain into special stains. (arXiv:2008.08871v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asikis_T/0/1/0/all/0/1\">Thomas Asikis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bottcher_L/0/1/0/all/0/1\">Lucas B&#xf6;ttcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antulov_Fantulin_N/0/1/0/all/0/1\">Nino Antulov-Fantulin</a>",
          "description": "We study the ability of neural networks to steer or control trajectories of\ncontinuous time non-linear dynamical systems on graphs, which we represent with\nneural ordinary differential equations (neural ODEs). To do so, we introduce a\nneural-ODE control (NODEC) framework and find that it can learn control signals\nthat drive graph dynamical systems into desired target states. While we use\nloss functions that do not constrain the control energy, our results show that\nNODEC produces low energy control signals. Finally, we showcase the performance\nand versatility of NODEC by using it to control a system of more than one\nthousand coupled, non-linear ODEs.",
          "link": "http://arxiv.org/abs/2006.09773",
          "publishedOn": "2021-08-16T00:47:32.648Z",
          "wordCount": 619,
          "title": "Neural Ordinary Differential Equation Control of Dynamics on Graphs. (arXiv:2006.09773v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao-Jung Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulicny_M/0/1/0/all/0/1\">Matej Ulicny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzke_M/0/1/0/all/0/1\">Michael Manzke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahyot_R/0/1/0/all/0/1\">Rozenn Dahyot</a>",
          "description": "Localization of street objects from images has gained a lot of attention in\nrecent years. We propose an approach to improve asset geolocation from street\nview imagery by enhancing the quality of the metadata associated with the\nimages using Structure from Motion. The predicted object geolocation is further\nrefined by imposing contextual geographic information extracted from\nOpenStreetMap. Our pipeline is validated experimentally against the state of\nthe art approaches for geotagging traffic lights.",
          "link": "http://arxiv.org/abs/2108.06302",
          "publishedOn": "2021-08-16T00:47:32.641Z",
          "wordCount": 501,
          "title": "Context Aware Object Geotagging. (arXiv:2108.06302v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhongyi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haoliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>",
          "description": "Unsupervised domain adaptation (UDA) enables a learning machine to adapt from\na labeled source domain to an unlabeled domain under the distribution shift.\nThanks to the strong representation ability of deep neural networks, recent\nremarkable achievements in UDA resort to learning domain-invariant features.\nIntuitively, the hope is that a good feature representation, together with the\nhypothesis learned from the source domain, can generalize well to the target\ndomain. However, the learning processes of domain-invariant features and source\nhypothesis inevitably involve domain-specific information that would degrade\nthe generalizability of UDA models on the target domain. In this paper,\nmotivated by the lottery ticket hypothesis that only partial parameters are\nessential for generalization, we find that only partial parameters are\nessential for learning domain-invariant information and generalizing well in\nUDA. Such parameters are termed transferable parameters. In contrast, the other\nparameters tend to fit domain-specific details and often fail to generalize,\nwhich we term as untransferable parameters. Driven by this insight, we propose\nTransferable Parameter Learning (TransPar) to reduce the side effect brought by\ndomain-specific information in the learning process and thus enhance the\nmemorization of domain-invariant information. Specifically, according to the\ndistribution discrepancy degree, we divide all parameters into transferable and\nuntransferable ones in each training iteration. We then perform separate\nupdates rules for the two types of parameters. Extensive experiments on image\nclassification and regression tasks (keypoint detection) show that TransPar\noutperforms prior arts by non-trivial margins. Moreover, experiments\ndemonstrate that TransPar can be integrated into the most popular deep UDA\nnetworks and be easily extended to handle any data distribution shift\nscenarios.",
          "link": "http://arxiv.org/abs/2108.06129",
          "publishedOn": "2021-08-16T00:47:32.636Z",
          "wordCount": 703,
          "title": "Learning Transferable Parameters for Unsupervised Domain Adaptation. (arXiv:2108.06129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ronge_R/0/1/0/all/0/1\">Raphael Ronge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nho_K/0/1/0/all/0/1\">Kwangsik Nho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>",
          "description": "The current state-of-the-art deep neural networks (DNNs) for Alzheimer's\nDisease diagnosis use different biomarker combinations to classify patients,\nbut do not allow extracting knowledge about the interactions of biomarkers.\nHowever, to improve our understanding of the disease, it is paramount to\nextract such knowledge from the learned model. In this paper, we propose a Deep\nFactorization Machine model that combines the ability of DNNs to learn complex\nrelationships and the ease of interpretability of a linear model. The proposed\nmodel has three parts: (i) an embedding layer to deal with sparse categorical\ndata, (ii) a Factorization Machine to efficiently learn pairwise interactions,\nand (iii) a DNN to implicitly model higher order interactions. In our\nexperiments on data from the Alzheimer's Disease Neuroimaging Initiative, we\ndemonstrate that our proposed model classifies cognitive normal, mild cognitive\nimpaired, and demented patients more accurately than competing models. In\naddition, we show that valuable knowledge about the interactions among\nbiomarkers can be obtained.",
          "link": "http://arxiv.org/abs/2108.05916",
          "publishedOn": "2021-08-16T00:47:32.620Z",
          "wordCount": 608,
          "title": "Alzheimer's Disease Diagnosis via Deep Factorization Machine Models. (arXiv:2108.05916v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06264",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bienkiewicz_M/0/1/0/all/0/1\">M. M. N. Bie&#x144;kiewicz</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Smykovskyi_A/0/1/0/all/0/1\">A. Smykovskyi</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Olugbade_T/0/1/0/all/0/1\">T. Olugbade</a> (2), <a href=\"http://arxiv.org/find/q-bio/1/au:+Janaqi_S/0/1/0/all/0/1\">S. Janaqi</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Camurri_A/0/1/0/all/0/1\">A. Camurri</a> (3), <a href=\"http://arxiv.org/find/q-bio/1/au:+Bianchi_Berthouze_N/0/1/0/all/0/1\">N. Bianchi-Berthouze</a> (2), <a href=\"http://arxiv.org/find/q-bio/1/au:+Bjorkman_M/0/1/0/all/0/1\">M. Bj&#xf6;rkman</a> (4), <a href=\"http://arxiv.org/find/q-bio/1/au:+Bardy_B/0/1/0/all/0/1\">B. G. Bardy</a> (1) ((1) EuroMov Digital Health in Motion Univ. Montpellier IMT Mines Ales France, (2) UCL, University College of London UK, (3) UNIGE InfoMus Casa Paganini Italy, (4) KTH Royal Institute of Technology Sweden)",
          "description": "Our daily human life is filled with a myriad of joint action moments, be it\nchildren playing, adults working together (i.e., team sports), or strangers\nnavigating through a crowd. Joint action brings individuals (and embodiment of\ntheir emotions) together, in space and in time. Yet little is known about how\nindividual emotions propagate through embodied presence in a group, and how\njoint action changes individual emotion. In fact, the multi-agent component is\nlargely missing from neuroscience-based approaches to emotion, and reversely\njoint action research has not found a way yet to include emotion as one of the\nkey parameters to model socio-motor interaction. In this review, we first\nidentify the gap and then stockpile evidence showing strong entanglement\nbetween emotion and acting together from various branches of sciences. We\npropose an integrative approach to bridge the gap, highlight five research\navenues to do so in behavioral neuroscience and digital sciences, and address\nsome of the key challenges in the area faced by modern societies.",
          "link": "http://arxiv.org/abs/2108.06264",
          "publishedOn": "2021-08-16T00:47:32.610Z",
          "wordCount": 677,
          "title": "Bridging the gap between emotion and joint action. (arXiv:2108.06264v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paudel_A/0/1/0/all/0/1\">Abhishek Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakal_R/0/1/0/all/0/1\">Roshan Dhakal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_S/0/1/0/all/0/1\">Sakshat Bhattarai</a>",
          "description": "We present our approach to improve room classification task on floor plan\nmaps of buildings by representing floor plans as undirected graphs and\nleveraging graph neural networks to predict the room categories. Rooms in the\nfloor plans are represented as nodes in the graph with edges representing their\nadjacency in the map. We experiment with House-GAN dataset that consists of\nfloor plan maps in vector format and train multilayer perceptron and graph\nneural networks. Our results show that graph neural networks, specifically\nGraphSAGE and Topology Adaptive GCN were able to achieve accuracy of 80% and\n81% respectively outperforming baseline multilayer perceptron by more than 15%\nmargin.",
          "link": "http://arxiv.org/abs/2108.05947",
          "publishedOn": "2021-08-16T00:47:32.604Z",
          "wordCount": 542,
          "title": "Room Classification on Floor Plan Graphs using Graph Neural Networks. (arXiv:2108.05947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1906.04591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Frances J. Ferri</a>",
          "description": "Acoustic scene classification (ASC) has been approached in the last years\nusing deep learning techniques such as convolutional neural networks or\nrecurrent neural networks. Many state-of-the-art solutions are based on image\nclassification frameworks and, as such, a 2D representation of the audio signal\nis considered for training these networks. Finding the most suitable audio\nrepresentation is still a research area of interest. In this paper, different\nlog-Mel representations and combinations are analyzed. Experiments show that\nthe best results are obtained using the harmonic and percussive components plus\nthe difference between left and right stereo channels, (L-R). On the other\nhand, it is a common strategy to ensemble different models in order to increase\nthe final accuracy. Even though averaging different model predictions is a\ncommon choice, an exhaustive analysis of different ensemble techniques has not\nbeen presented in ASC problems. In this paper, geometric and arithmetic mean\nplus the Ordered Weighted Averaging (OWA) operator are studied as aggregation\noperators for the output of the different models of the ensemble. Finally, the\nwork carried out in this paper is highly oriented towards real-time\nimplementations. In this context, as the number of applications for audio\nclassification on edge devices is increasing exponentially, we also analyze\ndifferent network depths and efficient solutions for aggregating ensemble\npredictions.",
          "link": "http://arxiv.org/abs/1906.04591",
          "publishedOn": "2021-08-16T00:47:32.598Z",
          "wordCount": 716,
          "title": "CNN depth analysis with different channel inputs for Acoustic Scene Classification. (arXiv:1906.04591v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>",
          "description": "Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, most existing\nlearning-based approaches usually suffer from limited manually annotated\nmedical data, which poses a major practical problem for accurate and robust\nmedical image segmentation. In addition, most existing semi-supervised\napproaches are usually not robust compared with the supervised counterparts,\nand also lack explicit modeling of geometric structure and semantic\ninformation, both of which limit the segmentation accuracy. In this work, we\npresent SimCVD, a simple contrastive distillation framework that significantly\nadvances state-of-the-art voxel-wise representation learning. We first describe\nan unsupervised training strategy, which takes two views of an input volume and\npredicts their signed distance maps of object boundaries in a contrastive\nobjective, with only two independent dropout as mask. This simple approach\nworks surprisingly well, performing on the same level as previous fully\nsupervised methods with much less labeled data. We hypothesize that dropout can\nbe viewed as a minimal form of data augmentation and makes the network robust\nto representation collapse. Then, we propose to perform structural distillation\nby distilling pair-wise similarities. We evaluate SimCVD on two popular\ndatasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\ndataset. The results on the LA dataset demonstrate that, in two types of\nlabeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\nprevious best results. Our method can be trained in an end-to-end fashion,\nshowing the promise of utilizing SimCVD as a general framework for downstream\ntasks, such as medical image synthesis and registration.",
          "link": "http://arxiv.org/abs/2108.06227",
          "publishedOn": "2021-08-16T00:47:32.582Z",
          "wordCount": 724,
          "title": "SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_D/0/1/0/all/0/1\">Daoming Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Hugh Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_L/0/1/0/all/0/1\">Levent Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>",
          "description": "Human-robot interactive decision-making is increasingly becoming ubiquitous,\nand trust is an influential factor in determining the reliance on autonomy.\nHowever, it is not reasonable to trust systems that are beyond our\ncomprehension, and typical machine learning and data-driven decision-making are\nblack-box paradigms that impede interpretability. Therefore, it is critical to\nestablish computational trustworthy decision-making mechanisms enhanced by\ninterpretability-aware strategies. To this end, we propose a Trustworthy\nDecision-Making (TDM) framework, which integrates symbolic planning into\nsequential decision-making. The framework learns interpretable subtasks that\nresult in a complex, higher-level composite task that can be formally evaluated\nusing the proposed trust metric. TDM enables the subtask-level interpretability\nby design and converges to an optimal symbolic plan from the learned subtasks.\nMoreover, a TDM-based algorithm is introduced to demonstrate the unification of\nsymbolic planning with other sequential-decision making algorithms, reaping the\nbenefits of both. Experimental results validate the effectiveness of\ntrust-score-based planning while improving the interpretability of subtasks.",
          "link": "http://arxiv.org/abs/2108.06080",
          "publishedOn": "2021-08-16T00:47:32.576Z",
          "wordCount": 599,
          "title": "TDM: Trustworthy Decision-Making via Interpretability Enhancement. (arXiv:2108.06080v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.11855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>",
          "description": "The transformer self-attention network has been extensively used in research\ndomains such as computer vision, image processing, and natural language\nprocessing. The transformer, however, has not been actively used in graph\nneural networks, where constructing an advanced aggregation function is\nessential. To this end, we present an effective model, named UGformer, which --\nby leveraging a transformer self-attention mechanism followed by a recurrent\ntransition -- induces an advanced aggregation function to learn graph\nrepresentations. Experimental results show that UGformer achieves\nstate-of-the-art accuracies on well-known benchmark datasets for graph\nclassification.",
          "link": "http://arxiv.org/abs/1909.11855",
          "publishedOn": "2021-08-16T00:47:32.549Z",
          "wordCount": 643,
          "title": "Universal Graph Transformer Self-Attention Networks. (arXiv:1909.11855v10 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_M/0/1/0/all/0/1\">Mohit Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1\">Daniel Sheldon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>",
          "description": "A key challenge in scaling Gaussian Process (GP) regression to massive\ndatasets is that exact inference requires computation with a dense n x n kernel\nmatrix, where n is the number of data points. Significant work focuses on\napproximating the kernel matrix via interpolation using a smaller set of m\ninducing points. Structured kernel interpolation (SKI) is among the most\nscalable methods: by placing inducing points on a dense grid and using\nstructured matrix algebra, SKI achieves per-iteration time of O(n + m log m)\nfor approximate inference. This linear scaling in n enables inference for very\nlarge data sets; however the cost is per-iteration, which remains a limitation\nfor extremely large n. We show that the SKI per-iteration time can be reduced\nto O(m log m) after a single O(n) time precomputation step by reframing SKI as\nsolving a natural Bayesian linear regression problem with a fixed set of m\ncompact basis functions. With per-iteration complexity independent of the\ndataset size n for a fixed grid, our method scales to truly massive data sets.\nWe demonstrate speedups in practice for a wide range of m and n and apply the\nmethod to GP inference on a three-dimensional weather radar dataset with over\n100 million points.",
          "link": "http://arxiv.org/abs/2101.11751",
          "publishedOn": "2021-08-16T00:47:32.535Z",
          "wordCount": 670,
          "title": "Faster Kernel Interpolation for Gaussian Processes. (arXiv:2101.11751v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05385",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ward_O/0/1/0/all/0/1\">Owen G. Ward</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Davison_A/0/1/0/all/0/1\">Andrew Davison</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zheng_T/0/1/0/all/0/1\">Tian Zheng</a>",
          "description": "Embedding nodes of a large network into a metric (e.g., Euclidean) space has\nbecome an area of active research in statistical machine learning, which has\nfound applications in natural and social sciences. Generally, a representation\nof a network object is learned in a Euclidean geometry and is then used for\nsubsequent tasks regarding the nodes and/or edges of the network, such as\ncommunity detection, node classification and link prediction. Network embedding\nalgorithms have been proposed in multiple disciplines, often with\ndomain-specific notations and details. In addition, different measures and\ntools have been adopted to evaluate and compare the methods proposed under\ndifferent settings, often dependent of the downstream tasks. As a result, it is\nchallenging to study these algorithms in the literature systematically.\nMotivated by the recently proposed Veridical Data Science (VDS) framework, we\npropose a framework for network embedding algorithms and discuss how the\nprinciples of predictability, computability and stability apply in this\ncontext. The utilization of this framework in network embedding holds the\npotential to motivate and point to new directions for future research.",
          "link": "http://arxiv.org/abs/2007.05385",
          "publishedOn": "2021-08-16T00:47:32.530Z",
          "wordCount": 631,
          "title": "Next Waves in Veridical Network Embedding. (arXiv:2007.05385v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qibiao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Recent studies have shown that Graph Convolutional Networks (GCNs) are\nvulnerable to adversarial attacks on the graph structure. Although multiple\nworks have been proposed to improve their robustness against such structural\nadversarial attacks, the reasons for the success of the attacks remain unclear.\nIn this work, we theoretically and empirically demonstrate that structural\nadversarial examples can be attributed to the non-robust aggregation scheme\n(i.e., the weighted mean) of GCNs. Specifically, our analysis takes advantage\nof the breakdown point which can quantitatively measure the robustness of\naggregation schemes. The key insight is that weighted mean, as the basic design\nof GCNs, has a low breakdown point and its output can be dramatically changed\nby injecting a single edge. We show that adopting the aggregation scheme with a\nhigh breakdown point (e.g., median or trimmed mean) could significantly enhance\nthe robustness of GCNs against structural attacks. Extensive experiments on\nfour real-world datasets demonstrate that such a simple but effective method\nachieves the best robustness performance compared to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.06280",
          "publishedOn": "2021-08-16T00:47:32.511Z",
          "wordCount": 608,
          "title": "Understanding Structural Vulnerability in Graph Convolutional Networks. (arXiv:2108.06280v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.07221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>",
          "description": "Weakly-supervised learning (WSL) has recently triggered substantial interest\nas it mitigates the lack of pixel-wise annotations.\n\nGiven global image labels, WSL methods yield pixel-level predictions\n(segmentations), which enable to interpret class predictions. Despite their\nrecent success, mostly with natural images, such methods can face important\nchallenges when the foreground and background regions have similar visual cues,\nyielding high false-positive rates in segmentations, as is the case in\nchallenging histology images. WSL training is commonly driven by standard\nclassification losses, which implicitly maximize model confidence, and locate\nthe discriminative regions linked to classification decisions. Therefore, they\nlack mechanisms for modeling explicitly non-discriminative regions and reducing\nfalse-positive rates. We propose novel regularization terms, which enable the\nmodel to seek both non-discriminative and discriminative regions, while\ndiscouraging unbalanced segmentations. We introduce high uncertainty as a\ncriterion to localize non-discriminative regions that do not affect classifier\ndecision, and describe it with original Kullback-Leibler (KL) divergence losses\nevaluating the deviation of posterior predictions from the uniform\ndistribution. Our KL terms encourage high uncertainty of the model when the\nlatter inputs the latent non-discriminative regions. Our loss integrates: (i) a\ncross-entropy seeking a foreground, where model confidence about class\nprediction is high; (ii) a KL regularizer seeking a background, where model\nuncertainty is high; and (iii) log-barrier terms discouraging unbalanced\nsegmentations. Comprehensive experiments and ablation studies over the public\nGlaS colon cancer data and a Camelyon16 patch-based benchmark for breast cancer\nshow substantial improvements over state-of-the-art WSL methods, and confirm\nthe effect of our new regularizers.",
          "link": "http://arxiv.org/abs/2011.07221",
          "publishedOn": "2021-08-16T00:47:32.501Z",
          "wordCount": 741,
          "title": "Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty. (arXiv:2011.07221v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.04872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hufei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chenghao Wei</a>",
          "description": "To accelerate the existing Broad Learning System (BLS) for new added nodes in\n[7], we extend the inverse Cholesky factorization in [10] to deduce an\nefficient inverse Cholesky factorization for a Hermitian matrix partitioned\ninto 2 * 2 blocks, which is utilized to develop the proposed BLS algorithm 1.\nThe proposed BLS algorithm 1 compute the ridge solution (i.e, the output\nweights) from the inverse Cholesky factor of the Hermitian matrix in the ridge\ninverse, and update the inverse Cholesky factor efficiently. From the proposed\nBLS algorithm 1, we deduce the proposed ridge inverse, which can be obtained\nfrom the generalized inverse in [7] by just change one matrix in the equation\nto compute the newly added sub-matrix. We also modify the proposed algorithm 1\ninto the proposed algorithm 2, which is equivalent to the existing BLS\nalgorithm [7] in terms of numerical computations. The proposed algorithms 1 and\n2 can reduce the computational complexity, since usually the Hermitian matrix\nin the ridge inverse is smaller than the ridge inverse. With respect to the\nexisting BLS algorithm, the proposed algorithms 1 and 2 usually require about\n13 and 2 3 of complexities, respectively, while in numerical experiments they\nachieve the speedups (in each additional training time) of 2.40 - 2.91 and 1.36\n- 1.60, respectively. Numerical experiments also show that the proposed\nalgorithm 1 and the standard ridge solution always bear the same testing\naccuracy, and usually so do the proposed algorithm 2 and the existing BLS\nalgorithm. The existing BLS assumes the ridge parameter lamda->0, since it is\nbased on the generalized inverse with the ridge regression approximation. When\nthe assumption of lamda-> 0 is not satisfied, the standard ridge solution\nobviously achieves a better testing accuracy than the existing BLS algorithm in\nnumerical experiments.",
          "link": "http://arxiv.org/abs/1911.04872",
          "publishedOn": "2021-08-16T00:47:32.494Z",
          "wordCount": 785,
          "title": "Efficient Ridge Solution for the Incremental Broad Learning System on Added Nodes by Inverse Cholesky Factorization of a Partitioned Matrix. (arXiv:1911.04872v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.14129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhuoyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xingzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1\">Rui Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee Keong Kwoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chi Xu</a>",
          "description": "Latent factor models play a dominant role among recommendation techniques.\nHowever, most of the existing latent factor models assume both historical\ninteractions and embedding dimensions are independent of each other, and thus\nregrettably ignore the high-order interaction information among historical\ninteractions and embedding dimensions. In this paper, we propose a novel latent\nfactor model called COMET (COnvolutional diMEnsion inTeraction), which\nsimultaneously model the high-order interaction patterns among historical\ninteractions and embedding dimensions. To be specific, COMET stacks the\nembeddings of historical interactions horizontally at first, which results in\ntwo \"embedding maps\". In this way, internal interactions and dimensional\ninteractions can be exploited by convolutional neural networks with kernels of\ndifferent sizes simultaneously. A fully-connected multi-layer perceptron is\nthen applied to obtain two interaction vectors. Lastly, the representations of\nusers and items are enriched by the learnt interaction vectors, which can\nfurther be used to produce the final prediction. Extensive experiments and\nablation studies on various public implicit feedback datasets clearly\ndemonstrate the effectiveness and the rationality of our proposed method.",
          "link": "http://arxiv.org/abs/2007.14129",
          "publishedOn": "2021-08-16T00:47:32.486Z",
          "wordCount": 646,
          "title": "COMET: Convolutional Dimension Interaction for Collaborative Filtering. (arXiv:2007.14129v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dohare_S/0/1/0/all/0/1\">Shibhansh Dohare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">A. Rupam Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "The Backprop algorithm for learning in neural networks utilizes two\nmechanisms: first, stochastic gradient descent and second, initialization with\nsmall random weights, where the latter is essential to the effectiveness of the\nformer. We show that in continual learning setups, Backprop performs well\ninitially, but over time its performance degrades. Stochastic gradient descent\nalone is insufficient to learn continually; the initial randomness enables only\ninitial learning but not continual learning. To the best of our knowledge, ours\nis the first result showing this degradation in Backprop's ability to learn. To\naddress this issue, we propose an algorithm that continually injects random\nfeatures alongside gradient descent using a new generate-and-test process. We\ncall this the Continual Backprop algorithm. We show that, unlike Backprop,\nContinual Backprop is able to continually adapt in both supervised and\nreinforcement learning problems. We expect that as continual learning becomes\nmore common in future applications, a method like Continual Backprop will be\nessential where the advantages of random initialization are present throughout\nlearning.",
          "link": "http://arxiv.org/abs/2108.06325",
          "publishedOn": "2021-08-16T00:47:32.444Z",
          "wordCount": 595,
          "title": "Continual Backprop: Stochastic Gradient Descent with Persistent Randomness. (arXiv:2108.06325v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1\">Avi Schwarzschild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgnia_E/0/1/0/all/0/1\">Eitan Borgnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arjun Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Arpit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emam_Z/0/1/0/all/0/1\">Zeyad Emam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "We describe new datasets for studying generalization from easy to hard\nexamples.",
          "link": "http://arxiv.org/abs/2108.06011",
          "publishedOn": "2021-08-16T00:47:32.439Z",
          "wordCount": 456,
          "title": "Datasets for Studying Generalization from Easy to Hard Examples. (arXiv:2108.06011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06094",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1\">Yang Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1\">Xuekui Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Esfahani_F/0/1/0/all/0/1\">Fatemeh Esfahani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Srinivasan_V/0/1/0/all/0/1\">Venkatesh Srinivasan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thomo_A/0/1/0/all/0/1\">Alex Thomo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xing_L/0/1/0/all/0/1\">Li Xing</a>",
          "description": "Mining dense subgraphs where vertices connect closely with each other is a\ncommon task when analyzing graphs. A very popular notion in subgraph analysis\nis core decomposition. Recently, Esfahani et al. presented a probabilistic core\ndecomposition algorithm based on graph peeling and Central Limit Theorem (CLT)\nthat is capable of handling very large graphs. Their proposed peeling algorithm\n(PA) starts from the lowest degree vertices and recursively deletes these\nvertices, assigning core numbers, and updating the degree of neighbour vertices\nuntil it reached the maximum core. However, in many applications, particularly\nin biology, more valuable information can be obtained from dense\nsub-communities and we are not interested in small cores where vertices do not\ninteract much with others. To make the previous PA focus more on dense\nsubgraphs, we propose a multi-stage graph peeling algorithm (M-PA) that has a\ntwo-stage data screening procedure added before the previous PA. After removing\nvertices from the graph based on the user-defined thresholds, we can reduce the\ngraph complexity largely and without affecting the vertices in subgraphs that\nwe are interested in. We show that M-PA is more efficient than the previous PA\nand with the properly set filtering threshold, can produce very similar if not\nidentical dense subgraphs to the previous PA (in terms of graph density and\nclustering coefficient).",
          "link": "http://arxiv.org/abs/2108.06094",
          "publishedOn": "2021-08-16T00:47:32.433Z",
          "wordCount": 655,
          "title": "Multi-Stage Graph Peeling Algorithm for Probabilistic Core Decomposition. (arXiv:2108.06094v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1906.05029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robberechts_P/0/1/0/all/0/1\">Pieter Robberechts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haaren_J/0/1/0/all/0/1\">Jan Van Haaren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jesse Davis</a>",
          "description": "In-game win probability models, which provide a sports team's likelihood of\nwinning at each point in a game based on historical observations, are becoming\nincreasingly popular. In baseball, basketball and American football, they have\nbecome important tools to enhance fan experience, to evaluate in-game\ndecision-making, and to inform coaching decisions. While equally relevant in\nsoccer, the adoption of these models is held back by technical challenges\narising from the low-scoring nature of the sport.\n\nIn this paper, we introduce an in-game win probability model for soccer that\naddresses the shortcomings of existing models. First, we demonstrate that\nin-game win probability models for other sports struggle to provide accurate\nestimates for soccer, especially towards the end of a game. Second, we\nintroduce a novel Bayesian statistical framework that estimates running win,\ntie and loss probabilities by leveraging a set of contextual game state\nfeatures. An empirical evaluation on eight seasons of data for the top-five\nsoccer leagues demonstrates that our framework provides well-calibrated\nprobabilities. Furthermore, two use cases show its ability to enhance fan\nexperience and to evaluate performance in crucial game situations.",
          "link": "http://arxiv.org/abs/1906.05029",
          "publishedOn": "2021-08-16T00:47:32.427Z",
          "wordCount": 667,
          "title": "A Bayesian Approach to In-Game Win Probability in Soccer. (arXiv:1906.05029v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stolfi_P/0/1/0/all/0/1\">Paola Stolfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastropietro_A/0/1/0/all/0/1\">Andrea Mastropietro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasculli_G/0/1/0/all/0/1\">Giuseppe Pasculli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tieri_P/0/1/0/all/0/1\">Paolo Tieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vergni_D/0/1/0/all/0/1\">Davide Vergni</a>",
          "description": "Positive-Unlabelled (PU) learning is the machine learning setting in which\nonly a set of positive instances are labelled, while the rest of the data set\nis unlabelled. The unlabelled instances may be either unspecified positive\nsamples or true negative samples. Over the years, many solutions have been\nproposed to deal with PU learning. Some techniques consider the unlabelled\nsamples as negative ones, reducing the problem to a binary classification with\na noisy negative set, while others aim to detect sets of possible negative\nexamples to later apply a supervised machine learning strategy (two-step\ntechniques). The approach proposed in this work falls in the latter category\nand works in a semi-supervised fashion: motivated and inspired by previous\nworks, a Markov diffusion process with restart is used to assign pseudo-labels\nto unlabelled instances. Afterward, a machine learning model, exploiting the\nnewly assigned classes, is trained. The principal aim of the algorithm is to\nidentify a set of instances which are likely to contain positive instances that\nwere originally unlabelled.",
          "link": "http://arxiv.org/abs/2108.06158",
          "publishedOn": "2021-08-16T00:47:32.420Z",
          "wordCount": 598,
          "title": "Adaptive Positive-Unlabelled Learning via Markov Diffusion. (arXiv:2108.06158v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Floryan_D/0/1/0/all/0/1\">Daniel Floryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_M/0/1/0/all/0/1\">Michael D. Graham</a>",
          "description": "We introduce a method for learning minimal-dimensional dynamical models from\nhigh-dimensional time series data that lie on a low-dimensional manifold, as\narises for many processes. For an arbitrary manifold, there is no smooth global\ncoordinate representation, so following the formalism of differential topology\nwe represent the manifold as an atlas of charts. We first partition the data\ninto overlapping regions. Then undercomplete autoencoders are used to find\nlow-dimensional coordinate representations for each region. We then use the\ndata to learn dynamical models in each region, which together yield a global\nlow-dimensional dynamical model. We apply this method to examples ranging from\nsimple periodic dynamics to complex, nominally high-dimensional non-periodic\nbursting dynamics of the Kuramoto-Sivashinsky equation. We demonstrate that it:\n(1) can yield dynamical models of the lowest possible dimension, where previous\nmethods generally cannot; (2) exhibits computational benefits including\nscalability, parallelizability, and adaptivity; and (3) separates state space\ninto regions of distinct behaviours.",
          "link": "http://arxiv.org/abs/2108.05928",
          "publishedOn": "2021-08-16T00:47:32.411Z",
          "wordCount": 595,
          "title": "Charts and atlases for nonlinear data-driven models of dynamics on manifolds. (arXiv:2108.05928v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadri_N/0/1/0/all/0/1\">Nima Sadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bihan Liu</a>",
          "description": "Creating abstractive summaries from meeting transcripts has proven to be\nchallenging due to the limited amount of labeled data available for training\nneural network models. Moreover, Transformer-based architectures have proven to\nbeat state-of-the-art models in summarizing news data. In this paper, we\nutilize a Transformer-based Pointer Generator Network to generate abstract\nsummaries for meeting transcripts. This model uses 2 LSTMs as an encoder and a\ndecoder, a Pointer network which copies words from the inputted text, and a\nGenerator network to produce out-of-vocabulary words (hence making the summary\nabstractive). Moreover, a coverage mechanism is used to avoid repetition of\nwords in the generated summary. First, we show that training the model on a\nnews summary dataset and using zero-shot learning to test it on the meeting\ndataset proves to produce better results than training it on the AMI meeting\ndataset. Second, we show that training this model first on out-of-domain data,\nsuch as the CNN-Dailymail dataset, followed by a fine-tuning stage on the AMI\nmeeting dataset is able to improve the performance of the model significantly.\nWe test our model on a testing set from the AMI dataset and report the ROUGE-2\nscore of the generated summary to compare with previous literature. We also\nreport the Factual score of our summaries since it is a better benchmark for\nabstractive summaries since the ROUGE-2 score is limited to measuring\nword-overlaps. We show that our improved model is able to improve on previous\nmodels by at least 5 ROUGE-2 scores, which is a substantial improvement. Also,\na qualitative analysis of the summaries generated by our model shows that these\nsummaries and human-readable and indeed capture most of the important\ninformation from the transcripts.",
          "link": "http://arxiv.org/abs/2108.06310",
          "publishedOn": "2021-08-16T00:47:32.363Z",
          "wordCount": 712,
          "title": "MeetSum: Transforming Meeting Transcript Summarization using Transformers!. (arXiv:2108.06310v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feiyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Min Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dapeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "The delayed feedback problem is one of the imperative challenges in online\nadvertising, which is caused by the highly diversified feedback delay of a\nconversion varying from a few minutes to several days. It is hard to design an\nappropriate online learning system under these non-identical delay for\ndifferent types of ads and users. In this paper, we propose to tackle the\ndelayed feedback problem in online advertising by \"Following the Prophet\" (FTP\nfor short). The key insight is that, if the feedback came instantly for all the\nlogged samples, we could get a model without delayed feedback, namely the\n\"prophet\". Although the prophet cannot be obtained during online learning, we\nshow that we could predict the prophet's predictions by an aggregation policy\non top of a set of multi-task predictions, where each task captures the\nfeedback patterns of different periods. We propose the objective and\noptimization approach for the policy, and use the logged data to imitate the\nprophet. Extensive experiments on three real-world advertising datasets show\nthat our method outperforms the previous state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2108.06167",
          "publishedOn": "2021-08-16T00:47:32.350Z",
          "wordCount": 664,
          "title": "Follow the Prophet: Accurate Online Conversion Rate Prediction in the Face of Delayed Feedback. (arXiv:2108.06167v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">James Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Min Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Hanqi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingquan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherubin_L/0/1/0/all/0/1\">Laurent Ch&#xe9;rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanZwieten_J/0/1/0/all/0/1\">James VanZwieten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yufei Tang</a>",
          "description": "Ocean current, fluid mechanics, and many other spatio-temporal physical\ndynamical systems are essential components of the universe. One key\ncharacteristic of such systems is that certain physics laws -- represented as\nordinary/partial differential equations (ODEs/PDEs) -- largely dominate the\nwhole process, irrespective of time or location. Physics-informed learning has\nrecently emerged to learn physics for accurate prediction, but they often lack\na mechanism to leverage localized spatial and temporal correlation or rely on\nhard-coded physics parameters. In this paper, we advocate a physics-coupled\nneural network model to learn parameters governing the physics of the system,\nand further couple the learned physics to assist the learning of recurring\ndynamics. A spatio-temporal physics-coupled neural network (ST-PCNN) model is\nproposed to achieve three goals: (1) learning the underlying physics\nparameters, (2) transition of local information between spatio-temporal\nregions, and (3) forecasting future values for the dynamical system. The\nphysics-coupled learning ensures that the proposed model can be tremendously\nimproved by using learned physics parameters, and can achieve good long-range\nforecasting (e.g., more than 30-steps). Experiments, using simulated and\nfield-collected ocean current data, validate that ST-PCNN outperforms existing\nphysics-informed models.",
          "link": "http://arxiv.org/abs/2108.05940",
          "publishedOn": "2021-08-16T00:47:32.323Z",
          "wordCount": 624,
          "title": "ST-PCNN: Spatio-Temporal Physics-Coupled Neural Networks for Dynamics Forecasting. (arXiv:2108.05940v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davydov_V/0/1/0/all/0/1\">Vasilii Davydov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1\">Konstantin Yakovlev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr I. Panov</a>",
          "description": "In this paper, we consider the problem of multi-agent navigation in partially\nobservable grid environments. This problem is challenging for centralized\nplanning approaches as they, typically, rely on the full knowledge of the\nenvironment. We suggest utilizing the reinforcement learning approach when the\nagents, first, learn the policies that map observations to actions and then\nfollow these policies to reach their goals. To tackle the challenge associated\nwith learning cooperative behavior, i.e. in many cases agents need to yield to\neach other to accomplish a mission, we use a mixing Q-network that complements\nlearning individual policies. In the experimental evaluation, we show that such\napproach leads to plausible results and scales well to large number of agents.",
          "link": "http://arxiv.org/abs/2108.06148",
          "publishedOn": "2021-08-16T00:47:32.300Z",
          "wordCount": 575,
          "title": "Q-Mixing Network for Multi-Agent Pathfinding in Partially Observable Grid Environments. (arXiv:2108.06148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1\">Amir Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novin_R/0/1/0/all/0/1\">Roya Sabbagh Novin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merryweather_A/0/1/0/all/0/1\">Andrew Merryweather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1\">Tucker Hermans</a>",
          "description": "Ergonomics and human comfort are essential concerns in physical human-robot\ninteraction applications, and common practical methods either fail in\nestimating the correct posture due to occlusion or suffer from less accurate\nergonomics models in their postural optimization methods. Instead, we propose a\nnovel framework for posture estimation, assessment, and optimization for\nergonomically intelligent physical human-robot interaction. We show that we can\nestimate human posture solely from the trajectory of the interacting robot. We\npropose DULA, a differentiable ergonomics model, and use it in gradient-free\npostural optimization for physical human-robot interaction tasks such as\nco-manipulation and teleoperation. We evaluate our framework through human and\nsimulation experiments.",
          "link": "http://arxiv.org/abs/2108.05971",
          "publishedOn": "2021-08-16T00:47:32.295Z",
          "wordCount": 550,
          "title": "Ergonomically Intelligent Physical Human-Robot Interaction: Postural Estimation, Assessment, and Optimization. (arXiv:2108.05971v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Anh Tran</a>",
          "description": "Bayesian optimization (BO) is a flexible and powerful framework that is\nsuitable for computationally expensive simulation-based applications and\nguarantees statistical convergence to the global optimum. While remaining as\none of the most popular optimization methods, its capability is hindered by the\nsize of data, the dimensionality of the considered problem, and the nature of\nsequential optimization. These scalability issues are intertwined with each\nother and must be tackled simultaneously. In this work, we propose the\nScalable$^3$-BO framework, which employs sparse GP as the underlying surrogate\nmodel to scope with Big Data and is equipped with a random embedding to\nefficiently optimize high-dimensional problems with low effective\ndimensionality. The Scalable$^3$-BO framework is further leveraged with\nasynchronous parallelization feature, which fully exploits the computational\nresource on HPC within a computational budget. As a result, the proposed\nScalable$^3$-BO framework is scalable in three independent perspectives: with\nrespect to data size, dimensionality, and computational resource on HPC. The\ngoal of this work is to push the frontiers of BO beyond its well-known\nscalability issues and minimize the wall-clock waiting time for optimizing\nhigh-dimensional computationally expensive applications. We demonstrate the\ncapability of Scalable$^3$-BO with 1 million data points, 10,000-dimensional\nproblems, with 20 concurrent workers in an HPC environment.",
          "link": "http://arxiv.org/abs/2108.05969",
          "publishedOn": "2021-08-16T00:47:32.289Z",
          "wordCount": 665,
          "title": "Scalable3-BO: Big Data meets HPC - A scalable asynchronous parallel high-dimensional Bayesian optimization framework on supercomputers. (arXiv:2108.05969v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2001.03040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianfeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper establishes the optimal approximation error characterization of\ndeep rectified linear unit (ReLU) networks for smooth functions in terms of\nboth width and depth simultaneously. To that end, we first prove that\nmultivariate polynomials can be approximated by deep ReLU networks of width\n$\\mathcal{O}(N)$ and depth $\\mathcal{O}(L)$ with an approximation error\n$\\mathcal{O}(N^{-L})$. Through local Taylor expansions and their deep ReLU\nnetwork approximations, we show that deep ReLU networks of width\n$\\mathcal{O}(N\\ln N)$ and depth $\\mathcal{O}(L\\ln L)$ can approximate $f\\in\nC^s([0,1]^d)$ with a nearly optimal approximation error\n$\\mathcal{O}(\\|f\\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our estimate is\nnon-asymptotic in the sense that it is valid for arbitrary width and depth\nspecified by $N\\in\\mathbb{N}^+$ and $L\\in\\mathbb{N}^+$, respectively.",
          "link": "http://arxiv.org/abs/2001.03040",
          "publishedOn": "2021-08-16T00:47:32.283Z",
          "wordCount": 621,
          "title": "Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.08919",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alyaev_S/0/1/0/all/0/1\">Sergey Alyaev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shahriari_M/0/1/0/all/0/1\">Mostafa Shahriari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pardo_D/0/1/0/all/0/1\">David Pardo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Omella_A/0/1/0/all/0/1\">Angel Javier Omella</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Larsen_D/0/1/0/all/0/1\">David Larsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahani_N/0/1/0/all/0/1\">Nazanin Jahani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suter_E/0/1/0/all/0/1\">Erich Suter</a>",
          "description": "Modern geosteering is heavily dependent on real-time interpretation of deep\nelectromagnetic (EM) measurements. We present a methodology to construct a deep\nneural network (DNN) model trained to reproduce a full set of extra-deep EM\nlogs consisting of 22 measurements per logging position. The model is trained\nin a 1D layered environment consisting of up to seven layers with different\nresistivity values. A commercial simulator provided by a tool vendor is used to\ngenerate a training dataset. The dataset size is limited because the simulator\nprovided by the vendor is optimized for sequential execution. Therefore, we\ndesign a training dataset that embraces the geological rules and geosteering\nspecifics supported by the forward model. We use this dataset to produce an EM\nsimulator based on a DNN without access to the proprietary information about\nthe EM tool configuration or the original simulator source code. Despite\nemploying a relatively small training set size, the resulting DNN forward model\nis quite accurate for the considered examples: a multi-layer synthetic case and\na section of a published historical operation from the Goliat Field. The\nobserved average evaluation time of 0.15 ms per logging position makes it also\nsuitable for future use as part of evaluation-hungry statistical and/or\nMonte-Carlo inversion algorithms within geosteering workflows.",
          "link": "http://arxiv.org/abs/2005.08919",
          "publishedOn": "2021-08-16T00:47:32.263Z",
          "wordCount": 697,
          "title": "Modeling extra-deep electromagnetic logs using a deep neural network. (arXiv:2005.08919v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Divya Shyam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Atul Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1\">D. Roy Mahapatra</a>",
          "description": "This paper reports a reduced-order modeling framework of bladed disks on a\nrotating shaft to simulate the vibration signature of faults like cracks in\ndifferent components aiming towards simulated data-driven machine learning. We\nhave employed lumped and one-dimensional analytical models of the subcomponents\nfor better insight into the complex dynamic response. The framework seeks to\naddress some of the challenges encountered in analyzing and optimizing fault\ndetection and identification schemes for health monitoring of rotating\nturbomachinery, including aero-engines. We model the bladed disks and shafts by\ncombining lumped elements and one-dimensional finite elements, leading to a\ncoupled system. The simulation results are in good agreement with previously\npublished data. We model the cracks in a blade analytically with their\neffective reduced stiffness approximation. Multiple types of faults are\nmodeled, including cracks in the blades of single and two-stage bladed disks,\nFan Blade Off (FBO), and Foreign Object Damage (FOD). We have applied\naero-engine operational loading conditions to simulate realistic scenarios of\nonline health monitoring. The proposed reduced-order simulation framework will\nhave applications in probabilistic signal modeling, machine learning toward\nfault signature identification, and parameter estimation with measured\nvibration signals.",
          "link": "http://arxiv.org/abs/2108.06265",
          "publishedOn": "2021-08-16T00:47:32.248Z",
          "wordCount": 648,
          "title": "A reduced-order modeling framework for simulating signatures of faults in a bladed disk. (arXiv:2108.06265v1 [cs.CE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Conglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>",
          "description": "Recent works have demonstrated great success in training high-capacity\nautoregressive language models (GPT, GPT-2, GPT-3) on a huge amount of\nunlabeled text corpus for text generation. Despite showing great results, this\ngenerates two training efficiency challenges. First, training large corpora can\nbe extremely timing consuming, and how to present training samples to the model\nto improve the token-wise convergence speed remains a challenging and open\nquestion. Second, many of these large models have to be trained with hundreds\nor even thousands of processors using data-parallelism with a very large batch\nsize. Despite of its better compute efficiency, it has been observed that\nlarge-batch training often runs into training instability issue or converges to\nsolutions with bad generalization performance. To overcome these two\nchallenges, we present a study of a curriculum learning based approach, which\nhelps improves the pre-training convergence speed of autoregressive models.\nMore importantly, we find that curriculum learning, as a regularization method,\nexerts a gradient variance reduction effect and enables to train autoregressive\nmodels with much larger batch sizes and learning rates without training\ninstability, further improving the training speed. Our evaluations demonstrate\nthat curriculum learning enables training GPT-2 models (with up to 1.5B\nparameters) with 8x larger batch size and 4x larger learning rate, whereas the\nbaseline approach struggles with training divergence. To achieve the same\nvalidation perplexity targets during pre-training, curriculum learning reduces\nthe required number of tokens and wall clock time by up to 59% and 54%,\nrespectively. To achieve the same or better zero-shot WikiText-103/LAMBADA\nevaluation results at the end of pre-training, curriculum learning reduces the\nrequired number of tokens and wall clock time by up to 13% and 61%,\nrespectively.",
          "link": "http://arxiv.org/abs/2108.06084",
          "publishedOn": "2021-08-16T00:47:32.242Z",
          "wordCount": 721,
          "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. (arXiv:2108.06084v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>",
          "description": "Both latent semantic analysis (LSA) and correspondence analysis (CA) use a\nsingular value decomposition (SVD) for dimensionality reduction. In this\narticle, LSA and CA are compared from a theoretical point of view and applied\nin both a toy example and an authorship attribution example. In text mining\ninterest goes out to the relationships among documents and terms: for example,\nwhat terms are more often used in what documents. However, the LSA solution\ndisplays a mix of marginal effects and these relationships. It appears that CA\nhas more attractive properties than LSA. One such property is that, in CA, the\neffect of the margins is effectively eliminated, so that the CA solution is\noptimally suited to focus on the relationships among documents and terms. Three\nmechanisms are distinguished to weight documents and terms, and a unifying\nframework is proposed that includes these three mechanisms and includes both CA\nand LSA as special cases. In the authorship attribution example, the national\nanthem of the Netherlands, the application of the discussed methods is\nillustrated.",
          "link": "http://arxiv.org/abs/2108.06197",
          "publishedOn": "2021-08-16T00:47:32.236Z",
          "wordCount": 621,
          "title": "A Comparison of Latent Semantic Analysis and Correspondence Analysis for Text Mining. (arXiv:2108.06197v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jeongwhan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jinsung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>",
          "description": "Collaborative filtering (CF) is a long-standing problem of recommender\nsystems. Many novel methods have been proposed, ranging from classical matrix\nfactorization to recent graph convolutional network-based approaches. After\nrecent fierce debates, researchers started to focus on linear graph\nconvolutional networks (GCNs) with a layer combination, which show\nstate-of-the-art accuracy in many datasets. In this work, we extend them based\non neural ordinary differential equations (NODEs), because the linear GCN\nconcept can be interpreted as a differential equation, and present the method\nof Learnable-Time ODE-based Collaborative Filtering (LT-OCF). The main novelty\nin our method is that after redesigning linear GCNs on top of the NODE regime,\ni) we learn the optimal architecture rather than relying on manually designed\nones, ii) we learn smooth ODE solutions that are considered suitable for CF,\nand iii) we test with various ODE solvers that internally build a diverse set\nof neural network connections. We also present a novel training method\nspecialized to our method. In our experiments with three benchmark datasets,\nGowalla, Yelp2018, and Amazon-Book, our method consistently shows better\naccuracy than existing methods, e.g., a recall of 0.0411 by LightGCN vs. 0.0442\nby LT-OCF and an NDCG of 0.0315 by LightGCN vs. 0.0341 by LT-OCF in\nAmazon-Book. One more important discovery in our experiments that is worth\nmentioning is that our best accuracy was achieved by dense connections rather\nthan linear connections.",
          "link": "http://arxiv.org/abs/2108.06208",
          "publishedOn": "2021-08-16T00:47:32.170Z",
          "wordCount": 673,
          "title": "LT-OCF: Learnable-Time ODE-based Collaborative Filtering. (arXiv:2108.06208v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.06217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storchan_V/0/1/0/all/0/1\">Victor Storchan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurshan_E/0/1/0/all/0/1\">Eren Kurshan</a>",
          "description": "We review practical challenges in building and deploying ethical AI at the\nscale of contemporary industrial and societal uses. Apart from the purely\ntechnical concerns that are the usual focus of academic research, the\noperational challenges of inconsistent regulatory pressures, conflicting\nbusiness goals, data quality issues, development processes, systems integration\npractices, and the scale of deployment all conspire to create new ethical\nrisks. Such ethical concerns arising from these practical considerations are\nnot adequately addressed by existing research results. We argue that a holistic\nconsideration of ethics in the development and deployment of AI systems is\nnecessary for building ethical AI in practice, and exhort researchers to\nconsider the full operational contexts of AI systems when assessing ethical\nrisks.",
          "link": "http://arxiv.org/abs/2108.06217",
          "publishedOn": "2021-08-16T00:47:32.143Z",
          "wordCount": 586,
          "title": "Beyond Fairness Metrics: Roadblocks and Challenges for Ethical AI in Practice. (arXiv:2108.06217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitin Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Hima Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_S/0/1/0/all/0/1\">Shazia Afzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panwar_N/0/1/0/all/0/1\">Naveen Panwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_R/0/1/0/all/0/1\">Ruhi Sharma Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttula_S/0/1/0/all/0/1\">Shanmukha Guttula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Abhinav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagalapatti_L/0/1/0/all/0/1\">Lokesh Nagalapatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sameep Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hans_S/0/1/0/all/0/1\">Sandeep Hans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohia_P/0/1/0/all/0/1\">Pranay Lohia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1\">Aniya Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_D/0/1/0/all/0/1\">Diptikalyan Saha</a>",
          "description": "The quality of training data has a huge impact on the efficiency, accuracy\nand complexity of machine learning tasks. Various tools and techniques are\navailable that assess data quality with respect to general cleaning and\nprofiling checks. However these techniques are not applicable to detect data\nissues in the context of machine learning tasks, like noisy labels, existence\nof overlapping classes etc. We attempt to re-look at the data quality issues in\nthe context of building a machine learning pipeline and build a tool that can\ndetect, explain and remediate issues in the data, and systematically and\nautomatically capture all the changes applied to the data. We introduce the\nData Quality Toolkit for machine learning as a library of some key quality\nmetrics and relevant remediation techniques to analyze and enhance the\nreadiness of structured training datasets for machine learning projects. The\ntoolkit can reduce the turn-around times of data preparation pipelines and\nstreamline the data quality assessment process. Our toolkit is publicly\navailable via IBM API Hub [1] platform, any developer can assess the data\nquality using the IBM's Data Quality for AI apis [2]. Detailed tutorials are\nalso available on IBM Learning Path [3].",
          "link": "http://arxiv.org/abs/2108.05935",
          "publishedOn": "2021-08-16T00:47:32.102Z",
          "wordCount": 657,
          "title": "Data Quality Toolkit: Automatic assessment of data quality and remediation for machine learning datasets. (arXiv:2108.05935v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andrade_Loarca_H/0/1/0/all/0/1\">H&#xe9;ctor Andrade-Loarca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1\">Gitta Kutyniok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktem_O/0/1/0/all/0/1\">Ozan &#xd6;ktem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_P/0/1/0/all/0/1\">Philipp Petersen</a>",
          "description": "We present a deep learning-based algorithm to jointly solve a reconstruction\nproblem and a wavefront set extraction problem in tomographic imaging. The\nalgorithm is based on a recently developed digital wavefront set extractor as\nwell as the well-known microlocal canonical relation for the Radon transform.\nWe use the wavefront set information about x-ray data to improve the\nreconstruction by requiring that the underlying neural networks simultaneously\nextract the correct ground truth wavefront set and ground truth image. As a\nnecessary theoretical step, we identify the digital microlocal canonical\nrelations for deep convolutional residual neural networks. We find strong\nnumerical evidence for the effectiveness of this approach.",
          "link": "http://arxiv.org/abs/2108.05732",
          "publishedOn": "2021-08-13T01:56:57.451Z",
          "wordCount": 558,
          "title": "Deep Microlocal Reconstruction for Limited-Angle Tomography. (arXiv:2108.05732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>",
          "description": "This paper aims at addressing the problem of substantial performance\ndegradation at extremely low computational cost (e.g. 5M FLOPs on ImageNet\nclassification). We found that two factors, sparse connectivity and dynamic\nactivation function, are effective to improve the accuracy. The former avoids\nthe significant reduction of network width, while the latter mitigates the\ndetriment of reduction in network depth. Technically, we propose\nmicro-factorized convolution, which factorizes a convolution matrix into low\nrank matrices, to integrate sparse connectivity into convolution. We also\npresent a new dynamic activation function, named Dynamic Shift Max, to improve\nthe non-linearity via maxing out multiple dynamic fusions between an input\nfeature map and its circular channel shift. Building upon these two new\noperators, we arrive at a family of networks, named MicroNet, that achieves\nsignificant performance gains over the state of the art in the low FLOP regime.\nFor instance, under the constraint of 12M FLOPs, MicroNet achieves 59.4\\% top-1\naccuracy on ImageNet classification, outperforming MobileNetV3 by 9.6\\%. Source\ncode is at\n\\href{https://github.com/liyunsheng13/micronet}{https://github.com/liyunsheng13/micronet}.",
          "link": "http://arxiv.org/abs/2108.05894",
          "publishedOn": "2021-08-13T01:56:57.444Z",
          "wordCount": 633,
          "title": "MicroNet: Improving Image Recognition with Extremely Low FLOPs. (arXiv:2108.05894v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyubok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>",
          "description": "Accurate terminology translation is crucial for ensuring the practicality and\nreliability of neural machine translation (NMT) systems. To address this,\nlexically constrained NMT explores various methods to ensure pre-specified\nwords and phrases appear in the translation output. However, in many cases,\nthose methods are studied on general domain corpora, where the terms are mostly\nuni- and bi-grams (>98%). In this paper, we instead tackle a more challenging\nsetup consisting of domain-specific corpora with much longer n-gram and highly\nspecialized terms. Inspired by the recent success of masked span prediction\nmodels, we propose a simple and effective training strategy that achieves\nconsistent improvements on both terminology and sentence-level translation for\nthree domain-specific corpora in two language pairs.",
          "link": "http://arxiv.org/abs/2105.05498",
          "publishedOn": "2021-08-13T01:56:57.438Z",
          "wordCount": 607,
          "title": "Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction. (arXiv:2105.05498v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.11057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qinliang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijia Zhang</a>",
          "description": "Textual network embeddings aim to learn a low-dimensional representation for\nevery node in the network so that both the structural and textual information\nfrom the networks can be well preserved in the representations. Traditionally,\nthe structural and textual embeddings were learned by models that rarely take\nthe mutual influences between them into account. In this paper, a deep neural\narchitecture is proposed to effectively fuse the two kinds of informations into\none representation. The novelties of the proposed architecture are manifested\nin the aspects of a newly defined objective function, the complementary\ninformation fusion method for structural and textual features, and the mutual\ngate mechanism for textual feature extraction. Experimental results show that\nthe proposed model outperforms the comparing methods on all three datasets.",
          "link": "http://arxiv.org/abs/1908.11057",
          "publishedOn": "2021-08-13T01:56:57.432Z",
          "wordCount": 622,
          "title": "A Deep Neural Information Fusion Architecture for Textual Network Embeddings. (arXiv:1908.11057v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.00215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Resnick_C/0/1/0/all/0/1\">Cinjon Resnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zeping Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Self-supervised research improved greatly over the past half decade, with\nmuch of the growth being driven by objectives that are hard to quantitatively\ncompare. These techniques include colorization, cyclical consistency, and\nnoise-contrastive estimation from image patches. Consequently, the field has\nsettled on a handful of measurements that depend on linear probes to adjudicate\nwhich approaches are the best. Our first contribution is to show that this test\nis insufficient and that models which perform poorly (strongly) on linear\nclassification can perform strongly (weakly) on more involved tasks like\ntemporal activity localization. Our second contribution is to analyze the\ncapabilities of five different representations. And our third contribution is a\nmuch needed new dataset for temporal activity localization.",
          "link": "http://arxiv.org/abs/1912.00215",
          "publishedOn": "2021-08-13T01:56:57.407Z",
          "wordCount": 597,
          "title": "Probing the State of the Art: A Critical Look at Visual Representation Evaluation. (arXiv:1912.00215v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samo_Y/0/1/0/all/0/1\">Yves-Laurent Kom Samo</a>",
          "description": "We introduce the first application of the lean methodology to machine\nlearning projects. Similar to lean startups and lean manufacturing, we argue\nthat lean machine learning (LeanML) can drastically slash avoidable wastes in\ncommercial machine learning projects, reduce the business risk in investing in\nmachine learning capabilities and, in so doing, further democratize access to\nmachine learning. The lean design pattern we propose in this paper is based on\ntwo realizations. First, it is possible to estimate the best performance one\nmay achieve when predicting an outcome $y \\in \\mathcal{Y}$ using a given set of\nexplanatory variables $x \\in \\mathcal{X}$, for a wide range of performance\nmetrics, and without training any predictive model. Second, doing so is\nconsiderably easier, faster, and cheaper than learning the best predictive\nmodel. We derive formulae expressing the best $R^2$, MSE, classification\naccuracy, and log-likelihood per observation achievable when using $x$ to\npredict $y$ as a function of the mutual information $I\\left(y; x\\right)$, and\npossibly a measure of the variability of $y$ (e.g. its Shannon entropy in the\ncase of classification accuracy, and its variance in the case regression MSE).\nWe illustrate the efficacy of the LeanML design pattern on a wide range of\nregression and classification problems, synthetic and real-life.",
          "link": "http://arxiv.org/abs/2107.08066",
          "publishedOn": "2021-08-13T01:56:57.399Z",
          "wordCount": 672,
          "title": "LeanML: A Design Pattern To Slash Avoidable Wastes in Machine Learning Projects. (arXiv:2107.08066v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Ben Green</a>",
          "description": "In the face of compounding crises of social and economic inequality, many\nhave turned to algorithmic decision-making to achieve greater fairness in\nsociety. As these efforts intensify, reasoning within the burgeoning field of\n\"algorithmic fairness\" increasingly shapes how fairness manifests in practice.\nThis paper interrogates whether algorithmic fairness provides the appropriate\nconceptual and practical tools for enhancing social equality. I argue that the\ndominant, \"formal\" approach to algorithmic fairness is ill-equipped as a\nframework for pursuing equality, as its narrow frame of analysis generates\nrestrictive approaches to reform. In light of these shortcomings, I propose an\nalternative: a \"substantive\" approach to algorithmic fairness that centers\nopposition to social hierarchies and provides a more expansive analysis of how\nto address inequality. This substantive approach enables more fruitful\ntheorizing about the role of algorithms in combatting oppression. The\ndistinction between formal and substantive algorithmic fairness is exemplified\nby each approach's responses to the \"impossibility of fairness\" (an\nincompatibility between mathematical definitions of algorithmic fairness).\nWhile the formal approach requires us to accept the \"impossibility of fairness\"\nas a harsh limit on efforts to enhance equality, the substantive approach\nallows us to escape the \"impossibility of fairness\" by suggesting reforms that\nare not subject to this false dilemma and that are better equipped to\nameliorate conditions of social oppression.",
          "link": "http://arxiv.org/abs/2107.04642",
          "publishedOn": "2021-08-13T01:56:57.394Z",
          "wordCount": 677,
          "title": "Escaping the \"Impossibility of Fairness\": From Formal to Substantive Algorithmic Fairness. (arXiv:2107.04642v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.05330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Arash A. Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paez_M/0/1/0/all/0/1\">Marina S. Paez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lizhen Lin</a>",
          "description": "Multiplex networks have become increasingly more prevalent in many fields,\nand have emerged as a powerful tool for modeling the complexity of real\nnetworks. There is a critical need for developing inference models for\nmultiplex networks that can take into account potential dependencies across\ndifferent layers, particularly when the aim is community detection. We add to a\nlimited literature by proposing a novel and efficient Bayesian model for\ncommunity detection in multiplex networks. A key feature of our approach is the\nability to model varying communities at different network layers. In contrast,\nmany existing models assume the same communities for all layers. Moreover, our\nmodel automatically picks up the necessary number of communities at each layer\n(as validated by real data examples). This is appealing, since deciding the\nnumber of communities is a challenging aspect of community detection, and\nespecially so in the multiplex setting, if one allows the communities to change\nacross layers. Borrowing ideas from hierarchical Bayesian modeling, we use a\nhierarchical Dirichlet prior to model community labels across layers, allowing\ndependency in their structure. Given the community labels, a stochastic block\nmodel (SBM) is assumed for each layer. We develop an efficient slice sampler\nfor sampling the posterior distribution of the community labels as well as the\nlink probabilities between communities. In doing so, we address some unique\nchallenges posed by coupling the complex likelihood of SBM with the\nhierarchical nature of the prior on the labels. An extensive empirical\nvalidation is performed on simulated and real data, demonstrating the superior\nperformance of the model over single-layer alternatives, as well as the ability\nto uncover interesting structures in real networks.",
          "link": "http://arxiv.org/abs/1904.05330",
          "publishedOn": "2021-08-13T01:56:57.366Z",
          "wordCount": 754,
          "title": "Hierarchical Stochastic Block Model for Community Detection in Multiplex Networks. (arXiv:1904.05330v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiss_T/0/1/0/all/0/1\">Tal Reiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Niv Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergman_L/0/1/0/all/0/1\">Liron Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Anomaly detection methods require high-quality features. In recent years, the\nanomaly detection community has attempted to obtain better features using\nadvances in deep self-supervised feature learning. Surprisingly, a very\npromising direction, using pretrained deep features, has been mostly\noverlooked. In this paper, we first empirically establish the perhaps expected,\nbut unreported result, that combining pretrained features with simple anomaly\ndetection and segmentation methods convincingly outperforms, much more complex,\nstate-of-the-art methods.\n\nIn order to obtain further performance gains in anomaly detection, we adapt\npretrained features to the target distribution. Although transfer learning\nmethods are well established in multi-class classification problems, the\none-class classification (OCC) setting is not as well explored. It turns out\nthat naive adaptation methods, which typically work well in supervised\nlearning, often result in catastrophic collapse (feature deterioration) and\nreduce performance in OCC settings. A popular OCC method, DeepSVDD, advocates\nusing specialized architectures, but this limits the adaptation performance\ngain. We propose two methods for combating collapse: i) a variant of early\nstopping that dynamically learns the stopping iteration ii) elastic\nregularization inspired by continual learning. Our method, PANDA, outperforms\nthe state-of-the-art in the OCC, outlier exposure and anomaly segmentation\nsettings by large margins.",
          "link": "http://arxiv.org/abs/2010.05903",
          "publishedOn": "2021-08-13T01:56:57.356Z",
          "wordCount": 676,
          "title": "PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation. (arXiv:2010.05903v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ajinkya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giguere_S/0/1/0/all/0/1\">Stephen Giguere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1\">Rudolf Lioutikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>",
          "description": "We propose a method that efficiently learns distributions over articulation\nmodel parameters directly from depth images without the need to know\narticulation model categories a priori. By contrast, existing methods that\nlearn articulation models from raw observations typically only predict point\nestimates of the model parameters, which are insufficient to guarantee the safe\nmanipulation of articulated objects. Our core contributions include a novel\nrepresentation for distributions over rigid body transformations and\narticulation model parameters based on screw theory, von Mises-Fisher\ndistributions, and Stiefel manifolds. Combining these concepts allows for an\nefficient, mathematically sound representation that implicitly satisfies the\nconstraints that rigid body transformations and articulations must adhere to.\nLeveraging this representation, we introduce a novel deep learning based\napproach, DUST-net, that performs category-independent articulation model\nestimation while also providing model uncertainties. We evaluate our approach\non several benchmarking datasets and real-world objects and compare its\nperformance with two current state-of-the-art methods. Our results demonstrate\nthat DUST-net can successfully learn distributions over articulation models for\nnovel objects across articulation model categories, which generate point\nestimates with better accuracy than state-of-the-art methods and effectively\ncapture the uncertainty over predicted model parameters due to noisy inputs.",
          "link": "http://arxiv.org/abs/2108.05875",
          "publishedOn": "2021-08-13T01:56:57.338Z",
          "wordCount": 647,
          "title": "Distributional Depth-Based Estimation of Object Articulation Models. (arXiv:2108.05875v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1\">Semih Cayci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1\">Niao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "Natural policy gradient (NPG) methods with function approximation achieve\nimpressive empirical success in reinforcement learning problems with large\nstate-action spaces. However, theoretical understanding of their convergence\nbehaviors remains limited in the function approximation setting. In this paper,\nwe perform a finite-time analysis of NPG with linear function approximation and\nsoftmax parameterization, and prove for the first time that widely used entropy\nregularization method, which encourages exploration, leads to linear\nconvergence rate. Under considerably weaker regularity conditions, we prove\nthat entropy-regularized Q-NPG variant with linear function approximation\nachieves $\\tilde{O}(1/T)$ convergence rate. We adopt a Lyapunov drift analysis\nto prove the convergence results and explain the effectiveness of entropy\nregularization in improving the convergence rates.",
          "link": "http://arxiv.org/abs/2106.04096",
          "publishedOn": "2021-08-13T01:56:57.332Z",
          "wordCount": 587,
          "title": "Linear Convergence of Entropy-Regularized Natural Policy Gradient with Linear Function Approximation. (arXiv:2106.04096v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1\">Pravin Chandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Raghavendra Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_A/0/1/0/all/0/1\">Avinash Chakravarthi</a>",
          "description": "Federated Learning (FL) solves many of this decade's concerns regarding data\nprivacy and computation challenges. FL ensures no data leaves its source as the\nmodel is trained at where the data resides. However, FL comes with its own set\nof challenges. The communication of model weight updates in this distributed\nenvironment comes with significant network bandwidth costs. In this context, we\npropose a mechanism of compressing the weight updates using Autoencoders (AE),\nwhich learn the data features of the weight updates and subsequently perform\ncompression. The encoder is set up on each of the nodes where the training is\nperformed while the decoder is set up on the node where the weights are\naggregated. This setup achieves compression through the encoder and recreates\nthe weights at the end of every communication round using the decoder. This\npaper shows that the dynamic and orthogonal AE based weight compression\ntechnique could serve as an advantageous alternative (or an add-on) in a large\nscale FL, as it not only achieves compression ratios ranging from 500x to 1720x\nand beyond, but can also be modified based on the accuracy requirements,\ncomputational capacity, and other requirements of the given FL setup.",
          "link": "http://arxiv.org/abs/2108.05670",
          "publishedOn": "2021-08-13T01:56:57.327Z",
          "wordCount": 671,
          "title": "Communication Optimization in Large Scale Federated Learning using Autoencoder Compressed Weight Updates. (arXiv:2108.05670v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.06294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhewei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_W/0/1/0/all/0/1\">Wen Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>",
          "description": "We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video\nFrame Interpolation (VFI). Many recent flow-based VFI methods first estimate\nthe bi-directional optical flows, then scale and reverse them to approximate\nintermediate flows, leading to artifacts on motion boundaries. RIFE uses a\nneural network named IFNet that can directly estimate the intermediate flows\nfrom coarse-to-fine with much better speed. We design a privileged distillation\nscheme for training intermediate flow model, which leads to a large performance\nimprovement. Experiments demonstrate that RIFE is flexible and can achieve\nstate-of-the-art performance on several public benchmarks. The code is\navailable at \\url{https://github.com/hzwer/arXiv2020-RIFE}",
          "link": "http://arxiv.org/abs/2011.06294",
          "publishedOn": "2021-08-13T01:56:57.321Z",
          "wordCount": 623,
          "title": "RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation. (arXiv:2011.06294v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihwan Kim</a>",
          "description": "All learning algorithms for recommendations face inevitable and critical\ntrade-off between exploiting partial knowledge of a user's preferences for\nshort-term satisfaction and exploring additional user preferences for long-term\ncoverage. Although exploration is indispensable for long success of a\nrecommender system, the exploration has been considered as the risk to decrease\nuser satisfaction. The reason for the risk is that items chosen for exploration\nfrequently mismatch with the user's interests. To mitigate this risk,\nrecommender systems have mixed items chosen for exploration into a\nrecommendation list, disguising the items as recommendations to elicit feedback\non the items to discover the user's additional tastes. This mix-in approach has\nbeen widely used in many recommenders, but there is rare research, evaluating\nthe effectiveness of the mix-in approach or proposing a new approach for\neliciting user feedback without deceiving users. In this work, we aim to\npropose a new approach for feedback elicitation without any deception and\ncompare our approach to the conventional mix-in approach for evaluation. To\nthis end, we designed a recommender interface that reveals which items are for\nexploration and conducted a within-subject study with 94 MTurk workers. Our\nresults indicated that users left significantly more feedback on items chosen\nfor exploration with our interface. Besides, users evaluated that our new\ninterface is better than the conventional mix-in interface in terms of novelty,\ndiversity, transparency, trust, and satisfaction. Finally, path analysis show\nthat, in only our new interface, exploration caused to increase user-centric\nevaluation metrics. Our work paves the way for how to design an interface,\nwhich utilizes learning algorithm based on users' feedback signals, giving\nbetter user experience and gathering more feedback data.",
          "link": "http://arxiv.org/abs/2108.00151",
          "publishedOn": "2021-08-13T01:56:57.316Z",
          "wordCount": 725,
          "title": "An Empirical Analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chih-Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rongjie Yan</a>",
          "description": "Continuous engineering of autonomous driving functions commonly requires\ndeploying vehicles in road testing to obtain inputs that cause problematic\ndecisions. Although the discovery leads to producing an improved system, it\nalso challenges the foundation of testing using equivalence classes and the\nassociated relative test coverage criterion. In this paper, we propose believed\nequivalence, where the establishment of an equivalence class is initially based\non expert belief and is subject to a set of available test cases having a\nconsistent valuation. Upon a newly encountered test case that breaks the\nconsistency, one may need to refine the established categorization in order to\nsplit the originally believed equivalence into two. Finally, we focus on\nmodules implemented using deep neural networks where every category partitions\nan input over the real domain. We present both analytical and lazy methods to\nsuggest the refinement. The concept is demonstrated in analyzing multiple\nautonomous driving modules, indicating the potential of our proposed approach.",
          "link": "http://arxiv.org/abs/2103.04578",
          "publishedOn": "2021-08-13T01:56:57.300Z",
          "wordCount": 629,
          "title": "Testing Autonomous Systems with Believed Equivalence Refinement. (arXiv:2103.04578v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kucherenko_T/0/1/0/all/0/1\">Taras Kucherenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_R/0/1/0/all/0/1\">Rajmund Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_M/0/1/0/all/0/1\">Michael Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>",
          "description": "Embodied conversational agents benefit from being able to accompany their\nspeech with gestures. Although many data-driven approaches to gesture\ngeneration have been proposed in recent years, it is still unclear whether such\nsystems can consistently generate gestures that convey meaning. We investigate\nwhich gesture properties (phase, category, and semantics) can be predicted from\nspeech text and/or audio using contemporary deep learning. In extensive\nexperiments, we show that gesture properties related to gesture meaning\n(semantics and category) are predictable from text features (time-aligned BERT\nembeddings) alone, but not from prosodic audio features, while rhythm-related\ngesture properties (phase) on the other hand can be predicted from either\naudio, text (with word-level timing information), or both. These results are\nencouraging as they indicate that it is possible to equip an embodied agent\nwith content-wise meaningful co-speech gestures using a machine-learning model.",
          "link": "http://arxiv.org/abs/2108.05762",
          "publishedOn": "2021-08-13T01:56:57.295Z",
          "wordCount": 583,
          "title": "Multimodal analysis of the predictability of hand-gesture properties. (arXiv:2108.05762v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Proper initialization is crucial to the optimization and the generalization\nof neural networks. However, most existing neural recommendation systems\ninitialize the user and item embeddings randomly. In this work, we propose a\nnew initialization scheme for user and item embeddings called Laplacian\nEigenmaps with Popularity-based Regularization for Isolated Data (LEPORID).\nLEPORID endows the embeddings with information regarding multi-scale\nneighborhood structures on the data manifold and performs adaptive\nregularization to compensate for high embedding variance on the tail of the\ndata distribution. Exploiting matrix sparsity, LEPORID embeddings can be\ncomputed efficiently. We evaluate LEPORID in a wide range of neural\nrecommendation models. In contrast to the recent surprising finding that the\nsimple K-nearest-neighbor (KNN) method often outperforms neural recommendation\nsystems, we show that existing neural systems initialized with LEPORID often\nperform on par or better than KNN. To maximize the effects of the\ninitialization, we propose the Dual-Loss Residual Recommendation (DLR2)\nnetwork, which, when initialized with LEPORID, substantially outperforms both\ntraditional and state-of-the-art neural recommender systems.",
          "link": "http://arxiv.org/abs/2106.04993",
          "publishedOn": "2021-08-13T01:56:57.290Z",
          "wordCount": 635,
          "title": "Initialization Matters: Regularizing Manifold-informed Initialization for Neural Recommendation Systems. (arXiv:2106.04993v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.10069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gravell_B/0/1/0/all/0/1\">Benjamin Gravell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summers_T/0/1/0/all/0/1\">Tyler Summers</a>",
          "description": "Despite decades of research and recent progress in adaptive control and\nreinforcement learning, there remains a fundamental lack of understanding in\ndesigning controllers that provide robustness to inherent non-asymptotic\nuncertainties arising from models estimated with finite, noisy data. We propose\na robust adaptive control algorithm that explicitly incorporates such\nnon-asymptotic uncertainties into the control design. The algorithm has three\ncomponents: (1) a least-squares nominal model estimator; (2) a bootstrap\nresampling method that quantifies non-asymptotic variance of the nominal model\nestimate; and (3) a non-conventional robust control design method using an\noptimal linear quadratic regulator (LQR) with multiplicative noise. A key\nadvantage of the proposed approach is that the system identification and robust\ncontrol design procedures both use stochastic uncertainty representations, so\nthat the actual inherent statistical estimation uncertainty directly aligns\nwith the uncertainty the robust controller is being designed against. We show\nthrough numerical experiments that the proposed robust adaptive controller can\nsignificantly outperform the certainty equivalent controller on both expected\nregret and measures of regret risk.",
          "link": "http://arxiv.org/abs/2002.10069",
          "publishedOn": "2021-08-13T01:56:57.284Z",
          "wordCount": 651,
          "title": "Robust Learning-Based Control via Bootstrapped Multiplicative Noise. (arXiv:2002.10069v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Podimata_C/0/1/0/all/0/1\">Chara Podimata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>",
          "description": "Lipschitz bandits is a prominent version of multi-armed bandits that studies\nlarge, structured action spaces such as the [0,1] interval, where similar\nactions are guaranteed to have similar rewards. A central theme here is the\nadaptive discretization of the action space, which gradually ``zooms in'' on\nthe more promising regions thereof. The goal is to take advantage of ``nicer''\nproblem instances, while retaining near-optimal worst-case performance. While\nthe stochastic version of the problem is well-understood, the general version\nwith adversarial rewards is not. We provide the first algorithm for adaptive\ndiscretization in the adversarial version, and derive instance-dependent regret\nbounds. In particular, we recover the worst-case optimal regret bound for the\nadversarial version, and the instance-dependent regret bound for the stochastic\nversion. Further, an application of our algorithm to dynamic pricing (where a\nseller repeatedly adjusts prices for a product) enjoys these regret bounds\nwithout any smoothness assumptions.",
          "link": "http://arxiv.org/abs/2006.12367",
          "publishedOn": "2021-08-13T01:56:57.278Z",
          "wordCount": 634,
          "title": "Adaptive Discretization for Adversarial Lipschitz Bandits. (arXiv:2006.12367v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hawke_J/0/1/0/all/0/1\">Jeffrey Hawke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_H/0/1/0/all/0/1\">Haibo E</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badrinarayanan_V/0/1/0/all/0/1\">Vijay Badrinarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kendall_A/0/1/0/all/0/1\">Alex Kendall</a>",
          "description": "The self driving challenge in 2021 is this century's technological equivalent\nof the space race, and is now entering the second major decade of development.\nSolving the technology will create social change which parallels the invention\nof the automobile itself. Today's autonomous driving technology is laudable,\nthough rooted in decisions made a decade ago. We argue that a rethink is\nrequired, reconsidering the autonomous vehicle (AV) problem in the light of the\nbody of knowledge that has been gained since the DARPA challenges which seeded\nthe industry. What does AV2.0 look like? We present an alternative vision: a\nrecipe for driving with machine learning, and grand challenges for research in\ndriving.",
          "link": "http://arxiv.org/abs/2108.05805",
          "publishedOn": "2021-08-13T01:56:57.272Z",
          "wordCount": 542,
          "title": "Reimagining an autonomous vehicle. (arXiv:2108.05805v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1\">Peter Kairouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1\">Thomas Steinke</a>",
          "description": "We consider training models on private data that are distributed across user\ndevices. To ensure privacy, we add on-device noise and use secure aggregation\nso that only the noisy sum is revealed to the server. We present a\ncomprehensive end-to-end system, which appropriately discretizes the data and\nadds discrete Gaussian noise before performing secure aggregation. We provide a\nnovel privacy analysis for sums of discrete Gaussians and carefully analyze the\neffects of data quantization and modular summation arithmetic. Our theoretical\nguarantees highlight the complex tension between communication, privacy, and\naccuracy. Our extensive experimental results demonstrate that our solution is\nessentially able to match the accuracy to central differential privacy with\nless than 16 bits of precision per value.",
          "link": "http://arxiv.org/abs/2102.06387",
          "publishedOn": "2021-08-13T01:56:57.256Z",
          "wordCount": 613,
          "title": "The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation. (arXiv:2102.06387v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.02572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yidong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuming Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingyu Wu</a>",
          "description": "In this paper we present an end-to-end framework for addressing the problem\nof dynamic pricing (DP) on E-commerce platform using methods based on deep\nreinforcement learning (DRL). By using four groups of different business data\nto represent the states of each time period, we model the dynamic pricing\nproblem as a Markov Decision Process (MDP). Compared with the state-of-the-art\nDRL-based dynamic pricing algorithms, our approaches make the following three\ncontributions. First, we extend the discrete set problem to the continuous\nprice set. Second, instead of using revenue as the reward function directly, we\ndefine a new function named difference of revenue conversion rates (DRCR).\nThird, the cold-start problem of MDP is tackled by pre-training and evaluation\nusing some carefully chosen historical sales data. Our approaches are evaluated\nby both offline evaluation method using real dataset of Alibaba Inc., and\nonline field experiments starting from July 2018 with thousands of items,\nlasting for months on Tmall.com. To our knowledge, there is no other DP field\nexperiment using DRL before. Field experiment results suggest that DRCR is a\nmore appropriate reward function than revenue, which is widely used by current\nliterature. Also, continuous price sets have better performance than discrete\nsets and our approaches significantly outperformed the manual pricing by\noperation experts.",
          "link": "http://arxiv.org/abs/1912.02572",
          "publishedOn": "2021-08-13T01:56:57.251Z",
          "wordCount": 692,
          "title": "Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning: A Field Experiment. (arXiv:1912.02572v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Yishai_A/0/1/0/all/0/1\">Assaf Ben-Yishai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordentlich_O/0/1/0/all/0/1\">Or Ordentlich</a>",
          "description": "The construction of multiclass classifiers from binary elements is studied in\nthis paper, and performance is quantified by the regret, defined with respect\nto the Bayes optimal log-loss. We discuss two known methods. The first is one\nvs. all (OVA), for which we prove that the multiclass regret is upper bounded\nby the sum of binary regrets of the constituent classifiers. The second is\nhierarchical classification, based on a binary tree. For this method we prove\nthat the multiclass regret is exactly a weighted sum of constituent binary\nregrets where the weighing is determined by the tree structure.\n\nWe also introduce a leverage-hierarchical classification method, which\npotentially yields smaller log-loss and regret. The advantages of these\nclassification methods are demonstrated by simulation on both synthetic and\nreal-life datasets.",
          "link": "http://arxiv.org/abs/2102.08184",
          "publishedOn": "2021-08-13T01:56:57.245Z",
          "wordCount": 608,
          "title": "Constructing Multiclass Classifiers using Binary Classifiers Under Log-Loss. (arXiv:2102.08184v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahav_O/0/1/0/all/0/1\">Ori Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Guy Katz</a>",
          "description": "Deep neural networks (DNNs) play an increasingly important role in various\ncomputer systems. In order to create these networks, engineers typically\nspecify a desired topology, and then use an automated training algorithm to\nselect the network's weights. While training algorithms have been studied\nextensively and are well understood, the selection of topology remains a form\nof art, and can often result in networks that are unnecessarily large - and\nconsequently are incompatible with end devices that have limited memory,\nbattery or computational power. Here, we propose to address this challenge by\nharnessing recent advances in DNN verification. We present a framework and a\nmethodology for discovering redundancies in DNNs - i.e., for finding neurons\nthat are not needed, and can be removed in order to reduce the size of the DNN.\nBy using sound verification techniques, we can formally guarantee that our\nsimplified network is equivalent to the original, either completely, or up to a\nprescribed tolerance. Further, we show how to combine our technique with\nslicing, which results in a family of very small DNNs, which are together\nequivalent to the original. Our approach can produce DNNs that are\nsignificantly smaller than the original, rendering them suitable for deployment\non additional kinds of systems, and even more amenable to subsequent formal\nverification. We provide a proof-of-concept implementation of our approach, and\nuse it to evaluate our techniques on several real-world DNNs.",
          "link": "http://arxiv.org/abs/2105.13649",
          "publishedOn": "2021-08-13T01:56:57.240Z",
          "wordCount": 686,
          "title": "Pruning and Slicing Neural Networks using Formal Verification. (arXiv:2105.13649v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malekzadeh_M/0/1/0/all/0/1\">Mohammad Malekzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clegg_R/0/1/0/all/0/1\">Richard G. Clegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>",
          "description": "Motion sensors embedded in wearable and mobile devices allow for dynamic\nselection of sensor streams and sampling rates, enabling several applications,\nsuch as power management and data-sharing control. While deep neural networks\n(DNNs) achieve competitive accuracy in sensor data classification, DNNs\ngenerally process incoming data from a fixed set of sensors with a fixed\nsampling rate, and changes in the dimensions of their inputs cause considerable\naccuracy loss, unnecessary computations, or failure in operation. We introduce\na dimension-adaptive pooling (DAP) layer that makes DNNs flexible and more\nrobust to changes in sensor availability and in sampling rate. DAP operates on\nconvolutional filter maps of variable dimensions and produces an input of fixed\ndimensions suitable for feedforward and recurrent layers. We also propose a\ndimension-adaptive training (DAT) procedure for enabling DNNs that use DAP to\nbetter generalize over the set of feasible data dimensions at inference time.\nDAT comprises the random selection of dimensions during the forward passes and\noptimization with accumulated gradients of several backward passes. Combining\nDAP and DAT, we show how to transform non-adaptive DNNs into a\nDimension-Adaptive Neural Architecture (DANA), while keeping the same number of\nparameters. Compared to existing approaches, our solution provides better\nclassification accuracy over the range of possible data dimensions at inference\ntime and does not require up-sampling or imputation, thus reducing unnecessary\ncomputations. Experiments on seven datasets (four benchmark real-world datasets\nfor human activity recognition and three synthetic datasets) show that DANA\nprevents significant losses in classification accuracy of the state-of-the-art\nDNNs and, compared to baselines, it better captures correlated patterns in\nsensor data under dynamic sensor availability and varying sampling rates.",
          "link": "http://arxiv.org/abs/2008.02397",
          "publishedOn": "2021-08-13T01:56:57.234Z",
          "wordCount": 776,
          "title": "DANA: Dimension-Adaptive Neural Architecture for Multivariate Sensor Data. (arXiv:2008.02397v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1901.03450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>",
          "description": "With the proliferation of Internet-of-Things devices, acoustic sensing\nattracts much attention in recent years. It exploits acoustic transceivers such\nas microphones and speakers beyond their primary functions, namely recording\nand playing, to enable novel applications and new user experiences. In this\npaper, we present the first systematic survey of recent advances in active\nacoustic sensing using commodity hardware with a frequency range below\n24~\\!kHz. We propose a general framework that categorizes main building blocks\nof acoustic sensing systems. This framework encompasses three layers, i.e.,\nphysical layer, core technique layer, and application layer. The physical layer\nincludes basic hardware components, acoustic platforms as well as the air-borne\nand structure-borne channel characteristics. The core technique layer\nencompasses key mechanisms to generate acoustic signals (waveforms) and to\nextract useful temporal, spatial and spectral information from received\nsignals. The application layer builds upon the functions offered by the core\ntechniques to realize different acoustic sensing applications. We highlight\nunique challenges due to the limitations of physical devices and acoustic\nchannels and how they are mitigated or overcame by core processing techniques\nand application-specific solutions. Finally, research opportunities and future\ndirections are discussed to spawn further in-depth investigation on acoustic\nsensing.",
          "link": "http://arxiv.org/abs/1901.03450",
          "publishedOn": "2021-08-13T01:56:57.218Z",
          "wordCount": 667,
          "title": "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey. (arXiv:1901.03450v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lundell_J/0/1/0/all/0/1\">Jens Lundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verdoja_F/0/1/0/all/0/1\">Francesco Verdoja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>",
          "description": "Recent advances in multi-fingered robotic grasping have enabled fast\n6-Degrees-Of-Freedom (DOF) single object grasping. Multi-finger grasping in\ncluttered scenes, on the other hand, remains mostly unexplored due to the added\ndifficulty of reasoning over obstacles which greatly increases the\ncomputational time to generate high-quality collision-free grasps. In this work\nwe address such limitations by introducing DDGC, a fast generative multi-finger\ngrasp sampling method that can generate high quality grasps in cluttered scenes\nfrom a single RGB-D image. DDGC is built as a network that encodes scene\ninformation to produce coarse-to-fine collision-free grasp poses and\nconfigurations. We experimentally benchmark DDGC against the\nsimulated-annealing planner in GraspIt! on 1200 simulated cluttered scenes and\n7 real world scenes. The results show that DDGC outperforms the baseline on\nsynthesizing high-quality grasps and removing clutter while being 5 times\nfaster. This, in turn, opens up the door for using multi-finger grasps in\npractical applications which has so far been limited due to the excessive\ncomputation time needed by other methods.",
          "link": "http://arxiv.org/abs/2103.04783",
          "publishedOn": "2021-08-13T01:56:57.212Z",
          "wordCount": 641,
          "title": "DDGC: Generative Deep Dexterous Grasping in Clutter. (arXiv:2103.04783v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_K/0/1/0/all/0/1\">Kai Fong Ernest Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1\">Tony Q. S. Quek</a>",
          "description": "Federated learning (FL) offers a solution to train a global machine learning\nmodel while still maintaining data privacy, without needing access to data\nstored locally at the clients. However, FL suffers performance degradation when\nclient data distribution is non-IID, and a longer training duration to combat\nthis degradation may not necessarily be feasible due to communication\nlimitations. To address this challenge, we propose a new adaptive training\nalgorithm $\\texttt{AdaFL}$, which comprises two components: (i) an\nattention-based client selection mechanism for a fairer training scheme among\nthe clients; and (ii) a dynamic fraction method to balance the trade-off\nbetween performance stability and communication efficiency. Experimental\nresults show that our $\\texttt{AdaFL}$ algorithm outperforms the usual\n$\\texttt{FedAvg}$ algorithm, and can be incorporated to further improve various\nstate-of-the-art FL algorithms, with respect to three aspects: model accuracy,\nperformance stability, and communication efficiency.",
          "link": "http://arxiv.org/abs/2108.05765",
          "publishedOn": "2021-08-13T01:56:57.197Z",
          "wordCount": 611,
          "title": "Dynamic Attention-based Communication-Efficient Federated Learning. (arXiv:2108.05765v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.04131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1\">Haotian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoli Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>",
          "description": "Multi-agent reinforcement learning (MARL), despite its popularity and\nempirical success, suffers from the curse of dimensionality. This paper builds\nthe mathematical framework to approximate cooperative MARL by a mean-field\ncontrol (MFC) approach, and shows that the approximation error is of\n$\\mathcal{O}(\\frac{1}{\\sqrt{N}})$. By establishing an appropriate form of the\ndynamic programming principle for both the value function and the Q function,\nit proposes a model-free kernel-based Q-learning algorithm (MFC-K-Q), which is\nshown to have a linear convergence rate for the MFC problem, the first of its\nkind in the MARL literature. It further establishes that the convergence rate\nand the sample complexity of MFC-K-Q are independent of the number of agents\n$N$, which provides an $\\mathcal{O}(\\frac{1}{\\sqrt{N}})$ approximation to the\nMARL problem with $N$ agents in the learning environment. Empirical studies for\nthe network traffic congestion problem demonstrate that MFC-K-Q outperforms\nexisting MARL algorithms when $N$ is large, for instance when $N>50$.",
          "link": "http://arxiv.org/abs/2002.04131",
          "publishedOn": "2021-08-13T01:56:57.190Z",
          "wordCount": 641,
          "title": "Mean-Field Controls with Q-learning for Cooperative MARL: Convergence and Complexity Analysis. (arXiv:2002.04131v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anctil_Robitaille_B/0/1/0/all/0/1\">Benoit Anctil-Robitaille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theberge_A/0/1/0/all/0/1\">Antoine Th&#xe9;berge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Descoteaux_M/0/1/0/all/0/1\">Maxime Descoteaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1\">Herv&#xe9; Lombaert</a>",
          "description": "The physical and clinical constraints surrounding diffusion-weighted imaging\n(DWI) often limit the spatial resolution of the produced images to voxels up to\n8 times larger than those of T1w images. Thus, the detailed information\ncontained in T1w imagescould help in the synthesis of diffusion images in\nhigher resolution. However, the non-Euclidean nature of diffusion imaging\nhinders current deep generative models from synthesizing physically plausible\nimages. In this work, we propose the first Riemannian network architecture for\nthe direct generation of diffusion tensors (DT) and diffusion orientation\ndistribution functions (dODFs) from high-resolution T1w images. Our integration\nof the Log-Euclidean Metric into a learning objective guarantees, unlike\nstandard Euclidean networks, the mathematically-valid synthesis of diffusion.\nFurthermore, our approach improves the fractional anisotropy mean squared error\n(FA MSE) between the synthesized diffusion and the ground-truth by more than\n23% and the cosine similarity between principal directions by almost 5% when\ncompared to our baselines. We validate our generated diffusion by comparing the\nresulting tractograms to our expected real data. We observe similar fiber\nbundles with streamlines having less than 3% difference in length, less than 1%\ndifference in volume, and a visually close shape. While our method is able to\ngenerate high-resolution diffusion images from structural inputs in less than\n15 seconds, we acknowledge and discuss the limits of diffusion inference solely\nrelying on T1w images. Our results nonetheless suggest a relationship between\nthe high-level geometry of the brain and the overall white matter architecture.",
          "link": "http://arxiv.org/abs/2108.04135",
          "publishedOn": "2021-08-13T01:56:57.182Z",
          "wordCount": 707,
          "title": "Manifold-aware Synthesis of High-resolution Diffusion from Structural Imaging. (arXiv:2108.04135v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agazzi_A/0/1/0/all/0/1\">Andrea Agazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianfeng Lu</a>",
          "description": "We discuss the approximation of the value function for infinite-horizon\ndiscounted Markov Reward Processes (MRP) with nonlinear functions trained with\nthe Temporal-Difference (TD) learning algorithm. We first consider this problem\nunder a certain scaling of the approximating function, leading to a regime\ncalled lazy training. In this regime, the parameters of the model vary only\nslightly during the learning process, a feature that has recently been observed\nin the training of neural networks, where the scaling we study arises\nnaturally, implicit in the initialization of their parameters. Both in the\nunder- and over-parametrized frameworks, we prove exponential convergence to\nlocal, respectively global minimizers of the above algorithm in the lazy\ntraining regime. We then compare this scaling of the parameters to the\nmean-field regime, where the approximately linear behavior of the model is\nlost. Under this alternative scaling we prove that all fixed points of the\ndynamics in parameter space are global minimizers. We finally give examples of\nour convergence results in the case of models that diverge if trained with\nnon-lazy TD learning, and in the case of neural networks.",
          "link": "http://arxiv.org/abs/1905.10917",
          "publishedOn": "2021-08-13T01:56:57.166Z",
          "wordCount": 680,
          "title": "Temporal-difference learning with nonlinear function approximation: lazy training and mean field regimes. (arXiv:1905.10917v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.01533",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haoqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_M/0/1/0/all/0/1\">Ming Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1\">Panayiotis Georgiou</a>",
          "description": "Representation learning for speech emotion recognition is challenging due to\nlabeled data sparsity issue and lack of gold standard references. In addition,\nthere is much variability from input speech signals, human subjective\nperception of the signals and emotion label ambiguity. In this paper, we\npropose a machine learning framework to obtain speech emotion representations\nby limiting the effect of speaker variability in the speech signals.\nSpecifically, we propose to disentangle the speaker characteristics from\nemotion through an adversarial training network in order to better represent\nemotion. Our method combines the gradient reversal technique with an entropy\nloss function to remove such speaker information. Our approach is evaluated on\nboth IEMOCAP and CMU-MOSEI datasets. We show that our method improves speech\nemotion classification and increases generalization to unseen speakers.",
          "link": "http://arxiv.org/abs/1911.01533",
          "publishedOn": "2021-08-13T01:56:57.154Z",
          "wordCount": 606,
          "title": "Speaker-invariant Affective Representation Learning via Adversarial Training. (arXiv:1911.01533v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.00472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuchnik_M/0/1/0/all/0/1\">Michael Kuchnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amvrosiadis_G/0/1/0/all/0/1\">George Amvrosiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>",
          "description": "Deep learning accelerators efficiently train over vast and growing amounts of\ndata, placing a newfound burden on commodity networks and storage devices. A\ncommon approach to conserve bandwidth involves resizing or compressing data\nprior to training. We introduce Progressive Compressed Records (PCRs), a data\nformat that uses compression to reduce the overhead of fetching and\ntransporting data, effectively reducing the training time required to achieve a\ntarget accuracy. PCRs deviate from previous storage formats by combining\nprogressive compression with an efficient storage layout to view a single\ndataset at multiple fidelities---all without adding to the total dataset size.\nWe implement PCRs and evaluate them on a range of datasets, training tasks, and\nhardware architectures. Our work shows that: (i) the amount of compression a\ndataset can tolerate exceeds 50% of the original encoding for many DL training\ntasks; (ii) it is possible to automatically and efficiently select appropriate\ncompression levels for a given task; and (iii) PCRs enable tasks to readily\naccess compressed data at runtime---utilizing as little as half the training\nbandwidth and thus potentially doubling training speed.",
          "link": "http://arxiv.org/abs/1911.00472",
          "publishedOn": "2021-08-13T01:56:57.148Z",
          "wordCount": 663,
          "title": "Progressive Compressed Records: Taking a Byte out of Deep Learning Data. (arXiv:1911.00472v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05761",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Loon_W/0/1/0/all/0/1\">Wouter van Loon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vos_F/0/1/0/all/0/1\">Frank de Vos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fokkema_M/0/1/0/all/0/1\">Marjolein Fokkema</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Szabo_B/0/1/0/all/0/1\">Botond Szabo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koini_M/0/1/0/all/0/1\">Marisa Koini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_R/0/1/0/all/0/1\">Reinhold Schmidt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rooij_M/0/1/0/all/0/1\">Mark de Rooij</a>",
          "description": "Multi-view data refers to a setting where features are divided into feature\nsets, for example because they correspond to different sources. Stacked\npenalized logistic regression (StaPLR) is a recently introduced method that can\nbe used for classification and automatically selecting the views that are most\nimportant for prediction. We show how this method can easily be extended to a\nsetting where the data has a hierarchical multi-view structure. We apply StaPLR\nto Alzheimer's disease classification where different MRI measures have been\ncalculated from three scan types: structural MRI, diffusion-weighted MRI, and\nresting-state fMRI. StaPLR can identify which scan types and which MRI measures\nare most important for classification, and it outperforms elastic net\nregression in classification performance.",
          "link": "http://arxiv.org/abs/2108.05761",
          "publishedOn": "2021-08-13T01:56:57.142Z",
          "wordCount": 581,
          "title": "Analyzing hierarchical multi-view MRI data with StaPLR: An application to Alzheimer's disease classification. (arXiv:2108.05761v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venuto_D/0/1/0/all/0/1\">David Venuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1\">Elaine Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1\">Ofir Nachum</a>",
          "description": "Reasoning about the future -- understanding how decisions in the present time\naffect outcomes in the future -- is one of the central challenges for\nreinforcement learning (RL), especially in highly-stochastic or partially\nobservable environments. While predicting the future directly is hard, in this\nwork we introduce a method that allows an agent to \"look into the future\"\nwithout explicitly predicting it. Namely, we propose to allow an agent, during\nits training on past experience, to observe what \\emph{actually} happened in\nthe future at that time, while enforcing an information bottleneck to avoid the\nagent overly relying on this privileged information. This gives our agent the\nopportunity to utilize rich and useful information about the future trajectory\ndynamics in addition to the present. Our method, Policy Gradients Incorporating\nthe Future (PGIF), is easy to implement and versatile, being applicable to\nvirtually any policy gradient algorithm. We apply our proposed method to a\nnumber of off-the-shelf RL algorithms and show that PGIF is able to achieve\nhigher reward faster in a variety of online and offline RL domains, as well as\nsparse-reward and partially observable environments.",
          "link": "http://arxiv.org/abs/2108.02096",
          "publishedOn": "2021-08-13T01:56:57.116Z",
          "wordCount": 625,
          "title": "Policy Gradients Incorporating the Future. (arXiv:2108.02096v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_N/0/1/0/all/0/1\">Natasha Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamoto_Y/0/1/0/all/0/1\">Yusuke Kawamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1\">Takao Murakami</a>",
          "description": "Extended differential privacy, a generalization of standard differential\nprivacy (DP) using a general metric, has been widely studied to provide\nrigorous privacy guarantees while keeping high utility. However, existing works\non extended DP are limited to few metrics, such as the Euclidean metric.\nConsequently, they have only a small number of applications, such as\nlocation-based services and document processing. In this paper, we propose a\ncouple of mechanisms providing extended DP with a different metric: angular\ndistance (or cosine distance). Our mechanisms are based on locality sensitive\nhashing (LSH), which can be applied to the angular distance and work well for\npersonal data in a high-dimensional space. We theoretically analyze the privacy\nproperties of our mechanisms, and prove extended DP for input data by taking\ninto account that LSH preserves the original metric only approximately. We\napply our mechanisms to friend matching based on high-dimensional personal data\nwith angular distance in the local model, and evaluate our mechanisms using two\nreal datasets. We show that LDP requires a very large privacy budget and that\nRAPPOR does not work in this application. Then we show that our mechanisms\nenable friend matching with high utility and rigorous privacy guarantees based\non extended DP.",
          "link": "http://arxiv.org/abs/2010.09393",
          "publishedOn": "2021-08-13T01:56:57.098Z",
          "wordCount": 713,
          "title": "Locality Sensitive Hashing with Extended Differential Privacy. (arXiv:2010.09393v5 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bangunharcana_A/0/1/0/all/0/1\">Antyanta Bangunharcana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seokju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soohyun Kim</a>",
          "description": "Volumetric deep learning approach towards stereo matching aggregates a cost\nvolume computed from input left and right images using 3D convolutions. Recent\nworks showed that utilization of extracted image features and a spatially\nvarying cost volume aggregation complements 3D convolutions. However, existing\nmethods with spatially varying operations are complex, cost considerable\ncomputation time, and cause memory consumption to increase. In this work, we\nconstruct Guided Cost volume Excitation (GCE) and show that simple channel\nexcitation of cost volume guided by image can improve performance considerably.\nMoreover, we propose a novel method of using top-k selection prior to\nsoft-argmin disparity regression for computing the final disparity estimate.\nCombining our novel contributions, we present an end-to-end network that we\ncall Correlate-and-Excite (CoEx). Extensive experiments of our model on the\nSceneFlow, KITTI 2012, and KITTI 2015 datasets demonstrate the effectiveness\nand efficiency of our model and show that our model outperforms other\nspeed-based algorithms while also being competitive to other state-of-the-art\nalgorithms. Codes will be made available at https://github.com/antabangun/coex.",
          "link": "http://arxiv.org/abs/2108.05773",
          "publishedOn": "2021-08-13T01:56:57.092Z",
          "wordCount": 633,
          "title": "Correlate-and-Excite: Real-Time Stereo Matching via Guided Cost Volume Excitation. (arXiv:2108.05773v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishida_S/0/1/0/all/0/1\">Shu Ishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jo&#xe3;o F. Henriques</a>",
          "description": "We train embodied neural networks to plan and navigate unseen complex 3D\nenvironments, emphasising real-world deployment. Rather than requiring prior\nknowledge of the agent or environment, the planner learns to model the state\ntransitions and rewards. To avoid the potentially hazardous trial-and-error of\nreinforcement learning, we focus on differentiable planners such as Value\nIteration Networks (VIN), which are trained offline from safe expert\ndemonstrations. Although they work well in small simulations, we address two\nmajor limitations that hinder their deployment. First, we observed that current\ndifferentiable planners struggle to plan long-term in environments with a high\nbranching complexity. While they should ideally learn to assign low rewards to\nobstacles to avoid collisions, we posit that the constraints imposed on the\nnetwork are not strong enough to guarantee the network to learn sufficiently\nlarge penalties for every possible collision. We thus impose a structural\nconstraint on the value iteration, which explicitly learns to model any\nimpossible actions. Secondly, we extend the model to work with a limited\nperspective camera under translation and rotation, which is crucial for real\nrobot deployment. Many VIN-like planners assume a 360 degrees or overhead view\nwithout rotation. In contrast, our method uses a memory-efficient lattice map\nto aggregate CNN embeddings of partial observations, and models the rotational\ndynamics explicitly using a 3D state-space grid (translation and rotation). Our\nproposals significantly improve semantic navigation and exploration on several\n2D and 3D environments, succeeding in settings that are otherwise challenging\nfor this class of methods. As far as we know, we are the first to successfully\nperform differentiable planning on the difficult Active Vision Dataset,\nconsisting of real images captured from a robot.",
          "link": "http://arxiv.org/abs/2108.05713",
          "publishedOn": "2021-08-13T01:56:57.086Z",
          "wordCount": 713,
          "title": "Towards real-world navigation with deep differentiable planners. (arXiv:2108.05713v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nassir_M/0/1/0/all/0/1\">Mohammad Nassir</a>",
          "description": "In the fields of statistics and unsupervised machine learning a fundamental\nand well-studied problem is anomaly detection. Although anomalies are difficult\nto define, many algorithms have been proposed. Underlying the approaches is the\nnebulous understanding that anomalies are rare, unusual or inconsistent with\nthe majority of data. The present work gives a philosophical approach to\nclearly define anomalies and to develop an algorithm for their efficient\ndetection with minimal user intervention. Inspired by the Gestalt School of\nPsychology and the Helmholtz principle of human perception, the idea is to\nassume anomalies are observations that are unexpected to occur with respect to\ncertain groupings made by the majority of the data. Thus, under appropriate\nrandom variable modelling anomalies are directly found in a set of data under a\nuniform and independent random assumption of the distribution of constituent\nelements of the observations; anomalies correspond to those observations where\nthe expectation of occurrence of the elements in a given view is $<1$. Starting\nfrom fundamental principles of human perception an unsupervised anomaly\ndetection algorithm is developed that is simple, real-time and parameter-free.\nExperiments suggest it as the prime choice for univariate data and it shows\npromising performance on the detection of global anomalies in multivariate\ndata.",
          "link": "http://arxiv.org/abs/2103.12323",
          "publishedOn": "2021-08-13T01:56:57.001Z",
          "wordCount": 662,
          "title": "Anomaly detection using principles of human perception. (arXiv:2103.12323v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salvador_T/0/1/0/all/0/1\">Tiago Salvador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cairns_S/0/1/0/all/0/1\">Stephanie Cairns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_N/0/1/0/all/0/1\">Noah Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1\">Adam Oberman</a>",
          "description": "Face recognition models suffer from bias: for example, the probability of a\nfalse positive (incorrect face match) strongly depends on sensitive attributes\nlike ethnicity. As a result, these models may disproportionately and negatively\nimpact minority groups when used in law enforcement. In this work, we introduce\nthe Bias Mitigation Calibration (BMC) method, which (i) increases model\naccuracy (improving the state-of-the-art), (ii) produces fairly-calibrated\nprobabilities, (iii) significantly reduces the gap in the false positive rates,\nand (iv) does not require knowledge of the sensitive attribute.",
          "link": "http://arxiv.org/abs/2106.03761",
          "publishedOn": "2021-08-13T01:56:56.962Z",
          "wordCount": 562,
          "title": "Bias Mitigation of Face Recognition Models Through Calibration. (arXiv:2106.03761v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yunkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yuyang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guozheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peiyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_W/0/1/0/all/0/1\">Wenjing Shan</a>",
          "description": "Useful information (UI) is an elusive concept in neural networks. A\nquantitative measurement of UI is absent, despite the variations of UI can be\nrecognized by prior knowledge. The communication bandwidth of feature maps\ndecreases after downscaling operations, but UI flows smoothly after training\ndue to lower Nyquist frequency. Inspired by the low-Nyqusit-frequency nature of\nUI, we propose the use of spectral roll-off points (SROPs) to estimate UI on\nvariations. The computation of an SROP is extended from a 1-D signal to a 2-D\nimage by the required rotation invariance in image classification tasks. SROP\nstatistics across feature maps are implemented as layer-wise useful information\nestimates. We design sanity checks to explore SROP variations when UI\nvariations are produced by variations in model input, model architecture and\ntraining stages. The variations of SROP is synchronizes with UI variations in\nvarious randomized and sufficiently trained model structures. Therefore, SROP\nvariations is an accurate and convenient sign of UI variations, which promotes\nthe explainability of data representations with respect to frequency-domain\nknowledge.",
          "link": "http://arxiv.org/abs/2102.00369",
          "publishedOn": "2021-08-13T01:56:56.933Z",
          "wordCount": 683,
          "title": "Spectral Roll-off Points Variations: Exploring Useful Information in Feature Maps by Its Variations. (arXiv:2102.00369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12351",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Nozari_E/0/1/0/all/0/1\">Erfan Nozari</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bertolero_M/0/1/0/all/0/1\">Maxwell A. Bertolero</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stiso_J/0/1/0/all/0/1\">Jennifer Stiso</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Caciagli_L/0/1/0/all/0/1\">Lorenzo Caciagli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cornblath_E/0/1/0/all/0/1\">Eli J. Cornblath</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+He_X/0/1/0/all/0/1\">Xiaosong He</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mahadevan_A/0/1/0/all/0/1\">Arun S. Mahadevan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bassett_D/0/1/0/all/0/1\">Dani Smith Bassett</a>",
          "description": "A central challenge in the computational modeling of neural dynamics is the\ntrade-off between accuracy and simplicity. At the level of individual neurons,\nnonlinear dynamics are both experimentally established and essential for\nneuronal functioning. An implicit assumption has thus formed that an accurate\ncomputational model of whole-brain dynamics must also be highly nonlinear,\nwhereas linear models may provide a first-order approximation. Here, we provide\na rigorous and data-driven investigation of this hypothesis at the level of\nwhole-brain blood-oxygen-level-dependent (BOLD) and macroscopic field potential\ndynamics by leveraging the theory of system identification. Using functional\nMRI (fMRI) and intracranial EEG (iEEG), we model the resting state activity of\n700 subjects in the Human Connectome Project (HCP) and 122 subjects from the\nRestoring Active Memory (RAM) project using state-of-the-art linear and\nnonlinear model families. We assess relative model fit using predictive power,\ncomputational complexity, and the extent of residual dynamics unexplained by\nthe model. Contrary to our expectations, linear auto-regressive models achieve\nthe best measures across all three metrics, eliminating the trade-off between\naccuracy and simplicity. To understand and explain this linearity, we highlight\nfour properties of macroscopic neurodynamics which can counteract or mask\nmicroscopic nonlinear dynamics: averaging over space, averaging over time,\nobservation noise, and limited data samples. Whereas the latter two are\ntechnological limitations and can improve in the future, the former two are\ninherent to aggregated macroscopic brain activity. Our results, together with\nthe unparalleled interpretability of linear models, can greatly facilitate our\nunderstanding of macroscopic neural dynamics and the principled design of\nmodel-based interventions for the treatment of neuropsychiatric disorders.",
          "link": "http://arxiv.org/abs/2012.12351",
          "publishedOn": "2021-08-13T01:56:56.780Z",
          "wordCount": 752,
          "title": "Is the brain macroscopically linear? A system identification of resting state dynamics. (arXiv:2012.12351v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and show excellent\nresults, in particular for rare answers. Furthermore, we demonstrate our method\nto significantly outperform the state of the art on MSRVTT-QA, MSVD-QA,\nActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce\niVQA, a new VideoQA dataset with reduced language biases and high-quality\nredundant manual annotations. Our code, datasets and trained models are\navailable at https://antoyang.github.io/just-ask.html.",
          "link": "http://arxiv.org/abs/2012.00451",
          "publishedOn": "2021-08-13T01:56:56.773Z",
          "wordCount": 684,
          "title": "Just Ask: Learning to Answer Questions from Millions of Narrated Videos. (arXiv:2012.00451v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Nhan Khanh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quang Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangzhou Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Quanwei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1\">Sandra Hirche</a>",
          "description": "Federated learning is the distributed machine learning framework that enables\ncollaborative training across multiple parties while ensuring data privacy.\nPractical adaptation of XGBoost, the state-of-the-art tree boosting framework,\nto federated learning remains limited due to high cost incurred by conventional\nprivacy-preserving methods. To address the problem, we propose two variants of\nfederated XGBoost with privacy guarantee: FedXGBoost-SMM and FedXGBoost-LDP.\nOur first protocol FedXGBoost-SMM deploys enhanced secure matrix multiplication\nmethod to preserve privacy with lossless accuracy and lower overhead than\nencryption-based techniques. Developed independently, the second protocol\nFedXGBoost-LDP is heuristically designed with noise perturbation for local\ndifferential privacy, and empirically evaluated on real-world and synthetic\ndatasets.",
          "link": "http://arxiv.org/abs/2106.10662",
          "publishedOn": "2021-08-13T01:56:56.767Z",
          "wordCount": 581,
          "title": "FedXGBoost: Privacy-Preserving XGBoost for Federated Learning. (arXiv:2106.10662v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1\">Verna Dankers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1\">Elia Bruni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>",
          "description": "Moving towards human-like linguistic performance is often argued to require\ncompositional generalisation. Whether neural networks exhibit this ability is\ntypically studied using artificial languages, for which the compositionality of\ninput fragments can be guaranteed and their meanings algebraically composed.\nHowever, compositionality in natural language is vastly more complex than this\nrigid, arithmetics-like version of compositionality, and as such artificial\ncompositionality tests do not allow us to draw conclusions about how neural\nmodels deal with compositionality in more realistic scenarios. In this work, we\nre-instantiate three compositionality tests from the literature and reformulate\nthem for neural machine translation (NMT). The results highlight two main\nissues: the inconsistent behaviour of NMT models and their inability to\n(correctly) modulate between local and global processing. Aside from an\nempirical study, our work is a call to action: we should rethink the evaluation\nof compositionality in neural networks of natural language, where composing\nmeaning is not as straightforward as doing the math.",
          "link": "http://arxiv.org/abs/2108.05885",
          "publishedOn": "2021-08-13T01:56:56.760Z",
          "wordCount": 606,
          "title": "The paradox of the compositionality of natural language: a neural machine translation case study. (arXiv:2108.05885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Marco Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xianjun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuanjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuzhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>",
          "description": "We propose a novel neural model compression strategy combining data\naugmentation, knowledge transfer, pruning, and quantization for device-robust\nacoustic scene classification (ASC). Specifically, we tackle the ASC task in a\nlow-resource environment leveraging a recently proposed advanced neural network\npruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a\nsub-network neural model associated with a small amount non-zero model\nparameters. The effectiveness of LTH for low-complexity acoustic modeling is\nassessed by investigating various data augmentation and compression schemes,\nand we report an efficient joint framework for low-complexity multi-device ASC,\ncalled Acoustic Lottery. Acoustic Lottery could compress an ASC model over\n$1/10^{4}$ and attain a superior performance (validation accuracy of 74.01% and\nLog loss of 0.76) compared to its not compressed seed model. All results\nreported in this work are based on a joint effort of four groups, namely\nGT-USTC-UKE-Tencent, aiming to address the \"Low-Complexity Acoustic Scene\nClassification (ASC) with Multiple Devices\" in the DCASE 2021 Challenge Task\n1a.",
          "link": "http://arxiv.org/abs/2107.01461",
          "publishedOn": "2021-08-13T01:56:56.738Z",
          "wordCount": 682,
          "title": "A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification. (arXiv:2107.01461v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_H/0/1/0/all/0/1\">Hyowoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jihong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1\">M&#xe9;rouane Debbah</a>",
          "description": "Spurred by a huge interest in the post-Shannon communication, it has recently\nbeen shown that leveraging semantics can significantly improve the\ncommunication effectiveness across many tasks. In this article, inspired by\nhuman communication, we propose a novel stochastic model of System 1\nsemantics-native communication (SNC) for generic tasks, where a speaker has an\nintention of referring to an entity, extracts the semantics, and communicates\nits symbolic representation to a target listener. To further reach its full\npotential, we additionally infuse contextual reasoning into SNC such that the\nspeaker locally and iteratively self-communicates with a virtual agent built on\nthe physical listener's unique way of coding its semantics, i.e., communication\ncontext. The resultant System 2 SNC allows the speaker to extract the most\neffective semantics for its listener. Leveraging the proposed stochastic model,\nwe show that the reliability of System 2 SNC increases with the number of\nmeaningful concepts, and derive the expected semantic representation (SR) bit\nlength which quantifies the extracted effective semantics. It is also shown\nthat System 2 SNC significantly reduces the SR length without compromising\ncommunication reliability.",
          "link": "http://arxiv.org/abs/2108.05681",
          "publishedOn": "2021-08-13T01:56:56.731Z",
          "wordCount": 621,
          "title": "Semantics-Native Communication with Contextual Reasoning. (arXiv:2108.05681v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "We present Mobile-Former, a parallel design of MobileNet and Transformer with\na two-way bridge in between. This structure leverages the advantage of\nMobileNet at local processing and transformer at global interaction. And the\nbridge enables bidirectional fusion of local and global features. Different\nwith recent works on vision transformer, the transformer in Mobile-Former\ncontains very few tokens (e.g. less than 6 tokens) that are randomly\ninitialized, resulting in low computational cost. Combining with the proposed\nlight-weight cross attention to model the bridge, Mobile-Former is not only\ncomputationally efficient, but also has more representation power,\noutperforming MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet\nclassification. For instance, it achieves 77.9\\% top-1 accuracy at 294M FLOPs,\ngaining 1.3\\% over MobileNetV3 but saving 17\\% of computations. When\ntransferring to object detection, Mobile-Former outperforms MobileNetV3 by 8.6\nAP.",
          "link": "http://arxiv.org/abs/2108.05895",
          "publishedOn": "2021-08-13T01:56:56.726Z",
          "wordCount": 578,
          "title": "Mobile-Former: Bridging MobileNet and Transformer. (arXiv:2108.05895v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gilmour_E/0/1/0/all/0/1\">Elizabeth Gilmour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plotkin_N/0/1/0/all/0/1\">Noah Plotkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Leslie Smith</a>",
          "description": "Reinforcement learning (RL) is successful at learning to play games where the\nentire environment is visible. However, RL approaches are challenged in complex\ngames like Starcraft II and in real-world environments where the entire\nenvironment is not visible. In these more complex games with more limited\nvisual information, agents must choose where to look and how to optimally use\ntheir limited visual information in order to succeed at the game. We verify\nthat with a relatively simple model the agent can learn where to look in\nscenarios with a limited visual bandwidth. We develop a method for masking part\nof the environment in Atari games to force the RL agent to learn both where to\nlook and how to play the game in order to study where the RL agent learns to\nlook. In addition, we develop a neural network architecture and method for\nallowing the agent to choose where to look and what action to take in the Pong\ngame. Further, we analyze the strategies the agent learns to better understand\nhow the RL agent learns to play the game.",
          "link": "http://arxiv.org/abs/2108.05701",
          "publishedOn": "2021-08-13T01:56:56.715Z",
          "wordCount": 653,
          "title": "An Approach to Partial Observability in Games: Learning to Both Act and Observe. (arXiv:2108.05701v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarov_J/0/1/0/all/0/1\">Jafar Jafarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalhan_S/0/1/0/all/0/1\">Sanchit Kalhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_K/0/1/0/all/0/1\">Konstantin Makarychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_Y/0/1/0/all/0/1\">Yury Makarychev</a>",
          "description": "In the Correlation Clustering problem, we are given a complete weighted graph\n$G$ with its edges labeled as \"similar\" and \"dissimilar\" by a noisy binary\nclassifier. For a clustering $\\mathcal{C}$ of graph $G$, a similar edge is in\ndisagreement with $\\mathcal{C}$, if its endpoints belong to distinct clusters;\nand a dissimilar edge is in disagreement with $\\mathcal{C}$ if its endpoints\nbelong to the same cluster. The disagreements vector, $\\text{dis}$, is a vector\nindexed by the vertices of $G$ such that the $v$-th coordinate $\\text{dis}_v$\nequals the weight of all disagreeing edges incident on $v$. The goal is to\nproduce a clustering that minimizes the $\\ell_p$ norm of the disagreements\nvector for $p\\geq 1$. We study the $\\ell_p$ objective in Correlation Clustering\nunder the following assumption: Every similar edge has weight in the range of\n$[\\alpha\\mathbf{w},\\mathbf{w}]$ and every dissimilar edge has weight at least\n$\\alpha\\mathbf{w}$ (where $\\alpha \\leq 1$ and $\\mathbf{w}>0$ is a scaling\nparameter). We give an\n$O\\left((\\frac{1}{\\alpha})^{\\frac{1}{2}-\\frac{1}{2p}}\\cdot\n\\log\\frac{1}{\\alpha}\\right)$ approximation algorithm for this problem.\nFurthermore, we show an almost matching convex programming integrality gap.",
          "link": "http://arxiv.org/abs/2108.05697",
          "publishedOn": "2021-08-13T01:56:56.700Z",
          "wordCount": 628,
          "title": "Local Correlation Clustering with Asymmetric Classification Errors. (arXiv:2108.05697v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">HeeSeung Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hoyong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jong-Hun Shin</a>",
          "description": "The flexibility of decision boundaries in neural networks that are unguided\nby training data is a well-known problem typically resolved with generalization\nmethods. A surprising result from recent knowledge distillation (KD) literature\nis that random, untrained, and equally structured teacher networks can also\nvastly improve generalization performance. It raises the possibility of\nexistence of undiscovered assumptions useful for generalization on an uncertain\nregion. In this paper, we shed light on the assumptions by analyzing decision\nboundaries and confidence distributions of both simple and KD-based\ngeneralization methods. Assuming that a decision boundary exists to represent\nthe most general tendency of distinction on an input sample space (i.e., the\nsimplest hypothesis), we show the various limitations of methods when using the\nhypothesis. To resolve these limitations, we propose matured dumb teacher based\nKD, conservatively transferring the hypothesis for generalization of the\nstudent without massive destruction of trained information. In practical\nexperiments on feed-forward and convolution neural networks for image\nclassification tasks on MNIST, CIFAR-10, and CIFAR-100 datasets, the proposed\nmethod shows stable improvement to the best test performance in the grid search\nof hyperparameters. The analysis and results imply that the proposed method can\nprovide finer generalization than existing methods.",
          "link": "http://arxiv.org/abs/2108.05776",
          "publishedOn": "2021-08-13T01:56:56.694Z",
          "wordCount": 654,
          "title": "Learning from Matured Dumb Teacher for Fine Generalization. (arXiv:2108.05776v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1\">Chieh Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_K/0/1/0/all/0/1\">Krutika Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Changchen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kathy Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platz_J/0/1/0/all/0/1\">Justin Platz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilardi_A/0/1/0/all/0/1\">Adam Ilardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhvanath_S/0/1/0/all/0/1\">Sriganesh Madhvanath</a>",
          "description": "The item details page (IDP) is a web page on an e-commerce website that\nprovides information on a specific product or item listing. Just below the\ndetails of the item on this page, the buyer can usually find recommendations\nfor other relevant items. These are typically in the form of a series of\nmodules or carousels, with each module containing a set of recommended items.\nThe selection and ordering of these item recommendation modules are intended to\nincrease discover-ability of relevant items and encourage greater user\nengagement, while simultaneously showcasing diversity of inventory and\nsatisfying other business objectives. Item recommendation modules on the IDP\nare often curated and statically configured for all customers, ignoring\nopportunities for personalization. In this paper, we present a scalable\nend-to-end production system to optimize the personalized selection and\nordering of item recommendation modules on the IDP in real-time by utilizing\ndeep neural networks. Through extensive offline experimentation and online A/B\ntesting, we show that our proposed system achieves significantly higher\nclick-through and conversion rates compared to other existing methods. In our\nonline A/B test, our framework improved click-through rate by 2.48% and\npurchase-through rate by 7.34% over a static configuration.",
          "link": "http://arxiv.org/abs/2108.05891",
          "publishedOn": "2021-08-13T01:56:56.688Z",
          "wordCount": 639,
          "title": "Page-level Optimization of e-Commerce Item Recommendations. (arXiv:2108.05891v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ackerman_S/0/1/0/all/0/1\">Samuel Ackerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raz_O/0/1/0/all/0/1\">Orna Raz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zalmanovici_M/0/1/0/all/0/1\">Marcel Zalmanovici</a>",
          "description": "Machine learning (ML) solutions are prevalent. However, many challenges exist\nin making these solutions business-grade. One major challenge is to ensure that\nthe ML solution provides its expected business value. In order to do that, one\nhas to bridge the gap between the way ML model performance is measured and the\nsolution requirements. In previous work (Barash et al, \"Bridging the gap...\")\nwe demonstrated the effectiveness of utilizing feature models in bridging this\ngap. Whereas ML performance metrics, such as the accuracy or F1-score of a\nclassifier, typically measure the average ML performance, feature models shed\nlight on explainable data slices that are too far from that average, and\ntherefore might indicate unsatisfied requirements. For example, the overall\naccuracy of a bank text terms classifier may be very high, say $98\\% \\pm 2\\%$,\nyet it might perform poorly for terms that include short descriptions and\noriginate from commercial accounts. A business requirement, which may be\nimplicit in the training data, may be to perform well regardless of the type of\naccount and length of the description. Therefore, the under-performing data\nslice that includes short descriptions and commercial accounts suggests\npoorly-met requirements. In this paper we show the feasibility of automatically\nextracting feature models that result in explainable data slices over which the\nML solution under-performs. Our novel technique, IBM FreaAI aka FreaAI,\nextracts such slices from structured ML test data or any other labeled data. We\ndemonstrate that FreaAI can automatically produce explainable and\nstatistically-significant data slices over seven open datasets.",
          "link": "http://arxiv.org/abs/2108.05620",
          "publishedOn": "2021-08-13T01:56:56.673Z",
          "wordCount": 705,
          "title": "FreaAI: Automated extraction of data slices to test machine learning models. (arXiv:2108.05620v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Siyu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1\">Xiang-Rong Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Ying Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guorui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>",
          "description": "One of the difficulties of conversion rate (CVR) prediction is that the\nconversions can delay and take place long after the clicks. The delayed\nfeedback poses a challenge: fresh data are beneficial to continuous training\nbut may not have complete label information at the time they are ingested into\nthe training pipeline. To balance model freshness and label certainty, previous\nmethods set a short waiting window or even do not wait for the conversion\nsignal. If conversion happens outside the waiting window, this sample will be\nduplicated and ingested into the training pipeline with a positive label.\nHowever, these methods have some issues. First, they assume the observed\nfeature distribution remains the same as the actual distribution. But this\nassumption does not hold due to the ingestion of duplicated samples. Second,\nthe certainty of the conversion action only comes from the positives. But the\npositives are scarce as conversions are sparse in commercial systems. These\nissues induce bias during the modeling of delayed feedback. In this paper, we\npropose DElayed FEedback modeling with Real negatives (DEFER) method to address\nthese issues. The proposed method ingests real negative samples into the\ntraining pipeline. The ingestion of real negatives ensures the observed feature\ndistribution is equivalent to the actual distribution, thus reducing the bias.\nThe ingestion of real negatives also brings more certainty information of the\nconversion. To correct the distribution shift, DEFER employs importance\nsampling to weigh the loss function. Experimental results on industrial\ndatasets validate the superiority of DEFER. DEFER have been deployed in the\ndisplay advertising system of Alibaba, obtaining over 6.0% improvement on CVR\nin several scenarios. The code and data in this paper are now open-sourced\n{https://github.com/gusuperstar/defer.git}.",
          "link": "http://arxiv.org/abs/2104.14121",
          "publishedOn": "2021-08-13T01:56:56.668Z",
          "wordCount": 755,
          "title": "Real Negatives Matter: Continuous Training with Real Negatives for Delayed Feedback Modeling. (arXiv:2104.14121v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05879",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chevyrev_I/0/1/0/all/0/1\">Ilya Chevyrev</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gerasimovics_A/0/1/0/all/0/1\">Andris Gerasimovics</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weber_H/0/1/0/all/0/1\">Hendrik Weber</a>",
          "description": "We investigate the use of models from the theory of regularity structure as\nfeatures in machine learning tasks. A model is a multi-linear function of a\nspace-time signal designed to well-approximate solutions to partial\ndifferential equations (PDEs), even in low regularity regimes. Models can be\nseen as natural multi-dimensional generalisations of signatures of paths; our\nwork therefore aims to extend the recent use of signatures in data science\nbeyond the context of time-ordered data. We provide a flexible definition of a\nmodel feature vector associated to a space-time signal, along with two\nalgorithms which illustrate ways in which these features can be combined with\nlinear regression. We apply these algorithms in several numerical experiments\ndesigned to learn solutions to PDEs with a given forcing and boundary data. Our\nexperiments include semi-linear parabolic and wave equations with forcing, and\nBurgers' equation with no forcing. We find an advantage in favour of our\nalgorithms when compared to several alternative methods. Additionally, in the\nexperiment with Burgers' equation, we noticed stability in the prediction power\nwhen noise is added to the observations.",
          "link": "http://arxiv.org/abs/2108.05879",
          "publishedOn": "2021-08-13T01:56:56.662Z",
          "wordCount": 615,
          "title": "Feature Engineering with Regularity Structures. (arXiv:2108.05879v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1\">Md. Mohsin Sarker Raihan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Md. Mohi Uddin Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akter_L/0/1/0/all/0/1\">Laboni Akter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shams_A/0/1/0/all/0/1\">Abdullah Bin Shams</a>",
          "description": "The Reverse Transcription Polymerase Chain Reaction (RTPCR) test is the\nsilver bullet diagnostic test to discern COVID infection. Rapid antigen\ndetection is a screening test to identify COVID positive patients in little as\n15 minutes, but has a lower sensitivity than the PCR tests. Besides having\nmultiple standardized test kits, many people are getting infected & either\nrecovering or dying even before the test due to the shortage and cost of kits,\nlack of indispensable specialists and labs, time-consuming result compared to\nbulk population especially in developing and underdeveloped countries.\nIntrigued by the parametric deviations in immunological & hematological profile\nof a COVID patient, this research work leveraged the concept of COVID-19\ndetection by proposing a risk-free and highly accurate Stacked Ensemble Machine\nLearning model to identify a COVID patient from communally\navailable-widespread-cheap routine blood tests which gives a promising\naccuracy, precision, recall & F1-score of 100%. Analysis from R-curve also\nshows the preciseness of the risk-free model to be implemented. The proposed\nmethod has the potential for large scale ubiquitous low-cost screening\napplication. This can add an extra layer of protection in keeping the number of\ninfected cases to a minimum and control the pandemic by identifying\nasymptomatic or pre-symptomatic people early.",
          "link": "http://arxiv.org/abs/2108.05660",
          "publishedOn": "2021-08-13T01:56:56.655Z",
          "wordCount": 734,
          "title": "Development of Risk-Free COVID-19 Screening Algorithm from Routine Blood Test using Ensemble Machine Learning. (arXiv:2108.05660v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiarui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shenggui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "The pre-trained model (PTM) is revolutionizing Artificial intelligence (AI)\ntechnology. It learns a model with general language features on the vast text\nand then fine-tunes the model using a task-specific dataset. Unfortunately, PTM\ntraining requires prohibitively expensive computing devices, especially\nfine-tuning, which is still a game for a small proportion of people in the AI\ncommunity. Enabling PTMs training on low-quality devices, PatrickStar now makes\nPTM accessible to everyone.\n\nPatrickStar reduces memory requirements of computing platforms by using the\nCPU-GPU heterogeneous memory space to store model data, consisting of\nparameters, gradients, and optimizer states. We observe that the GPU memory\navailable for model data changes regularly, in a tide-like pattern, decreasing\nand increasing iteratively. However, the existing heterogeneous training works\ndo not take advantage of this pattern. Instead, they statically partition the\nmodel data among CPU and GPU, leading to both memory waste and memory abuse. In\ncontrast, PatrickStar manages model data in chunks, which are dynamically\ndistributed in heterogeneous memory spaces. Chunks consist of stateful tensors\nwhich run as finite state machines during training. Guided by the runtime\nmemory statistics collected in a warm-up iteration, chunks are orchestrated\nefficiently in heterogeneous memory and generate lower CPU-GPU data\ntransmission volume. Symbiosis with the Zero Redundancy Optimizer, PatrickStar\nscales to multiple GPUs using data parallelism, with the lowest communication\nbandwidth requirements and more efficient bandwidth utilization. Experimental\nresults show PatrickStar trains a 12 billion parameters GPT model, 2x larger\nthan the STOA work, on an 8-V100 and 240GB CPU memory node, and is also more\nefficient on the same model size.",
          "link": "http://arxiv.org/abs/2108.05818",
          "publishedOn": "2021-08-13T01:56:56.649Z",
          "wordCount": 707,
          "title": "PatrickStar: Parallel Training of Pre-trained Models via a Chunk-based Memory Management. (arXiv:2108.05818v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Binici_K/0/1/0/all/0/1\">Kuluhan Binici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nam Trung Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1\">Tulika Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leman_K/0/1/0/all/0/1\">Karianto Leman</a>",
          "description": "With the increasing popularity of deep learning on edge devices, compressing\nlarge neural networks to meet the hardware requirements of resource-constrained\ndevices became a significant research direction. Numerous compression\nmethodologies are currently being used to reduce the memory sizes and energy\nconsumption of neural networks. Knowledge distillation (KD) is among such\nmethodologies and it functions by using data samples to transfer the knowledge\ncaptured by a large model (teacher) to a smaller one(student). However, due to\nvarious reasons, the original training data might not be accessible at the\ncompression stage. Therefore, data-free model compression is an ongoing\nresearch problem that has been addressed by various works. In this paper, we\npoint out that catastrophic forgetting is a problem that can potentially be\nobserved in existing data-free distillation methods. Moreover, the sample\ngeneration strategies in some of these methods could result in a mismatch\nbetween the synthetic and real data distributions. To prevent such problems, we\npropose a data-free KD framework that maintains a dynamic collection of\ngenerated samples over time. Additionally, we add the constraint of matching\nthe real data distribution in sample generation strategies that target maximum\ninformation gain. Our experiments demonstrate that we can improve the accuracy\nof the student models obtained via KD when compared with state-of-the-art\napproaches on the SVHN, Fashion MNIST and CIFAR100 datasets.",
          "link": "http://arxiv.org/abs/2108.05698",
          "publishedOn": "2021-08-13T01:56:56.643Z",
          "wordCount": 665,
          "title": "Preventing Catastrophic Forgetting and Distribution Mismatch in Knowledge Distillation via Synthetic Data. (arXiv:2108.05698v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1\">Anson Bastos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadgeri_A/0/1/0/all/0/1\">Abhishek Nadgeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarpour_S/0/1/0/all/0/1\">Saeedeh Shekarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulang_I/0/1/0/all/0/1\">Isaiah Onando Mulang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffart_J/0/1/0/all/0/1\">Johannes Hoffart</a>",
          "description": "Recently, several Knowledge Graph Embedding (KGE) approaches have been\ndevised to represent entities and relations in dense vector space and employed\nin downstream tasks such as link prediction. A few KGE techniques address\ninterpretability, i.e., mapping the connectivity patterns of the relations\n(i.e., symmetric/asymmetric, inverse, and composition) to a geometric\ninterpretation such as rotations. Other approaches model the representations in\nhigher dimensional space such as four-dimensional space (4D) to enhance the\nability to infer the connectivity patterns (i.e., expressiveness). However,\nmodeling relation and entity in a 4D space often comes at the cost of\ninterpretability. This paper proposes HopfE, a novel KGE approach aiming to\nachieve the interpretability of inferred relations in the four-dimensional\nspace. We first model the structural embeddings in 3D Euclidean space and view\nthe relation operator as an SO(3) rotation. Next, we map the entity embedding\nvector from a 3D space to a 4D hypersphere using the inverse Hopf Fibration, in\nwhich we embed the semantic information from the KG ontology. Thus, HopfE\nconsiders the structural and semantic properties of the entities without losing\nexpressivity and interpretability. Our empirical results on four well-known\nbenchmarks achieve state-of-the-art performance for the KG completion task.",
          "link": "http://arxiv.org/abs/2108.05774",
          "publishedOn": "2021-08-13T01:56:56.638Z",
          "wordCount": 656,
          "title": "HopfE: Knowledge Graph Representation Learning using Inverse Hopf Fibrations. (arXiv:2108.05774v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05796",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pham_C/0/1/0/all/0/1\">Cuong Pham</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1\">Tung Le</a>",
          "description": "Premier League is known as one of the most competitive football league in the\nworld, hence there are many goals are scored here every match. Which are the\nfactors that affect to the number of goal scored in each match? We use Poisson\nregression to find out the relation between many factors as shots on target,\ncorners, red cards, to the goals home team can score in their match.",
          "link": "http://arxiv.org/abs/2108.05796",
          "publishedOn": "2021-08-13T01:56:56.633Z",
          "wordCount": 500,
          "title": "Goal scoring in Premier League with Poisson regression. (arXiv:2108.05796v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Youxuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zongze Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shugong Xu</a>",
          "description": "In recent years, synthetic speech generated by advanced text-to-speech (TTS)\nand voice conversion (VC) systems has caused great harms to automatic speaker\nverification (ASV) systems, urging us to design a synthetic speech detection\nsystem to protect ASV systems. In this paper, we propose a new speech\nanti-spoofing model named ResWavegram-Resnet (RW-Resnet). The model contains\ntwo parts, Conv1D Resblocks and backbone Resnet34. The Conv1D Resblock is based\non the Conv1D block with a residual connection. For the first part, we use the\nraw waveform as input and feed it to the stacked Conv1D Resblocks to get the\nResWavegram. Compared with traditional methods, ResWavegram keeps all the\ninformation from the audio signal and has a stronger ability in extracting\nfeatures. For the second part, the extracted features are fed to the backbone\nResnet34 for the spoofed or bonafide decision. The ASVspoof2019 logical access\n(LA) corpus is used to evaluate our proposed RW-Resnet. Experimental results\nshow that the RW-Resnet achieves better performance than other state-of-the-art\nanti-spoofing models, which illustrates its effectiveness in detecting\nsynthetic speech attacks.",
          "link": "http://arxiv.org/abs/2108.05684",
          "publishedOn": "2021-08-13T01:56:56.627Z",
          "wordCount": 612,
          "title": "RW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform. (arXiv:2108.05684v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "How do the neural networks distinguish two images? It is of critical\nimportance to understand the matching mechanism of deep models for developing\nreliable intelligent systems for many risky visual applications such as\nsurveillance and access control. However, most existing deep metric learning\nmethods match the images by comparing feature vectors, which ignores the\nspatial structure of images and thus lacks interpretability. In this paper, we\npresent a deep interpretable metric learning (DIML) method for more transparent\nembedding learning. Unlike conventional metric learning methods based on\nfeature vector comparison, we propose a structural matching strategy that\nexplicitly aligns the spatial embeddings by computing an optimal matching flow\nbetween feature maps of the two images. Our method enables deep models to learn\nmetrics in a more human-friendly way, where the similarity of two images can be\ndecomposed to several part-wise similarities and their contributions to the\noverall similarity. Our method is model-agnostic, which can be applied to\noff-the-shelf backbone networks and metric learning methods. We evaluate our\nmethod on three major benchmarks of deep metric learning including CUB200-2011,\nCars196, and Stanford Online Products, and achieve substantial improvements\nover popular metric learning methods with better interpretability. Code is\navailable at https://github.com/wl-zhao/DIML",
          "link": "http://arxiv.org/abs/2108.05889",
          "publishedOn": "2021-08-13T01:56:56.604Z",
          "wordCount": 652,
          "title": "Towards Interpretable Deep Metric Learning with Structural Matching. (arXiv:2108.05889v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beal_J/0/1/0/all/0/1\">Josh Beal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao-Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dong Huk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_A/0/1/0/all/0/1\">Andrew Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kislyuk_D/0/1/0/all/0/1\">Dmitry Kislyuk</a>",
          "description": "Large-scale pretraining of visual representations has led to state-of-the-art\nperformance on a range of benchmark computer vision tasks, yet the benefits of\nthese techniques at extreme scale in complex production systems has been\nrelatively unexplored. We consider the case of a popular visual discovery\nproduct, where these representations are trained with multi-task learning, from\nuse-case specific visual understanding (e.g. skin tone classification) to\ngeneral representation learning for all visual content (e.g. embeddings for\nretrieval). In this work, we describe how we (1) generate a dataset with over a\nbillion images via large weakly-supervised pretraining to improve the\nperformance of these visual representations, and (2) leverage Transformers to\nreplace the traditional convolutional backbone, with insights into both system\nand performance improvements, especially at 1B+ image scale. To support this\nbackbone model, we detail a systematic approach to deriving weakly-supervised\nimage annotations from heterogenous text signals, demonstrating the benefits of\nclustering techniques to handle the long-tail distribution of image labels.\nThrough a comprehensive study of offline and online evaluation, we show that\nlarge-scale Transformer-based pretraining provides significant benefits to\nindustry computer vision applications. The model is deployed in a production\nvisual shopping system, with 36% improvement in top-1 relevance and 23%\nimprovement in click-through volume. We conduct extensive experiments to better\nunderstand the empirical relationships between Transformer-based architectures,\ndataset scale, and the performance of production vision systems.",
          "link": "http://arxiv.org/abs/2108.05887",
          "publishedOn": "2021-08-13T01:56:56.597Z",
          "wordCount": 679,
          "title": "Billion-Scale Pretraining with Vision Transformers for Multi-Task Visual Representations. (arXiv:2108.05887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1\">Kazuhisa Fujita</a>",
          "description": "Spectral clustering (SC) is one of the most popular clustering methods and\noften outperforms traditional clustering methods. SC uses the eigenvectors of a\nLaplacian matrix calculated from a similarity matrix of a dataset. SC has\nserious drawbacks: the significant increases in the time complexity derived\nfrom the computation of eigenvectors and the memory space complexity to store\nthe similarity matrix. To address the issues, I develop a new approximate\nspectral clustering using the network generated by growing neural gas (GNG),\ncalled ASC with GNG in this study. ASC with GNG uses not only reference vectors\nfor vector quantization but also the topology of the network for extraction of\nthe topological relationship between data points in a dataset. ASC with GNG\ncalculates the similarity matrix from both the reference vectors and the\ntopology of the network generated by GNG. Using the network generated from a\ndataset by GNG, ASC with GNG achieves to reduce the computational and space\ncomplexities and improve clustering quality. In this study, I demonstrate that\nASC with GNG effectively reduces the computational time. Moreover, this study\nshows that ASC with GNG provides equal to or better clustering performance than\nSC.",
          "link": "http://arxiv.org/abs/2009.07101",
          "publishedOn": "2021-08-13T01:56:56.587Z",
          "wordCount": 692,
          "title": "Approximate spectral clustering using both reference vectors and topology of the network generated by growing neural gas. (arXiv:2009.07101v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueh-Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaowei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hanwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "While we have made significant progress on understanding hand-object\ninteractions in computer vision, it is still very challenging for robots to\nperform complex dexterous manipulation. In this paper, we propose a new\nplatform and pipeline, DexMV (Dex Manipulation from Videos), for imitation\nlearning to bridge the gap between computer vision and robot learning. We\ndesign a platform with: (i) a simulation system for complex dexterous\nmanipulation tasks with a multi-finger robot hand and (ii) a computer vision\nsystem to record large-scale demonstrations of a human hand conducting the same\ntasks. In our new pipeline, we extract 3D hand and object poses from the\nvideos, and convert them to robot demonstrations via motion retargeting. We\nthen apply and compare multiple imitation learning algorithms with the\ndemonstrations. We show that the demonstrations can indeed improve robot\nlearning by a large margin and solve the complex tasks which reinforcement\nlearning alone cannot solve. Project page with video:\nhttps://yzqin.github.io/dexmv/",
          "link": "http://arxiv.org/abs/2108.05877",
          "publishedOn": "2021-08-13T01:56:56.565Z",
          "wordCount": 608,
          "title": "DexMV: Imitation Learning for Dexterous Manipulation from Human Videos. (arXiv:2108.05877v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aleksandrova_M/0/1/0/all/0/1\">Marharyta Aleksandrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chertov_O/0/1/0/all/0/1\">Oleg Chertov</a>",
          "description": "The property of conformal predictors to guarantee the required accuracy rate\nmakes this framework attractive in various practical applications. However,\nthis property is achieved at a price of reduction in precision. In the case of\nconformal classification, the systems can output multiple class labels instead\nof one. It is also known from the literature, that the choice of nonconformity\nfunction has a major impact on the efficiency of conformal classifiers.\nRecently, it was shown that different model-agnostic nonconformity functions\nresult in conformal classifiers with different characteristics. For a Neural\nNetwork-based conformal classifier, the inverse probability (or hinge loss)\nallows minimizing the average number of predicted labels, and margin results in\na larger fraction of singleton predictions. In this work, we aim to further\nextend this study. We perform an experimental evaluation using 8 different\nclassification algorithms and discuss when the previously observed relationship\nholds or not. Additionally, we propose a successful method to combine the\nproperties of these two nonconformity functions. The experimental evaluation is\ndone using 11 real and 5 synthetic datasets.",
          "link": "http://arxiv.org/abs/2108.05677",
          "publishedOn": "2021-08-13T01:56:56.555Z",
          "wordCount": 622,
          "title": "How Nonconformity Functions and Difficulty of Datasets Impact the Efficiency of Conformal Classifiers. (arXiv:2108.05677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yufei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingquan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Min Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ali Muhamed Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Hanqi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherubin_L/0/1/0/all/0/1\">Laurent Cherubin</a>",
          "description": "Spatio-temporal forecasting is of great importance in a wide range of\ndynamical systems applications from atmospheric science, to recent COVID-19\nspread modeling. These applications rely on accurate predictions of\nspatio-temporal structured data reflecting real-world phenomena. A stunning\ncharacteristic is that the dynamical system is not only driven by some physics\nlaws but also impacted by the localized factor in spatial and temporal regions.\nOne of the major challenges is to infer the underlying causes, which generate\nthe perceived data stream and propagate the involved causal dynamics through\nthe distributed observing units. Another challenge is that the success of\nmachine learning based predictive models requires massive annotated data for\nmodel training. However, the acquisition of high-quality annotated data is\nobjectively manual and tedious as it needs a considerable amount of human\nintervention, making it infeasible in fields that require high levels of\nexpertise. To tackle these challenges, we advocate a spatio-temporal\nphysics-coupled neural networks (ST-PCNN) model to learn the underlying physics\nof the dynamical system and further couple the learned physics to assist the\nlearning of the recurring dynamics. To deal with data-acquisition constraints,\nan active learning mechanism with Kriging for actively acquiring the most\ninformative data is proposed for ST-PCNN training in a partially observable\nenvironment. Our experiments on both synthetic and real-world datasets exhibit\nthat the proposed ST-PCNN with active learning converges to near optimal\naccuracy with substantially fewer instances.",
          "link": "http://arxiv.org/abs/2108.05385",
          "publishedOn": "2021-08-13T01:56:56.542Z",
          "wordCount": 711,
          "title": "Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems. (arXiv:2108.05385v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.02688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1\">Thomas L. Athey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingshan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedigo_B/0/1/0/all/0/1\">Benjamin D. Pedigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>",
          "description": "Background: Gaussian mixture modeling is a fundamental tool in clustering, as\nwell as discriminant analysis and semiparametric density estimation. However,\nestimating the optimal model for any given number of components is an NP-hard\nproblem, and estimating the number of components is in some respects an even\nharder problem. Findings: In R, a popular package called mclust addresses both\nof these problems. However, Python has lacked such a package. We therefore\nintroduce AutoGMM, a Python algorithm for automatic Gaussian mixture modeling,\nand its hierarchical version, HGMM. AutoGMM builds upon scikit-learn's\nAgglomerativeClustering and GaussianMixture classes, with certain modifications\nto make the results more stable. Empirically, on several different\napplications, AutoGMM performs approximately as well as mclust, and sometimes\nbetter. Conclusions: AutoMM, a freely available Python package, enables\nefficient Gaussian mixture modeling by automatically selecting the\ninitialization, number of clusters and covariance constraints.",
          "link": "http://arxiv.org/abs/1909.02688",
          "publishedOn": "2021-08-13T01:56:56.527Z",
          "wordCount": 634,
          "title": "AutoGMM: Automatic and Hierarchical Gaussian Mixture Modeling in Python. (arXiv:1909.02688v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aman Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1\">Rohan Ramanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1\">Anika Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sirou Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingzhou Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keerthi_S/0/1/0/all/0/1\">S. Sathiya Keerthi</a>",
          "description": "Over-parameterized deep networks trained using gradient-based optimizers are\na popular choice for solving classification and ranking problems. Without\nappropriately tuned $\\ell_2$ regularization or weight decay, such networks have\nthe tendency to make output scores (logits) and network weights large, causing\ntraining loss to become too small and the network to lose its adaptivity\n(ability to move around) in the parameter space. Although regularization is\ntypically understood from an overfitting perspective, we highlight its role in\nmaking the network more adaptive and enabling it to escape more easily from\nweights that generalize poorly. To provide such a capability, we propose a\nmethod called Logit Attenuating Weight Normalization (LAWN), that can be\nstacked onto any gradient-based optimizer. LAWN controls the logits by\nconstraining the weight norms of layers in the final homogeneous sub-network.\nEmpirically, we show that the resulting LAWN variant of the optimizer makes a\ndeep network more adaptive to finding minimas with superior generalization\nperformance on large-scale image classification and recommender systems. While\nLAWN is particularly impressive in improving Adam, it greatly improves all\noptimizers when used with large batch sizes",
          "link": "http://arxiv.org/abs/2108.05839",
          "publishedOn": "2021-08-13T01:56:56.520Z",
          "wordCount": 629,
          "title": "Logit Attenuating Weight Normalization. (arXiv:2108.05839v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_S/0/1/0/all/0/1\">Sharan Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Totaro_S/0/1/0/all/0/1\">Simone Totaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1\">Robert Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1\">Matthieu Geist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1\">Marlos C. Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>",
          "description": "We use functional mirror ascent to propose a general framework (referred to\nas FMA-PG) for designing policy gradient methods. The functional perspective\ndistinguishes between a policy's functional representation (what are its\nsufficient statistics) and its parameterization (how are these statistics\nrepresented) and naturally results in computationally efficient off-policy\nupdates. For simple policy parameterizations, the FMA-PG framework ensures that\nthe optimal policy is a fixed point of the updates. It also allows us to handle\ncomplex policy parameterizations (e.g., neural networks) while guaranteeing\npolicy improvement. Our framework unifies several PG methods and opens the way\nfor designing sample-efficient variants of existing methods. Moreover, it\nrecovers important implementation heuristics (e.g., using forward vs reverse KL\ndivergence) in a principled way. With a softmax functional representation,\nFMA-PG results in a variant of TRPO with additional desirable properties. It\nalso suggests an improved variant of PPO, whose robustness and efficiency we\nempirically demonstrate on MuJoCo. Via experiments on simple reinforcement\nlearning problems, we evaluate algorithms instantiated by FMA-PG.",
          "link": "http://arxiv.org/abs/2108.05828",
          "publishedOn": "2021-08-13T01:56:56.487Z",
          "wordCount": 622,
          "title": "A functional mirror ascent view of policy gradient methods with function approximation. (arXiv:2108.05828v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarov_J/0/1/0/all/0/1\">Jafar Jafarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalhan_S/0/1/0/all/0/1\">Sanchit Kalhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_K/0/1/0/all/0/1\">Konstantin Makarychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_Y/0/1/0/all/0/1\">Yury Makarychev</a>",
          "description": "In the Correlation Clustering problem, we are given a weighted graph $G$ with\nits edges labeled as \"similar\" or \"dissimilar\" by a binary classifier. The goal\nis to produce a clustering that minimizes the weight of \"disagreements\": the\nsum of the weights of \"similar\" edges across clusters and \"dissimilar\" edges\nwithin clusters. We study the correlation clustering problem under the\nfollowing assumption: Every \"similar\" edge $e$ has weight\n$\\mathbf{w}_e\\in[\\alpha \\mathbf{w}, \\mathbf{w}]$ and every \"dissimilar\" edge\n$e$ has weight $\\mathbf{w}_e\\geq \\alpha \\mathbf{w}$ (where $\\alpha\\leq 1$ and\n$\\mathbf{w}>0$ is a scaling parameter). We give a $(3 + 2 \\log_e (1/\\alpha))$\napproximation algorithm for this problem. This assumption captures well the\nscenario when classification errors are asymmetric. Additionally, we show an\nasymptotically matching Linear Programming integrality gap of $\\Omega(\\log\n1/\\alpha)$.",
          "link": "http://arxiv.org/abs/2108.05696",
          "publishedOn": "2021-08-13T01:56:56.482Z",
          "wordCount": 579,
          "title": "Correlation Clustering with Asymmetric Classification Errors. (arXiv:2108.05696v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakhamaneshi_K/0/1/0/all/0/1\">Kourosh Hakhamaneshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1\">Michael Laskin</a>",
          "description": "A promising approach to solving challenging long-horizon tasks has been to\nextract behavior priors (skills) by fitting generative models to large offline\ndatasets of demonstrations. However, such generative models inherit the biases\nof the underlying data and result in poor and unusable skills when trained on\nimperfect demonstration data. To better align skill extraction with human\nintent we present Skill Preferences (SkiP), an algorithm that learns a model\nover human preferences and uses it to extract human-aligned skills from offline\ndata. After extracting human-preferred skills, SkiP also utilizes human\nfeedback to solve down-stream tasks with RL. We show that SkiP enables a\nsimulated kitchen robot to solve complex multi-step manipulation tasks and\nsubstantially outperforms prior leading RL algorithms with human preferences as\nwell as leading skill extraction algorithms without human preferences.",
          "link": "http://arxiv.org/abs/2108.05382",
          "publishedOn": "2021-08-13T01:56:56.460Z",
          "wordCount": 586,
          "title": "Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback. (arXiv:2108.05382v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05801",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Akioyamen_P/0/1/0/all/0/1\">Peter Akioyamen</a> (1), <a href=\"http://arxiv.org/find/q-fin/1/au:+Tang_Y/0/1/0/all/0/1\">Yi Zhou Tang</a> (1), <a href=\"http://arxiv.org/find/q-fin/1/au:+Hussien_H/0/1/0/all/0/1\">Hussien Hussien</a> (1) ((1) Western University)",
          "description": "Financial markets are of much interest to researchers due to their dynamic\nand stochastic nature. With their relations to world populations, global\neconomies and asset valuations, understanding, identifying and forecasting\ntrends and regimes are highly important. Attempts have been made to forecast\nmarket trends by employing machine learning methodologies, while statistical\ntechniques have been the primary methods used in developing market regime\nswitching models used for trading and hedging. In this paper we present a novel\nframework for the detection of regime switches within the US financial markets.\nPrincipal component analysis is applied for dimensionality reduction and the\nk-means algorithm is used as a clustering technique. Using a combination of\ncluster analysis and classification, we identify regimes in financial markets\nbased on publicly available economic data. We display the efficacy of the\nframework by constructing and assessing the performance of two trading\nstrategies based on detected regimes.",
          "link": "http://arxiv.org/abs/2108.05801",
          "publishedOn": "2021-08-13T01:56:56.454Z",
          "wordCount": 630,
          "title": "A Hybrid Learning Approach to Detecting Regime Switches in Financial Markets. (arXiv:2108.05801v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_A/0/1/0/all/0/1\">Aditya Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouganis_C/0/1/0/all/0/1\">Christos-Savvas Bouganis</a>",
          "description": "The increased memory and processing capabilities of today's edge devices\ncreate opportunities for greater edge intelligence. In the domain of vision,\nthe ability to adapt a Convolutional Neural Network's (CNN) structure and\nparameters to the input data distribution leads to systems with lower memory\nfootprint, latency and power consumption. However, due to the limited compute\nresources and memory budget on edge devices, it is necessary for the system to\nbe able to predict the latency and memory footprint of the training process in\norder to identify favourable training configurations of the network topology\nand device combination for efficient network adaptation. This work proposes\nperf4sight, an automated methodology for developing accurate models that\npredict CNN training memory footprint and latency given a target device and\nnetwork. This enables rapid identification of network topologies that can be\nretrained on the edge device with low resource consumption. With PyTorch as the\nframework and NVIDIA Jetson TX2 as the target device, the developed models\npredict training memory footprint and latency with 95% and 91% accuracy\nrespectively for a wide range of networks, opening the path towards efficient\nnetwork adaptation on edge GPUs.",
          "link": "http://arxiv.org/abs/2108.05580",
          "publishedOn": "2021-08-13T01:56:56.443Z",
          "wordCount": 647,
          "title": "perf4sight: A toolflow to model CNN training performance on Edge GPUs. (arXiv:2108.05580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gama_P/0/1/0/all/0/1\">Pedro H. T. Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "In this paper, we propose a novel approach for few-shot semantic segmentation\nwith sparse labeled images. We investigate the effectiveness of our method,\nwhich is based on the Model-Agnostic Meta-Learning (MAML) algorithm, in the\nmedical scenario, where the use of sparse labeling and few-shot can alleviate\nthe cost of producing new annotated datasets. Our method uses sparse labels in\nthe meta-training and dense labels in the meta-test, thus making the model\nlearn to predict dense labels from sparse ones. We conducted experiments with\nfour Chest X-Ray datasets to evaluate two types of annotations (grid and\npoints). The results show that our method is the most suitable when the target\ndomain highly differs from source domains, achieving Jaccard scores comparable\nto dense labels, using less than 2% of the pixels of an image with labels in\nfew-shot scenarios.",
          "link": "http://arxiv.org/abs/2108.05476",
          "publishedOn": "2021-08-13T01:56:56.413Z",
          "wordCount": 573,
          "title": "Weakly Supervised Medical Image Segmentation. (arXiv:2108.05476v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1602.03822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Motivated by a $2$-dimensional (unsupervised) image segmentation task whereby\nlocal regions of pixels are clustered via edge detection methods, a more\ngeneral probabilistic mathematical framework is devised. Critical thresholds\nare calculated that indicate strong correlation between randomly-generated,\nhigh dimensional data points that have been projected into structures in a\npartition of a bounded, $2$-dimensional area, of which, an image is a special\ncase. A neighbor concept for structures in the partition is defined and a\ncritical radius is uncovered. Measured from a central structure in localized\nregions of the partition, the radius indicates strong, long and short range\ncorrelation in the count of occupied structures. The size of a short interval\nof radii is estimated upon which the transition from short-to-long range\ncorrelation is virtually assured, which defines a demarcation of when an image\nceases to be \"interesting\".",
          "link": "http://arxiv.org/abs/1602.03822",
          "publishedOn": "2021-08-13T01:56:56.304Z",
          "wordCount": 697,
          "title": "A Critical Connectivity Radius for Randomly-Generated, High Dimensional Data Points. (arXiv:1602.03822v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Cat P. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mohammadreza Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravier_R/0/1/0/all/0/1\">Robert Ravier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>",
          "description": "In this paper, we propose a neural architecture search framework based on a\nsimilarity measure between the baseline tasks and the incoming target task. We\nfirst define the notion of task similarity based on the log-determinant of the\nFisher Information Matrices. Next, we compute the task similarity from each of\nthe baseline tasks to the incoming target task. By utilizing the relation\nbetween a target and a set of learned baseline tasks, the search space of\narchitectures for the incoming target task can be significantly reduced, making\nthe discovery of the best candidates in the set of possible architectures\ntractable and efficient, in terms of GPU days. This method eliminates the\nrequirement for training the networks from scratch for the incoming target task\nas well as introducing the bias in the initialization of the search space from\nthe human domain. Experimental results with 8 classification tasks in MNIST and\nCIFAR-10 datasets illustrate the efficacy of our proposed approach and its\ncompetitiveness with other state-of-art methods in terms of the classification\nperformance, the number of parameters, and the search time.",
          "link": "http://arxiv.org/abs/2103.00241",
          "publishedOn": "2021-08-13T01:56:56.293Z",
          "wordCount": 661,
          "title": "Neural Architecture Search From Task Similarity Measure. (arXiv:2103.00241v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rella_E/0/1/0/all/0/1\">Edoardo Mello Rella</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zaech_J/0/1/0/all/0/1\">Jan-Nico Zaech</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a> (1 and 2) ((1) Computer Vision Lab, ETH Z&#xfc;urich (2) PSI, KU Leuven)",
          "description": "Forecasting the future behavior of all traffic agents in the vicinity is a\nkey task to achieve safe and reliable autonomous driving systems. It is a\nchallenging problem as agents adjust their behavior depending on their\nintentions, the others' actions, and the road layout. In this paper, we propose\nDecoder Fusion RNN (DF-RNN), a recurrent, attention-based approach for motion\nforecasting. Our network is composed of a recurrent behavior encoder, an\ninter-agent multi-headed attention module, and a context-aware decoder. We\ndesign a map encoder that embeds polyline segments, combines them to create a\ngraph structure, and merges their relevant parts with the agents' embeddings.\nWe fuse the encoded map information with further inter-agent interactions only\ninside the decoder and propose to use explicit training as a method to\neffectively utilize the information available. We demonstrate the efficacy of\nour method by testing it on the Argoverse motion forecasting dataset and show\nits state-of-the-art performance on the public benchmark.",
          "link": "http://arxiv.org/abs/2108.05814",
          "publishedOn": "2021-08-13T01:56:56.279Z",
          "wordCount": 618,
          "title": "Decoder Fusion RNN: Context and Interaction Aware Decoders for Trajectory Prediction. (arXiv:2108.05814v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang-Hua Gao</a>",
          "description": "In recent years, the connections between deep residual networks and\nfirst-order Ordinary Differential Equations (ODEs) have been disclosed. In this\nwork, we further bridge the deep neural architecture design with the\nsecond-order ODEs and propose a novel reversible neural network, termed as\nm-RevNet, that is characterized by inserting momentum update to residual\nblocks. The reversible property allows us to perform backward pass without\naccess to activation values of the forward pass, greatly relieving the storage\nburden during training. Furthermore, the theoretical foundation based on\nsecond-order ODEs grants m-RevNet with stronger representational power than\nvanilla residual networks, which potentially explains its performance gains.\nFor certain learning scenarios, we analytically and empirically reveal that our\nm-RevNet succeeds while standard ResNet fails. Comprehensive experiments on\nvarious image classification and semantic segmentation benchmarks demonstrate\nthe superiority of our m-RevNet over ResNet, concerning both memory efficiency\nand recognition performance.",
          "link": "http://arxiv.org/abs/2108.05862",
          "publishedOn": "2021-08-13T01:56:56.248Z",
          "wordCount": 582,
          "title": "m-RevNet: Deep Reversible Neural Networks with Momentum. (arXiv:2108.05862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_P/0/1/0/all/0/1\">Panhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ZHANG_L/0/1/0/all/0/1\">Lin ZHANG</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zijian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xuejing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_A/0/1/0/all/0/1\">Anlong Ming</a>",
          "description": "Retrieving occlusion relation among objects in a single image is challenging\ndue to sparsity of boundaries in image. We observe two key issues in existing\nworks: firstly, lack of an architecture which can exploit the limited amount of\ncoupling in the decoder stage between the two subtasks, namely occlusion\nboundary extraction and occlusion orientation prediction, and secondly,\nimproper representation of occlusion orientation. In this paper, we propose a\nnovel architecture called Occlusion-shared and Path-separated Network (OPNet),\nwhich solves the first issue by exploiting rich occlusion cues in shared\nhigh-level features and structured spatial information in task-specific\nlow-level features. We then design a simple but effective orthogonal occlusion\nrepresentation (OOR) to tackle the second issue. Our method surpasses the\nstate-of-the-art methods by 6.1%/8.3% Boundary-AP and 6.5%/10% Orientation-AP\non standard PIOD/BSDS ownership datasets. Code is available at\nhttps://github.com/fengpanhe/MT-ORL.",
          "link": "http://arxiv.org/abs/2108.05722",
          "publishedOn": "2021-08-13T01:56:56.235Z",
          "wordCount": 589,
          "title": "MT-ORL: Multi-Task Occlusion Relationship Learning. (arXiv:2108.05722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Golia_P/0/1/0/all/0/1\">Priyanka Golia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slivovsky_F/0/1/0/all/0/1\">Friedrich Slivovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Subhajit Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meel_K/0/1/0/all/0/1\">Kuldeep S. Meel</a>",
          "description": "Given a Boolean specification between a set of inputs and outputs, the\nproblem of Boolean functional synthesis is to synthesise each output as a\nfunction of inputs such that the specification is met. Although the past few\nyears have witnessed intense algorithmic development, accomplishing scalability\nremains the holy grail. The state-of-the-art approach combines machine learning\nand automated reasoning to efficiently synthesise Boolean functions. In this\npaper, we propose four algorithmic improvements for a data-driven framework for\nfunctional synthesis: using a dependency-driven multi-classifier to learn\ncandidate function, extracting uniquely defined functions by interpolation,\nvariables retention, and using lexicographic MaxSAT to repair candidates. We\nimplement these improvements in the state-of-the-art framework, called Manthan.\nThe proposed framework is called Manthan2. Manthan2 shows significantly\nimproved runtime performance compared to Manthan. In an extensive experimental\nevaluation on 609 benchmarks, Manthan2 is able to synthesise a Boolean function\nvector for 509 instances compared to 356 instances solved by Manthan--- an\nincrement of 153 instances over the state-of-the-art. To put this into\nperspective, Manthan improved on the prior state-of-the-art by only 76\ninstances.",
          "link": "http://arxiv.org/abs/2108.05717",
          "publishedOn": "2021-08-13T01:56:56.218Z",
          "wordCount": 628,
          "title": "Engineering an Efficient Boolean Functional Synthesis Engine. (arXiv:2108.05717v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makantasis_K/0/1/0/all/0/1\">Konstantinos Makantasis</a>",
          "description": "Many of the affect modelling tasks present an asymmetric distribution of\ninformation between training and test time; additional information is given\nabout the training data, which is not available at test time. Learning under\nthis setting is called Learning Under Privileged Information (LUPI). At the\nsame time, due to the ordinal nature of affect annotations, formulating affect\nmodelling tasks as supervised learning ranking problems is gaining ground\nwithin the Affective Computing research community. Motivated by the two facts\nabove, in this study, we introduce a ranking model that treats additional\ninformation about the training data as privileged information to accurately\nrank affect states. Our ranking model extends the well-known RankNet model to\nthe LUPI paradigm, hence its name AffRankNet+. To the best of our knowledge, it\nis the first time that a ranking model based on neural networks exploits\nprivileged information. We evaluate the performance of the proposed model on\nthe public available Afew-VA dataset and compare it against the RankNet model,\nwhich does not use privileged information. Experimental evaluation indicates\nthat the AffRankNet+ model can yield significantly better performance.",
          "link": "http://arxiv.org/abs/2108.05598",
          "publishedOn": "2021-08-13T01:56:56.193Z",
          "wordCount": 619,
          "title": "AffRankNet+: Ranking Affect Using Privileged Information. (arXiv:2108.05598v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05574",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1\">Jiangyuan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh V. Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wong_R/0/1/0/all/0/1\">Raymond K. W. Wong</a>",
          "description": "In this paper, we study the implicit bias of gradient descent for sparse\nregression. We extend results on regression with quadratic parametrization,\nwhich amounts to depth-2 diagonal linear networks, to more general depth-N\nnetworks, under more realistic settings of noise and correlated designs. We\nshow that early stopping is crucial for gradient descent to converge to a\nsparse model, a phenomenon that we call implicit sparse regularization. This\nresult is in sharp contrast to known results for noiseless and\nuncorrelated-design cases. We characterize the impact of depth and early\nstopping and show that for a general depth parameter N, gradient descent with\nearly stopping achieves minimax optimal sparse recovery with sufficiently small\ninitialization and step size. In particular, we show that increasing depth\nenlarges the scale of working initialization and the early-stopping window,\nwhich leads to more stable gradient paths for sparse recovery.",
          "link": "http://arxiv.org/abs/2108.05574",
          "publishedOn": "2021-08-13T01:56:56.179Z",
          "wordCount": 587,
          "title": "Implicit Sparse Regularization: The Impact of Depth and Early Stopping. (arXiv:2108.05574v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Werner_T/0/1/0/all/0/1\">Thorben Werner</a>",
          "description": "Machine Learning requires large amounts of labeled data to fit a model. Many\ndatasets are already publicly available, nevertheless forcing application\npossibilities of machine learning to the domains of those public datasets. The\never-growing penetration of machine learning algorithms in new application\nareas requires solutions for the need for data in those new domains. This\nthesis works on active learning as one possible solution to reduce the amount\nof data that needs to be processed by hand, by processing only those datapoints\nthat specifically benefit the training of a strong model for the task. A newly\nproposed framework for framing the active learning workflow as a reinforcement\nlearning problem is adapted for image classification and a series of three\nexperiments is conducted. Each experiment is evaluated and potential issues\nwith the approach are outlined. Each following experiment then proposes\nimprovements to the framework and evaluates their impact. After the last\nexperiment, a final conclusion is drawn, unfortunately rejecting this work's\nhypothesis and outlining that the proposed framework at the moment is not\ncapable of improving active learning for image classification with a trained\nreinforcement learning agent.",
          "link": "http://arxiv.org/abs/2108.05595",
          "publishedOn": "2021-08-13T01:56:56.162Z",
          "wordCount": 614,
          "title": "Reinforcement Learning Approach to Active Learning for Image Classification. (arXiv:2108.05595v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andoni_A/0/1/0/all/0/1\">Alexandr Andoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaglehole_D/0/1/0/all/0/1\">Daniel Beaglehole</a>",
          "description": "The indexing algorithms for the high-dimensional nearest neighbor search\n(NNS) with the best worst-case guarantees are based on the randomized Locality\nSensitive Hashing (LSH), and its derivatives. In practice, many heuristic\napproaches exist to \"learn\" the best indexing method in order to speed-up NNS,\ncrucially adapting to the structure of the given dataset.\n\nOftentimes, these heuristics outperform the LSH-based algorithms on real\ndatasets, but, almost always, come at the cost of losing the guarantees of\neither correctness or robust performance on adversarial queries, or apply to\ndatasets with an assumed extra structure/model. In this paper, we design an NNS\nalgorithm for the Hamming space that has worst-case guarantees essentially\nmatching that of theoretical algorithms, while optimizing the hashing to the\nstructure of the dataset (think instance-optimal algorithms) for performance on\nthe minimum-performing query. We evaluate the algorithm's ability to optimize\nfor a given dataset both theoretically and practically. On the theoretical\nside, we exhibit a natural setting (dataset model) where our algorithm is much\nbetter than the standard theoretical one. On the practical side, we run\nexperiments that show that our algorithm has a 1.8x and 2.1x better recall on\nthe worst-performing queries to the MNIST and ImageNet datasets.",
          "link": "http://arxiv.org/abs/2108.05433",
          "publishedOn": "2021-08-13T01:56:56.156Z",
          "wordCount": 629,
          "title": "Learning to Hash Robustly, with Guarantees. (arXiv:2108.05433v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Mingjian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indrakanti_S/0/1/0/all/0/1\">Saratchandra Indrakanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannadasan_M/0/1/0/all/0/1\">Manojkumar Rangasamy Kannadasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagherjeiran_A/0/1/0/all/0/1\">Abraham Bagherjeiran</a>",
          "description": "The top search results matching a user query that are displayed on the first\npage are critical to the effectiveness and perception of a search system. A\nsearch ranking system typically orders the results by independent\nquery-document scores to produce a slate of search results. However, such\nunilateral scoring methods may fail to capture inter-document dependencies that\nusers are sensitive to, thus producing a sub-optimal slate. Further, in\npractice, many real-world applications such as e-commerce search require\nenforcing certain distributional criteria at the slate-level, due to business\nobjectives or long term user retention goals. Unilateral scoring of results\ndoes not explicitly support optimizing for such objectives with respect to a\nslate. Hence, solutions to the slate optimization problem must consider the\noptimal selection and order of the documents, along with adherence to\nslate-level distributional criteria. To that end, we propose a hybrid framework\nextended from traditional slate optimization to solve the conditional slate\noptimization problem. We introduce conditional sequential slate optimization\n(CSSO), which jointly learns to optimize for traditional ranking metrics as\nwell as prescribed distribution criteria of documents within the slate. The\nproposed method can be applied to practical real world problems such as\nenforcing diversity in e-commerce search results, mitigating bias in top\nresults and personalization of results. Experiments on public datasets and\nreal-world data from e-commerce datasets show that CSSO outperforms popular\ncomparable ranking methods in terms of adherence to distributional criteria\nwhile producing comparable or better relevance metrics.",
          "link": "http://arxiv.org/abs/2108.05618",
          "publishedOn": "2021-08-13T01:56:56.150Z",
          "wordCount": 678,
          "title": "Conditional Sequential Slate Optimization. (arXiv:2108.05618v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Altaf_F/0/1/0/all/0/1\">Fouzia Altaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Syed M.S. Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>",
          "description": "Deep learning is gaining instant popularity in computer aided diagnosis of\nCOVID-19. Due to the high sensitivity of Computed Tomography (CT) to this\ndisease, CT-based COVID-19 detection with visual models is currently at the\nforefront of medical imaging research. Outcomes published in this direction are\nfrequently claiming highly accurate detection under deep transfer learning.\nThis is leading medical technologists to believe that deep transfer learning is\nthe mainstream solution for the problem. However, our critical analysis of the\nliterature reveals an alarming performance disparity between different\npublished results. Hence, we conduct a systematic thorough investigation to\nanalyze the effectiveness of deep transfer learning for COVID-19 detection with\nCT images. Exploring 14 state-of-the-art visual models with over 200 model\ntraining sessions, we conclusively establish that the published literature is\nfrequently overestimating transfer learning performance for the problem, even\nin the prestigious scientific sources. The roots of overestimation trace back\nto inappropriate data curation. We also provide case studies that consider more\nrealistic scenarios, and establish transparent baselines for the problem. We\nhope that our reproducible investigation will help in curbing hype-driven\nclaims for the critical problem of COVID-19 diagnosis, and pave the way for a\nmore transparent performance evaluation of techniques for CT-based COVID-19\ndetection.",
          "link": "http://arxiv.org/abs/2108.05649",
          "publishedOn": "2021-08-13T01:56:56.144Z",
          "wordCount": 714,
          "title": "Resetting the baseline: CT-based COVID-19 diagnosis with Deep Transfer Learning is not as accurate as widely thought. (arXiv:2108.05649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samadi_A/0/1/0/all/0/1\">Anahita Samadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debapriya Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1\">Shirin Nilizadeh</a>",
          "description": "Recently, some studies have shown that text classification tasks are\nvulnerable to poisoning and evasion attacks. However, little work has\ninvestigated attacks against decision making algorithms that use text\nembeddings, and their output is a ranking. In this paper, we focus on ranking\nalgorithms for recruitment process, that employ text embeddings for ranking\napplicants resumes when compared to a job description. We demonstrate both\nwhite box and black box attacks that identify text items, that based on their\nlocation in embedding space, have significant contribution in increasing the\nsimilarity score between a resume and a job description. The adversary then\nuses these text items to improve the ranking of their resume among others. We\ntested recruitment algorithms that use the similarity scores obtained from\nUniversal Sentence Encoder (USE) and Term Frequency Inverse Document Frequency\n(TF IDF) vectors. Our results show that in both adversarial settings, on\naverage the attacker is successful. We also found that attacks against TF IDF\nis more successful compared to USE.",
          "link": "http://arxiv.org/abs/2108.05490",
          "publishedOn": "2021-08-13T01:56:56.134Z",
          "wordCount": 609,
          "title": "Attacks against Ranking Algorithms with Text Embeddings: a Case Study on Recruitment Algorithms. (arXiv:2108.05490v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xiaohui Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_D/0/1/0/all/0/1\">Dan Pei</a>",
          "description": "To ensure robust and reliable classification results, OoD\n(out-of-distribution) indicators based on deep generative models are proposed\nrecently and are shown to work well on small datasets. In this paper, we\nconduct the first large collection of benchmarks (containing 92 dataset pairs,\nwhich is 1 order of magnitude larger than previous ones) for existing OoD\nindicators and observe that none perform well. We thus advocate that a large\ncollection of benchmarks is mandatory for evaluating OoD indicators. We propose\na novel theoretical framework, DOI, for divergence-based Out-of-Distribution\nindicators (instead of traditional likelihood-based) in deep generative models.\nFollowing this framework, we further propose a simple and effective OoD\ndetection algorithm: Single-shot Fine-tune. It significantly outperforms past\nworks by 5~8 in AUROC, and its performance is close to optimal. In recent, the\nlikelihood criterion is shown to be ineffective in detecting OoD. Single-shot\nFine-tune proposes a novel fine-tune criterion to detect OoD, by whether the\nlikelihood of the testing sample is improved after fine-tuning a well-trained\nmodel on it. Fine-tune criterion is a clear and easy-following criterion, which\nwill lead the OoD domain into a new stage.",
          "link": "http://arxiv.org/abs/2108.05509",
          "publishedOn": "2021-08-13T01:56:56.128Z",
          "wordCount": 615,
          "title": "DOI: Divergence-based Out-of-Distribution Indicators via Deep Generative Models. (arXiv:2108.05509v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_J/0/1/0/all/0/1\">Jovita Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>",
          "description": "Differentiable architecture search (DARTS) is a widely researched tool for\nneural architecture search, due to its promising results for image\nclassification. The main benefit of DARTS is the effectiveness achieved through\nthe weight-sharing one-shot paradigm, which allows efficient architecture\nsearch. In this work, we investigate DARTS in a systematic case study of\ninverse problems, which allows us to analyze these potential benefits in a\ncontrolled manner. Although we demonstrate that the success of DARTS can be\nextended from image classification to reconstruction, our experiments yield\nthree fundamental difficulties in the evaluation of DARTS-based methods: First,\nthe results show a large variance in all test cases. Second, the final\nperformance is highly dependent on the hyperparameters of the optimizer. And\nthird, the performance of the weight-sharing architecture used during training\ndoes not reflect the final performance of the found architecture well. Thus, we\nconclude the necessity to 1) report the results of any DARTS-based methods from\nseveral runs along with its underlying performance statistics, 2) show the\ncorrelation of the training and final architecture performance, and 3)\ncarefully consider if the computational efficiency of DARTS outweighs the costs\nof hyperparameter optimization and multiple runs.",
          "link": "http://arxiv.org/abs/2108.05647",
          "publishedOn": "2021-08-13T01:56:56.113Z",
          "wordCount": 637,
          "title": "DARTS for Inverse Problems: a Study on Hyperparameter Sensitivity. (arXiv:2108.05647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dereich_S/0/1/0/all/0/1\">S. Dereich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kassing_S/0/1/0/all/0/1\">S. Kassing</a>",
          "description": "The realization function of a shallow ReLU network is a continuous and\npiecewise affine function $f:\\mathbb R^d\\to \\mathbb R$, where the domain\n$\\mathbb R^{d}$ is partitioned by a set of $n$ hyperplanes into cells on which\n$f$ is affine. We show that the minimal representation for $f$ uses either $n$,\n$n+1$ or $n+2$ neurons and we characterize each of the three cases. In the\nparticular case, where the input layer is one-dimensional, minimal\nrepresentations always use at most $n+1$ neurons but in all higher dimensional\nsettings there are functions for which $n+2$ neurons are needed. Then we show\nthat the set of minimal networks representing $f$ forms a\n$C^\\infty$-submanifold $M$ and we derive the dimension and the number of\nconnected components of $M$. Additionally, we give a criterion for the\nhyperplanes that guarantees that all continuous, piecewise affine functions are\nrealization functions of appropriate ReLU networks.",
          "link": "http://arxiv.org/abs/2108.05643",
          "publishedOn": "2021-08-13T01:56:56.107Z",
          "wordCount": 586,
          "title": "On minimal representations of shallow ReLU networks. (arXiv:2108.05643v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_B/0/1/0/all/0/1\">Botao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1\">Yasin Abbasi-Yadkori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazi%7Bc%7D_N/0/1/0/all/0/1\">Nevena Lazi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesv%7Ba%7Dri_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>",
          "description": "We study query and computationally efficient planning algorithms with linear\nfunction approximation and a simulator. We assume that the agent only has local\naccess to the simulator, meaning that the agent can only query the simulator at\nstates that have been visited before. This setting is more practical than many\nprior works on reinforcement learning with a generative model. We propose an\nalgorithm named confident Monte Carlo least square policy iteration (Confident\nMC-LSPI) for this setting. Under the assumption that the Q-functions of all\ndeterministic policies are linear in known features of the state-action pairs,\nwe show that our algorithm has polynomial query and computational complexities\nin the dimension of the features, the effective planning horizon and the\ntargeted sub-optimality, while these complexities are independent of the size\nof the state space. One technical contribution of our work is the introduction\nof a novel proof technique that makes use of a virtual policy iteration\nalgorithm. We use this method to leverage existing results on\n$\\ell_\\infty$-bounded approximate policy iteration to show that our algorithm\ncan learn the optimal policy for the given initial state even only with local\naccess to the simulator. We believe that this technique can be extended to\nbroader settings beyond this work.",
          "link": "http://arxiv.org/abs/2108.05533",
          "publishedOn": "2021-08-13T01:56:56.095Z",
          "wordCount": 646,
          "title": "Efficient Local Planning with Linear Function Approximation. (arXiv:2108.05533v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>",
          "description": "We introduce the needs for explainable AI that arise from Standard No. 239\nfrom the Basel Committee on Banking Standards (BCBS 239), which outlines 11\nprinciples for effective risk data aggregation and risk reporting for financial\ninstitutions. Of these, explainableAI is necessary for compliance in two key\naspects: data quality, and appropriate reporting for multiple stakeholders. We\ndescribe the implementation challenges for one specific regulatory\nrequirement:that of having a complete data taxonomy that is appropriate for\nfirmwide use. The constantly evolving nature of financial ontologies\nnecessitate a continuous updating process to ensure ongoing compliance.",
          "link": "http://arxiv.org/abs/2108.05401",
          "publishedOn": "2021-08-13T01:56:56.088Z",
          "wordCount": 555,
          "title": "Ontology drift is a challenge for explainable data governance. (arXiv:2108.05401v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Tae Ha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosse_J/0/1/0/all/0/1\">Juergen Bosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmico_S/0/1/0/all/0/1\">Simone D&#x27;Amico</a>",
          "description": "This work presents the most recent advances of the Robotic Testbed for\nRendezvous and Optical Navigation (TRON) at Stanford University - the first\nrobotic testbed capable of validating machine learning algorithms for\nspaceborne optical navigation. The TRON facility consists of two 6\ndegrees-of-freedom KUKA robot arms and a set of Vicon motion track cameras to\nreconfigure an arbitrary relative pose between a camera and a target mockup\nmodel. The facility includes multiple Earth albedo light boxes and a sun lamp\nto recreate the high-fidelity spaceborne illumination conditions. After the\noverview of the facility, this work details the multi-source calibration\nprocedure which enables the estimation of the relative pose between the object\nand the camera with millimeter-level position and millidegree-level orientation\naccuracies. Finally, a comparative analysis of the synthetic and TRON simulated\nimageries is performed using a Convolutional Neural Network (CNN) pre-trained\non the synthetic images. The result shows a considerable gap in the CNN's\nperformance, suggesting the TRON simulated images can be used to validate the\nrobustness of any machine learning algorithms trained on more easily accessible\nsynthetic imagery from computer graphics.",
          "link": "http://arxiv.org/abs/2108.05529",
          "publishedOn": "2021-08-13T01:56:56.081Z",
          "wordCount": 650,
          "title": "Robotic Testbed for Rendezvous and Optical Navigation: Multi-Source Calibration and Machine Learning Use Cases. (arXiv:2108.05529v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05531",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sadghiani_N/0/1/0/all/0/1\">Nima Salehi Sadghiani</a>, <a href=\"http://arxiv.org/find/math/1/au:+Motiian_S/0/1/0/all/0/1\">Saeid Motiian</a>",
          "description": "This study is concerned with the determination of optimal appointment times\nfor a sequence of jobs with uncertain duration. We investigate the data-driven\nAppointment Scheduling Problem (ASP) when one has $n$ observations of $p$\nfeatures (covariates) related to the jobs as well as historical data. We\nformulate ASP as an Integrated Estimation and Optimization problem using a\ntask-based loss function. We justify the use of contexts by showing that not\nincluding the them yields to inconsistent decisions, which translates to\nsub-optimal appointments. We validate our approach through two numerical\nexperiments.",
          "link": "http://arxiv.org/abs/2108.05531",
          "publishedOn": "2021-08-13T01:56:56.063Z",
          "wordCount": 517,
          "title": "The Contextual Appointment Scheduling Problem. (arXiv:2108.05531v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>",
          "description": "Recommender systems aim to provide personalized services to users and are\nplaying an increasingly important role in our daily lives. The key of\nrecommender systems is to predict how likely users will interact with items\nbased on their historical online behaviors, e.g., clicks, add-to-cart,\npurchases, etc. To exploit these user-item interactions, there are increasing\nefforts on considering the user-item interactions as a user-item bipartite\ngraph and then performing information propagation in the graph via Graph Neural\nNetworks (GNNs). Given the power of GNNs in graph representation learning,\nthese GNN-based recommendation methods have remarkably boosted the\nrecommendation performance. Despite their success, most existing GNN-based\nrecommender systems overlook the existence of interactions caused by unreliable\nbehaviors (e.g., random/bait clicks) and uniformly treat all the interactions,\nwhich can lead to sub-optimal and unstable performance. In this paper, we\ninvestigate the drawbacks (e.g., non-adaptive propagation and non-robustness)\nof existing GNN-based recommendation methods. To address these drawbacks, we\npropose the Graph Trend Networks for recommendations (GTN) with principled\ndesigns that can capture the adaptive reliability of the interactions.\nComprehensive experiments and ablation studies are presented to verify and\nunderstand the effectiveness of the proposed framework. Our implementation and\ndatasets can be released after publication.",
          "link": "http://arxiv.org/abs/2108.05552",
          "publishedOn": "2021-08-13T01:56:56.056Z",
          "wordCount": 641,
          "title": "Graph Trend Networks for Recommendations. (arXiv:2108.05552v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Deep learning algorithms mine knowledge from the training data and thus would\nlikely inherit the dataset's bias information. As a result, the obtained model\nwould generalize poorly and even mislead the decision process in real-life\napplications. We propose to remove the bias information misused by the target\ntask with a cross-sample adversarial debiasing (CSAD) method. CSAD explicitly\nextracts target and bias features disentangled from the latent representation\ngenerated by a feature extractor and then learns to discover and remove the\ncorrelation between the target and bias features. The correlation measurement\nplays a critical role in adversarial debiasing and is conducted by a\ncross-sample neural mutual information estimator. Moreover, we propose joint\ncontent and local structural representation learning to boost mutual\ninformation estimation for better performance. We conduct thorough experiments\non publicly available datasets to validate the advantages of the proposed\nmethod over state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.05449",
          "publishedOn": "2021-08-13T01:56:56.050Z",
          "wordCount": 586,
          "title": "Learning Bias-Invariant Representation by Cross-Sample Mutual Information Minimization. (arXiv:2108.05449v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storchan_V/0/1/0/all/0/1\">Victor Storchan</a>",
          "description": "Regulators have signalled an interest in adopting explainable AI(XAI)\ntechniques to handle the diverse needs for model governance, operational\nservicing, and compliance in the financial services industry. In this short\noverview, we review the recent technical literature in XAI and argue that based\non our current understanding of the field, the use of XAI techniques in\npractice necessitate a highly contextualized approach considering the specific\nneeds of stakeholders for particular business applications.",
          "link": "http://arxiv.org/abs/2108.05390",
          "publishedOn": "2021-08-13T01:56:56.023Z",
          "wordCount": 528,
          "title": "Seven challenges for harmonizing explainability requirements. (arXiv:2108.05390v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Mengmeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zehui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_C/0/1/0/all/0/1\">Cyril Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Federated learning (FL) serves as a data privacy-preserved machine learning\nparadigm, and realizes the collaborative model trained by distributed clients.\nTo accomplish an FL task, the task publisher needs to pay financial incentives\nto the FL server and FL server offloads the task to the contributing FL\nclients. It is challenging to design proper incentives for the FL clients due\nto the fact that the task is privately trained by the clients. This paper aims\nto propose a contract theory based FL task training model towards minimizing\nincentive budget subject to clients being individually rational (IR) and\nincentive compatible (IC) in each FL training round. We design a\ntwo-dimensional contract model by formally defining two private types of\nclients, namely data quality and computation effort. To effectively aggregate\nthe trained models, a contract-based aggregator is proposed. We analyze the\nfeasible and optimal contract solutions to the proposed contract model.\n%Experimental results demonstrate that the proposed framework and contract\nmodel can effective improve the generation accuracy of FL tasks. Experimental\nresults show that the generalization accuracy of the FL tasks can be improved\nby the proposed incentive mechanism where contract-based aggregation is\napplied.",
          "link": "http://arxiv.org/abs/2108.05568",
          "publishedOn": "2021-08-13T01:56:55.995Z",
          "wordCount": 665,
          "title": "A Contract Theory based Incentive Mechanism for Federated Learning. (arXiv:2108.05568v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05421",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gao_A/0/1/0/all/0/1\">Angela F. Gao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Castellanos_J/0/1/0/all/0/1\">Jorge C. Castellanos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ross_Z/0/1/0/all/0/1\">Zachary E. Ross</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Azizzadenesheli_K/0/1/0/all/0/1\">Kamyar Azizzadenesheli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Clayton_R/0/1/0/all/0/1\">Robert W. Clayton</a>",
          "description": "Seismic wave propagation forms the basis for most aspects of seismological\nresearch, yet solving the wave equation is a major computational burden that\ninhibits the progress of research. This is exaspirated by the fact that new\nsimulations must be performed when the velocity structure or source location is\nperturbed. Here, we explore a prototype framework for learning general\nsolutions using a recently developed machine learning paradigm called Neural\nOperator. A trained Neural Operator can compute a solution in negligible time\nfor any velocity structure or source location. We develop a scheme to train\nNeural Operators on an ensemble of simulations performed with random velocity\nmodels and source locations. As Neural Operators are grid-free, it is possible\nto evaluate solutions on higher resolution velocity models than trained on,\nproviding additional computational efficiency. We illustrate the method with\nthe 2D acoustic wave equation and demonstrate the method's applicability to\nseismic tomography, using reverse mode automatic differentiation to compute\ngradients of the wavefield with respect to the velocity structure. The\ndeveloped procedure is nearly an order of magnitude faster than using\nconventional numerical methods for full waveform inversion.",
          "link": "http://arxiv.org/abs/2108.05421",
          "publishedOn": "2021-08-13T01:56:55.979Z",
          "wordCount": 625,
          "title": "Seismic wave propagation and inversion with Neural Operators. (arXiv:2108.05421v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin F. Yang</a>",
          "description": "For the problem of task-agnostic reinforcement learning (RL), an agent first\ncollects samples from an unknown environment without the supervision of reward\nsignals, then is revealed with a reward and is asked to compute a corresponding\nnear-optimal policy. Existing approaches mainly concern the worst-case\nscenarios, in which no structural information of the reward/transition-dynamics\nis utilized. Therefore the best sample upper bound is\n$\\propto\\widetilde{\\mathcal{O}}(1/\\epsilon^2)$, where $\\epsilon>0$ is the\ntarget accuracy of the obtained policy, and can be overly pessimistic. To\ntackle this issue, we provide an efficient algorithm that utilizes a gap\nparameter, $\\rho>0$, to reduce the amount of exploration. In particular, for an\nunknown finite-horizon Markov decision process, the algorithm takes only\n$\\widetilde{\\mathcal{O}} (1/\\epsilon \\cdot (H^3SA / \\rho + H^4 S^2 A) )$\nepisodes of exploration, and is able to obtain an $\\epsilon$-optimal policy for\na post-revealed reward with sub-optimality gap at least $\\rho$, where $S$ is\nthe number of states, $A$ is the number of actions, and $H$ is the length of\nthe horizon, obtaining a nearly \\emph{quadratic saving} in terms of $\\epsilon$.\nWe show that, information-theoretically, this bound is nearly tight for $\\rho <\n\\Theta(1/(HS))$ and $H>1$. We further show that\n$\\propto\\widetilde{\\mathcal{O}}(1)$ sample bound is possible for $H=1$ (i.e.,\nmulti-armed bandit) or with a sampling simulator, establishing a stark\nseparation between those settings and the RL setting.",
          "link": "http://arxiv.org/abs/2108.05439",
          "publishedOn": "2021-08-13T01:56:55.949Z",
          "wordCount": 644,
          "title": "Gap-Dependent Unsupervised Exploration for Reinforcement Learning. (arXiv:2108.05439v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agastya_C/0/1/0/all/0/1\">Chitra Agastya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghebremusse_S/0/1/0/all/0/1\">Sirak Ghebremusse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_I/0/1/0/all/0/1\">Ian Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Colorado Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahabi_H/0/1/0/all/0/1\">Hossein Vahabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todeschini_A/0/1/0/all/0/1\">Alberto Todeschini</a>",
          "description": "Climate change has caused reductions in river runoffs and aquifer recharge\nresulting in an increasingly unsustainable crop water demand from reduced\nfreshwater availability. Achieving food security while deploying water in a\nsustainable manner will continue to be a major challenge necessitating careful\nmonitoring and tracking of agricultural water usage. Historically, monitoring\nwater usage has been a slow and expensive manual process with many\nimperfections and abuses. Ma-chine learning and remote sensing developments\nhave increased the ability to automatically monitor irrigation patterns, but\nexisting techniques often require curated and labelled irrigation data, which\nare expensive and time consuming to obtain and may not exist for impactful\nareas such as developing countries. In this paper, we explore an end-to-end\nreal world application of irrigation detection with uncurated and unlabeled\nsatellite imagery. We apply state-of-the-art self-supervised deep learning\ntechniques to optical remote sensing data, and find that we are able to detect\nirrigation with up to nine times better precision, 90% better recall and 40%\nmore generalization ability than the traditional supervised learning methods.",
          "link": "http://arxiv.org/abs/2108.05484",
          "publishedOn": "2021-08-13T01:56:55.943Z",
          "wordCount": 616,
          "title": "Self-supervised Contrastive Learning for Irrigation Detection in Satellite Imagery. (arXiv:2108.05484v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malliaris_M/0/1/0/all/0/1\">Maryanthe Malliaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Shay Moran</a>",
          "description": "We revisit a key idea from the interaction of model theory and combinatorics,\nthe existence of large ``indivisible'' sets, called ``$\\epsilon$-excellent,''\nin $k$-edge stable graphs (equivalently, Littlestone classes). Translating to\nthe language of probability, we find a quite different existence proof for\n$\\epsilon$-excellent sets in Littlestone classes, using regret bounds in online\nlearning. This proof applies to any $\\epsilon < {1}/{2}$, compared to $<\n{1}/{2^{2^k}}$ or so in the original proof. We include a second proof using\nclosure properties and the VC theorem, with other advantages but weaker bounds.\nAs a simple corollary, the Littlestone dimension remains finite under some\nnatural modifications to the definition. A theme in these proofs is the\ninteraction of two abstract notions of majority, arising from measure, and from\nrank or dimension; we prove that these densely often coincide and that this is\ncharacteristic of Littlestone (stable) classes. The last section lists several\nopen problems.",
          "link": "http://arxiv.org/abs/2108.05569",
          "publishedOn": "2021-08-13T01:56:55.902Z",
          "wordCount": 589,
          "title": "Agnostic Online Learning and Excellent Sets. (arXiv:2108.05569v1 [cs.DM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shubham Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_B/0/1/0/all/0/1\">Bhuvni Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kash_I/0/1/0/all/0/1\">Ian A. Kash</a>",
          "description": "We revisit the application of predictive models by the Chicago Department of\nPublic Health to schedule restaurant inspections and prioritize the detection\nof critical violations of the food code. Performing the first analysis from the\nperspective of fairness to the population served by the restaurants, we find\nthat the model treats inspections unequally based on the sanitarian who\nconducted the inspection and that in turn there are both geographic and\ndemographic disparities in the benefits of the model. We examine both\napproaches to use the original model in a fairer way and ways to train the\nmodel to achieve fairness and find more success with the former class of\napproaches. The challenges from this application point to important directions\nfor future work around fairness with collective entities rather than\nindividuals, the use of critical violations as a proxy, and the disconnect\nbetween fair classification and fairness in the dynamic scheduling system.",
          "link": "http://arxiv.org/abs/2108.05523",
          "publishedOn": "2021-08-13T01:56:55.895Z",
          "wordCount": 579,
          "title": "Fair Decision-Making for Food Inspections. (arXiv:2108.05523v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13913",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Carderera_A/0/1/0/all/0/1\">Alejandro Carderera</a>, <a href=\"http://arxiv.org/find/math/1/au:+Besancon_M/0/1/0/all/0/1\">Mathieu Besan&#xe7;on</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pokutta_S/0/1/0/all/0/1\">Sebastian Pokutta</a>",
          "description": "Generalized self-concordance is a key property present in the objective\nfunction of many important learning problems. We establish the convergence rate\nof a simple Frank-Wolfe variant that uses the open-loop step size strategy\n$\\gamma_t = 2/(t+2)$, obtaining a $\\mathcal{O}(1/t)$ convergence rate for this\nclass of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the\niteration count. This avoids the use of second-order information or the need to\nestimate local smoothness parameters of previous work. We also show improved\nconvergence rates for various common cases, e.g., when the feasible region\nunder consideration is uniformly convex or polyhedral.",
          "link": "http://arxiv.org/abs/2105.13913",
          "publishedOn": "2021-08-12T01:56:24.520Z",
          "wordCount": 571,
          "title": "Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions. (arXiv:2105.13913v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiabao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuemin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Libin Zheng</a>",
          "description": "With the rapid development of smart mobile devices, the car-hailing platforms\n(e.g., Uber or Lyft) have attracted much attention from both the academia and\nthe industry. In this paper, we consider an important dynamic car-hailing\nproblem, namely \\textit{maximum revenue vehicle dispatching} (MRVD), in which\nrider requests dynamically arrive and drivers need to serve as many riders as\npossible such that the entire revenue of the platform is maximized. We prove\nthat the MRVD problem is NP-hard and intractable. In addition, the dynamic\ncar-hailing platforms have no information of the future riders, which makes the\nproblem even harder. To handle the MRVD problem, we propose a queueing-based\nvehicle dispatching framework, which first uses existing machine learning\nalgorithms to predict the future vehicle demand of each region, then estimates\nthe idle time periods of drivers through a queueing model for each region. With\nthe information of the predicted vehicle demands and estimated idle time\nperiods of drivers, we propose two batch-based vehicle dispatching algorithms\nto efficiently assign suitable drivers to riders such that the expected overall\nrevenue of the platform is maximized during each batch processing. Through\nextensive experiments, we demonstrate the efficiency and effectiveness of our\nproposed approaches over both real and synthetic datasets.",
          "link": "http://arxiv.org/abs/2107.08662",
          "publishedOn": "2021-08-12T01:56:24.409Z",
          "wordCount": 686,
          "title": "A Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic Car-Hailing [technical report]. (arXiv:2107.08662v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects using\npoint cloud data. We present anchor design along with custom loss functions\nused in this work. A combination of spatial and channel wise attention module\nis used in this work. We use the Kitti 3D Birds Eye View dataset for\nbenchmarking and validating our results. Our method surpasses previous state of\nthe art in this domain both in terms of average precision and speed running at\n> 30 FPS. Finally, we present the ablation study to demonstrate that the\nperformance of our network is generalizable. This makes it a feasible option to\nbe deployed in real time applications like self driving cars.",
          "link": "http://arxiv.org/abs/2107.12137",
          "publishedOn": "2021-08-12T01:56:24.399Z",
          "wordCount": 633,
          "title": "AA3DNet: Attention Augmented Real Time 3D Object Detection. (arXiv:2107.12137v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06887",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_Q/0/1/0/all/0/1\">Qing Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koh_J/0/1/0/all/0/1\">Jae Chul Koh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_W/0/1/0/all/0/1\">WonSook Lee</a>",
          "description": "Synthetic X-ray images are simulated X-ray images projected from CT data.\nHigh-quality synthetic X-ray images can facilitate various applications such as\nsurgical image guidance systems and VR training simulations. However, it is\ndifficult to produce high-quality arbitrary view synthetic X-ray images in\nreal-time due to different CT slice thickness, high computational cost, and the\ncomplexity of algorithms. Our goal is to generate high-resolution synthetic\nX-ray images in real-time by upsampling low-resolution images with deep\nlearning-based super-resolution methods. Reference-based Super Resolution\n(RefSR) has been well studied in recent years and has shown higher performance\nthan traditional Single Image Super-Resolution (SISR). It can produce fine\ndetails by utilizing the reference image but still inevitably generates some\nartifacts and noise. In this paper, we introduce frequency domain loss as a\nconstraint to further improve the quality of the RefSR results with fine\ndetails and without obvious artifacts. To the best of our knowledge, this is\nthe first paper utilizing the frequency domain for the loss functions in the\nfield of super-resolution. We achieved good results in evaluating our method on\nboth synthetic and real X-ray image datasets.",
          "link": "http://arxiv.org/abs/2105.06887",
          "publishedOn": "2021-08-12T01:56:24.393Z",
          "wordCount": 656,
          "title": "A Frequency Domain Constraint for Synthetic and Real X-ray Image Super Resolution. (arXiv:2105.06887v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Joo Hun Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">Ha Min Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Hyejun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1\">Eun-Hye Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Ah Young Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Young Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hong Jin Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tai-Myoung Chung</a>",
          "description": "While machine learning techniques are being applied to various fields for\ntheir exceptional ability to find complex relations in large datasets, the\nstrengthening of regulations on data ownership and privacy is causing\nincreasing difficulty in its application to medical data. In light of this,\nFederated Learning has recently been proposed as a solution to train on private\ndata without breach of confidentiality. This conservation of privacy is\nparticularly appealing in the field of healthcare, where patient data is highly\nconfidential. However, many studies have shown that its assumption of\nIndependent and Identically Distributed data is unrealistic for medical data.\nIn this paper, we propose Personalized Federated Cluster Models, a hierarchical\nclustering-based FL process, to predict Major Depressive Disorder severity from\nHeart Rate Variability. By allowing clients to receive more personalized model,\nwe address problems caused by non-IID data, showing an accuracy increase in\nseverity prediction. This increase in performance may be sufficient to use\nPersonalized Federated Cluster Models in many existing Federated Learning\nscenarios.",
          "link": "http://arxiv.org/abs/2108.01903",
          "publishedOn": "2021-08-12T01:56:24.386Z",
          "wordCount": 654,
          "title": "Personalized Federated Learning with Clustering: Non-IID Heart Rate Variability Data Application. (arXiv:2108.01903v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>",
          "description": "Understanding the structure of loss landscape of deep neural networks\n(DNNs)is obviously important. In this work, we prove an embedding principle\nthat the loss landscape of a DNN \"contains\" all the critical points of all the\nnarrower DNNs. More precisely, we propose a critical embedding such that any\ncritical point, e.g., local or global minima, of a narrower DNN can be embedded\nto a critical point/hyperplane of the target DNN with higher degeneracy and\npreserving the DNN output function. The embedding structure of critical points\nis independent of loss function and training data, showing a stark difference\nfrom other nonconvex problems such as protein-folding. Empirically, we find\nthat a wide DNN is often attracted by highly-degenerate critical points that\nare embedded from narrow DNNs. The embedding principle provides an explanation\nfor the general easy optimization of wide DNNs and unravels a potential\nimplicit low-complexity regularization during the training. Overall, our work\nprovides a skeleton for the study of loss landscape of DNNs and its\nimplication, by which a more exact and comprehensive understanding can be\nanticipated in the near",
          "link": "http://arxiv.org/abs/2105.14573",
          "publishedOn": "2021-08-12T01:56:24.372Z",
          "wordCount": 646,
          "title": "Embedding Principle of Loss Landscape of Deep Neural Networks. (arXiv:2105.14573v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungsoo Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Abstract reasoning, i.e., inferring complicated patterns from given\nobservations, is a central building block of artificial general intelligence.\nWhile humans find the answer by either eliminating wrong candidates or first\nconstructing the answer, prior deep neural network (DNN)-based methods focus on\nthe former discriminative approach. This paper aims to design a framework for\nthe latter approach and bridge the gap between artificial and human\nintelligence. To this end, we propose logic-guided generation (LoGe), a novel\ngenerative DNN framework that reduces abstract reasoning as an optimization\nproblem in propositional logic. LoGe is composed of three steps: extract\npropositional variables from images, reason the answer variables with a logic\nlayer, and reconstruct the answer image from the variables. We demonstrate that\nLoGe outperforms the black box DNN frameworks for generative abstract reasoning\nunder the RAVEN benchmark, i.e., reconstructing answers based on capturing\ncorrect rules of various attributes from observations.",
          "link": "http://arxiv.org/abs/2107.10493",
          "publishedOn": "2021-08-12T01:56:24.344Z",
          "wordCount": 629,
          "title": "Abstract Reasoning via Logic-guided Generation. (arXiv:2107.10493v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peilun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1\">Fan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>",
          "description": "Email threat is a serious issue for enterprise security, which consists of\nvarious malicious scenarios, such as phishing, fraud, blackmail and\nmalvertisement. Traditional anti-spam gateway commonly requires to maintain a\ngreylist to filter out unexpected emails based on suspicious vocabularies\nexisted in the mail subject and content. However, the signature-based approach\ncannot effectively discover novel and unknown suspicious emails that utilize\nvarious hot topics at present, such as COVID-19 and US election. To address the\nproblem, in this paper, we present Holmes, an efficient and lightweight\nsemantic based engine for anomalous email detection. Holmes can convert each\nevent log of email to a sentence through word embedding then extract\ninteresting items among them by novelty detection. Based on our observations,\nwe claim that, in an enterprise environment, there is a stable relation between\nsenders and receivers, but suspicious emails are commonly from unusual sources,\nwhich can be detected through the rareness selection. We evaluate the\nperformance of Holmes in a real-world enterprise environment, in which it sends\nand receives around 5,000 emails each day. As a result, Holmes can achieve a\nhigh detection rate (output around 200 suspicious emails per day) and maintain\na low false alarm rate for anomaly detection.",
          "link": "http://arxiv.org/abs/2104.08044",
          "publishedOn": "2021-08-12T01:56:24.336Z",
          "wordCount": 754,
          "title": "Holmes: An Efficient and Lightweight Semantic Based Anomalous Email Detector. (arXiv:2104.08044v7 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saengkyongam_S/0/1/0/all/0/1\">Sorawit Saengkyongam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thams_N/0/1/0/all/0/1\">Nikolaj Thams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jonas Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_N/0/1/0/all/0/1\">Niklas Pfister</a>",
          "description": "In the past decade, contextual bandit and reinforcement learning algorithms\nhave been successfully used in various interactive learning systems such as\nonline advertising, recommender systems, and dynamic pricing. However, they\nhave yet to be widely adopted in high-stakes application domains, such as\nhealthcare. One reason may be that existing approaches assume that the\nunderlying mechanisms are static in the sense that they do not change over\ndifferent environments. In many real world systems, however, the mechanisms are\nsubject to shifts across environments which may invalidate the static\nenvironment assumption. In this paper, we tackle the problem of environmental\nshifts under the framework of offline contextual bandits. We view the\nenvironmental shift problem through the lens of causality and propose\nmulti-environment contextual bandits that allow for changes in the underlying\nmechanisms. We adopt the concept of invariance from the causality literature\nand introduce the notion of policy invariance. We argue that policy invariance\nis only relevant if unobserved confounders are present and show that, in that\ncase, an optimal invariant policy is guaranteed to generalize across\nenvironments under suitable assumptions. Our results may be a first step\ntowards solving the environmental shift problem. They also establish concrete\nconnections among causality, invariance and contextual bandits.",
          "link": "http://arxiv.org/abs/2106.00808",
          "publishedOn": "2021-08-12T01:56:24.331Z",
          "wordCount": 675,
          "title": "Invariant Policy Learning: A Causal Perspective. (arXiv:2106.00808v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasy_B/0/1/0/all/0/1\">Brittany Terese Fasy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenk_C/0/1/0/all/0/1\">Carola Wenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summa_B/0/1/0/all/0/1\">Brian Summa</a>",
          "description": "Persistence diagrams have been widely used to quantify the underlying\nfeatures of filtered topological spaces in data visualization. In many\napplications, computing distances between diagrams is essential; however,\ncomputing these distances has been challenging due to the computational cost.\nIn this paper, we propose a persistence diagram hashing framework that learns a\nbinary code representation of persistence diagrams, which allows for fast\ncomputation of distances. This framework is built upon a generative adversarial\nnetwork (GAN) with a diagram distance loss function to steer the learning\nprocess. Instead of using standard representations, we hash diagrams into\nbinary codes, which have natural advantages in large-scale tasks. The training\nof this model is domain-oblivious in that it can be computed purely from\nsynthetic, randomly created diagrams. As a consequence, our proposed method is\ndirectly applicable to various datasets without the need for retraining the\nmodel. These binary codes, when compared using fast Hamming distance, better\nmaintain topological similarity properties between datasets than other\nvectorized representations. To evaluate this method, we apply our framework to\nthe problem of diagram clustering and we compare the quality and performance of\nour approach to the state-of-the-art. In addition, we show the scalability of\nour approach on a dataset with 10k persistence diagrams, which is not possible\nwith current techniques. Moreover, our experimental results demonstrate that\nour method is significantly faster with the potential of less memory usage,\nwhile retaining comparable or better quality comparisons.",
          "link": "http://arxiv.org/abs/2105.12208",
          "publishedOn": "2021-08-12T01:56:24.316Z",
          "wordCount": 721,
          "title": "A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces for Clustering. (arXiv:2105.12208v2 [cs.CG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yuanyi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jian Peng</a>",
          "description": "The control variates (CV) method is widely used in policy gradient estimation\nto reduce the variance of the gradient estimators in practice. A control\nvariate is applied by subtracting a baseline function from the state-action\nvalue estimates. Then the variance-reduced policy gradient presumably leads to\nhigher learning efficiency. Recent research on control variates with deep\nneural net policies mainly focuses on scalar-valued baseline functions. The\neffect of vector-valued baselines is under-explored. This paper investigates\nvariance reduction with coordinate-wise and layer-wise control variates\nconstructed from vector-valued baselines for neural net policies. We present\nexperimental evidence suggesting that lower variance can be obtained with such\nbaselines than with the conventional scalar-valued baseline. We demonstrate how\nto equip the popular Proximal Policy Optimization (PPO) algorithm with these\nnew control variates. We show that the resulting algorithm with proper\nregularization can achieve higher sample efficiency than scalar control\nvariates in continuous control benchmarks.",
          "link": "http://arxiv.org/abs/2107.04987",
          "publishedOn": "2021-08-12T01:56:24.278Z",
          "wordCount": 613,
          "title": "Coordinate-wise Control Variates for Deep Policy Gradients. (arXiv:2107.04987v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13152",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1\">Cencheng Shen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Huang_N/0/1/0/all/0/1\">Ningyuan Huang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>",
          "description": "Neural networks have achieved remarkable successes in machine learning tasks.\nThis has recently been extended to graph learning using neural networks.\nHowever, there is limited theoretical work in understanding how and when they\nperform well, especially relative to established statistical learning\ntechniques such as spectral embedding. In this short paper, we present a simple\ngenerative model where unsupervised graph convolutional network fails, while\nthe adjacency spectral embedding succeeds. Specifically, unsupervised graph\nconvolutional network is unable to look beyond the first eigenvector in certain\napproximately regular graphs, thus missing inference signals in non-leading\neigenvectors. The phenomenon is demonstrated by visual illustrations and\ncomprehensive simulations.",
          "link": "http://arxiv.org/abs/2010.13152",
          "publishedOn": "2021-08-12T01:56:24.267Z",
          "wordCount": 566,
          "title": "A Simple Spectral Failure Mode for Graph Convolutional Networks. (arXiv:2010.13152v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Dhruv Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1\">Benjamin Eysenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1\">Nicholas Rhinehart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "We describe a robotic learning system for autonomous exploration and\nnavigation in diverse, open-world environments. At the core of our method is a\nlearned latent variable model of distances and actions, along with a\nnon-parametric topological memory. We use an information bottleneck to\nregularize the learned policy, giving us (i) a compact visual representation of\ngoals, (ii) improved generalization capabilities, and (iii) a mechanism for\nsampling feasible goals for exploration. Trained on a large offline dataset of\nprior experience, the model acquires a representation of visual goals that is\nrobust to task-irrelevant distractors. We demonstrate our method on a mobile\nground robot in open-world exploration scenarios. Given an image of a goal that\nis up to 80 meters away, our method leverages its representation to explore and\ndiscover the goal in under 20 minutes, even amidst previously-unseen obstacles\nand weather conditions. We encourage the reader to visit the project website\nfor videos of our experiments and demonstrations\nhttps://sites.google.com/view/recon-robot",
          "link": "http://arxiv.org/abs/2104.05859",
          "publishedOn": "2021-08-12T01:56:24.261Z",
          "wordCount": 635,
          "title": "Rapid Exploration for Open-World Navigation with Latent Goal Models. (arXiv:2104.05859v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.14018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhuoyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1\">Rui Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee-Keong Kwoh</a>",
          "description": "Graph-based recommendation models work well for top-N recommender systems due\nto their capability to capture the potential relationships between entities.\nHowever, most of the existing methods only construct a single global item graph\nshared by all the users and regrettably ignore the diverse tastes between\ndifferent user groups. Inspired by the success of local models for\nrecommendation, this paper provides the first attempt to investigate multiple\nlocal item graphs along with a global item graph for graph-based recommendation\nmodels. We argue that recommendation on global and local graphs outperforms\nthat on a single global graph or multiple local graphs. Specifically, we\npropose a novel graph-based recommendation model named GLIMG (Global and Local\nIteM Graphs), which simultaneously captures both the global and local user\ntastes. By integrating the global and local graphs into an adapted\nsemi-supervised learning model, users' preferences on items are propagated\nglobally and locally. Extensive experimental results on real-world datasets\nshow that our proposed method consistently outperforms the state-of-the art\ncounterparts on the top-N recommendation task.",
          "link": "http://arxiv.org/abs/2007.14018",
          "publishedOn": "2021-08-12T01:56:24.228Z",
          "wordCount": 647,
          "title": "GLIMG: Global and Local Item Graphs for Top-N Recommender Systems. (arXiv:2007.14018v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>",
          "description": "Medical images, especially volumetric images, are of high resolution and\noften exceed the capacity of standard desktop GPUs. As a result, most deep\nlearning-based medical image analysis tasks require the input images to be\ndownsampled, often substantially, before these can be fed to a neural network.\nHowever, downsampling can lead to a loss of image quality, which is undesirable\nespecially in reconstruction tasks, where the fine geometric details need to be\npreserved. In this paper, we propose that high-resolution images can be\nreconstructed in a coarse-to-fine fashion, where a deep learning algorithm is\nonly responsible for generating a coarse representation of the image, which\nconsumes moderate GPU memory. For producing the high-resolution outcome, we\npropose two novel methods: learned voxel rearrangement of the coarse output and\nhierarchical image synthesis. Compared to the coarse output, the\nhigh-resolution counterpart allows for smooth surface triangulation, which can\nbe 3D-printed in the highest possible quality. Experiments of this paper are\ncarried out on the dataset of AutoImplant 2021\n(https://autoimplant2021.grand-challenge.org/), a MICCAI challenge on cranial\nimplant design. The dataset contains high-resolution skulls that can be viewed\nas 2D manifolds embedded in a 3D space. Codes associated with this study can be\naccessed at https://github.com/Jianningli/voxel_rearrangement.",
          "link": "http://arxiv.org/abs/2108.05269",
          "publishedOn": "2021-08-12T01:56:24.184Z",
          "wordCount": 670,
          "title": "Learning to Rearrange Voxels in Binary Segmentation Masks for Smooth Manifold Triangulation. (arXiv:2108.05269v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy Le Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_D/0/1/0/all/0/1\">Dominique Chu</a>",
          "description": "We introduce a new supervised learning algorithm based to train spiking\nneural networks for classification. The algorithm overcomes a limitation of\nexisting multi-spike learning methods: it solves the problem of interference\nbetween interacting output spikes during a learning trial. This problem of\nlearning interference causes learning performance in existing approaches to\ndecrease as the number of output spikes increases, and represents an important\nlimitation in existing multi-spike learning approaches. We address learning\ninterference by introducing a novel mechanism to balance the magnitudes of\nweight adjustments during learning, which in theory allows every spike to\nsimultaneously converge to their desired timings. Our results indicate that our\nmethod achieves significantly higher memory capacity and faster convergence\ncompared to existing approaches for multi-spike classification. In the\nubiquitous Iris and MNIST datasets, our algorithm achieves competitive\npredictive performance with state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2103.12564",
          "publishedOn": "2021-08-12T01:56:24.169Z",
          "wordCount": 601,
          "title": "Linear Constraints Learning for Spiking Neurons. (arXiv:2103.12564v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12141",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Grigoryeva_L/0/1/0/all/0/1\">Lyudmila Grigoryeva</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ortega_J/0/1/0/all/0/1\">Juan-Pablo Ortega</a>",
          "description": "Many recurrent neural network machine learning paradigms can be formulated\nusing state-space representations. The classical notion of canonical\nstate-space realization is adapted in this paper to accommodate semi-infinite\ninputs so that it can be used as a dimension reduction tool in the recurrent\nnetworks setup. The so-called input forgetting property is identified as the\nkey hypothesis that guarantees the existence and uniqueness (up to system\nisomorphisms) of canonical realizations for causal and time-invariant\ninput/output systems with semi-infinite inputs. Additionally, the notion of\noptimal reduction coming from the theory of symmetric Hamiltonian systems is\nimplemented in our setup to construct canonical realizations out of input\nforgetting but not necessarily canonical ones. These two procedures are studied\nin detail in the framework of linear fading memory input/output systems.\nFinally, the notion of implicit reduction using reproducing kernel Hilbert\nspaces (RKHS) is introduced which allows, for systems with linear readouts, to\nachieve dimension reduction without the need to actually compute the reduced\nspaces introduced in the first part of the paper.",
          "link": "http://arxiv.org/abs/2007.12141",
          "publishedOn": "2021-08-12T01:56:24.163Z",
          "wordCount": 629,
          "title": "Dimension reduction in recurrent networks by canonicalization. (arXiv:2007.12141v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heyman_G/0/1/0/all/0/1\">Geert Heyman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huysegems_R/0/1/0/all/0/1\">Rafael Huysegems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justen_P/0/1/0/all/0/1\">Pascal Justen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutsem_T/0/1/0/all/0/1\">Tom Van Cutsem</a>",
          "description": "In today's software world with its cornucopia of reusable software libraries,\nwhen a programmer is faced with a programming task that they suspect can be\ncompleted through the use of a library, they often look for code examples using\na search engine and then manually adapt found examples to their specific\ncontext of use. We put forward a vision based on a new breed of developer tools\nthat have the potential to largely automate this process. The key idea is to\nadapt code autocompletion tools such that they take into account not only the\ndeveloper's already-written code but also the intent of the task the developer\nis trying to achieve next, formulated in plain natural language. We call this\npractice of enriching the code with natural language intent to facilitate its\ncompletion natural language-guided programming.\n\nTo show that this idea is feasible we design, implement and benchmark a tool\nthat solves this problem in the context of a specific domain (data science) and\na specific programming language (Python). Central to the tool is the use of\nlanguage models trained on a large corpus of documented code. Our initial\nexperiments confirm the feasibility of the idea but also make it clear that we\nhave only scratched the surface of what may become possible in the future. We\nend the paper with a comprehensive research agenda to stimulate additional\nresearch in the budding area of natural language-guided programming.",
          "link": "http://arxiv.org/abs/2108.05198",
          "publishedOn": "2021-08-12T01:56:24.096Z",
          "wordCount": 661,
          "title": "Natural Language-guided Programming. (arXiv:2108.05198v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammad Masudur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeasmin_S/0/1/0/all/0/1\">Shamima Yeasmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_C/0/1/0/all/0/1\">Chanchal K. Roy</a>",
          "description": "Being light-weight and cost-effective, IR-based approaches for bug\nlocalization have shown promise in finding software bugs. However, the accuracy\nof these approaches heavily depends on their used bug reports. A significant\nnumber of bug reports contain only plain natural language texts. According to\nexisting studies, IR-based approaches cannot perform well when they use these\nbug reports as search queries. On the other hand, there is a piece of recent\nevidence that suggests that even these natural language-only reports contain\nenough good keywords that could help localize the bugs successfully. On one\nhand, these findings suggest that natural language-only bug reports might be a\nsufficient source for good query keywords. On the other hand, they cast serious\ndoubt on the query selection practices in the IR-based bug localization. In\nthis article, we attempted to clear the sky on this aspect by conducting an\nin-depth empirical study that critically examines the state-of-the-art query\nselection practices in IR-based bug localization. In particular, we use a\ndataset of 2,320 bug reports, employ ten existing approaches from the\nliterature, exploit the Genetic Algorithm-based approach to construct optimal,\nnear-optimal search queries from these bug reports, and then answer three\nresearch questions. We confirmed that the state-of-the-art query construction\napproaches are indeed not sufficient for constructing appropriate queries (for\nbug localization) from certain natural language-only bug reports although they\ncontain such queries. We also demonstrate that optimal queries and non-optimal\nqueries chosen from bug report texts are significantly different in terms of\nseveral keyword characteristics, which has led us to actionable insights.\nFurthermore, we demonstrate 27%--34% improvement in the performance of\nnon-optimal queries through the application of our actionable insights to them.",
          "link": "http://arxiv.org/abs/2108.05341",
          "publishedOn": "2021-08-12T01:56:24.083Z",
          "wordCount": 766,
          "title": "The Forgotten Role of Search Queries in IR-based Bug Localization: An Empirical Study. (arXiv:2108.05341v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1\">Awni Hannun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1\">Laurens van der Maaten</a>",
          "description": "Machine-learning models contain information about the data they were trained\non. This information leaks either through the model itself or through\npredictions made by the model. Consequently, when the training data contains\nsensitive attributes, assessing the amount of information leakage is paramount.\nWe propose a method to quantify this leakage using the Fisher information of\nthe model about the data. Unlike the worst-case a priori guarantees of\ndifferential privacy, Fisher information loss measures leakage with respect to\nspecific examples, attributes, or sub-populations within the dataset. We\nmotivate Fisher information loss through the Cram\\'{e}r-Rao bound and delineate\nthe implied threat model. We provide efficient methods to compute Fisher\ninformation loss for output-perturbed generalized linear models. Finally, we\nempirically validate Fisher information loss as a useful measure of information\nleakage.",
          "link": "http://arxiv.org/abs/2102.11673",
          "publishedOn": "2021-08-12T01:56:23.985Z",
          "wordCount": 591,
          "title": "Measuring Data Leakage in Machine-Learning Models with Fisher Information. (arXiv:2102.11673v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>",
          "description": "Modeling tap or click sequences of users on a mobile device can improve our\nunderstandings of interaction behavior and offers opportunities for UI\noptimization by recommending next element the user might want to click on. We\nanalyzed a large-scale dataset of over 20 million clicks from more than 4,000\nmobile users who opted in. We then designed a deep learning model that predicts\nthe next element that the user clicks given the user's click history, the\nstructural information of the UI screen, and the current context such as the\ntime of the day. We thoroughly investigated the deep model by comparing it with\na set of baseline methods based on the dataset. The experiments show that our\nmodel achieves 48% and 71% accuracy (top-1 and top-3) for predicting next\nclicks based on a held-out dataset of test users, which significantly\noutperformed all the baseline methods with a large margin. We discussed a few\nscenarios for integrating the model in mobile interaction and how users can\npotentially benefit from the model.",
          "link": "http://arxiv.org/abs/2108.05342",
          "publishedOn": "2021-08-12T01:56:23.978Z",
          "wordCount": 612,
          "title": "Large-Scale Modeling of Mobile User Click Behaviors Using Deep Learning. (arXiv:2108.05342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05350",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shao_S/0/1/0/all/0/1\">Simeng Shao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bien_J/0/1/0/all/0/1\">Jacob Bien</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Javanmard_A/0/1/0/all/0/1\">Adel Javanmard</a>",
          "description": "In many domains, data measurements can naturally be associated with the\nleaves of a tree, expressing the relationships among these measurements. For\nexample, companies belong to industries, which in turn belong to ever coarser\ndivisions such as sectors; microbes are commonly arranged in a taxonomic\nhierarchy from species to kingdoms; street blocks belong to neighborhoods,\nwhich in turn belong to larger-scale regions. The problem of tree-based\naggregation that we consider in this paper asks which of these tree-defined\nsubgroups of leaves should really be treated as a single entity and which of\nthese entities should be distinguished from each other.\n\nWe introduce the \"false split rate\", an error measure that describes the\ndegree to which subgroups have been split when they should not have been. We\nthen propose a multiple hypothesis testing algorithm for tree-based\naggregation, which we prove controls this error measure. We focus on two main\nexamples of tree-based aggregation, one which involves aggregating means and\nthe other which involves aggregating regression coefficients. We apply this\nmethodology to aggregate stocks based on their volatility and to aggregate\nneighborhoods of New York City based on taxi fares.",
          "link": "http://arxiv.org/abs/2108.05350",
          "publishedOn": "2021-08-12T01:56:23.972Z",
          "wordCount": 630,
          "title": "Controlling the False Split Rate in Tree-Based Aggregation. (arXiv:2108.05350v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05237",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Trunschke_P/0/1/0/all/0/1\">Philipp Trunschke</a>",
          "description": "We consider the problem of approximating a function in general nonlinear\nsubsets of $L^2$ when only a weighted Monte Carlo estimate of the $L^2$-norm\ncan be computed. Of particular interest in this setting is the concept of\nsample complexity, the number of samples that are necessary to recover the best\napproximation. Bounds for this quantity have been derived in a previous work\nand depend primarily on the model class and are not influenced positively by\nthe regularity of the sought function. This result however is only a worst-case\nbound and is not able to explain the remarkable performance of iterative hard\nthresholding algorithms that is observed in practice. We reexamine the results\nof the previous paper and derive a new bound that is able to utilize the\nregularity of the sought function. A critical analysis of our results allows us\nto derive a sample efficient algorithm for the model set of low-rank tensors.\nThe viability of this algorithm is demonstrated by recovering quantities of\ninterest for a classical high-dimensional random partial differential equation.",
          "link": "http://arxiv.org/abs/2108.05237",
          "publishedOn": "2021-08-12T01:56:23.967Z",
          "wordCount": 625,
          "title": "Convergence bounds for nonlinear least squares and applications to tensor recovery. (arXiv:2108.05237v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kyathanahally_S/0/1/0/all/0/1\">S. P. Kyathanahally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardeman_T/0/1/0/all/0/1\">T. Hardeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merz_E/0/1/0/all/0/1\">E. Merz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozakiewicz_T/0/1/0/all/0/1\">T. Kozakiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1\">M. Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isles_P/0/1/0/all/0/1\">P. Isles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomati_F/0/1/0/all/0/1\">F. Pomati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baity_Jesi_M/0/1/0/all/0/1\">M. Baity-Jesi</a>",
          "description": "Plankton are effective indicators of environmental change and ecosystem\nhealth in freshwater habitats, but collection of plankton data using manual\nmicroscopic methods is extremely labor-intensive and expensive. Automated\nplankton imaging offers a promising way forward to monitor plankton communities\nwith high frequency and accuracy in real-time. Yet, manual annotation of\nmillions of images proposes a serious challenge to taxonomists. Deep learning\nclassifiers have been successfully applied in various fields and provided\nencouraging results when used to categorize marine plankton images. Here, we\npresent a set of deep learning models developed for the identification of lake\nplankton, and study several strategies to obtain optimal performances,which\nlead to operational prescriptions for users. To this aim, we annotated into 35\nclasses over 17900 images of zooplankton and large phytoplankton colonies,\ndetected in Lake Greifensee (Switzerland) with the Dual Scripps Plankton\nCamera. Our best models were based on transfer learning and ensembling, which\nclassified plankton images with 98% accuracy and 93% F1 score. When tested on\nfreely available plankton datasets produced by other automated imaging tools\n(ZooScan, FlowCytobot and ISIIS), our models performed better than previously\nused models. Our annotated data, code and classification models are freely\navailable online.",
          "link": "http://arxiv.org/abs/2108.05258",
          "publishedOn": "2021-08-12T01:56:23.953Z",
          "wordCount": 656,
          "title": "Deep Learning Classification of Lake Zooplankton. (arXiv:2108.05258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trevithick_A/0/1/0/all/0/1\">Alex Trevithick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>",
          "description": "We present a simple yet powerful neural network that implicitly represents\nand renders 3D objects and scenes only from 2D observations. The network models\n3D geometries as a general radiance field, which takes a set of 2D images with\ncamera poses and intrinsics as input, constructs an internal representation for\neach point of the 3D space, and then renders the corresponding appearance and\ngeometry of that point viewed from an arbitrary position. The key to our\napproach is to learn local features for each pixel in 2D images and to then\nproject these features to 3D points, thus yielding general and rich point\nrepresentations. We additionally integrate an attention mechanism to aggregate\npixel features from multiple 2D views, such that visual occlusions are\nimplicitly taken into account. Extensive experiments demonstrate that our\nmethod can generate high-quality and realistic novel views for novel objects,\nunseen categories and challenging real-world scenes.",
          "link": "http://arxiv.org/abs/2010.04595",
          "publishedOn": "2021-08-12T01:56:23.937Z",
          "wordCount": 648,
          "title": "GRF: Learning a General Radiance Field for 3D Representation and Rendering. (arXiv:2010.04595v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Attention mechanism of late has been quite popular in the computer vision\ncommunity. A lot of work has been done to improve the performance of the\nnetwork, although almost always it results in increased computational\ncomplexity. In this paper, we propose a new attention module that not only\nachieves the best performance but also has lesser parameters compared to most\nexisting models. Our attention module can easily be integrated with other\nconvolutional neural networks because of its lightweight nature. The proposed\nnetwork named Dual Multi Scale Attention Network (DMSANet) is comprised of two\nparts: the first part is used to extract features at various scales and\naggregate them, the second part uses spatial and channel attention modules in\nparallel to adaptively integrate local features with their global dependencies.\nWe benchmark our network performance for Image Classification on ImageNet\ndataset, Object Detection and Instance Segmentation both on MS COCO dataset.",
          "link": "http://arxiv.org/abs/2106.08382",
          "publishedOn": "2021-08-12T01:56:23.931Z",
          "wordCount": 613,
          "title": "DMSANet: Dual Multi Scale Attention Network. (arXiv:2106.08382v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05184",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Brownlees_C/0/1/0/all/0/1\">Christian Brownlees</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Llorens_Terrazas_J/0/1/0/all/0/1\">Jordi Llorens-Terrazas</a>",
          "description": "Empirical risk minimization is a standard principle for choosing algorithms\nin learning theory. In this paper we study the properties of empirical risk\nminimization for time series. The analysis is carried out in a general\nframework that covers different types of forecasting applications encountered\nin the literature. We are concerned with 1-step-ahead prediction of a\nunivariate time series generated by a parameter-driven process. A class of\nrecursive algorithms is available to forecast the time series. The algorithms\nare recursive in the sense that the forecast produced in a given period is a\nfunction of the lagged values of the forecast and of the time series. The\nrelationship between the generating mechanism of the time series and the class\nof algorithms is unspecified. Our main result establishes that the algorithm\nchosen by empirical risk minimization achieves asymptotically the optimal\npredictive performance that is attainable within the class of algorithms.",
          "link": "http://arxiv.org/abs/2108.05184",
          "publishedOn": "2021-08-12T01:56:23.916Z",
          "wordCount": 584,
          "title": "Empirical Risk Minimization for Time Series: Nonparametric Performance Bounds for Prediction. (arXiv:2108.05184v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yushun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalaian_B/0/1/0/all/0/1\">Brian Jalaian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jundong Li</a>",
          "description": "Graph Neural Networks (GNNs) have recently demonstrated superior capability\nof tackling graph analytical problems in various applications. Nevertheless,\nwith the wide-spreading practice of GNNs in high-stake decision-making\nprocesses, there is an increasing societal concern that GNNs could make\ndiscriminatory decisions that may be illegal towards certain demographic\ngroups. Although some explorations have been made towards developing fair GNNs,\nexisting approaches are tailored for a specific GNN model. However, in\npractical scenarios, myriads of GNN variants have been proposed for different\ntasks, and it is costly to train and fine-tune existing debiasing models for\ndifferent GNNs. Also, bias in a trained model could originate from training\ndata, while how to mitigate bias in the graph data is usually overlooked. In\nthis work, different from existing work, we first propose novel definitions and\nmetrics to measure the bias in an attributed network, which leads to the\noptimization objective to mitigate bias. Based on the optimization objective,\nwe develop a framework named EDITS to mitigate the bias in attributed networks\nwhile preserving useful information. EDITS works in a model-agnostic manner,\nwhich means that it is independent of the specific GNNs applied for downstream\ntasks. Extensive experiments on both synthetic and real-world datasets\ndemonstrate the validity of the proposed bias metrics and the superiority of\nEDITS on both bias mitigation and utility maintenance. Open-source\nimplementation: https://github.com/yushundong/EDITS.",
          "link": "http://arxiv.org/abs/2108.05233",
          "publishedOn": "2021-08-12T01:56:23.911Z",
          "wordCount": 658,
          "title": "EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks. (arXiv:2108.05233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Changbo Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingqing Zhou</a>",
          "description": "Cardinality estimation is a fundamental but long unresolved problem in query\noptimization. Recently, multiple papers from different research groups\nconsistently report that learned models have the potential to replace existing\ncardinality estimators. In this paper, we ask a forward-thinking question: Are\nwe ready to deploy these learned cardinality models in production? Our study\nconsists of three main parts. Firstly, we focus on the static environment\n(i.e., no data updates) and compare five new learned methods with eight\ntraditional methods on four real-world datasets under a unified workload\nsetting. The results show that learned models are indeed more accurate than\ntraditional methods, but they often suffer from high training and inference\ncosts. Secondly, we explore whether these learned models are ready for dynamic\nenvironments (i.e., frequent data updates). We find that they cannot catch up\nwith fast data up-dates and return large errors for different reasons. For less\nfrequent updates, they can perform better but there is no clear winner among\nthemselves. Thirdly, we take a deeper look into learned models and explore when\nthey may go wrong. Our results show that the performance of learned methods can\nbe greatly affected by the changes in correlation, skewness, or domain size.\nMore importantly, their behaviors are much harder to interpret and often\nunpredictable. Based on these findings, we identify two promising research\ndirections (control the cost of learned models and make learned models\ntrustworthy) and suggest a number of research opportunities. We hope that our\nstudy can guide researchers and practitioners to work together to eventually\npush learned cardinality estimators into real database systems.",
          "link": "http://arxiv.org/abs/2012.06743",
          "publishedOn": "2021-08-12T01:56:23.905Z",
          "wordCount": 752,
          "title": "Are We Ready For Learned Cardinality Estimation?. (arXiv:2012.06743v4 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04554",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jagalur_Mohan_J/0/1/0/all/0/1\">Jayanth Jagalur-Mohan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Marzouk_Y/0/1/0/all/0/1\">Youssef Marzouk</a>",
          "description": "We propose and analyze batch greedy heuristics for cardinality constrained\nmaximization of non-submodular non-decreasing set functions. We consider the\nstandard greedy paradigm, along with its distributed greedy and stochastic\ngreedy variants. Our theoretical guarantees are characterized by the\ncombination of submodularity and supermodularity ratios. We argue how these\nparameters define tight modular bounds based on incremental gains, and provide\na novel reinterpretation of the classical greedy algorithm using the\nminorize-maximize (MM) principle. Based on that analogy, we propose a new class\nof methods exploiting any plausible modular bound. In the context of optimal\nexperimental design for linear Bayesian inverse problems, we bound the\nsubmodularity and supermodularity ratios when the underlying objective is based\non mutual information. We also develop novel modular bounds for the mutual\ninformation in this setting, and describe certain connections to polyhedral\ncombinatorics. We discuss how algorithms using these modular bounds relate to\nestablished statistical notions such as leverage scores and to more recent\nefforts such as volume sampling. We demonstrate our theoretical findings on\nsynthetic problems and on a real-world climate monitoring example.",
          "link": "http://arxiv.org/abs/2006.04554",
          "publishedOn": "2021-08-12T01:56:23.899Z",
          "wordCount": 655,
          "title": "Batch greedy maximization of non-submodular functions: Guarantees and applications to experimental design. (arXiv:2006.04554v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blandin_J/0/1/0/all/0/1\">Jack Blandin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kash_I/0/1/0/all/0/1\">Ian Kash</a>",
          "description": "Group fairness definitions such as Demographic Parity and Equal Opportunity\nmake assumptions about the underlying decision-problem that restrict them to\nclassification problems. Prior work has translated these definitions to other\nmachine learning environments, such as unsupervised learning and reinforcement\nlearning, by implementing their closest mathematical equivalent. As a result,\nthere are numerous bespoke interpretations of these definitions. Instead, we\nprovide a generalized set of group fairness definitions that unambiguously\nextend to all machine learning environments while still retaining their\noriginal fairness notions. We derive two fairness principles that enable such a\ngeneralized framework. First, our framework measures outcomes in terms of\nutilities, rather than predictions, and does so for both the decision-algorithm\nand the individual. Second, our framework considers counterfactual outcomes,\nrather than just observed outcomes, thus preventing loopholes where fairness\ncriteria are satisfied through self-fulfilling prophecies. We provide concrete\nexamples of how our counterfactual utility fairness framework resolves known\nfairness issues in classification, clustering, and reinforcement learning\nproblems. We also show that many of the bespoke interpretations of Demographic\nParity and Equal Opportunity fit nicely as special cases of our framework.",
          "link": "http://arxiv.org/abs/2108.05315",
          "publishedOn": "2021-08-12T01:56:23.894Z",
          "wordCount": 604,
          "title": "Fairness Through Counterfactual Utilities. (arXiv:2108.05315v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10458",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Aaronson_S/0/1/0/all/0/1\">Scott Aaronson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Grewal_S/0/1/0/all/0/1\">Sabee Grewal</a>",
          "description": "We give an efficient classical algorithm that recovers the distribution of a\nnon-interacting fermion state over the computational basis. For a system of $n$\nnon-interacting fermions and $m$ modes, we show that $O(m^2 n^4 \\log(m/\\delta)/\n\\varepsilon^4)$ samples and $O(m^4 n^4 \\log(m/\\delta)/ \\varepsilon^4)$ time are\nsufficient to learn the original distribution to total variation distance\n$\\varepsilon$ with probability $1 - \\delta$. Our algorithm empirically\nestimates the one- and two-mode correlations and uses them to reconstruct a\nsuccinct description of the entire distribution efficiently.",
          "link": "http://arxiv.org/abs/2102.10458",
          "publishedOn": "2021-08-12T01:56:23.889Z",
          "wordCount": 556,
          "title": "Efficient Learning of Non-Interacting Fermion Distributions. (arXiv:2102.10458v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinan Li</a>",
          "description": "We give a computationally-efficient PAC active learning algorithm for\n$d$-dimensional homogeneous halfspaces that can tolerate Massart noise (Massart\nand N\\'ed\\'elec, 2006) and Tsybakov noise (Tsybakov, 2004). Specialized to the\n$\\eta$-Massart noise setting, our algorithm achieves an\ninformation-theoretically near-optimal label complexity of $\\tilde{O}\\left(\n\\frac{d}{(1-2\\eta)^2} \\mathrm{polylog}(\\frac1\\epsilon) \\right)$ under a wide\nrange of unlabeled data distributions (specifically, the family of \"structured\ndistributions\" defined in Diakonikolas et al. (2020)). Under the more\nchallenging Tsybakov noise condition, we identify two subfamilies of noise\nconditions, under which our efficient algorithm provides label complexity\nguarantees strictly lower than passive learning algorithms.",
          "link": "http://arxiv.org/abs/2102.05312",
          "publishedOn": "2021-08-12T01:56:23.883Z",
          "wordCount": 566,
          "title": "Improved Algorithms for Efficient Active Learning Halfspaces with Massart and Tsybakov noise. (arXiv:2102.05312v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_B/0/1/0/all/0/1\">Boseung Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jicheol Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>",
          "description": "Attribute-based person search is the task of finding person images that are\nbest matched with a set of text attributes given as query. The main challenge\nof this task is the large modality gap between attributes and images. To reduce\nthe gap, we present a new loss for learning cross-modal embeddings in the\ncontext of attribute-based person search. We regard a set of attributes as a\ncategory of people sharing the same traits. In a joint embedding space of the\ntwo modalities, our loss pulls images close to their person categories for\nmodality alignment. More importantly, it pushes apart a pair of person\ncategories by a margin determined adaptively by their semantic distance, where\nthe distance metric is learned end-to-end so that the loss considers importance\nof each attribute when relating person categories. Our loss guided by the\nadaptive semantic margin leads to more discriminative and semantically\nwell-arranged distributions of person images. As a consequence, it enables a\nsimple embedding model to achieve state-of-the-art records on public benchmarks\nwithout bells and whistles.",
          "link": "http://arxiv.org/abs/2108.04533",
          "publishedOn": "2021-08-12T01:56:23.877Z",
          "wordCount": 621,
          "title": "ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer. (arXiv:2108.04533v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ackerman_S/0/1/0/all/0/1\">Samuel Ackerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dube_P/0/1/0/all/0/1\">Parijat Dube</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farchi_E/0/1/0/all/0/1\">Eitan Farchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raz_O/0/1/0/all/0/1\">Orna Raz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zalmanovici_M/0/1/0/all/0/1\">Marcel Zalmanovici</a>",
          "description": "Detecting drift in performance of Machine Learning (ML) models is an\nacknowledged challenge. For ML models to become an integral part of business\napplications it is essential to detect when an ML model drifts away from\nacceptable operation. However, it is often the case that actual labels are\ndifficult and expensive to get, for example, because they require expert\njudgment. Therefore, there is a need for methods that detect likely degradation\nin ML operation without labels. We propose a method that utilizes feature space\nrules, called data slices, for drift detection. We provide experimental\nindications that our method is likely to identify that the ML model will likely\nchange in performance, based on changes in the underlying data.",
          "link": "http://arxiv.org/abs/2108.05319",
          "publishedOn": "2021-08-12T01:56:23.864Z",
          "wordCount": 563,
          "title": "Machine Learning Model Drift Detection Via Weak Data Slices. (arXiv:2108.05319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weishen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Sen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>",
          "description": "Algorithmic fairness has aroused considerable interests in data mining and\nmachine learning communities recently. So far the existing research has been\nmostly focusing on the development of quantitative metrics to measure algorithm\ndisparities across different protected groups, and approaches for adjusting the\nalgorithm output to reduce such disparities. In this paper, we propose to study\nthe problem of identification of the source of model disparities. Unlike\nexisting interpretation methods which typically learn feature importance, we\nconsider the causal relationships among feature variables and propose a novel\nframework to decompose the disparity into the sum of contributions from\nfairness-aware causal paths, which are paths linking the sensitive attribute\nand the final predictions, on the graph. We also consider the scenario when the\ndirections on certain edges within those paths cannot be determined. Our\nframework is also model agnostic and applicable to a variety of quantitative\ndisparity measures. Empirical evaluations on both synthetic and real-world data\nsets are provided to show that our method can provide precise and comprehensive\nexplanations to the model disparities.",
          "link": "http://arxiv.org/abs/2108.05335",
          "publishedOn": "2021-08-12T01:56:23.849Z",
          "wordCount": 604,
          "title": "Explaining Algorithmic Fairness Through Fairness-Aware Causal Path Decomposition. (arXiv:2108.05335v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rigoni_D/0/1/0/all/0/1\">Davide Rigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serafini_L/0/1/0/all/0/1\">Luciano Serafini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1\">Alessandro Sperduti</a>",
          "description": "Given a textual phrase and an image, the visual grounding problem is defined\nas the task of locating the content of the image referenced by the sentence. It\nis a challenging task that has several real-world applications in\nhuman-computer interaction, image-text reference resolution, and video-text\nreference resolution. In the last years, several works have addressed this\nproblem with heavy and complex models that try to capture visual-textual\ndependencies better than before. These models are typically constituted by two\nmain components that focus on how to learn useful multi-modal features for\ngrounding and how to improve the predicted bounding box of the visual mention,\nrespectively. Finding the right learning balance between these two sub-tasks is\nnot easy, and the current models are not necessarily optimal with respect to\nthis issue. In this work, we propose a model that, although using a simple\nmulti-modal feature fusion component, is able to achieve a higher accuracy than\nstate-of-the-art models thanks to the adoption of a more effective loss\nfunction, based on the classes probabilities, that reach, in the considered\ndatasets, a better learning balance between the two sub-tasks mentioned above.",
          "link": "http://arxiv.org/abs/2108.05308",
          "publishedOn": "2021-08-12T01:56:23.829Z",
          "wordCount": 620,
          "title": "A Better Loss for Visual-Textual Grounding. (arXiv:2108.05308v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.01547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_R/0/1/0/all/0/1\">Robin M. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_F/0/1/0/all/0/1\">Frank Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1\">Philipp Hennig</a>",
          "description": "Choosing the optimizer is considered to be among the most crucial design\ndecisions in deep learning, and it is not an easy one. The growing literature\nnow lists hundreds of optimization methods. In the absence of clear theoretical\nguidance and conclusive empirical evidence, the decision is often made based on\nanecdotes. In this work, we aim to replace these anecdotes, if not with a\nconclusive ranking, then at least with evidence-backed heuristics. To do so, we\nperform an extensive, standardized benchmark of fifteen particularly popular\ndeep learning optimizers while giving a concise overview of the wide range of\npossible choices. Analyzing more than $50,000$ individual runs, we contribute\nthe following three points: (i) Optimizer performance varies greatly across\ntasks. (ii) We observe that evaluating multiple optimizers with default\nparameters works approximately as well as tuning the hyperparameters of a\nsingle, fixed optimizer. (iii) While we cannot discern an optimization method\nclearly dominating across all tested tasks, we identify a significantly reduced\nsubset of specific optimizers and parameter choices that generally lead to\ncompetitive results in our experiments: Adam remains a strong contender, with\nnewer methods failing to significantly and consistently outperform it. Our\nopen-sourced results are available as challenging and well-tuned baselines for\nmore meaningful evaluations of novel optimization methods without requiring any\nfurther computational efforts.",
          "link": "http://arxiv.org/abs/2007.01547",
          "publishedOn": "2021-08-12T01:56:23.796Z",
          "wordCount": 724,
          "title": "Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers. (arXiv:2007.01547v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Portisch_J/0/1/0/all/0/1\">Jan Portisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "The RDF2vec method for creating node embeddings on knowledge graphs is based\non word2vec, which, in turn, is agnostic towards the position of context words.\nIn this paper, we argue that this might be a shortcoming when training RDF2vec,\nand show that using a word2vec variant which respects order yields considerable\nperformance gains especially on tasks where entities of different classes are\ninvolved.",
          "link": "http://arxiv.org/abs/2108.05280",
          "publishedOn": "2021-08-12T01:56:23.775Z",
          "wordCount": 495,
          "title": "Putting RDF2vec in Order. (arXiv:2108.05280v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majarjan_D/0/1/0/all/0/1\">Drishti Majarjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaspel_P/0/1/0/all/0/1\">Peter Zaspel</a>",
          "description": "Recent progress in scientific visualization has expanded the scope of\nvisualization from being merely a way of presentation to an analysis and\ndiscovery tool. A given visualization result is usually generated by applying a\nseries of transformations or filters to the underlying data. Nowadays, such\nfilters use deterministic algorithms to process the data. In this work, we aim\nat extending this methodology towards data-driven filters, thus filters that\nexpose the abilities of pre-trained machine learning models to the\nvisualization system. The use of such data-driven filters is of particular\ninterest in fields like segmentation, classification, etc., where machine\nlearning models regularly outperform existing algorithmic approaches. To\nshowcase this idea, we couple Paraview, the well-known flow visualization tool,\nwith PyTorch, a deep learning framework. Paraview is extended by plugins that\nallow users to load pre-trained models of their choice in the form of newly\ndeveloped filters. The filters transform the input data by feeding it into the\nmodel and then provide the model's output as input to the remaining\nvisualization pipeline. A series of simplistic use cases for segmentation and\nclassification on image and fluid data is presented to showcase the technical\napplicability of such data-driven transformations in Paraview for future\ncomplex analysis tasks.",
          "link": "http://arxiv.org/abs/2108.05196",
          "publishedOn": "2021-08-12T01:56:23.770Z",
          "wordCount": 626,
          "title": "Towards data-driven filters in Paraview. (arXiv:2108.05196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangtong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>",
          "description": "Emphatic Temporal Difference (TD) methods are a class of off-policy\nReinforcement Learning (RL) methods involving the use of followon traces.\nDespite the theoretical success of emphatic TD methods in addressing the\nnotorious deadly triad (Sutton and Barto, 2018) of off-policy RL, there are\nstill three open problems. First, the motivation for emphatic TD methods\nproposed by Sutton et al. (2016) does not align with the convergence analysis\nof Yu (2015). Namely, a quantity used by Sutton et al. (2016) that is expected\nto be essential for the convergence of emphatic TD methods is not used in the\nactual convergence analysis of Yu (2015). Second, followon traces typically\nsuffer from large variance, making them hard to use in practice. Third, despite\nthe seminal work of Yu (2015) confirming the asymptotic convergence of some\nemphatic TD methods for prediction problems, there is still no finite sample\nanalysis for any emphatic TD method for prediction, much less control. In this\npaper, we address those three open problems simultaneously via using truncated\nfollowon traces in emphatic TD methods. Unlike the original followon traces,\nwhich depend on all previous history, truncated followon traces depend on only\nfinite history, reducing variance and enabling the finite sample analysis of\nour proposed emphatic TD methods for both prediction and control.",
          "link": "http://arxiv.org/abs/2108.05338",
          "publishedOn": "2021-08-12T01:56:23.726Z",
          "wordCount": 640,
          "title": "Truncated Emphatic Temporal Difference Methods for Prediction and Control. (arXiv:2108.05338v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04941",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Ning_B/0/1/0/all/0/1\">Brian Ning</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Jaimungal_S/0/1/0/all/0/1\">Sebastian Jaimungal</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaorong Zhang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bergeron_M/0/1/0/all/0/1\">Maxime Bergeron</a>",
          "description": "We propose a hybrid method for generating arbitrage-free implied volatility\n(IV) surfaces consistent with historical data by combining model-free\nVariational Autoencoders (VAEs) with continuous time stochastic differential\nequation (SDE) driven models. We focus on two classes of SDE models: regime\nswitching models and L\\'evy additive processes. By projecting historical\nsurfaces onto the space of SDE model parameters, we obtain a distribution on\nthe parameter subspace faithful to the data on which we then train a VAE.\nArbitrage-free IV surfaces are then generated by sampling from the posterior\ndistribution on the latent space, decoding to obtain SDE model parameters, and\nfinally mapping those parameters to IV surfaces.",
          "link": "http://arxiv.org/abs/2108.04941",
          "publishedOn": "2021-08-12T01:56:23.664Z",
          "wordCount": 559,
          "title": "Arbitrage-Free Implied Volatility Surface Generation with Variational Autoencoders. (arXiv:2108.04941v1 [q-fin.MF])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04321",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Das_A/0/1/0/all/0/1\">Avishek Das</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Rose_D/0/1/0/all/0/1\">Dominic C. Rose</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Garrahan_J/0/1/0/all/0/1\">Juan P. Garrahan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Limmer_D/0/1/0/all/0/1\">David T. Limmer</a>",
          "description": "We present a method to probe rare molecular dynamics trajectories directly\nusing reinforcement learning. We consider trajectories that are conditioned to\ntransition between regions of configuration space in finite time, like those\nrelevant in the study of reactive events, as well as trajectories exhibiting\nrare fluctuations of time-integrated quantities in the long time limit, like\nthose relevant in the calculation of large deviation functions. In both cases,\nreinforcement learning techniques are used to optimize an added force that\nminimizes the Kullback-Leibler divergence between the conditioned trajectory\nensemble and a driven one. Under the optimized added force, the system evolves\nthe rare fluctuation as a typical one, affording a variational estimate of its\nlikelihood in the original trajectory ensemble. Low variance gradients\nemploying value functions are proposed to increase the convergence of the\noptimal force. The method we develop employing these gradients leads to\nefficient and accurate estimates of both the optimal force and the likelihood\nof the rare event for a variety of model systems.",
          "link": "http://arxiv.org/abs/2105.04321",
          "publishedOn": "2021-08-12T01:56:23.658Z",
          "wordCount": 626,
          "title": "Reinforcement learning of rare diffusive dynamics. (arXiv:2105.04321v2 [cond-mat.stat-mech] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1\">Xiang-Rong Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liqin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guorui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinyao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Binding Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Siran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jingshan Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hongbo Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>",
          "description": "Traditional industrial recommenders are usually trained on a single business\ndomain and then serve for this domain. However, in large commercial platforms,\nit is often the case that the recommenders need to make click-through rate\n(CTR) predictions for multiple business domains. Different domains have\noverlapping user groups and items. Thus, there exist commonalities. Since the\nspecific user groups have disparity and the user behaviors may change in\nvarious business domains, there also have distinctions. The distinctions result\nin domain-specific data distributions, making it hard for a single shared model\nto work well on all domains. To learn an effective and efficient CTR model to\nhandle multiple domains simultaneously, we present Star Topology Adaptive\nRecommender (STAR). Concretely, STAR has the star topology, which consists of\nthe shared centered parameters and domain-specific parameters. The shared\nparameters are applied to learn commonalities of all domains, and the\ndomain-specific parameters capture domain distinction for more refined\nprediction. Given requests from different business domains, STAR can adapt its\nparameters conditioned on the domain characteristics. The experimental result\nfrom production data validates the superiority of the proposed STAR model.\nSince 2020, STAR has been deployed in the display advertising system of\nAlibaba, obtaining averaging 8.0% improvement on CTR and 6.0% on RPM (Revenue\nPer Mille).",
          "link": "http://arxiv.org/abs/2101.11427",
          "publishedOn": "2021-08-12T01:56:23.652Z",
          "wordCount": 704,
          "title": "One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction. (arXiv:2101.11427v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06539",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Papamarkou_T/0/1/0/all/0/1\">Theodore Papamarkou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hinkle_J/0/1/0/all/0/1\">Jacob Hinkle</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Young_M/0/1/0/all/0/1\">M. Todd Young</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Womble_D/0/1/0/all/0/1\">David Womble</a>",
          "description": "Markov chain Monte Carlo (MCMC) methods have not been broadly adopted in\nBayesian neural networks (BNNs). This paper initially reviews the main\nchallenges in sampling from the parameter posterior of a neural network via\nMCMC. Such challenges culminate to lack of convergence to the parameter\nposterior. Nevertheless, this paper shows that a non-converged Markov chain,\ngenerated via MCMC sampling from the parameter space of a neural network, can\nyield via Bayesian marginalization a valuable posterior predictive distribution\nof the output of the neural network. Classification examples based on\nmultilayer perceptrons showcase highly accurate posterior predictive\ndistributions. The postulate of limited scope for MCMC developments in BNNs is\npartially valid; an asymptotically exact parameter posterior seems less\nplausible, yet an accurate posterior predictive distribution is a tenable\nresearch avenue.",
          "link": "http://arxiv.org/abs/1910.06539",
          "publishedOn": "2021-08-12T01:56:23.645Z",
          "wordCount": 617,
          "title": "Challenges in Markov chain Monte Carlo for Bayesian neural networks. (arXiv:1910.06539v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Charig Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamdouar_H/0/1/0/all/0/1\">Hala Lamdouar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_E/0/1/0/all/0/1\">Erika Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>",
          "description": "Animals have evolved highly functional visual systems to understand motion,\nassisting perception even under complex environments. In this paper, we work\ntowards developing a computer vision system able to segment objects by\nexploiting motion cues, i.e. motion segmentation. We make the following\ncontributions: First, we introduce a simple variant of the Transformer to\nsegment optical flow frames into primary objects and the background. Second, we\ntrain the architecture in a self-supervised manner, i.e. without using any\nmanual annotations. Third, we analyze several critical components of our method\nand conduct thorough ablation studies to validate their necessity. Fourth, we\nevaluate the proposed architecture on public benchmarks (DAVIS2016, SegTrackv2,\nand FBMS59). Despite using only optical flow as input, our approach achieves\nsuperior or comparable results to previous state-of-the-art self-supervised\nmethods, while being an order of magnitude faster. We additionally evaluate on\na challenging camouflage dataset (MoCA), significantly outperforming the other\nself-supervised approaches, and comparing favourably to the top supervised\napproach, highlighting the importance of motion cues, and the potential bias\ntowards visual appearance in existing video segmentation models.",
          "link": "http://arxiv.org/abs/2104.07658",
          "publishedOn": "2021-08-12T01:56:23.638Z",
          "wordCount": 653,
          "title": "Self-supervised Video Object Segmentation by Motion Grouping. (arXiv:2104.07658v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia J. Biega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekstrand_M/0/1/0/all/0/1\">Michael D. Ekstrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sergey Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohlmeier_S/0/1/0/all/0/1\">Sebastian Kohlmeier</a>",
          "description": "This paper provides an overview of the NIST TREC 2020 Fair Ranking track. For\n2020, we again adopted an academic search task, where we have a corpus of\nacademic article abstracts and queries submitted to a production academic\nsearch engine. The central goal of the Fair Ranking track is to provide fair\nexposure to different groups of authors (a group fairness framing). We\nrecognize that there may be multiple group definitions (e.g. based on\ndemographics, stature, topic) and hoped for the systems to be robust to these.\nWe expected participants to develop systems that optimize for fairness and\nrelevance for arbitrary group definitions, and did not reveal the exact group\ndefinitions until after the evaluation runs were submitted.The track contains\ntwo tasks,reranking and retrieval, with a shared evaluation.",
          "link": "http://arxiv.org/abs/2108.05135",
          "publishedOn": "2021-08-12T01:56:23.622Z",
          "wordCount": 582,
          "title": "Overview of the TREC 2020 Fair Ranking Track. (arXiv:2108.05135v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ratzlaff_N/0/1/0/all/0/1\">Neale Ratzlaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Qinxun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuxin_L/0/1/0/all/0/1\">Li Fuxin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>",
          "description": "Recently, particle-based variational inference (ParVI) methods have gained\ninterest because they can avoid arbitrary parametric assumptions that are\ncommon in variational inference. However, many ParVI approaches do not allow\narbitrary sampling from the posterior, and the few that do allow such sampling\nsuffer from suboptimality. This work proposes a new method for learning to\napproximately sample from the posterior distribution. We construct a neural\nsampler that is trained with the functional gradient of the KL-divergence\nbetween the empirical sampling distribution and the target distribution,\nassuming the gradient resides within a reproducing kernel Hilbert space. Our\ngenerative ParVI (GPVI) approach maintains the asymptotic performance of ParVI\nmethods while offering the flexibility of a generative sampler. Through\ncarefully constructed experiments, we show that GPVI outperforms previous\ngenerative ParVI methods such as amortized SVGD, and is competitive with ParVI\nas well as gold-standard approaches like Hamiltonian Monte Carlo for fitting\nboth exactly known and intractable target distributions.",
          "link": "http://arxiv.org/abs/2103.01291",
          "publishedOn": "2021-08-12T01:56:23.617Z",
          "wordCount": 628,
          "title": "Generative Particle Variational Inference via Estimation of Functional Gradients. (arXiv:2103.01291v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rorabaugh_A/0/1/0/all/0/1\">Ariel Keller Rorabaugh</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Caino_Lores_S/0/1/0/all/0/1\">Silvina Ca&#xed;no-Lores</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wyatt_M/0/1/0/all/0/1\">Michael R. Wyatt II</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_T/0/1/0/all/0/1\">Travis Johnston</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Taufer_M/0/1/0/all/0/1\">Michela Taufer</a> (1) ((1) University of Tennessee, Knoxville, USA, (2) Oak Ridge National Lab, Oak Ridge, USA)",
          "description": "Neural network (NN) models are increasingly used in scientific simulations,\nAI, and other high performance computing (HPC) fields to extract knowledge from\ndatasets. Each dataset requires tailored NN model architecture, but designing\nstructures by hand is a time-consuming and error-prone process. Neural\narchitecture search (NAS) automates the design of NN architectures. NAS\nattempts to find well-performing NN models for specialized datsets, where\nperformance is measured by key metrics that capture the NN capabilities (e.g.,\naccuracy of classification of samples in a dataset). Existing NAS methods are\nresource intensive, especially when searching for highly accurate models for\nlarger and larger datasets.\n\nTo address this problem, we propose a performance estimation strategy that\nreduces the resources for training NNs and increases NAS throughput without\njeopardizing accuracy. We implement our strategy via an engine called PEng4NN\nthat plugs into existing NAS methods; in doing so, PEng4NN predicts the final\naccuracy of NNs early in the training process, informs the NAS of NN\nperformance, and thus enables the NAS to terminate training NNs early. We\nassess our engine on three diverse datasets (i.e., CIFAR-100, Fashion MNIST,\nand SVHN). By reducing the training epochs needed, our engine achieves\nsubstantial throughput gain; on average, our engine saves 61% to 82% of\ntraining epochs, increasing throughput by a factor of 2.5 to 5 compared to a\nstate-of-the-art NAS method. We achieve this gain without compromising\naccuracy, as we demonstrate with two key outcomes. First, across all our tests,\nbetween 74% and 97% of the ground truth best models lie in our set of predicted\nbest models. Second, the accuracy distributions of the ground truth best models\nand our predicted best models are comparable, with the mean accuracy values\ndiffering by at most .7 percentage points across all tests.",
          "link": "http://arxiv.org/abs/2101.04185",
          "publishedOn": "2021-08-12T01:56:23.599Z",
          "wordCount": 791,
          "title": "PEng4NN: An Accurate Performance Estimation Engine for Efficient Automated Neural Network Architecture Search. (arXiv:2101.04185v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahner_M/0/1/0/all/0/1\">Martin Hahner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "This work addresses the challenging task of LiDAR-based 3D object detection\nin foggy weather. Collecting and annotating data in such a scenario is very\ntime, labor and cost intensive. In this paper, we tackle this problem by\nsimulating physically accurate fog into clear-weather scenes, so that the\nabundant existing real datasets captured in clear weather can be repurposed for\nour task. Our contributions are twofold: 1) We develop a physically valid fog\nsimulation method that is applicable to any LiDAR dataset. This unleashes the\nacquisition of large-scale foggy training data at no extra cost. These\npartially synthetic data can be used to improve the robustness of several\nperception methods, such as 3D object detection and tracking or simultaneous\nlocalization and mapping, on real foggy data. 2) Through extensive experiments\nwith several state-of-the-art detection approaches, we show that our fog\nsimulation can be leveraged to significantly improve the performance for 3D\nobject detection in the presence of fog. Thus, we are the first to provide\nstrong 3D object detection baselines on the Seeing Through Fog dataset. Our\ncode is available at www.trace.ethz.ch/lidar_fog_simulation.",
          "link": "http://arxiv.org/abs/2108.05249",
          "publishedOn": "2021-08-12T01:56:23.554Z",
          "wordCount": 642,
          "title": "Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather. (arXiv:2108.05249v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.03988",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ahmad_N/0/1/0/all/0/1\">Nasir Ahmad</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ambrogioni_L/0/1/0/all/0/1\">Luca Ambrogioni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gerven_M/0/1/0/all/0/1\">Marcel A. J. van Gerven</a>",
          "description": "We propose a solution to the weight transport problem, which questions the\nbiological plausibility of the backpropagation algorithm. We derive our method\nbased upon a theoretical analysis of the (approximate) dynamics of leaky\nintegrate-and-fire neurons. We show that the use of spike timing alone\noutcompetes existing biologically plausible methods for synaptic weight\ninference in spiking neural network models. Furthermore, our proposed method is\nmore flexible, being applicable to any spiking neuron model, is conservative in\nhow many parameters are required for implementation and can be deployed in an\nonline-fashion with minimal computational overhead. These features, together\nwith its biological plausibility, make it an attractive mechanism underlying\nweight inference at single synapses.",
          "link": "http://arxiv.org/abs/2003.03988",
          "publishedOn": "2021-08-12T01:56:23.542Z",
          "wordCount": 590,
          "title": "Overcoming the Weight Transport Problem via Spike-Timing-Dependent Weight Inference. (arXiv:2003.03988v4 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05447",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mazuelas_S/0/1/0/all/0/1\">Santiago Mazuelas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shen_Y/0/1/0/all/0/1\">Yuan Shen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Perez_A/0/1/0/all/0/1\">Aritz P&#xe9;rez</a>",
          "description": "The maximum entropy principle advocates to evaluate events' probabilities\nusing a distribution that maximizes entropy among those that satisfy certain\nexpectations' constraints. Such principle can be generalized for arbitrary\ndecision problems where it corresponds to minimax approaches. This paper\nestablishes a framework for supervised classification based on the generalized\nmaximum entropy principle that leads to minimax risk classifiers (MRCs). We\ndevelop learning techniques that determine MRCs for general entropy functions\nand provide performance guarantees by means of convex optimization. In\naddition, we describe the relationship of the presented techniques with\nexisting classification methods, and quantify MRCs performance in comparison\nwith the proposed bounds and conventional methods.",
          "link": "http://arxiv.org/abs/2007.05447",
          "publishedOn": "2021-08-12T01:56:23.524Z",
          "wordCount": 552,
          "title": "Generalized Maximum Entropy for Supervised Classification. (arXiv:2007.05447v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stoian_M/0/1/0/all/0/1\">Mihail Stoian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_A/0/1/0/all/0/1\">Andreas Kipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_R/0/1/0/all/0/1\">Ryan Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraska_T/0/1/0/all/0/1\">Tim Kraska</a>",
          "description": "Latest research proposes to replace existing index structures with learned\nmodels. However, current learned indexes tend to have many hyperparameters,\noften do not provide any error guarantees, and are expensive to build. We\nintroduce Practical Learned Index (PLEX). PLEX only has a single hyperparameter\n$\\epsilon$ (maximum prediction error) and offers a better trade-off between\nbuild and lookup time than state-of-the-art approaches. Similar to RadixSpline,\nPLEX consists of a spline and a (multi-level) radix layer. It first builds a\nspline satisfying the given $\\epsilon$ and then performs an ad-hoc analysis of\nthe distribution of spline points to quickly tune the radix layer.",
          "link": "http://arxiv.org/abs/2108.05117",
          "publishedOn": "2021-08-12T01:56:23.518Z",
          "wordCount": 544,
          "title": "PLEX: Towards Practical Learned Indexing. (arXiv:2108.05117v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05039",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Dunca_A/0/1/0/all/0/1\">Anastasia Dunca</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Adler_F/0/1/0/all/0/1\">Frederick R. Adler</a>",
          "description": "According to the National Cancer Institute, there were 9.5 million\ncancer-related deaths in 2018. A challenge in improving treatment is resistance\nin genetically unstable cells. The purpose of this study is to evaluate\nunsupervised machine learning on classifying treatment-resistant phenotypes in\nheterogeneous tumors through analysis of single cell RNA sequencing(scRNAseq)\ndata with a pipeline and evaluation metrics. scRNAseq quantifies mRNA in cells\nand characterizes cell phenotypes. One scRNAseq dataset was analyzed\n(tumor/non-tumor cells of different molecular subtypes and patient\nidentifications). The pipeline consisted of data filtering, dimensionality\nreduction with Principal Component Analysis, projection with Uniform Manifold\nApproximation and Projection, clustering with nine approaches (Ward, BIRCH,\nGaussian Mixture Model, DBSCAN, Spectral, Affinity Propagation, Agglomerative\nClustering, Mean Shift, and K-Means), and evaluation. Seven models divided\ntumor versus non-tumor cells and molecular subtype while six models classified\ndifferent patient identification (13 of which were presented in the dataset);\nK-Means, Ward, and BIRCH often ranked highest with ~80% accuracy on the tumor\nversus non-tumor task and ~60% for molecular subtype and patient ID. An\noptimized classification pipeline using K-Means, Ward, and BIRCH models was\nevaluated to be most effective for further analysis. In clinical research where\nthere is currently no standard protocol for scRNAseq analysis, clusters\ngenerated from this pipeline can be used to understand cancer cell behavior and\nmalignant growth, directly affecting the success of treatment.",
          "link": "http://arxiv.org/abs/2108.05039",
          "publishedOn": "2021-08-12T01:56:23.505Z",
          "wordCount": 677,
          "title": "Predicting Molecular Phenotypes with Single Cell RNA Sequencing Data: an Assessment of Unsupervised Machine Learning Models. (arXiv:2108.05039v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05099",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qin_Z/0/1/0/all/0/1\">Zhaoming Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huaying Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuzhou Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1\">Hong Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_J/0/1/0/all/0/1\">Junwei Cao</a>",
          "description": "As a model-free optimization and decision-making method, deep reinforcement\nlearning (DRL) has been widely applied to the filed of energy management in\nenergy Internet. While, some DRL-based energy management schemes also\nincorporate the prediction module used by the traditional model-based methods,\nwhich seems to be unnecessary and even adverse. In this work, we present the\nstandard DRL-based energy management scheme with and without prediction. Then,\nthese two schemes are compared in the unified energy management framework. The\nsimulation results demonstrate that the energy management scheme without\nprediction is superior over the scheme with prediction. This work intends to\nrectify the misuse of DRL methods in the field of energy management.",
          "link": "http://arxiv.org/abs/2108.05099",
          "publishedOn": "2021-08-12T01:56:23.500Z",
          "wordCount": 571,
          "title": "Does Explicit Prediction Matter in Energy Management Based on Deep Reinforcement Learning?. (arXiv:2108.05099v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Beibin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuechterlein_N/0/1/0/all/0/1\">Nicholas Nuechterlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barney_E/0/1/0/all/0/1\">Erin Barney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_C/0/1/0/all/0/1\">Claire Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minah Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahony_M/0/1/0/all/0/1\">Monique Mahony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atyabi_A/0/1/0/all/0/1\">Adham Atyabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Li Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_P/0/1/0/all/0/1\">Pamela Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapiro_L/0/1/0/all/0/1\">Linda Shapiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shic_F/0/1/0/all/0/1\">Frederick Shic</a>",
          "description": "Identifying oculomotor behaviors relevant for eye-tracking applications is a\ncritical but often challenging task. Aiming to automatically learn and extract\nknowledge from existing eye-tracking data, we develop a novel method that\ncreates rich representations of oculomotor scanpaths to facilitate the learning\nof downstream tasks. The proposed stimulus-agnostic Oculomotor Behavior\nFramework (OBF) model learns human oculomotor behaviors from unsupervised and\nsemi-supervised tasks, including reconstruction, predictive coding, fixation\nidentification, and contrastive learning tasks. The resultant pre-trained OBF\nmodel can be used in a variety of applications. Our pre-trained model\noutperforms baseline approaches and traditional scanpath methods in autism\nspectrum disorder and viewed-stimulus classification tasks. Ablation\nexperiments further show our proposed method could achieve even better results\nwith larger model sizes and more diverse eye-tracking training datasets,\nsupporting the model's potential for future eye-tracking applications. Open\nsource code: this http URL",
          "link": "http://arxiv.org/abs/2108.05025",
          "publishedOn": "2021-08-12T01:56:23.494Z",
          "wordCount": 599,
          "title": "Learning Oculomotor Behaviors from Scanpath. (arXiv:2108.05025v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05149",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Francesco Giannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1\">Marco Maggini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>",
          "description": "The large and still increasing popularity of deep learning clashes with a\nmajor limit of neural network architectures, that consists in their lack of\ncapability in providing human-understandable motivations of their decisions. In\nsituations in which the machine is expected to support the decision of human\nexperts, providing a comprehensible explanation is a feature of crucial\nimportance. The language used to communicate the explanations must be formal\nenough to be implementable in a machine and friendly enough to be\nunderstandable by a wide audience. In this paper, we propose a general approach\nto Explainable Artificial Intelligence in the case of neural architectures,\nshowing how a mindful design of the networks leads to a family of interpretable\ndeep learning models called Logic Explained Networks (LENs). LENs only require\ntheir inputs to be human-understandable predicates, and they provide\nexplanations in terms of simple First-Order Logic (FOL) formulas involving such\npredicates. LENs are general enough to cover a large number of scenarios.\nAmongst them, we consider the case in which LENs are directly used as special\nclassifiers with the capability of being explainable, or when they act as\nadditional networks with the role of creating the conditions for making a\nblack-box classifier explainable by FOL formulas. Despite supervised learning\nproblems are mostly emphasized, we also show that LENs can learn and provide\nexplanations in unsupervised learning settings. Experimental results on several\ndatasets and tasks show that LENs may yield better classifications than\nestablished white-box models, such as decision trees and Bayesian rule lists,\nwhile providing more compact and meaningful explanations.",
          "link": "http://arxiv.org/abs/2108.05149",
          "publishedOn": "2021-08-12T01:56:23.443Z",
          "wordCount": 698,
          "title": "Logic Explained Networks. (arXiv:2108.05149v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirnap_O/0/1/0/all/0/1\">&#xd6;mer K&#x131;rnap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia Biega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekstrand_M/0/1/0/all/0/1\">Michael Ekstrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carterette_B/0/1/0/all/0/1\">Ben Carterette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Y&#x131;lmaz</a>",
          "description": "There is increasing attention to evaluating the fairness of search system\nranking decisions. These metrics often consider the membership of items to\nparticular groups, often identified using protected attributes such as gender\nor ethnicity. To date, these metrics typically assume the availability and\ncompleteness of protected attribute labels of items. However, the protected\nattributes of individuals are rarely present, limiting the application of fair\nranking metrics in large scale systems. In order to address this problem, we\npropose a sampling strategy and estimation technique for four fair ranking\nmetrics. We formulate a robust and unbiased estimator which can operate even\nwith very limited number of labeled items. We evaluate our approach using both\nsimulated and real world data. Our experimental results demonstrate that our\nmethod can estimate this family of fair ranking metrics and provides a robust,\nreliable alternative to exhaustive or random data annotation.",
          "link": "http://arxiv.org/abs/2108.05152",
          "publishedOn": "2021-08-12T01:56:23.437Z",
          "wordCount": 599,
          "title": "Estimation of Fair Ranking Metrics with Incomplete Judgments. (arXiv:2108.05152v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1\">Gaurav Sukhatme</a>",
          "description": "Language-guided robots performing home and office tasks must navigate in and\ninteract with the world. Grounding language instructions against visual\nobservations and actions to take in an environment is an open challenge. We\npresent Embodied BERT (EmBERT), a transformer-based model which can attend to\nhigh-dimensional, multi-modal inputs across long temporal horizons for\nlanguage-conditioned task completion. Additionally, we bridge the gap between\nsuccessful object-centric navigation models used for non-interactive agents and\nthe language-guided visual task completion benchmark, ALFRED, by introducing\nobject navigation targets for EmBERT training. We achieve competitive\nperformance on the ALFRED benchmark, and EmBERT marks the first\ntransformer-based model to successfully handle the long-horizon, dense,\nmulti-modal histories of ALFRED, and the first ALFRED model to utilize\nobject-centric navigation targets.",
          "link": "http://arxiv.org/abs/2108.04927",
          "publishedOn": "2021-08-12T01:56:23.432Z",
          "wordCount": 581,
          "title": "Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion. (arXiv:2108.04927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+pourmirzaei_M/0/1/0/all/0/1\">Mahdi pourmirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+montazer_g/0/1/0/all/0/1\">gholam ali montazer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+esmaili_f/0/1/0/all/0/1\">farzaneh esmaili</a>",
          "description": "Recent progress of Self-Supervised Learning (SSL) demonstrates the capability\nof these methods in computer vision field. However, this progress could not\nshow any promises for fine-grained tasks such as Head Pose estimation. In this\narticle, we have tried to answer a question: How SSL can be used for Head Pose\nestimation? In general, there are two main approaches to use SSL: 1. Using\npre-trained weights which can be done via weights pre-training on ImageNet or\nvia SSL tasks. 2. Leveraging SSL as an auxiliary co-training task besides of\nSupervised Learning (SL) tasks at the same time. In this study, modified\nversions of jigsaw puzzling and rotation as SSL pre-text tasks are used and the\nbest architecture for our proposed Hybrid Multi-Task Learning (HMTL) is found.\nFinally, the HopeNet method as a baseline is selected and the impact of SSL\npre-training and ImageNet pre-training on both HMTL and SL are compared. The\nerror rate reduced by the HTML method up to 11% compare to the SL. Moreover,\nHMTL method showed that it was good with all kinds of initial weights: random,\nImageNet and SSL pre-training weights. Also, it was observed, when puzzled\nimages are used for SL alone, the average error rate placed between SL and HMTL\nwhich showed the importance of local spatial features compare to global spatial\nfeatures.",
          "link": "http://arxiv.org/abs/2108.04893",
          "publishedOn": "2021-08-12T01:56:23.412Z",
          "wordCount": 662,
          "title": "How Self-Supervised Learning Can be Used for Fine-Grained Head Pose Estimation?. (arXiv:2108.04893v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Usman_B/0/1/0/all/0/1\">Ben Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sud_A/0/1/0/all/0/1\">Avneesh Sud</a>",
          "description": "Recently, huge strides were made in monocular and multi-view pose estimation\nwith known camera parameters, whereas pose estimation from multiple cameras\nwith unknown positions and orientations received much less attention. In this\npaper, we show how to train a neural model that can perform accurate 3D pose\nand camera estimation, takes into account joint location uncertainty due\nocclusion from multiple views, and requires only 2D keypoint data for training.\nOur method outperforms both classical bundle adjustment and weakly-supervised\nmonocular 3D baselines on the well-established Human3.6M dataset, as well as\nthe more challenging in-the-wild Ski-Pose PTZ dataset with moving cameras. We\nprovide an extensive ablation study separating the error due to the camera\nmodel, number of cameras, initialization, and image-space joint localization\nfrom the additional error introduced by our model.",
          "link": "http://arxiv.org/abs/2108.04869",
          "publishedOn": "2021-08-12T01:56:23.402Z",
          "wordCount": 572,
          "title": "MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision. (arXiv:2108.04869v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Young Ah Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyung Ho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1\">Eunji Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Huy Kang Kim</a>",
          "description": "In the era of intelligent transportation, driver behavior profiling has\nbecome a beneficial technology as it provides knowledge regarding the driver's\naggressiveness. Previous approaches achieved promising driver behavior\nprofiling performance through establishing statistical heuristics rules or\nsupervised learning-based models. Still, there exist limits that the\npractitioner should prepare a labeled dataset, and prior approaches could not\nclassify aggressive behaviors which are not known a priori. In pursuit of\nimproving the aforementioned drawbacks, we propose a novel approach to driver\nbehavior profiling leveraging an unsupervised learning paradigm. First, we cast\nthe driver behavior profiling problem as anomaly detection. Second, we\nestablished recurrent neural networks that predict the next feature vector\ngiven a sequence of feature vectors. We trained the model with normal driver\ndata only. As a result, our model yields high regression error given a sequence\nof aggressive driver behavior and low error given at a sequence of normal\ndriver behavior. We figured this difference of error between normal and\naggressive driver behavior can be an adequate flag for driver behavior\nprofiling and accomplished a precise performance in experiments. Lastly, we\nfurther analyzed the optimal level of sequence length for identifying each\naggressive driver behavior. We expect the proposed approach to be a useful\nbaseline for unsupervised driver behavior profiling and contribute to the\nefficient, intelligent transportation ecosystem.",
          "link": "http://arxiv.org/abs/2108.05079",
          "publishedOn": "2021-08-12T01:56:23.357Z",
          "wordCount": 678,
          "title": "Unsupervised Driver Behavior Profiling leveraging Recurrent Neural Networks. (arXiv:2108.05079v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>",
          "description": "Learning with noisy labels has gained the enormous interest in the robust\ndeep learning area. Recent studies have empirically disclosed that utilizing\ndual networks can enhance the performance of single network but without\ntheoretic proof. In this paper, we propose Cooperative Learning (CooL)\nframework for noisy supervision that analytically explains the effects of\nleveraging dual or multiple networks. Specifically, the simple but efficient\ncombination in CooL yields a more reliable risk minimization for unseen clean\ndata. A range of experiments have been conducted on several benchmarks with\nboth synthetic and real-world settings. Extensive results indicate that CooL\noutperforms several state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.05092",
          "publishedOn": "2021-08-12T01:56:23.342Z",
          "wordCount": 530,
          "title": "Cooperative Learning for Noisy Supervision. (arXiv:2108.05092v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05183",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chawdhury_T/0/1/0/all/0/1\">Tarun Kumar Chawdhury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Grant_D/0/1/0/all/0/1\">David J. Grant</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jin_H/0/1/0/all/0/1\">Hyun Yong Jin</a>",
          "description": "Lead optimization is a key step in drug discovery to produce potent and\nselective compounds. Historically, in silico screening and structure-based\nsmall molecule designing facilitated the processes. Although the recent\napplication of deep learning to drug discovery piloted the possibility of their\nin silico application lead optimization steps, the real-world application is\nlacking due to the tool availability. Here, we developed a single user\ninterface application, called Deep2Lead. Our web-based application integrates\nVAE and DeepPurpose DTI and allows a user to quickly perform a lead\noptimization task with no prior programming experience.",
          "link": "http://arxiv.org/abs/2108.05183",
          "publishedOn": "2021-08-12T01:56:23.335Z",
          "wordCount": 540,
          "title": "Deep2Lead: A distributed deep learning application for small molecule lead optimization. (arXiv:2108.05183v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zitao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_P/0/1/0/all/0/1\">Pritam Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattabiraman_K/0/1/0/all/0/1\">Karthik Pattabiraman</a>",
          "description": "Adversarial patch attack against image classification deep neural networks\n(DNNs), in which the attacker can inject arbitrary distortions within a bounded\nregion of an image, is able to generate adversarial perturbations that are\nrobust (i.e., remain adversarial in physical world) and universal (i.e., remain\nadversarial on any input). It is thus important to detect and mitigate such\nattack to ensure the security of DNNs.\n\nThis work proposes Jujutsu, a technique to detect and mitigate robust and\nuniversal adversarial patch attack. Jujutsu leverages the universal property of\nthe patch attack for detection. It uses explainable AI technique to identify\nsuspicious features that are potentially malicious, and verify their\nmaliciousness by transplanting the suspicious features to new images. An\nadversarial patch continues to exhibit the malicious behavior on the new images\nand thus can be detected based on prediction consistency. Jujutsu leverages the\nlocalized nature of the patch attack for mitigation, by randomly masking the\nsuspicious features to \"remove\" adversarial perturbations. However, the network\nmight fail to classify the images as some of the contents are removed (masked).\nTherefore, Jujutsu uses image inpainting for synthesizing alternative contents\nfrom the pixels that are masked, which can reconstruct the \"clean\" image for\ncorrect prediction. We evaluate Jujutsu on five DNNs on two datasets, and show\nthat Jujutsu achieves superior performance and significantly outperforms\nexisting techniques. Jujutsu can further defend against various variants of the\nbasic attack, including 1) physical-world attack; 2) attacks that target\ndiverse classes; 3) attacks that use patches in different shapes and 4)\nadaptive attacks.",
          "link": "http://arxiv.org/abs/2108.05075",
          "publishedOn": "2021-08-12T01:56:23.299Z",
          "wordCount": 703,
          "title": "Turning Your Strength against You: Detecting and Mitigating Robust and Universal Adversarial Patch Attack. (arXiv:2108.05075v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mantowsky_S/0/1/0/all/0/1\">Sven Mantowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuer_F/0/1/0/all/0/1\">Falk Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhari_S/0/1/0/all/0/1\">Syed Saqib Bukhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keckeisen_M/0/1/0/all/0/1\">Michael Keckeisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Georg Schneider</a>",
          "description": "Development in the field of Single Board Computers (SBC) have been increasing\nfor several years. They provide a good balance between computing performance\nand power consumption which is usually required for mobile platforms, like\napplication in vehicles for Advanced Driver Assistance Systems (ADAS) and\nAutonomous Driving (AD). However, there is an ever-increasing need of more\npowerful and efficient SBCs which can run power intensive Deep Neural Networks\n(DNNs) in real-time and can also satisfy necessary functional safety\nrequirements such as Automotive Safety Integrity Level (ASIL). ProAI is being\ndeveloped by ZF mainly to run powerful and efficient applications such as\nmultitask DNNs and on top of that it also has the required safety certification\nfor AD. In this work, we compare and discuss state of the art SBC on the basis\nof power intensive multitask DNN architecture called Multitask-CenterNet with\nrespect to performance measures such as, FPS and power efficiency. As an\nautomotive supercomputer, ProAI delivers an excellent combination of\nperformance and efficiency, managing nearly twice the number of FPS per watt\nthan a modern workstation laptop and almost four times compared to the Jetson\nNano. Furthermore, it was also shown that there is still power in reserve for\nfurther and more complex tasks on the ProAI, based on the CPU and GPU\nutilization during the benchmark.",
          "link": "http://arxiv.org/abs/2108.05170",
          "publishedOn": "2021-08-12T01:56:23.284Z",
          "wordCount": 684,
          "title": "ProAI: An Efficient Embedded AI Hardware for Automotive Applications - a Benchmark Study. (arXiv:2108.05170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orr_L/0/1/0/all/0/1\">Laurel Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1\">Atindriyo Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1\">Karan Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leszczynski_M/0/1/0/all/0/1\">Megan Leszczynski</a>",
          "description": "The industrial machine learning pipeline requires iterating on model\nfeatures, training and deploying models, and monitoring deployed models at\nscale. Feature stores were developed to manage and standardize the engineer's\nworkflow in this end-to-end pipeline, focusing on traditional tabular feature\ndata. In recent years, however, model development has shifted towards using\nself-supervised pretrained embeddings as model features. Managing these\nembeddings and the downstream systems that use them introduces new challenges\nwith respect to managing embedding training data, measuring embedding quality,\nand monitoring downstream models that use embeddings. These challenges are\nlargely unaddressed in standard feature stores. Our goal in this tutorial is to\nintroduce the feature store system and discuss the challenges and current\nsolutions to managing these new embedding-centric pipelines.",
          "link": "http://arxiv.org/abs/2108.05053",
          "publishedOn": "2021-08-12T01:56:23.278Z",
          "wordCount": 568,
          "title": "Managing ML Pipelines: Feature Stores and the Coming Wave of Embedding Ecosystems. (arXiv:2108.05053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "Learning to rank systems has become an important aspect of our daily life.\nHowever, the implicit user feedback that is used to train many learning to rank\nmodels is usually noisy and suffered from user bias (i.e., position bias).\nThus, obtaining an unbiased model using biased feedback has become an important\nresearch field for IR. Existing studies on unbiased learning to rank (ULTR) can\nbe generalized into two families-algorithms that attain unbiasedness with\nlogged data, offline learning, and algorithms that achieve unbiasedness by\nestimating unbiased parameters with real-time user interactions, namely online\nlearning. While there exist many algorithms from both families, there lacks a\nunified way to compare and benchmark them. As a result, it can be challenging\nfor researchers to choose the right technique for their problems or for people\nwho are new to the field to learn and understand existing algorithms. To solve\nthis problem, we introduced ULTRA, which is a flexible, extensible, and easily\nconfigure ULTR toolbox. Its key features include support for multiple ULTR\nalgorithms with configurable hyperparameters, a variety of built-in click\nmodels that can be used separately to simulate clicks, different ranking model\narchitecture and evaluation metrics, and simple learning to rank pipeline\ncreation. In this paper, we discuss the general framework of ULTR, briefly\ndescribe the algorithms in ULTRA, detailed the structure, and pipeline of the\ntoolbox. We experimented on all the algorithms supported by ultra and showed\nthat the toolbox performance is reasonable. Our toolbox is an important\nresource for researchers to conduct experiments on ULTR algorithms with\ndifferent configurations as well as testing their own algorithms with the\nsupported features.",
          "link": "http://arxiv.org/abs/2108.05073",
          "publishedOn": "2021-08-12T01:56:23.273Z",
          "wordCount": 708,
          "title": "ULTRA: An Unbiased Learning To Rank Algorithm Toolbox. (arXiv:2108.05073v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kessl_R/0/1/0/all/0/1\">Robert Kessl</a>",
          "description": "In the recent decade companies started collecting of large amount of data.\nWithout a proper analyse, the data are usually useless. The field of analysing\nthe data is called data mining. Unfortunately, the amount of data is quite\nlarge: the data do not fit into main memory and the processing time can become\nquite huge. Therefore, we need parallel data mining algorithms. One of the\npopular and important data mining algorithm is the algorithm for generation of\nso called frequent itemsets. The problem of mining of frequent itemsets can be\nexplained on the following example: customers goes in a store put into theirs\nbaskets some goods; the owner of the store collects the baskets and wants to\nknow the set of goods that are bought together in at least p% of the baskets.\nCurrently, the sequential algorithms for mining of frequent itemsets are quite\ngood in the means of performance. However, the parallel algorithms for mining\nof frequent itemsets still do not achieve good speedup. In this thesis, we\ndevelop a parallel method for mining of frequent itemsets that can be used for\nan arbitrary depth first search sequential algorithms on a distributed memory\nparallel computer. Our method achieves speedup of ~ 6 on 10 processors. The\nmethod is based on an approximate estimation of processor load from a database\nsample - however it always computes the set of frequent itemsets from the whole\ndatabase. In this thesis, we show a theory underlying our method and show the\nperformance of the estimation process.",
          "link": "http://arxiv.org/abs/2108.05038",
          "publishedOn": "2021-08-12T01:56:23.267Z",
          "wordCount": 676,
          "title": "Parallel algorithms for mining of frequent itemsets. (arXiv:2108.05038v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lukas_N/0/1/0/all/0/1\">Nils Lukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_E/0/1/0/all/0/1\">Edward Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1\">Florian Kerschbaum</a>",
          "description": "Deep Neural Network (DNN) watermarking is a method for provenance\nverification of DNN models. Watermarking should be robust against watermark\nremoval attacks that derive a surrogate model that evades provenance\nverification. Many watermarking schemes that claim robustness have been\nproposed, but their robustness is only validated in isolation against a\nrelatively small set of attacks. There is no systematic, empirical evaluation\nof these claims against a common, comprehensive set of removal attacks. This\nuncertainty about a watermarking scheme's robustness causes difficulty to trust\ntheir deployment in practice. In this paper, we evaluate whether recently\nproposed watermarking schemes that claim robustness are robust against a large\nset of removal attacks. We survey methods from the literature that (i) are\nknown removal attacks, (ii) derive surrogate models but have not been evaluated\nas removal attacks, and (iii) novel removal attacks. Weight shifting and smooth\nretraining are novel removal attacks adapted to the DNN watermarking schemes\nsurveyed in this paper. We propose taxonomies for watermarking schemes and\nremoval attacks. Our empirical evaluation includes an ablation study over sets\nof parameters for each attack and watermarking scheme on the CIFAR-10 and\nImageNet datasets. Surprisingly, none of the surveyed watermarking schemes is\nrobust in practice. We find that schemes fail to withstand adaptive attacks and\nknown methods for deriving surrogate models that have not been evaluated as\nremoval attacks. This points to intrinsic flaws in how robustness is currently\nevaluated. We show that watermarking schemes need to be evaluated against a\nmore extensive set of removal attacks with a more realistic adversary model.\nOur source code and a complete dataset of evaluation results are publicly\navailable, which allows to independently verify our conclusions.",
          "link": "http://arxiv.org/abs/2108.04974",
          "publishedOn": "2021-08-12T01:56:23.261Z",
          "wordCount": 721,
          "title": "SoK: How Robust is Image Classification Deep Neural Network Watermarking? (Extended Version). (arXiv:2108.04974v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05024",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Grigoryeva_L/0/1/0/all/0/1\">Lyudmila Grigoryeva</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hart_A/0/1/0/all/0/1\">Allen Hart</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ortega_J/0/1/0/all/0/1\">Juan-Pablo Ortega</a>",
          "description": "This paper shows that the celebrated Embedding Theorem of Takens is a\nparticular case of a much more general statement according to which, randomly\ngenerated linear state-space representations of generic observations of an\ninvertible dynamical system carry in their wake an embedding of the phase space\ndynamics into the chosen Euclidean state space. This embedding coincides with a\nnatural generalized synchronization that arises in this setup and that yields a\ntopological conjugacy between the state-space dynamics driven by the generic\nobservations of the dynamical system and the dynamical system itself. This\nresult provides additional tools for the representation, learning, and analysis\nof chaotic attractors and sheds additional light on the reservoir computing\nphenomenon that appears in the context of recurrent neural networks.",
          "link": "http://arxiv.org/abs/2108.05024",
          "publishedOn": "2021-08-12T01:56:23.238Z",
          "wordCount": 567,
          "title": "Learning strange attractors with reservoir systems. (arXiv:2108.05024v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kai Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_D/0/1/0/all/0/1\">Da Kuang</a>",
          "description": "Autocomplete (a.k.a \"Query Auto-Completion\", \"AC\") suggests full queries\nbased on a prefix typed by customer. Autocomplete has been a core feature of\ncommercial search engine. In this paper, we propose a novel context-aware\nneural network based pairwise ranker (DeepPLTR) to improve AC ranking, DeepPLTR\nleverages contextual and behavioral features to rank queries by minimizing a\npairwise loss, based on a fully-connected neural network structure. Compared to\nLambdaMART ranker, DeepPLTR shows +3.90% MeanReciprocalRank (MRR) lift in\noffline evaluation, and yielded +0.06% (p < 0.1) Gross Merchandise Value (GMV)\nlift in an Amazon's online A/B experiment.",
          "link": "http://arxiv.org/abs/2108.04976",
          "publishedOn": "2021-08-12T01:56:23.231Z",
          "wordCount": 536,
          "title": "Deep Pairwise Learning To Rank For Search Autocomplete. (arXiv:2108.04976v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koga_K/0/1/0/all/0/1\">Kazuki Koga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takemoto_K/0/1/0/all/0/1\">Kazuhiro Takemoto</a>",
          "description": "Universal adversarial attacks, which hinder most deep neural network (DNN)\ntasks using only a small single perturbation called a universal adversarial\nperturbation (UAP), is a realistic security threat to the practical application\nof a DNN. In particular, such attacks cause serious problems in medical\nimaging. Given that computer-based systems are generally operated under a\nblack-box condition in which only queries on inputs are allowed and outputs are\naccessible, the impact of UAPs seems to be limited because well-used algorithms\nfor generating UAPs are limited to a white-box condition in which adversaries\ncan access the model weights and loss gradients. Nevertheless, we demonstrate\nthat UAPs are easily generatable using a relatively small dataset under\nblack-box conditions. In particular, we propose a method for generating UAPs\nusing a simple hill-climbing search based only on DNN outputs and demonstrate\nthe validity of the proposed method using representative DNN-based medical\nimage classifications. Black-box UAPs can be used to conduct both non-targeted\nand targeted attacks. Overall, the black-box UAPs showed high attack success\nrates (40% to 90%), although some of them had relatively low success rates\nbecause the method only utilizes limited information to generate UAPs. The\nvulnerability of black-box UAPs was observed in several model architectures.\nThe results indicate that adversaries can also generate UAPs through a simple\nprocedure under the black-box condition to foil or control DNN-based medical\nimage diagnoses, and that UAPs are a more realistic security threat.",
          "link": "http://arxiv.org/abs/2108.04979",
          "publishedOn": "2021-08-12T01:56:23.212Z",
          "wordCount": 694,
          "title": "Simple black-box universal adversarial attacks on medical image classification based on deep neural networks. (arXiv:2108.04979v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Juwei Lu</a>",
          "description": "State of the art (SOTA) few-shot learning (FSL) methods suffer significant\nperformance drop in the presence of domain differences between source and\ntarget datasets. The strong discrimination ability on the source dataset does\nnot necessarily translate to high classification accuracy on the target\ndataset. In this work, we address this cross-domain few-shot learning (CDFSL)\nproblem by boosting the generalization capability of the model. Specifically,\nwe teach the model to capture broader variations of the feature distributions\nwith a novel noise-enhanced supervised autoencoder (NSAE). NSAE trains the\nmodel by jointly reconstructing inputs and predicting the labels of inputs as\nwell as their reconstructed pairs. Theoretical analysis based on intra-class\ncorrelation (ICC) shows that the feature embeddings learned from NSAE have\nstronger discrimination and generalization abilities in the target domain. We\nalso take advantage of NSAE structure and propose a two-step fine-tuning\nprocedure that achieves better adaption and improves classification performance\nin the target domain. Extensive experiments and ablation studies are conducted\nto demonstrate the effectiveness of the proposed method. Experimental results\nshow that our proposed method consistently outperforms SOTA methods under\nvarious conditions.",
          "link": "http://arxiv.org/abs/2108.05028",
          "publishedOn": "2021-08-12T01:56:23.206Z",
          "wordCount": 634,
          "title": "Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder. (arXiv:2108.05028v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jinsung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Soyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_M/0/1/0/all/0/1\">Minju Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seunghyeon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chiyoung Song</a>",
          "description": "Mobile digital billboards are an effective way to augment brand-awareness.\nAmong various such mobile billboards, taxicab rooftop devices are emerging in\nthe market as a brand new media. Motov is a leading company in South Korea in\nthe taxicab rooftop advertising market. In this work, we present a lightweight\nyet accurate deep learning-based method to predict taxicabs' next locations to\nbetter prepare for targeted advertising based on demographic information of\nlocations. Considering the fact that next POI recommendation datasets are\nfrequently sparse, we design our presented model based on neural ordinary\ndifferential equations (NODEs), which are known to be robust to\nsparse/incorrect input, with several enhancements. Our model, which we call\nLightMove, has a larger prediction accuracy, a smaller number of parameters,\nand/or a smaller training/inference time, when evaluating with various\ndatasets, in comparison with state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.04993",
          "publishedOn": "2021-08-12T01:56:23.201Z",
          "wordCount": 579,
          "title": "LightMove: A Lightweight Next-POI Recommendation for Taxicab Rooftop Advertising. (arXiv:2108.04993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Peide Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Autonomous driving in multi-agent and dynamic traffic scenarios is\nchallenging, where the behaviors of other road agents are uncertain and hard to\nmodel explicitly, and the ego-vehicle should apply complicated negotiation\nskills with them to achieve both safe and efficient driving in various\nsettings, such as giving way, merging and taking turns. Traditional planning\nmethods are largely rule-based and scale poorly in these complex dynamic\nscenarios, often leading to reactive or even overly conservative behaviors.\nTherefore, they require tedious human efforts to maintain workability.\nRecently, deep learning-based methods have shown promising results with better\ngeneralization capability but less hand engineering effort. However, they are\neither implemented with supervised imitation learning (IL) that suffers from\nthe dataset bias and distribution mismatch problems, or trained with deep\nreinforcement learning (DRL) but focus on one specific traffic scenario. In\nthis work, we propose DQ-GAT to achieve scalable and proactive autonomous\ndriving, where graph attention-based networks are used to implicitly model\ninteractions, and asynchronous deep Q-learning is employed to train the network\nend-to-end in an unsupervised manner. Extensive experiments through a\nhigh-fidelity driving simulation show that our method can better trade-off\nsafety and efficiency in both seen and unseen scenarios, achieving higher goal\nsuccess rates than the baselines (at most 4.7$\\times$) with comparable task\ncompletion time. Demonstration videos are available at\nhttps://caipeide.github.io/dq-gat/.",
          "link": "http://arxiv.org/abs/2108.05030",
          "publishedOn": "2021-08-12T01:56:23.194Z",
          "wordCount": 706,
          "title": "DQ-GAT: Towards Safe and Efficient Autonomous Driving with Deep Q-Learning and Graph Attention Networks. (arXiv:2108.05030v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhakta_A/0/1/0/all/0/1\">Arnav Bhakta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_C/0/1/0/all/0/1\">Carolyn Byrne</a>",
          "description": "Creutzfeldt-Jakob disease (CJD) is a rapidly progressive and fatal\nneurodegenerative disease, that causes approximately 350 deaths in the United\nStates every year. In specific, it is a prion disease that is caused by a\nmisfolded prion protein, termed $PrP^{Sc}$, which is the infectious form of the\nprion protein $PrP^{C}$. Rather than being recycled by the body, the $PrP^{Sc}$\naggregates in the brain as plaques, leading to neurodegeneration of surrounding\ncells and the spongiform characteristics of the pathology. However, there has\nbeen very little research done into factors that can affect one's chances of\nacquiring $PrP^{Sc}$. In this paper, Elastic Net Regression, Long Short-Term\nMemory Recurrent Neural Network Architectures, and Random Forest have been used\nto predict Creutzfeldt-Jakob Disease Levels in the United States. New variables\nwere created as data for the models to use on the basis of common factors that\nare known to affect CJD, such as soil, food, and water quality. Based on the\nroot mean square error (RMSE), mean bias error (MBE), and mean absolute error\n(MAE) values, the study reveals the high impact of unhealthy lifestyle choices,\nCO$_{2}$ Levels, Pesticide Usage, and Potash K$_{2}$O Usage on CJD Levels. In\ndoing so, the study highlights new avenues of research for CJD prevention and\ndetection, as well as potential causes.",
          "link": "http://arxiv.org/abs/2108.04972",
          "publishedOn": "2021-08-12T01:56:23.179Z",
          "wordCount": 640,
          "title": "Creutzfeldt-Jakob Disease Prediction Using Machine Learning Techniques. (arXiv:2108.04972v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04951",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Esmaili_F/0/1/0/all/0/1\">Farzaneh Esmaili</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pourmirzaei_M/0/1/0/all/0/1\">Mahdi Pourmirzaei</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ramazi_S/0/1/0/all/0/1\">Shahin Ramazi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yavari_E/0/1/0/all/0/1\">Elham Yavari</a>",
          "description": "Reversible Post-Translational Modifications (PTMs) have vital roles in\nextending the functional diversity of proteins and effect meaningfully the\nregulation of protein functions in prokaryotic and eukaryotic organisms. PTMs\nhave happened as crucial molecular regulatory mechanisms that are utilized to\nregulate diverse cellular processes. Nevertheless, among the most well-studied\nPTMs can say mainly types of proteins are containing phosphorylation and\nsignificant roles in many biological processes. Disorder in this modification\ncan be caused by multiple diseases including neurological disorders and\ncancers. Therefore, it is necessary to predict the phosphorylation of target\nresidues in an uncharacterized amino acid sequence. Most experimental\ntechniques for predicting phosphorylation are time-consuming, costly, and\nerror-prone. By the way, computational methods have replaced these techniques.\nThese days, a vast amount of phosphorylation data is publicly accessible\nthrough many online databases. In this study, at first, all datasets of PTMs\nthat include phosphorylation sites (p-sites) were comprehensively reviewed.\nFurthermore, we showed that there are basically two main approaches for\nphosphorylation prediction by machine learning: End-to-End and conventional. We\ngave an overview for both of them. Also, we introduced 15 important feature\nextraction techniques which mostly have been used for conventional machine\nlearning methods",
          "link": "http://arxiv.org/abs/2108.04951",
          "publishedOn": "2021-08-12T01:56:23.145Z",
          "wordCount": 636,
          "title": "A Brief Review of Machine Learning Techniques for Protein Phosphorylation Sites Prediction. (arXiv:2108.04951v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidl_T/0/1/0/all/0/1\">Thomas Seidl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>",
          "description": "Transformers have improved the state-of-the-art across numerous tasks in\nsequence modeling. Besides the quadratic computational and memory complexity\nw.r.t the sequence length, the self-attention mechanism only processes\ninformation at the same scale, i.e., all attention heads are in the same\nresolution, resulting in the limited power of the Transformer. To remedy this,\nwe propose a novel and efficient structure named Adaptive Multi-Resolution\nAttention (AdaMRA for short), which scales linearly to sequence length in terms\nof time and space. Specifically, we leverage a multi-resolution multi-head\nattention mechanism, enabling attention heads to capture long-range contextual\ninformation in a coarse-to-fine fashion. Moreover, to capture the potential\nrelations between query representation and clues of different attention\ngranularities, we leave the decision of which resolution of attention to use to\nquery, which further improves the model's capacity compared to vanilla\nTransformer. In an effort to reduce complexity, we adopt kernel attention\nwithout degrading the performance. Extensive experiments on several benchmarks\ndemonstrate the effectiveness and efficiency of our model by achieving a\nstate-of-the-art performance-efficiency-memory trade-off. To facilitate AdaMRA\nutilization by the scientific community, the code implementation will be made\npublicly available.",
          "link": "http://arxiv.org/abs/2108.04962",
          "publishedOn": "2021-08-12T01:56:23.118Z",
          "wordCount": 618,
          "title": "Adaptive Multi-Resolution Attention with Linear Complexity. (arXiv:2108.04962v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasser_A/0/1/0/all/0/1\">Alex Nasser</a>",
          "description": "Proposed are alternative generator architectures for Boundary Equilibrium\nGenerative Adversarial Networks, motivated by Learning from Simulated and\nUnsupervised Images through Adversarial Training. It disentangles the need for\na noise-based latent space. The generator will operate mainly as a refiner\nnetwork to gain a photo-realistic presentation of the given synthetic images.\nIt also attempts to resolve the latent space's poorly understood properties by\neliminating the need for noise injection and replacing it with an image-based\nconcept. The new flexible and simple generator architecture will also give the\npower to control the trade-off between restrictive refinement and\nexpressiveness ability. Contrary to other available methods, this architecture\nwill not require a paired or unpaired dataset of real and synthetic images for\nthe training phase. Only a relatively small set of real images would suffice.",
          "link": "http://arxiv.org/abs/2108.04957",
          "publishedOn": "2021-08-12T01:56:23.088Z",
          "wordCount": 564,
          "title": "An Image-based Generator Architecture for Synthetic Image Refinement. (arXiv:2108.04957v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Joe Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaoka_Y/0/1/0/all/0/1\">Yusuke Inaoka</a>",
          "description": "This paper considers an extension of the linear non-Gaussian acyclic model\n(LiNGAM) that determines the causal order among variables from a dataset when\nthe variables are expressed by a set of linear equations, including noise. In\nparticular, we assume that the variables are binary. The existing LiNGAM\nassumes that no confounding is present, which is restrictive in practice. Based\non the concept of independent component analysis (ICA), this paper proposes an\nextended framework in which the mutual information among the noises is\nminimized. Another significant contribution is to reduce the realization of the\nshortest path problem. The distance between each pair of nodes expresses an\nassociated mutual information value, and the path with the minimum sum (KL\ndivergence) is sought. Although $p!$ mutual information values should be\ncompared, this paper dramatically reduces the computation when no confounding\nis present. The proposed algorithm finds the globally optimal solution, while\nthe existing locally greedily seek the order based on hypothesis testing. We\nuse the best estimator in the sense of Bayes/MDL that correctly detects\nindependence for mutual information estimation. Experiments using artificial\nand actual data show that the proposed version of LiNGAM achieves significantly\nbetter performance, particularly when confounding is present.",
          "link": "http://arxiv.org/abs/2108.04947",
          "publishedOn": "2021-08-12T01:56:23.075Z",
          "wordCount": 636,
          "title": "Causal Order Identification to Address Confounding: Binary Variables. (arXiv:2108.04947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Frances Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "Although the fairness community has recognized the importance of data,\nresearchers in the area primarily rely on UCI Adult when it comes to tabular\ndata. Derived from a 1994 US Census survey, this dataset has appeared in\nhundreds of research papers where it served as the basis for the development\nand comparison of many algorithmic fairness interventions. We reconstruct a\nsuperset of the UCI Adult data from available US Census sources and reveal\nidiosyncrasies of the UCI Adult dataset that limit its external validity. Our\nprimary contribution is a suite of new datasets derived from US Census surveys\nthat extend the existing data ecosystem for research on fair machine learning.\nWe create prediction tasks relating to income, employment, health,\ntransportation, and housing. The data span multiple years and all states of the\nUnited States, allowing researchers to study temporal shift and geographic\nvariation. We highlight a broad initial sweep of new empirical insights\nrelating to trade-offs between fairness criteria, performance of algorithmic\ninterventions, and the role of distribution shift based on our new datasets.\nOur findings inform ongoing debates, challenge some existing narratives, and\npoint to future research directions. Our datasets are available at\nhttps://github.com/zykls/folktables.",
          "link": "http://arxiv.org/abs/2108.04884",
          "publishedOn": "2021-08-12T01:56:23.049Z",
          "wordCount": 632,
          "title": "Retiring Adult: New Datasets for Fair Machine Learning. (arXiv:2108.04884v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_C/0/1/0/all/0/1\">Chong Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Songzi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adekkanattu_P/0/1/0/all/0/1\">Prakash Adekkanattu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_J/0/1/0/all/0/1\">Jyotishman Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thomas J. George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William R. Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "Social and behavioral determinants of health (SBDoH) have important roles in\nshaping people's health. In clinical research studies, especially comparative\neffectiveness studies, failure to adjust for SBDoH factors will potentially\ncause confounding issues and misclassification errors in either statistical\nanalyses and machine learning-based models. However, there are limited studies\nto examine SBDoH factors in clinical outcomes due to the lack of structured\nSBDoH information in current electronic health record (EHR) systems, while much\nof the SBDoH information is documented in clinical narratives. Natural language\nprocessing (NLP) is thus the key technology to extract such information from\nunstructured clinical text. However, there is not a mature clinical NLP system\nfocusing on SBDoH. In this study, we examined two state-of-the-art\ntransformer-based NLP models, including BERT and RoBERTa, to extract SBDoH\nconcepts from clinical narratives, applied the best performing model to extract\nSBDoH concepts on a lung cancer screening patient cohort, and examined the\ndifference of SBDoH information between NLP extracted results and structured\nEHRs (SBDoH information captured in standard vocabularies such as the\nInternational Classification of Diseases codes). The experimental results show\nthat the BERT-based NLP model achieved the best strict/lenient F1-score of\n0.8791 and 0.8999, respectively. The comparison between NLP extracted SBDoH\ninformation and structured EHRs in the lung cancer patient cohort of 864\npatients with 161,933 various types of clinical notes showed that much more\ndetailed information about smoking, education, and employment were only\ncaptured in clinical narratives and that it is necessary to use both clinical\nnarratives and structured EHRs to construct a more complete picture of\npatients' SBDoH factors.",
          "link": "http://arxiv.org/abs/2108.04949",
          "publishedOn": "2021-08-12T01:56:23.025Z",
          "wordCount": 743,
          "title": "A Study of Social and Behavioral Determinants of Health in Lung Cancer Patients Using Transformers-based Natural Language Processing Models. (arXiv:2108.04949v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04842",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Haah_J/0/1/0/all/0/1\">Jeongwan Haah</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kothari_R/0/1/0/all/0/1\">Robin Kothari</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tang_E/0/1/0/all/0/1\">Ewin Tang</a>",
          "description": "We study the problem of learning a Hamiltonian $H$ to precision\n$\\varepsilon$, supposing we are given copies of its Gibbs state\n$\\rho=\\exp(-\\beta H)/\\operatorname{Tr}(\\exp(-\\beta H))$ at a known inverse\ntemperature $\\beta$. Anshu, Arunachalam, Kuwahara, and Soleimanifar (Nature\nPhysics, 2021) recently studied the sample complexity (number of copies of\n$\\rho$ needed) of this problem for geometrically local $N$-qubit Hamiltonians.\nIn the high-temperature (low $\\beta$) regime, their algorithm has sample\ncomplexity poly$(N, 1/\\beta,1/\\varepsilon)$ and can be implemented with\npolynomial, but suboptimal, time complexity.\n\nIn this paper, we study the same question for a more general class of\nHamiltonians. We show how to learn the coefficients of a Hamiltonian to error\n$\\varepsilon$ with sample complexity $S = O(\\log N/(\\beta\\varepsilon)^{2})$ and\ntime complexity linear in the sample size, $O(S N)$. Furthermore, we prove a\nmatching lower bound showing that our algorithm's sample complexity is optimal,\nand hence our time complexity is also optimal.\n\nIn the appendix, we show that virtually the same algorithm can be used to\nlearn $H$ from a real-time evolution unitary $e^{-it H}$ in a small $t$ regime\nwith similar sample and time complexity.",
          "link": "http://arxiv.org/abs/2108.04842",
          "publishedOn": "2021-08-12T01:56:23.019Z",
          "wordCount": 624,
          "title": "Optimal learning of quantum Hamiltonians from high-temperature Gibbs states. (arXiv:2108.04842v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04964",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Long_J/0/1/0/all/0/1\">Jihao Long</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_L/0/1/0/all/0/1\">Lei Wu</a>",
          "description": "In this paper, we present a spectral-based approach to study the linear\napproximation of two-layer neural networks. We first consider the case of\nsingle neuron and show that the linear approximability, quantified by the\nKolmogorov width, is controlled by the eigenvalue decay of an associate kernel.\nThen, we show that similar results also hold for two-layer neural networks.\nThis spectral-based approach allows us to obtain upper bounds, lower bounds,\nand explicit hard examples in a united manner. In particular, these bounds\nimply that for networks activated by smooth functions, restricting the norms of\ninner-layer weights may significantly impair the expressiveness. By contrast,\nfor non-smooth activation functions, such as ReLU, the network expressiveness\nis independent of the inner-layer weight norms. In addition, we prove that for\na family of non-smooth activation functions, including ReLU, approximating any\nsingle neuron with random features suffers from the \\emph{curse of\ndimensionality}. This provides an explicit separation of expressiveness between\nneural networks and random feature models.",
          "link": "http://arxiv.org/abs/2108.04964",
          "publishedOn": "2021-08-12T01:56:23.002Z",
          "wordCount": 605,
          "title": "Linear approximability of two-layer neural networks: A comprehensive analysis based on spectral decay. (arXiv:2108.04964v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koyuncu_B/0/1/0/all/0/1\">Batuhan Koyuncu</a>",
          "description": "Deep generative models aim to learn underlying distributions that generate\nthe observed data. Given the fact that the generative distribution may be\ncomplex and intractable, deep latent variable models use probabilistic\nframeworks to learn more expressive joint probability distributions over the\ndata and their low-dimensional hidden variables. Learning complex probability\ndistributions over sequential data without any supervision is a difficult task\nfor deep generative models. Ordinary Differential Equation Variational\nAuto-Encoder (ODE2VAE) is a deep latent variable model that aims to learn\ncomplex distributions over high-dimensional sequential data and their\nlow-dimensional representations. ODE2VAE infers continuous latent dynamics of\nthe high-dimensional input in a low-dimensional hierarchical latent space. The\nhierarchical organization of the continuous latent space embeds a\nphysics-guided inductive bias in the model. In this paper, we analyze the\nlatent representations inferred by the ODE2VAE model over three different\nphysical motion datasets: bouncing balls, projectile motion, and simple\npendulum. Through our experiments, we explore the effects of the physics-guided\ninductive bias of the ODE2VAE model over the learned dynamical latent\nrepresentations. We show that the model is able to learn meaningful latent\nrepresentations to an extent without any supervision.",
          "link": "http://arxiv.org/abs/2108.04899",
          "publishedOn": "2021-08-12T01:56:22.989Z",
          "wordCount": 611,
          "title": "Analysis of ODE2VAE with Examples. (arXiv:2108.04899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04939",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vegetabile_B/0/1/0/all/0/1\">Brian G. Vegetabile</a>",
          "description": "Recent years have seen a swell in methods that focus on estimating\n\"individual treatment effects\". These methods are often focused on the\nestimation of heterogeneous treatment effects under ignorability assumptions.\nThis paper hopes to draw attention to the fact that there is nothing\nnecessarily \"individual\" about such effects under ignorability assumptions and\nisolating individual effects may require additional assumptions. Such\nindividual effects, more often than not, are more precisely described as\n\"conditional average treatment effects\" and confusion between the two has the\npotential to hinder advances in personalized and individualized effect\nestimation.",
          "link": "http://arxiv.org/abs/2108.04939",
          "publishedOn": "2021-08-12T01:56:22.984Z",
          "wordCount": 559,
          "title": "On the Distinction Between \"Conditional Average Treatment Effects\" (CATE) and \"Individual Treatment Effects\" (ITE) Under Ignorability Assumptions. (arXiv:2108.04939v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04883",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+You_H/0/1/0/all/0/1\">Huaiqian You</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Silling_S/0/1/0/all/0/1\">Stewart Silling</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+DElia_M/0/1/0/all/0/1\">Marta D&#x27;Elia</a>",
          "description": "Nonlocal models, including peridynamics, often use integral operators that\nembed lengthscales in their definition. However, the integrands in these\noperators are difficult to define from the data that are typically available\nfor a given physical system, such as laboratory mechanical property tests. In\ncontrast, molecular dynamics (MD) does not require these integrands, but it\nsuffers from computational limitations in the length and time scales it can\naddress. To combine the strengths of both methods and to obtain a\ncoarse-grained, homogenized continuum model that efficiently and accurately\ncaptures materials' behavior, we propose a learning framework to extract, from\nMD data, an optimal Linear Peridynamic Solid (LPS) model as a surrogate for MD\ndisplacements. To maximize the accuracy of the learnt model we allow the\nperidynamic influence function to be partially negative, while preserving the\nwell-posedness of the resulting model. To achieve this, we provide sufficient\nwell-posedness conditions for discretized LPS models with sign-changing\ninfluence functions and develop a constrained optimization algorithm that\nminimizes the equation residual while enforcing such solvability conditions.\nThis framework guarantees that the resulting model is mathematically\nwell-posed, physically consistent, and that it generalizes well to settings\nthat are different from the ones used during training. We illustrate the\nefficacy of the proposed approach with several numerical tests for single layer\ngraphene. Our two-dimensional tests show the robustness of the proposed\nalgorithm on validation data sets that include thermal noise, different domain\nshapes and external loadings, and discretizations substantially different from\nthe ones used for training.",
          "link": "http://arxiv.org/abs/2108.04883",
          "publishedOn": "2021-08-12T01:56:22.887Z",
          "wordCount": 690,
          "title": "A data-driven peridynamic continuum model for upscaling molecular dynamics. (arXiv:2108.04883v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04820",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Dong_T/0/1/0/all/0/1\">Thi Ngan Dong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khosla_M/0/1/0/all/0/1\">Megha Khosla</a>",
          "description": "Growing evidence from recent studies implies that microRNA or miRNA could\nserve as biomarkers in various complex human diseases. Since wet-lab\nexperiments are expensive and time-consuming, computational techniques for\nmiRNA-disease association prediction have attracted a lot of attention in\nrecent years. Data scarcity is one of the major challenges in building reliable\nmachine learning models. Data scarcity combined with the use of pre-calculated\nhand-crafted input features has led to problems of overfitting and data\nleakage.\n\nWe overcome the limitations of existing works by proposing a novel\nmulti-tasking convolution-based approach, which we refer to as MuCoMiD. MuCoMiD\nallows automatic feature extraction while incorporating knowledge from 4\nheterogeneous biological information sources (interactions between\nmiRNA/diseases and protein-coding genes (PCG), miRNA family information, and\ndisease ontology) in a multi-task setting which is a novel perspective and has\nnot been studied before. The use of multi-channel convolutions allows us to\nextract expressive representations while keeping the model linear and,\ntherefore, simple. To effectively test the generalization capability of our\nmodel, we construct large-scale experiments on standard benchmark datasets as\nwell as our proposed larger independent test sets and case studies. MuCoMiD\nshows an improvement of at least 5% in 5-fold CV evaluation on HMDDv2.0 and\nHMDDv3.0 datasets and at least 49% on larger independent test sets with unseen\nmiRNA and diseases over state-of-the-art approaches. We share our code for\nreproducibility and future research at\nhttps://git.l3s.uni-hannover.de/dong/cmtt.",
          "link": "http://arxiv.org/abs/2108.04820",
          "publishedOn": "2021-08-12T01:56:22.882Z",
          "wordCount": 667,
          "title": "MuCoMiD: A Multitask Convolutional Learning Framework for miRNA-Disease Association Prediction. (arXiv:2108.04820v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04879",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Axelrod_S/0/1/0/all/0/1\">Simon Axelrod</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shakhnovich_E/0/1/0/all/0/1\">Eugene Shakhnovich</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gomez_Bombarelli_R/0/1/0/all/0/1\">Rafael G&#xf3;mez-Bombarelli</a>",
          "description": "Light-induced chemical processes are ubiquitous in nature and have widespread\ntechnological applications. For example, the photoisomerization of azobenzene\nallows a drug with an azo scaffold to be activated with light. In principle,\nphotoswitches with useful reactive properties, such as high isomerization\nyields, can be identified through virtual screening with reactive simulations.\nIn practice these simulations are rarely used for screening, since they require\nhundreds of trajectories and expensive quantum chemical methods to account for\nnon-adiabatic excited state effects. Here we introduce a neural network\npotential to accelerate such simulations for azobenzene derivatives. The model,\nwhich is based on diabatic states, is called the \\textit{diabatic artificial\nneural network} (DANN). The network is six orders of magnitude faster than the\nquantum chemistry method used for training. DANN is transferable to molecules\noutside the training set, predicting quantum yields for unseen species that are\ncorrelated with experiment. We use the model to virtually screen 3,100\nhypothetical molecules, and identify several species with extremely high\nquantum yields. Our results pave the way for fast and accurate virtual\nscreening of photoactive compounds.",
          "link": "http://arxiv.org/abs/2108.04879",
          "publishedOn": "2021-08-12T01:56:22.876Z",
          "wordCount": 622,
          "title": "Excited state, non-adiabatic dynamics of large photoswitchable molecules using a chemically transferable machine learning potential. (arXiv:2108.04879v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sendera_M/0/1/0/all/0/1\">Marcin Sendera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1\">Marek &#x15a;mieja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maziarka_L/0/1/0/all/0/1\">&#x141;ukasz Maziarka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struski_L/0/1/0/all/0/1\">&#x141;ukasz Struski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1\">Przemys&#x142;aw Spurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>",
          "description": "We propose FlowSVDD -- a flow-based one-class classifier for anomaly/outliers\ndetection that realizes a well-known SVDD principle using deep learning tools.\nContrary to other approaches to deep SVDD, the proposed model is instantiated\nusing flow-based models, which naturally prevents from collapsing of bounding\nhypersphere into a single point. Experiments show that FlowSVDD achieves\ncomparable results to the current state-of-the-art methods and significantly\noutperforms related deep SVDD methods on benchmark datasets.",
          "link": "http://arxiv.org/abs/2108.04907",
          "publishedOn": "2021-08-12T01:56:22.854Z",
          "wordCount": 497,
          "title": "Flow-based SVDD for anomaly detection. (arXiv:2108.04907v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoran Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simmons_Edler_R/0/1/0/all/0/1\">Riley Simmons-Edler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daewon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackel_L/0/1/0/all/0/1\">Larry Jackel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_R/0/1/0/all/0/1\">Richard Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daniel Lee</a>",
          "description": "Perceiving obstacles and avoiding collisions is fundamental to the safe\noperation of a robot system, particularly when the robot must operate in highly\ndynamic human environments. Proximity detection using on-robot sensors can be\nused to avoid or mitigate impending collisions. However, existing proximity\nsensing methods are orientation and placement dependent, resulting in blind\nspots even with large numbers of sensors. In this paper, we introduce the\nphenomenon of the Leaky Surface Wave (LSW), a novel sensing modality, and\npresent AuraSense, a proximity detection system using the LSW. AuraSense is the\nfirst system to realize no-dead-spot proximity sensing for robot arms. It\nrequires only a single pair of piezoelectric transducers, and can easily be\napplied to off-the-shelf robots with minimal modifications. We further\nintroduce a set of signal processing techniques and a lightweight neural\nnetwork to address the unique challenges in using the LSW for proximity\nsensing. Finally, we demonstrate a prototype system consisting of a single\npiezoelectric element pair on a robot manipulator, which validates our design.\nWe conducted several micro benchmark experiments and performed more than 2000\non-robot proximity detection trials with various potential robot arm materials,\ncolliding objects, approach patterns, and robot movement patterns. AuraSense\nachieves 100% and 95.3% true positive proximity detection rates when the arm\napproaches static and mobile obstacles respectively, with a true negative rate\nover 99%, showing the real-world viability of this system.",
          "link": "http://arxiv.org/abs/2108.04867",
          "publishedOn": "2021-08-12T01:56:22.830Z",
          "wordCount": 686,
          "title": "AuraSense: Robot Collision Avoidance by Full Surface Proximity Detection. (arXiv:2108.04867v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changshu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guangchun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Attempting to fully exploit the rich information of topological structure and\nnode features for attributed graph, we introduce self-supervised learning\nmechanism to graph representation learning and propose a novel Self-supervised\nConsensus Representation Learning (SCRL) framework. In contrast to most\nexisting works that only explore one graph, our proposed SCRL method treats\ngraph from two perspectives: topology graph and feature graph. We argue that\ntheir embeddings should share some common information, which could serve as a\nsupervisory signal. Specifically, we construct the feature graph of node\nfeatures via k-nearest neighbor algorithm. Then graph convolutional network\n(GCN) encoders extract features from two graphs respectively. Self-supervised\nloss is designed to maximize the agreement of the embeddings of the same node\nin the topology graph and the feature graph. Extensive experiments on real\ncitation networks and social networks demonstrate the superiority of our\nproposed SCRL over the state-of-the-art methods on semi-supervised node\nclassification task. Meanwhile, compared with its main competitors, SCRL is\nrather efficient.",
          "link": "http://arxiv.org/abs/2108.04822",
          "publishedOn": "2021-08-12T01:56:22.808Z",
          "wordCount": 617,
          "title": "Self-supervised Consensus Representation Learning for Attributed Graph. (arXiv:2108.04822v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Konstantinov_A/0/1/0/all/0/1\">Andrei V. Konstantinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utkin_L/0/1/0/all/0/1\">Lev V. Utkin</a>",
          "description": "A new method for local and global explanation of the machine learning\nblack-box model predictions by tabular data is proposed. It is implemented as a\nsystem called AFEX (Attention-like Feature EXplanation) and consisting of two\nmain parts. The first part is a set of the one-feature neural subnetworks which\naim to get a specific representation for every feature in the form of a basis\nof shape functions. The subnetworks use shortcut connections with trainable\nparameters to improve the network performance. The second part of AFEX produces\nshape functions of features as the weighted sum of the basis shape functions\nwhere weights are computed by using an attention-like mechanism. AFEX\nidentifies pairwise interactions between features based on pairwise\nmultiplications of shape functions corresponding to different features. A\nmodification of AFEX with incorporating an additional surrogate model which\napproximates the black-box model is proposed. AFEX is trained end-to-end on a\nwhole dataset only once such that it does not require to train neural networks\nagain in the explanation stage. Numerical experiments with synthetic and real\ndata illustrate AFEX.",
          "link": "http://arxiv.org/abs/2108.04855",
          "publishedOn": "2021-08-12T01:56:22.795Z",
          "wordCount": 604,
          "title": "Attention-like feature explanation for tabular data. (arXiv:2108.04855v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1\">Andreas Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>",
          "description": "Natural Language Processing (NLP) models have become increasingly more\ncomplex and widespread. With recent developments in neural networks, a growing\nconcern is whether it is responsible to use these models. Concerns such as\nsafety and ethics can be partially addressed by providing explanations.\nFurthermore, when models do fail, providing explanations is paramount for\naccountability purposes. To this end, interpretability serves to provide these\nexplanations in terms that are understandable to humans. Central to what is\nunderstandable is how explanations are communicated. Therefore, this survey\nprovides a categorization of how recent interpretability methods communicate\nexplanations and discusses the methods in depth. Furthermore, the survey\nfocuses on post-hoc methods, which provide explanations after a model is\nlearned and generally model-agnostic. A common concern for this class of\nmethods is whether they accurately reflect the model. Hence, how these post-hoc\nmethods are evaluated is discussed throughout the paper.",
          "link": "http://arxiv.org/abs/2108.04840",
          "publishedOn": "2021-08-12T01:56:22.766Z",
          "wordCount": 582,
          "title": "Post-hoc Interpretability for Neural NLP: A Survey. (arXiv:2108.04840v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuesi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1\">Guangda Huzhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qianying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qing Da</a>",
          "description": "Ensemble models in E-commerce combine predictions from multiple sub-models\nfor ranking and revenue improvement. Industrial ensemble models are typically\ndeep neural networks, following the supervised learning paradigm to infer\nconversion rate given inputs from sub-models. However, this process has the\nfollowing two problems. Firstly, the point-wise scoring approach disregards the\nrelationships between items and leads to homogeneous displayed results, while\ndiversified display benefits user experience and revenue. Secondly, the\nlearning paradigm focuses on the ranking metrics and does not directly optimize\nthe revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework\nRAEGO, which replaces the ensemble model with a contextual Rank Aggregator (RA)\nand explores the best weights of sub-models by the Evaluator-Generator\nOptimization (EGO). To achieve the best online performance, we propose a new\nrank aggregation algorithm TournamentGreedy as a refinement of classic rank\naggregators, which also produces the best average weighted Kendall Tau Distance\n(KTD) amongst all the considered algorithms with quadratic time complexity.\nUnder the assumption that the best output list should be Pareto Optimal on the\nKTD metric for sub-models, we show that our RA algorithm has higher efficiency\nand coverage in exploring the optimal weights. Combined with the idea of\nBayesian Optimization and gradient descent, we solve the online contextual\nBlack-Box Optimization task that finds the optimal weights for sub-models given\na chosen RA model. RA-EGO has been deployed in our online system and has\nimproved the revenue significantly.",
          "link": "http://arxiv.org/abs/2107.08598",
          "publishedOn": "2021-08-11T01:55:24.192Z",
          "wordCount": 697,
          "title": "Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce. (arXiv:2107.08598v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1\">Antoine Wehenkel Damien Lanaspeze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1\">Bertrand Corn&#xe9;lusse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1\">Antonio Sutera</a>",
          "description": "Greater direct electrification of end-use sectors with a higher share of\nrenewables is one of the pillars to power a carbon-neutral society by 2050.\nHowever, in contrast to conventional power plants, renewable energy is subject\nto uncertainty raising challenges for their interaction with power systems.\nScenario-based probabilistic forecasting models have become an important tool\nto equip decision-makers. This paper proposes to present to the power systems\nforecasting practitioners a recent deep learning technique, the normalizing\nflows, to produce accurate scenario-based probabilistic forecasts that are\ncrucial to face the new challenges in power systems applications. The strength\nof this technique is to directly learn the stochastic multivariate distribution\nof the underlying process by maximizing the likelihood. Through comprehensive\nempirical evaluations using the open data of the Global Energy Forecasting\nCompetition 2014, we demonstrate that this methodology is competitive with\nother state-of-the-art deep learning generative models: generative adversarial\nnetworks and variational autoencoders. The models producing weather-based wind,\nsolar power, and load scenarios are properly compared both in terms of forecast\nvalue, by considering the case study of an energy retailer, and quality using\nseveral complementary metrics. The numerical experiments are simple and easily\nreproducible. Thus, we hope it will encourage other forecasting practitioners\nto test and use normalizing flows in power system applications such as bidding\non electricity markets, scheduling of power systems with high renewable energy\nsources penetration, energy management of virtual power plan or microgrids, and\nunit commitment.",
          "link": "http://arxiv.org/abs/2106.09370",
          "publishedOn": "2021-08-11T01:55:24.148Z",
          "wordCount": 741,
          "title": "A deep generative model for probabilistic energy forecasting in power systems: normalizing flows. (arXiv:2106.09370v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1\">Jisoo Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1\">Byunggook Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1\">Hyeokjun Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Deep neural networks continue to awe the world with their remarkable\nperformance. Their predictions, however, are prone to be corrupted by\nadversarial examples that are imperceptible to humans. Current efforts to\nimprove the robustness of neural networks against adversarial examples are\nfocused on developing robust training methods, which update the weights of a\nneural network in a more robust direction. In this work, we take a step beyond\ntraining of the weight parameters and consider the problem of designing an\nadversarially robust neural architecture with high intrinsic robustness. We\npropose AdvRush, a novel adversarial robustness-aware neural architecture\nsearch algorithm, based upon a finding that independent of the training method,\nthe intrinsic robustness of a neural network can be represented with the\nsmoothness of its input loss landscape. Through a regularizer that favors a\ncandidate architecture with a smoother input loss landscape, AdvRush\nsuccessfully discovers an adversarially robust neural architecture. Along with\na comprehensive theoretical motivation for AdvRush, we conduct an extensive\namount of experiments to demonstrate the efficacy of AdvRush on various\nbenchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust\naccuracy under FGSM attack after standard training and 50.04% robust accuracy\nunder AutoAttack after 7-step PGD adversarial training.",
          "link": "http://arxiv.org/abs/2108.01289",
          "publishedOn": "2021-08-11T01:55:24.118Z",
          "wordCount": 650,
          "title": "AdvRush: Searching for Adversarially Robust Neural Architectures. (arXiv:2108.01289v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sirui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anmoll Kumar Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Mohammad Qazim Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kwei-Herng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1\">Na Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Action recognition is a crucial task for video understanding. In this paper,\nwe present AutoVideo, a Python system for automated video action recognition.\nIt currently supports seven action recognition algorithms and various\npre-processing modules. Unlike the existing libraries that only provide model\nzoos, AutoVideo is built with the standard pipeline language. The basic\nbuilding block is primitive, which wraps a pre-processing module or an\nalgorithm with some hyperparameters. AutoVideo is highly modular and\nextendable. It can be easily combined with AutoML searchers. The pipeline\nlanguage is quite general so that we can easily enrich AutoVideo with\nalgorithms for various other video-related tasks in the future. AutoVideo is\nreleased under MIT license at https://github.com/datamllab/autovideo",
          "link": "http://arxiv.org/abs/2108.04212",
          "publishedOn": "2021-08-11T01:55:24.079Z",
          "wordCount": 598,
          "title": "AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.12975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1\">Florian Kromp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1\">Lukas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1\">Eva Bozsaky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1\">Inge Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1\">Wolfgang Doerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1\">Sabine Taschner-Mandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1\">Peter Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>",
          "description": "Separating and labeling each instance of a nucleus (instance-aware\nsegmentation) is the key challenge in segmenting single cell nuclei on\nfluorescence microscopy images. Deep Neural Networks can learn the implicit\ntransformation of a nuclear image into a probability map indicating the class\nmembership of each pixel (nucleus or background), but the use of\npost-processing steps to turn the probability map into a labeled object mask is\nerror-prone. This especially accounts for nuclear images of tissue sections and\nnuclear images across varying tissue preparations. In this work, we aim to\nevaluate the performance of state-of-the-art deep learning architectures to\nsegment nuclei in fluorescence images of various tissue origins and sample\npreparation types without post-processing. We compare architectures that\noperate on pixel to pixel translation and an architecture that operates on\nobject detection and subsequent locally applied segmentation. In addition, we\npropose a novel strategy to create artificial images to extend the training\nset. We evaluate the influence of ground truth annotation quality, image scale\nand segmentation complexity on segmentation performance. Results show that\nthree out of four deep learning architectures (U-Net, U-Net with ResNet34\nbackbone, Mask R-CNN) can segment fluorescent nuclear images on most of the\nsample preparation types and tissue origins with satisfactory segmentation\nperformance. Mask R-CNN, an architecture designed to address instance aware\nsegmentation tasks, outperforms other architectures. Equal nuclear mean size,\nconsistent nuclear annotations and the use of artificially generated images\nresult in overall acceptable precision and recall across different tissues and\nsample preparation types.",
          "link": "http://arxiv.org/abs/1907.12975",
          "publishedOn": "2021-08-11T01:55:24.073Z",
          "wordCount": 725,
          "title": "Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation. (arXiv:1907.12975v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1\">Tolulope Odetola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_F/0/1/0/all/0/1\">Faiq Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandefur_T/0/1/0/all/0/1\">Travis Sandefur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1\">Hawzhin Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Syed Rafay Hasan</a>",
          "description": "To reduce the time-to-market and access to state-of-the-art techniques, CNN\nhardware mapping and deployment on embedded accelerators are often outsourced\nto untrusted third parties, which is going to be more prevalent in futuristic\nartificial intelligence of things (AIoT) systems. These AIoT systems anticipate\nhorizontal collaboration among different resource-constrained AIoT node\ndevices, where CNN layers are partitioned and these devices collaboratively\ncompute complex CNN tasks. This horizontal collaboration opens another attack\nsurface to the CNN-based application, like inserting the hardware Trojans (HT)\ninto the embedded accelerators designed for the CNN. Therefore, there is a dire\nneed to explore this attack surface for designing secure embedded hardware\naccelerators for CNNs. Towards this goal, in this paper, we exploited this\nattack surface to propose an HT-based attack called FeSHI. Since in horizontal\ncollaboration of RC AIoT devices different sections of CNN architectures are\noutsourced to different untrusted third parties, the attacker may not know the\ninput image, but it has access to the layer-by-layer output feature maps\ninformation for the assigned sections of the CNN architecture. This attack\nexploits the statistical distribution, i.e., Gaussian distribution, of the\nlayer-by-layer feature maps of the CNN to design two triggers for stealthy HT\nwith a very low probability of triggering. Also, three different novel,\nstealthy and effective trigger designs are proposed.",
          "link": "http://arxiv.org/abs/2106.06895",
          "publishedOn": "2021-08-11T01:55:24.067Z",
          "wordCount": 694,
          "title": "FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack. (arXiv:2106.06895v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1\">Ludwig Bothmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strickroth_S/0/1/0/all/0/1\">Sven Strickroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casalicchio_G/0/1/0/all/0/1\">Giuseppe Casalicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1\">David R&#xfc;gamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheipl_F/0/1/0/all/0/1\">Fabian Scheipl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>",
          "description": "Education should not be a privilege but a common good. It should be openly\naccessible to everyone, with as few barriers as possible; even more so for key\ntechnologies such as Machine Learning (ML) and Data Science (DS). Open\nEducational Resources (OER) are a crucial factor for greater educational\nequity. In this paper, we describe the specific requirements for OER in ML and\nDS and argue that it is especially important for these fields to make source\nfiles publicly available, leading to Open Source Educational Resources (OSER).\nWe present our view on the collaborative development of OSER, the challenges\nthis poses, and first steps towards their solutions. We outline how OSER can be\nused for blended learning scenarios and share our experiences in university\neducation. Finally, we discuss additional challenges such as credit assignment\nor granting certificates.",
          "link": "http://arxiv.org/abs/2107.14330",
          "publishedOn": "2021-08-11T01:55:24.033Z",
          "wordCount": 605,
          "title": "Developing Open Source Educational Resources for Machine Learning and Data Science. (arXiv:2107.14330v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hanxu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>",
          "description": "Studying the implicit regularization effect of the nonlinear training\ndynamics of neural networks (NNs) is important for understanding why\nover-parameterized neural networks often generalize well on real dataset.\nEmpirically, for two-layer NN, existing works have shown that input weights of\nhidden neurons (the input weight of a hidden neuron consists of the weight from\nits input layer to the hidden neuron and its bias term) condense on isolated\norientations with a small initialization. The condensation dynamics implies\nthat NNs can learn features from the training data with a network configuration\neffectively equivalent to a much smaller network during the training. In this\nwork, we show that the multiple roots of activation function at origin\n(referred as ``multiplicity'') is a key factor for understanding the\ncondensation at the initial stage of training. Our experiments of multilayer\nnetworks suggest that the maximal number of condensed orientations is twice the\nmultiplicity of the activation function used. Our theoretical analysis of\ntwo-layer networks confirms experiments for two cases, one is for the\nactivation function of multiplicity one, which contains many common activation\nfunctions, and the other is for the one-dimensional input. This work makes a\nstep towards understanding how small initialization implicitly leads NNs to\ncondensation at initial training stage, which lays a foundation for the future\nstudy of the nonlinear dynamics of NNs and its implicit regularization effect\nat a later stage of training.",
          "link": "http://arxiv.org/abs/2105.11686",
          "publishedOn": "2021-08-11T01:55:23.984Z",
          "wordCount": 712,
          "title": "Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training. (arXiv:2105.11686v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Ting-Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>",
          "description": "In real applications, different computation-resource devices need\ndifferent-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,\nexisting methods either design multiple networks and train them independently,\nor construct depth-level/width-level dynamic neural networks which is hard to\nprove the accuracy of each sub-net. In this article, we propose an elegant\nDepth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets\nof similar architectures. To improve the generalization of sub-nets, we design\nthe Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to\nimplement knowledge transfer from the teacher (full-net) to multiple students\n(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to\nconstrain the posterior class probability consistency between full-net and\nsub-nets, and self-attention distillation on the same resolution feature of\ndifferent depth is addressed to drive more abundant feature representations of\nsub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in\na DDNN via the online knowledge distillation in each training iteration without\nextra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet\ndatasets demonstrate that sub-nets in DDNN with EKD training achieve better\nperformance than individually training networks while preserving the original\nperformance of full-nets.",
          "link": "http://arxiv.org/abs/2103.00793",
          "publishedOn": "2021-08-11T01:55:23.977Z",
          "wordCount": 674,
          "title": "Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network. (arXiv:2103.00793v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13430",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1\">Kiheiji Nishida</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1\">Kanta Naito</a>",
          "description": "This study proposes multivariate kernel density estimation by stagewise\nminimization algorithm based on $U$-divergence and a simple dictionary. The\ndictionary consists of an appropriate scalar bandwidth matrix and a part of the\noriginal data. The resulting estimator brings us data-adaptive weighting\nparameters and bandwidth matrices, and realizes a sparse representation of\nkernel density estimation. We develop the non-asymptotic error bound of\nestimator obtained via the proposed stagewise minimization algorithm. It is\nconfirmed from simulation studies that the proposed estimator performs\ncompetitive to or sometime better than other well-known density estimators.",
          "link": "http://arxiv.org/abs/2107.13430",
          "publishedOn": "2021-08-11T01:55:23.972Z",
          "wordCount": 548,
          "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary. (arXiv:2107.13430v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1\">Ana Lucic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolomei_G/0/1/0/all/0/1\">Gabriele Tolomei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>",
          "description": "Given the increasing promise of Graph Neural Networks (GNNs) in real-world\napplications, several methods have been developed for explaining their\npredictions. So far, these methods have primarily focused on generating\nsubgraphs that are especially relevant for a particular prediction. However,\nsuch methods do not provide a clear opportunity for recourse: given a\nprediction, we want to understand how the prediction can be changed in order to\nachieve a more desirable outcome. In this work, we propose a method for\ngenerating counterfactual (CF) explanations for GNNs: the minimal perturbation\nto the input (graph) data such that the prediction changes. Using only edge\ndeletions, we find that our method, CF-GNNExplainer can generate CF\nexplanations for the majority of instances across three widely used datasets\nfor GNN explanations, while removing less than 3 edges on average, with at\nleast 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes\nedges that are crucial for the original predictions, resulting in minimal CF\nexplanations.",
          "link": "http://arxiv.org/abs/2102.03322",
          "publishedOn": "2021-08-11T01:55:22.356Z",
          "wordCount": 647,
          "title": "CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks. (arXiv:2102.03322v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "The application of differential privacy to the training of deep neural\nnetworks holds the promise of allowing large-scale (decentralized) use of\nsensitive data while providing rigorous privacy guarantees to the individual.\nThe predominant approach to differentially private training of neural networks\nis DP-SGD, which relies on norm-based gradient clipping as a method for\nbounding sensitivity, followed by the addition of appropriately calibrated\nGaussian noise. In this work we propose NeuralDP, a technique for privatising\nactivations of some layer within a neural network, which by the post-processing\nproperties of differential privacy yields a differentially private network. We\nexperimentally demonstrate on two datasets (MNIST and Pediatric Pneumonia\nDataset (PPD)) that our method offers substantially improved privacy-utility\ntrade-offs compared to DP-SGD.",
          "link": "http://arxiv.org/abs/2107.14582",
          "publishedOn": "2021-08-11T01:55:22.291Z",
          "wordCount": 594,
          "title": "NeuralDP Differentially private neural networks by design. (arXiv:2107.14582v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kejie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohua Jia</a>",
          "description": "In recent years, many deep learning models have been adopted in autonomous\ndriving. At the same time, these models introduce new vulnerabilities that may\ncompromise the safety of autonomous vehicles. Specifically, recent studies have\ndemonstrated that adversarial attacks can cause a significant decline in\ndetection precision of deep learning-based 3D object detection models. Although\ndriving safety is the ultimate concern for autonomous driving, there is no\ncomprehensive study on the linkage between the performance of deep learning\nmodels and the driving safety of autonomous vehicles under adversarial attacks.\nIn this paper, we investigate the impact of two primary types of adversarial\nattacks, perturbation attacks and patch attacks, on the driving safety of\nvision-based autonomous vehicles rather than the detection precision of deep\nlearning models. In particular, we consider two state-of-the-art models in\nvision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving\nsafety, we propose an end-to-end evaluation framework with a set of driving\nsafety performance metrics. By analyzing the results of our extensive\nevaluation experiments, we find that (1) the attack's impact on the driving\nsafety of autonomous vehicles and the attack's impact on the precision of 3D\nobject detectors are decoupled, and (2) the DSGN model demonstrates stronger\nrobustness to adversarial attacks than the Stereo R-CNN model. In addition, we\nfurther investigate the causes behind the two findings with an ablation study.\nThe findings of this paper provide a new perspective to evaluate adversarial\nattacks and guide the selection of deep learning models in autonomous driving.",
          "link": "http://arxiv.org/abs/2108.02940",
          "publishedOn": "2021-08-11T01:55:22.260Z",
          "wordCount": 704,
          "title": "Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruoxuan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1\">Allison Koenecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1\">Michael Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1\">Susan Athey</a>",
          "description": "Analyzing observational data from multiple sources can be useful for\nincreasing statistical power to detect a treatment effect; however, practical\nconstraints such as privacy considerations may restrict individual-level\ninformation sharing across data sets. This paper develops federated methods\nthat only utilize summary-level information from heterogeneous data sets. Our\nfederated methods provide doubly-robust point estimates of treatment effects as\nwell as variance estimates. We derive the asymptotic distributions of our\nfederated estimators, which are shown to be asymptotically equivalent to the\ncorresponding estimators from the combined, individual-level data. We show that\nto achieve these properties, federated methods should be adjusted based on\nconditions such as whether models are correctly specified and stable across\nheterogeneous data sets.",
          "link": "http://arxiv.org/abs/2107.11732",
          "publishedOn": "2021-08-11T01:55:22.248Z",
          "wordCount": 591,
          "title": "Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xianbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.",
          "link": "http://arxiv.org/abs/2103.13689",
          "publishedOn": "2021-08-11T01:55:22.204Z",
          "wordCount": 670,
          "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hatt_T/0/1/0/all/0/1\">Tobias Hatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1\">Stefan Feuerriegel</a>",
          "description": "Decision-making often requires accurate estimation of treatment effects from\nobservational data. This is challenging as outcomes of alternative decisions\nare not observed and have to be estimated. Previous methods estimate outcomes\nbased on unconfoundedness but neglect any constraints that unconfoundedness\nimposes on the outcomes. In this paper, we propose a novel regularization\nframework for estimating average treatment effects that exploits\nunconfoundedness. To this end, we formalize unconfoundedness as an\northogonality constraint, which ensures that the outcomes are orthogonal to the\ntreatment assignment. This orthogonality constraint is then included in the\nloss function via a regularization. Based on our regularization framework, we\ndevelop deep orthogonal networks for unconfounded treatments (DONUT), which\nlearn outcomes that are orthogonal to the treatment assignment. Using a variety\nof benchmark datasets for estimating average treatment effects, we demonstrate\nthat DONUT outperforms the state-of-the-art substantially.",
          "link": "http://arxiv.org/abs/2101.08490",
          "publishedOn": "2021-08-11T01:55:22.198Z",
          "wordCount": 593,
          "title": "Estimating Average Treatment Effects via Orthogonal Regularization. (arXiv:2101.08490v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosel_F/0/1/0/all/0/1\">Fabian R&#xf6;sel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahrenkrog_Petersen_S/0/1/0/all/0/1\">Stephan A. Fahrenkrog-Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1\">Han van der Aa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1\">Matthias Weidlich</a>",
          "description": "To enable process analysis based on an event log without compromising the\nprivacy of individuals involved in process execution, a log may be anonymized.\nSuch anonymization strives to transform a log so that it satisfies provable\nprivacy guarantees, while largely maintaining its utility for process analysis.\nExisting techniques perform anonymization using simple, syntactic measures to\nidentify suitable transformation operations. This way, the semantics of the\nactivities referenced by the events in a trace are neglected, potentially\nleading to transformations in which events of unrelated activities are merged.\nTo avoid this and incorporate the semantics of activities during anonymization,\nwe propose to instead incorporate a distance measure based on feature learning.\nSpecifically, we show how embeddings of events enable the definition of a\ndistance measure for traces to guide event log anonymization. Our experiments\nwith real-world data indicate that anonymization using this measure, compared\nto a syntactic one, yields logs that are closer to the original log in various\ndimensions and, hence, have higher utility for process analysis.",
          "link": "http://arxiv.org/abs/2107.06578",
          "publishedOn": "2021-08-11T01:55:22.192Z",
          "wordCount": 657,
          "title": "A Distance Measure for Privacy-preserving Process Mining based on Feature Learning. (arXiv:2107.06578v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any\nuser-information. We optimize these faces, by using an evolutionary algorithm\nin the latent embedding space of the StyleGAN face generator. Multiple\nevolutionary strategies are compared, and we propose a novel approach that\nemploys a neural network in order to direct the search in the direction of\npromising samples, without adding fitness evaluations. The results we present\ndemonstrate that it is possible to obtain a high coverage of the LFW identities\n(over 40%) with less than 10 master faces, for three leading deep face\nrecognition systems.",
          "link": "http://arxiv.org/abs/2108.01077",
          "publishedOn": "2021-08-11T01:55:22.185Z",
          "wordCount": 607,
          "title": "Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaomin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaojin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loh_P/0/1/0/all/0/1\">Po-Ling Loh</a>",
          "description": "We investigate problems in penalized $M$-estimation, inspired by applications\nin machine learning debugging. Data are collected from two pools, one\ncontaining data with possibly contaminated labels, and the other which is known\nto contain only cleanly labeled points. We first formulate a general\nstatistical algorithm for identifying buggy points and provide rigorous\ntheoretical guarantees under the assumption that the data follow a linear\nmodel. We then present two case studies to illustrate the results of our\ngeneral theory and the dependence of our estimator on clean versus buggy\npoints. We further propose an algorithm for tuning parameter selection of our\nLasso-based algorithm and provide corresponding theoretical guarantees.\nFinally, we consider a two-person \"game\" played between a bug generator and a\ndebugger, where the debugger can augment the contaminated data set with cleanly\nlabeled versions of points in the original data pool. We establish a\ntheoretical result showing a sufficient condition under which the bug generator\ncan always fool the debugger. Nonetheless, we provide empirical results showing\nthat such a situation may not occur in practice, making it possible for natural\naugmentation strategies combined with our Lasso debugging algorithm to succeed.",
          "link": "http://arxiv.org/abs/2006.09009",
          "publishedOn": "2021-08-11T01:55:22.169Z",
          "wordCount": 651,
          "title": "Provable Training Set Debugging for Linear Regression. (arXiv:2006.09009v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingheng Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1\">Rob Gorbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1\">Dana Kuli&#x107;</a>",
          "description": "A promising characteristic of Deep Reinforcement Learning (DRL) is its\ncapability to learn optimal policy in an end-to-end manner without relying on\nfeature engineering. However, most approaches assume a fully observable state\nspace, i.e. fully observable Markov Decision Processes (MDPs). In real-world\nrobotics, this assumption is unpractical, because of issues such as sensor\nsensitivity limitations and sensor noise, and the lack of knowledge about\nwhether the observation design is complete or not. These scenarios lead to\nPartially Observable MDPs (POMDPs). In this paper, we propose\nLong-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient\n(LSTM-TD3) by introducing a memory component to TD3, and compare its\nperformance with other DRL algorithms in both MDPs and POMDPs. Our results\ndemonstrate the significant advantages of the memory component in addressing\nPOMDPs, including the ability to handle missing and noisy observation data.",
          "link": "http://arxiv.org/abs/2102.12344",
          "publishedOn": "2021-08-11T01:55:22.163Z",
          "wordCount": 617,
          "title": "Memory-based Deep Reinforcement Learning for POMDPs. (arXiv:2102.12344v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>",
          "description": "Conventional wisdom dictates that learning rate should be in the stable\nregime so that gradient-based algorithms don't blow up. This letter introduces\na simple scenario where an unstably large learning rate scheme leads to a super\nfast convergence, with the convergence rate depending only logarithmically on\nthe condition number of the problem. Our scheme uses a Cyclical Learning Rate\n(CLR) where we periodically take one large unstable step and several small\nstable steps to compensate for the instability. These findings also help\nexplain the empirical observations of [Smith and Topin, 2019] where they show\nthat CLR with a large maximum learning rate can dramatically accelerate\nlearning and lead to so-called \"super-convergence\". We prove that our scheme\nexcels in the problems where Hessian exhibits a bimodal spectrum and the\neigenvalues can be grouped into two clusters (small and large). The unstably\nlarge step is the key to enabling fast convergence over the small\neigen-spectrum.",
          "link": "http://arxiv.org/abs/2102.10734",
          "publishedOn": "2021-08-11T01:55:22.158Z",
          "wordCount": 602,
          "title": "Provable Super-Convergence with a Large Cyclical Learning Rate. (arXiv:2102.10734v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1\">Kim Hammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1\">Rolf Stadler</a>",
          "description": "We study automated intrusion prevention using reinforcement learning. In a\nnovel approach, we formulate the problem of intrusion prevention as an optimal\nstopping problem. This formulation allows us insight into the structure of the\noptimal policies, which turn out to be threshold based. Since the computation\nof the optimal defender policy using dynamic programming is not feasible for\npractical cases, we approximate the optimal policy through reinforcement\nlearning in a simulation environment. To define the dynamics of the simulation,\nwe emulate the target infrastructure and collect measurements. Our evaluations\nshow that the learned policies are close to optimal and that they indeed can be\nexpressed using thresholds.",
          "link": "http://arxiv.org/abs/2106.07160",
          "publishedOn": "2021-08-11T01:55:22.152Z",
          "wordCount": 591,
          "title": "Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufis</a>",
          "description": "EuroVoc is a multilingual thesaurus that was built for organizing the\nlegislative documentary of the European Union institutions. It contains\nthousands of categories at different levels of specificity and its descriptors\nare targeted by legal texts in almost thirty languages. In this work we propose\na unified framework for EuroVoc classification on 22 languages by fine-tuning\nmodern Transformer-based pretrained language models. We study extensively the\nperformance of our trained models and show that they significantly improve the\nresults obtained by a similar tool - JEX - on the same dataset. The code and\nthe fine-tuned models were open sourced, together with a programmatic interface\nthat eases the process of loading the weights of a trained model and of\nclassifying a new document.",
          "link": "http://arxiv.org/abs/2108.01139",
          "publishedOn": "2021-08-11T01:55:22.131Z",
          "wordCount": 594,
          "title": "PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Infante_G/0/1/0/all/0/1\">Guillermo Infante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1\">Anders Jonsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>",
          "description": "In this work we present a novel approach to hierarchical reinforcement\nlearning for linearly-solvable Markov decision processes. Our approach assumes\nthat the state space is partitioned, and the subtasks consist in moving between\nthe partitions. We represent value functions on several levels of abstraction,\nand use the compositionality of subtasks to estimate the optimal values of the\nstates in each partition. The policy is implicitly defined on these optimal\nvalue estimates, rather than being decomposed among the subtasks. As a\nconsequence, our approach can learn the globally optimal policy, and does not\nsuffer from the non-stationarity of high-level decisions. If several partitions\nhave equivalent dynamics, the subtasks of those partitions can be shared. If\nthe set of boundary states is smaller than the entire state space, our approach\ncan have significantly smaller sample complexity than that of a flat learner,\nand we validate this empirically in several experiments.",
          "link": "http://arxiv.org/abs/2106.15380",
          "publishedOn": "2021-08-11T01:55:22.114Z",
          "wordCount": 610,
          "title": "Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes. (arXiv:2106.15380v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>",
          "description": "Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.",
          "link": "http://arxiv.org/abs/2107.11170",
          "publishedOn": "2021-08-11T01:55:22.109Z",
          "wordCount": 744,
          "title": "Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04682",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Sun_X/0/1/0/all/0/1\">Xiangyan Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1\">Yuquan Lin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wu_L/0/1/0/all/0/1\">Lingjie Wu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xing_H/0/1/0/all/0/1\">Haoming Xing</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gao_M/0/1/0/all/0/1\">Minghong Gao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tan_S/0/1/0/all/0/1\">Suocheng Tan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ni_Z/0/1/0/all/0/1\">Zekun Ni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1\">Junqiu Wu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fan_J/0/1/0/all/0/1\">Jie Fan</a>",
          "description": "We have developed an end-to-end, retrosynthesis system, named ChemiRise, that\ncan propose complete retrosynthesis routes for organic compounds rapidly and\nreliably. The system was trained on a processed patent database of over 3\nmillion organic reactions. Experimental reactions were atom-mapped, clustered,\nand extracted into reaction templates. We then trained a graph convolutional\nneural network-based one-step reaction proposer using template embeddings and\ndeveloped a guiding algorithm on the directed acyclic graph (DAG) of chemical\ncompounds to find the best candidate to explore. The atom-mapping algorithm and\nthe one-step reaction proposer were benchmarked against previous studies and\nshowed better results. The final product was demonstrated by retrosynthesis\nroutes reviewed and rated by human experts, showing satisfying functionality\nand a potential productivity boost in real-life use cases.",
          "link": "http://arxiv.org/abs/2108.04682",
          "publishedOn": "2021-08-11T01:55:22.102Z",
          "wordCount": 597,
          "title": "ChemiRise: a data-driven retrosynthesis engine. (arXiv:2108.04682v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06559",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_W/0/1/0/all/0/1\">Winnie Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1\">Ricky T.Q. Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duvenaud_D/0/1/0/all/0/1\">David Duvenaud</a>",
          "description": "We perform scalable approximate inference in a continuous-depth Bayesian\nneural network family. In this model class, uncertainty about separate weights\nin each layer gives hidden units that follow a stochastic differential\nequation. We demonstrate gradient-based stochastic variational inference in\nthis infinite-parameter setting, producing arbitrarily-flexible approximate\nposteriors. We also derive a novel gradient estimator that approaches zero\nvariance as the approximate posterior over weights approaches the true\nposterior. This approach brings continuous-depth Bayesian neural nets to a\ncompetitive comparison against discrete-depth alternatives, while inheriting\nthe memory-efficient training and tunable precision of Neural ODEs.",
          "link": "http://arxiv.org/abs/2102.06559",
          "publishedOn": "2021-08-11T01:55:22.062Z",
          "wordCount": 555,
          "title": "Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations. (arXiv:2102.06559v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aman Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1\">Mayank Kothyari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>",
          "description": "Multimodal IR, spanning text corpus, knowledge graph and images, called\noutside knowledge visual question answering (OKVQA), is of much recent\ninterest. However, the popular data set has serious limitations. A surprisingly\nlarge fraction of queries do not assess the ability to integrate cross-modal\ninformation. Instead, some are independent of the image, some depend on\nspeculation, some require OCR or are otherwise answerable from the image alone.\nTo add to the above limitations, frequency-based guessing is very effective\nbecause of (unintended) widespread answer overlaps between the train and test\nfolds. Overall, it is hard to determine when state-of-the-art systems exploit\nthese weaknesses rather than really infer the answers, because they are opaque\nand their 'reasoning' process is uninterpretable. An equally important\nlimitation is that the dataset is designed for the quantitative assessment only\nof the end-to-end answer retrieval task, with no provision for assessing the\ncorrect(semantic) interpretation of the input query. In response, we identify a\nkey structural idiom in OKVQA ,viz., S3 (select, substitute and search), and\nbuild a new data set and challenge around it. Specifically, the questioner\nidentifies an entity in the image and asks a question involving that entity\nwhich can be answered only by consulting a knowledge graph or corpus passage\nmentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA\nannotated based on the structural idiom and (ii)S3VQA, a new dataset built from\nscratch. We also present a neural but structurally transparent OKVQA system,\nS3, that explicitly addresses our challenge dataset, and outperforms recent\ncompetitive baselines.",
          "link": "http://arxiv.org/abs/2103.05568",
          "publishedOn": "2021-08-11T01:55:22.046Z",
          "wordCount": 749,
          "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. (arXiv:2103.05568v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "Semi-supervised machine learning models learn from a (small) set of labeled\ntraining examples, and a (large) set of unlabeled training examples.\nState-of-the-art models can reach within a few percentage points of\nfully-supervised training, while requiring 100x less labeled data.\n\nWe study a new class of vulnerabilities: poisoning attacks that modify the\nunlabeled dataset. In order to be useful, unlabeled datasets are given strictly\nless review than labeled datasets, and adversaries can therefore poison them\neasily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%\nof the dataset size, we can manipulate a model trained on this poisoned dataset\nto misclassify arbitrary examples at test time (as any desired label). Our\nattacks are highly effective across datasets and semi-supervised learning\nmethods.\n\nWe find that more accurate methods (thus more likely to be used) are\nsignificantly more vulnerable to poisoning attacks, and as such better training\nmethods are unlikely to prevent this attack. To counter this we explore the\nspace of defenses, and propose two methods that mitigate our attack.",
          "link": "http://arxiv.org/abs/2105.01622",
          "publishedOn": "2021-08-11T01:55:22.041Z",
          "wordCount": 627,
          "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning. (arXiv:2105.01622v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhixiong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xintan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongchao Liu</a>",
          "description": "Graph neural networks (GNNs) have been popularly used in analyzing\ngraph-structured data, showing promising results in various applications such\nas node classification, link prediction and network recommendation. In this\npaper, we present a new graph attention neural network, namely GIPA, for\nattributed graph data learning. GIPA consists of three key components:\nattention, feature propagation and aggregation. Specifically, the attention\ncomponent introduces a new multi-layer perceptron based multi-head to generate\nbetter non-linear feature mapping and representation than conventional\nimplementations such as dot-product. The propagation component considers not\nonly node features but also edge features, which differs from existing GNNs\nthat merely consider node features. The aggregation component uses a residual\nconnection to generate the final embedding. We evaluate the performance of GIPA\nusing the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The\nexperimental results reveal that GIPA can beat the state-of-the-art models in\nterms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of\n$0.8700\\pm 0.0010$ and outperforms all the previous methods listed in the\nogbn-proteins leaderboard.",
          "link": "http://arxiv.org/abs/2105.06035",
          "publishedOn": "2021-08-11T01:55:21.993Z",
          "wordCount": 641,
          "title": "GIPA: General Information Propagation Algorithm for Graph Learning. (arXiv:2105.06035v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1\">Noriyuki Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>",
          "description": "We study continual learning for natural language instruction generation, by\nobserving human users' instruction execution. We focus on a collaborative\nscenario, where the system both acts and delegates tasks to human users using\nnatural language. We compare user execution of generated instructions to the\noriginal system intent as an indication to the system's success communicating\nits intent. We show how to use this signal to improve the system's ability to\ngenerate instructions via contextual bandit learning. In interaction with real\nusers, our system demonstrates dramatic improvements in its ability to generate\nlanguage over time.",
          "link": "http://arxiv.org/abs/2108.04812",
          "publishedOn": "2021-08-11T01:55:21.891Z",
          "wordCount": 553,
          "title": "Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior. (arXiv:2108.04812v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1808.00560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yijue Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Feng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1\">Elena Marchiori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1\">Sergios Theodoridis</a>",
          "description": "Spectral mixture (SM) kernels comprise a powerful class of kernels for\nGaussian processes (GPs) capable of discovering structurally complex patterns\nand modeling negative covariances. Being a linear superposition of\nquasi-periodical kernel components, the state-of-the-art SM kernel does not\nconsider component compression and dependency structures between components. In\nthis paper, we investigate the benefits of component compression and modeling\nof both time and phase delay structures between basis components in the SM\nkernel. By verifying the presence of dependencies between function components\nusing Gaussian conditionals and posterior covariance, we first propose a new SM\nkernel variant with a time and phase delay dependency structure (SMD) and then\nprovide a structure adaptation (SA) algorithm for the SMD. The SMD kernel is\nconstructed in two steps: first, time delay and phase delay are incorporated\ninto each basis component; next, cross-convolution between a basis component\nand the reversed complex conjugate of another basis component is performed,\nwhich yields a complex-valued and positive definite kernel incorporating\ndependency structures between basis components. The model compression and\ndependency sparsity of the SMD kernel can be obtained by using automatic\npruning in SA. We perform a thorough comparative experimental analysis of the\nSMD on both synthetic and real-life datasets. The results corroborate the\nefficacy of the dependency structure and SA in the SMD.",
          "link": "http://arxiv.org/abs/1808.00560",
          "publishedOn": "2021-08-11T01:55:21.885Z",
          "wordCount": 745,
          "title": "Novel Compressible Adaptive Spectral Mixture Kernels for Gaussian Processes with Sparse Time and Phase Delay Structures. (arXiv:1808.00560v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.11890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhaohan Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ren Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>",
          "description": "One intriguing property of deep neural networks (DNNs) is their inherent\nvulnerability to backdoor attacks -- a trojan model responds to\ntrigger-embedded inputs in a highly predictable manner while functioning\nnormally otherwise. Despite the plethora of prior work on DNNs for continuous\ndata (e.g., images), the vulnerability of graph neural networks (GNNs) for\ndiscrete-structured data (e.g., graphs) is largely unexplored, which is highly\nconcerning given their increasing use in security-sensitive domains. To bridge\nthis gap, we present GTA, the first backdoor attack on GNNs. Compared with\nprior work, GTA departs in significant ways: graph-oriented -- it defines\ntriggers as specific subgraphs, including both topological structures and\ndescriptive features, entailing a large design spectrum for the adversary;\ninput-tailored -- it dynamically adapts triggers to individual graphs, thereby\noptimizing both attack effectiveness and evasiveness; downstream model-agnostic\n-- it can be readily launched without knowledge regarding downstream models or\nfine-tuning strategies; and attack-extensible -- it can be instantiated for\nboth transductive (e.g., node classification) and inductive (e.g., graph\nclassification) tasks, constituting severe threats for a range of\nsecurity-critical applications. Through extensive evaluation using benchmark\ndatasets and state-of-the-art models, we demonstrate the effectiveness of GTA.\nWe further provide analytical justification for its effectiveness and discuss\npotential countermeasures, pointing to several promising research directions.",
          "link": "http://arxiv.org/abs/2006.11890",
          "publishedOn": "2021-08-11T01:55:21.878Z",
          "wordCount": 699,
          "title": "Graph Backdoor. (arXiv:2006.11890v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Izzo_C/0/1/0/all/0/1\">Cosimo Izzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhrati_R/0/1/0/all/0/1\">Ramin Okhrati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medda_F/0/1/0/all/0/1\">Francesca Medda</a>",
          "description": "Deep neural networks have gained momentum based on their accuracy, but their\ninterpretability is often criticised. As a result, they are labelled as black\nboxes. In response, several methods have been proposed in the literature to\nexplain their predictions. Among the explanatory methods, Shapley values is a\nfeature attribution method favoured for its robust theoretical foundation.\nHowever, the analysis of feature attributions using Shapley values requires\nchoosing a baseline that represents the concept of missingness. An arbitrary\nchoice of baseline could negatively impact the explanatory power of the method\nand possibly lead to incorrect interpretations. In this paper, we present a\nmethod for choosing a baseline according to a neutrality value: as a parameter\nselected by decision-makers, the point at which their choices are determined by\nthe model predictions being either above or below it. Hence, the proposed\nbaseline is set based on a parameter that depends on the actual use of the\nmodel. This procedure stands in contrast to how other baselines are set, i.e.\nwithout accounting for how the model is used. We empirically validate our\nchoice of baseline in the context of binary classification tasks, using two\ndatasets: a synthetic dataset and a dataset derived from the financial domain.",
          "link": "http://arxiv.org/abs/2006.04896",
          "publishedOn": "2021-08-11T01:55:21.873Z",
          "wordCount": 679,
          "title": "A Baseline for Shapley Values in MLPs: from Missingness to Neutrality. (arXiv:2006.04896v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1\">Jesper Kers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1\">Clarissa A. Cassol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1\">Joris J. Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1\">Najia Idrees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1\">Alik Farber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1\">Samir Haroon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1\">Kevin P. Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Suvranu Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1\">Vipul C. Chitalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1\">Vijaya B. Kolachalama</a>",
          "description": "Development of deep learning systems for biomedical segmentation often\nrequires access to expert-driven, manually annotated datasets. If more than a\nsingle expert is involved in the annotation of the same images, then the\ninter-expert agreement is not necessarily perfect, and no single expert\nannotation can precisely capture the so-called ground truth of the regions of\ninterest on all images. Also, it is not trivial to generate a reference\nestimate using annotations from multiple experts. Here we present a deep neural\nnetwork, defined as U-Net-and-a-half, which can simultaneously learn from\nannotations performed by multiple experts on the same set of images.\nU-Net-and-a-half contains a convolutional encoder to generate features from the\ninput images, multiple decoders that allow simultaneous learning from image\nmasks obtained from annotations that were independently generated by multiple\nexperts, and a shared low-dimensional feature space. To demonstrate the\napplicability of our framework, we used two distinct datasets from digital\npathology and radiology, respectively. Specifically, we trained two separate\nmodels using pathologist-driven annotations of glomeruli on whole slide images\nof human kidney biopsies (10 patients), and radiologist-driven annotations of\nlumen cross-sections of human arteriovenous fistulae obtained from\nintravascular ultrasound images (10 patients), respectively. The models based\non U-Net-and-a-half exceeded the performance of the traditional U-Net models\ntrained on single expert annotations alone, thus expanding the scope of\nmultitask learning in the context of biomedical image segmentation.",
          "link": "http://arxiv.org/abs/2108.04658",
          "publishedOn": "2021-08-11T01:55:21.851Z",
          "wordCount": 697,
          "title": "U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations. (arXiv:2108.04658v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Moyu Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinning Zhu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhong Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yang Ji</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feng Pan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Changchuan Yin</a> (1) ((1) Beijing University of Posts and Telecommunications)",
          "description": "With the increasing demands of personalized learning, knowledge tracing has\nbecome important which traces students' knowledge states based on their\nhistorical practices. Factor analysis methods mainly use two kinds of factors\nwhich are separately related to students and questions to model students'\nknowledge states. These methods use the total number of attempts of students to\nmodel students' learning progress and hardly highlight the impact of the most\nrecent relevant practices. Besides, current factor analysis methods ignore rich\ninformation contained in questions. In this paper, we propose Multi-Factors\nAware Dual-Attentional model (MF-DAKT) which enriches question representations\nand utilizes multiple factors to model students' learning progress based on a\ndual-attentional mechanism. More specifically, we propose a novel\nstudent-related factor which records the most recent attempts on relevant\nconcepts of students to highlight the impact of recent exercises. To enrich\nquestions representations, we use a pre-training method to incorporate two\nkinds of question information including questions' relation and difficulty\nlevel. We also add a regularization term about questions' difficulty level to\nrestrict pre-trained question representations to fine-tuning during the process\nof predicting students' performance. Moreover, we apply a dual-attentional\nmechanism to differentiate contributions of factors and factor interactions to\nfinal prediction in different practice records. At last, we conduct experiments\non several real-world datasets and results show that MF-DAKT can outperform\nexisting knowledge tracing methods. We also conduct several studies to validate\nthe effects of each component of MF-DAKT.",
          "link": "http://arxiv.org/abs/2108.04741",
          "publishedOn": "2021-08-11T01:55:21.846Z",
          "wordCount": 691,
          "title": "Multi-Factors Aware Dual-Attentional Knowledge Tracing. (arXiv:2108.04741v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heaton_J/0/1/0/all/0/1\">Jeff Heaton</a>",
          "description": "Deep learning is a group of exciting new technologies for neural networks.\nThrough a combination of advanced training techniques and neural network\narchitectural components, it is now possible to create neural networks that can\nhandle tabular data, images, text, and audio as both input and output. Deep\nlearning allows a neural network to learn hierarchies of information in a way\nthat is like the function of the human brain. This course will introduce the\nstudent to classic neural network structures, Convolution Neural Networks\n(CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU),\nGeneral Adversarial Networks (GAN), and reinforcement learning. Application of\nthese architectures to computer vision, time series, security, natural language\nprocessing (NLP), and data generation will be covered. High-Performance\nComputing (HPC) aspects will demonstrate how deep learning can be leveraged\nboth on graphical processing units (GPUs), as well as grids. Focus is primarily\nupon the application of deep learning to problems, with some introduction to\nmathematical foundations. Readers will use the Python programming language to\nimplement deep learning using Google TensorFlow and Keras. It is not necessary\nto know Python prior to this book; however, familiarity with at least one\nprogramming language is assumed.",
          "link": "http://arxiv.org/abs/2009.05673",
          "publishedOn": "2021-08-11T01:55:21.821Z",
          "wordCount": 656,
          "title": "Applications of Deep Neural Networks. (arXiv:2009.05673v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>",
          "description": "Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017)\nis a classical federated learning algorithm in which clients run multiple local\nSGD steps before communicating their update to an orchestrating server. We\npropose a new federated learning algorithm, FedPAGE, able to further reduce the\ncommunication complexity by utilizing the recent optimal PAGE method (Li et\nal., 2021) instead of plain SGD in FedAvg. We show that FedPAGE uses much fewer\ncommunication rounds than previous local methods for both federated convex and\nnonconvex optimization. Concretely, 1) in the convex setting, the number of\ncommunication rounds of FedPAGE is $O(\\frac{N^{3/4}}{S\\epsilon})$, improving\nthe best-known result $O(\\frac{N}{S\\epsilon})$ of SCAFFOLD (Karimireddy et\nal.,2020) by a factor of $N^{1/4}$, where $N$ is the total number of clients\n(usually is very large in federated learning), $S$ is the sampled subset of\nclients in each communication round, and $\\epsilon$ is the target error; 2) in\nthe nonconvex setting, the number of communication rounds of FedPAGE is\n$O(\\frac{\\sqrt{N}+S}{S\\epsilon^2})$, improving the best-known result\n$O(\\frac{N^{2/3}}{S^{2/3}\\epsilon^2})$ of SCAFFOLD (Karimireddy et al.,2020) by\na factor of $N^{1/6}S^{1/3}$, if the sampled clients $S\\leq \\sqrt{N}$. Note\nthat in both settings, the communication cost for each round is the same for\nboth FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new state-of-the-art\nresults in terms of communication complexity for both federated convex and\nnonconvex optimization.",
          "link": "http://arxiv.org/abs/2108.04755",
          "publishedOn": "2021-08-11T01:55:21.815Z",
          "wordCount": 677,
          "title": "FedPAGE: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning. (arXiv:2108.04755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_N/0/1/0/all/0/1\">Nat&#xe1;lia V. N. Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramo_L/0/1/0/all/0/1\">L. Raul Abramo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirata_N/0/1/0/all/0/1\">Nina S. Hirata</a>",
          "description": "Errors in measurements are key to weighting the value of data, but are often\nneglected in Machine Learning (ML). We show how Convolutional Neural Networks\n(CNNs) are able to learn about the context and patterns of signal and noise,\nleading to improvements in the performance of classification methods. We\nconstruct a model whereby two classes of objects follow an underlying Gaussian\ndistribution, and where the features (the input data) have varying, but known,\nlevels of noise. This model mimics the nature of scientific data sets, where\nthe noises arise as realizations of some random processes whose underlying\ndistributions are known. The classification of these objects can then be\nperformed using standard statistical techniques (e.g., least-squares\nminimization or Markov-Chain Monte Carlo), as well as ML techniques. This\nallows us to take advantage of a maximum likelihood approach to object\nclassification, and to measure the amount by which the ML methods are\nincorporating the information in the input data uncertainties. We show that,\nwhen each data point is subject to different levels of noise (i.e., noises with\ndifferent distribution functions), that information can be learned by the CNNs,\nraising the ML performance to at least the same level of the least-squares\nmethod -- and sometimes even surpassing it. Furthermore, we show that, with\nvarying noise levels, the confidence of the ML classifiers serves as a proxy\nfor the underlying cumulative distribution function, but only if the\ninformation about specific input data uncertainties is provided to the CNNs.",
          "link": "http://arxiv.org/abs/2108.04742",
          "publishedOn": "2021-08-11T01:55:21.809Z",
          "wordCount": 702,
          "title": "The information of attribute uncertainties: what convolutional neural networks can learn about errors in input data. (arXiv:2108.04742v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_a/0/1/0/all/0/1\">and Jianjun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zheng-Hua Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>",
          "description": "Due to lack of data, overfitting ubiquitously exists in real-world\napplications of deep neural networks (DNNs). We propose advanced dropout, a\nmodel-free methodology, to mitigate overfitting and improve the performance of\nDNNs. The advanced dropout technique applies a model-free and easily\nimplemented distribution with parametric prior, and adaptively adjusts dropout\nrate. Specifically, the distribution parameters are optimized by stochastic\ngradient variational Bayes in order to carry out an end-to-end training. We\nevaluate the effectiveness of the advanced dropout against nine dropout\ntechniques on seven computer vision datasets (five small-scale datasets and two\nlarge-scale datasets) with various base models. The advanced dropout\noutperforms all the referred techniques on all the datasets.We further compare\nthe effectiveness ratios and find that advanced dropout achieves the highest\none on most cases. Next, we conduct a set of analysis of dropout rate\ncharacteristics, including convergence of the adaptive dropout rate, the\nlearned distributions of dropout masks, and a comparison with dropout rate\ngeneration without an explicit distribution. In addition, the ability of\noverfitting prevention is evaluated and confirmed. Finally, we extend the\napplication of the advanced dropout to uncertainty inference, network pruning,\ntext classification, and regression. The proposed advanced dropout is also\nsuperior to the corresponding referred methods. Codes are available at\nhttps://github.com/PRIS-CV/AdvancedDropout.",
          "link": "http://arxiv.org/abs/2010.05244",
          "publishedOn": "2021-08-11T01:55:21.803Z",
          "wordCount": 693,
          "title": "Advanced Dropout: A Model-free Methodology for Bayesian Dropout Optimization. (arXiv:2010.05244v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lukina_A/0/1/0/all/0/1\">Anna Lukina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1\">Christian Schilling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1\">Thomas A. Henzinger</a>",
          "description": "Neural-network classifiers achieve high accuracy when predicting the class of\nan input that they were trained to identify. Maintaining this accuracy in\ndynamic environments, where inputs frequently fall outside the fixed set of\ninitially known classes, remains a challenge. The typical approach is to detect\ninputs from novel classes and retrain the classifier on an augmented dataset.\nHowever, not only the classifier but also the detection mechanism needs to\nadapt in order to distinguish between newly learned and yet unknown input\nclasses. To address this challenge, we introduce an algorithmic framework for\nactive monitoring of a neural network. A monitor wrapped in our framework\noperates in parallel with the neural network and interacts with a human user\nvia a series of interpretable labeling queries for incremental adaptation. In\naddition, we propose an adaptive quantitative monitor to improve precision. An\nexperimental evaluation on a diverse set of benchmarks with varying numbers of\nclasses confirms the benefits of our active monitoring framework in dynamic\nscenarios.",
          "link": "http://arxiv.org/abs/2009.06429",
          "publishedOn": "2021-08-11T01:55:21.797Z",
          "wordCount": 645,
          "title": "Into the Unknown: Active Monitoring of Neural Networks. (arXiv:2009.06429v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonassi_F/0/1/0/all/0/1\">Fabio Bonassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scattolini_R/0/1/0/all/0/1\">Riccardo Scattolini</a>",
          "description": "Owing to their superior modeling capabilities, gated Recurrent Neural\nNetworks (RNNs), such as Gated Recurrent Units (GRUs) and Long Short-Term\nMemory networks (LSTMs), have become popular tools for learning dynamical\nsystems. This paper aims to discuss how these networks can be adopted for the\nsynthesis of Internal Model Control (IMC) architectures. To this end, a first\ngated RNN is used to learn a model of the unknown input-output stable plant.\nThen, another gated RNN approximating the model inverse is trained. The\nproposed scheme is able to cope with the saturation of the control variables,\nand it can be deployed on low-power embedded controllers since it does not\nrequire any online computation. The approach is then tested on the Quadruple\nTank benchmark system, resulting in satisfactory closed-loop performances.",
          "link": "http://arxiv.org/abs/2108.04585",
          "publishedOn": "2021-08-11T01:55:21.791Z",
          "wordCount": 596,
          "title": "Recurrent neural network-based Internal Model Control of unknown nonlinear stable systems. (arXiv:2108.04585v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulanikar_A/0/1/0/all/0/1\">Abhiram Anand Gulanikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1\">Lov Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1\">Lalita Bhanu Murthy Neti</a>",
          "description": "A code smell is a surface indicator of an inherent problem in the system,\nmost often due to deviation from standard coding practices on the developers\npart during the development phase. Studies observe that code smells made the\ncode more susceptible to call for modifications and corrections than code that\ndid not contain code smells. Restructuring the code at the early stage of\ndevelopment saves the exponentially increasing amount of effort it would\nrequire to address the issues stemming from the presence of these code smells.\nInstead of using traditional features to detect code smells, we use user\ncomments to manually construct features to predict code smells. We use three\nExtreme learning machine kernels over 629 packages to identify eight code\nsmells by leveraging feature engineering aspects and using sampling techniques.\nOur findings indicate that the radial basis functional kernel performs best out\nof the three kernel methods with a mean accuracy of 98.52.",
          "link": "http://arxiv.org/abs/2108.04656",
          "publishedOn": "2021-08-11T01:55:21.780Z",
          "wordCount": 628,
          "title": "Empirical Analysis on Effectiveness of NLP Methods for Predicting Code Smell. (arXiv:2108.04656v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baier_L/0/1/0/all/0/1\">Lucas Baier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1\">Niklas K&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoffer_J/0/1/0/all/0/1\">Jakob Sch&#xf6;ffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satzger_G/0/1/0/all/0/1\">Gerhard Satzger</a>",
          "description": "As a reaction to the high infectiousness and lethality of the COVID-19 virus,\ncountries around the world have adopted drastic policy measures to contain the\npandemic. However, it remains unclear which effect these measures, so-called\nnon-pharmaceutical interventions (NPIs), have on the spread of the virus. In\nthis article, we use machine learning and apply drift detection methods in a\nnovel way to predict the time lag of policy interventions with respect to the\ndevelopment of daily case numbers of COVID-19 across 9 European countries and\n28 US states. Our analysis shows that there are, on average, more than two\nweeks between NPI enactment and a drift in the case numbers.",
          "link": "http://arxiv.org/abs/2012.03728",
          "publishedOn": "2021-08-11T01:55:21.757Z",
          "wordCount": 630,
          "title": "Utilizing Concept Drift for Measuring the Effectiveness of Policy Interventions: The Case of the COVID-19 Pandemic. (arXiv:2012.03728v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakano_F/0/1/0/all/0/1\">Felipe Kenji Nakano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pliakos_K/0/1/0/all/0/1\">Konstantinos Pliakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vens_C/0/1/0/all/0/1\">Celine Vens</a>",
          "description": "Recently, deep neural networks have expanded the state-of-art in various\nscientific fields and provided solutions to long standing problems across\nmultiple application domains. Nevertheless, they also suffer from weaknesses\nsince their optimal performance depends on massive amounts of training data and\nthe tuning of an extended number of parameters. As a countermeasure, some\ndeep-forest methods have been recently proposed, as efficient and low-scale\nsolutions. Despite that, these approaches simply employ label classification\nprobabilities as induced features and primarily focus on traditional\nclassification and regression tasks, leaving multi-output prediction\nunder-explored. Moreover, recent work has demonstrated that tree-embeddings are\nhighly representative, especially in structured output prediction. In this\ndirection, we propose a novel deep tree-ensemble (DTE) model, where every layer\nenriches the original feature set with a representation learning component\nbased on tree-embeddings. In this paper, we specifically focus on two\nstructured output prediction tasks, namely multi-label classification and\nmulti-target regression. We conducted experiments using multiple benchmark\ndatasets and the obtained results confirm that our method provides superior\nresults to state-of-the-art methods in both tasks.",
          "link": "http://arxiv.org/abs/2011.02829",
          "publishedOn": "2021-08-11T01:55:21.749Z",
          "wordCount": 626,
          "title": "Deep tree-ensembles for multi-output prediction. (arXiv:2011.02829v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qingpeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yiyuan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongle Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>",
          "description": "Data processing and analytics are fundamental and pervasive. Algorithms play\na vital role in data processing and analytics where many algorithm designs have\nincorporated heuristics and general rules from human knowledge and experience\nto improve their effectiveness. Recently, reinforcement learning, deep\nreinforcement learning (DRL) in particular, is increasingly explored and\nexploited in many areas because it can learn better strategies in complicated\nenvironments it is interacting with than statically designed algorithms.\nMotivated by this trend, we provide a comprehensive review of recent works\nfocusing on utilizing deep reinforcement learning to improve data processing\nand analytics. First, we present an introduction to key concepts, theories, and\nmethods in deep reinforcement learning. Next, we discuss deep reinforcement\nlearning deployment on database systems, facilitating data processing and\nanalytics in various aspects, including data organization, scheduling, tuning,\nand indexing. Then, we survey the application of deep reinforcement learning in\ndata processing and analytics, ranging from data preparation, natural language\ninterface to healthcare, fintech, etc. Finally, we discuss important open\nchallenges and future research directions of using deep reinforcement learning\nin data processing and analytics.",
          "link": "http://arxiv.org/abs/2108.04526",
          "publishedOn": "2021-08-11T01:55:21.725Z",
          "wordCount": 629,
          "title": "A Survey on Deep Reinforcement Learning for Data Processing and Analytics. (arXiv:2108.04526v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01184",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1\">Renato Vicente</a>",
          "description": "In supervised learning, training and test datasets are often sampled from\ndistinct distributions. Domain adaptation techniques are thus required.\nCovariate shift adaptation yields good generalization performance when domains\ndiffer only by the marginal distribution of features. Covariate shift\nadaptation is usually implemented using importance weighting, which may fail,\naccording to common wisdom, due to small effective sample sizes (ESS). Previous\nresearch argues this scenario is more common in high-dimensional settings.\nHowever, how effective sample size, dimensionality, and model\nperformance/generalization are formally related in supervised learning,\nconsidering the context of covariate shift adaptation, is still somewhat\nobscure in the literature. Thus, a main challenge is presenting a unified\ntheory connecting those points. Hence, in this paper, we focus on building a\nunified view connecting the ESS, data dimensionality, and generalization in the\ncontext of covariate shift adaptation. Moreover, we also demonstrate how\ndimensionality reduction or feature selection can increase the ESS, and argue\nthat our results support dimensionality reduction before covariate shift\nadaptation as a good practice.",
          "link": "http://arxiv.org/abs/2010.01184",
          "publishedOn": "2021-08-11T01:55:21.718Z",
          "wordCount": 634,
          "title": "Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation. (arXiv:2010.01184v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Unlu_A/0/1/0/all/0/1\">Ali Unlu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>",
          "description": "We develop variational Laplace for Bayesian neural networks (BNNs) which\nexploits a local approximation of the curvature of the likelihood to estimate\nthe ELBO without the need for stochastic sampling of the neural-network\nweights. The Variational Laplace objective is simple to evaluate, as it is (in\nessence) the log-likelihood, plus weight-decay, plus a squared-gradient\nregularizer. Variational Laplace gave better test performance and expected\ncalibration errors than maximum a-posteriori inference and standard\nsampling-based variational inference, despite using the same variational\napproximate posterior. Finally, we emphasise care needed in benchmarking\nstandard VI as there is a risk of stopping before the variance parameters have\nconverged. We show that early-stopping can be avoided by increasing the\nlearning rate for the variance parameters.",
          "link": "http://arxiv.org/abs/2011.10443",
          "publishedOn": "2021-08-11T01:55:21.705Z",
          "wordCount": 562,
          "title": "Variational Laplace for Bayesian neural networks. (arXiv:2011.10443v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07588",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Deep learning motivated by convolutional neural networks has been highly\nsuccessful in a range of medical imaging problems like image classification,\nimage segmentation, image synthesis etc. However for validation and\ninterpretability, not only do we need the predictions made by the model but\nalso how confident it is while making those predictions. This is important in\nsafety critical applications for the people to accept it. In this work, we used\nan encoder decoder architecture based on variational inference techniques for\nsegmenting brain tumour images. We evaluate our work on the publicly available\nBRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over\nUnion (IOU) as the evaluation metrics. Our model is able to segment brain\ntumours while taking into account both aleatoric uncertainty and epistemic\nuncertainty in a principled bayesian manner.",
          "link": "http://arxiv.org/abs/2008.07588",
          "publishedOn": "2021-08-11T01:55:21.693Z",
          "wordCount": 602,
          "title": "Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation. (arXiv:2008.07588v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.04575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>",
          "description": "Channel attention mechanisms have been commonly applied in many visual tasks\nfor effective performance improvement. It is able to reinforce the informative\nchannels as well as to suppress the useless channels. Recently, different\nchannel attention modules have been proposed and implemented in various ways.\nGenerally speaking, they are mainly based on convolution and pooling\noperations. In this paper, we propose Gaussian process embedded channel\nattention (GPCA) module and further interpret the channel attention schemes in\na probabilistic way. The GPCA module intends to model the correlations among\nthe channels, which are assumed to be captured by beta distributed variables.\nAs the beta distribution cannot be integrated into the end-to-end training of\nconvolutional neural networks (CNNs) with a mathematically tractable solution,\nwe utilize an approximation of the beta distribution to solve this problem. To\nspecify, we adapt a Sigmoid-Gaussian approximation, in which the Gaussian\ndistributed variables are transferred into the interval [0,1]. The Gaussian\nprocess is then utilized to model the correlations among different channels. In\nthis case, a mathematically tractable solution is derived. The GPCA module can\nbe efficiently implemented and integrated into the end-to-end training of the\nCNNs. Experimental results demonstrate the promising performance of the\nproposed GPCA module. Codes are available at https://github.com/PRIS-CV/GPCA.",
          "link": "http://arxiv.org/abs/2003.04575",
          "publishedOn": "2021-08-11T01:55:21.678Z",
          "wordCount": 684,
          "title": "GPCA: A Probabilistic Framework for Gaussian Process Embedded Channel Attention. (arXiv:2003.04575v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_T/0/1/0/all/0/1\">Tanmay G. Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1\">Lov Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1\">Lalita Bhanu Murthy Neti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Aneesh Krishna</a>",
          "description": "Code Smell, similar to a bad smell, is a surface indication of something\ntainted but in terms of software writing practices. This metric is an\nindication of a deeper problem lies within the code and is associated with an\nissue which is prominent to experienced software developers with acceptable\ncoding practices. Recent studies have often observed that codes having code\nsmells are often prone to a higher probability of change in the software\ndevelopment cycle. In this paper, we developed code smell prediction models\nwith the help of features extracted from source code to predict eight types of\ncode smell. Our work also presents the application of data sampling techniques\nto handle class imbalance problem and feature selection techniques to find\nrelevant feature sets. Previous studies had made use of techniques such as\nNaive - Bayes and Random forest but had not explored deep learning methods to\npredict code smell. A total of 576 distinct Deep Learning models were trained\nusing the features and datasets mentioned above. The study concluded that the\ndeep learning models which used data from Synthetic Minority Oversampling\nTechnique gave better results in terms of accuracy, AUC with the accuracy of\nsome models improving from 88.47 to 96.84.",
          "link": "http://arxiv.org/abs/2108.04659",
          "publishedOn": "2021-08-11T01:55:21.663Z",
          "wordCount": 695,
          "title": "An Empirical Study on Predictability of Software Code Smell Using Deep Learning Models. (arXiv:2108.04659v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet-Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1\">Guillaume Salha-Galvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1\">Manuel Moussallam</a>",
          "description": "Collaborative Metric Learning (CML) recently emerged as a powerful paradigm\nfor recommendation based on implicit feedback collaborative filtering. However,\nstandard CML methods learn fixed user and item representations, which fails to\ncapture the complex interests of users. Existing extensions of CML also either\nignore the heterogeneity of user-item relations, i.e. that a user can\nsimultaneously like very different items, or the latent item-item relations,\ni.e. that a user's preference for an item depends, not only on its intrinsic\ncharacteristics, but also on items they previously interacted with. In this\npaper, we present a hierarchical CML model that jointly captures latent\nuser-item and item-item relations from implicit data. Our approach is inspired\nby translation mechanisms from knowledge graph embedding and leverages\nmemory-based attention networks. We empirically show the relevance of this\njoint relational modeling, by outperforming existing CML models on\nrecommendation tasks on several real-world datasets. Our experiments also\nemphasize the limits of current CML relational models on very sparse datasets.",
          "link": "http://arxiv.org/abs/2108.04655",
          "publishedOn": "2021-08-11T01:55:21.655Z",
          "wordCount": 602,
          "title": "Hierarchical Latent Relation Modeling for Collaborative Metric Learning. (arXiv:2108.04655v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1\">Benjamin Stadnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1\">Jan Witowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1\">Vishwaesh Rajiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>",
          "description": "Artificial intelligence (AI) is transforming medicine and showing promise in\nimproving clinical diagnosis. In breast cancer screening, several recent\nstudies show that AI has the potential to improve radiologists' accuracy,\nsubsequently helping in early cancer diagnosis and reducing unnecessary workup.\nAs the number of proposed models and their complexity grows, it is becoming\nincreasingly difficult to re-implement them in order to reproduce the results\nand to compare different approaches. To enable reproducibility of research in\nthis application area and to enable comparison between different methods, we\nrelease a meta-repository containing deep learning models for classification of\nscreening mammograms. This meta-repository creates a framework that enables the\nevaluation of machine learning models on any private or public screening\nmammography data set. At its inception, our meta-repository contains five\nstate-of-the-art models with open-source implementations and cross-platform\ncompatibility. We compare their performance on five international data sets:\ntwo private New York University breast cancer screening data sets as well as\nthree public (DDSM, INbreast and Chinese Mammography Database) data sets. Our\nframework has a flexible design that can be generalized to other medical image\nanalysis tasks. The meta-repository is available at\nhttps://www.github.com/nyukat/mammography_metarepository.",
          "link": "http://arxiv.org/abs/2108.04800",
          "publishedOn": "2021-08-11T01:55:21.639Z",
          "wordCount": 641,
          "title": "Meta-repository of screening mammography classifiers. (arXiv:2108.04800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karadla_L/0/1/0/all/0/1\">Lahari Karadla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weizi Li</a>",
          "description": "The COVID-19 pandemic has resulted in significant social and economic impacts\nthroughout the world. In addition to the health consequences, the impacts on\ntraffic behaviors have also been sudden and dramatic. We have analyzed how the\nroad traffic safety of New York City, Los Angeles, and Boston in the U.S. have\nbeen impacted by the pandemic and corresponding local government orders and\nrestrictions. To be specific, we have studied the accident hotspots'\ndistributions before and after the outbreak of the pandemic and found that\ntraffic accidents have shifted in both location and time compared to previous\nyears. In addition, we have studied the road network characteristics in those\nhotspot regions with the hope to understand the underlying cause of the hotspot\nshifts.",
          "link": "http://arxiv.org/abs/2108.04787",
          "publishedOn": "2021-08-11T01:55:21.624Z",
          "wordCount": 621,
          "title": "Analyzing Effects of The COVID-19 Pandemic on Road Traffic Safety: The Cases of New York City, Los Angeles, and Boston. (arXiv:2108.04787v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khorshidi_H/0/1/0/all/0/1\">Hadi A. Khorshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aickelin_U/0/1/0/all/0/1\">Uwe Aickelin</a>",
          "description": "Class imbalance is a substantial challenge in classifying many real-world\ncases. Synthetic over-sampling methods have been effective to improve the\nperformance of classifiers for imbalance problems. However, most synthetic\nover-sampling methods generate non-diverse synthetic instances within the\nconvex hull formed by the existing minority instances as they only concentrate\non the minority class and ignore the vast information provided by the majority\nclass. They also often do not perform well for extremely imbalanced data as the\nfewer the minority instances, the less information to generate synthetic\ninstances. Moreover, existing methods that generate synthetic instances using\nthe majority class distributional information cannot perform effectively when\nthe majority class has a multi-modal distribution. We propose a new method to\ngenerate diverse and adaptable synthetic instances using Synthetic\nOver-sampling with Minority and Majority classes (SOMM). SOMM generates\nsynthetic instances diversely within the minority data space. It updates the\ngenerated instances adaptively to the neighbourhood including both classes.\nThus, SOMM performs well for both binary and multiclass imbalance problems. We\nexamine the performance of SOMM for binary and multiclass problems using\nbenchmark data sets for different imbalance levels. The empirical results show\nthe superiority of SOMM compared to other existing methods.",
          "link": "http://arxiv.org/abs/2011.04170",
          "publishedOn": "2021-08-11T01:55:21.618Z",
          "wordCount": 672,
          "title": "A Synthetic Over-sampling method with Minority and Majority classes for imbalance problems. (arXiv:2011.04170v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandi_J/0/1/0/all/0/1\">Jayanta Mandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canoy_R/0/1/0/all/0/1\">Rocsildes Canoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucarey_V/0/1/0/all/0/1\">V&#xed;ctor Bucarey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1\">Tias Guns</a>",
          "description": "The traditional Capacitated Vehicle Routing Problem (CVRP) minimizes the\ntotal distance of the routes under the capacity constraints of the vehicles.\nBut more often, the objective involves multiple criteria including not only the\ntotal distance of the tour but also other factors such as travel costs, travel\ntime, and fuel consumption.Moreover, in reality, there are numerous implicit\npreferences ingrained in the minds of the route planners and the drivers.\nDrivers, for instance, have familiarity with certain neighborhoods and\nknowledge of the state of roads, and often consider the best places for rest\nand lunch breaks. This knowledge is difficult to formulate and balance when\noperational routing decisions have to be made. This motivates us to learn the\nimplicit preferences from past solutions and to incorporate these learned\npreferences in the optimization process. These preferences are in the form of\narc probabilities, i.e., the more preferred a route is, the higher is the joint\nprobability. The novelty of this work is the use of a neural network model to\nestimate the arc probabilities, which allows for additional features and\nautomatic parameter estimation. This first requires identifying suitable\nfeatures, neural architectures and loss functions, taking into account that\nthere is typically few data available. We investigate the difference with a\nprior weighted Markov counting approach, and study the applicability of neural\nnetworks in this setting.",
          "link": "http://arxiv.org/abs/2108.04578",
          "publishedOn": "2021-08-11T01:55:21.612Z",
          "wordCount": 666,
          "title": "Data Driven VRP: A Neural Network Model to Learn Hidden Preferences for VRP. (arXiv:2108.04578v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.14442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">William B. Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1\">Priya Kasimbeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1\">Alexander Toshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present Interactive Gibson Benchmark, the first comprehensive benchmark\nfor training and evaluating Interactive Navigation: robot navigation strategies\nwhere physical interaction with objects is allowed and even encouraged to\naccomplish a task. For example, the robot can move objects if needed in order\nto clear a path leading to the goal location. Our benchmark comprises two novel\nelements: 1) a new experimental setup, the Interactive Gibson Environment\n(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high\nfidelity physical dynamics of the robot and common objects found in these\nscenes; 2) a set of Interactive Navigation metrics which allows one to study\nthe interplay between navigation and physical interaction. We present and\nevaluate multiple learning-based baselines in Interactive Gibson, and provide\ninsights into regimes of navigation with different trade-offs between\nnavigation path efficiency and disturbance of surrounding objects. We make our\nbenchmark publicly\navailable(https://sites.google.com/view/interactivegibsonenv) and encourage\nresearchers from all disciplines in robotics (e.g. planning, learning, control)\nto propose, evaluate, and compare their Interactive Navigation solutions in\nInteractive Gibson.",
          "link": "http://arxiv.org/abs/1910.14442",
          "publishedOn": "2021-08-11T01:55:21.597Z",
          "wordCount": 707,
          "title": "Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments. (arXiv:1910.14442v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.14408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Sida Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Justine Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Atish Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1\">Ray Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yinying Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1\">Vivek Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1\">Mubarak Seyed Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1\">Gang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1\">Nan Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yidan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1\">Andy O</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1\">Pooja Chitkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Alkesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1\">Kushal Tayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Roger Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1\">Peter Grasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason D. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>",
          "description": "Named Entity Recognition (NER) and Entity Linking (EL) play an essential role\nin voice assistant interaction, but are challenging due to the special\ndifficulties associated with spoken user queries. In this paper, we propose a\nnovel architecture that jointly solves the NER and EL tasks by combining them\nin a joint reranking module. We show that our proposed framework improves NER\naccuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features\nused also lead to better accuracies in other natural language understanding\ntasks, such as domain classification and semantic parsing.",
          "link": "http://arxiv.org/abs/2005.14408",
          "publishedOn": "2021-08-11T01:55:21.591Z",
          "wordCount": 638,
          "title": "Noise Robust Named Entity Understanding for Voice Assistants. (arXiv:2005.14408v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Graph Neural Networks (GNNs) have achieved great success among various\ndomains. Nevertheless, most GNN methods are sensitive to the quality of graph\nstructures. To tackle this problem, some studies exploit different graph\nstructure learning strategies to refine the original graph structure. However,\nthese methods only consider feature information while ignoring available label\ninformation. In this paper, we propose a novel label-informed graph structure\nlearning framework which incorporates label information explicitly through a\nclass transition matrix. We conduct extensive experiments on seven node\nclassification benchmark datasets and the results show that our method\noutperforms or matches the state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2108.04595",
          "publishedOn": "2021-08-11T01:55:21.585Z",
          "wordCount": 533,
          "title": "Label-informed Graph Structure Learning for Node Classification. (arXiv:2108.04595v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1908.04741",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nuske_F/0/1/0/all/0/1\">Feliks N&#xfc;ske</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gelss_P/0/1/0/all/0/1\">Patrick Gel&#xdf;</a>, <a href=\"http://arxiv.org/find/math/1/au:+Klus_S/0/1/0/all/0/1\">Stefan Klus</a>, <a href=\"http://arxiv.org/find/math/1/au:+Clementi_C/0/1/0/all/0/1\">Cecilia Clementi</a>",
          "description": "Recent years have seen rapid advances in the data-driven analysis of\ndynamical systems based on Koopman operator theory and related approaches. On\nthe other hand, low-rank tensor product approximations -- in particular the\ntensor train (TT) format -- have become a valuable tool for the solution of\nlarge-scale problems in a number of fields. In this work, we combine\nKoopman-based models and the TT format, enabling their application to\nhigh-dimensional problems in conjunction with a rich set of basis functions or\nfeatures. We derive efficient algorithms to obtain a reduced matrix\nrepresentation of the system's evolution operator starting from an appropriate\nlow-rank representation of the data. These algorithms can be applied to both\nstationary and non-stationary systems. We establish the infinite-data limit of\nthese matrix representations, and demonstrate our methods' capabilities using\nseveral benchmark data sets.",
          "link": "http://arxiv.org/abs/1908.04741",
          "publishedOn": "2021-08-11T01:55:21.580Z",
          "wordCount": 609,
          "title": "Tensor-based computation of metastable and coherent sets. (arXiv:1908.04741v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bontrager_P/0/1/0/all/0/1\">Philip Bontrager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>",
          "description": "Machine learning for procedural content generation has recently become an\nactive area of research. Levels vary in both form and function and are mostly\nunrelated to each other across games. This has made it difficult to assemble\nsuitably large datasets to bring machine learning to level design in the same\nway as it's been used for image generation. Here we propose Generative Playing\nNetworks which design levels for itself to play. The algorithm is built in two\nparts; an agent that learns to play game levels, and a generator that learns\nthe distribution of playable levels. As the agent learns and improves its\nability, the space of playable levels, as defined by the agent, grows. The\ngenerator targets the agent's playability estimates to then update its\nunderstanding of what constitutes a playable level. We call this process of\nlearning the distribution of data found through self-discovery with an\nenvironment, self-supervised inductive learning. Unlike previous approaches to\nprocedural content generation, Generative Playing Networks are end-to-end\ndifferentiable and do not require human-designed examples or domain knowledge.\nWe demonstrate the capability of this framework by training an agent and level\ngenerator for a 2D dungeon crawler game.",
          "link": "http://arxiv.org/abs/2002.05259",
          "publishedOn": "2021-08-11T01:55:21.565Z",
          "wordCount": 662,
          "title": "Learning to Generate Levels From Nothing. (arXiv:2002.05259v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulos_G/0/1/0/all/0/1\">George Panagopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tziortziotis_N/0/1/0/all/0/1\">Nikolaos Tziortziotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1\">Fragkiskos D. Malliaros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>",
          "description": "As the field of machine learning for combinatorial optimization advances,\ntraditional problems are resurfaced and readdressed through this new\nperspective. The overwhelming majority of the literature focuses on small graph\nproblems, while several real-world problems are devoted to large graphs. Here,\nwe focus on two such problems that are related: influence estimation, a\n\\#P-hard counting problem, and influence maximization, an NP-hard problem. We\ndevelop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an\nupper bound of influence estimation and train it on small simulated graphs.\nExperiments show that GLIE can provide accurate predictions faster than the\nalternatives for graphs 10 times larger than the train set. More importantly,\nit can be used on arbitrary large graphs for influence maximization, as the\npredictions can rank effectively seed sets even when the accuracy deteriorates.\nTo showcase this, we propose a version of a standard Influence Maximization\n(IM) algorithm where we substitute traditional influence estimation with the\npredictions of GLIE.We also transfer GLIE into a reinforcement learning model\nthat learns how to choose seeds to maximize influence sequentially using GLIE's\nhidden representations and predictions. The final results show that the\nproposed methods surpasses a previous GNN-RL approach and perform on par with a\nstate-of-the-art IM algorithm.",
          "link": "http://arxiv.org/abs/2108.04623",
          "publishedOn": "2021-08-11T01:55:21.556Z",
          "wordCount": 638,
          "title": "Learning to Maximize Influence. (arXiv:2108.04623v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04698",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gu_S/0/1/0/all/0/1\">Shuting Gu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1\">Hongqiao Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>",
          "description": "The transition state (TS) calculation is a grand challenge for computational\nintensive energy function. The traditional methods need to evaluate the\ngradients of the energy function at a very large number of locations. To reduce\nthe number of expensive computations of the true gradients, we propose an\nactive learning framework consisting of a statistical surrogate model, Gaussian\nprocess regression (GPR) for the energy function, and a single-walker dynamics\nmethod, gentle accent dynamics (GAD), for the saddle-type transition states. TS\nis detected by the GAD applied to the GPR surrogate for the gradient vector and\nthe Hessian matrix. Our key ingredient for efficiency improvements is an active\nlearning method which sequentially designs the most informative locations and\ntakes evaluations of the original model at these locations to train GPR. We\nformulate this active learning task as the optimal experimental design problem\nand propose a very efficient sample-based sub-optimal criterion to construct\nthe optimal locations. We show that the new method significantly decreases the\nrequired number of energy or force evaluations of the original model.",
          "link": "http://arxiv.org/abs/2108.04698",
          "publishedOn": "2021-08-11T01:55:21.550Z",
          "wordCount": 604,
          "title": "Active Learning for Transition State Calculation. (arXiv:2108.04698v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04809",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Shin_D/0/1/0/all/0/1\">Dongil Shin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Cupertino_A/0/1/0/all/0/1\">Andrea Cupertino</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jong_M/0/1/0/all/0/1\">Matthijs H. J. de Jong</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Steeneken_P/0/1/0/all/0/1\">Peter G. Steeneken</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Bessa_M/0/1/0/all/0/1\">Miguel A. Bessa</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Norte_R/0/1/0/all/0/1\">Richard A. Norte</a>",
          "description": "From ultra-sensitive detectors of fundamental forces to quantum networks and\nsensors, mechanical resonators are enabling next-generation technologies to\noperate in room temperature environments. Currently, silicon nitride\nnanoresonators stand as a leading microchip platform in these advances by\nallowing for mechanical resonators whose motion is remarkably isolated from\nambient thermal noise. However, to date, human intuition has remained the\ndriving force behind design processes. Here, inspired by nature and guided by\nmachine learning, a spiderweb nanomechanical resonator is developed that\nexhibits vibration modes which are isolated from ambient thermal environments\nvia a novel \"torsional soft-clamping\" mechanism discovered by the data-driven\noptimization algorithm. This bio-inspired resonator is then fabricated;\nexperimentally confirming a new paradigm in mechanics with quality factors\nabove 1 billion in room temperature environments. In contrast to other\nstate-of-the-art resonators, this milestone is achieved with a compact design\nwhich does not require sub-micron lithographic features or complex phononic\nbandgaps, making it significantly easier and cheaper to manufacture at large\nscales. Here we demonstrate the ability of machine learning to work in tandem\nwith human intuition to augment creative possibilities and uncover new\nstrategies in computing and nanotechnology.",
          "link": "http://arxiv.org/abs/2108.04809",
          "publishedOn": "2021-08-11T01:55:21.539Z",
          "wordCount": 652,
          "title": "Spiderweb nanomechanical resonators via Bayesian optimization: inspired by nature and guided by machine learning. (arXiv:2108.04809v1 [cond-mat.mes-hall])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1\">Praveen Kumar Bodigutla</a>",
          "description": "\"High Quality Related Search Query Suggestions\" task aims at recommending\nsearch queries which are real, accurate, diverse, relevant and engaging.\nObtaining large amounts of query-quality human annotations is expensive. Prior\nwork on supervised query suggestion models suffered from selection and exposure\nbias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),\nleading to low quality suggestions. Reinforcement Learning techniques employed\nto reformulate a query using terms from search results, have limited\nscalability to large-scale industry applications. To recommend high quality\nrelated search queries, we train a Deep Reinforcement Learning model to predict\nthe query a user would enter next. The reward signal is composed of long-term\nsession-based user feedback, syntactic relatedness and estimated naturalness of\ngenerated query. Over the baseline supervised model, our proposed approach\nachieves a significant relative improvement in terms of recommendation\ndiversity (3%), down-stream user-engagement (4.2%) and per-sentence word\nrepetitions (82%).",
          "link": "http://arxiv.org/abs/2108.04452",
          "publishedOn": "2021-08-11T01:55:21.533Z",
          "wordCount": 598,
          "title": "High Quality Related Search Query Suggestions using Deep Reinforcement Learning. (arXiv:2108.04452v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Behr_D/0/1/0/all/0/1\">David Behr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maina_C/0/1/0/all/0/1\">Ciira wa Maina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>",
          "description": "This paper is an investigation into aspects of an audio classification\npipeline that will be appropriate for the monitoring of bird species on edges\ndevices. These aspects include transfer learning, data augmentation and model\noptimization. The hope is that the resulting models will be good candidates to\ndeploy on edge devices to monitor bird populations. Two classification\napproaches will be taken into consideration, one which explores the\neffectiveness of a traditional Deep Neural Network(DNN) and another that makes\nuse of Convolutional layers.This study aims to contribute empirical evidence of\nthe merits and demerits of each approach.",
          "link": "http://arxiv.org/abs/2108.04449",
          "publishedOn": "2021-08-11T01:55:21.517Z",
          "wordCount": 548,
          "title": "An empirical investigation into audio pipeline approaches for classifying bird species. (arXiv:2108.04449v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hongwu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shanglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weitze_S/0/1/0/all/0/1\">Scott Weitze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Sahidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1\">Tong Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Minghu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mimi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>",
          "description": "Being able to learn from complex data with phase information is imperative\nfor many signal processing applications. Today' s real-valued deep neural\nnetworks (DNNs) have shown efficiency in latent information analysis but fall\nshort when applied to the complex domain. Deep complex networks (DCN), in\ncontrast, can learn from complex data, but have high computational costs;\ntherefore, they cannot satisfy the instant decision-making requirements of many\ndeployable systems dealing with short observations or short signal bursts.\nRecent, Binarized Complex Neural Network (BCNN), which integrates DCNs with\nbinarized neural networks (BNN), shows great potential in classifying complex\ndata in real-time. In this paper, we propose a structural pruning based\naccelerator of BCNN, which is able to provide more than 5000 frames/s inference\nthroughput on edge devices. The high performance comes from both the algorithm\nand hardware sides. On the algorithm side, we conduct structural pruning to the\noriginal BCNN models and obtain 20 $\\times$ pruning rates with negligible\naccuracy loss; on the hardware side, we propose a novel 2D convolution\noperation accelerator for the binary complex neural network. Experimental\nresults show that the proposed design works with over 90% utilization and is\nable to achieve the inference throughput of 5882 frames/s and 4938 frames/s for\ncomplex NIN-Net and ResNet-18 using CIFAR-10 dataset and Alveo U280 Board.",
          "link": "http://arxiv.org/abs/2108.04811",
          "publishedOn": "2021-08-11T01:55:21.512Z",
          "wordCount": 669,
          "title": "Binary Complex Neural Network Acceleration on FPGA. (arXiv:2108.04811v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04620",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Riekert_A/0/1/0/all/0/1\">Adrian Riekert</a>",
          "description": "Gradient descent (GD) type optimization methods are the standard instrument\nto train artificial neural networks (ANNs) with rectified linear unit (ReLU)\nactivation. Despite the great success of GD type optimization methods in\nnumerical simulations for the training of ANNs with ReLU activation, it remains\n- even in the simplest situation of the plain vanilla GD optimization method\nwith random initializations and ANNs with one hidden layer - an open problem to\nprove (or disprove) the conjecture that the risk of the GD optimization method\nconverges in the training of such ANNs to zero as the width of the ANNs, the\nnumber of independent random initializations, and the number of GD steps\nincrease to infinity. In this article we prove this conjecture in the situation\nwhere the probability distribution of the input data is equivalent to the\ncontinuous uniform distribution on a compact interval, where the probability\ndistributions for the random initializations of the ANN parameters are standard\nnormal distributions, and where the target function under consideration is\ncontinuous and piecewise affine linear. Roughly speaking, the key ingredients\nin our mathematical convergence analysis are (i) to prove that suitable sets of\nglobal minima of the risk functions are \\emph{twice continuously differentiable\nsubmanifolds of the ANN parameter spaces}, (ii) to prove that the Hessians of\nthe risk functions on these sets of global minima satisfy an appropriate\n\\emph{maximal rank condition}, and, thereafter, (iii) to apply the machinery in\n[Fehrman, B., Gess, B., Jentzen, A., Convergence rates for the stochastic\ngradient descent method for non-convex objective functions. J. Mach. Learn.\nRes. 21(136): 1--48, 2020] to establish convergence of the GD optimization\nmethod with random initializations.",
          "link": "http://arxiv.org/abs/2108.04620",
          "publishedOn": "2021-08-11T01:55:21.484Z",
          "wordCount": 750,
          "title": "A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear target functions. (arXiv:2108.04620v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bshouty_N/0/1/0/all/0/1\">Nader H. Bshouty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddad_Zaknoon_C/0/1/0/all/0/1\">Catherine A. Haddad-Zaknoon</a>",
          "description": "In this paper, we study learning and testing decision tree of size and depth\nthat are significantly smaller than the number of attributes $n$.\n\nOur main result addresses the problem of poly$(n,1/\\epsilon)$ time algorithms\nwith poly$(s,1/\\epsilon)$ query complexity (independent of $n$) that\ndistinguish between functions that are decision trees of size $s$ from\nfunctions that are $\\epsilon$-far from any decision tree of size\n$\\phi(s,1/\\epsilon)$, for some function $\\phi > s$. The best known result is\nthe recent one that follows from Blank, Lange and Tan,~\\cite{BlancLT20}, that\ngives $\\phi(s,1/\\epsilon)=2^{O((\\log^3s)/\\epsilon^3)}$. In this paper, we give\na new algorithm that achieves $\\phi(s,1/\\epsilon)=2^{O(\\log^2 (s/\\epsilon))}$.\n\nMoreover, we study the testability of depth-$d$ decision tree and give a {\\it\ndistribution free} tester that distinguishes between depth-$d$ decision tree\nand functions that are $\\epsilon$-far from depth-$d^2$ decision tree. In\nparticular, for decision trees of size $s$, the above result holds in the\ndistribution-free model when the tree depth is $O(\\log(s/\\epsilon))$.\n\nWe also give other new results in learning and testing of size-$s$ decision\ntrees and depth-$d$ decision trees that follow from results in the literature\nand some results we prove in this paper.",
          "link": "http://arxiv.org/abs/2108.04587",
          "publishedOn": "2021-08-11T01:55:21.479Z",
          "wordCount": 618,
          "title": "On Learning and Testing Decision Tree. (arXiv:2108.04587v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1\">Stefano Gasperini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Patrick Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1\">Vinzenz Dallabetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "While self-supervised monocular depth estimation in driving scenarios has\nachieved comparable performance to supervised approaches, violations of the\nstatic world assumption can still lead to erroneous depth predictions of\ntraffic participants, posing a potential safety issue. In this paper, we\npresent R4Dyn, a novel set of techniques to use cost-efficient radar data on\ntop of a self-supervised depth estimation framework. In particular, we show how\nradar can be used during training as weak supervision signal, as well as an\nextra input to enhance the estimation robustness at inference time. Since\nautomotive radars are readily available, this allows to collect training data\nfrom a variety of existing vehicles. Moreover, by filtering and expanding the\nsignal to make it compatible with learning-based approaches, we address radar\ninherent issues, such as noise and sparsity. With R4Dyn we are able to overcome\na major limitation of self-supervised depth estimation, i.e. the prediction of\ntraffic participants. We substantially improve the estimation on dynamic\nobjects, such as cars by 37% on the challenging nuScenes dataset, hence\ndemonstrating that radar is a valuable additional sensor for monocular depth\nestimation in autonomous vehicles. Additionally, we plan on making the code\npublicly available.",
          "link": "http://arxiv.org/abs/2108.04814",
          "publishedOn": "2021-08-11T01:55:21.452Z",
          "wordCount": 651,
          "title": "R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes. (arXiv:2108.04814v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chierichetti_F/0/1/0/all/0/1\">Flavio Chierichetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panconesi_A/0/1/0/all/0/1\">Alessandro Panconesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_G/0/1/0/all/0/1\">Giuseppe Re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trevisan_L/0/1/0/all/0/1\">Luca Trevisan</a>",
          "description": "Correlation Clustering is an important clustering problem with many\napplications. We study the reconstruction version of this problem in which one\nis seeking to reconstruct a latent clustering that has been corrupted by random\nnoise and adversarial modifications.\n\nConcerning the latter, we study a standard \"post-adversarial\" model, in which\nadversarial modifications come after the noise, and also introduce and analyze\na \"pre-adversarial\" model in which adversarial modifications come before the\nnoise. Given an input coming from such a semi-adversarial generative model, the\ngoal is to reconstruct almost perfectly and with high probability the latent\nclustering.\n\nWe focus on the case where the hidden clusters have equal size and show the\nfollowing. In the pre-adversarial setting, spectral algorithms are optimal, in\nthe sense that they reconstruct all the way to the information-theoretic\nthreshold beyond which no reconstruction is possible. In contrast, in the\npost-adversarial setting their ability to restore the hidden clusters stops\nbefore the threshold, but the gap is optimally filled by SDP-based algorithms.",
          "link": "http://arxiv.org/abs/2108.04729",
          "publishedOn": "2021-08-11T01:55:21.447Z",
          "wordCount": 598,
          "title": "Correlation Clustering Reconstruction in Semi-Adversarial Models. (arXiv:2108.04729v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1\">NareshKumar Gurulingan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Scene understanding is crucial for autonomous systems which intend to operate\nin the real world. Single task vision networks extract information only based\non some aspects of the scene. In multi-task learning (MTL), on the other hand,\nthese single tasks are jointly learned, thereby providing an opportunity for\ntasks to share information and obtain a more comprehensive understanding. To\nthis end, we develop UniNet, a unified scene understanding network that\naccurately and efficiently infers vital vision tasks including object\ndetection, semantic segmentation, instance segmentation, monocular depth\nestimation, and monocular instance depth prediction. As these tasks look at\ndifferent semantic and geometric information, they can either complement or\nconflict with each other. Therefore, understanding inter-task relationships can\nprovide useful cues to enable complementary information sharing. We evaluate\nthe task relationships in UniNet through the lens of adversarial attacks based\non the notion that they can exploit learned biases and task interactions in the\nneural network. Extensive experiments on the Cityscapes dataset, using\nuntargeted and targeted attacks reveal that semantic tasks strongly interact\namongst themselves, and the same holds for geometric tasks. Additionally, we\nshow that the relationship between semantic and geometric tasks is asymmetric\nand their interaction becomes weaker as we move towards higher-level\nrepresentations.",
          "link": "http://arxiv.org/abs/2108.04584",
          "publishedOn": "2021-08-11T01:55:21.440Z",
          "wordCount": 668,
          "title": "UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks. (arXiv:2108.04584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04782",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1\">Yangyi Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1\">Ziping Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1\">Ambuj Tewari</a>",
          "description": "The Oxford English Dictionary defines precision medicine as \"medical care\ndesigned to optimize efficiency or therapeutic benefit for particular groups of\npatients, especially by using genetic or molecular profiling.\" It is not an\nentirely new idea: physicians from ancient times have recognized that medical\ntreatment needs to consider individual variations in patient characteristics.\nHowever, the modern precision medicine movement has been enabled by a\nconfluence of events: scientific advances in fields such as genetics and\npharmacology, technological advances in mobile devices and wearable sensors,\nand methodological advances in computing and data sciences.\n\nThis chapter is about bandit algorithms: an area of data science of special\nrelevance to precision medicine. With their roots in the seminal work of\nBellman, Robbins, Lai and others, bandit algorithms have come to occupy a\ncentral place in modern data science ( Lattimore and Szepesvari, 2020). Bandit\nalgorithms can be used in any situation where treatment decisions need to be\nmade to optimize some health outcome. Since precision medicine focuses on the\nuse of patient characteristics to guide treatment, contextual bandit algorithms\nare especially useful since they are designed to take such information into\naccount. The role of bandit algorithms in areas of precision medicine such as\nmobile health and digital phenotyping has been reviewed before (Tewari and\nMurphy, 2017; Rabbi et al., 2019). Since these reviews were published, bandit\nalgorithms have continued to find uses in mobile health and several new topics\nhave emerged in the research on bandit algorithms. This chapter is written for\nquantitative researchers in fields such as statistics, machine learning, and\noperations research who might be interested in knowing more about the\nalgorithmic and mathematical details of bandit algorithms that have been used\nin mobile health.",
          "link": "http://arxiv.org/abs/2108.04782",
          "publishedOn": "2021-08-11T01:55:21.435Z",
          "wordCount": 741,
          "title": "Bandit Algorithms for Precision Medicine. (arXiv:2108.04782v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_G/0/1/0/all/0/1\">Gon&#xe7;alo Sim&#xf5;es de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abreu_G/0/1/0/all/0/1\">Gon&#xe7;alo Faria Abreu</a>",
          "description": "In this study we propose a new concept of databases (crowdsourced databases),\nadding a new conceptual approach to the debate on legal protection of databases\nin Europe. We also summarise the current legal framework and current indexing\nand web scraping practices - it would not be prudent to suggest a new theory\nwithout contextualising it in the legal and practical context in which it is\ndeveloped.",
          "link": "http://arxiv.org/abs/2108.04727",
          "publishedOn": "2021-08-11T01:55:21.429Z",
          "wordCount": 494,
          "title": "Crowdsourced Databases and Sui Generis Rights. (arXiv:2108.04727v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheliga_D/0/1/0/all/0/1\">Daniel Scheliga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1\">Patrick M&#xe4;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeland_M/0/1/0/all/0/1\">Marco Seeland</a>",
          "description": "Collaborative training of neural networks leverages distributed data by\nexchanging gradient information between different clients. Although training\ndata entirely resides with the clients, recent work shows that training data\ncan be reconstructed from such exchanged gradient information. To enhance\nprivacy, gradient perturbation techniques have been proposed. However, they\ncome at the cost of reduced model performance, increased convergence time, or\nincreased data demand. In this paper, we introduce PRECODE, a PRivacy EnhanCing\nmODulE that can be used as generic extension for arbitrary model architectures.\nWe propose a simple yet effective realization of PRECODE using variational\nmodeling. The stochastic sampling induced by variational modeling effectively\nprevents privacy leakage from gradients and in turn preserves privacy of data\nowners. We evaluate PRECODE using state of the art gradient inversion attacks\non two different model architectures trained on three datasets. In contrast to\ncommonly used defense mechanisms, we find that our proposed modification\nconsistently reduces the attack success rate to 0% while having almost no\nnegative impact on model training and final performance. As a result, PRECODE\nreveals a promising path towards privacy enhancing model extensions.",
          "link": "http://arxiv.org/abs/2108.04725",
          "publishedOn": "2021-08-11T01:55:21.423Z",
          "wordCount": 631,
          "title": "PRECODE - A Generic Model Extension to Prevent Deep Gradient Leakage. (arXiv:2108.04725v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.06731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1\">Guy Tennenholtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1\">Uri Shalit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1\">Yonathan Efroni</a>",
          "description": "We study linear contextual bandits with access to a large, confounded,\noffline dataset that was sampled from some fixed policy. We show that this\nproblem is closely related to a variant of the bandit problem with side\ninformation. We construct a linear bandit algorithm that takes advantage of the\nprojected information, and prove regret bounds. Our results demonstrate the\nability to take advantage of confounded offline data. Particularly, we prove\nregret bounds that improve current bounds by a factor related to the visible\ndimensionality of the contexts in the data. Our results indicate that\nconfounded offline data can significantly improve online learning algorithms.\nFinally, we demonstrate various characteristics of our approach through\nsynthetic simulations.",
          "link": "http://arxiv.org/abs/2006.06731",
          "publishedOn": "2021-08-11T01:55:21.407Z",
          "wordCount": 587,
          "title": "Bandits with Partially Observable Confounded Data. (arXiv:2006.06731v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1\">Dean P. Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham M. Kakade</a>",
          "description": "Stochastic gradient descent (SGD) exhibits strong algorithmic regularization\neffects in practice, which has been hypothesized to play an important role in\nthe generalization of modern machine learning approaches. In this work, we seek\nto understand these issues in the simpler setting of linear regression\n(including both underparameterized and overparameterized regimes), where our\ngoal is to make sharp instance-based comparisons of the implicit regularization\nafforded by (unregularized) average SGD with the explicit regularization of\nridge regression. For a broad class of least squares problem instances (that\nare natural in high-dimensional settings), we show: (1) for every problem\ninstance and for every ridge parameter, (unregularized) SGD, when provided with\nlogarithmically more samples than that provided to the ridge algorithm,\ngeneralizes no worse than the ridge solution (provided SGD uses a tuned\nconstant stepsize); (2) conversely, there exist instances (in this wide problem\nclass) where optimally-tuned ridge regression requires quadratically more\nsamples than SGD in order to have the same generalization performance. Taken\ntogether, our results show that, up to the logarithmic factors, the\ngeneralization performance of SGD is always no worse than that of ridge\nregression in a wide range of overparameterized problems, and, in fact, could\nbe much better for some problem instances. More generally, our results show how\nalgorithmic regularization has important consequences even in simpler\n(overparameterized) convex settings.",
          "link": "http://arxiv.org/abs/2108.04552",
          "publishedOn": "2021-08-11T01:55:21.400Z",
          "wordCount": 677,
          "title": "The Benefits of Implicit Regularization from SGD in Least Squares Problems. (arXiv:2108.04552v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04763",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ciosek_K/0/1/0/all/0/1\">Kamil Ciosek</a>",
          "description": "Imitation Learning algorithms learn a policy from demonstrations of expert\nbehavior. Somewhat counterintuitively, we show that, for deterministic experts,\nimitation learning can be done by reduction to reinforcement learning, which is\ncommonly considered more difficult. We conduct experiments which confirm that\nour reduction works well in practice for a continuous control task.",
          "link": "http://arxiv.org/abs/2108.04763",
          "publishedOn": "2021-08-11T01:55:21.394Z",
          "wordCount": 474,
          "title": "Imitation Learning by Reinforcement Learning. (arXiv:2108.04763v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Hyejun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Joonyong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tai Myung Chung</a>",
          "description": "Federated Learning is a distributed machine learning framework designed for\ndata privacy preservation i.e., local data remain private throughout the entire\ntraining and testing procedure. Federated Learning is gaining popularity\nbecause it allows one to use machine learning techniques while preserving\nprivacy. However, it inherits the vulnerabilities and susceptibilities raised\nin deep learning techniques. For instance, Federated Learning is particularly\nvulnerable to data poisoning attacks that may deteriorate its performance and\nintegrity due to its distributed nature and inaccessibility to the raw data. In\naddition, it is extremely difficult to correctly identify malicious clients due\nto the non-Independently and/or Identically Distributed (non-IID) data. The\nreal-world data can be complex and diverse, making them hardly distinguishable\nfrom the malicious data without direct access to the raw data. Prior research\nhas focused on detecting malicious clients while treating only the clients\nhaving IID data as benign. In this study, we propose a method that detects and\nclassifies anomalous clients from benign clients when benign ones have non-IID\ndata. Our proposed method leverages feature dimension reduction, dynamic\nclustering, and cosine similarity-based clipping. The experimental results\nvalidates that our proposed method not only classifies the malicious clients\nbut also alleviates their negative influences from the entire procedure. Our\nfindings may be used in future studies to effectively eliminate anomalous\nclients when building a model with diverse data.",
          "link": "http://arxiv.org/abs/2108.04551",
          "publishedOn": "2021-08-11T01:55:21.389Z",
          "wordCount": 669,
          "title": "ABC-FL: Anomalous and Benign client Classification in Federated Learning. (arXiv:2108.04551v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>",
          "description": "Decentralized optimization and communication compression have exhibited their\ngreat potential in accelerating distributed machine learning by mitigating the\ncommunication bottleneck in practice. While existing decentralized algorithms\nwith communication compression mostly focus on the problems with only smooth\ncomponents, we study the decentralized stochastic composite optimization\nproblem with a potentially non-smooth component. A \\underline{Prox}imal\ngradient \\underline{L}in\\underline{EA}r convergent \\underline{D}ecentralized\nalgorithm with compression, Prox-LEAD, is proposed with rigorous theoretical\nanalyses in the general stochastic setting and the finite-sum setting. Our\ntheorems indicate that Prox-LEAD works with arbitrary compression precision,\nand it tremendously reduces the communication cost almost for free. The\nsuperiorities of the proposed algorithms are demonstrated through the\ncomparison with state-of-the-art algorithms in terms of convergence\ncomplexities and numerical experiments. Our algorithmic framework also\ngenerally enlightens the compressed communication on other primal-dual\nalgorithms by reducing the impact of inexact iterations, which might be of\nindependent interest.",
          "link": "http://arxiv.org/abs/2108.04448",
          "publishedOn": "2021-08-11T01:55:21.383Z",
          "wordCount": 583,
          "title": "Decentralized Composite Optimization with Compression. (arXiv:2108.04448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1\">Harald K&#xf6;stler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1\">Marco Heisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1\">Patrick Krauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seung Hee Yang</a>",
          "description": "In this article, we perform a review of the state-of-the-art of hybrid\nmachine learning in medical imaging. We start with a short summary of the\ngeneral developments of the past in machine learning and how general and\nspecialized approaches have been in competition in the past decades. A\nparticular focus will be the theoretical and experimental evidence pro and\ncontra hybrid modelling. Next, we inspect several new developments regarding\nhybrid machine learning with a particular focus on so-called known operator\nlearning and how hybrid approaches gain more and more momentum across\nessentially all applications in medical imaging and medical image analysis. As\nwe will point out by numerous examples, hybrid models are taking over in image\nreconstruction and analysis. Even domains such as physical simulation and\nscanner and acquisition design are being addressed using machine learning grey\nbox modelling approaches. Towards the end of the article, we will investigate a\nfew future directions and point out relevant areas in which hybrid modelling,\nmeta learning, and other domains will likely be able to drive the\nstate-of-the-art ahead.",
          "link": "http://arxiv.org/abs/2108.04543",
          "publishedOn": "2021-08-11T01:55:21.367Z",
          "wordCount": 657,
          "title": "Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future. (arXiv:2108.04543v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yousfi_S/0/1/0/all/0/1\">Siham Yousfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhanoui_M/0/1/0/all/0/1\">Maryem Rhanoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiadmi_D/0/1/0/all/0/1\">Dalila Chiadmi</a>",
          "description": "Big Data are rapidly produced from various heterogeneous data sources. They\nare of different types (text, image, video or audio) and have different levels\nof reliability and completeness. One of the most interesting architectures that\ndeal with the large amount of emerging data at high velocity is called the\nlambda architecture. In fact, it combines two different processing layers\nnamely batch and speed layers, each providing specific views of data while\nensuring robustness, fast and scalable data processing. However, most papers\ndealing with the lambda architecture are focusing one single type of data\ngenerally produced by a single data source. Besides, the layers of the\narchitecture are implemented independently, or, at best, are combined to\nperform basic processing without assessing either the data reliability or\ncompleteness. Therefore, inspired by the lambda architecture, we propose in\nthis paper a generic multimodal architecture that combines both batch and\nstreaming processing in order to build a complete, global and accurate insight\nin near-real-time based on the knowledge extracted from multiple heterogeneous\nBig Data sources. Our architecture uses batch processing to analyze the data\nstructures and contents, build the learning models and calculate the\nreliability index of the involved sources, while the streaming processing uses\nthe built-in models of the batch layer to immediately process incoming data and\nrapidly provide results. We validate our architecture in the context of urban\ntraffic management systems in order to detect congestions.",
          "link": "http://arxiv.org/abs/2108.04343",
          "publishedOn": "2021-08-11T01:55:21.352Z",
          "wordCount": 686,
          "title": "Towards a Generic Multimodal Architecture for Batch and Streaming Big Data Integration. (arXiv:2108.04343v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Zefang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Depeng/0/1/0/all/0/1\">Depeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Recent technology development brings the booming of numerous new\nDemand-Driven Services (DDS) into urban lives, including ridesharing, on-demand\ndelivery, express systems and warehousing. In DDS, a service loop is an\nelemental structure, including its service worker, the service providers and\ncorresponding service targets. The service workers should transport either\nhumans or parcels from the providers to the target locations. Various planning\ntasks within DDS can thus be classified into two individual stages: 1)\nDispatching, which is to form service loops from demand/supply distributions,\nand 2)Routing, which is to decide specific serving orders within the\nconstructed loops. Generating high-quality strategies in both stages is\nimportant to develop DDS but faces several challenging. Meanwhile, deep\nreinforcement learning (DRL) has been developed rapidly in recent years. It is\na powerful tool to solve these problems since DRL can learn a parametric model\nwithout relying on too many problem-based assumptions and optimize long-term\neffect by learning sequential decisions. In this survey, we first define DDS,\nthen highlight common applications and important decision/control problems\nwithin. For each problem, we comprehensively introduce the existing DRL\nsolutions, and further summarize them in\n\\textit{https://github.com/tsinghua-fib-lab/DDS\\_Survey}. We also introduce\nopen simulation environments for development and evaluation of DDS\napplications. Finally, we analyze remaining challenges and discuss further\nresearch opportunities in DRL solutions for DDS.",
          "link": "http://arxiv.org/abs/2108.04462",
          "publishedOn": "2021-08-11T01:55:21.345Z",
          "wordCount": 667,
          "title": "Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation Systems: A Survey. (arXiv:2108.04462v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Differentiable Neural Architecture Search is one of the most popular Neural\nArchitecture Search (NAS) methods for its search efficiency and simplicity,\naccomplished by jointly optimizing the model weight and architecture parameters\nin a weight-sharing supernet via gradient-based algorithms. At the end of the\nsearch phase, the operations with the largest architecture parameters will be\nselected to form the final architecture, with the implicit assumption that the\nvalues of architecture parameters reflect the operation strength. While much\nhas been discussed about the supernet's optimization, the architecture\nselection process has received little attention. We provide empirical and\ntheoretical analysis to show that the magnitude of architecture parameters does\nnot necessarily indicate how much the operation contributes to the supernet's\nperformance. We propose an alternative perturbation-based architecture\nselection that directly measures each operation's influence on the supernet. We\nre-evaluate several differentiable NAS methods with the proposed architecture\nselection and find that it is able to extract significantly improved\narchitectures from the underlying supernets consistently. Furthermore, we find\nthat several failure modes of DARTS can be greatly alleviated with the proposed\nselection method, indicating that much of the poor generalization observed in\nDARTS can be attributed to the failure of magnitude-based architecture\nselection rather than entirely the optimization of its supernet.",
          "link": "http://arxiv.org/abs/2108.04392",
          "publishedOn": "2021-08-11T01:55:21.338Z",
          "wordCount": 656,
          "title": "Rethinking Architecture Selection in Differentiable NAS. (arXiv:2108.04392v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patrascu_A/0/1/0/all/0/1\">Andrei Patrascu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irofti_P/0/1/0/all/0/1\">Paul Irofti</a>",
          "description": "Several decades ago the Proximal Point Algorithm (PPA) started to gain much\nattraction for both abstract operator theory and the numerical optimization\ncommunities. Even in modern applications, researchers still use proximal\nminimization theory to design scalable algorithms that overcome nonsmoothness\nin high dimensional models. Several remarkable references as\n\\cite{Fer:91,Ber:82constrained,Ber:89parallel,Tom:11} analyzed the tight local\nrelations between the convergence rate of PPA and the regularity of the\nobjective function. However, without taking into account the concrete\ncomputational effort paid for computing each PPA iteration, any iteration\ncomplexity remains abstract and purely informative. In this manuscript we aim\nto evaluate the computational complexity of practical PPA in terms of\n(proximal) gradient/subgradient iterations, which might allow a fair\npositioning of the famous PPA numerical performance in the class of first order\nmethods. First, we derive nonasymptotic iteration complexity estimates of exact\nand inexact PPA to minimize convex functions under $\\gamma-$Holderian growth:\n$\\BigO{\\log(1/\\epsilon)}$ (for $\\gamma \\in [1,2]$) and\n$\\BigO{1/\\epsilon^{\\gamma - 2}}$ (for $\\gamma > 2$). In particular, we recover\nwell-known results on exact PPA: finite convergence for sharp minima and linear\nconvergence for quadratic growth, even under presence of inexactness. Second,\nassuming that an usual (proximal) gradient/subgradient method subroutine is\nemployed to compute inexact PPA iteration, we show novel computational\ncomplexity bounds on a restarted variant of the inexact PPA, available when no\ninformation on the growth of the objective function is known. In the numerical\nexperiments we confirm the practical performance and implementability of our\nschemes.",
          "link": "http://arxiv.org/abs/2108.04482",
          "publishedOn": "2021-08-11T01:55:21.322Z",
          "wordCount": 678,
          "title": "Computational complexity of Inexact Proximal Point Algorithm for Convex Optimization under Holderian Growth. (arXiv:2108.04482v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1\">Hamza Rasaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>",
          "description": "Ultrasound is a non-invasive imaging modality that can be conveniently used\nto classify suspicious breast nodules and potentially detect the onset of\nbreast cancer. Recently, Convolutional Neural Networks (CNN) techniques have\nshown promising results in classifying ultrasound images of the breast into\nbenign or malignant. However, CNN inference acts as a black-box model, and as\nsuch, its decision-making is not interpretable. Therefore, increasing effort\nhas been dedicated to explaining this process, most notably through GRAD-CAM\nand other techniques that provide visual explanations into inner workings of\nCNNs. In addition to interpretation, these methods provide clinically important\ninformation, such as identifying the location for biopsy or treatment. In this\nwork, we analyze how adversarial assaults that are practically undetectable may\nbe devised to alter these importance maps dramatically. Furthermore, we will\nshow that this change in the importance maps can come with or without altering\nthe classification result, rendering them even harder to detect. As such, care\nmust be taken when using these importance maps to shed light on the inner\nworkings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and\npropose a new network based on ResNet-50 to improve the classification\naccuracies. Our sensitivity and specificity is comparable to the state of the\nart results.",
          "link": "http://arxiv.org/abs/2108.04345",
          "publishedOn": "2021-08-11T01:55:21.313Z",
          "wordCount": 667,
          "title": "Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images. (arXiv:2108.04345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaopeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhijie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1\">Mingyu Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Maojing Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>",
          "description": "We study the problem of knowledge tracing (KT) where the goal is to trace the\nstudents' knowledge mastery over time so as to make predictions on their future\nperformance. Owing to the good representation capacity of deep neural networks\n(DNNs), recent advances on KT have increasingly concentrated on exploring DNNs\nto improve the performance of KT. However, we empirically reveal that the DNNs\nbased KT models may run the risk of overfitting, especially on small datasets,\nleading to limited generalization. In this paper, by leveraging the current\nadvances in adversarial training (AT), we propose an efficient AT based KT\nmethod (ATKT) to enhance KT model's generalization and thus push the limit of\nKT. Specifically, we first construct adversarial perturbations and add them on\nthe original interaction embeddings as adversarial examples. The original and\nadversarial examples are further used to jointly train the KT model, forcing it\nis not only to be robust to the adversarial examples, but also to enhance the\ngeneralization over the original ones. To better implement AT, we then present\nan efficient attentive-LSTM model as KT backbone, where the key is a proposed\nknowledge hidden state attention module that adaptively aggregates information\nfrom previous knowledge hidden states while simultaneously highlighting the\nimportance of current knowledge hidden state to make a more accurate\nprediction. Extensive experiments on four public benchmark datasets demonstrate\nthat our ATKT achieves new state-of-the-art performance. Code is available at:\n\\color{blue} {\\url{https://github.com/xiaopengguo/ATKT}}.",
          "link": "http://arxiv.org/abs/2108.04430",
          "publishedOn": "2021-08-11T01:55:21.306Z",
          "wordCount": 681,
          "title": "Enhancing Knowledge Tracing via Adversarial Training. (arXiv:2108.04430v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Renjie Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiabao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Aiqun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1\">Derrick Wing Kwan Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swindlehurst_A/0/1/0/all/0/1\">A. Lee Swindlehurst</a>",
          "description": "Radio-frequency fingerprints~(RFFs) are promising solutions for realizing\nlow-cost physical layer authentication. Machine learning-based methods have\nbeen proposed for RFF extraction and discrimination. However, most existing\nmethods are designed for the closed-set scenario where the set of devices is\nremains unchanged. These methods can not be generalized to the RFF\ndiscrimination of unknown devices. To enable the discrimination of RFF from\nboth known and unknown devices, we propose a new end-to-end deep learning\nframework for extracting RFFs from raw received signals. The proposed framework\ncomprises a novel preprocessing module, called neural synchronization~(NS),\nwhich incorporates the data-driven learning with signal processing priors as an\ninductive bias from communication-model based processing. Compared to\ntraditional carrier synchronization techniques, which are static, this module\nestimates offsets by two learnable deep neural networks jointly trained by the\nRFF extractor. Additionally, a hypersphere representation is proposed to\nfurther improve the discrimination of RFF. Theoretical analysis shows that such\na data-and-model framework can better optimize the mutual information between\ndevice identity and the RFF, which naturally leads to better performance.\nExperimental results verify that the proposed RFF significantly outperforms\npurely data-driven DNN-design and existing handcrafted RFF methods in terms of\nboth discrimination and network generalizability.",
          "link": "http://arxiv.org/abs/2108.04436",
          "publishedOn": "2021-08-11T01:55:21.300Z",
          "wordCount": 659,
          "title": "A Generalizable Model-and-Data Driven Approach for Open-Set RFF Authentication. (arXiv:2108.04436v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "This survey is meant to provide an introduction to the fundamental theorem of\nlinear algebra and the theories behind them. Our goal is to give a rigorous\nintroduction to the readers with prior exposure to linear algebra.\nSpecifically, we provide some details and proofs of some results from (Strang,\n1993). We then describe the fundamental theorem of linear algebra from\ndifferent views and find the properties and relationships behind the views. The\nfundamental theorem of linear algebra is essential in many fields, such as\nelectrical engineering, computer science, machine learning, and deep learning.\nThis survey is primarily a summary of purpose, significance of important\ntheories behind it.\n\nThe sole aim of this survey is to give a self-contained introduction to\nconcepts and mathematical tools in theory behind the fundamental theorem of\nlinear algebra and rigorous analysis in order to seamlessly introduce its\nproperties in four subspaces in subsequent sections. However, we clearly\nrealize our inability to cover all the useful and interesting results and given\nthe paucity of scope to present this discussion, e.g., the separated analysis\nof the (orthogonal) projection matrices. We refer the reader to literature in\nthe field of linear algebra for a more detailed introduction to the related\nfields. Some excellent examples include (Rose, 1982; Strang, 2009; Trefethen\nand Bau III, 1997; Strang, 2019, 2021).",
          "link": "http://arxiv.org/abs/2108.04432",
          "publishedOn": "2021-08-11T01:55:21.293Z",
          "wordCount": 646,
          "title": "Revisit the Fundamental Theorem of Linear Algebra. (arXiv:2108.04432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Ming Xiao</a>",
          "description": "The recurrent neural networks (RNN) with richly distributed internal states\nand flexible non-linear transition functions, have overtaken the dynamic\nBayesian networks such as the hidden Markov models (HMMs) in the task of\nmodeling highly structured sequential data. These data, such as from speech and\nhandwriting, often contain complex relationships between the underlaying\nvariational factors and the observed data. The standard RNN model has very\nlimited randomness or variability in its structure, coming from the output\nconditional probability model. This paper will present different ways of using\nhigh level latent random variables in RNN to model the variability in the\nsequential data, and the training method of such RNN model under the VAE\n(Variational Autoencoder) principle. We will explore possible ways of using\nadversarial method to train a variational RNN model. Contrary to competing\napproaches, our approach has theoretical optimum in the model training and\nprovides better model training stability. Our approach also improves the\nposterior approximation in the variational inference network by a separated\nadversarial training step. Numerical results simulated from TIMIT speech data\nshow that reconstruction loss and evidence lower bound converge to the same\nlevel and adversarial training loss converges to 0.",
          "link": "http://arxiv.org/abs/2108.04496",
          "publishedOn": "2021-08-11T01:55:21.273Z",
          "wordCount": 627,
          "title": "Regularized Sequential Latent Variable Models with Adversarial Neural Networks. (arXiv:2108.04496v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1\">Ipsita Sutradhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mahziba Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mehedi Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1\">Malabika Sarker</a>",
          "description": "Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as\na result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye\nillness caused by diabetes, can lead to blindness if it is not identified and\ntreated in its early stages. Unfortunately, diagnosis of DR requires medically\ntrained professionals, but Bangladesh has limited specialists in comparison to\nits population. Moreover, the screening process is often expensive, prohibiting\nmany from receiving timely and proper diagnosis. To address the problem, we\nintroduce a deep learning algorithm which screens for different stages of DR.\nWe use a state-of-the-art CNN architecture to diagnose patients based on\nretinal fundus imagery. This paper is an experimental evaluation of the\nalgorithm we developed for DR diagnosis and screening specifically for\nBangladeshi patients. We perform this validation study using separate pools of\nretinal image data of real patients from a hospital and field studies in\nBangladesh. Our results show that the algorithm is effective at screening\nBangladeshi eyes even when trained on a public dataset which is out of domain,\nand can accurately determine the stage of DR as well, achieving an overall\naccuracy of 92.27\\% and 93.02\\% on two validation sets of Bangladeshi eyes. The\nresults confirm the ability of the algorithm to be used in real clinical\nsettings and applications due to its high accuracy and classwise metrics. Our\nalgorithm is implemented in the application Drishti, which is used to screen\nfor DR in patients living in rural areas in Bangladesh, where access to\nprofessional screening is limited.",
          "link": "http://arxiv.org/abs/2108.04358",
          "publishedOn": "2021-08-11T01:55:21.268Z",
          "wordCount": 706,
          "title": "Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients. (arXiv:2108.04358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.",
          "link": "http://arxiv.org/abs/2108.04351",
          "publishedOn": "2021-08-11T01:55:21.262Z",
          "wordCount": 633,
          "title": "Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Haocheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caleb Chen Cao</a>",
          "description": "Explanation of AI, as well as fairness of algorithms' decisions and the\ntransparency of the decision model, are becoming more and more important. And\nit is crucial to design effective and human-friendly techniques when opening\nthe black-box model. Counterfactual conforms to the human way of thinking and\nprovides a human-friendly explanation, and its corresponding explanation\nalgorithm refers to a strategic alternation of a given data point so that its\nmodel output is \"counter-facted\", i.e. the prediction is reverted. In this\npaper, we adapt counterfactual explanation over fine-grained image\nclassification problem. We demonstrated an adaptive method that could give a\ncounterfactual explanation by showing the composed counterfactual feature map\nusing top-down layer searching algorithm (TDLS). We have proved that our TDLS\nalgorithm could provide more flexible counterfactual visual explanation in an\nefficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,\nwe discussed several applicable scenarios of counterfactual visual\nexplanations.",
          "link": "http://arxiv.org/abs/2108.04238",
          "publishedOn": "2021-08-11T01:55:21.256Z",
          "wordCount": 608,
          "title": "TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curtis_C/0/1/0/all/0/1\">Christopher W. Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alford_Lago_D/0/1/0/all/0/1\">Daniel Jay Alford-Lago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Issan_O/0/1/0/all/0/1\">Opal Issan</a>",
          "description": "Koopman operator theory shows how nonlinear dynamical systems can be\nrepresented as an infinite-dimensional, linear operator acting on a Hilbert\nspace of observables of the system. However, determining the relevant modes and\neigenvalues of this infinite-dimensional operator can be difficult. The\nextended dynamic mode decomposition (EDMD) is one such method for generating\napproximations to Koopman spectra and modes, but the EDMD method faces its own\nset of challenges due to the need of user defined observables. To address this\nissue, we explore the use of convolutional autoencoder networks to\nsimultaneously find optimal families of observables which also generate both\naccurate embeddings of the flow into a space of observables and immersions of\nthe observables back into flow coordinates. This network results in a global\ntransformation of the flow and affords future state prediction via EDMD and the\ndecoder network. We call this method deep learning dynamic mode decomposition\n(DLDMD). The method is tested on canonical nonlinear data sets and is shown to\nproduce results that outperform a standard DMD approach.",
          "link": "http://arxiv.org/abs/2108.04433",
          "publishedOn": "2021-08-11T01:55:21.250Z",
          "wordCount": 606,
          "title": "Deep Learning Enhanced Dynamic Mode Decomposition. (arXiv:2108.04433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wancheng Ge</a>",
          "description": "Deep Neural Networks (DNNs) are vulnerable to adversarial examples which\nwould inveigle neural networks to make prediction errors with small per-\nturbations on the input images. Researchers have been devoted to promoting the\nresearch on the universal adversarial perturbations (UAPs) which are\ngradient-free and have little prior knowledge on data distributions. Procedural\nadversarial noise at- tack is a data-free universal perturbation generation\nmethod. In this paper, we propose two universal adversarial perturbation (UAP)\ngeneration methods based on procedural noise functions: Simplex noise and\nWorley noise. In our framework, the shading which disturbs visual\nclassification is generated with rendering technology. Without changing the\nsemantic representations, the adversarial examples generated via our methods\nshow superior performance on the attack.",
          "link": "http://arxiv.org/abs/2108.04409",
          "publishedOn": "2021-08-11T01:55:21.244Z",
          "wordCount": 560,
          "title": "On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wenjie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Sinno Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Time series has wide applications in the real world and is known to be\ndifficult to forecast. Since its statistical properties change over time, its\ndistribution also changes temporally, which will cause severe distribution\nshift problem to existing methods. However, it remains unexplored to model the\ntime series in the distribution perspective. In this paper, we term this as\nTemporal Covariate Shift (TCS). This paper proposes Adaptive RNNs (AdaRNN) to\ntackle the TCS problem by building an adaptive model that generalizes well on\nthe unseen test data. AdaRNN is sequentially composed of two novel algorithms.\nFirst, we propose Temporal Distribution Characterization to better characterize\nthe distribution information in the TS. Second, we propose Temporal\nDistribution Matching to reduce the distribution mismatch in TS to learn the\nadaptive TS model. AdaRNN is a general framework with flexible distribution\ndistances integrated. Experiments on human activity recognition, air quality\nprediction, and financial analysis show that AdaRNN outperforms the latest\nmethods by a classification accuracy of 2.6% and significantly reduces the RMSE\nby 9.0%. We also show that the temporal distribution matching algorithm can be\nextended in Transformer structure to boost its performance.",
          "link": "http://arxiv.org/abs/2108.04443",
          "publishedOn": "2021-08-11T01:55:21.221Z",
          "wordCount": 642,
          "title": "AdaRNN: Adaptive Learning and Forecasting of Time Series. (arXiv:2108.04443v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tailin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "The task of skeleton-based action recognition remains a core challenge in\nhuman-centred scene understanding due to the multiple granularities and large\nvariation in human motion. Existing approaches typically employ a single neural\nrepresentation for different motion patterns, which has difficulty in capturing\nfine-grained action classes given limited training data. To address the\naforementioned problems, we propose a novel multi-granular spatio-temporal\ngraph network for skeleton-based action classification that jointly models the\ncoarse- and fine-grained skeleton motion patterns. To this end, we develop a\ndual-head graph network consisting of two interleaved branches, which enables\nus to extract features at two spatio-temporal resolutions in an effective and\nefficient manner. Moreover, our network utilises a cross-head communication\nstrategy to mutually enhance the representations of both heads. We conducted\nextensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU\nRGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance\non all the benchmarks, which validates the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.04536",
          "publishedOn": "2021-08-11T01:55:21.215Z",
          "wordCount": 607,
          "title": "Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition. (arXiv:2108.04536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04428",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Han_Y/0/1/0/all/0/1\">Yuefeng Han</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1\">Cun-Hui Zhang</a>",
          "description": "The CP decomposition for high dimensional non-orthogonal spike tensors is an\nimportant problem with broad applications across many disciplines. However,\nprevious works with theoretical guarantee typically assume restrictive\nincoherence conditions on the basis vectors for the CP components. In this\npaper, we propose new computationally efficient composite PCA and concurrent\northogonalization algorithms for tensor CP decomposition with theoretical\nguarantees under mild incoherence conditions. The composite PCA applies the\nprincipal component or singular value decompositions twice, first to a matrix\nunfolding of the tensor data to obtain singular vectors and then to the matrix\nfolding of the singular vectors obtained in the first step. It can be used as\nan initialization for any iterative optimization schemes for the tensor CP\ndecomposition. The concurrent orthogonalization algorithm iteratively estimates\nthe basis vector in each mode of the tensor by simultaneously applying\nprojections to the orthogonal complements of the spaces generated by others CP\ncomponents in other modes. It is designed to improve the alternating least\nsquares estimator and other forms of the high order orthogonal iteration for\ntensors with low or moderately high CP ranks. Our theoretical investigation\nprovides estimation accuracy and statistical convergence rates for the two\nproposed algorithms. Our implementations on synthetic data demonstrate\nsignificant practical superiority of our approach over existing methods.",
          "link": "http://arxiv.org/abs/2108.04428",
          "publishedOn": "2021-08-11T01:55:21.207Z",
          "wordCount": 649,
          "title": "Tensor Principal Component Analysis in High Dimensional CP Models. (arXiv:2108.04428v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1\">Nathalie Baracaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_J/0/1/0/all/0/1\">James Joshi</a>",
          "description": "Machine learning (ML) is increasingly being adopted in a wide variety of\napplication domains. Usually, a well-performing ML model, especially, emerging\ndeep neural network model, relies on a large volume of training data and\nhigh-powered computational resources. The need for a vast volume of available\ndata raises serious privacy concerns because of the risk of leakage of highly\nprivacy-sensitive information and the evolving regulatory environments that\nincreasingly restrict access to and use of privacy-sensitive data. Furthermore,\na trained ML model may also be vulnerable to adversarial attacks such as\nmembership/property inference attacks and model inversion attacks. Hence,\nwell-designed privacy-preserving ML (PPML) solutions are crucial and have\nattracted increasing research interest from academia and industry. More and\nmore efforts of PPML are proposed via integrating privacy-preserving techniques\ninto ML algorithms, fusing privacy-preserving approaches into ML pipeline, or\ndesigning various privacy-preserving architectures for existing ML systems. In\nparticular, existing PPML arts cross-cut ML, system, security, and privacy;\nhence, there is a critical need to understand state-of-art studies, related\nchallenges, and a roadmap for future research. This paper systematically\nreviews and summarizes existing privacy-preserving approaches and proposes a\nPGU model to guide evaluation for various PPML solutions through elaborately\ndecomposing their privacy-preserving functionalities. The PGU model is designed\nas the triad of Phase, Guarantee, and technical Utility. Furthermore, we also\ndiscuss the unique characteristics and challenges of PPML and outline possible\ndirections of future work that benefit a wide range of research communities\namong ML, distributed systems, security, and privacy areas.",
          "link": "http://arxiv.org/abs/2108.04417",
          "publishedOn": "2021-08-11T01:55:21.191Z",
          "wordCount": 682,
          "title": "Privacy-Preserving Machine Learning: Methods, Challenges and Directions. (arXiv:2108.04417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1\">Shaheer U. Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunguan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1\">Vasilis Stavrinides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M. C. Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Richard E. Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey A. Sonn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>",
          "description": "The performance of many medical image analysis tasks are strongly associated\nwith image data quality. When developing modern deep learning algorithms,\nrather than relying on subjective (human-based) image quality assessment (IQA),\ntask amenability potentially provides an objective measure of task-specific\nimage quality. To predict task amenability, an IQA agent is trained using\nreinforcement learning (RL) with a simultaneously optimised task predictor,\nsuch as a classification or segmentation neural network. In this work, we\ndevelop transfer learning or adaptation strategies to increase the adaptability\nof both the IQA agent and the task predictor so that they are less dependent on\nhigh-quality, expert-labelled training data. The proposed transfer learning\nstrategy re-formulates the original RL problem for task amenability in a\nmeta-reinforcement learning (meta-RL) framework. The resulting algorithm\nfacilitates efficient adaptation of the agent to different definitions of image\nquality, each with its own Markov decision process environment including\ndifferent images, labels and an adaptable task predictor. Our work demonstrates\nthat the IQA agents pre-trained on non-expert task labels can be adapted to\npredict task amenability as defined by expert task labels, using only a small\nset of expert labels. Using 6644 clinical ultrasound images from 249 prostate\ncancer patients, our results for image classification and segmentation tasks\nshow that the proposed IQA method can be adapted using data with as few as\nrespective 19.7% and 29.6% expert-reviewed consensus labels and still achieve\ncomparable IQA and task performance, which would otherwise require a training\ndataset with 100% expert labels.",
          "link": "http://arxiv.org/abs/2108.04359",
          "publishedOn": "2021-08-11T01:55:21.186Z",
          "wordCount": 727,
          "title": "Adaptable image quality assessment using meta-reinforcement learning of task amenability. (arXiv:2108.04359v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1\">Balagopal Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1\">Shafa Balaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan Sheng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1\">Pavitra Krishnaswamy</a>",
          "description": "Deep learning models achieve strong performance for radiology image\nclassification, but their practical application is bottlenecked by the need for\nlarge labeled training datasets. Semi-supervised learning (SSL) approaches\nleverage small labeled datasets alongside larger unlabeled datasets and offer\npotential for reducing labeling cost. In this work, we introduce NoTeacher, a\nnovel consistency-based SSL framework which incorporates probabilistic\ngraphical models. Unlike Mean Teacher which maintains a teacher network updated\nvia a temporal ensemble, NoTeacher employs two independent networks, thereby\neliminating the need for a teacher network. We demonstrate how NoTeacher can be\ncustomized to handle a range of challenges in radiology image classification.\nSpecifically, we describe adaptations for scenarios with 2D and 3D inputs, uni\nand multi-label classification, and class distribution mismatch between labeled\nand unlabeled portions of the training data. In realistic empirical evaluations\non three public benchmark datasets spanning the workhorse modalities of\nradiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the\nfully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher\noutperforms established SSL methods with minimal hyperparameter tuning, and has\nimplications as a principled and practical option for semisupervised learning\nin radiology applications.",
          "link": "http://arxiv.org/abs/2108.04423",
          "publishedOn": "2021-08-11T01:55:21.164Z",
          "wordCount": 669,
          "title": "Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean. (arXiv:2108.04423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1\">Yuki Tatsunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1\">Masato Taki</a>",
          "description": "For the past ten years, CNN has reigned supreme in the world of computer\nvision, but recently, Transformer is on the rise. However, the quadratic\ncomputational cost of self-attention has become a severe problem of practice.\nThere has been much research on architectures without CNN and self-attention in\nthis context. In particular, MLP-Mixer is a simple idea designed using MLPs and\nhit an accuracy comparable to the Vision Transformer. However, the only\ninductive bias in this architecture is the embedding of tokens. Thus, there is\nstill a possibility to build a non-convolutional inductive bias into the\narchitecture itself, and we built in an inductive bias using two simple ideas.\nA way is to divide the token-mixing block vertically and horizontally. Another\nway is to make spatial correlations denser among some channels of token-mixing.\nWith this approach, we were able to improve the accuracy of the MLP-Mixer while\nreducing its parameters and computational complexity. Compared to other\nMLP-based models, the proposed model, named RaftMLP has a good balance of\ncomputational complexity, the number of parameters, and actual memory usage. In\naddition, our work indicates that MLP-based models have the potential to\nreplace CNNs by adopting inductive bias. The source code in PyTorch version is\navailable at \\url{https://github.com/okojoalg/raft-mlp}.",
          "link": "http://arxiv.org/abs/2108.04384",
          "publishedOn": "2021-08-11T01:55:21.158Z",
          "wordCount": 651,
          "title": "RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?. (arXiv:2108.04384v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this paper, we present a new network named Attention Aware Network (AASeg)\nfor real time semantic image segmentation. Our network incorporates spatial and\nchannel information using Spatial Attention (SA) and Channel Attention (CA)\nmodules respectively. It also uses dense local multi-scale context information\nusing Multi Scale Context (MSC) module. The feature maps are concatenated\nindividually to produce the final segmentation map. We demonstrate the\neffectiveness of our method using a comprehensive analysis, quantitative\nexperimental results and ablation study using Cityscapes, ADE20K and Camvid\ndatasets. Our network performs better than most previous architectures with a\n74.4\\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.",
          "link": "http://arxiv.org/abs/2108.04349",
          "publishedOn": "2021-08-11T01:55:21.151Z",
          "wordCount": 556,
          "title": "AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonga_W/0/1/0/all/0/1\">Wenwen Gonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yucong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yawei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chena_Y/0/1/0/all/0/1\">Yifei Chena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lianyong Qi</a>",
          "description": "With the ever-increasing prevalence of web APIs (Application Programming\nInterfaces) in enabling smart software developments, finding and composing a\nlist of existing web APIs that can corporately fulfil the software developers'\nfunctional needs have become a promising way to develop a successful mobile\napp, economically and conveniently. However, the big volume and diversity of\ncandidate web APIs put additional burden on the app developers' web APIs\nselection decision-makings, since it is often a challenging task to\nsimultaneously guarantee the diversity and compatibility of the finally\nselected a set of web APIs. Considering this challenge, a Diversity-aware and\nCompatibility-driven web APIs Recommendation approach, namely DivCAR, is put\nforward in this paper. First, to achieve diversity, DivCAR employs random walk\nsampling technique on a pre-built correlation graph to generate diverse\ncorrelation subgraphs. Afterwards, with the diverse correlation subgraphs, we\nmodel the compatible web APIs recommendation problem to be a minimum group\nSteiner tree search problem. Through solving the minimum group Steiner tree\nsearch problem, manifold sets of compatible and diverse web APIs ranked are\nreturned to the app developers. At last, we design and enact a set of\nexperiments on a real-world dataset crawled from www.programmableWeb.com.\nExperimental results validate the effectiveness and efficiency of our proposed\nDivCAR approach in balancing the web APIs recommendation diversity and\ncompatibility.",
          "link": "http://arxiv.org/abs/2108.04389",
          "publishedOn": "2021-08-11T01:55:21.139Z",
          "wordCount": 655,
          "title": "Diversity-aware Web APIs Recommendation with Compatibility Guarantee. (arXiv:2108.04389v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04327",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1\">Karol Mikula</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1\">Michal Kollar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1\">Aneta A. Ozvat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1\">Martin Ambroz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1\">Lucia Cahojova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1\">Ivan Jarolimek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1\">Jozef Sibik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1\">Maria Sibikova</a>",
          "description": "Natural numerical networks are introduced as a new classification algorithm\nbased on the numerical solution of nonlinear partial differential equations of\nforward-backward diffusion type on complete graphs. The proposed natural\nnumerical network is applied to open important environmental and nature\nconservation task, the automated identification of protected habitats by using\nsatellite images. In the natural numerical network, the forward diffusion\ncauses the movement of points in a feature space toward each other. The\nopposite effect, keeping the points away from each other, is caused by backward\ndiffusion. This yields the desired classification. The natural numerical\nnetwork contains a few parameters that are optimized in the learning phase of\nthe method. After learning parameters and optimizing the topology of the\nnetwork graph, classification necessary for habitat identification is\nperformed. A relevancy map for each habitat is introduced as a tool for\nvalidating the classification and finding new Natura 2000 habitat appearances.",
          "link": "http://arxiv.org/abs/2108.04327",
          "publishedOn": "2021-08-11T01:55:21.132Z",
          "wordCount": 615,
          "title": "Natural Numerical Networks for Natura 2000 habitats classification by satellite images. (arXiv:2108.04327v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1\">Ashild Kummen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1\">Ali Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1\">Teodora Ganeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qianying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1\">Robert Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1\">Chenuka Ratwatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1\">Emil Almazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1\">Sheena Visram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Andrew Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1\">Neil J Sebire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1\">Lee Stott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1\">Yvonne Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1\">Graham Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1\">Dean Mohamedally</a>",
          "description": "Touchless computer interaction has become an important consideration during\nthe COVID-19 pandemic period. Despite progress in machine learning and computer\nvision that allows for advanced gesture recognition, an integrated collection\nof such open-source methods and a user-customisable approach to utilising them\nin a low-cost solution for touchless interaction in existing software is still\nmissing. In this paper, we introduce the MotionInput v2.0 application. This\napplication utilises published open-source libraries and additional gesture\ndefinitions developed to take the video stream from a standard RGB webcam as\ninput. It then maps human motion gestures to input operations for existing\napplications and games. The user can choose their own preferred way of\ninteracting from a series of motion types, including single and bi-modal hand\ngesturing, full-body repetitive or extremities-based exercises, head and facial\nmovements, eye tracking, and combinations of the above. We also introduce a\nseries of bespoke gesture recognition classifications as DirectInput triggers,\nincluding gestures for idle states, auto calibration, depth capture from a 2D\nRGB webcam stream and tracking of facial motions such as mouth motions,\nwinking, and head direction with rotation. Three use case areas assisted the\ndevelopment of the modules: creativity software, office and clinical software,\nand gaming software. A collection of open-source libraries has been integrated\nand provide a layer of modular gesture mapping on top of existing mouse and\nkeyboard controls in Windows via DirectX. With ease of access to webcams\nintegrated into most laptops and desktop computers, touchless computing becomes\nmore available with MotionInput v2.0, in a federated and locally processed\nmethod.",
          "link": "http://arxiv.org/abs/2108.04357",
          "publishedOn": "2021-08-11T01:55:21.119Z",
          "wordCount": 818,
          "title": "MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam. (arXiv:2108.04357v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1\">Aishwarza Panday</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Muhammad Ashad Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1\">Nihad Karim Chowdhury</a>",
          "description": "Due to the limited availability and high cost of the reverse\ntranscription-polymerase chain reaction (RT-PCR) test, many studies have\nproposed machine learning techniques for detecting COVID-19 from medical\nimaging. The purpose of this study is to systematically review, assess, and\nsynthesize research articles that have used different machine learning\ntechniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.\nA structured literature search was conducted in the relevant bibliographic\ndatabases to ensure that the survey solely centered on reproducible and\nhigh-quality research. We selected papers based on our inclusion criteria. In\nthis survey, we reviewed $98$ articles that fulfilled our inclusion criteria.\nWe have surveyed a complete pipeline of chest imaging analysis techniques\nrelated to COVID-19, including data collection, pre-processing, feature\nextraction, classification, and visualization. We have considered CT scans and\nX-rays as both are widely used to describe the latest developments in medical\nimaging to detect COVID-19. This survey provides researchers with valuable\ninsights into different machine learning techniques and their performance in\nthe detection and diagnosis of COVID-19 from chest imaging. At the end, the\nchallenges and limitations in detecting COVID-19 using machine learning\ntechniques and the future direction of research are discussed.",
          "link": "http://arxiv.org/abs/2108.04344",
          "publishedOn": "2021-08-11T01:55:21.078Z",
          "wordCount": 714,
          "title": "A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging. (arXiv:2108.04344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1\">Santiago Estrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1\">Kersten Diers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Weiyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1\">Philipp Ehses</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1\">Tony St&#xf6;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1\">Monique M.B Breteler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>",
          "description": "The neuroimage analysis community has neglected the automated segmentation of\nthe olfactory bulb (OB) despite its crucial role in olfactory function. The\nlack of an automatic processing method for the OB can be explained by its\nchallenging properties. Nonetheless, recent advances in MRI acquisition\ntechniques and resolution have allowed raters to generate more reliable manual\nannotations. Furthermore, the high accuracy of deep learning methods for\nsolving semantic segmentation problems provides us with an option to reliably\nassess even small structures. In this work, we introduce a novel, fast, and\nfully automated deep learning pipeline to accurately segment OB tissue on\nsub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we\ndesigned a three-stage pipeline: (1) Localization of a region containing both\nOBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized\nregion through four independent AttFastSurferCNN - a novel deep learning\narchitecture with a self-attention mechanism to improve modeling of contextual\ninformation, and (3) Ensemble of the predicted label maps. The OB pipeline\nexhibits high performance in terms of boundary delineation, OB localization,\nand volume estimation across a wide range of ages in 203 participants of the\nRhineland Study. Moreover, it also generalizes to scans of an independent\ndataset never encountered during training, the Human Connectome Project (HCP),\nwith different acquisition parameters and demographics, evaluated in 30 cases\nat the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.\nWe extensively validated our pipeline not only with respect to segmentation\naccuracy but also to known OB volume effects, where it can sensitively\nreplicate age effects.",
          "link": "http://arxiv.org/abs/2108.04267",
          "publishedOn": "2021-08-11T01:55:21.005Z",
          "wordCount": 707,
          "title": "Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI. (arXiv:2108.04267v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04289",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Krempl/0/1/0/all/0/1\">Krempl</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Georg/0/1/0/all/0/1\">Georg</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kottke/0/1/0/all/0/1\">Kottke</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Daniel/0/1/0/all/0/1\">Daniel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Minh_P/0/1/0/all/0/1\">Pham Minh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tuan/0/1/0/all/0/1\">Tuan</a>",
          "description": "Analysing correlations between streams of events is an important problem. It\narises for example in Neurosciences, when the connectivity of neurons should be\ninferred from spike trains that record neurons' individual spiking activity.\nWhile recently some approaches for inferring delayed synaptic connections have\nbeen proposed, they are limited in the types of connectivities and delays they\nare able to handle, or require computation-intensive procedures. This paper\nproposes a faster and more flexible approach for analysing such delayed\ncorrelated activity: a statistical approach for the Analysis of Connectivity in\nspiking Events (ACE), based on the idea of hypothesis testing. It first\ncomputes for any pair of a source and a target neuron the inter-spike delays\nbetween subsequent source- and target-spikes. Then, it derives a null model for\nthe distribution of inter-spike delays for \\emph{uncorrelated}~neurons.\nFinally, it compares the observed distribution of inter-spike delays to this\nnull model and infers pairwise connectivity based on the Pearson's Chi-squared\ntest statistic. Thus, ACE is capable to detect connections with a priori\nunknown, non-discrete (and potentially large) inter-spike delays, which might\nvary between pairs of neurons. Since ACE works incrementally, it has potential\nfor being used in online processing. In our experiments, we visualise the\nadvantages of ACE in varying experimental scenarios (except for one special\ncase) and in a state-of-the-art dataset which has been generated for\nneuro-scientific research under most realistic conditions.",
          "link": "http://arxiv.org/abs/2108.04289",
          "publishedOn": "2021-08-11T01:55:20.948Z",
          "wordCount": 696,
          "title": "ACE: A Novel Approach for the Statistical Analysis of Pairwise Connectivity. (arXiv:2108.04289v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04240",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chrysostomou_C/0/1/0/all/0/1\">Charalambos Chrysostomou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Alexandrou_F/0/1/0/all/0/1\">Floris Alexandrou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nicolaou_M/0/1/0/all/0/1\">Mihalis A. Nicolaou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Seker_H/0/1/0/all/0/1\">Huseyin Seker</a>",
          "description": "The Influenza virus can be considered as one of the most severe viruses that\ncan infect multiple species with often fatal consequences to the hosts. The\nHemagglutinin (HA) gene of the virus can be a target for antiviral drug\ndevelopment realised through accurate identification of its sub-types and\npossible the targeted hosts. This paper focuses on accurately predicting if an\nInfluenza type A virus can infect specific hosts, and more specifically, Human,\nAvian and Swine hosts, using only the protein sequence of the HA gene. In more\ndetail, we propose encoding the protein sequences into numerical signals using\nthe Hydrophobicity Index and subsequently utilising a Convolutional Neural\nNetwork-based predictive model. The Influenza HA protein sequences used in the\nproposed work are obtained from the Influenza Research Database (IRD).\nSpecifically, complete and unique HA protein sequences were used for avian,\nhuman and swine hosts. The data obtained for this work was 17999 human-host\nproteins, 17667 avian-host proteins and 9278 swine-host proteins. Given this\nset of collected proteins, the proposed method yields as much as 10% higher\naccuracy for an individual class (namely, Avian) and 5% higher overall accuracy\nthan in an earlier study. It is also observed that the accuracy for each class\nin this work is more balanced than what was presented in this earlier study. As\nthe results show, the proposed model can distinguish HA protein sequences with\nhigh accuracy whenever the virus under investigation can infect Human, Avian or\nSwine hosts.",
          "link": "http://arxiv.org/abs/2108.04240",
          "publishedOn": "2021-08-11T01:55:20.890Z",
          "wordCount": 682,
          "title": "Classification of Influenza Hemagglutinin Protein Sequences using Convolutional Neural Networks. (arXiv:2108.04240v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Gang Mei</a>",
          "description": "Geohazards such as landslides have caused great losses to the safety of\npeople's lives and property, which is often accompanied with surface cracks. If\nsuch surface cracks could be identified in time, it is of great significance\nfor the monitoring and early warning of geohazards. Currently, the most common\nmethod for crack identification is manual detection, which is with low\nefficiency and accuracy. In this paper, a deep transfer learning framework is\nproposed to effectively and efficiently identify slope surface cracks for the\nsake of fast monitoring and early warning of geohazards such as landslides. The\nessential idea is to employ transfer learning by training (a) the large sample\ndataset of concrete cracks and (b) the small sample dataset of soil and rock\nmasses cracks. In the proposed framework, (1) pretrained cracks identification\nmodels are constructed based on the large sample dataset of concrete cracks;\n(2) refined cracks identification models are further constructed based on the\nsmall sample dataset of soil and rock masses cracks. The proposed framework\ncould be applied to conduct UAV surveys on high-steep slopes to realize the\nmonitoring and early warning of landslides to ensure the safety of people's\nlives and property.",
          "link": "http://arxiv.org/abs/2108.04235",
          "publishedOn": "2021-08-11T01:55:20.882Z",
          "wordCount": 632,
          "title": "Deep Transfer Learning for Identifications of Slope Surface Cracks. (arXiv:2108.04235v1 [cs.CV])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}