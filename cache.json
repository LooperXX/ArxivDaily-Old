{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2101.11431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fareri_S/0/1/0/all/0/1\">Silvia Fareri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melluso_N/0/1/0/all/0/1\">Nicola Melluso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiarello_F/0/1/0/all/0/1\">Filippo Chiarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fantoni_G/0/1/0/all/0/1\">Gualtiero Fantoni</a>",
          "description": "In today's digital world, there is an increasing focus on soft skills. On the\none hand, they facilitate innovation at companies, but on the other, they are\nunlikely to be automated soon. Researchers struggle with accurately approaching\nquantitatively the study of soft skills due to the lack of data-driven methods\nto retrieve them. This limits the possibility for psychologists and HR managers\nto understand the relation between humans and digitalisation. This paper\npresents SkillNER, a novel data-driven method for automatically extracting soft\nskills from text. It is a named entity recognition (NER) system trained with a\nsupport vector machine (SVM) on a corpus of more than 5000 scientific papers.\nWe developed this system by measuring the performance of our approach against\ndifferent training models and validating the results together with a team of\npsychologists. Finally, SkillNER was tested in a real-world case study using\nthe job descriptions of ESCO (European Skill/Competence Qualification and\nOccupation) as textual source. The system enabled the detection of communities\nof job profiles based on their shared soft skills and communities of soft\nskills based on their shared job profiles. This case study demonstrates that\nthe tool can automatically retrieve soft skills from a large corpus in an\nefficient way, proving useful for firms, institutions, and workers. The tool is\nopen and available online to foster quantitative methods for the study of soft\nskills.",
          "link": "http://arxiv.org/abs/2101.11431",
          "publishedOn": "2021-07-14T01:41:49.489Z",
          "wordCount": 699,
          "title": "SkillNER: Mining and Mapping Soft Skills from any Text. (arXiv:2101.11431v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiusheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1\">Nikhil Bhendawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Ting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1\">Desheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1\">Bingyu Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>",
          "description": "Transformer-based models have made tremendous impacts in natural language\ngeneration. However the inference speed is a bottleneck due to large model size\nand intensive computing involved in auto-regressive decoding process. We\ndevelop FastSeq framework to accelerate sequence generation without accuracy\nloss. The proposed optimization techniques include an attention cache\noptimization, an efficient algorithm for detecting repeated n-grams, and an\nasynchronous generation pipeline with parallel I/O. These optimizations are\ngeneral enough to be applicable to Transformer-based models (e.g., T5, GPT2,\nand UniLM). Our benchmark results on a set of widely used and diverse models\ndemonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use\nwith a simple one-line code change. The source code is available at\nhttps://github.com/microsoft/fastseq.",
          "link": "http://arxiv.org/abs/2106.04718",
          "publishedOn": "2021-07-14T01:41:49.481Z",
          "wordCount": 595,
          "title": "FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.00773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>",
          "description": "This paper analyzes challenges in cloze-style reading comprehension on\nmultiparty dialogue and suggests two new tasks for more comprehensive\npredictions of personal entities in daily conversations. We first demonstrate\nthat there are substantial limitations to the evaluation methods of previous\nwork, namely that randomized assignment of samples to training and test data\nsubstantially decreases the complexity of cloze-style reading comprehension.\nAccording to our analysis, replacing the random data split with a chronological\ndata split reduces test accuracy on previous single-variable passage completion\ntask from 72\\% to 34\\%, that leaves much more room to improve. Our proposed\ntasks extend the previous single-variable passage completion task by replacing\nmore character mentions with variables. Several deep learning models are\ndeveloped to validate these three tasks. A thorough error analysis is provided\nto understand the challenges and guide the future direction of this research.",
          "link": "http://arxiv.org/abs/1911.00773",
          "publishedOn": "2021-07-14T01:41:49.461Z",
          "wordCount": 611,
          "title": "Design and Challenges of Cloze-Style Reading Comprehension Tasks on Multiparty Dialogue. (arXiv:1911.00773v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doo Soon Kim</a>",
          "description": "Since the first end-to-end neural coreference resolution model was\nintroduced, many extensions to the model have been proposed, ranging from using\nhigher-order inference to directly optimizing evaluation metrics using\nreinforcement learning. Despite improving the coreference resolution\nperformance by a large margin, these extensions add a lot of extra complexity\nto the original model. Motivated by this observation and the recent advances in\npre-trained Transformer language models, we propose a simple yet effective\nbaseline for coreference resolution. Our model is a simplified version of the\noriginal neural coreference resolution model, however, it achieves impressive\nperformance, outperforming all recent extended works on the public English\nOntoNotes benchmark. Our work provides evidence for the necessity of carefully\njustifying the complexity of existing or newly proposed models, as introducing\na conceptual or practical simplification to an existing model can still yield\ncompetitive results.",
          "link": "http://arxiv.org/abs/2107.01700",
          "publishedOn": "2021-07-14T01:41:49.447Z",
          "wordCount": 593,
          "title": "End-to-end Neural Coreference Resolution Revisited: A Simple yet Effective Baseline. (arXiv:2107.01700v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David I. Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adebonojo_D/0/1/0/all/0/1\">Damilola Adebonojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayeni_A/0/1/0/all/0/1\">Adesina Ayeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofe Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1\">Ayodele Awokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>",
          "description": "Massively multilingual machine translation (MT) has shown impressive\ncapabilities, including zero and few-shot translation between low-resource\nlanguage pairs. However, these models are often evaluated on high-resource\nlanguages with the assumption that they generalize to low-resource ones. The\ndifficulty of evaluating MT models on low-resource pairs is often due to lack\nof standardized evaluation datasets. In this paper, we present MENYO-20k, the\nfirst multi-domain parallel corpus with a special focus on clean orthography\nfor Yor\\`ub\\'a--English with standardized train-test splits for benchmarking.\nWe provide several neural MT benchmarks and compare them to the performance of\npopular pre-trained (massively multilingual) MT models both for the\nheterogeneous test set and its subdomains. Since these pre-trained models use\nhuge amounts of data with uncertain quality, we also analyze the effect of\ndiacritics, a major characteristic of Yor\\`ub\\'a, in the training data. We\ninvestigate how and when this training condition affects the final quality and\nintelligibility of a translation. Our models outperform massively multilingual\nmodels such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$ BLEU) when\ntranslating to Yor\\`ub\\'a, setting a high quality benchmark for future\nresearch.",
          "link": "http://arxiv.org/abs/2103.08647",
          "publishedOn": "2021-07-14T01:41:49.440Z",
          "wordCount": 662,
          "title": "The Effect of Domain and Diacritics in Yor\\`ub\\'a-English Neural Machine Translation. (arXiv:2103.08647v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1\">Pedram Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1\">Pouya Pezeshkpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminnaseri_M/0/1/0/all/0/1\">Moin Aminnaseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitaab_M/0/1/0/all/0/1\">Marzieh Bitaab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheini_M/0/1/0/all/0/1\">Mozhdeh Gheini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabiri_A/0/1/0/all/0/1\">Arman Kabiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1\">Rabeeh Karimi Mahabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memarrast_O/0/1/0/all/0/1\">Omid Memarrast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosallanezhad_A/0/1/0/all/0/1\">Ahmadreza Mosallanezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noury_E/0/1/0/all/0/1\">Erfan Noury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raji_S/0/1/0/all/0/1\">Shahab Raji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_S/0/1/0/all/0/1\">Sepideh Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azer_E/0/1/0/all/0/1\">Erfan Sadeqi Azer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samghabadi_N/0/1/0/all/0/1\">Niloofar Safi Samghabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_M/0/1/0/all/0/1\">Mahsa Shafaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheybani_S/0/1/0/all/0/1\">Saber Sheybani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1\">Ali Tazarv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>",
          "description": "Despite the progress made in recent years in addressing natural language\nunderstanding (NLU) challenges, the majority of this progress remains to be\nconcentrated on resource-rich languages like English. This work focuses on\nPersian language, one of the widely spoken languages in the world, and yet\nthere are few NLU datasets available for this rich language. The availability\nof high-quality evaluation datasets is a necessity for reliable assessment of\nthe progress on different NLU tasks and domains. We introduce ParsiNLU, the\nfirst benchmark in Persian language that includes a range of high-level tasks\n-- Reading Comprehension, Textual Entailment, etc. These datasets are collected\nin a multitude of ways, often involving manual annotations by native speakers.\nThis results in over 14.5$k$ new instances across 6 distinct NLU tasks.\nBesides, we present the first results on state-of-the-art monolingual and\nmulti-lingual pre-trained language-models on this benchmark and compare them\nwith human performance, which provides valuable insights into our ability to\ntackle natural language understanding challenges in Persian. We hope ParsiNLU\nfosters further research and advances in Persian language understanding.",
          "link": "http://arxiv.org/abs/2012.06154",
          "publishedOn": "2021-07-14T01:41:49.433Z",
          "wordCount": 703,
          "title": "ParsiNLU: A Suite of Language Understanding Challenges for Persian. (arXiv:2012.06154v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Jui Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>",
          "description": "In recent times, BERT based transformer models have become an inseparable\npart of the 'tech stack' of text processing models. Similar progress is being\nobserved in the speech domain with a multitude of models observing\nstate-of-the-art results by using audio transformer models to encode speech.\nThis begs the question of what are these audio transformer models learning.\nMoreover, although the standard methodology is to choose the last layer\nembedding for any downstream task, but is it the optimal choice? We try to\nanswer these questions for the two recent audio transformer models, Mockingjay\nand wave2vec2.0. We compare them on a comprehensive set of language delivery\nand structure features including audio, fluency and pronunciation features.\nAdditionally, we probe the audio models' understanding of textual surface,\nsyntax, and semantic features and compare them to BERT. We do this over\nexhaustive settings for native, non-native, synthetic, read and spontaneous\nspeech datasets",
          "link": "http://arxiv.org/abs/2101.00387",
          "publishedOn": "2021-07-14T01:41:49.397Z",
          "wordCount": 641,
          "title": "What all do audio transformer models hear? Probing Acoustic Representations for Language Delivery and its Structure. (arXiv:2101.00387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahim Shahriar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1\">Mueeze Al Mushabbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>",
          "description": "Chatbots are intelligent software built to be used as a replacement for human\ninteraction. However, existing studies typically do not provide enough support\nfor low-resource languages like Bangla. Moreover, due to the increasing\npopularity of social media, we can also see the rise of interactions in Bangla\ntransliteration (mostly in English) among the native Bangla speakers. In this\npaper, we propose a novel approach to build a Bangla chatbot aimed to be used\nas a business assistant which can communicate in Bangla and Bangla\nTransliteration in English with high confidence consistently. Since annotated\ndata was not available for this purpose, we had to work on the whole machine\nlearning life cycle (data preparation, machine learning modeling, and model\ndeployment) using Rasa Open Source Framework, fastText embeddings, Polyglot\nembeddings, Flask, and other systems as building blocks. While working with the\nskewed annotated dataset, we try out different setups and pipelines to evaluate\nwhich works best and provide possible reasoning behind the observed results.\nFinally, we present a pipeline for intent classification and entity extraction\nwhich achieves reasonable performance (accuracy: 83.02\\%, precision: 80.82\\%,\nrecall: 83.02\\%, F1-score: 80\\%).",
          "link": "http://arxiv.org/abs/2107.05541",
          "publishedOn": "2021-07-14T01:41:49.390Z",
          "wordCount": 652,
          "title": "End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengdi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Minghui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shaosheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>",
          "description": "In the Chinese medical insurance industry, the assessor's role is essential\nand requires significant efforts to converse with the claimant. This is a\nhighly professional job that involves many parts, such as identifying personal\ninformation, collecting related evidence, and making a final insurance report.\nDue to the coronavirus (COVID-19) pandemic, the previous offline insurance\nassessment has to be conducted online. However, for the junior assessor often\nlacking practical experience, it is not easy to quickly handle such a complex\nonline procedure, yet this is important as the insurance company needs to\ndecide how much compensation the claimant should receive based on the\nassessor's feedback. In order to promote assessors' work efficiency and speed\nup the overall procedure, in this paper, we propose a dialogue-based\ninformation extraction system that integrates advanced NLP technologies for\nmedical insurance assessment. With the assistance of our system, the average\ntime cost of the procedure is reduced from 55 minutes to 35 minutes, and the\ntotal human resources cost is saved 30% compared with the previous offline\nprocedure. Until now, the system has already served thousands of online claim\ncases.",
          "link": "http://arxiv.org/abs/2107.05866",
          "publishedOn": "2021-07-14T01:41:49.383Z",
          "wordCount": 687,
          "title": "A Dialogue-based Information Extraction System for Medical Insurance Assessment. (arXiv:2107.05866v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalamkar_P/0/1/0/all/0/1\">Prathamesh Kalamkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D%2E_J/0/1/0/all/0/1\">Janani Venugopalan Ph.D.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D_V/0/1/0/all/0/1\">Vivek Raghavan Ph.D</a>",
          "description": "Availability of challenging benchmarks is the key to advancement of AI in a\nspecific field.Since Legal Text is significantly different than normal English\ntext, there is a need to create separate Natural Language Processing benchmarks\nfor Indian Legal Text which are challenging and focus on tasks specific to\nLegal Systems. This will spur innovation in applications of Natural language\nProcessing for Indian Legal Text and will benefit AI community and Legal\nfraternity. We review the existing work in this area and propose ideas to\ncreate new benchmarks for Indian Legal Natural Language Processing.",
          "link": "http://arxiv.org/abs/2107.06056",
          "publishedOn": "2021-07-14T01:41:49.355Z",
          "wordCount": 526,
          "title": "Indian Legal NLP Benchmarks : A Survey. (arXiv:2107.06056v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.12006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>",
          "description": "We present a method to represent input texts by contextualizing them jointly\nwith dynamically retrieved textual encyclopedic background knowledge from\nmultiple documents. We apply our method to reading comprehension tasks by\nencoding questions and passages together with background sentences about the\nentities they mention. We show that integrating background knowledge from text\nis effective for tasks focusing on factual reasoning and allows direct reuse of\npowerful pretrained BERT-style encoders. Moreover, knowledge integration can be\nfurther improved with suitable pretraining via a self-supervised masked\nlanguage model objective over words in background-augmented input text. On\nTriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable\nRoBERTa models which do not integrate background knowledge dynamically. On\nMRQA, a large collection of diverse QA datasets, we see consistent gains\nin-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2\nF1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1).",
          "link": "http://arxiv.org/abs/2004.12006",
          "publishedOn": "2021-07-14T01:41:49.338Z",
          "wordCount": 612,
          "title": "Contextualized Representations Using Textual Encyclopedic Knowledge. (arXiv:2004.12006v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Boyuan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Augmenting pre-trained language models with knowledge graphs (KGs) has\nachieved success on various commonsense reasoning tasks. However, for a given\ntask instance, the KG, or certain parts of the KG, may not be useful. Although\nKG-augmented models often use attention to focus on specific KG components, the\nKG is still always used, and the attention mechanism is never explicitly taught\nwhich KG components should be used. Meanwhile, saliency methods can measure how\nmuch a KG feature (e.g., graph, node, path) influences the model to make the\ncorrect prediction, thus explaining which KG features are useful. This paper\nexplores how saliency explanations can be used to improve KG-augmented models'\nperformance. First, we propose to create coarse (Is the KG useful?) and fine\n(Which nodes/paths in the KG are useful?) saliency explanations. Second, we\npropose SalKG, a framework for KG-augmented models to learn from coarse and/or\nfine saliency explanations. Given saliency explanations created from a task's\ntraining set, SalKG jointly trains the model to predict the explanations, then\nsolve the task by attending to KG features highlighted by the predicted\nexplanations. On two popular commonsense QA benchmarks (CSQA, OBQA), we show\nthat \\textsc{SalKG} models can yield large performance gains -- up to 3.27% on\nCSQA.",
          "link": "http://arxiv.org/abs/2104.08793",
          "publishedOn": "2021-07-14T01:41:49.331Z",
          "wordCount": 675,
          "title": "SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning. (arXiv:2104.08793v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1\">Alina Karakanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>",
          "description": "Speech translation (ST) has lately received growing interest for the\ngeneration of subtitles without the need for an intermediate source language\ntranscription and timing (i.e. captions). However, the joint generation of\nsource captions and target subtitles does not only bring potential output\nquality advantages when the two decoding processes inform each other, but it is\nalso often required in multilingual scenarios. In this work, we focus on ST\nmodels which generate consistent captions-subtitles in terms of structure and\nlexical content. We further introduce new metrics for evaluating subtitling\nconsistency. Our findings show that joint decoding leads to increased\nperformance and consistency between the generated captions and subtitles while\nstill allowing for sufficient flexibility to produce subtitles conforming to\nlanguage-specific needs and norms.",
          "link": "http://arxiv.org/abs/2107.06246",
          "publishedOn": "2021-07-14T01:41:49.303Z",
          "wordCount": 563,
          "title": "Between Flexibility and Consistency: Joint Generation of Captions and Subtitles. (arXiv:2107.06246v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rossenbach_N/0/1/0/all/0/1\">Nick Rossenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilmes_B/0/1/0/all/0/1\">Benedikt Hilmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "Recent publications on automatic-speech-recognition (ASR) have a strong focus\non attention encoder-decoder (AED) architectures which tend to suffer from\nover-fitting in low resource scenarios. One solution to tackle this issue is to\ngenerate synthetic data with a trained text-to-speech system (TTS) if\nadditional text is available. This was successfully applied in many\npublications with AED systems, but only very limited in the context of other\nASR architectures. We investigate the effect of varying pre-processing, the\nspeaker embedding and input encoding of the TTS system w.r.t. the effectiveness\nof the synthesized data for AED-ASR training. Additionally, we also consider\ninternal language model subtraction for the first time, resulting in up to 38%\nrelative improvement. We compare the AED results to a state-of-the-art hybrid\nASR system, a monophone based system using\nconnectionist-temporal-classification (CTC) and a monotonic transducer based\nsystem. We show that for the later systems the addition of synthetic data has\nno relevant effect, but they still outperform the AED systems on\nLibriSpeech-100h. We achieve a final word-error-rate of 3.3%/10.0% with a\nhybrid system on the clean/noisy test-sets, surpassing any previous\nstate-of-the-art systems on Librispeech-100h that do not include unlabeled\naudio data.",
          "link": "http://arxiv.org/abs/2104.05379",
          "publishedOn": "2021-07-14T01:41:49.292Z",
          "wordCount": 681,
          "title": "Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures. (arXiv:2104.05379v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guojun Wu</a>",
          "description": "The rise of manipulating fake news as a political weapon has become a global\nconcern and highlighted the incapability of manually fact checking against\nrapidly produced fake news. Thus, statistical approaches are required if we are\nto address this problem efficiently. The shortage of publicly available\ndatasets is one major bottleneck of automated fact checking. To remedy this, we\ncollected 24K manually rated statements from PolitiFact. The class values\nexhibit a natural order with respect to truthfulness as shown in Table 1. Thus,\nour task represents a twist from standard classification, due to the various\ndegrees of similarity between classes. To investigate this, we defined\ncoarse-to-fine classification regimes, which presents new challenge for\nclassification. To address this, we propose BERT-based models. After training,\nclass similarity is sensible over the multi-class datasets, especially in the\nfine-grained one. Under all the regimes, BERT achieves state of the art, while\nthe additional layers provide insignificant improvement.",
          "link": "http://arxiv.org/abs/2107.06051",
          "publishedOn": "2021-07-14T01:41:49.285Z",
          "wordCount": 573,
          "title": "Rating Facts under Coarse-to-fine Regimes. (arXiv:2107.06051v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.01463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">Didier Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>",
          "description": "Adapter modules were recently introduced as an efficient alternative to\nfine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters\nof a model and injecting lightweight modules between layers, resulting in the\naddition of only a small number of task-specific trainable parameters. While\nadapter tuning was investigated for multilingual neural machine translation,\nthis paper proposes a comprehensive analysis of adapters for multilingual\nspeech translation (ST). Starting from different pre-trained models (a\nmultilingual ST trained on parallel data or a multilingual BART (mBART) trained\non non-parallel multilingual data), we show that adapters can be used to: (a)\nefficiently specialize ST to specific language pairs with a low extra cost in\nterms of parameters, and (b) transfer from an automatic speech recognition\n(ASR) task and an mBART pre-trained model to a multilingual ST task.\nExperiments show that adapter tuning offer competitive results to full\nfine-tuning, while being much more parameter-efficient.",
          "link": "http://arxiv.org/abs/2106.01463",
          "publishedOn": "2021-07-14T01:41:49.258Z",
          "wordCount": 616,
          "title": "Lightweight Adapter Tuning for Multilingual Speech Translation. (arXiv:2106.01463v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vydana_H/0/1/0/all/0/1\">Hari Krishna Vydana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karafiat_M/0/1/0/all/0/1\">Martin Karafi&#x27;at</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burget_L/0/1/0/all/0/1\">Luk&#x27;as Burget</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cernocky_%22/0/1/0/all/0/1\">&quot;Honza&quot; Cernock&#x27;y</a>",
          "description": "The paper describes BUT's English to German offline speech translation(ST)\nsystems developed for IWSLT2021. They are based on jointly trained Automatic\nSpeech Recognition-Machine Translation models. Their performances is evaluated\non MustC-Common test set. In this work, we study their efficiency from the\nperspective of having a large amount of separate ASR training data and MT\ntraining data, and a smaller amount of speech-translation training data. Large\namounts of ASR and MT training data are utilized for pre-training the ASR and\nMT models. Speech-translation data is used to jointly optimize ASR-MT models by\ndefining an end-to-end differentiable path from speech to translations. For\nthis purpose, we use the internal continuous representations from the\nASR-decoder as the input to MT module. We show that speech translation can be\nfurther improved by training the ASR-decoder jointly with the MT-module using\nlarge amount of text-only MT training data. We also show significant\nimprovements by training an ASR module capable of generating punctuated text,\nrather than leaving the punctuation task to the MT module.",
          "link": "http://arxiv.org/abs/2107.06155",
          "publishedOn": "2021-07-14T01:41:49.250Z",
          "wordCount": 611,
          "title": "The IWSLT 2021 BUT Speech Translation Systems. (arXiv:2107.06155v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renggli_C/0/1/0/all/0/1\">Cedric Renggli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaus_B/0/1/0/all/0/1\">Benjamin Glaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_M/0/1/0/all/0/1\">Maria Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troendle_M/0/1/0/all/0/1\">Marius Troendle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_N/0/1/0/all/0/1\">Nicolas Langer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Until recently, human behavioral data from reading has mainly been of\ninterest to researchers to understand human cognition. However, these human\nlanguage processing signals can also be beneficial in machine learning-based\nnatural language processing tasks. Using EEG brain activity to this purpose is\nlargely unexplored as of yet. In this paper, we present the first large-scale\nstudy of systematically analyzing the potential of EEG brain activity data for\nimproving natural language processing tasks, with a special focus on which\nfeatures of the signal are most beneficial. We present a multi-modal machine\nlearning architecture that learns jointly from textual input as well as from\nEEG features. We find that filtering the EEG signals into frequency bands is\nmore beneficial than using the broadband signal. Moreover, for a range of word\nembedding types, EEG data improves binary and ternary sentiment classification\nand outperforms multiple baselines. For more complex tasks such as relation\ndetection, further research is needed. Finally, EEG data shows to be\nparticularly promising when limited training data is available.",
          "link": "http://arxiv.org/abs/2102.08655",
          "publishedOn": "2021-07-14T01:41:49.242Z",
          "wordCount": 639,
          "title": "Decoding EEG Brain Activity for Multi-Modal Natural Language Processing. (arXiv:2102.08655v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+We_S/0/1/0/all/0/1\">Shi-jie We</a>",
          "description": "This paper presents the participation of the MiniTrue team in the FinSim-3\nshared task on learning semantic similarities for the financial domain in\nEnglish language. Our approach combines contextual embeddings learned by\ntransformer-based language models with network structures embeddings extracted\non external knowledge sources, to create more meaningful representations of\nfinancial domain entities and terms. For this, two BERT based language models\nand a knowledge graph embedding model are used. Besides, we propose a voting\nfunction to joint three basic models for the final inference. Experimental\nresults show that the model with the knowledge graph embeddings has achieved a\nsuperior result than these models with only contextual embeddings.\nNevertheless, we also observe that our voting function brings an extra benefit\nto the final system.",
          "link": "http://arxiv.org/abs/2107.05885",
          "publishedOn": "2021-07-14T01:41:49.233Z",
          "wordCount": 566,
          "title": "Exploiting Network Structures to Improve Semantic Representation for the Financial Domain. (arXiv:2107.05885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jiajie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>",
          "description": "Attention is a key mechanism for information selection in both biological\nbrains and many state-of-the-art deep neural networks (DNNs). Here, we\ninvestigate whether humans and DNNs allocate attention in comparable ways when\nreading a text passage to subsequently answer a specific question. We analyze 3\ntransformer-based DNNs that reach human-level performance when trained to\nperform the reading comprehension task. We find that the DNN attention\ndistribution quantitatively resembles human attention distribution measured by\nfixation times. Human readers fixate longer on words that are more relevant to\nthe question-answering task, demonstrating that attention is modulated by\ntop-down reading goals, on top of lower-level visual and text features of the\nstimulus. Further analyses reveal that the attention weights in DNNs are also\ninfluenced by both top-down reading goals and lower-level stimulus features,\nwith the shallow layers more strongly influenced by lower-level text features\nand the deep layers attending more to task-relevant words. Additionally, deep\nlayers' attention to task-relevant words gradually emerges when pre-trained DNN\nmodels are fine-tuned to perform the reading comprehension task, which\ncoincides with the improvement in task performance. These results demonstrate\nthat DNNs can evolve human-like attention distribution through task\noptimization, which suggests that human attention during goal-directed reading\ncomprehension is a consequence of task optimization.",
          "link": "http://arxiv.org/abs/2107.05799",
          "publishedOn": "2021-07-14T01:41:49.225Z",
          "wordCount": 641,
          "title": "Deep Neural Networks Evolve Human-like Attention Distribution during Reading Comprehension. (arXiv:2107.05799v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>",
          "description": "Transformer-based end-to-end speech recognition models have received\nconsiderable attention in recent years due to their high training speed and\nability to model a long-range global context. Position embedding in the\ntransformer architecture is indispensable because it provides supervision for\ndependency modeling between elements at different positions in the input\nsequence. To make use of the time order of the input sequence, many works\ninject some information about the relative or absolute position of the element\ninto the input sequence. In this work, we investigate various position\nembedding methods in the convolution-augmented transformer (conformer) and\nadopt a novel implementation named rotary position embedding (RoPE). RoPE\nencodes absolute positional information into the input sequence by a rotation\nmatrix, and then naturally incorporates explicit relative position information\ninto a self-attention module. To evaluate the effectiveness of the RoPE method,\nwe conducted experiments on AISHELL-1 and LibriSpeech corpora. Results show\nthat the conformer enhanced with RoPE achieves superior performance in the\nspeech recognition task. Specifically, our model achieves a relative word error\nrate reduction of 8.70% and 7.27% over the conformer on test-clean and\ntest-other sets of the LibriSpeech corpus respectively.",
          "link": "http://arxiv.org/abs/2107.05907",
          "publishedOn": "2021-07-14T01:41:49.217Z",
          "wordCount": 630,
          "title": "Conformer-based End-to-end Speech Recognition With Rotary Position Embedding. (arXiv:2107.05907v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nishtha Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovic_M/0/1/0/all/0/1\">Maja Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groves_D/0/1/0/all/0/1\">Declan Groves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>",
          "description": "Gender bias is a frequent occurrence in NLP-based applications, especially\npronounced in gender-inflected languages. Bias can appear through associations\nof certain adjectives and animate nouns with the natural gender of referents,\nbut also due to unbalanced grammatical gender frequencies of inflected words.\nThis type of bias becomes more evident in generating conversational utterances\nwhere gender is not specified within the sentence, because most current NLP\napplications still work on a sentence-level context. As a step towards more\ninclusive NLP, this paper proposes an automatic and generalisable rewriting\napproach for short conversational sentences. The rewriting method can be\napplied to sentences that, without extra-sentential context, have multiple\nequivalent alternatives in terms of gender. The method can be applied both for\ncreating gender balanced outputs as well as for creating gender balanced\ntraining data. The proposed approach is based on a neural machine translation\n(NMT) system trained to 'translate' from one gender alternative to another.\nBoth the automatic and manual analysis of the approach show promising results\nfor automatic generation of gender alternatives for conversational sentences in\nSpanish.",
          "link": "http://arxiv.org/abs/2107.05987",
          "publishedOn": "2021-07-14T01:41:49.168Z",
          "wordCount": 608,
          "title": "Generating Gender Augmented Data for NLP. (arXiv:2107.05987v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1\">Ahmet &#xdc;st&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sportel_S/0/1/0/all/0/1\">Stephan Sportel</a>",
          "description": "Identifying factors that make certain languages harder to model than others\nis essential to reach language equality in future Natural Language Processing\ntechnologies. Free-order case-marking languages, such as Russian, Latin or\nTamil, have proved more challenging than fixed-order languages for the tasks of\nsyntactic parsing and subject-verb agreement prediction. In this work, we\ninvestigate whether this class of languages is also more difficult to translate\nby state-of-the-art Neural Machine Translation models (NMT). Using a variety of\nsynthetic languages and a newly introduced translation challenge set, we find\nthat word order flexibility in the source language only leads to a very small\nloss of NMT quality, even though the core verb arguments become impossible to\ndisambiguate in sentences without semantic cues. The latter issue is indeed\nsolved by the addition of case marking. However, in medium- and low-resource\nsettings, the overall NMT quality of fixed-order languages remains unmatched.",
          "link": "http://arxiv.org/abs/2107.06055",
          "publishedOn": "2021-07-14T01:41:49.071Z",
          "wordCount": 585,
          "title": "On the Difficulty of Translating Free-Order Case-Marking Languages. (arXiv:2107.06055v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinyin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>",
          "description": "Named entity recognition (NER) is a well-studied task in natural language\nprocessing. Traditional NER research only deals with flat entities and ignores\nnested entities. The span-based methods treat entity recognition as a span\nclassification task. Although these methods have the innate ability to handle\nnested NER, they suffer from high computational cost, ignorance of boundary\ninformation, under-utilization of the spans that partially match with entities,\nand difficulties in long entity recognition. To tackle these issues, we propose\na two-stage entity identifier. First we generate span proposals by filtering\nand boundary regression on the seed spans to locate the entities, and then\nlabel the boundary-adjusted span proposals with the corresponding categories.\nOur method effectively utilizes the boundary information of entities and\npartially matched spans during training. Through boundary regression, entities\nof any length can be covered theoretically, which improves the ability to\nrecognize long entities. In addition, many low-quality seed spans are filtered\nout in the first stage, which reduces the time complexity of inference.\nExperiments on nested NER datasets demonstrate that our proposed method\noutperforms previous state-of-the-art models.",
          "link": "http://arxiv.org/abs/2105.06804",
          "publishedOn": "2021-07-14T01:41:49.059Z",
          "wordCount": 655,
          "title": "Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition. (arXiv:2105.06804v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tu Anh Dinh</a>",
          "description": "Speech Translation (ST) is the task of translating speech in one language\ninto text in another language. Traditional cascaded approaches for ST, using\nAutomatic Speech Recognition (ASR) and Machine Translation (MT) systems, are\nprone to error propagation. End-to-end approaches use only one system to avoid\npropagating error, yet are difficult to employ due to data scarcity. We explore\nzero-shot translation, which enables translating a pair of languages that is\nunseen during training, thus avoid the use of end-to-end ST data. Zero-shot\ntranslation has been shown to work for multilingual machine translation, yet\nhas not been studied for speech translation. We attempt to build zero-shot ST\nmodels that are trained only on ASR and MT tasks but can do ST task during\ninference. The challenge is that the representation of text and audio is\nsignificantly different, thus the models learn ASR and MT tasks in different\nways, making it non-trivial to perform zero-shot. These models tend to output\nthe wrong language when performing zero-shot ST. We tackle the issues by\nincluding additional training data and an auxiliary loss function that\nminimizes the text-audio difference. Our experiment results and analysis show\nthat the methods are promising for zero-shot ST. Moreover, our methods are\nparticularly useful in the few-shot settings where a limited amount of ST data\nis available, with improvements of up to +11.8 BLEU points compared to direct\nend-to-end ST models and +3.9 BLEU points compared to ST models fine-tuned from\npre-trained ASR model.",
          "link": "http://arxiv.org/abs/2107.06010",
          "publishedOn": "2021-07-14T01:41:49.035Z",
          "wordCount": 663,
          "title": "Zero-shot Speech Translation. (arXiv:2107.06010v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>",
          "description": "The predominant challenge in weakly supervised semantic parsing is that of\nspurious programs that evaluate to correct answers for the wrong reasons. Prior\nwork uses elaborate search strategies to mitigate the prevalence of spurious\nprograms; however, they typically consider only one input at a time. In this\nwork we explore the use of consistency between the output programs for related\ninputs to reduce the impact of spurious programs. We bias the program search\n(and thus the model's training signal) towards programs that map the same\nphrase in related inputs to the same sub-parts in their respective programs.\nAdditionally, we study the importance of designing logical formalisms that\nfacilitate this kind of consAistency-based training. We find that a more\nconsistent formalism leads to improved model performance even without\nconsistency-based training. When combined together, these two insights lead to\na 10% absolute improvement over the best prior result on the Natural Language\nVisual Reasoning dataset.",
          "link": "http://arxiv.org/abs/2107.05833",
          "publishedOn": "2021-07-14T01:41:48.992Z",
          "wordCount": 585,
          "title": "Enforcing Consistency in Weakly Supervised Semantic Parsing. (arXiv:2107.05833v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Williams_E/0/1/0/all/0/1\">Evan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_P/0/1/0/all/0/1\">Paul Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Sieu Tran</a>",
          "description": "This paper discusses the approach used by the Accenture Team for CLEF2021\nCheckThat! Lab, Task 1, to identify whether a claim made in social media would\nbe interesting to a wide audience and should be fact-checked. Twitter training\nand test data were provided in English, Arabic, Spanish, Turkish, and\nBulgarian. Claims were to be classified (check-worthy/not check-worthy) and\nranked in priority order for the fact-checker. Our method used deep neural\nnetwork transformer models with contextually sensitive lexical augmentation\napplied on the supplied training datasets to create additional training\nsamples. This augmentation approach improved the performance for all languages.\nOverall, our architecture and data augmentation pipeline produced the best\nsubmitted system for Arabic, and performance scales according to the quantity\nof provided training data for English, Spanish, Turkish, and Bulgarian. This\npaper investigates the deep neural network architectures for each language as\nwell as the provided data to examine why the approach worked so effectively for\nArabic, and discusses additional data augmentation measures that should could\nbe useful to this problem.",
          "link": "http://arxiv.org/abs/2107.05684",
          "publishedOn": "2021-07-14T01:41:48.984Z",
          "wordCount": 665,
          "title": "Accenture at CheckThat! 2021: Interesting claim identification and ranking with contextually sensitive lexical training data augmentation. (arXiv:2107.05684v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naylor_M/0/1/0/all/0/1\">Mitchell Naylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+French_C/0/1/0/all/0/1\">Christi French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terker_S/0/1/0/all/0/1\">Samantha Terker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_U/0/1/0/all/0/1\">Uday Kamath</a>",
          "description": "The healthcare domain is one of the most exciting application areas for\nmachine learning, but a lack of model transparency contributes to a lag in\nadoption within the industry. In this work, we explore the current art of\nexplainability and interpretability within a case study in clinical text\nclassification, using a task of mortality prediction within MIMIC-III clinical\nnotes. We demonstrate various visualization techniques for fully interpretable\nmethods as well as model-agnostic post hoc attributions, and we provide a\ngeneralized method for evaluating the quality of explanations using infidelity\nand local Lipschitz across model types from logistic regression to BERT\nvariants. With these metrics, we introduce a framework through which\npractitioners and researchers can assess the frontier between a model's\npredictive performance and the quality of its available explanations. We make\nour code available to encourage continued refinement of these methods.",
          "link": "http://arxiv.org/abs/2107.05693",
          "publishedOn": "2021-07-14T01:41:48.976Z",
          "wordCount": 598,
          "title": "Quantifying Explainability in NLP and Analyzing Algorithms for Performance-Explainability Tradeoff. (arXiv:2107.05693v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>",
          "description": "$\\textit{No man is an island.}$ Humans communicate with a large community by\ncoordinating with different interlocutors within short conversations. This\nability has been understudied by the research on building neural communicative\nagents. We study the task of few-shot $\\textit{language coordination}$: agents\nquickly adapting to their conversational partners' language abilities.\nDifferent from current communicative agents trained with self-play, we require\nthe lead agent to coordinate with a $\\textit{population}$ of agents with\ndifferent linguistic abilities, quickly adapting to communicate with unseen\nagents in the population. This requires the ability to model the partner's\nbeliefs, a vital component of human communication. Drawing inspiration from\ntheory-of-mind (ToM; Premack& Woodruff (1978)), we study the effect of the\nspeaker explicitly modeling the listeners' mental states. The speakers, as\nshown in our experiments, acquire the ability to predict the reactions of their\npartner, which helps it generate instructions that concisely express its\ncommunicative goal. We examine our hypothesis that the instructions generated\nwith ToM modeling yield better communication performance in both a referential\ngame and a language navigation task. Positive results from our experiments hint\nat the importance of explicitly modeling communication as a socio-pragmatic\nprogress.",
          "link": "http://arxiv.org/abs/2107.05697",
          "publishedOn": "2021-07-14T01:41:48.952Z",
          "wordCount": 627,
          "title": "Few-shot Language Coordination by Modeling Theory of Mind. (arXiv:2107.05697v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>",
          "description": "Active learning is the iterative construction of a classification model\nthrough targeted labeling, enabling significant labeling cost savings. As most\nresearch on active learning has been carried out before transformer-based\nlanguage models (\"transformers\") became popular, despite its practical\nimportance, comparably few papers have investigated how transformers can be\ncombined with active learning to date. This can be attributed to the fact that\nusing state-of-the-art query strategies for transformers induces a prohibitive\nruntime overhead, which effectively cancels out, or even outweighs\naforementioned cost savings. In this paper, we revisit uncertainty-based query\nstrategies, which had been largely outperformed before, but are particularly\nsuited in the context of fine-tuning transformers. In an extensive evaluation\non five widely used text classification benchmarks, we show that considerable\nimprovements of up to 14.4 percentage points in area under the learning curve\nare achieved, as well as a final accuracy close to the state of the art for all\nbut one benchmark, using only between 0.4% and 15% of the training data.",
          "link": "http://arxiv.org/abs/2107.05687",
          "publishedOn": "2021-07-14T01:41:48.940Z",
          "wordCount": 599,
          "title": "Uncertainty-based Query Strategies for Active Learning with Transformers. (arXiv:2107.05687v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05876",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>",
          "description": "Multilingual automatic speech recognition (ASR) models have shown great\npromise in recent years because of the simplified model training and deployment\nprocess. Conventional methods either train a universal multilingual model\nwithout taking any language information or with a 1-hot language ID (LID)\nvector to guide the recognition of the target language. In practice, the user\ncan be prompted to pre-select several languages he/she can speak. The\nmultilingual model without LID cannot well utilize the language information set\nby the user while the multilingual model with LID can only handle one\npre-selected language. In this paper, we propose a novel configurable\nmultilingual model (CMM) which is trained only once but can be configured as\ndifferent models based on users' choices by extracting language-specific\nmodules together with a universal model from the trained CMM. Particularly, a\nsingle CMM can be deployed to any user scenario where the users can pre-select\nany combination of languages. Trained with 75K hours of transcribed anonymized\nMicrosoft multilingual data and evaluated with 10-language test sets, the\nproposed CMM improves from the universal multilingual model by 26.0%, 16.9%,\nand 10.4% relative word error reduction when the user selects 1, 2, or 3\nlanguages, respectively. CMM also performs significantly better on\ncode-switching test sets.",
          "link": "http://arxiv.org/abs/2107.05876",
          "publishedOn": "2021-07-14T01:41:48.929Z",
          "wordCount": 659,
          "title": "A Configurable Multilingual Model is All You Need to Recognize All Languages. (arXiv:2107.05876v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>",
          "description": "Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.",
          "link": "http://arxiv.org/abs/2107.05768",
          "publishedOn": "2021-07-14T01:41:48.921Z",
          "wordCount": 664,
          "title": "Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keymanesh_M/0/1/0/all/0/1\">Moniba Keymanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1\">Tanya Berger-Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsner_M/0/1/0/all/0/1\">Micha Elsner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>",
          "description": "In many applications such as recidivism prediction, facility inspection, and\nbenefit assignment, it's important for individuals to know the\ndecision-relevant information for the model's prediction. In addition, the\nmodel's predictions should be fairly justified. Essentially, decision-relevant\nfeatures should provide sufficient information for the predicted outcome and\nshould be independent of the membership of individuals in protected groups such\nas race and gender. In this work, we focus on the problem of (un)fairness in\nthe justification of the text-based neural models. We tie the explanatory power\nof the model to fairness in the outcome and propose a fairness-aware\nsummarization mechanism to detect and counteract the bias in such models. Given\na potentially biased natural language explanation for a decision, we use a\nmulti-task neural model and an attribution mechanism based on integrated\ngradients to extract the high-utility and discrimination-free justifications in\nthe form of a summary. The extracted summary is then used for training a model\nto make decisions for individuals. Results on several real-world datasets\nsuggests that our method: (i) assists users to understand what information is\nused for the model's decision and (ii) enhances the fairness in outcomes while\nsignificantly reducing the demographic leakage.",
          "link": "http://arxiv.org/abs/2107.06243",
          "publishedOn": "2021-07-14T01:41:48.911Z",
          "wordCount": 633,
          "title": "Fairness-aware Summarization for Justified Decision-Making. (arXiv:2107.06243v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genzel_D/0/1/0/all/0/1\">Dmitriy Genzel</a>",
          "description": "Pretraining and multitask learning are widely used to improve the speech to\ntext translation performance. In this study, we are interested in training a\nspeech to text translation model along with an auxiliary text to text\ntranslation task. We conduct a detailed analysis to understand the impact of\nthe auxiliary task on the primary task within the multitask learning framework.\nOur analysis confirms that multitask learning tends to generate similar decoder\nrepresentations from different modalities and preserve more information from\nthe pretrained text translation modules. We observe minimal negative transfer\neffect between the two tasks and sharing more parameters is helpful to transfer\nknowledge from the text task to the speech task. The analysis also reveals that\nthe modality representation difference at the top decoder layers is still not\nnegligible, and those layers are critical for the translation quality. Inspired\nby these findings, we propose three methods to improve translation quality.\nFirst, a parameter sharing and initialization strategy is proposed to enhance\ninformation sharing between the tasks. Second, a novel attention-based\nregularization is proposed for the encoders and pulls the representations from\ndifferent modalities closer. Third, an online knowledge distillation is\nproposed to enhance the knowledge transfer from the text to the speech task.\nOur experiments show that the proposed approach improves translation\nperformance by more than 2 BLEU over a strong baseline and achieves\nstate-of-the-art results on the \\textsc{MuST-C} English-German, English-French\nand English-Spanish language pairs.",
          "link": "http://arxiv.org/abs/2107.05782",
          "publishedOn": "2021-07-14T01:41:48.902Z",
          "wordCount": 695,
          "title": "Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task. (arXiv:2107.05782v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinhong Zhang</a>",
          "description": "This paper describes our system at NLPTEA-2020 Task: Chinese Grammatical\nError Diagnosis (CGED). The goal of CGED is to diagnose four types of\ngrammatical errors: word selection (S), redundant words (R), missing words (M),\nand disordered words (W). The automatic CGED system contains two parts\nincluding error detection and error correction and our system is designed to\nsolve the error detection problem. Our system is built on three models: 1) a\nBERT-based model leveraging syntactic information; 2) a BERT-based model\nleveraging contextual embeddings; 3) a lexicon-based graph neural network\nleveraging lexical information. We also design an ensemble mechanism to improve\nthe single model's performance. Finally, our system achieves the highest F1\nscores at detection level and identification level among all teams\nparticipating in the CGED 2020 task.",
          "link": "http://arxiv.org/abs/2105.09085",
          "publishedOn": "2021-07-13T01:59:33.737Z",
          "wordCount": 589,
          "title": "Combining GCN and Transformer for Chinese Grammatical Error Detection. (arXiv:2105.09085v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Celli_F/0/1/0/all/0/1\">Fabio Celli</a>",
          "description": "In this technical report, we propose an algorithm, called Lex2vec that\nexploits lexical resources to inject information into word embeddings and name\nthe embedding dimensions by means of knowledge bases. We evaluate the optimal\nparameters to extract a number of informative labels that is readable and has a\ngood coverage for the embedding dimensions.",
          "link": "http://arxiv.org/abs/2103.02269",
          "publishedOn": "2021-07-13T01:59:33.699Z",
          "wordCount": 512,
          "title": "Lex2vec: making Explainable Word Embeddings via Lexical Resources. (arXiv:2103.02269v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Thi-Vinh Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong-Thai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1\">Thanh-Le Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_K/0/1/0/all/0/1\">Khac-Quy Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>",
          "description": "Prior works have demonstrated that a low-resource language pair can benefit\nfrom multilingual machine translation (MT) systems, which rely on many language\npairs' joint training. This paper proposes two simple strategies to address the\nrare word issue in multilingual MT systems for two low-resource language pairs:\nFrench-Vietnamese and English-Vietnamese. The first strategy is about dynamical\nlearning word similarity of tokens in the shared space among source languages\nwhile another one attempts to augment the translation ability of rare words\nthrough updating their embeddings during the training. Besides, we leverage\nmonolingual data for multilingual MT systems to increase the amount of\nsynthetic parallel corpora while dealing with the data sparsity problem. We\nhave shown significant improvements of up to +1.62 and +2.54 BLEU points over\nthe bilingual baseline systems for both language pairs and released our\ndatasets for the research community.",
          "link": "http://arxiv.org/abs/2012.08743",
          "publishedOn": "2021-07-13T01:59:33.693Z",
          "wordCount": 627,
          "title": "Improving Multilingual Neural Machine Translation For Low-Resource Languages: French,English - Vietnamese. (arXiv:2012.08743v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gamzu_I/0/1/0/all/0/1\">Iftah Gamzu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutiel_G/0/1/0/all/0/1\">Gilad Kutiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Ran Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1\">Eugene Agichtein</a>",
          "description": "In recent years online shopping has gained momentum and became an important\nvenue for customers wishing to save time and simplify their shopping process. A\nkey advantage of shopping online is the ability to read what other customers\nare saying about products of interest. In this work, we aim to maintain this\nadvantage in situations where extreme brevity is needed, for example, when\nshopping by voice. We suggest a novel task of extracting a single\nrepresentative helpful sentence from a set of reviews for a given product. The\nselected sentence should meet two conditions: first, it should be helpful for a\npurchase decision and second, the opinion it expresses should be supported by\nmultiple reviewers. This task is closely related to the task of Multi Document\nSummarization in the product reviews domain but differs in its objective and\nits level of conciseness. We collect a dataset in English of sentence\nhelpfulness scores via crowd-sourcing and demonstrate its reliability despite\nthe inherent subjectivity involved. Next, we describe a complete model that\nextracts representative helpful sentences with positive and negative sentiment\ntowards the product and demonstrate that it outperforms several baselines.",
          "link": "http://arxiv.org/abs/2104.09792",
          "publishedOn": "2021-07-13T01:59:33.642Z",
          "wordCount": 660,
          "title": "Identifying Helpful Sentences in Product Reviews. (arXiv:2104.09792v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>",
          "description": "We introduce a novel embedding model, named NoGE, which aims to integrate\nco-occurrence among entities and relations into graph neural networks to\nimprove knowledge graph completion (i.e., link prediction). Given a knowledge\ngraph, NoGE constructs a single graph considering entities and relations as\nindividual nodes. NoGE then computes weights for edges among nodes based on the\nco-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion\nGraph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector\nrepresentations for entity and relation nodes. NoGE then adopts a score\nfunction to produce the triple scores. Comprehensive experimental results show\nthat NoGE obtains state-of-the-art results on three new and difficult benchmark\ndatasets CoDEx for knowledge graph completion.",
          "link": "http://arxiv.org/abs/2104.07396",
          "publishedOn": "2021-07-13T01:59:33.622Z",
          "wordCount": 593,
          "title": "Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction. (arXiv:2104.07396v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1\">Rafa&#x142; Powalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1\">&#x141;ukasz Borchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1\">Dawid Jurkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1\">Tomasz Dwojak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1\">Micha&#x142; Pietruszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1\">Gabriela Pa&#x142;ka</a>",
          "description": "We address the challenging problem of Natural Language Comprehension beyond\nplain-text documents by introducing the TILT neural network architecture which\nsimultaneously learns layout information, visual features, and textual\nsemantics. Contrary to previous approaches, we rely on a decoder capable of\nunifying a variety of problems involving natural language. The layout is\nrepresented as an attention bias and complemented with contextualized visual\ninformation, while the core of our model is a pretrained encoder-decoder\nTransformer. Our novel approach achieves state-of-the-art results in extracting\ninformation from documents and answering questions which demand layout\nunderstanding (DocVQA, CORD, SROIE). At the same time, we simplify the process\nby employing an end-to-end model.",
          "link": "http://arxiv.org/abs/2102.09550",
          "publishedOn": "2021-07-13T01:59:33.584Z",
          "wordCount": 589,
          "title": "Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. (arXiv:2102.09550v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1\">Nihal Potdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1\">Anderson R. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yiran Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>",
          "description": "End-to-end spoken language understanding (SLU) has recently attracted\nincreasing interest. Compared to the conventional tandem-based approach that\ncombines speech recognition and language understanding as separate modules, the\nnew approach extracts users' intentions directly from the speech signals,\nresulting in joint optimization and low latency. Such an approach, however, is\ntypically designed to process one intention at a time, which leads users to\ntake multiple rounds to fulfill their requirements while interacting with a\ndialogue system. In this paper, we propose a streaming end-to-end framework\nthat can process multiple intentions in an online and incremental way. The\nbackbone of our framework is a unidirectional RNN trained with the\nconnectionist temporal classification (CTC) criterion. By this design, an\nintention can be identified when sufficient evidence has been accumulated, and\nmultiple intentions can be identified sequentially. We evaluate our solution on\nthe Fluent Speech Commands (FSC) dataset and the intent detection accuracy is\nabout 97 % on all multi-intent settings. This result is comparable to the\nperformance of the state-of-the-art non-streaming models, but is achieved in an\nonline and incremental way. We also employ our model to a keyword spotting task\nusing the Google Speech Commands dataset and the results are also highly\npromising.",
          "link": "http://arxiv.org/abs/2105.10042",
          "publishedOn": "2021-07-13T01:59:33.511Z",
          "wordCount": 692,
          "title": "A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_D/0/1/0/all/0/1\">Dhivya Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1\">Vijay Mago</a>",
          "description": "Semantic textual similarity is one of the open research challenges in the\nfield of Natural Language Processing. Extensive research has been carried out\nin this field and near-perfect results are achieved by recent transformer-based\nmodels in existing benchmark datasets like the STS dataset and the SICK\ndataset. In this paper, we study the sentences in these datasets and analyze\nthe sensitivity of various word embeddings with respect to the complexity of\nthe sentences. We build a complex sentences dataset comprising of 50 sentence\npairs with associated semantic similarity values provided by 15 human\nannotators. Readability analysis is performed to highlight the increase in\ncomplexity of the sentences in the existing benchmark datasets and those in the\nproposed dataset. Further, we perform a comparative analysis of the performance\nof various word embeddings and language models on the existing benchmark\ndatasets and the proposed dataset. The results show the increase in complexity\nof the sentences has a significant impact on the performance of the embedding\nmodels resulting in a 10-20% decrease in Pearson's and Spearman's correlation.",
          "link": "http://arxiv.org/abs/2010.12637",
          "publishedOn": "2021-07-13T01:59:33.486Z",
          "wordCount": 661,
          "title": "Comparative analysis of word embeddings in assessing semantic similarity of complex sentences. (arXiv:2010.12637v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaotao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "Due to the excessive cost of large-scale language model pre-training,\nconsiderable efforts have been made to train BERT progressively -- start from\nan inferior but low-cost model and gradually grow the model to increase the\ncomputational complexity. Our objective is to advance the understanding of\nTransformer growth and discover principles that guide progressive training.\nFirst, we find that similar to network architecture search, Transformer growth\nalso favors compound scaling. Specifically, while existing methods only conduct\nnetwork growth in a single dimension, we observe that it is beneficial to use\ncompound growth operators and balance multiple dimensions (e.g., depth, width,\nand input length of the model). Moreover, we explore alternative growth\noperators in each dimension via controlled comparison to give operator\nselection practical guidance. In light of our analyses, the proposed method\nspeeds up BERT pre-training by 73.6% and 82.2% for the base and large models\nrespectively, while achieving comparable performances",
          "link": "http://arxiv.org/abs/2010.12562",
          "publishedOn": "2021-07-13T01:59:33.466Z",
          "wordCount": 632,
          "title": "On the Transformer Growth for Progressive BERT Training. (arXiv:2010.12562v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wood_Doughty_Z/0/1/0/all/0/1\">Zach Wood-Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Paiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>",
          "description": "Computational social science studies often contextualize content analysis\nwithin standard demographics. Since demographics are unavailable on many social\nmedia platforms (e.g. Twitter) numerous studies have inferred demographics\nautomatically. Despite many studies presenting proof of concept inference of\nrace and ethnicity, training of practical systems remains elusive since there\nare few annotated datasets. Existing datasets are small, inaccurate, or fail to\ncover the four most common racial and ethnic groups in the United States. We\npresent a method to identify self-reports of race and ethnicity from Twitter\nprofile descriptions. Despite errors inherent in automated supervision, we\nproduce models with good performance when measured on gold standard self-report\nsurvey data. The result is a reproducible method for creating large-scale\ntraining resources for race and ethnicity.",
          "link": "http://arxiv.org/abs/2005.00635",
          "publishedOn": "2021-07-13T01:59:33.459Z",
          "wordCount": 600,
          "title": "Using Noisy Self-Reports to Predict Twitter User Demographics. (arXiv:2005.00635v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>",
          "description": "We propose a semi-supervised learning method for building end-to-end rich\ntranscription-style automatic speech recognition (RT-ASR) systems from\nsmall-scale rich transcription-style and large-scale common transcription-style\ndatasets. In spontaneous speech tasks, various speech phenomena such as\nfillers, word fragments, laughter and coughs, etc. are often included. While\ncommon transcriptions do not give special awareness to these phenomena, rich\ntranscriptions explicitly convert them into special phenomenon tokens as well\nas textual tokens. In previous studies, the textual and phenomenon tokens were\nsimultaneously estimated in an end-to-end manner. However, it is difficult to\nbuild accurate RT-ASR systems because large-scale rich transcription-style\ndatasets are often unavailable. To solve this problem, our training method uses\na limited rich transcription-style dataset and common transcription-style\ndataset simultaneously. The Key process in our semi-supervised learning is to\nconvert the common transcription-style dataset into a pseudo-rich\ntranscription-style dataset. To this end, we introduce style tokens which\ncontrol phenomenon tokens are generated or not into transformer-based\nautoregressive modeling. We use this modeling for generating the pseudo-rich\ntranscription-style datasets and for building RT-ASR system from the pseudo and\noriginal datasets. Our experiments on spontaneous ASR tasks showed the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.05382",
          "publishedOn": "2021-07-13T01:59:33.442Z",
          "wordCount": 648,
          "title": "End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning. (arXiv:2107.05382v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hromada_D/0/1/0/all/0/1\">Daniel Devatman Hromada</a>",
          "description": "Departing from the postulate that Voynich Manuscript is not a hoax but rather\nencodes authentic contents, our article presents an evolutionary algorithm\nwhich aims to find the most optimal mapping between voynichian glyphs and\ncandidate phonemic values. Core component of the decoding algorithm is a\nprocess of maximization of a fitness function which aims to find most optimal\nset of substitution rules allowing to transcribe the part of the manuscript --\nwhich we call the Calendar -- into lists of feminine names. This leads to sets\nof character subsitution rules which allow us to consistently transcribe dozens\namong three hundred calendar tokens into feminine names: a result far\nsurpassing both ``popular'' as well as \"state of the art\" tentatives to crack\nthe manuscript. What's more, by using name lists stemming from different\nlanguages as potential cribs, our ``adaptive'' method can also be useful in\nidentification of the language in which the manuscript is written.\n\nAs far as we can currently tell, results of our experiments indicate that the\nCalendar part of the manuscript contains names from baltoslavic, balkanic or\nhebrew language strata. Two further indications are also given: primo, highest\nfitness values were obtained when the crib list contains names with specific\ninfixes at token's penultimate position as is the case, for example, for slavic\n\\textbf{feminine diminutives} (i.e. names ending with -ka and not -a). In the\nmost successful scenario, 240 characters contained in 35 distinct Voynichese\ntokens were successfully transcribed. Secundo, in case of crib stemming from\nHebrew language, whole adaptation process converges to significantly better\nfitness values when transcribing voynichian tokens whose order of individual\ncharacters have been reversed, and when lists feminine and not masculine names\nare used as the crib.",
          "link": "http://arxiv.org/abs/2107.05381",
          "publishedOn": "2021-07-13T01:59:33.435Z",
          "wordCount": 730,
          "title": "Can Evolutionary Computation Help us to Crib the Voynich Manuscript ?. (arXiv:2107.05381v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Anish Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rudrajit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1\">Sujay Sanghavi</a>",
          "description": "In this paper we study test time decoding; an ubiquitous step in almost all\nsequential text generation task spanning across a wide array of natural\nlanguage processing (NLP) problems. Our main contribution is to develop a\ncontinuous relaxation framework for the combinatorial NP-hard decoding problem\nand propose Disco - an efficient algorithm based on standard first order\ngradient based. We provide tight analysis and show that our proposed algorithm\nlinearly converges to within $\\epsilon$ neighborhood of the optima. Finally, we\nperform preliminary experiments on the task of adversarial text generation and\nshow superior performance of Disco over several popular decoding approaches.",
          "link": "http://arxiv.org/abs/2107.05380",
          "publishedOn": "2021-07-13T01:59:33.426Z",
          "wordCount": 552,
          "title": "DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation. (arXiv:2107.05380v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1\">Sebastian Monka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1\">Lavdim Halilaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1\">Stefan Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>",
          "description": "Traditional computer vision approaches, based on neural networks (NN), are\ntypically trained on a large amount of image data. By minimizing the\ncross-entropy loss between a prediction and a given class label, the NN and its\nvisual embedding space are learned to fulfill a given task. However, due to the\nsole dependence on the image data distribution of the training domain, these\nmodels tend to fail when applied to a target domain that differs from their\nsource domain. To learn a more robust NN to domain shifts, we propose the\nknowledge graph neural network (KG-NN), a neuro-symbolic approach that\nsupervises the training using image-data-invariant auxiliary knowledge. The\nauxiliary knowledge is first encoded in a knowledge graph with respective\nconcepts and their relationships, which is then transformed into a dense vector\nrepresentation via an embedding method. Using a contrastive loss function,\nKG-NN learns to adapt its visual embedding space and thus its weights according\nto the image-data invariant knowledge graph embedding space. We evaluate KG-NN\non visual transfer learning tasks for classification using the mini-ImageNet\ndataset and its derivatives, as well as road sign recognition datasets from\nGermany and China. The results show that a visual model trained with a\nknowledge graph as a trainer outperforms a model trained with cross-entropy in\nall experiments, in particular when the domain gap increases. Besides better\nperformance and stronger robustness to domain shifts, these KG-NN adapts to\nmultiple datasets and classes without suffering heavily from catastrophic\nforgetting.",
          "link": "http://arxiv.org/abs/2102.08747",
          "publishedOn": "2021-07-13T01:59:33.414Z",
          "wordCount": 728,
          "title": "Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.07019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhijie Sasha Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christenson_L/0/1/0/all/0/1\">Lauren Christenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulton_L/0/1/0/all/0/1\">Lawrence Fulton</a>",
          "description": "Social media has become an essential channel for posting disaster-related\ninformation, which provide governments and relief agencies real-time data for\nbetter disaster management. However, research in this field has not received\nsufficient attention and extracting useful information is still challenging.\nThis paper aims to improve disaster relief efficiency via mining and analyzing\nsocial media data like public attitudes towards disaster response and public\ndemands for targeted relief supplies during different types of disasters. We\nfocus on different natural disasters based on properties such as types,\ndurations, and damages, which contains a total of 41,993 tweets. In this paper,\npublic perception is assessed qualitatively by manually classified tweets,\nwhich contain information like the demand for targeted relief supplies,\nsatisfactions of disaster response, and public fear. Public attitudes to\nnatural disasters are studied via a quantitative analysis using eight machine\nlearning models. To better provide decision-makers with the appropriate model,\nthe comparison of machine learning models based on computational time and\nprediction accuracy is conducted. The change of public opinion during different\nnatural disasters and the evolution of people's behavior of using social media\nfor disaster relief in the face of the identical type of natural disasters as\nTwitter continues to evolve are studied. The results in this paper demonstrate\nthe feasibility and validation of the proposed research approach and provide\nrelief agencies with insights into better disaster management.",
          "link": "http://arxiv.org/abs/2005.07019",
          "publishedOn": "2021-07-13T01:59:33.407Z",
          "wordCount": 737,
          "title": "Social Media Information Sharing for Natural Disaster Response. (arXiv:2005.07019v5 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1\">Yiming Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qingyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>",
          "description": "The fifth Oriental Language Recognition (OLR) Challenge focuses on language\nrecognition in a variety of complex environments to promote its development.\nThe OLR 2020 Challenge includes three tasks: (1) cross-channel language\nidentification, (2) dialect identification, and (3) noisy language\nidentification. We choose Cavg as the principle evaluation metric, and the\nEqual Error Rate (EER) as the secondary metric. There were 58 teams\nparticipating in this challenge and one third of the teams submitted valid\nresults. Compared with the best baseline, the Cavg values of Top 1 system for\nthe three tasks were relatively reduced by 82%, 62% and 48%, respectively. This\npaper describes the three tasks, the database profile, and the final results.\nWe also outline the novel approaches that improve the performance of language\nrecognition systems most significantly, such as the utilization of auxiliary\ninformation.",
          "link": "http://arxiv.org/abs/2107.05365",
          "publishedOn": "2021-07-13T01:59:33.383Z",
          "wordCount": 575,
          "title": "Oriental Language Recognition (OLR) 2020: Summary and Analysis. (arXiv:2107.05365v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Che Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>",
          "description": "There are two major classes of natural language grammar -- the dependency\ngrammar that models one-to-one correspondences between words and the\nconstituency grammar that models the assembly of one or several corresponded\nwords. While previous unsupervised parsing methods mostly focus on only\ninducing one class of grammars, we introduce a novel model, StructFormer, that\ncan simultaneously induce dependency and constituency structure. To achieve\nthis, we propose a new parsing framework that can jointly generate a\nconstituency tree and dependency graph. Then we integrate the induced\ndependency relations into the transformer, in a differentiable manner, through\na novel dependency-constrained self-attention mechanism. Experimental results\nshow that our model can achieve strong results on unsupervised constituency\nparsing, unsupervised dependency parsing, and masked language modeling at the\nsame time.",
          "link": "http://arxiv.org/abs/2012.00857",
          "publishedOn": "2021-07-13T01:59:33.377Z",
          "wordCount": 626,
          "title": "StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling. (arXiv:2012.00857v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1\">Kenneth Enevoldsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielbo_K/0/1/0/all/0/1\">Kristoffer Nielbo</a>",
          "description": "Danish natural language processing (NLP) has in recent years obtained\nconsiderable improvements with the addition of multiple new datasets and\nmodels. However, at present, there is no coherent framework for applying\nstate-of-the-art models for Danish. We present DaCy: a unified framework for\nDanish NLP built on SpaCy. DaCy uses efficient multitask models which obtain\nstate-of-the-art performance on named entity recognition, part-of-speech\ntagging, and dependency parsing. DaCy contains tools for easy integration of\nexisting models such as for polarity, emotion, or subjectivity detection. In\naddition, we conduct a series of tests for biases and robustness of Danish NLP\npipelines through augmentation of the test set of DaNE. DaCy large compares\nfavorably and is especially robust to long input lengths and spelling\nvariations and errors. All models except DaCy large display significant biases\nrelated to ethnicity while only Polyglot shows a significant gender bias. We\nargue that for languages with limited benchmark sets, data augmentation can be\nparticularly useful for obtaining more realistic and fine-grained performance\nestimates. We provide a series of augmenters as a first step towards a more\nthorough evaluation of language models for low and medium resource languages\nand encourage further development.",
          "link": "http://arxiv.org/abs/2107.05295",
          "publishedOn": "2021-07-13T01:59:33.354Z",
          "wordCount": 632,
          "title": "DaCy: A Unified Framework for Danish NLP. (arXiv:2107.05295v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianwen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianwei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shenghuan He</a>",
          "description": "In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.",
          "link": "http://arxiv.org/abs/2107.05377",
          "publishedOn": "2021-07-13T01:59:33.347Z",
          "wordCount": 580,
          "title": "A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengsen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jinqiao Dai</a>",
          "description": "Controlling the model to generate texts of different categories is a\nchallenging task that is getting more and more attention. Recently, generative\nadversarial net (GAN) has shown promising results in category text generation.\nHowever, the texts generated by GANs usually suffer from the problems of mode\ncollapse and training instability. To avoid the above problems, we propose a\nnovel model named category-aware variational recurrent neural network\n(CatVRNN), which is inspired by multi-task learning. In our model, generation\nand classification are trained simultaneously, aiming at generating texts of\ndifferent categories. Moreover, the use of multi-task learning can improve the\nquality of generated texts, when the classification task is appropriate. And we\npropose a function to initialize the hidden state of CatVRNN to force model to\ngenerate texts of a specific category. Experimental results on three datasets\ndemonstrate that our model can do better than several state-of-the-art text\ngeneration methods based GAN in the category accuracy and quality of generated\ntexts.",
          "link": "http://arxiv.org/abs/2107.05219",
          "publishedOn": "2021-07-13T01:59:33.333Z",
          "wordCount": 586,
          "title": "CatVRNN: Generating Category Texts via Multi-task Learning. (arXiv:2107.05219v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duzha_A/0/1/0/all/0/1\">Armend Duzha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casadei_C/0/1/0/all/0/1\">Cristiano Casadei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosi_M/0/1/0/all/0/1\">Michael Tosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celli_F/0/1/0/all/0/1\">Fabio Celli</a>",
          "description": "Accurate detection of hate speech against politicians, policy making and\npolitical ideas is crucial to maintain democracy and free speech.\nUnfortunately, the amount of labelled data necessary for training models to\ndetect hate speech are limited and domain-dependent. In this paper, we address\nthe issue of classification of hate speech against policy makers from Twitter\nin Italian, producing the first resource of this type in this language. We\ncollected and annotated 1264 tweets, examined the cases of disagreements\nbetween annotators, and performed in-domain and cross-domain hate speech\nclassifications with different features and algorithms. We achieved a\nperformance of ROC AUC 0.83 and analyzed the most predictive attributes, also\nfinding the different language features in the anti-policymakers and\nanti-immigration domains. Finally, we visualized networks of hashtags to\ncapture the topics used in hateful and normal tweets.",
          "link": "http://arxiv.org/abs/2107.05357",
          "publishedOn": "2021-07-13T01:59:33.314Z",
          "wordCount": 595,
          "title": "Hate versus Politics: Detection of Hate against Policy makers in Italian tweets. (arXiv:2107.05357v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuqing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin I. P. Rubinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>",
          "description": "Neural machine translation systems are known to be vulnerable to adversarial\ntest inputs, however, as we show in this paper, these systems are also\nvulnerable to training attacks. Specifically, we propose a poisoning attack in\nwhich a malicious adversary inserts a small poisoned sample of monolingual text\ninto the training set of a system trained using back-translation. This sample\nis designed to induce a specific, targeted translation behaviour, such as\npeddling misinformation. We present two methods for crafting poisoned examples,\nand show that only a tiny handful of instances, amounting to only 0.02% of the\ntraining set, is sufficient to enact a successful attack. We outline a defence\nmethod against said attacks, which partly ameliorates the problem. However, we\nstress that this is a blind-spot in modern NMT, demanding immediate attention.",
          "link": "http://arxiv.org/abs/2107.05243",
          "publishedOn": "2021-07-13T01:59:33.307Z",
          "wordCount": 600,
          "title": "Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning. (arXiv:2107.05243v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">E. Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">C. M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mieder_W/0/1/0/all/0/1\">W. Mieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">P. S. Dodds</a>",
          "description": "Proverbs are an essential component of language and culture, and though much\nattention has been paid to their history and currency, there has been\ncomparatively little quantitative work on changes in the frequency with which\nthey are used over time. With wider availability of large corpora reflecting\nmany diverse genres of documents, it is now possible to take a broad and\ndynamic view of the importance of the proverb. Here, we measure temporal\nchanges in the relevance of proverbs within three corpora, differing in kind,\nscale, and time frame: Millions of books over centuries; hundreds of millions\nof news articles over twenty years; and billions of tweets over a decade. We\nfind that proverbs present heavy-tailed frequency-of-usage rank distributions\nin each venue; exhibit trends reflecting the cultural dynamics of the eras\ncovered; and have evolved into contemporary forms on social media.",
          "link": "http://arxiv.org/abs/2107.04929",
          "publishedOn": "2021-07-13T01:59:33.301Z",
          "wordCount": 608,
          "title": "Computational Paremiology: Charting the temporal, ecological dynamics of proverb use in books, news articles, and tweets. (arXiv:2107.04929v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Luyao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yating Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>",
          "description": "Legal judgment prediction(LJP) is an essential task for legal AI. While prior\nmethods studied on this topic in a pseudo setting by employing the\njudge-summarized case narrative as the input to predict the judgment,\nneglecting critical case life-cycle information in real court setting could\nthreaten the case logic representation quality and prediction correctness. In\nthis paper, we introduce a novel challenging dataset from real courtrooms to\npredict the legal judgment in a reasonably encyclopedic manner by leveraging\nthe genuine input of the case -- plaintiff's claims and court debate data, from\nwhich the case's facts are automatically recognized by comprehensively\nunderstanding the multi-role dialogues of the court debate, and then learnt to\ndiscriminate the claims so as to reach the final judgment through multi-task\nlearning. An extensive set of experiments with a large civil trial data set\nshows that the proposed model can more accurately characterize the interactions\namong claims, fact and debate for legal judgment prediction, achieving\nsignificant improvements over strong state-of-the-art baselines. Moreover, the\nuser study conducted with real judges and law school students shows the neural\npredictions can also be interpretable and easily observed, and thus enhancing\nthe trial efficiency and judgment quality.",
          "link": "http://arxiv.org/abs/2107.05192",
          "publishedOn": "2021-07-13T01:59:33.292Z",
          "wordCount": 648,
          "title": "Legal Judgment Prediction with Multi-Stage CaseRepresentation Learning in the Real Court Setting. (arXiv:2107.05192v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangyue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Y. Chai</a>",
          "description": "In this paper, we study the problem of recognizing compositional\nattribute-object concepts within the zero-shot learning (ZSL) framework. We\npropose an episode-based cross-attention (EpiCA) network which combines merits\nof cross-attention mechanism and episode-based training strategy to recognize\nnovel compositional concepts. Firstly, EpiCA bases on cross-attention to\ncorrelate concept-visual information and utilizes the gated pooling layer to\nbuild contextualized representations for both images and concepts. The updated\nrepresentations are used for a more in-depth multi-modal relevance calculation\nfor concept recognition. Secondly, a two-phase episode training strategy,\nespecially the transductive phase, is adopted to utilize unlabeled test\nexamples to alleviate the low-resource learning problem. Experiments on two\nwidely-used zero-shot compositional learning (ZSCL) benchmarks have\ndemonstrated the effectiveness of the model compared with recent approaches on\nboth conventional and generalized ZSCL settings.",
          "link": "http://arxiv.org/abs/2107.05176",
          "publishedOn": "2021-07-13T01:59:33.279Z",
          "wordCount": 560,
          "title": "Zero-Shot Compositional Concept Learning. (arXiv:2107.05176v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chengrui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1\">Keyu An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huahuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>",
          "description": "The use of phonological features (PFs) potentially allows language-specific\nphones to remain linked in training, which is highly desirable for information\nsharing for multilingual and crosslingual speech recognition methods for\nlow-resourced languages. A drawback suffered by previous methods in using\nphonological features is that the acoustic-to-PF extraction in a bottom-up way\nis itself difficult. In this paper, we propose to join phonology driven phone\nembedding (top-down) and deep neural network (DNN) based acoustic feature\nextraction (bottom-up) to calculate phone probabilities. The new method is\ncalled JoinAP (Joining of Acoustics and Phonology). Remarkably, no inversion\nfrom acoustics to phonological features is required for speech recognition. For\neach phone in the IPA (International Phonetic Alphabet) table, we encode its\nphonological features to a phonological-vector, and then apply linear or\nnonlinear transformation of the phonological-vector to obtain the phone\nembedding. A series of multilingual and crosslingual (both zero-shot and\nfew-shot) speech recognition experiments are conducted on the CommonVoice\ndataset (German, French, Spanish and Italian) and the AISHLL-1 dataset\n(Mandarin), and demonstrate the superiority of JoinAP with nonlinear phone\nembeddings over both JoinAP with linear phone embeddings and the traditional\nmethod with flat phone embeddings.",
          "link": "http://arxiv.org/abs/2107.05038",
          "publishedOn": "2021-07-13T01:59:33.262Z",
          "wordCount": 636,
          "title": "Multilingual and crosslingual speech recognition using phonological-vector based phone embeddings. (arXiv:2107.05038v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1\">Seethalakshmi Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Victor Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_Powell_G/0/1/0/all/0/1\">Gus Hahn-Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirunagar_B/0/1/0/all/0/1\">Bharadwaj Tirunagar</a>",
          "description": "The number of research articles in business and management has dramatically\nincreased along with terminology, constructs, and measures. Proper\nclassification of organizational performance constructs from research articles\nplays an important role in categorizing the literature and understanding to\nwhom its research implications may be relevant. In this work, we classify\nconstructs (i.e., concepts and terminology used to capture different aspects of\norganizational performance) in research articles into a three-level\ncategorization: (a) performance and non-performance categories (Level 0); (b)\nfor performance constructs, stakeholder group-level of performance concerning\ninvestors, customers, employees, and the society (community and natural\nenvironment) (Level 1); and (c) for each stakeholder group-level, subcategories\nof different ways of measurement (Level 2). We observed that increasing\ncontextual information with features extracted from surrounding sentences and\nexternal references improves classification of disaggregate-level labels, given\nlimited training data. Our research has implications for computer-assisted\nconstruct identification and classification - an essential step for research\nsynthesis.",
          "link": "http://arxiv.org/abs/2107.05133",
          "publishedOn": "2021-07-13T01:59:33.252Z",
          "wordCount": 589,
          "title": "Computer-assisted construct classification of organizational performance concerning different stakeholder groups. (arXiv:2107.05133v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haipang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>",
          "description": "Most recently proposed approaches in dialogue state tracking (DST) leverage\nthe context and the last dialogue states to track current dialogue states,\nwhich are often slot-value pairs. Although the context contains the complete\ndialogue information, the information is usually indirect and even requires\nreasoning to obtain. The information in the lastly predicted dialogue states is\ndirect, but when there is a prediction error, the dialogue information from\nthis source will be incomplete or erroneous. In this paper, we propose the\nDialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States\nand Conversations network (FPDSC). This model extracts information of each\ndialogue turn by modeling interactions among each turn utterance, the\ncorresponding last dialogue states, and dialogue slots. Then the representation\nof each dialogue turn is aggregated by a hierarchical structure to form the\npassage information, which is utilized in the current turn of DST. Experimental\nresults validate the effectiveness of the fusion network with 55.03% and 59.07%\njoint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets, which reaches the\nstate-of-the-art performance. Furthermore, we conduct the deleted-value and\nrelated-slot experiments on MultiWOZ 2.1 to evaluate our model.",
          "link": "http://arxiv.org/abs/2107.05168",
          "publishedOn": "2021-07-13T01:59:33.246Z",
          "wordCount": 633,
          "title": "Dialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States and Conversations. (arXiv:2107.05168v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gaochen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu1_B/0/1/0/all/0/1\">Bin Xu1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bangchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongwen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dejie Chang</a>",
          "description": "Extractive Reading Comprehension (ERC) has made tremendous advances enabled\nby the availability of large-scale high-quality ERC training data. Despite of\nsuch rapid progress and widespread application, the datasets in languages other\nthan high-resource languages such as English remain scarce. To address this\nissue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by\nmodelling existing high-quality extractive reading comprehension datasets in a\nmultilingual environment. To be specific, we present multilingual adaptive\nattention (MAA) to combine intra-attention and inter-attention to learn more\ngeneral generalizable semantic and lexical knowledge from each pair of language\nfamilies. Furthermore, to make full use of existing datasets, we adopt a new\ntraining framework to train our model by calculating task-level similarities\nbetween each existing dataset and target dataset. The experimental results show\nthat our XLTT model surpasses six baselines on two multilingual ERC benchmarks,\nespecially more effective for low-resource languages with 3.9 and 4.1 average\nimprovement in F1 and EM, respectively.",
          "link": "http://arxiv.org/abs/2107.05002",
          "publishedOn": "2021-07-13T01:59:33.238Z",
          "wordCount": 594,
          "title": "Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking. (arXiv:2107.05002v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1\">Bryar A. Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1\">Tarik A. Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirjalili_S/0/1/0/all/0/1\">Seyedali Mirjalili</a>",
          "description": "It is beneficial to automate the process of deriving concept hierarchies from\ncorpora since a manual construction of concept hierarchies is typically a\ntime-consuming and resource-intensive process. As such, the overall process of\nlearning concept hierarchies from corpora encompasses a set of steps: parsing\nthe text into sentences, splitting the sentences and then tokenising it. After\nthe lemmatisation step, the pairs are extracted using FCA. However, there might\nbe some uninteresting and erroneous pairs in the formal context. Generating\nformal context may lead to a time-consuming process, so formal context size\nreduction is required to remove uninterested and erroneous pairs, taking less\ntime to extract the concept lattice and concept hierarchies accordingly. In\nthis premise, this study aims to propose two frameworks: (1) A framework to\nreview the current process of deriving concept hierarchies from corpus\nutilising FCA; (2) A framework to decrease the formal contexts ambiguity of the\nfirst framework using an adaptive version of ECA*. Experiments are conducted by\napplying 385 sample corpora from Wikipedia on the two frameworks to examine the\nreducing size of formal context, which leads to yield concept lattice and\nconcept hierarchy. The resulting lattice of formal context is evaluated to the\nstandard one using concept lattice-invariants. Accordingly, the homomorphic\nbetween the two lattices preserves the quality of resulting concept hierarchies\nby 89% in contrast to the basic ones, and the reduced concept lattice inherits\nthe structural relation of the standard one. The adaptive ECA* is examined\nagainst its four counterpart baseline algorithms to measure the execution time\non random datasets with different densities (fill ratios). The results show\nthat adaptive ECA* performs concept lattice faster than other mentioned\ncompetitive techniques in different fill ratios.",
          "link": "http://arxiv.org/abs/2107.04781",
          "publishedOn": "2021-07-13T01:59:33.228Z",
          "wordCount": 736,
          "title": "Formal context reduction in deriving concept hierarchies from corpora using adaptive evolutionary clustering algorithm star. (arXiv:2107.04781v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Ju-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>",
          "description": "Recently proposed self-supervised learning approaches have been successful\nfor pre-training speech representation models. The utility of these learned\nrepresentations has been observed empirically, but not much has been studied\nabout the type or extent of information encoded in the pre-trained\nrepresentations themselves. Developing such insights can help understand the\ncapabilities and limits of these models and enable the research community to\nmore efficiently develop their usage for downstream applications. In this work,\nwe begin to fill this gap by examining one recent and successful pre-trained\nmodel (wav2vec 2.0), via its intermediate representation vectors, using a suite\nof analysis tools. We use the metrics of canonical correlation, mutual\ninformation, and performance on simple downstream tasks with non-parametric\nprobes, in order to (i) query for acoustic and linguistic information content,\n(ii) characterize the evolution of information across model layers, and (iii)\nunderstand how fine-tuning the model for automatic speech recognition (ASR)\naffects these observations. Our findings motivate modifying the fine-tuning\nprotocol for ASR, which produces improved word error rates in a low-resource\nsetting.",
          "link": "http://arxiv.org/abs/2107.04734",
          "publishedOn": "2021-07-13T01:59:33.186Z",
          "wordCount": 612,
          "title": "Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gaochen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bangchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongwen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dejie Chang</a>",
          "description": "Although there are a small number of work to conduct patent research by\nbuilding knowledge graph, but without constructing patent knowledge graph using\npatent documents and combining latest natural language processing methods to\nmine hidden rich semantic relationships in existing patents and predict new\npossible patents. In this paper, we propose a new patent vacancy prediction\napproach named PatentMiner to mine rich semantic knowledge and predict new\npotential patents based on knowledge graph (KG) and graph attention mechanism.\nFirstly, patent knowledge graph over time (e.g. year) is constructed by\ncarrying out named entity recognition and relation extrac-tion from patent\ndocuments. Secondly, Common Neighbor Method (CNM), Graph Attention Networks\n(GAT) and Context-enhanced Graph Attention Networks (CGAT) are proposed to\nperform link prediction in the constructed knowledge graph to dig out the\npotential triples. Finally, patents are defined on the knowledge graph by means\nof co-occurrence relationship, that is, each patent is represented as a fully\nconnected subgraph containing all its entities and co-occurrence relationships\nof the patent in the knowledge graph; Furthermore, we propose a new patent\nprediction task which predicts a fully connected subgraph with newly added\nprediction links as a new pa-tent. The experimental results demonstrate that\nour proposed patent predic-tion approach can correctly predict new patents and\nContext-enhanced Graph Attention Networks is much better than the baseline.\nMeanwhile, our proposed patent vacancy prediction task still has significant\nroom to im-prove.",
          "link": "http://arxiv.org/abs/2107.04880",
          "publishedOn": "2021-07-13T01:59:33.177Z",
          "wordCount": 672,
          "title": "PatentMiner: Patent Vacancy Mining via Context-enhanced and Knowledge-guided Graph Attention. (arXiv:2107.04880v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J.F. Gales</a>",
          "description": "Text-based machine comprehension (MC) systems have a wide-range of\napplications, and standard corpora exist for developing and evaluating\napproaches. There has been far less research on spoken question answering (SQA)\nsystems. The SQA task considered in this paper is to extract the answer from a\ncandidate$\\text{'}$s spoken response to a question in a prompt-response style\nlanguage assessment test. Applying these MC approaches to this SQA task rather\nthan, for example, off-topic response detection provides far more detailed\ninformation that can be used for further downstream processing. One significant\nchallenge is the lack of appropriately annotated speech corpora to train\nsystems for this task. Hence, a transfer-learning style approach is adopted\nwhere a system trained on text-based MC is evaluated on an SQA task with\nnon-native speakers. Mismatches must be considered between text documents and\nspoken responses; non-native spoken grammar and written grammar. In practical\nSQA, ASR systems are used, necessitating an investigation of the impact of ASR\nerrors. We show that a simple text-based ELECTRA MC model trained on SQuAD2.0\ntransfers well for SQA. It is found that there is an approximately linear\nrelationship between ASR errors and the SQA assessment scores but grammar\nmismatches have minimal impact.",
          "link": "http://arxiv.org/abs/2107.04691",
          "publishedOn": "2021-07-13T01:59:33.140Z",
          "wordCount": 629,
          "title": "An Initial Investigation of Non-Native Spoken Question-Answering. (arXiv:2107.04691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1\">Pierce Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiatong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>",
          "description": "Automatic speech recognition (ASR) has become increasingly ubiquitous on\nmodern edge devices. Past work developed streaming End-to-End (E2E) all-neural\nspeech recognizers that can run compactly on edge devices. However, E2E ASR\nmodels are prone to overfitting and have difficulties in generalizing to unseen\ntesting data. Various techniques have been proposed to regularize the training\nof ASR models, including layer normalization, dropout, spectrum data\naugmentation and speed distortions in the inputs. In this work, we present a\nsimple yet effective noisy training strategy to further improve the E2E ASR\nmodel training. By introducing random noise to the parameter space during\ntraining, our method can produce smoother models at convergence that generalize\nbetter. We apply noisy training to improve both dense and sparse\nstate-of-the-art Emformer models and observe consistent WER reduction.\nSpecifically, when training Emformers with 90% sparsity, we achieve 12% and 14%\nWER improvements on the LibriSpeech Test-other and Test-clean data set,\nrespectively.",
          "link": "http://arxiv.org/abs/2107.04677",
          "publishedOn": "2021-07-13T01:59:33.110Z",
          "wordCount": 593,
          "title": "Noisy Training Improves E2E ASR for the Edge. (arXiv:2107.04677v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shrey Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rill_J/0/1/0/all/0/1\">Justin Rill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_B/0/1/0/all/0/1\">Brian Moran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_S/0/1/0/all/0/1\">Safiyyah Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zotov_A/0/1/0/all/0/1\">Alexander Zotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1\">Ahmed Aly</a>",
          "description": "Data efficiency, despite being an attractive characteristic, is often\nchallenging to measure and optimize for in task-oriented semantic parsing;\nunlike exact match, it can require both model- and domain-specific setups,\nwhich have, historically, varied widely across experiments. In our work, as a\nstep towards providing a unified solution to data-efficiency-related questions,\nwe introduce a four-stage protocol which gives an approximate measure of how\nmuch in-domain, \"target\" data a parser requires to achieve a certain quality\nbar. Specifically, our protocol consists of (1) sampling target subsets of\ndifferent cardinalities, (2) fine-tuning parsers on each subset, (3) obtaining\na smooth curve relating target subset (%) vs. exact match (%), and (4)\nreferencing the curve to mine ad-hoc (target subset, exact match) points. We\napply our protocol in two real-world case studies -- model generalizability and\nintent complexity -- illustrating its flexibility and applicability to\npractitioners in task-oriented semantic parsing.",
          "link": "http://arxiv.org/abs/2107.04736",
          "publishedOn": "2021-07-13T01:59:33.084Z",
          "wordCount": 583,
          "title": "Assessing Data Efficiency in Task-Oriented Semantic Parsing. (arXiv:2107.04736v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1\">Hang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Fine-tuning pre-trained language models such as BERT has become a common\npractice dominating leaderboards across various NLP tasks. Despite its recent\nsuccess and wide adoption, this process is unstable when there are only a small\nnumber of training samples available. The brittleness of this process is often\nreflected by the sensitivity to random seeds. In this paper, we propose to\ntackle this problem based on the noise stability property of deep nets, which\nis investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020).\nSpecifically, we introduce a novel and effective regularization method to\nimprove fine-tuning on NLP tasks, referred to as Layer-wise Noise Stability\nRegularization (LNSR). We extend the theories about adding noise to the input\nand prove that our method gives a stabler regularization effect. We provide\nsupportive evidence by experimentally confirming that well-performing models\nshow a low sensitivity to noise and fine-tuning with LNSR exhibits clearly\nhigher generalizability and stability. Furthermore, our method also\ndemonstrates advantages over other state-of-the-art algorithms including L2-SP\n(Li et al., 2018), Mixout (Lee et al., 2020) and SMART (Jiang et al., 2020).",
          "link": "http://arxiv.org/abs/2107.04835",
          "publishedOn": "2021-07-13T01:59:33.076Z",
          "wordCount": 633,
          "title": "Noise Stability Regularization for Improving BERT Fine-tuning. (arXiv:2107.04835v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.10433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>",
          "description": "We study the pre-train + fine-tune strategy for data-to-text tasks. Our\nexperiments indicate that text-to-text pre-training in the form of T5, enables\nsimple, end-to-end transformer based models to outperform pipelined neural\narchitectures tailored for data-to-text generation, as well as alternative\nlanguage model based pre-training techniques such as BERT and GPT-2.\nImportantly, T5 pre-training leads to better generalization, as evidenced by\nlarge improvements on out-of-domain test sets. We hope our work serves as a\nuseful baseline for future research, as transfer learning becomes ever more\nprevalent for data-to-text tasks.",
          "link": "http://arxiv.org/abs/2005.10433",
          "publishedOn": "2021-07-12T01:55:14.803Z",
          "wordCount": 549,
          "title": "Text-to-Text Pre-Training for Data-to-Text Tasks. (arXiv:2005.10433v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Potamias_R/0/1/0/all/0/1\">Rolandos Alexandros Potamias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siolas_G/0/1/0/all/0/1\">Georgios Siolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stafylopatis_A/0/1/0/all/0/1\">Andreas - Georgios Stafylopatis</a>",
          "description": "Recognition and classification of Figurative Language (FL) is an open problem\nof Sentiment Analysis in the broader field of Natural Language Processing (NLP)\ndue to the contradictory meaning contained in phrases with metaphorical\ncontent. The problem itself contains three interrelated FL recognition tasks:\nsarcasm, irony and metaphor which, in the present paper, are dealt with\nadvanced Deep Learning (DL) techniques. First, we introduce a data\nprepossessing framework towards efficient data representation formats so that\nto optimize the respective inputs to the DL models. In addition, special\nfeatures are extracted in order to characterize the syntactic, expressive,\nemotional and temper content reflected in the respective social media text\nreferences. These features aim to capture aspects of the social network user's\nwriting method. Finally, features are fed to a robust, Deep Ensemble Soft\nClassifier (DESC) which is based on the combination of different DL techniques.\nUsing three different benchmark datasets (one of them containing various FL\nforms) we conclude that the DESC model achieves a very good performance, worthy\nof comparison with relevant methodologies and state-of-the-art technologies in\nthe challenging field of FL recognition.",
          "link": "http://arxiv.org/abs/2107.04372",
          "publishedOn": "2021-07-12T01:55:14.789Z",
          "wordCount": 628,
          "title": "A Robust Deep Ensemble Classifier for Figurative Language Detection. (arXiv:2107.04372v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.09692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xusen Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>",
          "description": "Users frequently ask simple factoid questions for question answering (QA)\nsystems, attenuating the impact of myriad recent works that support more\ncomplex questions. Prompting users with automatically generated suggested\nquestions (SQs) can improve user understanding of QA system capabilities and\nthus facilitate more effective use. We aim to produce self-explanatory\nquestions that focus on main document topics and are answerable with variable\nlength passages as appropriate. We satisfy these requirements by using a\nBERT-based Pointer-Generator Network trained on the Natural Questions (NQ)\ndataset. Our model shows SOTA performance of SQ generation on the NQ dataset\n(20.1 BLEU-4). We further apply our model on out-of-domain news articles,\nevaluating with a QA system due to the lack of gold questions and demonstrate\nthat our model produces better SQs for news articles -- with further\nconfirmation via a human evaluation.",
          "link": "http://arxiv.org/abs/2010.09692",
          "publishedOn": "2021-07-12T01:55:14.669Z",
          "wordCount": 597,
          "title": "Summary-Oriented Question Generation for Informational Queries. (arXiv:2010.09692v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengge Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Lirong Dai</a>",
          "description": "This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous\nSpeech Translation task. We proposed a novel simultaneous translation model,\nCross Attention Augmented Transducer (CAAT), which extends conventional RNN-T\nto sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous\ntranslation. Experiments on speech-to-text (S2T) and text-to-text (T2T)\nsimultaneous translation tasks shows CAAT achieves better quality-latency\ntrade-offs compared to \\textit{wait-k}, one of the previous state-of-the-art\napproaches. Based on CAAT architecture and data augmentation, we build S2T and\nT2T simultaneous translation systems in this evaluation campaign. Compared to\nlast year's optimal systems, our S2T simultaneous translation system improves\nby an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous\ntranslation system improves by an average of 4.6 BLEU.",
          "link": "http://arxiv.org/abs/2107.00279",
          "publishedOn": "2021-07-12T01:55:14.661Z",
          "wordCount": 582,
          "title": "The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021. (arXiv:2107.00279v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panwar_M/0/1/0/all/0/1\">Madhur Panwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shailabh_S/0/1/0/all/0/1\">Shashank Shailabh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Milan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>",
          "description": "Topic models have been widely used to learn text representations and gain\ninsight into document corpora. To perform topic discovery, most existing neural\nmodels either take document bag-of-words (BoW) or sequence of tokens as input\nfollowed by variational inference and BoW reconstruction to learn topic-word\ndistribution. However, leveraging topic-word distribution for learning better\nfeatures during document encoding has not been explored much. To this end, we\ndevelop a framework TAN-NTM, which processes document as a sequence of tokens\nthrough a LSTM whose contextual outputs are attended in a topic-aware manner.\nWe propose a novel attention mechanism which factors in topic-word distribution\nto enable the model to attend on relevant words that convey topic related cues.\nThe output of topic attention module is then used to carry out variational\ninference. We perform extensive ablations and experiments resulting in ~9-15\npercentage improvement over score of existing SOTA topic models in NPMI\ncoherence on several benchmark datasets - 20Newsgroups, Yelp Review Polarity\nand AGNews. Further, we show that our method learns better latent\ndocument-topic features compared to existing topic models through improvement\non two downstream tasks: document classification and topic guided keyphrase\ngeneration.",
          "link": "http://arxiv.org/abs/2012.01524",
          "publishedOn": "2021-07-12T01:55:14.653Z",
          "wordCount": 659,
          "title": "TAN-NTM: Topic Attention Networks for Neural Topic Modeling. (arXiv:2012.01524v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mutian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingzhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1\">Frank K. Soong</a>",
          "description": "To scale neural speech synthesis to various real-world languages, we present\na multilingual end-to-end framework that maps byte inputs to spectrograms, thus\nallowing arbitrary input scripts. Besides strong results on 40+ languages, the\nframework demonstrates capabilities to adapt to new languages under extreme\nlow-resource and even few-shot scenarios of merely 40s transcribed recording,\nwithout the need of per-language resources like lexicon, extra corpus,\nauxiliary models, or linguistic expertise, thus ensuring scalability. While it\nretains satisfactory intelligibility and naturalness matching rich-resource\nmodels. Exhaustive comparative and ablation studies are performed to reveal the\npotential of the framework for low-resource languages. Furthermore, we propose\na novel method to extract language-specific sub-networks in a multilingual\nmodel for a better understanding of its mechanism.",
          "link": "http://arxiv.org/abs/2103.03541",
          "publishedOn": "2021-07-12T01:55:14.639Z",
          "wordCount": 596,
          "title": "Multilingual Byte2Speech Models for Scalable Low-resource Speech Synthesis. (arXiv:2103.03541v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1\">Tarun Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1\">Agus Sudjianto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Vijayan N. Nair</a>",
          "description": "Deep learning models for natural language processing (NLP) are inherently\ncomplex and often viewed as black box in nature. This paper develops an\napproach for interpreting convolutional neural networks for text classification\nproblems by exploiting the local-linear models inherent in ReLU-DNNs. The CNN\nmodel combines the word embedding through convolutional layers, filters them\nusing max-pooling, and optimizes using a ReLU-DNN for classification. To get an\noverall self-interpretable model, the system of local linear models from the\nReLU DNN are mapped back through the max-pool filter to the appropriate\nn-grams. Our results on experimental datasets demonstrate that our proposed\ntechnique produce parsimonious models that are self-interpretable and have\ncomparable performance with respect to a more complex CNN model. We also study\nthe impact of the complexity of the convolutional layers and the classification\nlayers on the model performance.",
          "link": "http://arxiv.org/abs/2105.08589",
          "publishedOn": "2021-07-12T01:55:14.612Z",
          "wordCount": 598,
          "title": "Self-interpretable Convolutional Neural Networks for Text Classification. (arXiv:2105.08589v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1\">Cl&#xe9;ment Rebuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberti_M/0/1/0/all/0/1\">Marco Roberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1\">Laure Soulier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1\">Rossella Cancelliere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "Data-to-Text Generation (DTG) is a subfield of Natural Language Generation\naiming at transcribing structured data in natural language descriptions. The\nfield has been recently boosted by the use of neural-based generators which\nexhibit on one side great syntactic skills without the need of hand-crafted\npipelines; on the other side, the quality of the generated text reflects the\nquality of the training data, which in realistic settings only offer\nimperfectly aligned structure-text pairs. Consequently, state-of-art neural\nmodels include misleading statements - usually called hallucinations - in their\noutputs. The control of this phenomenon is today a major challenge for DTG, and\nis the problem addressed in the paper.\n\nPrevious work deal with this issue at the instance level: using an alignment\nscore for each table-reference pair. In contrast, we propose a finer-grained\napproach, arguing that hallucinations should rather be treated at the word\nlevel. Specifically, we propose a Multi-Branch Decoder which is able to\nleverage word-level labels to learn the relevant parts of each training\ninstance. These labels are obtained following a simple and efficient scoring\nprocedure based on co-occurrence analysis and dependency parsing. Extensive\nevaluations, via automated metrics and human judgment on the standard WikiBio\nbenchmark, show the accuracy of our alignment labels and the effectiveness of\nthe proposed Multi-Branch Decoder. Our model is able to reduce and control\nhallucinations, while keeping fluency and coherence in generated texts. Further\nexperiments on a degraded version of ToTTo show that our model could be\nsuccessfully used on very noisy settings.",
          "link": "http://arxiv.org/abs/2102.02810",
          "publishedOn": "2021-07-12T01:55:14.598Z",
          "wordCount": 752,
          "title": "Controlling Hallucinations at Word Level in Data-to-Text Generation. (arXiv:2102.02810v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1\">Tarun Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Vijayan N. Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1\">Agus Sudjianto</a>",
          "description": "Deep neural networks are increasingly used in natural language processing\n(NLP) models. However, the need to interpret and explain the results from\ncomplex algorithms are limiting their widespread adoption in regulated\nindustries such as banking. There has been recent work on interpretability of\nmachine learning algorithms with structured data. But there are only limited\ntechniques for NLP applications where the problem is more challenging due to\nthe size of the vocabulary, high-dimensional nature, and the need to consider\ntextual coherence and language structure. This paper develops a methodology to\ncompute SHAP values for local explainability of CNN-based text classification\nmodels. The approach is also extended to compute global scores to assess the\nimportance of features. The results are illustrated on sentiment analysis of\nAmazon Electronic Review data.",
          "link": "http://arxiv.org/abs/2008.11825",
          "publishedOn": "2021-07-12T01:55:14.588Z",
          "wordCount": 597,
          "title": "SHAP values for Explaining CNN-based Text Classification Models. (arXiv:2008.11825v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trummer_I/0/1/0/all/0/1\">Immanuel Trummer</a>",
          "description": "For humans, it is often possible to predict data correlations from column\nnames. We conduct experiments to find out whether deep neural networks can\nlearn to do the same. If so, e.g., it would open up the possibility of tuning\ntools that use NLP analysis on schema elements to prioritize their efforts for\ncorrelation detection.\n\nWe analyze correlations for around 120,000 column pairs, taken from around\n4,000 data sets. We try to predict correlations, based on column names alone.\nFor predictions, we exploit pre-trained language models, based on the recently\nproposed Transformer architecture. We consider different types of correlations,\nmultiple prediction methods, and various prediction scenarios. We study the\nimpact of factors such as column name length or the amount of training data on\nprediction accuracy. Altogether, we find that deep neural networks can predict\ncorrelations with a relatively high accuracy in many scenarios (e.g., with an\naccuracy of 95% for long column names).",
          "link": "http://arxiv.org/abs/2107.04553",
          "publishedOn": "2021-07-12T01:55:14.578Z",
          "wordCount": 585,
          "title": "Can Deep Neural Networks Predict Data Correlations from Column Names?. (arXiv:2107.04553v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1\">Usman Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1\">Adam G. Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khushi_M/0/1/0/all/0/1\">Matloob Khushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>",
          "description": "The availability of biomedical text data and advances in natural language\nprocessing (NLP) have made new applications in biomedical NLP possible.\nLanguage models trained or fine tuned using domain specific corpora can\noutperform general models, but work to date in biomedical NLP has been limited\nin terms of corpora and tasks. We present BioALBERT, a domain-specific\nadaptation of A Lite Bidirectional Encoder Representations from Transformers\n(ALBERT), trained on biomedical (PubMed and PubMed Central) and clinical\n(MIMIC-III) corpora and fine tuned for 6 different tasks across 20 benchmark\ndatasets. Experiments show that BioALBERT outperforms the state of the art on\nnamed entity recognition (+11.09% BLURB score improvement), relation extraction\n(+0.80% BLURB score), sentence similarity (+1.05% BLURB score), document\nclassification (+0.62% F1-score), and question answering (+2.83% BLURB score).\nIt represents a new state of the art in 17 out of 20 benchmark datasets. By\nmaking BioALBERT models and data available, our aim is to help the biomedical\nNLP community avoid computational costs of training and establish a new set of\nbaselines for future efforts across a broad range of biomedical NLP tasks.",
          "link": "http://arxiv.org/abs/2107.04374",
          "publishedOn": "2021-07-12T01:55:14.570Z",
          "wordCount": 620,
          "title": "Benchmarking for Biomedical Natural Language Processing Tasks with a Domain Specific ALBERT. (arXiv:2107.04374v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter A Jansen</a>",
          "description": "Text Worlds are virtual environments for embodied agents that, unlike 2D or\n3D environments, are rendered exclusively using textual descriptions. These\nenvironments offer an alternative to higher-fidelity 3D environments due to\ntheir low barrier to entry, providing the ability to study semantics,\ncompositional inference, and other high-level tasks with rich high-level action\nspaces while controlling for perceptual input. This systematic survey outlines\nrecent developments in tooling, environments, and agent modeling for Text\nWorlds, while examining recent trends in knowledge graphs, common sense\nreasoning, transfer learning of Text World performance to higher-fidelity\nenvironments, as well as near-term development targets that, once achieved,\nmake Text Worlds an attractive general research paradigm for natural language\nprocessing.",
          "link": "http://arxiv.org/abs/2107.04132",
          "publishedOn": "2021-07-12T01:55:14.562Z",
          "wordCount": 552,
          "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments. (arXiv:2107.04132v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Neural approaches have achieved state-of-the-art accuracy on machine\ntranslation but suffer from the high cost of collecting large scale parallel\ndata. Thus, a lot of research has been conducted for neural machine translation\n(NMT) with very limited parallel data, i.e., the low-resource setting. In this\npaper, we provide a survey for low-resource NMT and classify related works into\nthree categories according to the auxiliary data they used: (1) exploiting\nmonolingual data of source and/or target languages, (2) exploiting data from\nauxiliary languages, and (3) exploiting multi-modal data. We hope that our\nsurvey can help researchers to better understand this field and inspire them to\ndesign better algorithms, and help industry practitioners to choose appropriate\nalgorithms for their applications.",
          "link": "http://arxiv.org/abs/2107.04239",
          "publishedOn": "2021-07-12T01:55:14.525Z",
          "wordCount": 578,
          "title": "A Survey on Low-Resource Neural Machine Translation. (arXiv:2107.04239v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xinying Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanwu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dawei Lu</a>",
          "description": "Deep learning models for automatic readability assessment generally discard\nlinguistic features traditionally used in machine learning models for the task.\nWe propose to incorporate linguistic features into neural network models by\nlearning syntactic dense embeddings based on linguistic features. To cope with\nthe relationships between the features, we form a correlation graph among\nfeatures and use it to learn their embeddings so that similar features will be\nrepresented by similar embeddings. Experiments with six data sets of two\nproficiency levels demonstrate that our proposed methodology can complement\nBERT-only model to achieve significantly better performances for automatic\nreadability assessment.",
          "link": "http://arxiv.org/abs/2107.04268",
          "publishedOn": "2021-07-12T01:55:14.501Z",
          "wordCount": 554,
          "title": "Learning Syntactic Dense Embedding with Correlation Graph for Automatic Readability Assessment. (arXiv:2107.04268v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1\">Diptanu Gon Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frank Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kritika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Assaf Sela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>",
          "description": "Language identification greatly impacts the success of downstream tasks such\nas automatic speech recognition. Recently, self-supervised speech\nrepresentations learned by wav2vec 2.0 have been shown to be very effective for\na range of speech tasks. We extend previous self-supervised work on language\nidentification by experimenting with pre-trained models which were learned on\nreal-world unconstrained speech in multiple languages and not just on English.\nWe show that models pre-trained on many languages perform better and enable\nlanguage identification systems that require very little labeled data to\nperform well. Results on a 25 languages setup show that with only 10 minutes of\nlabeled data per language, a cross-lingually pre-trained model can achieve over\n93% accuracy.",
          "link": "http://arxiv.org/abs/2107.04082",
          "publishedOn": "2021-07-12T01:55:14.471Z",
          "wordCount": 567,
          "title": "Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Han He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>",
          "description": "Coupled with biaffine decoders, transformers have been effectively adapted to\ntext-to-graph transduction and achieved state-of-the-art performance on AMR\nparsing. Many prior works, however, rely on the biaffine decoder for either or\nboth arc and label predictions although most features used by the decoder may\nbe learned by the transformer already. This paper presents a novel approach to\nAMR parsing by combining heterogeneous data (tokens, concepts, labels) as one\ninput to a transformer to learn attention, and use only attention matrices from\nthe transformer to predict all elements in AMR graphs (concepts, arcs, labels).\nAlthough our models use significantly fewer parameters than the previous\nstate-of-the-art graph parser, they show similar or better accuracy on AMR 2.0\nand 3.0.",
          "link": "http://arxiv.org/abs/2107.04152",
          "publishedOn": "2021-07-12T01:55:14.460Z",
          "wordCount": 560,
          "title": "Levi Graph AMR Parser using Heterogeneous Attention. (arXiv:2107.04152v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zeyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thuy Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>",
          "description": "This paper studies joint models for selecting correct answer sentences among\nthe top $k$ provided by answer sentence selection (AS2) modules, which are core\ncomponents of retrieval-based Question Answering (QA) systems. Our work shows\nthat a critical step to effectively exploit an answer set regards modeling the\ninterrelated information between pair of answers. For this purpose, we build a\nthree-way multi-classifier, which decides if an answer supports, refutes, or is\nneutral with respect to another one. More specifically, our neural architecture\nintegrates a state-of-the-art AS2 model with the multi-classifier, and a joint\nlayer connecting all components. We tested our models on WikiQA, TREC-QA, and a\nreal-world dataset. The results show that our models obtain the new state of\nthe art in AS2.",
          "link": "http://arxiv.org/abs/2107.04217",
          "publishedOn": "2021-07-12T01:55:14.451Z",
          "wordCount": 554,
          "title": "Joint Models for Answer Verification in Question Answering Systems. (arXiv:2107.04217v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kivlichan_I/0/1/0/all/0/1\">Ian D. Kivlichan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasserman_L/0/1/0/all/0/1\">Lucy Vasserman</a>",
          "description": "Content moderation is often performed by a collaboration between humans and\nmachine learning models. However, it is not well understood how to design the\ncollaborative process so as to maximize the combined moderator-model system\nperformance. This work presents a rigorous study of this problem, focusing on\nan approach that incorporates model uncertainty into the collaborative process.\nFirst, we introduce principled metrics to describe the performance of the\ncollaborative system under capacity constraints on the human moderator,\nquantifying how efficiently the combined system utilizes human decisions. Using\nthese metrics, we conduct a large benchmark study evaluating the performance of\nstate-of-the-art uncertainty models under different collaborative review\nstrategies. We find that an uncertainty-based strategy consistently outperforms\nthe widely used strategy based on toxicity scores, and moreover that the choice\nof review strategy drastically changes the overall system performance. Our\nresults demonstrate the importance of rigorous metrics for understanding and\ndeveloping effective moderator-model systems for content moderation, as well as\nthe utility of uncertainty estimation in this domain.",
          "link": "http://arxiv.org/abs/2107.04212",
          "publishedOn": "2021-07-12T01:55:14.406Z",
          "wordCount": 605,
          "title": "Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation. (arXiv:2107.04212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Scott Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunk_C/0/1/0/all/0/1\">Cliff Brunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyu-Young Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Justin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_G/0/1/0/all/0/1\">Gagan Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1\">Sidharth Mudgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varano_C/0/1/0/all/0/1\">Chris Varano</a>",
          "description": "One of the challenges in a task oriented natural language application like\nthe Google Assistant, Siri, or Alexa is to localize the output to many\nlanguages. This paper explores doing this by applying machine translation to\nthe English output. Using machine translation is very scalable, as it can work\nwith any English output and can handle dynamic text, but otherwise the problem\nis a poor fit. The required quality bar is close to perfection, the range of\nsentences is extremely narrow, and the sentences are often very different than\nthe ones in the machine translation training data. This combination of\nrequirements is novel in the field of domain adaptation for machine\ntranslation. We are able to reach the required quality bar by building on\nexisting ideas and adding new ones: finetuning on in-domain translations,\nadding sentences from the Web, adding semantic annotations, and using automatic\nerror detection. The paper shares our approach and results, together with a\ndistillation model to serve the translation models at scale.",
          "link": "http://arxiv.org/abs/2107.04512",
          "publishedOn": "2021-07-12T01:55:14.390Z",
          "wordCount": 619,
          "title": "Using Machine Translation to Localize Task Oriented NLG Output. (arXiv:2107.04512v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>",
          "description": "Many joint entity relation extraction models setup two separated label spaces\nfor the two sub-tasks (i.e., entity detection and relation classification). We\nargue that this setting may hinder the information interaction between entities\nand relations. In this work, we propose to eliminate the different treatment on\nthe two sub-tasks' label spaces. The input of our model is a table containing\nall word pairs from a sentence. Entities and relations are represented by\nsquares and rectangles in the table. We apply a unified classifier to predict\neach cell's label, which unifies the learning of two sub-tasks. For testing, an\neffective (yet fast) approximate decoder is proposed for finding squares and\nrectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC)\nshow that, using only half the number of parameters, our model achieves\ncompetitive accuracy with the best extractor, and is faster.",
          "link": "http://arxiv.org/abs/2107.04292",
          "publishedOn": "2021-07-12T01:55:14.376Z",
          "wordCount": 584,
          "title": "UniRE: A Unified Label Space for Entity Relation Extraction. (arXiv:2107.04292v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1\">Tien-Hong Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>",
          "description": "An important research direction in automatic speech recognition (ASR) has\ncentered around the development of effective methods to rerank the output\nhypotheses of an ASR system with more sophisticated language models (LMs) for\nfurther gains. A current mainstream school of thoughts for ASR N-best\nhypothesis reranking is to employ a recurrent neural network (RNN)-based LM or\nits variants, with performance superiority over the conventional n-gram LMs\nacross a range of ASR tasks. In real scenarios such as a long conversation, a\nsequence of consecutive sentences may jointly contain ample cues of\nconversation-level information such as topical coherence, lexical entrainment\nand adjacency pairs, which however remains to be underexplored. In view of\nthis, we first formulate ASR N-best reranking as a prediction problem, putting\nforward an effective cross-sentence neural LM approach that reranks the ASR\nN-best hypotheses of an upcoming sentence by taking into consideration the word\nusage in its precedent sentences. Furthermore, we also explore to extract\ntask-specific global topical information of the cross-sentence history in an\nunsupervised manner for better ASR performance. Extensive experiments conducted\non the AMI conversational benchmark corpus indicate the effectiveness and\nfeasibility of our methods in comparison to several state-of-the-art reranking\nmethods.",
          "link": "http://arxiv.org/abs/2106.06922",
          "publishedOn": "2021-07-09T01:58:26.931Z",
          "wordCount": 681,
          "title": "Cross-sentence Neural Language Models for Conversational Speech Recognition. (arXiv:2106.06922v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Le Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaochun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>",
          "description": "We investigate large-scale latent variable models (LVMs) for neural story\ngeneration -- an under-explored application for open-domain long text -- with\nobjectives in two threads: generation effectiveness and controllability. LVMs,\nespecially the variational autoencoder (VAE), have achieved both effective and\ncontrollable generation through exploiting flexible distributional latent\nrepresentations. Recently, Transformers and its variants have achieved\nremarkable effectiveness without explicit latent representation learning, thus\nlack satisfying controllability in generation. In this paper, we advocate to\nrevive latent variable modeling, essentially the power of representation\nlearning, in the era of Transformers to enhance controllability without hurting\nstate-of-the-art generation effectiveness. Specifically, we integrate latent\nrepresentation vectors with a Transformer-based pre-trained architecture to\nbuild conditional variational autoencoder (CVAE). Model components such as\nencoder, decoder and the variational posterior are all built on top of\npre-trained language models -- GPT2 specifically in this paper. Experiments\ndemonstrate state-of-the-art conditional generation ability of our model, as\nwell as its excellent representation learning capability and controllability.",
          "link": "http://arxiv.org/abs/2101.00828",
          "publishedOn": "2021-07-09T01:58:26.805Z",
          "wordCount": 632,
          "title": "Transformer-based Conditional Variational Autoencoder for Controllable Story Generation. (arXiv:2101.00828v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DeLucia_A/0/1/0/all/0/1\">Alexandra DeLucia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lisa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>",
          "description": "Narrative generation is an open-ended NLP task in which a model generates a\nstory given a prompt. The task is similar to neural response generation for\nchatbots; however, innovations in response generation are often not applied to\nnarrative generation, despite the similarity between these tasks. We aim to\nbridge this gap by applying and evaluating advances in decoding methods for\nneural response generation to neural narrative generation. In particular, we\nemploy GPT-2 and perform ablations across nucleus sampling thresholds and\ndiverse decoding hyperparameters -- specifically, maximum mutual information --\nanalyzing results over multiple criteria with automatic and human evaluation.\nWe find that (1) nucleus sampling is generally best with thresholds between 0.7\nand 0.9; (2) a maximum mutual information objective can improve the quality of\ngenerated stories; and (3) established automatic metrics do not correlate well\nwith human judgments of narrative quality on any qualitative metric.",
          "link": "http://arxiv.org/abs/2010.07375",
          "publishedOn": "2021-07-09T01:58:26.695Z",
          "wordCount": 622,
          "title": "Decoding Methods for Neural Narrative Generation. (arXiv:2010.07375v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; Martins</a>",
          "description": "Visual attention mechanisms are widely used in multimodal tasks, as visual\nquestion answering (VQA). One drawback of softmax-based attention mechanisms is\nthat they assign some probability mass to all image regions, regardless of\ntheir adjacency structure and of their relevance to the text. In this paper, to\nbetter link the image structure with the text, we replace the traditional\nsoftmax attention mechanism with two alternative sparsity-promoting\ntransformations: sparsemax, which is able to select only the relevant regions\n(assigning zero weight to the rest), and a newly proposed Total-Variation\nSparse Attention (TVmax), which further encourages the joint selection of\nadjacent spatial locations. Experiments in VQA show gains in accuracy as well\nas higher similarity to human attention, which suggests better\ninterpretability.",
          "link": "http://arxiv.org/abs/2002.05556",
          "publishedOn": "2021-07-09T01:58:26.488Z",
          "wordCount": 587,
          "title": "Sparse and Structured Visual Attention. (arXiv:2002.05556v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.08964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1\">Matthew Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mues_C/0/1/0/all/0/1\">Christophe Mues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1\">Cristi&#xe1;n Bravo</a>",
          "description": "Compared to consumer lending, Micro, Small and Medium Enterprise (mSME)\ncredit risk modelling is particularly challenging, as, often, the same sources\nof information are not available. Therefore, it is standard policy for a loan\nofficer to provide a textual loan assessment to mitigate limited data\navailability. In turn, this statement is analysed by a credit expert alongside\nany available standard credit data. In our paper, we exploit recent advances\nfrom the field of Deep Learning and Natural Language Processing (NLP),\nincluding the BERT (Bidirectional Encoder Representations from Transformers)\nmodel, to extract information from 60 000 textual assessments provided by a\nlender. We consider the performance in terms of the AUC (Area Under the\nreceiver operating characteristic Curve) and Brier Score metrics and find that\nthe text alone is surprisingly effective for predicting default. However, when\ncombined with traditional data, it yields no additional predictive capability,\nwith performance dependent on the text's length. Our proposed deep learning\nmodel does, however, appear to be robust to the quality of the text and\ntherefore suitable for partly automating the mSME lending process. We also\ndemonstrate how the content of loan assessments influences performance, leading\nus to a series of recommendations on a new strategy for collecting future mSME\nloan assessments.",
          "link": "http://arxiv.org/abs/2003.08964",
          "publishedOn": "2021-07-09T01:58:26.416Z",
          "wordCount": 712,
          "title": "The value of text for small business default prediction: A deep learning approach. (arXiv:2003.08964v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AzariJafari_M/0/1/0/all/0/1\">Mohammad AzariJafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourjafari_Z/0/1/0/all/0/1\">Zohreh Pourjafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faridan_Jahromi_S/0/1/0/all/0/1\">Soroush Faridan-Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouhkan_Z/0/1/0/all/0/1\">Zeinab Kouhkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirak_A/0/1/0/all/0/1\">Azadeh Amirak</a>",
          "description": "Natural language inference (NLI) is known as one of the central tasks in\nnatural language processing (NLP) which encapsulates many fundamental aspects\nof language understanding. With the considerable achievements of data-hungry\ndeep learning methods in NLP tasks, a great amount of e ort has been devoted to\ndevelop more diverse datasets for di erent languages. In this paper, we present\na new dataset for the NLI task in the Persian language, also known as Farsi,\nwhich is one of the dominant languages in the Middle East. This dataset, named\nFarsTail, includes 10,367 samples which are provided in both the Persian\nlanguage as well as the indexed format to be useful for non-Persian\nresearchers. The samples are generated from 3,539 multiple-choice questions\nwith the least amount of annotator interventions in a way similar to the\nSciTail dataset. A carefully designed multi-step process is adopted to ensure\nthe quality of the dataset. We also present the results of traditional and\nstate-of-the-art methods on FarsTail including di erent embedding methods such\nas word2vec, fastText, ELMo, BERT, and LASER, as well as di erent modeling\napproaches such as DecompAtt, ESIM, HBMP, and ULMFiT to provide a solid\nbaseline for the future research. The best obtained test accuracy is 83.38%\nwhich shows that there is a big room for improving the current methods to be\nuseful for real-world NLP applications in di erent languages. We also\ninvestigate the extent to which the models exploit super cial clues, also known\nas dataset biases, in FarsTail, and partition the test set into easy and hard\nsubsets according to the success of biased models. The dataset is available at\nhttps://github.com/dml-qom/ FarsTail.",
          "link": "http://arxiv.org/abs/2009.08820",
          "publishedOn": "2021-07-09T01:58:26.389Z",
          "wordCount": 733,
          "title": "FarsTail: A Persian Natural Language Inference Dataset. (arXiv:2009.08820v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>",
          "description": "Automatic evaluation metrics are a crucial component of dialog systems\nresearch. Standard language evaluation metrics are known to be ineffective for\nevaluating dialog. As such, recent research has proposed a number of novel,\ndialog-specific metrics that correlate better with human judgements. Due to the\nfast pace of research, many of these metrics have been assessed on different\ndatasets and there has as yet been no time for a systematic comparison between\nthem. To this end, this paper provides a comprehensive assessment of recently\nproposed dialog evaluation metrics on a number of datasets. In this paper, 23\ndifferent automatic evaluation metrics are evaluated on 10 different datasets.\nFurthermore, the metrics are assessed in different settings, to better qualify\ntheir respective strengths and weaknesses. Metrics are assessed (1) on both the\nturn level and the dialog level, (2) for different dialog lengths, (3) for\ndifferent dialog qualities (e.g., coherence, engaging), (4) for different types\nof response generation models (i.e., generative, retrieval, simple models and\nstate-of-the-art models), (5) taking into account the similarity of different\nmetrics and (6) exploring combinations of different metrics. This comprehensive\nassessment offers several takeaways pertaining to dialog evaluation metrics in\ngeneral. It also suggests how to best assess evaluation metrics and indicates\npromising directions for future work.",
          "link": "http://arxiv.org/abs/2106.03706",
          "publishedOn": "2021-07-09T01:58:26.360Z",
          "wordCount": 686,
          "title": "A Comprehensive Assessment of Dialog Evaluation Metrics. (arXiv:2106.03706v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.07648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1\">Corey Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1\">Pierre Sermanet</a>",
          "description": "Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image -- something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. \"open the drawer...now pick up the block...now\npress the green button...\"). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io",
          "link": "http://arxiv.org/abs/2005.07648",
          "publishedOn": "2021-07-09T01:58:25.724Z",
          "wordCount": 735,
          "title": "Language Conditioned Imitation Learning over Unstructured Data. (arXiv:2005.07648v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Gautam Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Milind Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1\">Pranav Dheram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Bryan Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_B/0/1/0/all/0/1\">Bach Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>",
          "description": "We propose an end-to-end trained spoken language understanding (SLU) system\nthat extracts transcripts, intents and slots from an input speech utterance. It\nconsists of a streaming recurrent neural network transducer (RNNT) based\nautomatic speech recognition (ASR) model connected to a neural natural language\nunderstanding (NLU) model through a neural interface. This interface allows for\nend-to-end training using multi-task RNNT and NLU losses. Additionally, we\nintroduce semantic sequence loss training for the joint RNNT-NLU system that\nallows direct optimization of non-differentiable SLU metrics. This end-to-end\nSLU model paradigm can leverage state-of-the-art advancements and pretrained\nmodels in both ASR and NLU research communities, outperforming recently\nproposed direct speech-to-semantics models, and conventional pipelined ASR and\nNLU systems. We show that this method improves both ASR and NLU metrics on both\npublic SLU datasets and large proprietary datasets.",
          "link": "http://arxiv.org/abs/2106.15919",
          "publishedOn": "2021-07-09T01:58:25.658Z",
          "wordCount": 599,
          "title": "End-to-End Spoken Language Understanding using RNN-Transducer ASR. (arXiv:2106.15919v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roemmele_M/0/1/0/all/0/1\">Melissa Roemmele</a>",
          "description": "Getting machines to generate text perceived as creative is a long-pursued\ngoal. A growing body of research directs this goal towards augmenting the\ncreative writing abilities of human authors. In this paper, we pursue this\nobjective by analyzing how observing examples of automatically generated text\ninfluences writing. In particular, we examine a task referred to as sentence\ninfilling, which involves transforming a list of words into a complete\nsentence. We emphasize \"storiability\" as a desirable feature of the resulting\nsentences, where \"storiable\" sentences are those that suggest a story a reader\nwould be curious to hear about. Both humans and an automated system (based on a\nneural language model) performed this sentence infilling task. In one setting,\npeople wrote sentences on their own; in a different setting, people observed\nthe sentences produced by the model while writing their own sentences. Readers\nthen assigned storiability preferences to the resulting sentences in a\nsubsequent evaluation. We find that human-authored sentences were judged as\nmore storiable when authors observed the generated examples, and that\nstoriability increased as authors derived more semantic content from the\nexamples. This result gives evidence of an \"inspiration through observation\"\nparadigm for human-computer collaborative writing, through which human writing\ncan be enhanced by text generation models without directly copying their\noutput.",
          "link": "http://arxiv.org/abs/2107.04007",
          "publishedOn": "2021-07-09T01:58:25.614Z",
          "wordCount": 660,
          "title": "Inspiration through Observation: Demonstrating the Influence of Automatically Generated Text on Creative Writing. (arXiv:2107.04007v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.09828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schumacher_E/0/1/0/all/0/1\">Elliot Schumacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayfield_J/0/1/0/all/0/1\">James Mayfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>",
          "description": "Cross-language entity linking grounds mentions in multiple languages to a\nsingle-language knowledge base. We propose a neural ranking architecture for\nthis task that uses multilingual BERT representations of the mention and the\ncontext in a neural network. We find that the multilingual ability of BERT\nleads to robust performance in monolingual and multilingual settings.\nFurthermore, we explore zero-shot language transfer and find surprisingly\nrobust performance. We investigate the zero-shot degradation and find that it\ncan be partially mitigated by a proposed auxiliary training objective, but that\nthe remaining error can best be attributed to domain shift rather than language\ntransfer.",
          "link": "http://arxiv.org/abs/2010.09828",
          "publishedOn": "2021-07-09T01:58:25.599Z",
          "wordCount": 564,
          "title": "Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking. (arXiv:2010.09828v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haqbeen_J/0/1/0/all/0/1\">J. Haqbeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1\">T. Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahab_S/0/1/0/all/0/1\">S. Sahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfi_R/0/1/0/all/0/1\">R. Hadfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">T. Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okuhara_S/0/1/0/all/0/1\">S. Okuhara</a>",
          "description": "In this paper, we report about a large-scale online discussion with 1099\ncitizens on the Afghanistan Sustainable Development Goals.",
          "link": "http://arxiv.org/abs/2107.04011",
          "publishedOn": "2021-07-09T01:58:25.216Z",
          "wordCount": 490,
          "title": "Meeting the SDGs : Enabling the Goals by Cooperation with Crowd using a Conversational AI Platform. (arXiv:2107.04011v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yu-Ying Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Mihi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuefeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>",
          "description": "This paper presents three case studies of modeling aspects of lexical\nprocessing with Linear Discriminative Learning (LDL), the computational engine\nof the Discriminative Lexicon model (Baayen et al., 2019). With numeric\nrepresentations of word forms and meanings, LDL learns to map one vector space\nonto the other, without being informed about any morphological structure or\ninflectional classes. The modeling results demonstrated that LDL not only\nperforms well for understanding and producing morphologically complex words,\nbut also generates quantitative measures that are predictive for human\nbehavioral data. LDL models are straightforward to implement with the JudiLing\npackage (Luo et al., 2021). Worked examples are provided for three modeling\nchallenges: producing and understanding Korean verb inflection, predicting\nprimed Dutch lexical decision latencies, and predicting the acoustic duration\nof Mandarin words.",
          "link": "http://arxiv.org/abs/2107.03950",
          "publishedOn": "2021-07-09T01:58:25.209Z",
          "wordCount": 558,
          "title": "Vector Space Morphology with Linear Discriminative Learning. (arXiv:2107.03950v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1\">Maximilian Mozes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegt_I/0/1/0/all/0/1\">Isabelle van der Vegt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>",
          "description": "The introduction of COVID-19 lockdown measures and an outlook on return to\nnormality are demanding societal changes. Among the most pressing questions is\nhow individuals adjust to the pandemic. This paper examines the emotional\nresponses to the pandemic in a repeated-measures design. Data (n=1698) were\ncollected in April 2020 (during strict lockdown measures) and in April 2021\n(when vaccination programmes gained traction). We asked participants to report\ntheir emotions and express these in text data. Statistical tests revealed an\naverage trend towards better adjustment to the pandemic. However, clustering\nanalyses suggested a more complex heterogeneous pattern with a well-coping and\na resigning subgroup of participants. Linguistic computational analyses\nuncovered that topics and n-gram frequencies shifted towards attention to the\nvaccination programme and away from general worrying. Implications for public\nmental health efforts in identifying people at heightened risk are discussed.\nThe dataset is made publicly available.",
          "link": "http://arxiv.org/abs/2107.03466",
          "publishedOn": "2021-07-09T01:58:25.183Z",
          "wordCount": 647,
          "title": "Worry, coping and resignation -- A repeated-measures study on emotional responses after a year in the pandemic. (arXiv:2107.03466v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saglam_R/0/1/0/all/0/1\">Rahime Belen Saglam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1\">Jason R.C. Nurse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodges_D/0/1/0/all/0/1\">Duncan Hodges</a>",
          "description": "Through advances in their conversational abilities, chatbots have started to\nrequest and process an increasing variety of sensitive personal information.\nThe accurate disclosure of sensitive information is essential where it is used\nto provide advice and support to users in the healthcare and finance sectors.\nIn this study, we explore users' concerns regarding factors associated with the\nuse of sensitive data by chatbot providers. We surveyed a representative sample\nof 491 British citizens. Our results show that the user concerns focus on\ndeleting personal information and concerns about their data's inappropriate\nuse. We also identified that individuals were concerned about losing control\nover their data after a conversation with conversational agents. We found no\neffect from a user's gender or education but did find an effect from the user's\nage, with those over 45 being more concerned than those under 45. We also\nconsidered the factors that engender trust in a chatbot. Our respondents'\nprimary focus was on the chatbot's technical elements, with factors such as the\nresponse quality being identified as the most critical factor. We again found\nno effect from the user's gender or education level; however, when we\nconsidered some social factors (e.g. avatars or perceived 'friendliness'), we\nfound those under 45 years old rated these as more important than those over\n45. The paper concludes with a discussion of these results within the context\nof designing inclusive, digital systems that support a wide range of users.",
          "link": "http://arxiv.org/abs/2107.03959",
          "publishedOn": "2021-07-09T01:58:25.157Z",
          "wordCount": 707,
          "title": "Privacy Concerns in Chatbot Interactions: When to Trust and When to Worry. (arXiv:2107.03959v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aadesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh D.Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarway_R/0/1/0/all/0/1\">Rahul Tarway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_S/0/1/0/all/0/1\">Swetha Prabhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ashish Shrivastava</a>",
          "description": "Domain-specific dialogue systems generally determine user intents by relying\non sentence-level classifiers which mainly focus on single action sentences.\nSuch classifiers are not designed to effectively handle complex queries\ncomposed of conditional and sequential clauses that represent multiple actions.\nWe attempt to decompose such queries into smaller single-action sub-queries\nthat are reasonable for intent classifiers to understand in a dialogue\npipeline. We release CANDLE (Conditional & AND type Expressions), a dataset\nconsisting of 3124 utterances manually tagged with conditional and sequential\nlabels and demonstrates this decomposition by training two baseline taggers.",
          "link": "http://arxiv.org/abs/2107.03884",
          "publishedOn": "2021-07-09T01:58:25.149Z",
          "wordCount": 532,
          "title": "CANDLE: Decomposing Conditional and Conjunctive Queries for Task-Oriented Dialogue Systems. (arXiv:2107.03884v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>",
          "description": "A comparison of formulaic sequences in human and neural machine translation\nof quality newspaper articles shows that neural machine translations contain\nless lower-frequency, but strongly-associated formulaic sequences, and more\nhigh-frequency formulaic sequences. These differences were statistically\nsignificant and the effect sizes were almost always medium or large. These\nobservations can be related to the differences between second language learners\nof various levels and between translated and untranslated texts. The comparison\nbetween the neural machine translation systems indicates that some systems\nproduce more formulaic sequences of both types than other systems.",
          "link": "http://arxiv.org/abs/2107.03625",
          "publishedOn": "2021-07-09T01:58:25.139Z",
          "wordCount": 535,
          "title": "Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation. (arXiv:2107.03625v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvir Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-09T01:58:25.107Z",
          "wordCount": 691,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Camps_J/0/1/0/all/0/1\">Jean-Baptiste Camps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_Gorene_C/0/1/0/all/0/1\">Chahan Vidal-Gor&#xe8;ne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernet_M/0/1/0/all/0/1\">Marguerite Vernet</a>",
          "description": "Although abbreviations are fairly common in handwritten sources, particularly\nin medieval and modern Western manuscripts, previous research dealing with\ncomputational approaches to their expansion is scarce. Yet abbreviations\npresent particular challenges to computational approaches such as handwritten\ntext recognition and natural language processing tasks. Often, pre-processing\nultimately aims to lead from a digitised image of the source to a normalised\ntext, which includes expansion of the abbreviations. We explore different\nsetups to obtain such a normalised text, either directly, by training HTR\nengines on normalised (i.e., expanded, disabbreviated) text, or by decomposing\nthe process into discrete steps, each making use of specialist models for\nrecognition, word segmentation and normalisation. The case studies considered\nhere are drawn from the medieval Latin tradition.",
          "link": "http://arxiv.org/abs/2107.03450",
          "publishedOn": "2021-07-09T01:58:25.098Z",
          "wordCount": 561,
          "title": "Handling Heavily Abbreviated Manuscripts: HTR engines vs text normalisation approaches. (arXiv:2107.03450v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Ke Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>",
          "description": "Speech evaluation is an essential component in computer-assisted language\nlearning (CALL). While speech evaluation on English has been popular, automatic\nspeech scoring on low resource languages remains challenging. Work in this area\nhas focused on monolingual specific designs and handcrafted features stemming\nfrom resource-rich languages like English. Such approaches are often difficult\nto generalize to other languages, especially if we also want to consider\nsuprasegmental qualities such as rhythm. In this work, we examine three\ndifferent languages that possess distinct rhythm patterns: English\n(stress-timed), Malay (syllable-timed), and Tamil (mora-timed). We exploit\nrobust feature representations inspired by music processing and vector\nrepresentation learning. Empirical validations show consistent gains for all\nthree languages when predicting pronunciation, rhythm and intonation\nperformance.",
          "link": "http://arxiv.org/abs/2107.03675",
          "publishedOn": "2021-07-09T01:58:25.090Z",
          "wordCount": 568,
          "title": "Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil. (arXiv:2107.03675v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klimaszewski_M/0/1/0/all/0/1\">Mateusz Klimaszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Alina Wr&#xf3;blewska</a>",
          "description": "We introduce the COMBO-based approach for EUD parsing and its implementation,\nwhich took part in the IWPT 2021 EUD shared task. The goal of this task is to\nparse raw texts in 17 languages into Enhanced Universal Dependencies (EUD). The\nproposed approach uses COMBO to predict UD trees and EUD graphs. These\nstructures are then merged into the final EUD graphs. Some EUD edge labels are\nextended with case information using a single language-independent expansion\nrule. In the official evaluation, the solution ranked fourth, achieving an\naverage ELAS of 83.79%. The source code is available at\nhttps://gitlab.clarin-pl.eu/syntactic-tools/combo.",
          "link": "http://arxiv.org/abs/2107.03809",
          "publishedOn": "2021-07-09T01:58:25.082Z",
          "wordCount": 530,
          "title": "COMBO: a new module for EUD parsing. (arXiv:2107.03809v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">A. Stevie Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spruit_S/0/1/0/all/0/1\">Shannon Spruit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>",
          "description": "Over the last several years, end-to-end neural conversational agents have\nvastly improved in their ability to carry a chit-chat conversation with humans.\nHowever, these models are often trained on large datasets from the internet,\nand as a result, may learn undesirable behaviors from this data, such as toxic\nor otherwise harmful language. Researchers must thus wrestle with the issue of\nhow and when to release these models. In this paper, we survey the problem\nlandscape for safety for end-to-end conversational AI and discuss recent and\nrelated work. We highlight tensions between values, potential positive impact\nand potential harms, and provide a framework for making decisions about whether\nand how to release these models, following the tenets of value-sensitive\ndesign. We additionally provide a suite of tools to enable researchers to make\nbetter-informed decisions about training and releasing end-to-end\nconversational AI models.",
          "link": "http://arxiv.org/abs/2107.03451",
          "publishedOn": "2021-07-09T01:58:25.069Z",
          "wordCount": 588,
          "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling. (arXiv:2107.03451v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abeysinghe_B/0/1/0/all/0/1\">Bhashithe Abeysinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Dhara Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freas_C/0/1/0/all/0/1\">Chris Freas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_R/0/1/0/all/0/1\">Robert Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderraman_R/0/1/0/all/0/1\">Rajshekhar Sunderraman</a>",
          "description": "Most online message threads inherently will be cluttered and any new user or\nan existing user visiting after a hiatus will have a difficult time\nunderstanding whats being discussed in the thread. Similarly cluttered\nresponses in a message thread makes analyzing the messages a difficult problem.\nThe need for disentangling the clutter is much higher when the platform where\nthe discussion is taking place does not provide functions to retrieve reply\nrelations of the messages. This introduces an interesting problem to which\n\\cite{wang2011learning} phrases as a structural learning problem. We create\nvector embeddings for posts in a thread so that it captures both linguistic and\npositional features in relation to a context of where a given message is in.\nUsing these embeddings for posts we compute a similarity based connectivity\nmatrix which then converted into a graph. After employing a pruning mechanisms\nthe resultant graph can be used to discover the reply relation for the posts in\nthe thread. The process of discovering or disentangling chat is kept as an\nunsupervised mechanism. We present our experimental results on a data set\nobtained from Telegram with limited meta data.",
          "link": "http://arxiv.org/abs/2107.03529",
          "publishedOn": "2021-07-09T01:58:25.046Z",
          "wordCount": 625,
          "title": "POSLAN: Disentangling Chat with Positional and Language encoded Post Embeddings. (arXiv:2107.03529v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Text generation is a highly active area of research in the computational\nlinguistic community. The evaluation of the generated text is a challenging\ntask and multiple theories and metrics have been proposed over the years.\nUnfortunately, text generation and evaluation are relatively understudied due\nto the scarcity of high-quality resources in code-mixed languages where the\nwords and phrases from multiple languages are mixed in a single utterance of\ntext and speech. To address this challenge, we present a corpus (HinGE) for a\nwidely popular code-mixed language Hinglish (code-mixing of Hindi and English\nlanguages). HinGE has Hinglish sentences generated by humans as well as two\nrule-based algorithms corresponding to the parallel Hindi-English sentences. In\naddition, we demonstrate the inefficacy of widely-used evaluation metrics on\nthe code-mixed data. The HinGE dataset will facilitate the progress of natural\nlanguage generation research in code-mixed languages.",
          "link": "http://arxiv.org/abs/2107.03760",
          "publishedOn": "2021-07-09T01:58:24.979Z",
          "wordCount": 574,
          "title": "HinGE: A Dataset for Generation and Evaluation of Code-Mixed Hinglish Text. (arXiv:2107.03760v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_T/0/1/0/all/0/1\">Tobias Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>",
          "description": "This work presents Keep it Simple (KiS), a new approach to unsupervised text\nsimplification which learns to balance a reward across three properties:\nfluency, salience and simplicity. We train the model with a novel algorithm to\noptimize the reward (k-SCST), in which the model proposes several candidate\nsimplifications, computes each candidate's reward, and encourages candidates\nthat outperform the mean reward. Finally, we propose a realistic text\ncomprehension task as an evaluation method for text simplification. When tested\non the English news domain, the KiS model outperforms strong supervised\nbaselines by more than 4 SARI points, and can help people complete a\ncomprehension task an average of 18% faster while retaining accuracy, when\ncompared to the original text. Code available:\nhttps://github.com/tingofurro/keep_it_simple",
          "link": "http://arxiv.org/abs/2107.03444",
          "publishedOn": "2021-07-09T01:58:24.879Z",
          "wordCount": 571,
          "title": "Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text. (arXiv:2107.03444v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Luke Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandarkar_L/0/1/0/all/0/1\">Lucas Bandarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>",
          "description": "The Shuffle Test is the most common task to evaluate whether NLP models can\nmeasure coherence in text. Most recent work uses direct supervision on the\ntask; we show that by simply finetuning a RoBERTa model, we can achieve a near\nperfect accuracy of 97.8%, a state-of-the-art. We argue that this outstanding\nperformance is unlikely to lead to a good model of text coherence, and suggest\nthat the Shuffle Test should be approached in a Zero-Shot setting: models\nshould be evaluated without being trained on the task itself. We evaluate\ncommon models in this setting, such as Generative and Bi-directional\nTransformers, and find that larger architectures achieve high-performance\nout-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of\nthe original by increasing the size of blocks shuffled. Even though human\nreader performance remains high (around 95% accuracy), model performance drops\nfrom 94% to 78% as block size increases, creating a conceptually simple\nchallenge to benchmark NLP models. Code available:\nhttps://github.com/tingofurro/shuffle_test/",
          "link": "http://arxiv.org/abs/2107.03448",
          "publishedOn": "2021-07-09T01:58:24.700Z",
          "wordCount": 620,
          "title": "Can Transformer Models Measure Coherence In Text? Re-Thinking the Shuffle Test. (arXiv:2107.03448v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Junha Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.",
          "link": "http://arxiv.org/abs/2107.03438",
          "publishedOn": "2021-07-09T01:58:24.521Z",
          "wordCount": 614,
          "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v1 [cs.RO])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.02390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Visually-aware recommendation on E-commerce platforms aims to leverage visual\ninformation of items to predict a user's preference. It is commonly observed\nthat user's attention to visual features does not always reflect the real\npreference. Although a user may click and view an item in light of a visual\nsatisfaction of their expectations, a real purchase does not always occur due\nto the unsatisfaction of other essential features (e.g., brand, material,\nprice). We refer to the reason for such a visually related interaction\ndeviating from the real preference as a visual bias. Existing visually-aware\nmodels make use of the visual features as a separate collaborative signal\nsimilarly to other features to directly predict the user's preference without\nconsidering a potential bias, which gives rise to a visually biased\nrecommendation. In this paper, we derive a causal graph to identify and analyze\nthe visual bias of these existing methods. In this causal graph, the visual\nfeature of an item acts as a mediator, which could introduce a spurious\nrelationship between the user and the item. To eliminate this spurious\nrelationship that misleads the prediction of the user's real preference, an\nintervention and a counterfactual inference are developed over the mediator.\nParticularly, the Total Indirect Effect is applied for a debiased prediction\nduring the testing phase of the model. This causal inference framework is model\nagnostic such that it can be integrated into the existing methods. Furthermore,\nwe propose a debiased visually-aware recommender system, denoted as CausalRec\nto effectively retain the supportive significance of the visual information and\nremove the visual bias. Extensive experiments are conducted on eight benchmark\ndatasets, which shows the state-of-the-art performance of CausalRec and the\nefficacy of debiasing.",
          "link": "http://arxiv.org/abs/2107.02390",
          "publishedOn": "2021-07-14T01:41:48.864Z",
          "wordCount": 741,
          "title": "CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation. (arXiv:2107.02390v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zamanifar_K/0/1/0/all/0/1\">Kamran Zamanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_G/0/1/0/all/0/1\">Golsa Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematbakhsh_N/0/1/0/all/0/1\">Naser Nematbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardookhi_F/0/1/0/all/0/1\">Farhad Mardookhi</a>",
          "description": "In this work we propose a new approach for semantic web matching to improve\nthe performance of Web Service replacement. Because in automatic systems we\nshould ensure the self-healing, self-configuration, self-optimization and\nself-management, all services should be always available and if one of them\ncrashes, it should be replaced with the most similar one. Candidate services\nare advertised in Universal Description, Discovery and Integration (UDDI) all\nin Web Ontology Language (OWL). By the help of bipartite graph, we did the\nmatching between the crashed service and a Candidate one. Then we chose the\nbest service, which had the maximum rate of matching. In fact we compare two\nservices` functionalities and capabilities to see how much they match. We found\nthat the best way for matching two web services, is comparing the\nfunctionalities of them.",
          "link": "http://arxiv.org/abs/2107.06083",
          "publishedOn": "2021-07-14T01:41:48.852Z",
          "wordCount": 573,
          "title": "A New Approach for Semantic Web Matching. (arXiv:2107.06083v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Formal_T/0/1/0/all/0/1\">Thibault Formal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>",
          "description": "In neural Information Retrieval, ongoing research is directed towards\nimproving the first retriever in ranking pipelines. Learning dense embeddings\nto conduct retrieval using efficient approximate nearest neighbors methods has\nproven to work well. Meanwhile, there has been a growing interest in learning\nsparse representations for documents and queries, that could inherit from the\ndesirable properties of bag-of-words models such as the exact matching of terms\nand the efficiency of inverted indexes. In this work, we present a new\nfirst-stage ranker based on explicit sparsity regularization and a\nlog-saturation effect on term weights, leading to highly sparse representations\nand competitive results with respect to state-of-the-art dense and sparse\nmethods. Our approach is simple, trained end-to-end in a single stage. We also\nexplore the trade-off between effectiveness and efficiency, by controlling the\ncontribution of the sparsity regularization.",
          "link": "http://arxiv.org/abs/2107.05720",
          "publishedOn": "2021-07-14T01:41:48.840Z",
          "wordCount": 572,
          "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. (arXiv:2107.05720v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fareri_S/0/1/0/all/0/1\">Silvia Fareri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melluso_N/0/1/0/all/0/1\">Nicola Melluso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiarello_F/0/1/0/all/0/1\">Filippo Chiarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fantoni_G/0/1/0/all/0/1\">Gualtiero Fantoni</a>",
          "description": "In today's digital world, there is an increasing focus on soft skills. On the\none hand, they facilitate innovation at companies, but on the other, they are\nunlikely to be automated soon. Researchers struggle with accurately approaching\nquantitatively the study of soft skills due to the lack of data-driven methods\nto retrieve them. This limits the possibility for psychologists and HR managers\nto understand the relation between humans and digitalisation. This paper\npresents SkillNER, a novel data-driven method for automatically extracting soft\nskills from text. It is a named entity recognition (NER) system trained with a\nsupport vector machine (SVM) on a corpus of more than 5000 scientific papers.\nWe developed this system by measuring the performance of our approach against\ndifferent training models and validating the results together with a team of\npsychologists. Finally, SkillNER was tested in a real-world case study using\nthe job descriptions of ESCO (European Skill/Competence Qualification and\nOccupation) as textual source. The system enabled the detection of communities\nof job profiles based on their shared soft skills and communities of soft\nskills based on their shared job profiles. This case study demonstrates that\nthe tool can automatically retrieve soft skills from a large corpus in an\nefficient way, proving useful for firms, institutions, and workers. The tool is\nopen and available online to foster quantitative methods for the study of soft\nskills.",
          "link": "http://arxiv.org/abs/2101.11431",
          "publishedOn": "2021-07-14T01:41:48.823Z",
          "wordCount": 699,
          "title": "SkillNER: Mining and Mapping Soft Skills from any Text. (arXiv:2101.11431v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1\">Keping Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croft_W/0/1/0/all/0/1\">W. Bruce Croft</a>",
          "description": "Users often need to look through multiple search result pages or reformulate\nqueries when they have complex information-seeking needs. Conversational search\nsystems make it possible to improve user satisfaction by asking questions to\nclarify users' search intents. This, however, can take significant effort to\nanswer a series of questions starting with \"what/why/how\". To quickly identify\nuser intent and reduce effort during interactions, we propose an intent\nclarification task based on yes/no questions where the system needs to ask the\ncorrect question about intents within the fewest conversation turns. In this\ntask, it is essential to use negative feedback about the previous questions in\nthe conversation history. To this end, we propose a Maximum-Marginal-Relevance\n(MMR) based BERT model (MMR-BERT) to leverage negative feedback based on the\nMMR principle for the next clarifying question selection. Experiments on the\nQulac dataset show that MMR-BERT outperforms state-of-the-art baselines\nsignificantly on the intent identification task and the selected questions also\nachieve significantly better performance in the associated document retrieval\ntasks.",
          "link": "http://arxiv.org/abs/2107.05760",
          "publishedOn": "2021-07-14T01:41:48.791Z",
          "wordCount": 606,
          "title": "Asking Clarifying Questions Based on Negative Feedback in Conversational Search. (arXiv:2107.05760v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirshafiee_M/0/1/0/all/0/1\">Mitra Sadat Mirshafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allahyari_M/0/1/0/all/0/1\">Mehdi Allahyari</a>",
          "description": "With the surge of pretrained language models, a new pathway has been opened\nto incorporate Persian text contextual information. Meanwhile, as many other\ncountries, including Iran, are fighting against COVID-19, a plethora of\nCOVID-19 related articles has been published in Iranian Healthcare magazines to\nbetter inform the public of the situation. However, finding answers in this\nsheer volume of information is an extremely difficult task. In this paper, we\ncollected a large dataset of these articles, leveraged different BERT\nvariations as well as other keyword models such as BM25 and TF-IDF, and created\na search engine to sift through these documents and rank them, given a user's\nquery. Our final search engine consists of a ranker and a re-ranker, which\nadapts itself to the query. We fine-tune our models using Semantic Textual\nSimilarity and evaluate them with standard task metrics. Our final method\noutperforms the rest by a considerable margin.",
          "link": "http://arxiv.org/abs/2107.05722",
          "publishedOn": "2021-07-14T01:41:48.775Z",
          "wordCount": 631,
          "title": "COPER a query-adaptable Semantics-based Search Engine for Persian COVID-19 Articles. (arXiv:2107.05722v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1\">Rodrigo Castellon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We demonstrate that language models pre-trained on codified\n(discretely-encoded) music audio learn representations that are useful for\ndownstream MIR tasks. Specifically, we explore representations from Jukebox\n(Dhariwal et al. 2020): a music generation system containing a language model\ntrained on codified audio from 1M songs. To determine if Jukebox's\nrepresentations contain useful information for MIR, we use them as input\nfeatures to train shallow models on several MIR tasks. Relative to\nrepresentations from conventional MIR models which are pre-trained on tagging,\nwe find that using representations from Jukebox as input features yields 30%\nstronger performance on average across four MIR tasks: tagging, genre\nclassification, emotion recognition, and key detection. For key detection, we\nobserve that representations from Jukebox are considerably stronger than those\nfrom models pre-trained on tagging, suggesting that pre-training via codified\naudio language modeling may address blind spots in conventional approaches. We\ninterpret the strength of Jukebox's representations as evidence that modeling\naudio instead of tags provides richer representations for MIR.",
          "link": "http://arxiv.org/abs/2107.05677",
          "publishedOn": "2021-07-14T01:41:48.737Z",
          "wordCount": 621,
          "title": "Codified audio language modeling learns useful representations for music information retrieval. (arXiv:2107.05677v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Williams_E/0/1/0/all/0/1\">Evan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_P/0/1/0/all/0/1\">Paul Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Sieu Tran</a>",
          "description": "This paper discusses the approach used by the Accenture Team for CLEF2021\nCheckThat! Lab, Task 1, to identify whether a claim made in social media would\nbe interesting to a wide audience and should be fact-checked. Twitter training\nand test data were provided in English, Arabic, Spanish, Turkish, and\nBulgarian. Claims were to be classified (check-worthy/not check-worthy) and\nranked in priority order for the fact-checker. Our method used deep neural\nnetwork transformer models with contextually sensitive lexical augmentation\napplied on the supplied training datasets to create additional training\nsamples. This augmentation approach improved the performance for all languages.\nOverall, our architecture and data augmentation pipeline produced the best\nsubmitted system for Arabic, and performance scales according to the quantity\nof provided training data for English, Spanish, Turkish, and Bulgarian. This\npaper investigates the deep neural network architectures for each language as\nwell as the provided data to examine why the approach worked so effectively for\nArabic, and discusses additional data augmentation measures that should could\nbe useful to this problem.",
          "link": "http://arxiv.org/abs/2107.05684",
          "publishedOn": "2021-07-14T01:41:48.465Z",
          "wordCount": 665,
          "title": "Accenture at CheckThat! 2021: Interesting claim identification and ranking with contextually sensitive lexical training data augmentation. (arXiv:2107.05684v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Volske_M/0/1/0/all/0/1\">Michael V&#xf6;lske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondarenko_A/0/1/0/all/0/1\">Alexander Bondarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1\">Maik Fr&#xf6;be</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1\">Matthias Hagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Recently, neural networks have been successfully employed to improve upon\nstate-of-the-art performance in ad-hoc retrieval tasks via machine-learned\nranking functions. While neural retrieval models grow in complexity and impact,\nlittle is understood about their correspondence with well-studied IR\nprinciples. Recent work on interpretability in machine learning has provided\ntools and techniques to understand neural models in general, yet there has been\nlittle progress towards explaining ranking models.\n\nWe investigate whether one can explain the behavior of neural ranking models\nin terms of their congruence with well understood principles of document\nranking by using established theories from axiomatic IR. Axiomatic analysis of\ninformation retrieval models has formalized a set of constraints on ranking\ndecisions that reasonable retrieval models should fulfill. We operationalize\nthis axiomatic thinking to reproduce rankings based on combinations of\nelementary constraints. This allows us to investigate to what extent the\nranking decisions of neural rankers can be explained in terms of retrieval\naxioms, and which axioms apply in which situations. Our experimental study\nconsiders a comprehensive set of axioms over several representative neural\nrankers. While the existing axioms can already explain the particularly\nconfident ranking decisions rather well, future work should extend the axiom\nset to also cover the other still \"unexplainable\" neural IR rank decisions.",
          "link": "http://arxiv.org/abs/2106.08019",
          "publishedOn": "2021-07-13T01:59:33.207Z",
          "wordCount": 686,
          "title": "Towards Axiomatic Explanations for Neural Ranking Models. (arXiv:2106.08019v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hromada_D/0/1/0/all/0/1\">Daniel Devatman Hromada</a>",
          "description": "Departing from the postulate that Voynich Manuscript is not a hoax but rather\nencodes authentic contents, our article presents an evolutionary algorithm\nwhich aims to find the most optimal mapping between voynichian glyphs and\ncandidate phonemic values. Core component of the decoding algorithm is a\nprocess of maximization of a fitness function which aims to find most optimal\nset of substitution rules allowing to transcribe the part of the manuscript --\nwhich we call the Calendar -- into lists of feminine names. This leads to sets\nof character subsitution rules which allow us to consistently transcribe dozens\namong three hundred calendar tokens into feminine names: a result far\nsurpassing both ``popular'' as well as \"state of the art\" tentatives to crack\nthe manuscript. What's more, by using name lists stemming from different\nlanguages as potential cribs, our ``adaptive'' method can also be useful in\nidentification of the language in which the manuscript is written.\n\nAs far as we can currently tell, results of our experiments indicate that the\nCalendar part of the manuscript contains names from baltoslavic, balkanic or\nhebrew language strata. Two further indications are also given: primo, highest\nfitness values were obtained when the crib list contains names with specific\ninfixes at token's penultimate position as is the case, for example, for slavic\n\\textbf{feminine diminutives} (i.e. names ending with -ka and not -a). In the\nmost successful scenario, 240 characters contained in 35 distinct Voynichese\ntokens were successfully transcribed. Secundo, in case of crib stemming from\nHebrew language, whole adaptation process converges to significantly better\nfitness values when transcribing voynichian tokens whose order of individual\ncharacters have been reversed, and when lists feminine and not masculine names\nare used as the crib.",
          "link": "http://arxiv.org/abs/2107.05381",
          "publishedOn": "2021-07-13T01:59:33.170Z",
          "wordCount": 730,
          "title": "Can Evolutionary Computation Help us to Crib the Voynich Manuscript ?. (arXiv:2107.05381v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1\">Naicheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaoshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qiongxu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kaixin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaobo Guo</a>",
          "description": "Session-based recommendation (SBR) learns users' preferences by capturing the\nshort-term and sequential patterns from the evolution of user behaviors. Among\nthe studies in the SBR field, graph-based approaches are a relatively powerful\nkind of way, which generally extract item information by message aggregation\nunder Euclidean space. However, such methods can't effectively extract the\nhierarchical information contained among consecutive items in a session, which\nis critical to represent users' preferences. In this paper, we present a\nhyperbolic contrastive graph recommender (HCGR), a principled session-based\nrecommendation framework involving Lorentz hyperbolic space to adequately\ncapture the coherence and hierarchical representations of the items. Within\nthis framework, we design a novel adaptive hyperbolic attention computation to\naggregate the graph message of each user's preference in a session-based\nbehavior sequence. In addition, contrastive learning is leveraged to optimize\nthe item representation by considering the geodesic distance between positive\nand negative samples in hyperbolic space. Extensive experiments on four\nreal-world datasets demonstrate that HCGR consistently outperforms\nstate-of-the-art baselines by 0.43$\\%$-28.84$\\%$ in terms of $HitRate$, $NDCG$\nand $MRR$.",
          "link": "http://arxiv.org/abs/2107.05366",
          "publishedOn": "2021-07-13T01:59:33.155Z",
          "wordCount": 616,
          "title": "HCGR: Hyperbolic Contrastive Graph Representation Learning for Session-based Recommendation. (arXiv:2107.05366v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Victor Zitian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montano_Campos_F/0/1/0/all/0/1\">Felipe Montano-Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_W/0/1/0/all/0/1\">Wlodek Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canfield_E/0/1/0/all/0/1\">Evan Canfield</a>",
          "description": "The volume of scientific publications in organizational research becomes\nexceedingly overwhelming for human researchers who seek to timely extract and\nreview knowledge. This paper introduces natural language processing (NLP)\nmodels to accelerate the discovery, extraction, and organization of theoretical\ndevelopments (i.e., hypotheses) from social science publications. We illustrate\nand evaluate NLP models in the context of a systematic review of stakeholder\nvalue constructs and hypotheses. Specifically, we develop NLP models to\nautomatically 1) detect sentences in scholarly documents as hypotheses or not\n(Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs)\nand links (causal/associative relationships) (Relationship Deconstruction ),\nand 3) classify the features of links in terms causality (versus association)\nand direction (positive, negative, versus nonlinear) (Feature Classification).\nOur models have reported high performance metrics for all three tasks. While\nour models are built in Python, we have made the pre-trained models fully\naccessible for non-programmers. We have provided instructions on installing and\nusing our pre-trained models via an R Shiny app graphic user interface (GUI).\nFinally, we suggest the next paths to extend our methodology for\ncomputer-assisted knowledge synthesis.",
          "link": "http://arxiv.org/abs/2106.16102",
          "publishedOn": "2021-07-13T01:59:33.132Z",
          "wordCount": 649,
          "title": "Machine Reading of Hypotheses for Organizational Research Reviews and Pre-trained Models via R Shiny App for Non-Programmers. (arXiv:2106.16102v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_D/0/1/0/all/0/1\">Dhivya Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1\">Vijay Mago</a>",
          "description": "Semantic textual similarity is one of the open research challenges in the\nfield of Natural Language Processing. Extensive research has been carried out\nin this field and near-perfect results are achieved by recent transformer-based\nmodels in existing benchmark datasets like the STS dataset and the SICK\ndataset. In this paper, we study the sentences in these datasets and analyze\nthe sensitivity of various word embeddings with respect to the complexity of\nthe sentences. We build a complex sentences dataset comprising of 50 sentence\npairs with associated semantic similarity values provided by 15 human\nannotators. Readability analysis is performed to highlight the increase in\ncomplexity of the sentences in the existing benchmark datasets and those in the\nproposed dataset. Further, we perform a comparative analysis of the performance\nof various word embeddings and language models on the existing benchmark\ndatasets and the proposed dataset. The results show the increase in complexity\nof the sentences has a significant impact on the performance of the embedding\nmodels resulting in a 10-20% decrease in Pearson's and Spearman's correlation.",
          "link": "http://arxiv.org/abs/2010.12637",
          "publishedOn": "2021-07-13T01:59:33.120Z",
          "wordCount": 661,
          "title": "Comparative analysis of word embeddings in assessing semantic similarity of complex sentences. (arXiv:2010.12637v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heidari_G/0/1/0/all/0/1\">Golsa Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamanifar_K/0/1/0/all/0/1\">Kamran Zamanifar</a>",
          "description": "Since using environments that are made according to the service oriented\narchitecture, we have more effective and dynamic applications. Semantic\nmatchmaking process is finding valuable service candidates for substitution. It\nis a very important aspect of using semantic Web Services. Our proposed\nmatchmaker algorithm performs semantic matching of Web Services on the basis of\ninput and output descriptions of semantic Web Services matching. This technique\ntakes advantages from a graph structure and flow networks. Our novel approach\nis assigning matchmaking scores to semantics of the inputs and outputs\nparameters and their types. It makes a flow network in which the weights of the\nedges are these scores, using FordFulkerson algorithm, we find matching rate of\ntwo web services. So, all services should be described in the same Ontology Web\nLanguage. Among these candidates, best one is chosen for substitution in the\ncase of an execution failure. Our approach uses the algorithm that has the\nleast running time among all others that can be used for bipartite matching.\nThe importance of problem is that in real systems, many fundamental problems\nwill occur by late answering. So system`s service should always be on and if\none of them crashes, it would be replaced fast. Semantic web matchmaker eases\nthis process.",
          "link": "http://arxiv.org/abs/2107.05368",
          "publishedOn": "2021-07-13T01:59:32.950Z",
          "wordCount": 660,
          "title": "A Three Phase Semantic Web Matchmaker. (arXiv:2107.05368v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanhua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiwen Xu</a>",
          "description": "Content feed, a type of product that recommends a sequence of items for users\nto browse and engage with, has gained tremendous popularity among social media\nplatforms. In this paper, we propose to study the diversity problem in such a\nscenario from an item sequence perspective using time series analysis\ntechniques. We derive a method called sliding spectrum decomposition (SSD) that\ncaptures users' perception of diversity in browsing a long item sequence. We\nalso share our experiences in designing and implementing a suitable item\nembedding method for accurate similarity measurement under long tail effect.\nCombined together, they are now fully implemented and deployed in Xiaohongshu\nApp's production recommender system that serves the main Explore Feed product\nfor tens of millions of users every day. We demonstrate the effectiveness and\nefficiency of the method through theoretical analysis, offline experiments and\nonline A/B tests.",
          "link": "http://arxiv.org/abs/2107.05204",
          "publishedOn": "2021-07-13T01:59:32.926Z",
          "wordCount": 596,
          "title": "Sliding Spectrum Decomposition for Diversified Recommendation. (arXiv:2107.05204v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1\">Jonathan Stray</a>",
          "description": "Polarization is implicated in the erosion of democracy and the progression to\nviolence, which makes the polarization properties of large algorithmic content\nselection systems (recommender systems) a matter of concern for peace and\nsecurity. While algorithm-driven social media does not seem to be a primary\ndriver of polarization at the country level, it could be a useful intervention\npoint in polarized societies. This paper examines algorithmic depolarization\ninterventions with the goal of conflict transformation: not suppressing or\neliminating conflict but moving towards more constructive conflict. Algorithmic\nintervention is considered at three stages: which content is available\n(moderation), how content is selected and personalized (ranking), and content\npresentation and controls (user interface). Empirical studies of online\nconflict suggest that the exposure diversity intervention proposed as an\nantidote to \"filter bubbles\" can be improved and can even worsen polarization\nunder some conditions. Using civility metrics in conjunction with diversity in\ncontent selection may be more effective. However, diversity-based interventions\nhave not been tested at scale and may not work in the diverse and dynamic\ncontexts of real platforms. Instead, intervening in platform polarization\ndynamics will likely require continuous monitoring of polarization metrics,\nsuch as the widely used \"feeling thermometer.\" These metrics can be used to\nevaluate product features, and potentially engineered as algorithmic\nobjectives. It may further prove necessary to include polarization measures in\nthe objective functions of recommender algorithms to prevent optimization\nprocesses from creating conflict as a side effect.",
          "link": "http://arxiv.org/abs/2107.04953",
          "publishedOn": "2021-07-13T01:59:32.859Z",
          "wordCount": 678,
          "title": "Designing Recommender Systems to Depolarize. (arXiv:2107.04953v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yutao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deyi Li</a>",
          "description": "Most of the existing deep learning-based sequential recommendation approaches\nutilize the recurrent neural network architecture or self-attention to model\nthe sequential patterns and temporal influence among a user's historical\nbehavior and learn the user's preference at a specific time. However, these\nmethods have two main drawbacks. First, they focus on modeling users' dynamic\nstates from a user-centric perspective and always neglect the dynamics of items\nover time. Second, most of them deal with only the first-order user-item\ninteractions and do not consider the high-order connectivity between users and\nitems, which has recently been proved helpful for the sequential\nrecommendation. To address the above problems, in this article, we attempt to\nmodel user-item interactions by a bipartite graph structure and propose a new\nrecommendation approach based on a Position-enhanced and Time-aware Graph\nConvolutional Network (PTGCN) for the sequential recommendation. PTGCN models\nthe sequential patterns and temporal dynamics between user-item interactions by\ndefining a position-enhanced and time-aware graph convolution operation and\nlearning the dynamic representations of users and items simultaneously on the\nbipartite graph with a self-attention aggregator. Also, it realizes the\nhigh-order connectivity between users and items by stacking multi-layer graph\nconvolutions. To demonstrate the effectiveness of PTGCN, we carried out a\ncomprehensive evaluation of PTGCN on three real-world datasets of different\nsizes compared with a few competitive baselines. Experimental results indicate\nthat PTGCN outperforms several state-of-the-art models in terms of two\ncommonly-used evaluation metrics for ranking.",
          "link": "http://arxiv.org/abs/2107.05235",
          "publishedOn": "2021-07-13T01:59:32.850Z",
          "wordCount": 686,
          "title": "Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations. (arXiv:2107.05235v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shuchang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "In recent years, graph neural networks (GNNs) have shown powerful ability in\ncollaborative filtering, which is a widely adopted recommendation scenario.\nWhile without any side information, existing graph neural network based methods\ngenerally learn a one-hot embedding for each user or item as the initial input\nrepresentation of GNNs. However, such one-hot embedding is intrinsically\ntransductive, making these methods with no inductive ability, i.e., failing to\ndeal with new users or new items that are unseen during training. Besides, the\nnumber of model parameters depends on the number of users and items, which is\nexpensive and not scalable. In this paper, we give a formal definition of\ninductive recommendation and solve the above problems by proposing Inductive\nrepresentation based Graph Convolutional Network (IGCN) for collaborative\nfiltering. Specifically, we design an inductive representation layer, which\nutilizes the interaction behavior with core users or items as the initial\nrepresentation, improving the general recommendation performance while bringing\ninductive ability. Note that, the number of parameters of IGCN only depends on\nthe number of core users or items, which is adjustable and scalable. Extensive\nexperiments on three public benchmarks demonstrate the state-of-the-art\nperformance of IGCN in both transductive and inductive recommendation\nscenarios, while with remarkably fewer model parameters. Our implementations\nare available here in PyTorch.",
          "link": "http://arxiv.org/abs/2107.05247",
          "publishedOn": "2021-07-13T01:59:32.788Z",
          "wordCount": 645,
          "title": "Inductive Representation Based Graph Convolution Network for Collaborative Filtering. (arXiv:2107.05247v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi-Geng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hui-Chu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>",
          "description": "Visual object localization is the key step in a series of object detection\ntasks. In the literature, high localization accuracy is achieved with the\nmainstream strongly supervised frameworks. However, such methods require\nobject-level annotations and are unable to detect objects of unknown\ncategories. Weakly supervised methods face similar difficulties. In this paper,\na self-paced learning framework is proposed to achieve accurate object\nlocalization on the rank list returned by instance search. The proposed\nframework mines the target instance gradually from the queries and their\ncorresponding top-ranked search results. Since a common instance is shared\nbetween the query and the images in the rank list, the target visual instance\ncan be accurately localized even without knowing what the object category is.\nIn addition to performing localization on instance search, the issue of\nfew-shot object detection is also addressed under the same framework. Superior\nperformance over state-of-the-art methods is observed on both tasks.",
          "link": "http://arxiv.org/abs/2107.05005",
          "publishedOn": "2021-07-13T01:59:32.764Z",
          "wordCount": 590,
          "title": "Towards Accurate Localization by Instance Search. (arXiv:2107.05005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young Kyun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>",
          "description": "Face image retrieval, which searches for images of the same identity from the\nquery input face image, is drawing more attention as the size of the image\ndatabase increases rapidly. In order to conduct fast and accurate retrieval, a\ncompact hash code-based methods have been proposed, and recently, deep face\nimage hashing methods with supervised classification training have shown\noutstanding performance. However, classification-based scheme has a\ndisadvantage in that it cannot reveal complex similarities between face images\ninto the hash code learning. In this paper, we attempt to improve the face\nimage retrieval quality by proposing a Similarity Guided Hashing (SGH) method,\nwhich gently considers self and pairwise-similarity simultaneously. SGH employs\nvarious data augmentations designed to explore elaborate similarities between\nface images, solving both intra and inter identity-wise difficulties. Extensive\nexperimental results on the protocols with existing benchmarks and an\nadditionally proposed large scale higher resolution face image dataset\ndemonstrate that our SGH delivers state-of-the-art retrieval performance.",
          "link": "http://arxiv.org/abs/2107.05025",
          "publishedOn": "2021-07-13T01:59:32.754Z",
          "wordCount": 596,
          "title": "Similarity Guided Deep Face Image Retrieval. (arXiv:2107.05025v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1\">Gabriel de Souza P. Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1\">Sara Rabhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ak_R/0/1/0/all/0/1\">Ronay Ak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md Yasin Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1\">Even Oldridge</a>",
          "description": "Session-based recommendation is an important task for e-commerce services,\nwhere a large number of users browse anonymously or may have very distinct\ninterests for different sessions. In this paper we present one of the winning\nsolutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce\nData Challenge. Our solution was inspired by NLP techniques and consists of an\nensemble of two Transformer architectures - Transformer-XL and XLNet - trained\nwith autoregressive and autoencoding approaches. To leverage most of the rich\ndataset made available for the competition, we describe how we prepared\nmulti-model features by combining tabular events with textual and image\nvectors. We also present a model prediction analysis to better understand the\neffectiveness of our architectures for the session-based recommendation.",
          "link": "http://arxiv.org/abs/2107.05124",
          "publishedOn": "2021-07-13T01:59:32.739Z",
          "wordCount": 593,
          "title": "Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. (arXiv:2107.05124v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1\">Noveen Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "We study the practical consequences of dataset sampling strategies on the\nperformance of recommendation algorithms. Recommender systems are generally\ntrained and evaluated on samples of larger datasets. Samples are often taken in\na naive or ad-hoc fashion: e.g. by sampling a dataset randomly or by selecting\nusers or items with many interactions. As we demonstrate, commonly-used data\nsampling schemes can have significant consequences on algorithm performance --\nmasking performance deficiencies in algorithms or altering the relative\nperformance of algorithms, as compared to models trained on the complete\ndataset. Following this observation, this paper makes the following main\ncontributions: (1) characterizing the effect of sampling on algorithm\nperformance, in terms of algorithm and dataset characteristics (e.g. sparsity\ncharacteristics, sequential dynamics, etc.); and (2) designing SVP-CF, which is\na data-specific sampling strategy, that aims to preserve the relative\nperformance of models after sampling, and is especially suited to long-tail\ninteraction data. Detailed experiments show that SVP-CF is more accurate than\ncommonly used sampling schemes in retaining the relative ranking of different\nrecommendation algorithms.",
          "link": "http://arxiv.org/abs/2107.04984",
          "publishedOn": "2021-07-13T01:59:32.720Z",
          "wordCount": 615,
          "title": "SVP-CF: Selection via Proxy for Collaborative Filtering Data. (arXiv:2107.04984v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haodong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yabo Chu</a>",
          "description": "Social-aware recommendation approaches have been recognized as an effective\nway to solve the data sparsity issue of traditional recommender systems. The\nassumption behind is that the knowledge in social user-user connections can be\nshared and transferred to the domain of user-item interactions, whereby to help\nlearn user preferences. However, most existing approaches merely adopt the\nfirst-order connections among users during transfer learning, ignoring those\nconnections in higher orders. We argue that better recommendation performance\ncan also benefit from high-order social relations. In this paper, we propose a\nnovel Propagation-aware Transfer Learning Network (PTLN) based on the\npropagation of social relations. We aim to better mine the sharing knowledge\nhidden in social networks and thus further improve recommendation performance.\nSpecifically, we explore social influence in two aspects: (a) higher-order\nfriends have been taken into consideration by order bias; (b) different friends\nin the same order will have distinct importance for recommendation by an\nattention mechanism. Besides, we design a novel regularization to bridge the\ngap between social relations and user-item interactions. We conduct extensive\nexperiments on two real-world datasets and beat other counterparts in terms of\nranking accuracy, especially for the cold-start users with few historical\ninteractions.",
          "link": "http://arxiv.org/abs/2107.04846",
          "publishedOn": "2021-07-13T01:59:32.708Z",
          "wordCount": 625,
          "title": "Propagation-aware Social Recommendation by Transfer Learning. (arXiv:2107.04846v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Recommending cold-start items is a long-standing and fundamental challenge in\nrecommender systems. Without any historical interaction on cold-start items, CF\nscheme fails to use collaborative signals to infer user preference on these\nitems. To solve this problem, extensive studies have been conducted to\nincorporate side information into the CF scheme. Specifically, they employ\nmodern neural network techniques (e.g., dropout, consistency constraint) to\ndiscover and exploit the coalition effect of content features and collaborative\nrepresentations. However, we argue that these works less explore the mutual\ndependencies between content features and collaborative representations and\nlack sufficient theoretical supports, thus resulting in unsatisfactory\nperformance. In this work, we reformulate the cold-start item representation\nlearning from an information-theoretic standpoint. It aims to maximize the\nmutual dependencies between item content and collaborative signals.\nSpecifically, the representation learning is theoretically lower-bounded by the\nintegration of two terms: mutual information between collaborative embeddings\nof users and items, and mutual information between collaborative embeddings and\nfeature representations of items. To model such a learning process, we devise a\nnew objective function founded upon contrastive learning and develop a simple\nyet effective Contrastive Learning-based Cold-start Recommendation\nframework(CLCRec). In particular, CLCRec consists of three components:\ncontrastive pair organization, contrastive embedding, and contrastive\noptimization modules. It allows us to preserve collaborative signals in the\ncontent representations for both warm and cold-start items. Through extensive\nexperiments on four publicly accessible datasets, we observe that CLCRec\nachieves significant improvements over state-of-the-art approaches in both\nwarm- and cold-start scenarios.",
          "link": "http://arxiv.org/abs/2107.05315",
          "publishedOn": "2021-07-13T01:59:32.618Z",
          "wordCount": 684,
          "title": "Contrastive Learning for Cold-Start Recommendation. (arXiv:2107.05315v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magron_P/0/1/0/all/0/1\">Paul Magron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>",
          "description": "State-of-the-art music recommender systems are based on collaborative\nfiltering, which builds upon learning similarities between users and songs from\nthe available listening data. These approaches inherently face the cold-start\nproblem, as they cannot recommend novel songs with no listening history.\nContent-aware recommendation addresses this issue by incorporating content\ninformation about the songs on top of collaborative filtering. However, methods\nfalling in this category rely on a shallow user/item interaction that\noriginates from a matrix factorization framework. In this work, we introduce\nneural content-aware collaborative filtering, a unified framework which\nalleviates these limits, and extends the recently introduced neural\ncollaborative filtering to its content-aware counterpart. We propose a\ngenerative model which leverages deep learning for both extracting content\ninformation from low-level acoustic features and for modeling the interaction\nbetween users and songs embeddings. The deep content feature extractor can\neither directly predict the item embedding, or serve as a regularization prior,\nyielding two variants (strict} and relaxed) of our model. Experimental results\nshow that the proposed method reaches state-of-the-art results for a cold-start\nmusic recommendation task. We notably observe that exploiting deep neural\nnetworks for learning refined user/item interactions outperforms approaches\nusing a more simple interaction model in a content-aware framework.",
          "link": "http://arxiv.org/abs/2102.12369",
          "publishedOn": "2021-07-12T01:55:14.512Z",
          "wordCount": 654,
          "title": "Neural content-aware collaborative filtering for cold-start music recommendation. (arXiv:2102.12369v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>",
          "description": "For present e-commerce platforms, session-based recommender systems are\ndeveloped to predict users' preference for next-item recommendation. Although a\nsession can usually reflect a user's current preference, a local shift of the\nuser's intention within the session may still exist. Specifically, the\ninteractions that take place in the early positions within a session generally\nindicate the user's initial intention, while later interactions are more likely\nto represent the latest intention. Such positional information has been rarely\nconsidered in existing methods, which restricts their ability to capture the\nsignificance of interactions at different positions. To thoroughly exploit the\npositional information within a session, a theoretical framework is developed\nin this paper to provide an in-depth analysis of the positional information. We\nformally define the properties of forward-awareness and backward-awareness to\nevaluate the ability of positional encoding schemes in capturing the initial\nand the latest intention. According to our analysis, existing positional\nencoding schemes are generally forward-aware only, which can hardly represent\nthe dynamics of the intention in a session. To enhance the positional encoding\nscheme for the session-based recommendation, a dual positional encoding (DPE)\nis proposed to account for both forward-awareness and backward-awareness. Based\non DPE, we propose a novel Positional Recommender (PosRec) model with a\nwell-designed Position-aware Gated Graph Neural Network module to fully exploit\nthe positional information for session-based recommendation tasks. Extensive\nexperiments are conducted on two e-commerce benchmark datasets, Yoochoose and\nDiginetica and the experimental results show the superiority of the PosRec by\ncomparing it with the state-of-the-art session-based recommender models.",
          "link": "http://arxiv.org/abs/2107.00846",
          "publishedOn": "2021-07-12T01:55:14.284Z",
          "wordCount": 697,
          "title": "Exploiting Positional Information for Session-based Recommendation. (arXiv:2107.00846v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>",
          "description": "Different from the traditional recommender system, the session-based\nrecommender system introduces the concept of the session, i.e., a sequence of\ninteractions between a user and multiple items within a period, to preserve the\nuser's recent interest. The existing work on the session-based recommender\nsystem mainly relies on mining sequential patterns within individual sessions,\nwhich are not expressive enough to capture more complicated dependency\nrelationships among items. In addition, it does not consider the cross-session\ninformation due to the anonymity of the session data, where the linkage between\ndifferent sessions is prevented. In this paper, we solve these problems with\nthe graph neural networks technique. First, each session is represented as a\ngraph rather than a linear sequence structure, based on which a novel Full\nGraph Neural Network (FGNN) is proposed to learn complicated item dependency.\nTo exploit and incorporate cross-session information in the individual\nsession's representation learning, we further construct a Broadly Connected\nSession (BCS) graph to link different sessions and a novel Mask-Readout\nfunction to improve session embedding based on the BCS graph. Extensive\nexperiments have been conducted on two e-commerce benchmark datasets, i.e.,\nYoochoose and Diginetica, and the experimental results demonstrate the\nsuperiority of our proposal through comparisons with state-of-the-art\nsession-based recommender models.",
          "link": "http://arxiv.org/abs/2107.00852",
          "publishedOn": "2021-07-12T01:55:14.243Z",
          "wordCount": 668,
          "title": "Exploiting Cross-Session Information for Session-based Recommendation with Graph Neural Networks. (arXiv:2107.00852v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>",
          "description": "Streaming session-based recommendation (SSR) is a challenging task that\nrequires the recommender system to do the session-based recommendation (SR) in\nthe streaming scenario. In the real-world applications of e-commerce and social\nmedia, a sequence of user-item interactions generated within a certain period\nare grouped as a session, and these sessions consecutively arrive in the form\nof streams. Most of the recent SR research has focused on the static setting\nwhere the training data is first acquired and then used to train a\nsession-based recommender model. They need several epochs of training over the\nwhole dataset, which is infeasible in the streaming setting. Besides, they can\nhardly well capture long-term user interests because of the neglect or the\nsimple usage of the user information. Although some streaming recommendation\nstrategies have been proposed recently, they are designed for streams of\nindividual interactions rather than streams of sessions. In this paper, we\npropose a Global Attributed Graph (GAG) neural network model with a Wasserstein\nreservoir for the SSR problem. On one hand, when a new session arrives, a\nsession graph with a global attribute is constructed based on the current\nsession and its associate user. Thus, the GAG can take both the global\nattribute and the current session into consideration to learn more\ncomprehensive representations of the session and the user, yielding a better\nperformance in the recommendation. On the other hand, for the adaptation to the\nstreaming session scenario, a Wasserstein reservoir is proposed to help\npreserve a representative sketch of the historical data. Extensive experiments\non two real-world datasets have been conducted to verify the superiority of the\nGAG model compared with the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2007.02747",
          "publishedOn": "2021-07-12T01:55:14.054Z",
          "wordCount": 738,
          "title": "GAG: Global Attributed Graph Neural Network for Streaming Session-based Recommendation. (arXiv:2007.02747v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Shaina Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chen Ding</a>",
          "description": "Nowadays, more and more news readers tend to read news online where they have\naccess to millions of news articles from multiple sources. In order to help\nusers to find the right and relevant content, news recommender systems (NRS)\nare developed to relieve the information overload problem and suggest news\nitems that users might be interested in. In this paper, we highlight the major\nchallenges faced by the news recommendation domain and identify the possible\nsolutions from the state-of-the-art. Due to the rapid growth of building\nrecommender systems using deep learning models, we divide our discussion in two\nparts. In the first part, we present an overview of the conventional\nrecommendation solutions, datasets, evaluation criteria beyond accuracy and\nrecommendation platforms being used in NRS. In the second part, we explain the\ndeep learning-based recommendation solutions applied in NRS. Different from\nprevious surveys, we also study the effects of news recommendations on user\nbehavior and try to suggest the possible remedies to mitigate these effects. By\nproviding the state-of-the-art knowledge, this survey can help researchers and\npractical professionals in their understanding of developments in news\nrecommendation algorithms. It also sheds light on potential new directions",
          "link": "http://arxiv.org/abs/2009.04964",
          "publishedOn": "2021-07-12T01:55:14.040Z",
          "wordCount": 676,
          "title": "News Recommender System: A review of recent progress, challenges, and opportunities. (arXiv:2009.04964v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.11942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>",
          "description": "Predicting a user's preference in a short anonymous interaction session\ninstead of long-term history is a challenging problem in the real-life\nsession-based recommendation, e.g., e-commerce and media stream. Recent\nresearch of the session-based recommender system mainly focuses on sequential\npatterns by utilizing the attention mechanism, which is straightforward for the\nsession's natural sequence sorted by time. However, the user's preference is\nmuch more complicated than a solely consecutive time pattern in the transition\nof item choices. In this paper, therefore, we study the item transition pattern\nby constructing a session graph and propose a novel model which collaboratively\nconsiders the sequence order and the latent order in the session graph for a\nsession-based recommender system. We formulate the next item recommendation\nwithin the session as a graph classification problem. Specifically, we propose\na weighted attention graph layer and a Readout function to learn embeddings of\nitems and sessions for the next item recommendation. Extensive experiments have\nbeen conducted on two benchmark E-commerce datasets, Yoochoose and Diginetica,\nand the experimental results show that our model outperforms other\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/1911.11942",
          "publishedOn": "2021-07-12T01:55:13.979Z",
          "wordCount": 645,
          "title": "Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks. (arXiv:1911.11942v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pournaras_E/0/1/0/all/0/1\">Evangelos Pournaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghulam_A/0/1/0/all/0/1\">Atif Nabi Ghulam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunz_R/0/1/0/all/0/1\">Renato Kunz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanggli_R/0/1/0/all/0/1\">Regula H&#xe4;nggli</a>",
          "description": "Outdoor `living lab' experimentation using pervasive computing provides new\nopportunities: higher realism, external validity and large-scale\nsocio-spatio-temporal observations. However, experimentation `in the wild' is\nhighly complex and costly. Noise, biases, privacy concerns to comply with\nstandards of ethical review boards, remote moderation, control of experimental\nconditions and equipment perplex the collection of high-quality data for causal\ninference. This article introduces Smart Agora, a novel open-source software\nplatform for rigorous systematic outdoor experimentation. Without writing a\nsingle line of code, highly complex experimental scenarios are visually\ndesigned and automatically deployed to smart phones. Novel geolocated survey\nand sensor data are collected subject of participants verifying desired\nexperimental conditions, for instance. their presence at certain urban spots.\nThis new approach drastically improves the quality and purposefulness of crowd\nsensing, tailored to conditions that confirm/reject hypotheses. The features\nthat support this innovative functionality and the broad spectrum of its\napplicability are demonstrated.",
          "link": "http://arxiv.org/abs/2107.04117",
          "publishedOn": "2021-07-12T01:55:13.946Z",
          "wordCount": 598,
          "title": "Crowd Sensing and Living Lab Outdoor Experimentation Made Easy. (arXiv:2107.04117v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takiguchi_K/0/1/0/all/0/1\">Kentaro Takiguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fain_M/0/1/0/all/0/1\">Mikhail Fain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1\">Niall Twomey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaquero_L/0/1/0/all/0/1\">Luis M Vaquero</a>",
          "description": "Explicitly modelling field interactions and correlations in complex document\nstructures has recently gained popularity in neural document embedding and\nretrieval tasks. Although this requires the specification of bespoke\ntask-dependent models, encouraging empirical results are beginning to emerge.\nWe present the first in-depth analyses of non-linear multi-field interaction\n(NL-MFI) ranking in the cooking domain in this work. Our results show that\nfield-weighted factorisation machines models provide a statistically\nsignificant improvement over baselines in recipe retrieval tasks. Additionally,\nwe show that sparsely capturing subsets of field interactions based on domain\nknowledge and feature selection heuristics offers significant advantages over\nbaselines and exhaustive alternatives. Although field-interaction aware models\nare more elaborate from an architectural basis, they are often more\ndata-efficient in optimisation and are better suited for explainability due to\nmirrored document and model factorisation.",
          "link": "http://arxiv.org/abs/2105.05710",
          "publishedOn": "2021-07-09T01:58:24.869Z",
          "wordCount": 592,
          "title": "Evaluation of Field-Aware Neural Ranking Models for Recipe Search. (arXiv:2105.05710v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Recent studies have shown that providing personalized explanations alongside\nrecommendations increases trust and perceived quality. Furthermore, it gives\nusers an opportunity to refine the recommendations by critiquing parts of the\nexplanations. On one hand, current recommender systems model the\nrecommendation, explanation, and critiquing objectives jointly, but this\ncreates an inherent trade-off between their respective performance. On the\nother hand, although recent latent linear critiquing approaches are built upon\nan existing recommender system, they suffer from computational inefficiency at\ninference due to the objective optimized at each conversation's turn. We\naddress these deficiencies with M&Ms-VAE, a novel variational autoencoder for\nrecommendation and explanation that is based on multimodal modeling\nassumptions. We train the model under a weak supervision scheme to simulate\nboth fully and partially observed variables. Then, we leverage the\ngeneralization ability of a trained M&Ms-VAE model to embed the user preference\nand the critique separately. Our work's most important innovation is our\ncritiquing module, which is built upon and trained in a self-supervised manner\nwith a simple ranking objective. Experiments on four real-world datasets\ndemonstrate that among state-of-the-art models, our system is the first to\ndominate or match the performance in terms of recommendation, explanation, and\nmulti-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x\nfaster than the best baselines. Finally, we show that our model infers coherent\njoint and cross generation, even under weak supervision, thanks to our\nmultimodal-based modeling and training scheme.",
          "link": "http://arxiv.org/abs/2105.00774",
          "publishedOn": "2021-07-09T01:58:24.689Z",
          "wordCount": 706,
          "title": "Fast Multi-Step Critiquing for VAE-based Recommender Systems. (arXiv:2105.00774v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Manli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangzhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guoxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jingyuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Heng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baogui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weihao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zongzheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yueqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yida Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuqing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1\">Danyang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chuhao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Multi-modal pre-training models have been intensively explored to bridge\nvision and language in recent years. However, most of them explicitly model the\ncross-modal interaction between image-text pairs, by assuming that there exists\nstrong semantic correlation between the text and image modalities. Since this\nstrong assumption is often invalid in real-world scenarios, we choose to\nimplicitly model the cross-modal correlation for large-scale multi-modal\npre-training, which is the focus of the Chinese project `WenLan' led by our\nteam. Specifically, with the weak correlation assumption over image-text pairs,\nwe propose a two-tower pre-training model called BriVL within the cross-modal\ncontrastive learning framework. Unlike OpenAI CLIP that adopts a simple\ncontrastive learning method, we devise a more advanced algorithm by adapting\nthe latest method MoCo into the cross-modal scenario. By building a large\nqueue-based dictionary, our BriVL can incorporate more negative samples in\nlimited GPU resources. We further construct a large Chinese multi-source\nimage-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.\nExtensive experiments demonstrate that the pre-trained BriVL model outperforms\nboth UNITER and OpenAI CLIP on various downstream tasks.",
          "link": "http://arxiv.org/abs/2103.06561",
          "publishedOn": "2021-07-09T01:58:24.680Z",
          "wordCount": 761,
          "title": "WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training. (arXiv:2103.06561v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "Personalized news recommendation is an important technique to help users find\ntheir interested news information and alleviate their information overload. It\nhas been extensively studied over decades and has achieved notable success in\nimproving users' news reading experience. However, there are still many\nunsolved problems and challenges that need to be further studied. To help\nresearchers master the advances in personalized news recommendation over the\npast years, in this paper we present a comprehensive overview of personalized\nnews recommendation. Instead of following the conventional taxonomy of news\nrecommendation methods, in this paper we propose a novel perspective to\nunderstand personalized news recommendation based on its core problems and the\nassociated techniques and challenges. We first review the techniques for\ntackling each core problem in a personalized news recommender system and the\nchallenges they face. Next, we introduce the public datasets and evaluation\nmethods for personalized news recommendation. We then discuss the key points on\nimproving the responsibility of personalized news recommender systems. Finally,\nwe raise several research directions that are worth investigating in the\nfuture. This paper can provide up-to-date and comprehensive views to help\nreaders understand the personalized news recommendation field. We hope this\npaper can facilitate research on personalized news recommendation and as well\nas related fields in natural language processing and data mining.",
          "link": "http://arxiv.org/abs/2106.08934",
          "publishedOn": "2021-07-09T01:58:24.665Z",
          "wordCount": 667,
          "title": "Personalized News Recommendation: A Survey. (arXiv:2106.08934v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansoury_M/0/1/0/all/0/1\">Masoud Mansoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdollahpouri_H/0/1/0/all/0/1\">Himan Abdollahpouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1\">Bamshad Mobasher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_R/0/1/0/all/0/1\">Robin Burke</a>",
          "description": "Fairness is a critical system-level objective in recommender systems that has\nbeen the subject of extensive recent research. A specific form of fairness is\nsupplier exposure fairness where the objective is to ensure equitable coverage\nof items across all suppliers in recommendations provided to users. This is\nespecially important in multistakeholder recommendation scenarios where it may\nbe important to optimize utilities not just for the end-user, but also for\nother stakeholders such as item sellers or producers who desire a fair\nrepresentation of their items. This type of supplier fairness is sometimes\naccomplished by attempting to increasing aggregate diversity in order to\nmitigate popularity bias and to improve the coverage of long-tail items in\nrecommendations. In this paper, we introduce FairMatch, a general graph-based\nalgorithm that works as a post processing approach after recommendation\ngeneration to improve exposure fairness for items and suppliers. The algorithm\niteratively adds high quality items that have low visibility or items from\nsuppliers with low exposure to the users' final recommendation lists. A\ncomprehensive set of experiments on two datasets and comparison with\nstate-of-the-art baselines show that FairMatch, while significantly improves\nexposure fairness and aggregate diversity, maintains an acceptable level of\nrelevance of the recommendations.",
          "link": "http://arxiv.org/abs/2107.03415",
          "publishedOn": "2021-07-09T01:58:24.650Z",
          "wordCount": 653,
          "title": "A Graph-based Approach for Mitigating Multi-sided Exposure Bias in Recommender Systems. (arXiv:2107.03415v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1\">Iv&#xe1;n Cantador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1\">Fernando Diez</a>",
          "description": "The success of neural network embeddings has entailed a renewed interest in\nusing knowledge graphs for a wide variety of machine learning and information\nretrieval tasks. In particular, recent recommendation methods based on graph\nembeddings have shown state-of-the-art performance. In general, these methods\nencode latent rating patterns and content features. Differently from previous\nwork, in this paper, we propose to exploit embeddings extracted from graphs\nthat combine information from ratings and aspect-based opinions expressed in\ntextual reviews. We then adapt and evaluate state-of-the-art graph embedding\ntechniques over graphs generated from Amazon and Yelp reviews on six domains,\noutperforming baseline recommenders. Additionally, our method has the advantage\nof providing explanations that involve the coverage of aspect-based opinions\ngiven by users about recommended items.",
          "link": "http://arxiv.org/abs/2107.03385",
          "publishedOn": "2021-07-09T01:58:24.618Z",
          "wordCount": 568,
          "title": "Rating and aspect-based opinion graph embeddings for explainable recommendations. (arXiv:2107.03385v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>",
          "description": "Leveraging the side information associated with entities (i.e.\\ users and\nitems) to enhance the performance of recommendation systems has been widely\nrecognized as an important modelling dimension. While many existing approaches\nfocus on the \\emph{integration scheme} to incorporate entity side information\n-- by combining the recommendation loss function with an extra side\ninformation-aware loss -- in this paper, we propose instead a novel\n\\emph{pre-training scheme} for leveraging the side information. In particular,\nwe first pre-train a representation model using the side information of the\nentities, and then fine-tune it using an existing general representation-based\nrecommendation model. Specifically, we propose two pre-training models, named\n\\gcn{} and \\com{}, by considering the entities and their relations constructed\nfrom side information as two different types of graphs respectively, to\npre-train entity embeddings. For the \\gcn{} model, two single-relational graphs\nare constructed from all the users' and items' side information respectively,\nto pre-train entity representations by using the Graph Convolutional Networks.\nFor the \\com{} model, two multi-relational graphs are constructed to pre-train\nthe entity representations by using the Composition-based Graph Convolutional\nNetworks. An extensive evaluation of our pre-training models fine-tuned under\nfour general representation-based recommender models, i.e.\\ MF, NCF, NGCF and\nLightGCN, shows that effectively pre-training embeddings with both the user's\nand item's side information can significantly improve these original models in\nterms of both effectiveness and stability.",
          "link": "http://arxiv.org/abs/2107.03936",
          "publishedOn": "2021-07-09T01:58:24.600Z",
          "wordCount": 655,
          "title": "Graph Neural Pre-training for Enhancing Recommendations using Side Information. (arXiv:2107.03936v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">SeongKu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyun_D/0/1/0/all/0/1\">Dongmin Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>",
          "description": "Session-based Recommender Systems (SRSs) have been actively developed to\nrecommend the next item of an anonymous short item sequence (i.e., session).\nUnlike sequence-aware recommender systems where the whole interaction sequence\nof each user can be used to model both the short-term interest and the general\ninterest of the user, the absence of user-dependent information in SRSs makes\nit difficult to directly derive the user's general interest from data.\nTherefore, existing SRSs have focused on how to effectively model the\ninformation about short-term interest within the sessions, but they are\ninsufficient to capture the general interest of users. To this end, we propose\na novel framework to overcome the limitation of SRSs, named ProxySR, which\nimitates the missing information in SRSs (i.e., general interest of users) by\nmodeling proxies of sessions. ProxySR selects a proxy for the input session in\nan unsupervised manner, and combines it with the encoded short-term interest of\nthe session. As a proxy is jointly learned with the short-term interest and\nselected by multiple sessions, a proxy learns to play the role of the general\ninterest of a user and ProxySR learns how to select a suitable proxy for an\ninput session. Moreover, we propose another real-world situation of SRSs where\na few users are logged-in and leave their identifiers in sessions, and a\nrevision of ProxySR for the situation. Our experiments on real-world datasets\nshow that ProxySR considerably outperforms the state-of-the-art competitors,\nand the proxies successfully imitate the general interest of the users without\nany user-dependent information.",
          "link": "http://arxiv.org/abs/2107.03564",
          "publishedOn": "2021-07-09T01:58:24.586Z",
          "wordCount": 690,
          "title": "Unsupervised Proxy Selection for Session-based Recommender Systems. (arXiv:2107.03564v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvir Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-09T01:58:24.571Z",
          "wordCount": 691,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yitong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ethan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Predicting the next interaction of a short-term interaction session is a\nchallenging task in session-based recommendation. Almost all existing works\nrely on item transition patterns, and neglect the impact of user historical\nsessions while modeling user preference, which often leads to non-personalized\nrecommendation. Additionally, existing personalized session-based recommenders\ncapture user preference only based on the sessions of the current user, but\nignore the useful item-transition patterns from other user's historical\nsessions. To address these issues, we propose a novel Heterogeneous Global\nGraph Neural Networks (HG-GNN) to exploit the item transitions over all\nsessions in a subtle manner for better inferring user preference from the\ncurrent and historical sessions. To effectively exploit the item transitions\nover all sessions from users, we propose a novel heterogeneous global graph\nthat contains item transitions of sessions, user-item interactions and global\nco-occurrence items. Moreover, to capture user preference from sessions\ncomprehensively, we propose to learn two levels of user representations from\nthe global graph via two graph augmented preference encoders. Specifically, we\ndesign a novel heterogeneous graph neural network (HGNN) on the heterogeneous\nglobal graph to learn the long-term user preference and item representations\nwith rich semantics. Based on the HGNN, we propose the Current Preference\nEncoder and the Historical Preference Encoder to capture the different levels\nof user preference from the current and historical sessions, respectively. To\nachieve personalized recommendation, we integrate the representations of the\nuser current preference and historical interests to generate the final user\npreference representation. Extensive experimental results on three real-world\ndatasets show that our model outperforms other state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.03813",
          "publishedOn": "2021-07-09T01:58:24.546Z",
          "wordCount": 709,
          "title": "Heterogeneous Global Graph Neural Networks for Personalized Session-based Recommendation. (arXiv:2107.03813v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.06262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qingyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargteil_A/0/1/0/all/0/1\">Adam Bargteil</a>",
          "description": "We explore computational approaches for visual guidance to aid in creating\naesthetically pleasing art and graphic design. Our work complements and builds\non previous work that developed models for how humans look at images. Our\napproach comprises three steps. First, we collected a dataset of art\nmasterpieces and labeled the visual fixations with state-of-art vision models.\nSecond, we clustered the visual guidance templates of the art masterpieces with\nunsupervised learning. Third, we developed a pipeline using generative\nadversarial networks to learn the principles of visual guidance and that can\nproduce aesthetically pleasing layouts. We show that the aesthetic visual\nguidance principles can be learned and integrated into a high-dimensional model\nand can be queried by the features of graphic elements. We evaluate our\napproach by generating layouts on various drawings and graphic designs.\nMoreover, our model considers the color and structure of graphic elements when\ngenerating layouts. Consequently, we believe our tool, which generates multiple\naesthetic layout options in seconds, can help artists create beautiful art and\ngraphic designs.",
          "link": "http://arxiv.org/abs/2107.06262",
          "publishedOn": "2021-07-14T01:41:48.550Z",
          "wordCount": 607,
          "title": "Learning Aesthetic Layouts via Visual Guidance. (arXiv:2107.06262v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoxin Liu</a>",
          "description": "Domain generalization (DG) aims to help models trained on a set of source\ndomains generalize better on unseen target domains. The performances of current\nDG methods largely rely on sufficient labeled data, which however are usually\ncostly or unavailable. While unlabeled data are far more accessible, we seek to\nexplore how unsupervised learning can help deep models generalizes across\ndomains. Specifically, we study a novel generalization problem called\nunsupervised domain generalization, which aims to learn generalizable models\nwith unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised\nLearning (DIUL) method to cope with the significant and misleading\nheterogeneity within unlabeled data and severe distribution shifts between\nsource and target data. Surprisingly we observe that DIUL can not only\ncounterbalance the scarcity of labeled data but also further strengthen the\ngeneralization ability of models when the labeled data are sufficient. As a\npretraining approach, DIUL shows superior to ImageNet pretraining protocol even\nwhen the available data are unlabeled and of a greatly smaller amount compared\nto ImageNet. Extensive experiments clearly demonstrate the effectiveness of our\nmethod compared with state-of-the-art unsupervised learning counterparts.",
          "link": "http://arxiv.org/abs/2107.06219",
          "publishedOn": "2021-07-14T01:41:48.534Z",
          "wordCount": 625,
          "title": "Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization. (arXiv:2107.06219v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gunjan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "Dance and music typically go hand in hand. The complexities in dance, music,\nand their synchronisation make them fascinating to study from a computational\ncreativity perspective. While several works have looked at generating dance for\na given music, automatically generating music for a given dance remains\nunder-explored. This capability could have several creative expression and\nentertainment applications. We present some early explorations in this\ndirection. We present a search-based offline approach that generates music\nafter processing the entire dance video and an online approach that uses a deep\nneural network to generate music on-the-fly as the video proceeds. We compare\nthese approaches to a strong heuristic baseline via human studies and present\nour findings. We have integrated our online approach in a live demo! A video of\nthe demo can be found here:\nhttps://sites.google.com/view/dance2music/live-demo.",
          "link": "http://arxiv.org/abs/2107.06252",
          "publishedOn": "2021-07-14T01:41:48.495Z",
          "wordCount": 564,
          "title": "Dance2Music: Automatic Dance-driven Music Generation. (arXiv:2107.06252v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1\">Rodrigo Castellon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We demonstrate that language models pre-trained on codified\n(discretely-encoded) music audio learn representations that are useful for\ndownstream MIR tasks. Specifically, we explore representations from Jukebox\n(Dhariwal et al. 2020): a music generation system containing a language model\ntrained on codified audio from 1M songs. To determine if Jukebox's\nrepresentations contain useful information for MIR, we use them as input\nfeatures to train shallow models on several MIR tasks. Relative to\nrepresentations from conventional MIR models which are pre-trained on tagging,\nwe find that using representations from Jukebox as input features yields 30%\nstronger performance on average across four MIR tasks: tagging, genre\nclassification, emotion recognition, and key detection. For key detection, we\nobserve that representations from Jukebox are considerably stronger than those\nfrom models pre-trained on tagging, suggesting that pre-training via codified\naudio language modeling may address blind spots in conventional approaches. We\ninterpret the strength of Jukebox's representations as evidence that modeling\naudio instead of tags provides richer representations for MIR.",
          "link": "http://arxiv.org/abs/2107.05677",
          "publishedOn": "2021-07-14T01:41:48.398Z",
          "wordCount": 621,
          "title": "Codified audio language modeling learns useful representations for music information retrieval. (arXiv:2107.05677v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Recommending cold-start items is a long-standing and fundamental challenge in\nrecommender systems. Without any historical interaction on cold-start items, CF\nscheme fails to use collaborative signals to infer user preference on these\nitems. To solve this problem, extensive studies have been conducted to\nincorporate side information into the CF scheme. Specifically, they employ\nmodern neural network techniques (e.g., dropout, consistency constraint) to\ndiscover and exploit the coalition effect of content features and collaborative\nrepresentations. However, we argue that these works less explore the mutual\ndependencies between content features and collaborative representations and\nlack sufficient theoretical supports, thus resulting in unsatisfactory\nperformance. In this work, we reformulate the cold-start item representation\nlearning from an information-theoretic standpoint. It aims to maximize the\nmutual dependencies between item content and collaborative signals.\nSpecifically, the representation learning is theoretically lower-bounded by the\nintegration of two terms: mutual information between collaborative embeddings\nof users and items, and mutual information between collaborative embeddings and\nfeature representations of items. To model such a learning process, we devise a\nnew objective function founded upon contrastive learning and develop a simple\nyet effective Contrastive Learning-based Cold-start Recommendation\nframework(CLCRec). In particular, CLCRec consists of three components:\ncontrastive pair organization, contrastive embedding, and contrastive\noptimization modules. It allows us to preserve collaborative signals in the\ncontent representations for both warm and cold-start items. Through extensive\nexperiments on four publicly accessible datasets, we observe that CLCRec\nachieves significant improvements over state-of-the-art approaches in both\nwarm- and cold-start scenarios.",
          "link": "http://arxiv.org/abs/2107.05315",
          "publishedOn": "2021-07-13T01:59:32.896Z",
          "wordCount": 684,
          "title": "Contrastive Learning for Cold-Start Recommendation. (arXiv:2107.05315v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Ahmed Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moy_F/0/1/0/all/0/1\">Foong Ming Moy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_N/0/1/0/all/0/1\">Nadine Kong</a>",
          "description": "Obesity is known to lower the quality of life substantially. It is often\nassociated with increased chances of non-communicable diseases such as\ndiabetes, cardiovascular problems, various cancers, etc. Evidence suggests that\ndiet-related mobile applications play a vital role in assisting individuals in\nmaking healthier choices and keeping track of food intake. However, due to an\nabundance of similar applications, it becomes pertinent to evaluate each of\nthem in terms of functionality, usability, and possible design issues to truly\ndetermine state-of-the-art solutions for the future. Since these applications\ninvolve implementing multiple user requirements and recommendations from\ndifferent dietitians, the evaluation becomes quite complex. Therefore, this\nstudy aims to review existing dietary applications at length to highlight key\nfeatures and problems that enhance or undermine an application's usability. For\nthis purpose, we have examined the published literature from various scientific\ndatabases of the PUBMED, CINAHL (January 2010-December 2019) and Science Direct\n(2010-2019). We followed PRISMA guidelines, and out of our findings, fifty-six\nprimary studies met our inclusion criteria after identification, screening,\neligibility and full-text evaluation. We analyzed 35 apps from the selected\nstudies and extracted the data of each of the identified apps.Following our\ndetailed analysis on the comprehensiveness of freely available mHealth\napplications, we specified potential future research challenges and stated\nrecommendations to help grow clinically accurate diet-related applications.",
          "link": "http://arxiv.org/abs/2008.09883",
          "publishedOn": "2021-07-13T01:59:32.823Z",
          "wordCount": 720,
          "title": "A Review of Critical Features and General Issues of Freely Available mHealth Apps For Dietary Assessment. (arXiv:2008.09883v4 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yi-Hui Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1\">I-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chin-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1\">Joann Ching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "This paper presents an attempt to employ the mask language modeling approach\nof BERT to pre-train a 12-layer Transformer model over 4,166 pieces of\npolyphonic piano MIDI files for tackling a number of symbolic-domain\ndiscriminative music understanding tasks. These include two note-level\nclassification tasks, i.e., melody extraction and velocity prediction, as well\nas two sequence-level classification tasks, i.e., composer classification and\nemotion classification. We find that, given a pre-trained Transformer, our\nmodels outperform recurrent neural network based baselines with less than 10\nepochs of fine-tuning. Ablation studies show that the pre-training remains\neffective even if none of the MIDI data of the downstream tasks are seen at the\npre-training stage, and that freezing the self-attention layers of the\nTransformer at the fine-tuning stage slightly degrades performance. All the\nfive datasets employed in this work are publicly available, as well as\ncheckpoints of our pre-trained and fine-tuned models. As such, our research can\nbe taken as a benchmark for symbolic-domain music understanding.",
          "link": "http://arxiv.org/abs/2107.05223",
          "publishedOn": "2021-07-13T01:59:32.810Z",
          "wordCount": 603,
          "title": "MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding. (arXiv:2107.05223v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1\">Shivangi Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Midoglu_C/0/1/0/all/0/1\">Cise Midoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Nguyen_D/0/1/0/all/0/1\">Duc-Tien Dang-Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael Alexander Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">Paal Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Niessner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bregler_C/0/1/0/all/0/1\">Chris Bregler</a>",
          "description": "Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\nmanipulations of multimedia content. Cheapfakes are known to be more prevalent\nthan deepfakes. Cheapfake media can be created using editing software for\nimage/video manipulations, or even without using any software, by simply\naltering the context of an image/video by sharing the media alongside\nmisleading claims. This alteration of context is referred to as out-of-context\n(OOC) misuse} of media. OOC media is much harder to detect than fake media,\nsince the images and videos are not tampered. In this challenge, we focus on\ndetecting OOC images, and more specifically the misuse of real photographs with\nconflicting image captions in news items. The aim of this challenge is to\ndevelop and benchmark models that can be used to detect whether given samples\n(news image and associated captions) are OOC, based on the recently compiled\nCOSMOS dataset.",
          "link": "http://arxiv.org/abs/2107.05297",
          "publishedOn": "2021-07-13T01:59:32.800Z",
          "wordCount": 577,
          "title": "MMSys'21 Grand Challenge on Detecting Cheapfakes. (arXiv:2107.05297v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-13T01:59:32.430Z",
          "wordCount": 612,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sunami_T/0/1/0/all/0/1\">Tomoya Sunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itahara_S/0/1/0/all/0/1\">Sohei Itahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koda_Y/0/1/0/all/0/1\">Yusuke Koda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1\">Takayuki Nishio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1\">Koji Yamamoto</a>",
          "description": "This paper demonstrates the feasibility of received power strength indicator\n(RSSI)-based single-antenna localization (R-SAL) with decimeter-level\nlocalization accuracy. To achieve decimeter-level accuracy, either fine-grained\nradio frequency (RF) information (e.g., channel state information) or\ncoarse-grained RF information (e.g., RSSI) from more than multiple antennas is\nrequired. Meanwhile, owing to deficiency of single-antenna RSSI which only\nindicates a distance between a receiver and a transmitter, realizing\nfine-grained localization accuracy with single coarse-grained RF information is\nchallenging. Our key idea to address this challenge is to leverage computer\nvision (CV) and to estimate the most likely Fresnel zone between the receiver\nand transmitter, where the role of RSSI is to detect blockage timings.\nSpecifically, historical positions of an obstacle that dynamically blocks the\nFresnel zone are detected by the CV technique, and we estimate positions at\nwhich a blockage starts and ends via a time series of RSSI. These estimated\nobstacle positions, in principle, coincide with points on the Fresnel zone\nboundaries, enabling the estimation of the Fresnel zone and localization of the\ntransmitter. The experimental evaluation revealed that the proposed R-SAL\nachieved decimeter-level localization in an indoor environment, which is\ncomparable to that of a simple previous RSSI-based localization with three\nreceivers.",
          "link": "http://arxiv.org/abs/2107.04770",
          "publishedOn": "2021-07-13T01:59:32.408Z",
          "wordCount": 642,
          "title": "Computer Vision-assisted Decimeter-level Single-antenna RSSI Localization Harnessing Dynamic Blockage Events. (arXiv:2107.04770v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1\">Bing-Kun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>",
          "description": "Video question answering is a challenging task, which requires agents to be\nable to understand rich video contents and perform spatial-temporal reasoning.\nHowever, existing graph-based methods fail to perform multi-step reasoning\nwell, neglecting two properties of VideoQA: (1) Even for the same video,\ndifferent questions may require different amount of video clips or objects to\ninfer the answer with relational reasoning; (2) During reasoning, appearance\nand motion features have complicated interdependence which are correlated and\ncomplementary to each other. Based on these observations, we propose a\nDual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an\nend-to-end fashion. The first contribution of our DualVGR is the design of an\nexplainable Query Punishment Module, which can filter out irrelevant visual\nfeatures through multiple cycles of reasoning. The second contribution is the\nproposed Video-based Multi-view Graph Attention Network, which captures the\nrelations between appearance and motion features. Our DualVGR network achieves\nstate-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and\ndemonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is\navailable at https://github.com/MMIR/DualVGR-VideoQA.",
          "link": "http://arxiv.org/abs/2107.04768",
          "publishedOn": "2021-07-13T01:59:32.384Z",
          "wordCount": 628,
          "title": "DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering. (arXiv:2107.04768v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shubham_K/0/1/0/all/0/1\">Kumar Shubham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnihotri_P/0/1/0/all/0/1\">Prateek Agnihotri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Movva_N/0/1/0/all/0/1\">Nitin D. Movva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessenyei_S/0/1/0/all/0/1\">Szilard Bessenyei</a>",
          "description": "It is easier to hear birds than see them, however, they still play an\nessential role in nature and they are excellent indicators of deteriorating\nenvironmental quality and pollution. Recent advances in Machine Learning and\nConvolutional Neural Networks allow us to detect and classify bird sounds, by\ndoing this, we can assist researchers in monitoring the status and trends of\nbird populations and biodiversity in ecosystems. We propose a sound detection\nand classification pipeline for analyzing complex soundscape recordings and\nidentify birdcalls in the background. Our pipeline learns from weak labels,\nclassifies fine-grained bird vocalizations in the wild, and is robust against\nbackground sounds (e.g., airplanes, rain, etc). Our solution achieved 10th\nplace of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle.",
          "link": "http://arxiv.org/abs/2107.04878",
          "publishedOn": "2021-07-13T01:59:32.340Z",
          "wordCount": 589,
          "title": "Weakly-Supervised Classification and Detection of Bird Sounds in the Wild. A BirdCLEF 2021 Solution. (arXiv:2107.04878v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.469 for valence and 0.649 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-07-12T01:55:14.429Z",
          "wordCount": 628,
          "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1\">Maksim Siniukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1\">Dmitriy Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>",
          "description": "Video quality measurement plays a critical role in the development of video\nprocessing applications. In this paper, we show how popular quality metrics\nVMAF and its tuning-resistant version VMAF NEG can be artificially increased by\nvideo preprocessing. We propose a pipeline for tuning parameters of processing\nalgorithms that allows increasing VMAF by up to 218.8%. A subjective comparison\nof preprocessed videos showed that with the majority of methods visual quality\ndrops down or stays unchanged. We show that VMAF NEG scores can also be\nincreased by some preprocessing methods by up to 23.6%.",
          "link": "http://arxiv.org/abs/2107.04510",
          "publishedOn": "2021-07-12T01:55:14.002Z",
          "wordCount": 535,
          "title": "Hacking VMAF and VMAF NEG: metrics vulnerability to different preprocessing. (arXiv:2107.04510v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet. This is fully probabilistic,\nauto-regressive, and causal, i.e. each sample generated depends only on the\npreviously observed samples. Our approach outperforms a widely used wavenet\narchitecture by up to 9% on a similar dataset for predicting the next step.\nUsing the attention mechanism, we enable the architecture to learn which audio\nsamples are important for the prediction of the future sample. We show how\ncausal transformer generative models can be used for raw waveform synthesis. We\nalso show that this performance can be improved by another 2% by conditioning\nsamples over a wider context. The flexibility of the current model to\nsynthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-09T01:58:24.045Z",
          "wordCount": 646,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v3 [cs.SD] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2009.03871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foti_S/0/1/0/all/0/1\">Simone Foti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1\">Bongjin Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowrick_T/0/1/0/all/0/1\">Thomas Dowrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalhinho_J/0/1/0/all/0/1\">Joao Ramalhinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allam_M/0/1/0/all/0/1\">Moustafa Allam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1\">Brian Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>",
          "description": "In this work we propose a method based on geometric deep learning to predict\nthe complete surface of the liver, given a partial point cloud of the organ\nobtained during the surgical laparoscopic procedure. We introduce a new data\naugmentation technique that randomly perturbs shapes in their frequency domain\nto compensate the limited size of our dataset. The core of our method is a\nvariational autoencoder (VAE) that is trained to learn a latent space for\ncomplete shapes of the liver. At inference time, the generative part of the\nmodel is embedded in an optimisation procedure where the latent representation\nis iteratively updated to generate a model that matches the intraoperative\npartial point cloud. The effect of this optimisation is a progressive non-rigid\ndeformation of the initially generated shape. Our method is qualitatively\nevaluated on real data and quantitatively evaluated on synthetic data. We\ncompared with a state-of-the-art rigid registration algorithm, that our method\noutperformed in visible areas.",
          "link": "http://arxiv.org/abs/2009.03871",
          "publishedOn": "2021-07-14T01:41:50.599Z",
          "wordCount": 640,
          "title": "Intraoperative Liver Surface Completion with Graph Convolutional VAE. (arXiv:2009.03871v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1\">Kemal Oksuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cam_B/0/1/0/all/0/1\">Baris Can Cam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1\">Sinan Kalkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>",
          "description": "Despite being widely used as a performance measure for visual detection\ntasks, Average Precision (AP) is limited in reflecting localisation quality,\n(ii) interpretability and (iii) robustness to the design choices regarding its\ncomputation, and its applicability to outputs without confidence scores.\nPanoptic Quality (PQ), a measure proposed for evaluating panoptic segmentation\n(Kirillov et al., 2019), does not suffer from these limitations but is limited\nto panoptic segmentation. In this paper, we propose Localisation Recall\nPrecision (LRP) Error as the performance measure for all visual detection\ntasks. LRP Error, initially proposed only for object detection by Oksuz et al.\n(2018), does not suffer from the aforementioned limitations and is applicable\nto all visual detection tasks. We also introduce Optimal LRP (oLRP) Error as\nthe minimum LRP error obtained over confidence scores to evaluate visual\ndetectors and obtain optimal thresholds for deployment. We provide a detailed\ncomparative analysis of LRP with AP and PQ, and use nearly 100 state-of-the-art\nvisual detectors from seven visual detection tasks (i.e. object detection,\nkeypoint detection, instance segmentation, panoptic segmentation, visual\nrelationship detection, zero-shot detection and generalised zero-shot\ndetection) using ten datasets (i.e. different COCO variants, LVIS, Open Images,\nPascal, ILSVRC) to empirically show that LRP provides richer and more\ndiscriminative information than its counterparts. Code available at:\nhttps://github.com/kemaloksuz/LRP-Error",
          "link": "http://arxiv.org/abs/2011.10772",
          "publishedOn": "2021-07-14T01:41:50.468Z",
          "wordCount": 701,
          "title": "One Metric to Measure them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks. (arXiv:2011.10772v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chaoyou Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>",
          "description": "Combinatorial optimization (CO) has been a hot research topic because of its\ntheoretic and practical importance. As a classic CO problem, deep hashing aims\nto find an optimal code for each data from finite discrete possibilities, while\nthe discrete nature brings a big challenge to the optimization process.\nPrevious methods usually mitigate this challenge by binary approximation,\nsubstituting binary codes for real-values via activation functions or\nregularizations. However, such approximation leads to uncertainty between\nreal-values and binary ones, degrading retrieval performance. In this paper, we\npropose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly\nestimates the uncertainty during training and leverages the uncertainty\ninformation to guide the approximation process. Specifically, we model\nbit-level uncertainty via measuring the discrepancy between the output of a\nhashing network and that of a momentum-updated network. The discrepancy of each\nbit indicates the uncertainty of the hashing network to the approximate output\nof that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can\nbe regarded as image-level uncertainty. It embodies the uncertainty of the\nhashing network to the corresponding input image. The hashing bit and image\nwith higher uncertainty are paid more attention during optimization. To the\nbest of our knowledge, this is the first work to study the uncertainty in\nhashing bits. Extensive experiments are conducted on four datasets to verify\nthe superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a\nmillion-scale dataset Clothing1M. Our method achieves the best performance on\nall of the datasets and surpasses existing state-of-the-art methods by a large\nmargin.",
          "link": "http://arxiv.org/abs/2009.08012",
          "publishedOn": "2021-07-14T01:41:50.382Z",
          "wordCount": 725,
          "title": "Deep Momentum Uncertainty Hashing. (arXiv:2009.08012v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "It is well noted that coordinate based MLPs benefit greatly -- in terms of\npreserving high-frequency information -- through the encoding of coordinate\npositions as an array of Fourier features. Hitherto, the rationale for the\neffectiveness of these positional encodings has been solely studied through a\nFourier lens. In this paper, we strive to broaden this understanding by showing\nthat alternative non-Fourier embedding functions can indeed be used for\npositional encoding. Moreover, we show that their performance is entirely\ndetermined by a trade-off between the stable rank of the embedded matrix and\nthe distance preservation between embedded coordinates. We further establish\nthat the now ubiquitous Fourier feature mapping of position is a special case\nthat fulfills these conditions. Consequently, we present a more general theory\nto analyze positional encoding in terms of shifted basis functions. To this\nend, we develop the necessary theoretical formulae and empirically verify that\nour theoretical claims hold in practice. Codes available at\nhttps://github.com/osiriszjq/Rethinking-positional-encoding.",
          "link": "http://arxiv.org/abs/2107.02561",
          "publishedOn": "2021-07-14T01:41:50.364Z",
          "wordCount": 603,
          "title": "Rethinking Positional Encoding. (arXiv:2107.02561v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parashar_S/0/1/0/all/0/1\">Shaifali Parashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yuxuan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "A recent trend in Non-Rigid Structure-from-Motion (NRSfM) is to express\nlocal, differential constraints between pairs of images, from which the surface\nnormal at any point can be obtained by solving a system of polynomial\nequations. The systems of equations derived in previous work, however, are of\nhigh degree, having up to five real solutions, thus requiring a computationally\nexpensive strategy to select a unique solution. Furthermore, they suffer from\ndegeneracies that make the resulting estimates unreliable, without any\nmechanism to identify this situation.\n\nIn this paper, we show that, under widely applicable assumptions, we can\nderive a new system of equation in terms of the surface normals whose two\nsolutions can be obtained in closed-form and can easily be disambiguated\nlocally. Our formalism further allows us to assess how reliable the estimated\nlocal normals are and, hence, to discard them if they are not. Our experiments\nshow that our reconstructions, obtained from two or more views, are\nsignificantly more accurate than those of state-of-the-art methods, while also\nbeing faster.",
          "link": "http://arxiv.org/abs/2011.11567",
          "publishedOn": "2021-07-14T01:41:50.356Z",
          "wordCount": 630,
          "title": "A Closed-Form Solution to Local Non-Rigid Structure-from-Motion. (arXiv:2011.11567v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenlong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_J/0/1/0/all/0/1\">James M. Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_J/0/1/0/all/0/1\">Jeffrey Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bezdek_J/0/1/0/all/0/1\">James C. Bezdek</a>",
          "description": "Examining most streaming clustering algorithms leads to the understanding\nthat they are actually incremental classification models. They model existing\nand newly discovered structures via summary information that we call\nfootprints. Incoming data is normally assigned a crisp label (into one of the\nstructures) and that structure's footprint is incrementally updated. There is\nno reason that these assignments need to be crisp. In this paper, we propose a\nnew streaming classification algorithm that uses Neural Gas prototypes as\nfootprints and produces a possibilistic label vector (of typicalities) for each\nincoming vector. These typicalities are generated by a modified possibilistic\nk-nearest neighbor algorithm. The approach is tested on synthetic and real\nimage datasets. We compare our approach to three other streaming classifiers\nbased on the Adaptive Random Forest, Very Fast Decision Rules, and the\nDenStream algorithm with excellent results.",
          "link": "http://arxiv.org/abs/2010.00635",
          "publishedOn": "2021-07-14T01:41:50.317Z",
          "wordCount": 606,
          "title": "StreamSoNG: A Soft Streaming Classification Approach. (arXiv:2010.00635v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiquan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total\nof 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,\n120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to\nrealize the function of valuating image classification. In order to prove that\nthe methods of different periods in the field of image classification have\ndiscrepancies on GasHisSDB, we select a variety of classifiers for evaluation.\nSeven classical machine learning classifiers, three CNN classifiers and a novel\ntransformer-based classifier are selected for testing on image classification\ntasks. GasHisSDB is available at the\nURL:https://github.com/NEUhwm/GasHisSDB.git.",
          "link": "http://arxiv.org/abs/2106.02473",
          "publishedOn": "2021-07-14T01:41:50.302Z",
          "wordCount": 614,
          "title": "A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jizong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanfeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Caiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>",
          "description": "Despite their outstanding accuracy, semi-supervised segmentation methods\nbased on deep neural networks can still yield predictions that are considered\nanatomically impossible by clinicians, for instance, containing holes or\ndisconnected regions. To solve this problem, we present a Context-aware Virtual\nAdversarial Training (CaVAT) method for generating anatomically plausible\nsegmentation. Unlike approaches focusing solely on accuracy, our method also\nconsiders complex topological constraints like connectivity which cannot be\neasily modeled in a differentiable loss function. We use adversarial training\nto generate examples violating the constraints, so the network can learn to\navoid making such incorrect predictions on new examples, and employ the\nReinforce algorithm to handle non-differentiable segmentation constraints. The\nproposed method offers a generic and efficient way to add any constraint on top\nof any segmentation network. Experiments on two clinically-relevant datasets\nshow our method to produce segmentations that are both accurate and\nanatomically-plausible in terms of region connectivity.",
          "link": "http://arxiv.org/abs/2107.05532",
          "publishedOn": "2021-07-14T01:41:50.280Z",
          "wordCount": 610,
          "title": "Context-aware virtual adversarial training for anatomically-plausible segmentation. (arXiv:2107.05532v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Abdurrahim Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_R/0/1/0/all/0/1\">Rahmetullah Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goktay_F/0/1/0/all/0/1\">Fatih Goktay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencoglan_G/0/1/0/all/0/1\">Gulsum Gencoglan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demircali_A/0/1/0/all/0/1\">Ali Anil Demircali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilsizoglu_B/0/1/0/all/0/1\">Berk Dilsizoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uvet_H/0/1/0/all/0/1\">Huseyin Uvet</a>",
          "description": "Clinical dermatology, still relies heavily on manual introspection of fungi\nwithin a Potassium Hydroxide (KOH) solution using a brightfield microscope.\nHowever, this method takes a long time, is based on the experience of the\nclinician, and has a low accuracy. With the increase of neural network\napplications in the field of clinical microscopy it is now possible to automate\nsuch manual processes increasing both efficiency and accuracy. This study\npresents a deep neural network structure that enables the rapid solutions for\nthese problems and can perform automatic fungi detection in grayscale images\nwithout colorants. Microscopic images of 81 fungi and 235 ceratine were\ncollected. Then, smaller patches were extracted containing 2062 fungi and 2142\nceratine. In order to detect fungus and ceratine, two models were created one\nof which was a custom neural network and the other was based on the VGG16\narchitecture. The developed custom model had 99.84% accuracy, and an area under\nthe curve (AUC) value of 1.00, while the VGG16 model had 98.89% accuracy and an\nAUC value of 0.99. However, average accuracy and AUC value of clinicians is\n72.8% and 0.87 respectively. This deep learning model allows the development of\nan automated system that can detect fungi within microscopic images.",
          "link": "http://arxiv.org/abs/2106.16139",
          "publishedOn": "2021-07-14T01:41:50.272Z",
          "wordCount": 726,
          "title": "Automated Onychomycosis Detection Using Deep Neural Networks. (arXiv:2106.16139v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1\">Ramon Manzorro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1\">Joshua L. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Binh Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1\">Dev Yashpal Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1\">Eero P. Simoncelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteson_D/0/1/0/all/0/1\">David S. Matteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1\">Peter A. Crozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>",
          "description": "Denoising is a fundamental challenge in scientific imaging. Deep\nconvolutional neural networks (CNNs) provide the current state of the art in\ndenoising natural images, where they produce impressive results. However, their\npotential has barely been explored in the context of scientific imaging.\nDenoising CNNs are typically trained on real natural images artificially\ncorrupted with simulated noise. In contrast, in scientific applications,\nnoiseless ground-truth images are usually not available. To address this issue,\nwe propose a simulation-based denoising (SBD) framework, in which CNNs are\ntrained on simulated images. We test the framework on data obtained from\ntransmission electron microscopy (TEM), an imaging technique with widespread\napplications in material science, biology, and medicine. SBD outperforms\nexisting techniques by a wide margin on a simulated benchmark dataset, as well\nas on real data. Apart from the denoised images, SBD generates likelihood maps\nto visualize the agreement between the structure of the denoised image and the\nobserved data. Our results reveal shortcomings of state-of-the-art denoising\narchitectures, such as their small field-of-view: substantially increasing the\nfield-of-view of the CNNs allows them to exploit non-local periodic patterns in\nthe data, which is crucial at high noise levels. In addition, we analyze the\ngeneralization capability of SBD, demonstrating that the trained networks are\nrobust to variations of imaging parameters and of the underlying signal\nstructure. Finally, we release the first publicly available benchmark dataset\nof TEM images, containing 18,000 examples.",
          "link": "http://arxiv.org/abs/2010.12970",
          "publishedOn": "2021-07-14T01:41:50.265Z",
          "wordCount": 750,
          "title": "Deep Denoising For Scientific Discovery: A Case Study In Electron Microscopy. (arXiv:2010.12970v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Huijie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wentao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yandong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>",
          "description": "Accurate labeling is essential for supervised deep learning methods. In this\npaper, to accurately segment images of multiple overlapping cervical cells with\ndeep learning models, we propose an automatic label correction algorithm to\nimprove the edge positioning accuracy of overlapping cervical cells in manual\nlabeling. Our algorithm is designed based on gradient guidance, and can\nautomatically correct edge positions for overlapping cervical cells and\ndifferences among manual labeling with different annotators. Using the proposed\nalgorithm, we constructed an open cervical cell edge detection dataset (CCEDD)\nwith high labeling accuracy. The experiments on the dataset for training show\nthat our automatic label correction algorithm can improve the accuracy of\nmanual labels and further improve the positioning accuracy of overlapping cells\nwith deep learning models. We have released the dataset and code at\nhttps://github.com/nachifur/automatic-label-correction-CCEDD.",
          "link": "http://arxiv.org/abs/2010.01919",
          "publishedOn": "2021-07-14T01:41:50.257Z",
          "wordCount": 623,
          "title": "Automatic Label Correction for the Accurate Edge Detection of Overlapping Cervical Cells. (arXiv:2010.01919v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weixiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boom_B/0/1/0/all/0/1\">Bas Boom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ledoux_H/0/1/0/all/0/1\">Hugo Ledoux</a>",
          "description": "Recent developments in data acquisition technology allow us to collect 3D\ntexture meshes quickly. Those can help us understand and analyse the urban\nenvironment, and as a consequence are useful for several applications like\nspatial analysis and urban planning. Semantic segmentation of texture meshes\nthrough deep learning methods can enhance this understanding, but it requires a\nlot of labelled data. The contributions of this work are threefold: (1) a new\nbenchmark dataset of semantic urban meshes, (2) a novel semi-automatic\nannotation framework, and (3) an annotation tool for 3D meshes. In particular,\nour dataset covers about 4 km2 in Helsinki (Finland), with six classes, and we\nestimate that we save about 600 hours of labelling work using our annotation\nframework, which includes initial segmentation and interactive refinement. We\nalso compare the performance of several state-of-theart 3D semantic\nsegmentation methods on the new benchmark dataset. Other researchers can use\nour results to train their networks: the dataset is publicly available, and the\nannotation tool is released as open-source.",
          "link": "http://arxiv.org/abs/2103.00355",
          "publishedOn": "2021-07-14T01:41:50.250Z",
          "wordCount": 640,
          "title": "SUM: A Benchmark Dataset of Semantic Urban Meshes. (arXiv:2103.00355v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yidong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>",
          "description": "Machine learning systems generally assume that the training and testing\ndistributions are the same. To this end, a key requirement is to develop models\nthat can generalize to unseen distributions. Domain generalization (DG), i.e.,\nout-of-distribution generalization, has attracted increasing interests in\nrecent years. Domain generalization deals with a challenging setting where one\nor several different but related domain(s) are given, and the goal is to learn\na model that can generalize to an unseen test domain. Great progress has been\nmade in the area of domain generalization for years. This paper presents the\nfirst review of recent advances in this area. First, we provide a formal\ndefinition of domain generalization and discuss several related fields. We then\nthoroughly review the theories related to domain generalization and carefully\nanalyze the theory behind generalization. We categorize recent algorithms into\nthree classes: data manipulation, representation learning, and learning\nstrategy, and present several popular algorithms in detail for each category.\nThird, we introduce the commonly used datasets and applications. Finally, we\nsummarize existing literature and present some potential research topics for\nthe future.",
          "link": "http://arxiv.org/abs/2103.03097",
          "publishedOn": "2021-07-14T01:41:50.230Z",
          "wordCount": 690,
          "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fei-Fei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaitanya_K/0/1/0/all/0/1\">Krishna Chaitanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengda Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1\">Benedikt Wiestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>",
          "description": "Radiomic representations can quantify properties of regions of interest in\nmedical image data. Classically, they account for pre-defined statistics of\nshape, texture, and other low-level image features. Alternatively, deep\nlearning-based representations are derived from supervised learning but require\nexpensive annotations from experts and often suffer from overfitting and data\nimbalance issues. In this work, we address the challenge of learning\nrepresentations of 3D medical images for an effective quantification under data\nimbalance. We propose a \\emph{self-supervised} representation learning\nframework to learn high-level features of 3D volumes as a complement to\nexisting radiomics features. Specifically, we demonstrate how to learn image\nrepresentations in a self-supervised fashion using a 3D Siamese network. More\nimportantly, we deal with data imbalance by exploiting two unsupervised\nstrategies: a) sample re-weighting, and b) balancing the composition of\ntraining batches. When combining our learned self-supervised feature with\ntraditional radiomics, we show significant improvement in brain tumor\nclassification and lung cancer staging tasks covering MRI and CT imaging\nmodalities.",
          "link": "http://arxiv.org/abs/2103.04167",
          "publishedOn": "2021-07-14T01:41:50.223Z",
          "wordCount": 644,
          "title": "Imbalance-Aware Self-Supervised Learning for 3D Radiomic Representations. (arXiv:2103.04167v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1\">Maxim Rakhuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We introduce T-Basis, a novel concept for a compact representation of a set\nof tensors, each of an arbitrary shape, which is often seen in Neural Networks.\nEach of the tensors in the set is modeled using Tensor Rings, though the\nconcept applies to other Tensor Networks. Owing its name to the T-shape of\nnodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally\nshaped three-dimensional tensors, used to represent Tensor Ring nodes. Such\nrepresentation allows us to parameterize the tensor set with a small number of\nparameters (coefficients of the T-Basis tensors), scaling logarithmically with\neach tensor's size in the set and linearly with the dimensionality of T-Basis.\nWe evaluate the proposed approach on the task of neural network compression and\ndemonstrate that it reaches high compression rates at acceptable performance\ndrops. Finally, we analyze memory and operation requirements of the compressed\nnetworks and conclude that T-Basis networks are equally well suited for\ntraining and inference in resource-constrained environments and usage on the\nedge devices.",
          "link": "http://arxiv.org/abs/2007.06631",
          "publishedOn": "2021-07-14T01:41:50.214Z",
          "wordCount": 650,
          "title": "T-Basis: a Compact Representation for Neural Networks. (arXiv:2007.06631v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hesen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "Data augmentation is a commonly used approach to improving the generalization\nof deep learning models. Recent works show that learned data augmentation\npolicies can achieve better generalization than hand-crafted ones. However,\nmost of these works use unified augmentation policies for all samples in a\ndataset, which is observed not necessarily beneficial for all labels in\nmulti-label classification tasks, i.e., some policies may have negative impacts\non some labels while benefitting the others. To tackle this problem, we propose\na novel Label-Based AutoAugmentation (LB-Aug) method for multi-label scenarios,\nwhere augmentation policies are generated with respect to labels by an\naugmentation-policy network. The policies are learned via reinforcement\nlearning using policy gradient methods, providing a mapping from instance\nlabels to their optimal augmentation policies. Numerical experiments show that\nour LB-Aug outperforms previous state-of-the-art augmentation methods by large\nmargins in multiple benchmarks on image and video classification.",
          "link": "http://arxiv.org/abs/2107.05384",
          "publishedOn": "2021-07-14T01:41:50.206Z",
          "wordCount": 598,
          "title": "Fine-Grained AutoAugmentation for Multi-Label Classification. (arXiv:2107.05384v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00138",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grodecki_K/0/1/0/all/0/1\">Kajetan Grodecki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Killekar_A/0/1/0/all/0/1\">Aditya Killekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1\">Andrew Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cadet_S/0/1/0/all/0/1\">Sebastien Cadet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McElhinney_P/0/1/0/all/0/1\">Priscilla McElhinney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razipour_A/0/1/0/all/0/1\">Aryabod Razipour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Cato Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pressman_B/0/1/0/all/0/1\">Barry D. Pressman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Julien_P/0/1/0/all/0/1\">Peter Julien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simon_J/0/1/0/all/0/1\">Judit Simon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maurovich_Horvat_P/0/1/0/all/0/1\">Pal Maurovich-Horvat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaibazzi_N/0/1/0/all/0/1\">Nicola Gaibazzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thakur_U/0/1/0/all/0/1\">Udit Thakur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mancini_E/0/1/0/all/0/1\">Elisabetta Mancini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agalbato_C/0/1/0/all/0/1\">Cecilia Agalbato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munechika_J/0/1/0/all/0/1\">Jiro Munechika</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matsumoto_H/0/1/0/all/0/1\">Hidenari Matsumoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mene_R/0/1/0/all/0/1\">Roberto Men&#xe8;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parati_G/0/1/0/all/0/1\">Gianfranco Parati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cernigliaro_F/0/1/0/all/0/1\">Franco Cernigliaro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nerlekar_N/0/1/0/all/0/1\">Nitesh Nerlekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torlasco_C/0/1/0/all/0/1\">Camilla Torlasco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pontone_G/0/1/0/all/0/1\">Gianluca Pontone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_D/0/1/0/all/0/1\">Damini Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slomka_P/0/1/0/all/0/1\">Piotr J. Slomka</a>",
          "description": "Quantitative lung measures derived from computed tomography (CT) have been\ndemonstrated to improve prognostication in coronavirus disease (COVID-19)\npatients, but are not part of the clinical routine since required manual\nsegmentation of lung lesions is prohibitively time-consuming. We propose a new\nfully automated deep learning framework for rapid quantification and\ndifferentiation between lung lesions in COVID-19 pneumonia from both contrast\nand non-contrast CT images using convolutional Long Short-Term Memory\n(ConvLSTM) networks. Utilizing the expert annotations, model training was\nperformed 5 times with separate hold-out sets using 5-fold cross-validation to\nsegment ground-glass opacity and high opacity (including consolidation and\npleural effusion). The performance of the method was evaluated on CT data sets\nfrom 197 patients with positive reverse transcription polymerase chain reaction\ntest result for SARS-CoV-2. Strong agreement between expert manual and\nautomatic segmentation was obtained for lung lesions with a Dice score\ncoefficient of 0.876 $\\pm$ 0.005; excellent correlations of 0.978 and 0.981 for\nground-glass opacity and high opacity volumes. In the external validation set\nof 67 patients, there was dice score coefficient of 0.767 $\\pm$ 0.009 as well\nas excellent correlations of 0.989 and 0.996 for ground-glass opacity and high\nopacity volumes. Computations for a CT scan comprising 120 slices were\nperformed under 2 seconds on a personal computer equipped with NVIDIA Titan RTX\ngraphics processing unit. Therefore, our deep learning-based method allows\nrapid fully-automated quantitative measurement of pneumonia burden from CT and\nmay generate results with an accuracy similar to the expert readers.",
          "link": "http://arxiv.org/abs/2104.00138",
          "publishedOn": "2021-07-14T01:41:50.199Z",
          "wordCount": 824,
          "title": "Rapid quantification of COVID-19 pneumonia burden from computed tomography with convolutional LSTM networks. (arXiv:2104.00138v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.07936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bozkir_E/0/1/0/all/0/1\">Efe Bozkir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1\">Ali Burak &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1\">Mete Akg&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1\">Nico Pfeifer</a>",
          "description": "Eye tracking is handled as one of the key technologies for applications that\nassess and evaluate human attention, behavior, and biometrics, especially using\ngaze, pupillary, and blink behaviors. One of the challenges with regard to the\nsocial acceptance of eye tracking technology is however the preserving of\nsensitive and personal information. To tackle this challenge, we employ a\nprivacy-preserving framework based on randomized encoding to train a Support\nVector Regression model using synthetic eye images privately to estimate the\nhuman gaze. During the computation, none of the parties learn about the data or\nthe result that any other party has. Furthermore, the party that trains the\nmodel cannot reconstruct pupil, blinks or visual scanpath. The experimental\nresults show that our privacy-preserving framework is capable of working in\nreal-time, with the same accuracy as compared to non-private version and could\nbe extended to other eye tracking related problems.",
          "link": "http://arxiv.org/abs/1911.07936",
          "publishedOn": "2021-07-14T01:41:50.178Z",
          "wordCount": 688,
          "title": "Privacy Preserving Gaze Estimation using Synthetic Images via a Randomized Encoding Based Framework. (arXiv:1911.07936v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+QingsongYao/0/1/0/all/0/1\">QingsongYao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S.kevin Zhou</a>",
          "description": "Detecting anatomical landmarks in medical images plays an essential role in\nunderstanding the anatomy and planning automated processing. In recent years, a\nvariety of deep neural network methods have been developed to detect landmarks\nautomatically. However, all of those methods are unary in the sense that a\nhighly specialized network is trained for a single task say associated with a\nparticular anatomical region. In this work, for the first time, we investigate\nthe idea of ``You Only Learn Once (YOLO)'' and develop a universal anatomical\nlandmark detection model to realize multiple landmark detection tasks with\nend-to-end training based on mixed datasets. The model consists of a local\nnetwork and a global network: The local network is built upon the idea of\nuniversal U-Net to learn multi-domain local features and the global network is\na parallelly-duplicated sequential of dilated convolutions that extract global\nfeatures to further disambiguate the landmark locations. It is worth mentioning\nthat the new model design requires much fewer parameters than models with\nstandard convolutions to train. We evaluate our YOLO model on three X-ray\ndatasets of 1,588 images on the head, hand, and chest, collectively\ncontributing 62 landmarks. The experimental results show that our proposed\nuniversal model behaves largely better than any previous models trained on\nmultiple datasets. It even beats the performance of the model that is trained\nseparately for every single dataset.",
          "link": "http://arxiv.org/abs/2103.04657",
          "publishedOn": "2021-07-14T01:41:50.170Z",
          "wordCount": 698,
          "title": "You Only Learn Once: Universal Anatomical Landmark Detection. (arXiv:2103.04657v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiluan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philion_J/0/1/0/all/0/1\">Jonah Philion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>",
          "description": "A high-performing object detection system plays a crucial role in autonomous\ndriving (AD). The performance, typically evaluated in terms of mean Average\nPrecision, does not take into account orientation and distance of the actors in\nthe scene, which are important for the safe AD. It also ignores environmental\ncontext. Recently, Philion et al. proposed a neural planning metric (PKL),\nbased on the KL divergence of a planner's trajectory and the groundtruth route,\nto accommodate these requirements. In this paper, we use this neural planning\nmetric to score all submissions of the nuScenes detection challenge and analyze\nthe results. We find that while somewhat correlated with mAP, the PKL metric\nshows different behavior to increased traffic density, ego velocity, road\ncurvature and intersections. Finally, we propose ideas to extend the neural\nplanning metric.",
          "link": "http://arxiv.org/abs/2010.09350",
          "publishedOn": "2021-07-14T01:41:50.160Z",
          "wordCount": 630,
          "title": "The efficacy of Neural Planning Metrics: A meta-analysis of PKL on nuScenes. (arXiv:2010.09350v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06211",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zaifeng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_T/0/1/0/all/0/1\">Tsz Nam Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chau_L/0/1/0/all/0/1\">Lap-Pui Chau</a>",
          "description": "High Dynamic Range (HDR) imaging via multi-exposure fusion is an important\ntask for most modern imaging platforms. In spite of recent developments in both\nhardware and algorithm innovations, challenges remain over content association\nambiguities caused by saturation, motion, and various artifacts introduced\nduring multi-exposure fusion such as ghosting, noise, and blur. In this work,\nwe propose an Attention-guided Progressive Neural Texture Fusion (APNT-Fusion)\nHDR restoration model which aims to address these issues within one framework.\nAn efficient two-stream structure is proposed which separately focuses on\ntexture feature transfer over saturated regions and multi-exposure tonal and\ntexture feature fusion. A neural feature transfer mechanism is proposed which\nestablishes spatial correspondence between different exposures based on\nmulti-scale VGG features in the masked saturated HDR domain for discriminative\ncontextual clues over the ambiguous image areas. A progressive texture blending\nmodule is designed to blend the encoded two-stream features in a multi-scale\nand progressive manner. In addition, we introduce several novel attention\nmechanisms, i.e., the motion attention module detects and suppresses the\ncontent discrepancies among the reference images; the saturation attention\nmodule facilitates differentiating the misalignment caused by saturation from\nthose caused by motion; and the scale attention module ensures texture blending\nconsistency between different coder/decoder scales. We carry out comprehensive\nqualitative and quantitative evaluations and ablation studies, which validate\nthat these novel modules work coherently under the same framework and\noutperform state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.06211",
          "publishedOn": "2021-07-14T01:41:50.153Z",
          "wordCount": 686,
          "title": "Attention-Guided Progressive Neural Texture Fusion for High Dynamic Range Image Restoration. (arXiv:2107.06211v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fain_M/0/1/0/all/0/1\">Mikhail Fain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1\">Niall Twomey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponikar_A/0/1/0/all/0/1\">Andrey Ponikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_R/0/1/0/all/0/1\">Ryan Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>",
          "description": "We propose a novel non-parametric method for cross-modal recipe retrieval\nwhich is applied on top of precomputed image and text embeddings. By combining\nour method with standard approaches for building image and text encoders,\ntrained independently with a self-supervised classification objective, we\ncreate a baseline model which outperforms most existing methods on a\nchallenging image-to-recipe task. We also use our method for comparing image\nand text encoders trained using different modern approaches, thus addressing\nthe issues hindering the development of novel methods for cross-modal recipe\nretrieval. We demonstrate how to use the insights from model comparison and\nextend our baseline model with standard triplet loss that improves\nstate-of-the-art on the Recipe1M dataset by a large margin, while using only\nprecomputed features and with much less complexity than existing methods.\nFurther, our approach readily generalizes beyond recipe retrieval to other\nchallenging domains, achieving state-of-the-art performance on Politics and\nGoodNews cross-modal retrieval tasks.",
          "link": "http://arxiv.org/abs/1911.12763",
          "publishedOn": "2021-07-14T01:41:50.146Z",
          "wordCount": 623,
          "title": "Dividing and Conquering Cross-Modal Recipe Retrieval: from Nearest Neighbours Baselines to SoTA. (arXiv:1911.12763v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bowen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>",
          "description": "Modern approaches typically formulate semantic segmentation as a per-pixel\nclassification task, while instance-level segmentation is handled with an\nalternative mask classification. Our key insight: mask classification is\nsufficiently general to solve both semantic- and instance-level segmentation\ntasks in a unified manner using the exact same model, loss, and training\nprocedure. Following this observation, we propose MaskFormer, a simple mask\nclassification model which predicts a set of binary masks, each associated with\na single global class label prediction. Overall, the proposed mask\nclassification-based method simplifies the landscape of effective approaches to\nsemantic and panoptic segmentation tasks and shows excellent empirical results.\nIn particular, we observe that MaskFormer outperforms per-pixel classification\nbaselines when the number of classes is large. Our mask classification-based\nmethod outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K)\nand panoptic segmentation (52.7 PQ on COCO) models.",
          "link": "http://arxiv.org/abs/2107.06278",
          "publishedOn": "2021-07-14T01:41:50.128Z",
          "wordCount": 583,
          "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation. (arXiv:2107.06278v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pilato_A/0/1/0/all/0/1\">Antonio Di Pilato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taggio_N/0/1/0/all/0/1\">Nicol&#xf2; Taggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pompili_A/0/1/0/all/0/1\">Alexis Pompili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobellis_M/0/1/0/all/0/1\">Michele Iacobellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florio_A/0/1/0/all/0/1\">Adriano Di Florio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passarelli_D/0/1/0/all/0/1\">Davide Passarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samarelli_S/0/1/0/all/0/1\">Sergio Samarelli</a>",
          "description": "The interest for change detection in the field of remote sensing has\nincreased in the last few years. Searching for changes in satellite images has\nmany useful applications, ranging from land cover and land use analysis to\nanomaly detection. In particular, urban change detection provides an efficient\ntool to study urban spread and growth through several years of observation. At\nthe same time, change detection is often a computationally challenging and\ntime-consuming task, which requires innovative methods to guarantee optimal\nresults with unquestionable value and within reasonable time. In this paper we\npresent two different approaches to change detection (semantic segmentation and\nclassification) that both exploit convolutional neural networks to achieve good\nresults, which can be further refined and used in a post-processing workflow\nfor a large variety of applications.",
          "link": "http://arxiv.org/abs/2107.06132",
          "publishedOn": "2021-07-14T01:41:50.121Z",
          "wordCount": 578,
          "title": "Deep learning approaches to Earth Observation change detection. (arXiv:2107.06132v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piva_F/0/1/0/all/0/1\">Fabrizio J. Piva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1\">Gijs Dubbelman</a>",
          "description": "We introduce an unsupervised domain adaption (UDA) strategy that combines\nmultiple image translations, ensemble learning and self-supervised learning in\none coherent approach. We focus on one of the standard tasks of UDA in which a\nsemantic segmentation model is trained on labeled synthetic data together with\nunlabeled real-world data, aiming to perform well on the latter. To exploit the\nadvantage of using multiple image translations, we propose an ensemble learning\napproach, where three classifiers calculate their prediction by taking as input\nfeatures of different image translations, making each classifier learn\nindependently, with the purpose of combining their outputs by sparse\nMultinomial Logistic Regression. This regression layer known as meta-learner\nhelps to reduce the bias during pseudo label generation when performing\nself-supervised learning and improves the generalizability of the model by\ntaking into consideration the contribution of each classifier. We evaluate our\nmethod on the standard UDA benchmarks, i.e. adapting GTA V and Synthia to\nCityscapes, and achieve state-of-the-art results in the mean intersection over\nunion metric. Extensive ablation experiments are reported to highlight the\nadvantageous properties of our proposed UDA strategy.",
          "link": "http://arxiv.org/abs/2107.06235",
          "publishedOn": "2021-07-14T01:41:50.113Z",
          "wordCount": 632,
          "title": "Exploiting Image Translations via Ensemble Self-Supervised Learning for Unsupervised Domain Adaptation. (arXiv:2107.06235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Vision transformers have been successfully applied to image recognition tasks\ndue to their ability to capture long-range dependencies within an image.\nHowever, there are still gaps in both performance and computational cost\nbetween transformers and existing convolutional neural networks (CNNs). In this\npaper, we aim to address this issue and develop a network that can outperform\nnot only the canonical transformers, but also the high-performance\nconvolutional models. We propose a new transformer based hybrid network by\ntaking advantage of transformers to capture long-range dependencies, and of\nCNNs to model local features. Furthermore, we scale it to obtain a family of\nmodels, called CMTs, obtaining much better accuracy and efficiency than\nprevious convolution and transformer based models. In particular, our CMT-S\nachieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on\nFLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S\nalso generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%),\nand other challenging vision datasets such as COCO (44.3% mAP), with\nconsiderably less computational cost.",
          "link": "http://arxiv.org/abs/2107.06263",
          "publishedOn": "2021-07-14T01:41:50.107Z",
          "wordCount": 609,
          "title": "CMT: Convolutional Neural Networks Meet Vision Transformers. (arXiv:2107.06263v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoxin Liu</a>",
          "description": "Domain generalization (DG) aims to help models trained on a set of source\ndomains generalize better on unseen target domains. The performances of current\nDG methods largely rely on sufficient labeled data, which however are usually\ncostly or unavailable. While unlabeled data are far more accessible, we seek to\nexplore how unsupervised learning can help deep models generalizes across\ndomains. Specifically, we study a novel generalization problem called\nunsupervised domain generalization, which aims to learn generalizable models\nwith unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised\nLearning (DIUL) method to cope with the significant and misleading\nheterogeneity within unlabeled data and severe distribution shifts between\nsource and target data. Surprisingly we observe that DIUL can not only\ncounterbalance the scarcity of labeled data but also further strengthen the\ngeneralization ability of models when the labeled data are sufficient. As a\npretraining approach, DIUL shows superior to ImageNet pretraining protocol even\nwhen the available data are unlabeled and of a greatly smaller amount compared\nto ImageNet. Extensive experiments clearly demonstrate the effectiveness of our\nmethod compared with state-of-the-art unsupervised learning counterparts.",
          "link": "http://arxiv.org/abs/2107.06219",
          "publishedOn": "2021-07-14T01:41:50.100Z",
          "wordCount": 625,
          "title": "Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization. (arXiv:2107.06219v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.13751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "Recent works on plug-and-play image restoration have shown that a denoiser\ncan implicitly serve as the image prior for model-based methods to solve many\ninverse problems. Such a property induces considerable advantages for\nplug-and-play image restoration (e.g., integrating the flexibility of\nmodel-based method and effectiveness of learning-based methods) when the\ndenoiser is discriminatively learned via deep convolutional neural network\n(CNN) with large modeling capacity. However, while deeper and larger CNN models\nare rapidly gaining popularity, existing plug-and-play image restoration\nhinders its performance due to the lack of suitable denoiser prior. In order to\npush the limits of plug-and-play image restoration, we set up a benchmark deep\ndenoiser prior by training a highly flexible and effective CNN denoiser. We\nthen plug the deep denoiser prior as a modular part into a half quadratic\nsplitting based iterative algorithm to solve various image restoration\nproblems. We, meanwhile, provide a thorough analysis of parameter setting,\nintermediate results and empirical convergence to better understand the working\nmechanism. Experimental results on three representative image restoration\ntasks, including deblurring, super-resolution and demosaicing, demonstrate that\nthe proposed plug-and-play image restoration with deep denoiser prior not only\nsignificantly outperforms other state-of-the-art model-based methods but also\nachieves competitive or even superior performance against state-of-the-art\nlearning-based methods. The source code is available at\nhttps://github.com/cszn/DPIR.",
          "link": "http://arxiv.org/abs/2008.13751",
          "publishedOn": "2021-07-14T01:41:50.080Z",
          "wordCount": 692,
          "title": "Plug-and-Play Image Restoration with Deep Denoiser Prior. (arXiv:2008.13751v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shihua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Cheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Changxiao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "Despite the remarkable successes of Convolutional Neural Networks (CNNs) in\ncomputer vision, it is time-consuming and error-prone to manually design a CNN.\nAmong various Neural Architecture Search (NAS) methods that are motivated to\nautomate designs of high-performance CNNs, the differentiable NAS and\npopulation-based NAS are attracting increasing interests due to their unique\ncharacters. To benefit from the merits while overcoming the deficiencies of\nboth, this work proposes a novel NAS method, RelativeNAS. As the key to\nefficient search, RelativeNAS performs joint learning between fast-learners\n(i.e. networks with relatively higher accuracy) and slow-learners in a pairwise\nmanner. Moreover, since RelativeNAS only requires low-fidelity performance\nestimation to distinguish each pair of fast-learner and slow-learner, it saves\ncertain computation costs for training the candidate architectures. The\nproposed RelativeNAS brings several unique advantages: (1) it achieves\nstate-of-the-art performance on ImageNet with top-1 error rate of 24.88%, i.e.\noutperforming DARTS and AmoebaNet-B by 1.82% and 1.12% respectively; (2) it\nspends only nine hours with a single 1080Ti GPU to obtain the discovered cells,\ni.e. 3.75x and 7875x faster than DARTS and AmoebaNet respectively; (3) it\nprovides that the discovered cells obtained on CIFAR-10 can be directly\ntransferred to object detection, semantic segmentation, and keypoint detection,\nyielding competitive results of 73.1% mAP on PASCAL VOC, 78.7% mIoU on\nCityscapes, and 68.5% AP on MSCOCO, respectively. The implementation of\nRelativeNAS is available at https://github.com/EMI-Group/RelativeNAS",
          "link": "http://arxiv.org/abs/2009.06193",
          "publishedOn": "2021-07-14T01:41:50.073Z",
          "wordCount": 710,
          "title": "RelativeNAS: Relative Neural Architecture Search via Slow-Fast Learning. (arXiv:2009.06193v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Junbao Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Due to the domain discrepancy in visual domain adaptation, the performance of\nsource model degrades when bumping into the high data density near decision\nboundary in target domain. A common solution is to minimize the Shannon Entropy\nto push the decision boundary away from the high density area. However, entropy\nminimization also leads to severe reduction of prediction diversity, and\nunfortunately brings harm to the domain adaptation. In this paper, we\ninvestigate the prediction discriminability and diversity by studying the\nstructure of the classification output matrix of a randomly selected data\nbatch. We find by theoretical analysis that the prediction discriminability and\ndiversity could be separately measured by the Frobenius-norm and rank of the\nbatch output matrix. The nuclear-norm is an upperbound of the former, and a\nconvex approximation of the latter. Accordingly, we propose Batch Nuclear-norm\nMaximization and Minimization, which performs nuclear-norm maximization on the\ntarget output matrix to enhance the target prediction ability, and nuclear-norm\nminimization on the source batch output matrix to increase applicability of the\nsource domain knowledge. We further approximate the nuclear-norm by\nL_{1,2}-norm, and design multi-batch optimization for stable solution on large\nnumber of categories. The fast approximation method achieves O(n^2)\ncomputational complexity and better convergence property. Experiments show that\nour method could boost the adaptation accuracy and robustness under three\ntypical domain adaptation scenarios. The code is available at\nhttps://github.com/cuishuhao/BNM.",
          "link": "http://arxiv.org/abs/2107.06154",
          "publishedOn": "2021-07-14T01:41:50.066Z",
          "wordCount": 683,
          "title": "Fast Batch Nuclear-norm Maximization and Minimization for Robust Domain Adaptation. (arXiv:2107.06154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Mai Lan Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1\">Volker Blanz</a>",
          "description": "We propose a simple modification from a fixed margin triplet loss to an\nadaptive margin triplet loss. While the original triplet loss is used widely in\nclassification problems such as face recognition, face re-identification and\nfine-grained similarity, our proposed loss is well suited for rating datasets\nin which the ratings are continuous values. In contrast to original triplet\nloss where we have to sample data carefully, in out method, we can generate\ntriplets using the whole dataset, and the optimization can still converge\nwithout frequently running into a model collapsing issue. The adaptive margins\nonly need to be computed once before the training, which is much less expensive\nthan generating triplets after every epoch as in the fixed margin case. Besides\nsubstantially improved training stability (the proposed model never collapsed\nin our experiments compared to a couple of times that the training collapsed on\nexisting triplet loss), we achieved slightly better performance than the\noriginal triplet loss on various rating datasets and network architectures.",
          "link": "http://arxiv.org/abs/2107.06187",
          "publishedOn": "2021-07-14T01:41:50.059Z",
          "wordCount": 599,
          "title": "Deep Ranking with Adaptive Margin Triplet Loss. (arXiv:2107.06187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1\">Syed Muhammad Arsalan Bashir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mahrukh Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yilong Niu</a>",
          "description": "Image super-resolution (SR) is one of the vital image processing methods that\nimprove the resolution of an image in the field of computer vision. In the last\ntwo decades, significant progress has been made in the field of\nsuper-resolution, especially by utilizing deep learning methods. This survey is\nan effort to provide a detailed survey of recent progress in single-image\nsuper-resolution in the perspective of deep learning while also informing about\nthe initial classical methods used for image super-resolution. The survey\nclassifies the image SR methods into four categories, i.e., classical methods,\nsupervised learning-based methods, unsupervised learning-based methods, and\ndomain-specific SR methods. We also introduce the problem of SR to provide\nintuition about image quality metrics, available reference datasets, and SR\nchallenges. Deep learning-based approaches of SR are evaluated using a\nreference dataset. Some of the reviewed state-of-the-art image SR methods\ninclude the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN),\nmultiscale residual network (MSRN), meta residual dense network (Meta-RDN),\nrecurrent back-projection network (RBPN), second-order attention network (SAN),\nSR feedback network (SRFBN) and the wavelet-based residual attention network\n(WRAN). Finally, this survey is concluded with future directions and trends in\nSR and open problems in SR to be addressed by the researchers.",
          "link": "http://arxiv.org/abs/2102.09351",
          "publishedOn": "2021-07-14T01:41:50.052Z",
          "wordCount": 699,
          "title": "A Comprehensive Review of Deep Learning-based Single Image Super-resolution. (arXiv:2102.09351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06179",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mojrian_S/0/1/0/all/0/1\">Sanaz Mojrian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nodehi_I/0/1/0/all/0/1\">Issa Nodehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mashmool_A/0/1/0/all/0/1\">Amir Mashmool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zadegan_Z/0/1/0/all/0/1\">Zeynab Kiani Zadegan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirkharkolaie_S/0/1/0/all/0/1\">Sahar Khanjani Shirkharkolaie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tamadon_T/0/1/0/all/0/1\">Tahereh Tamadon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_S/0/1/0/all/0/1\">Samiyeh Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akbari_M/0/1/0/all/0/1\">Mitra Akbari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassannataj_E/0/1/0/all/0/1\">Edris Hassannataj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>",
          "description": "Myocardial infarction disease (MID) is caused to the rapid progress of\nundiagnosed coronary artery disease (CAD) that indicates the injury of a heart\ncell by decreasing the blood flow to the cardiac muscles. MID is the leading\ncause of death in middle-aged and elderly subjects all over the world. In\ngeneral, raw Electrocardiogram (ECG) signals are tested for MID identification\nby clinicians that is exhausting, time-consuming, and expensive. Artificial\nintelligence-based methods are proposed to handle the problems to diagnose MID\non the ECG signals automatically. Hence, in this survey paper, artificial\nintelligence-based methods, including machine learning and deep learning, are\nreview for MID diagnosis on the ECG signals. Using the methods demonstrate that\nthe feature extraction and selection of ECG signals required to be handcrafted\nin the ML methods. In contrast, these tasks are explored automatically in the\nDL methods. Based on our best knowledge, Deep Convolutional Neural Network\n(DCNN) methods are highly required methods developed for the early diagnosis of\nMID on the ECG signals. Most researchers have tended to use DCNN methods, and\nno studies have surveyed using artificial intelligence methods for MID\ndiagnosis on the ECG signals.",
          "link": "http://arxiv.org/abs/2107.06179",
          "publishedOn": "2021-07-14T01:41:50.033Z",
          "wordCount": 664,
          "title": "A Survey of Applications of Artificial Intelligence for Myocardial Infarction Disease Diagnosis. (arXiv:2107.06179v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>",
          "description": "We present MaX-DeepLab, the first end-to-end model for panoptic segmentation.\nOur approach simplifies the current pipeline that depends heavily on surrogate\nsub-tasks and hand-designed components, such as box detection, non-maximum\nsuppression, thing-stuff merging, etc. Although these sub-tasks are tackled by\narea experts, they fail to comprehensively solve the target task. By contrast,\nour MaX-DeepLab directly predicts class-labeled masks with a mask transformer,\nand is trained with a panoptic quality inspired loss via bipartite matching.\nOur mask transformer employs a dual-path architecture that introduces a global\nmemory path in addition to a CNN path, allowing direct communication with any\nCNN layers. As a result, MaX-DeepLab shows a significant 7.1% PQ gain in the\nbox-free regime on the challenging COCO dataset, closing the gap between\nbox-based and box-free methods for the first time. A small variant of\nMaX-DeepLab improves 3.0% PQ over DETR with similar parameters and M-Adds.\nFurthermore, MaX-DeepLab, without test time augmentation, achieves new\nstate-of-the-art 51.3% PQ on COCO test-dev set. Code is available at\nhttps://github.com/google-research/deeplab2.",
          "link": "http://arxiv.org/abs/2012.00759",
          "publishedOn": "2021-07-14T01:41:50.026Z",
          "wordCount": 647,
          "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. (arXiv:2012.00759v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05990",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolf_T/0/1/0/all/0/1\">Tom Nuno Wolf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>",
          "description": "Prior work on diagnosing Alzheimer's disease from magnetic resonance images\nof the brain established that convolutional neural networks (CNNs) can leverage\nthe high-dimensional image information for classifying patients. However,\nlittle research focused on how these models can utilize the usually\nlow-dimensional tabular information, such as patient demographics or laboratory\nmeasurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a\ngeneral-purpose module for CNNs that dynamically rescales and shifts the\nfeature maps of a convolutional layer, conditional on a patient's tabular\nclinical information. We show that DAFT is highly effective in combining 3D\nimage and tabular information for diagnosis and time-to-dementia prediction,\nwhere it outperforms competing CNNs with a mean balanced accuracy of 0.622 and\nmean c-index of 0.748, respectively. Our extensive ablation study provides\nvaluable insights into the architectural properties of DAFT. Our implementation\nis available at https://github.com/ai-med/DAFT.",
          "link": "http://arxiv.org/abs/2107.05990",
          "publishedOn": "2021-07-14T01:41:50.018Z",
          "wordCount": 614,
          "title": "Combining 3D Image and Tabular Data via the Dynamic Affine Feature Map Transform. (arXiv:2107.05990v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1\">Pierre Marza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1\">Laetitia Matignon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1\">Olivier Simonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>",
          "description": "In the context of visual navigation, the capacity to map a novel environment\nis necessary for an agent to exploit its observation history in the considered\nplace and efficiently reach known goals. This ability can be associated with\nspatial reasoning, where an agent is able to perceive spatial relationships and\nregularities, and discover object affordances. In classical Reinforcement\nLearning (RL) setups, this capacity is learned from reward alone. We introduce\nsupplementary supervision in the form of auxiliary tasks designed to favor the\nemergence of spatial perception capabilities in agents trained for a\ngoal-reaching downstream objective. We show that learning to estimate metrics\nquantifying the spatial relationships between an agent at a given location and\na goal to reach has a high positive impact in Multi-Object Navigation settings.\nOur method significantly improves the performance of different baseline agents,\nthat either build an explicit or implicit representation of the environment,\neven matching the performance of incomparable oracle agents taking ground-truth\nmaps as input.",
          "link": "http://arxiv.org/abs/2107.06011",
          "publishedOn": "2021-07-14T01:41:50.011Z",
          "wordCount": 608,
          "title": "Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jinqing Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>",
          "description": "Recently, with the advance of deep Convolutional Neural Networks (CNNs),\nperson Re-Identification (Re-ID) has witnessed great success in various\napplications. However, with limited receptive fields of CNNs, it is still\nchallenging to extract discriminative representations in a global view for\npersons under non-overlapped cameras. Meanwhile, Transformers demonstrate\nstrong abilities of modeling long-range dependencies for spatial and sequential\ndata. In this work, we take advantages of both CNNs and Transformers, and\npropose a novel learning framework named Hierarchical Aggregation Transformer\n(HAT) for image-based person Re-ID with high performance. To achieve this goal,\nwe first propose a Deeply Supervised Aggregation (DSA) to recurrently aggregate\nhierarchical features from CNN backbones. With multi-granularity supervisions,\nthe DSA can enhance multi-scale features for person retrieval, which is very\ndifferent from previous methods. Then, we introduce a Transformer-based Feature\nCalibration (TFC) to integrate low-level detail information as the global prior\nfor high-level semantic information. The proposed TFC is inserted to each level\nof hierarchical features, resulting in great performance improvements. To our\nbest knowledge, this work is the first to take advantages of both CNNs and\nTransformers for image-based person Re-ID. Comprehensive experiments on four\nlarge-scale Re-ID benchmarks demonstrate that our method shows better results\nthan several state-of-the-art methods. The code is released at\nhttps://github.com/AI-Zhpp/HAT.",
          "link": "http://arxiv.org/abs/2107.05946",
          "publishedOn": "2021-07-14T01:41:50.004Z",
          "wordCount": 685,
          "title": "HAT: Hierarchical Aggregation Transformers for Person Re-identification. (arXiv:2107.05946v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1\">Tao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhouhui Lian</a>",
          "description": "Arbitrary-shaped text detection has recently attracted increasing interests\nand witnessed rapid development with the popularity of deep learning\nalgorithms. Nevertheless, existing approaches often obtain inaccurate detection\nresults, mainly due to the relatively weak ability to utilize context\ninformation and the inappropriate choice of offset references. This paper\npresents a novel text instance expression which integrates both foreground and\nbackground information into the pipeline, and naturally uses the pixels near\ntext boundaries as the offset starts. Besides, a corresponding post-processing\nalgorithm is also designed to sequentially combine the four prediction results\nand reconstruct the text instance accurately. We evaluate our method on several\nchallenging scene text benchmarks, including both curved and multi-oriented\ntext datasets. Experimental results demonstrate that the proposed approach\nobtains superior or competitive performance compared to other state-of-the-art\nmethods, e.g., 83.4% F-score for Total-Text, 82.4% F-score for MSRA-TD500, etc.",
          "link": "http://arxiv.org/abs/2107.06129",
          "publishedOn": "2021-07-14T01:41:49.985Z",
          "wordCount": 575,
          "title": "Bidirectional Regression for Arbitrary-Shaped Text Detection. (arXiv:2107.06129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goswami_S/0/1/0/all/0/1\">Suranjan Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member_I/0/1/0/all/0/1\">IEEE Student Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member_S/0/1/0/all/0/1\">Senior Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_B/0/1/0/all/0/1\">Bidyut B. Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fellow_L/0/1/0/all/0/1\">Life Fellow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>",
          "description": "Thermal Images profile the passive radiation of objects and capture them in\ngrayscale images. Such images have a very different distribution of data\ncompared to optical colored images. We present here a work that produces a\ngrayscale thermo-optical fused mask given a thermal input. This is a deep\nlearning based pioneering work since to the best of our knowledge, there exists\nno other work on thermal-optical grayscale fusion. Our method is also unique in\nthe sense that the deep learning method we are proposing here works on the\nDiscrete Wavelet Transform (DWT) domain instead of the gray level domain. As a\npart of this work, we also present a new and unique database for obtaining the\nregion of interest in thermal images based on an existing thermal visual paired\ndatabase, containing the Region of Interest on 5 different classes of data.\nFinally, we are proposing a simple low cost overhead statistical measure for\nidentifying the region of interest in the fused images, which we call as the\nRegion of Fusion (RoF). Experiments on the database show encouraging results in\nidentifying the region of interest in the fused images. We also show that they\ncan be processed better in the mixed form rather than with only thermal images.",
          "link": "http://arxiv.org/abs/2107.05942",
          "publishedOn": "2021-07-14T01:41:49.977Z",
          "wordCount": 658,
          "title": "A Novel Deep Learning Method for Thermal to Annotated Thermal-Optical Fused Images. (arXiv:2107.05942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wen-Sheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>",
          "description": "We present Retrieve in Style (RIS), an unsupervised framework for\nfine-grained facial feature transfer and retrieval on real images. Recent work\nshows that it is possible to learn a catalog that allows local semantic\ntransfers of facial features on generated images by capitalizing on the\ndisentanglement property of the StyleGAN latent space. RIS improves existing\nart on: 1) feature disentanglement and allows for challenging transfers (i.e.,\nhair and pose) that were not shown possible in SoTA methods. 2) eliminating the\nneed for per-image hyperparameter tuning, and for computing a catalog over a\nlarge batch of images. 3) enabling face retrieval using the proposed facial\nfeatures (e.g., eyes), and to our best knowledge, is the first work to retrieve\nface images at the fine-grained level. 4) robustness and natural application to\nreal images. Our qualitative and quantitative analyses show RIS achieves both\nhigh-fidelity feature transfers and accurate fine-grained retrievals on real\nimages. We discuss the responsible application of RIS.",
          "link": "http://arxiv.org/abs/2107.06256",
          "publishedOn": "2021-07-14T01:41:49.969Z",
          "wordCount": 607,
          "title": "Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval. (arXiv:2107.06256v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06276",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Suman_S/0/1/0/all/0/1\">Sudhir Suman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sakla_N/0/1/0/all/0/1\">Nicole Sakla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "With more than 60,000 deaths annually in the United States, Pulmonary\nEmbolism (PE) is among the most fatal cardiovascular diseases. It is caused by\nan artery blockage in the lung; confirming its presence is time-consuming and\nis prone to over-diagnosis. The utilization of automated PE detection systems\nis critical for diagnostic accuracy and efficiency. In this study we propose a\ntwo-stage attention-based CNN-LSTM network for predicting PE, its associated\ntype (chronic, acute) and corresponding location (leftsided, rightsided or\ncentral) on computed tomography (CT) examinations. We trained our model on the\nlargest available public Computed Tomography Pulmonary Angiogram PE dataset\n(RSNA-STR Pulmonary Embolism CT (RSPECT) Dataset, N=7279 CT studies) and tested\nit on an in-house curated dataset of N=106 studies. Our framework mirrors the\nradiologic diagnostic process via a multi-slice approach so that the accuracy\nand pathologic sequela of true pulmonary emboli may be meticulously assessed,\nenabling physicians to better appraise the morbidity of a PE when present. Our\nproposed method outperformed a baseline CNN classifier and a single-stage\nCNN-LSTM network, achieving an AUC of 0.95 on the test set for detecting the\npresence of PE in the study.",
          "link": "http://arxiv.org/abs/2107.06276",
          "publishedOn": "2021-07-14T01:41:49.962Z",
          "wordCount": 663,
          "title": "Attention based CNN-LSTM Network for Pulmonary Embolism Prediction on Chest Computed Tomography Pulmonary Angiograms. (arXiv:2107.06276v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qingyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargteil_A/0/1/0/all/0/1\">Adam Bargteil</a>",
          "description": "We explore computational approaches for visual guidance to aid in creating\naesthetically pleasing art and graphic design. Our work complements and builds\non previous work that developed models for how humans look at images. Our\napproach comprises three steps. First, we collected a dataset of art\nmasterpieces and labeled the visual fixations with state-of-art vision models.\nSecond, we clustered the visual guidance templates of the art masterpieces with\nunsupervised learning. Third, we developed a pipeline using generative\nadversarial networks to learn the principles of visual guidance and that can\nproduce aesthetically pleasing layouts. We show that the aesthetic visual\nguidance principles can be learned and integrated into a high-dimensional model\nand can be queried by the features of graphic elements. We evaluate our\napproach by generating layouts on various drawings and graphic designs.\nMoreover, our model considers the color and structure of graphic elements when\ngenerating layouts. Consequently, we believe our tool, which generates multiple\naesthetic layout options in seconds, can help artists create beautiful art and\ngraphic designs.",
          "link": "http://arxiv.org/abs/2107.06262",
          "publishedOn": "2021-07-14T01:41:49.954Z",
          "wordCount": 607,
          "title": "Learning Aesthetic Layouts via Visual Guidance. (arXiv:2107.06262v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1\">Shubham Dhayarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1\">V.K. Viekash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.",
          "link": "http://arxiv.org/abs/2107.06212",
          "publishedOn": "2021-07-14T01:41:49.934Z",
          "wordCount": 684,
          "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06028",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bauermeister_H/0/1/0/all/0/1\">Hartmut Bauermeister</a>, <a href=\"http://arxiv.org/find/math/1/au:+Laude_E/0/1/0/all/0/1\">Emanuel Laude</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mollenhoff_T/0/1/0/all/0/1\">Thomas M&#xf6;llenhoff</a>, <a href=\"http://arxiv.org/find/math/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>",
          "description": "Dual decomposition approaches in nonconvex optimization may suffer from a\nduality gap. This poses a challenge when applying them directly to nonconvex\nproblems such as MAP-inference in a Markov random field (MRF) with continuous\nstate spaces. To eliminate such gaps, this paper considers a reformulation of\nthe original nonconvex task in the space of measures. This infinite-dimensional\nreformulation is then approximated by a semi-infinite one, which is obtained\nvia a piecewise polynomial discretization in the dual. We provide a geometric\nintuition behind the primal problem induced by the dual discretization and draw\nconnections to optimization over moment spaces. In contrast to existing\ndiscretizations which suffer from a grid bias, we show that a piecewise\npolynomial discretization better preserves the continuous nature of our\nproblem. Invoking results from optimal transport theory and convex algebraic\ngeometry we reduce the semi-infinite program to a finite one and provide a\npractical implementation based on semidefinite programming. We show,\nexperimentally and in theory, that the approach successfully reduces the\nduality gap. To showcase the scalability of our approach, we apply it to the\nstereo matching problem between two images.",
          "link": "http://arxiv.org/abs/2107.06028",
          "publishedOn": "2021-07-14T01:41:49.925Z",
          "wordCount": 638,
          "title": "Lifting the Convex Conjugate in Lagrangian Relaxations: A Tractable Approach for Continuous Markov Random Fields. (arXiv:2107.06028v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sulzer_R/0/1/0/all/0/1\">Raphael Sulzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallet_B/0/1/0/all/0/1\">Bruno Vallet</a>",
          "description": "We introduce a novel learning-based, visibility-aware, surface reconstruction\nmethod for large-scale, defect-laden point clouds. Our approach can cope with\nthe scale and variety of point cloud defects encountered in real-life\nMulti-View Stereo (MVS) acquisitions. Our method relies on a 3D Delaunay\ntetrahedralization whose cells are classified as inside or outside the surface\nby a graph neural network and an energy model solvable with a graph cut. Our\nmodel, making use of both local geometric attributes and line-of-sight\nvisibility information, is able to learn a visibility model from a small amount\nof synthetic training data and generalizes to real-life acquisitions. Combining\nthe efficiency of deep learning methods and the scalability of energy based\nmodels, our approach outperforms both learning and non learning-based\nreconstruction algorithms on two publicly available reconstruction benchmarks.",
          "link": "http://arxiv.org/abs/2107.06130",
          "publishedOn": "2021-07-14T01:41:49.918Z",
          "wordCount": 593,
          "title": "Scalable Surface Reconstruction with Delaunay-Graph Neural Networks. (arXiv:2107.06130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05975",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gotkowski_K/0/1/0/all/0/1\">Karol Gotkowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bucher_A/0/1/0/all/0/1\">Andreas Bucher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischbach_R/0/1/0/all/0/1\">Ricarda Fischbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaltenborn_I/0/1/0/all/0/1\">Isabel Kaltenborn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Automatic segmentation of lung lesions in computer tomography has the\npotential to ease the burden of clinicians during the Covid-19 pandemic. Yet\npredictive deep learning models are not trusted in the clinical routine due to\nfailing silently in out-of-distribution (OOD) data. We propose a lightweight\nOOD detection method that exploits the Mahalanobis distance in the feature\nspace. The proposed approach can be seamlessly integrated into state-of-the-art\nsegmentation pipelines without requiring changes in model architecture or\ntraining procedure, and can therefore be used to assess the suitability of\npre-trained models to new data. We validate our method with a patch-based\nnnU-Net architecture trained with a multi-institutional dataset and find that\nit effectively detects samples that the model segments incorrectly.",
          "link": "http://arxiv.org/abs/2107.05975",
          "publishedOn": "2021-07-14T01:41:49.911Z",
          "wordCount": 617,
          "title": "Detecting when pre-trained nnU-Net models fail silently for Covid-19. (arXiv:2107.05975v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Daniel Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oort_C/0/1/0/all/0/1\">Colin Van Oort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1\">Jonathan Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wshah_S/0/1/0/all/0/1\">Safwan Wshah</a>",
          "description": "Geo-localizing static objects from street images is challenging but also very\nimportant for road asset mapping and autonomous driving. In this paper we\npresent a two-stage framework that detects and geolocalizes traffic signs from\nlow frame rate street videos. Our proposed system uses a modified version of\nRetinaNet (GPS-RetinaNet), which predicts a positional offset for each sign\nrelative to the camera, in addition to performing the standard classification\nand bounding box regression. Candidate sign detections from GPS-RetinaNet are\ncondensed into geolocalized signs by our custom tracker, which consists of a\nlearned metric network and a variant of the Hungarian Algorithm. Our metric\nnetwork estimates the similarity between pairs of detections, then the\nHungarian Algorithm matches detections across images using the similarity\nscores provided by the metric network. Our models were trained using an updated\nversion of the ARTS dataset, which contains 25,544 images and 47.589 sign\nannotations ~\\cite{arts}. The proposed dataset covers a diverse set of\nenvironments gathered from a broad selection of roads. Each annotaiton contains\na sign class label, its geospatial location, an assembly label, a side of road\nindicator, and unique identifiers that aid in the evaluation. This dataset will\nsupport future progress in the field, and the proposed system demonstrates how\nto take advantage of some of the unique characteristics of a realistic\ngeolocalization dataset.",
          "link": "http://arxiv.org/abs/2107.06257",
          "publishedOn": "2021-07-14T01:41:49.904Z",
          "wordCount": 677,
          "title": "Object Tracking and Geo-localization from Street Images. (arXiv:2107.06257v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigner_C/0/1/0/all/0/1\">Christina Aigner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>",
          "description": "Deep Neural Networks (DNNs) have an enormous potential to learn from complex\nbiomedical data. In particular, DNNs have been used to seamlessly fuse\nheterogeneous information from neuroanatomy, genetics, biomarkers, and\nneuropsychological tests for highly accurate Alzheimer's disease diagnosis. On\nthe other hand, their black-box nature is still a barrier for the adoption of\nsuch a system in the clinic, where interpretability is absolutely essential. We\npropose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for\nexplaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of\nthe neuroanatomy and tabular biomarkers. Our explanations are based on the\nShapley value, which is the unique method that satisfies all fundamental axioms\nfor local explanations previously established in the literature. Thus, SVEHNN\nhas many desirable characteristics that previous work on interpretability for\nmedical decision making is lacking. To avoid the exponential time complexity of\nthe Shapley value, we propose to transform a given DNN into a Lightweight\nProbabilistic Deep Network without re-training, thus achieving a complexity\nonly quadratic in the number of features. In our experiments on synthetic and\nreal data, we show that we can closely approximate the exact Shapley value with\na dramatically reduced runtime and can reveal the hidden knowledge the network\nhas learned from the data.",
          "link": "http://arxiv.org/abs/2107.05997",
          "publishedOn": "2021-07-14T01:41:49.897Z",
          "wordCount": 669,
          "title": "Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from Heterogeneous Data. (arXiv:2107.05997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06149",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haocheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiaxiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>",
          "description": "With the rapid development of data-driven techniques, data has played an\nessential role in various computer vision tasks. Many realistic and synthetic\ndatasets have been proposed to address different problems. However, there are\nlots of unresolved challenges: (1) the creation of dataset is usually a tedious\nprocess with manual annotations, (2) most datasets are only designed for a\nsingle specific task, (3) the modification or randomization of the 3D scene is\ndifficult, and (4) the release of commercial 3D data may encounter copyright\nissue.\n\nThis paper presents MINERVAS, a Massive INterior EnviRonments VirtuAl\nSynthesis system, to facilitate the 3D scene modification and the 2D image\nsynthesis for various vision tasks. In particular, we design a programmable\npipeline with Domain-Specific Language, allowing users to (1) select scenes\nfrom the commercial indoor scene database, (2) synthesize scenes for different\ntasks with customized rules, and (3) render various imagery data, such as\nvisual color, geometric structures, semantic label. Our system eases the\ndifficulty of customizing massive numbers of scenes for different tasks and\nrelieves users from manipulating fine-grained scene configurations by providing\nuser-controllable randomness using multi-level samplers. Most importantly, it\nempowers users to access commercial scene databases with millions of indoor\nscenes and protects the copyright of core data assets, e.g., 3D CAD models. We\ndemonstrate the validity and flexibility of our system by using our synthesized\ndata to improve the performance on different kinds of computer vision tasks.",
          "link": "http://arxiv.org/abs/2107.06149",
          "publishedOn": "2021-07-14T01:41:49.878Z",
          "wordCount": 686,
          "title": "MINERVAS: Massive INterior EnviRonments VirtuAl Synthesis. (arXiv:2107.06149v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lemkhenter_A/0/1/0/all/0/1\">Abdelhak Lemkhenter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1\">Adam Bielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_A/0/1/0/all/0/1\">Alp Eren Sari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "We introduce Kernel Density Discrimination GAN (KDD GAN), a novel method for\ngenerative adversarial learning. KDD GAN formulates the training as a\nlikelihood ratio optimization problem where the data distributions are written\nexplicitly via (local) Kernel Density Estimates (KDE). This is inspired by the\nrecent progress in contrastive learning and its relation to KDE. We define the\nKDEs directly in feature space and forgo the requirement of invertibility of\nthe kernel feature mappings. In our approach, features are no longer optimized\nfor linear separability, as in the original GAN formulation, but for the more\ngeneral discrimination of distributions in the feature space. We analyze the\ngradient of our loss with respect to the feature representation and show that\nit is better behaved than that of the original hinge loss. We perform\nexperiments with the proposed KDE-based loss, used either as a training loss or\na regularization term, on both CIFAR10 and scaled versions of ImageNet. We use\nBigGAN/SA-GAN as a backbone and baseline, since our focus is not to design the\narchitecture of the networks. We show a boost in the quality of generated\nsamples with respect to FID from 10% to 40% compared to the baseline. Code will\nbe made available.",
          "link": "http://arxiv.org/abs/2107.06197",
          "publishedOn": "2021-07-14T01:41:49.871Z",
          "wordCount": 638,
          "title": "Generative Adversarial Learning via Kernel Density Discrimination. (arXiv:2107.06197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandt_I/0/1/0/all/0/1\">Irma van den Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fok_F/0/1/0/all/0/1\">Floris Fok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulders_B/0/1/0/all/0/1\">Bas Mulders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1\">Joaquin Vanschoren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>",
          "description": "Transfer learning is a commonly used strategy for medical image\nclassification, especially via pretraining on source data and fine-tuning on\ntarget data. There is currently no consensus on how to choose appropriate\nsource data, and in the literature we can find both evidence of favoring large\nnatural image datasets such as ImageNet, and evidence of favoring more\nspecialized medical datasets. In this paper we perform a systematic study with\nnine source datasets with natural or medical images, and three target medical\ndatasets, all with 2D images. We find that ImageNet is the source leading to\nthe highest performances, but also that larger datasets are not necessarily\nbetter. We also study different definitions of data similarity. We show that\ncommon intuitions about similarity may be inaccurate, and therefore not\nsufficient to predict an appropriate source a priori. Finally, we discuss\nseveral steps needed for further research in this field, especially with regard\nto other types (for example 3D) medical images. Our experiments and pretrained\nmodels are available via \\url{https://www.github.com/vcheplygina/cats-scans}",
          "link": "http://arxiv.org/abs/2107.05940",
          "publishedOn": "2021-07-14T01:41:49.864Z",
          "wordCount": 627,
          "title": "Cats, not CAT scans: a study of dataset similarity in transfer learning for 2D medical image classification. (arXiv:2107.05940v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lanqing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>",
          "description": "Low-light image enhancement (LLIE) is a pervasive yet challenging problem,\nsince: 1) low-light measurements may vary due to different imaging conditions\nin practice; 2) images can be enlightened subjectively according to diverse\npreferences by each individual. To tackle these two challenges, this paper\npresents a novel deep reinforcement learning based method, dubbed ReLLIE, for\ncustomized low-light enhancement. ReLLIE models LLIE as a markov decision\nprocess, i.e., estimating the pixel-wise image-specific curves sequentially and\nrecurrently. Given the reward computed from a set of carefully crafted\nnon-reference loss functions, a lightweight network is proposed to estimate the\ncurves for enlightening of a low-light image input. As ReLLIE learns a policy\ninstead of one-one image translation, it can handle various low-light\nmeasurements and provide customized enhanced outputs by flexibly applying the\npolicy different times. Furthermore, ReLLIE can enhance real-world images with\nhybrid corruptions, e.g., noise, by using a plug-and-play denoiser easily.\nExtensive experiments on various benchmarks demonstrate the advantages of\nReLLIE, comparing to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.05830",
          "publishedOn": "2021-07-14T01:41:49.857Z",
          "wordCount": 607,
          "title": "ReLLIE: Deep Reinforcement Learning for Customized Low-Light Image Enhancement. (arXiv:2107.05830v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of obese human mesh recovery, i.e., fitting a\nparametric human mesh to images of obese people. Despite obese person mesh\nfitting being an important problem with numerous applications (e.g.,\nhealthcare), much recent progress in mesh recovery has been restricted to\nimages of non-obese people. In this work, we identify this crucial gap in the\ncurrent literature by presenting and discussing limitations of existing\nalgorithms. Next, we present a simple baseline to address this problem that is\nscalable and can be easily used in conjunction with existing algorithms to\nimprove their performance. Finally, we present a generalized human mesh\noptimization algorithm that substantially improves the performance of existing\nmethods on both obese person images as well as community-standard benchmark\ndatasets. A key innovation of this technique is that it does not rely on\nsupervision from expensive-to-create mesh parameters. Instead, starting from\nwidely and cheaply available 2D keypoints annotations, our method automatically\ngenerates mesh parameters that can in turn be used to re-train and fine-tune\nany existing mesh estimation algorithm. This way, we show our method acts as a\ndrop-in to improve the performance of a wide variety of contemporary mesh\nestimation methods. We conduct extensive experiments on multiple datasets\ncomprising both standard and obese person images and demonstrate the efficacy\nof our proposed techniques.",
          "link": "http://arxiv.org/abs/2107.06239",
          "publishedOn": "2021-07-14T01:41:49.839Z",
          "wordCount": 677,
          "title": "Everybody Is Unique: Towards Unbiased Human Mesh Recovery. (arXiv:2107.06239v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Mai Lan Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1\">Volker Blanz</a>",
          "description": "Discriminative features play an important role in image and object\nclassification and also in other fields of research such as semi-supervised\nlearning, fine-grained classification, out of distribution detection. Inspired\nby Linear Discriminant Analysis (LDA), we propose an optimization called Neural\nDiscriminant Analysis (NDA) for Deep Convolutional Neural Networks (DCNNs). NDA\ntransforms deep features to become more discriminative and, therefore, improves\nthe performances in various tasks. Our proposed optimization has two primary\ngoals for inter- and intra-class variances. The first one is to minimize\nvariances within each individual class. The second goal is to maximize pairwise\ndistances between features coming from different classes. We evaluate our NDA\noptimization in different research fields: general supervised classification,\nfine-grained classification, semi-supervised learning, and out of distribution\ndetection. We achieve performance improvements in all the fields compared to\nbaseline methods that do not use NDA. Besides, using NDA, we also surpass the\nstate of the art on the four tasks on various testing datasets.",
          "link": "http://arxiv.org/abs/2107.06209",
          "publishedOn": "2021-07-14T01:41:49.831Z",
          "wordCount": 602,
          "title": "Learning a Discriminant Latent Space with Neural Discriminant Analysis. (arXiv:2107.06209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matveev_A/0/1/0/all/0/1\">Albert Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "We present a pipeline for parametric wireframe extraction from densely\nsampled point clouds. Our approach processes a scalar distance field that\nrepresents proximity to the nearest sharp feature curve. In intermediate\nstages, it detects corners, constructs curve segmentation, and builds a\ntopological graph fitted to the wireframe. As an output, we produce parametric\nspline curves that can be edited and sampled arbitrarily. We evaluate our\nmethod on 50 complex 3D shapes and compare it to the novel deep learning-based\ntechnique, demonstrating superior quality.",
          "link": "http://arxiv.org/abs/2107.06165",
          "publishedOn": "2021-07-14T01:41:49.824Z",
          "wordCount": 521,
          "title": "3D Parametric Wireframe Extraction Based on Distance Fields. (arXiv:2107.06165v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1\">Tao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhouhui Lian</a>",
          "description": "Scene text detection remains a grand challenge due to the variation in text\ncurvatures, orientations, and aspect ratios. One of the most intractable\nproblems is how to represent text instances of arbitrary shapes. Although many\nstate-of-the-art methods have been proposed to model irregular texts in a\nflexible manner, most of them lose simplicity and robustness. Their complicated\npost-processings and the regression under Dirac delta distribution undermine\nthe detection performance and the generalization ability. In this paper, we\npropose an efficient text instance representation named CentripetalText (CT),\nwhich decomposes text instances into the combination of text kernels and\ncentripetal shifts. Specifically, we utilize the centripetal shifts to\nimplement the pixel aggregation, which guide the external text pixels to the\ninternal text kernels. The relaxation operation is integrated into the dense\nregression for centripetal shifts, allowing the correct prediction in a range,\nnot a specific value. The convenient reconstruction of the text contours and\nthe tolerance of the prediction errors in our method guarantee the high\ndetection accuracy and the fast inference speed respectively. Besides, we\nshrink our text detector into a proposal generation module, namely\nCentripetalText Proposal Network (CPN), replacing SPN in Mask TextSpotter v3\nand producing more accurate proposals. To validate the effectiveness of our\ndesigns, we conduct experiments on several commonly used scene text benchmarks,\nincluding both curved and multi-oriented text datasets. For the task of scene\ntext detection, our approach achieves superior or competitive performance\ncompared to other existing methods, e.g., F-measure of 86.3% at 40.0 FPS on\nTotal-Text, F-measure of 86.1% at 34.8 FPS on MSRA-TD500, etc. For the task of\nend-to-end scene text recognition, we outperform Mask TextSpotter v3 by 1.1% on\nTotal-Text.",
          "link": "http://arxiv.org/abs/2107.05945",
          "publishedOn": "2021-07-14T01:41:49.818Z",
          "wordCount": 714,
          "title": "CentripetalText: An Efficient Text Instance Representation for Scene Text Detection. (arXiv:2107.05945v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Filbrandt_G/0/1/0/all/0/1\">Gregory Filbrandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1\">Konstantinos Kamnitsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_D/0/1/0/all/0/1\">David Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Alexandra Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Scarcity of high quality annotated images remains a limiting factor for\ntraining accurate image segmentation models. While more and more annotated\ndatasets become publicly available, the number of samples in each individual\ndatabase is often small. Combining different databases to create larger amounts\nof training data is appealing yet challenging due to the heterogeneity as a\nresult of differences in data acquisition and annotation processes, often\nyielding incompatible or even conflicting information. In this paper, we\ninvestigate and propose several strategies for learning from partially\noverlapping labels in the context of abdominal organ segmentation. We find that\ncombining a semi-supervised approach with an adaptive cross entropy loss can\nsuccessfully exploit heterogeneously annotated data and substantially improve\nsegmentation accuracy compared to baseline and alternative approaches.",
          "link": "http://arxiv.org/abs/2107.05938",
          "publishedOn": "2021-07-14T01:41:49.811Z",
          "wordCount": 566,
          "title": "Learning from Partially Overlapping Labels: Image Segmentation under Annotation Shift. (arXiv:2107.05938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zudi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Donglai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkova_M/0/1/0/all/0/1\">Mariela D. Petkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuelong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zergham Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Krishna Swaroop K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Silin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_N/0/1/0/all/0/1\">Nils Wendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulanger_Weill_J/0/1/0/all/0/1\">Jonathan Boulanger-Weill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhanyasi_N/0/1/0/all/0/1\">Nagaraju Dhanyasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arganda_Carreras_I/0/1/0/all/0/1\">Ignacio Arganda-Carreras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engert_F/0/1/0/all/0/1\">Florian Engert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lichtman_J/0/1/0/all/0/1\">Jeff Lichtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "Segmenting 3D cell nuclei from microscopy image volumes is critical for\nbiological and clinical analysis, enabling the study of cellular expression\npatterns and cell lineages. However, current datasets for neuronal nuclei\nusually contain volumes smaller than $10^{\\text{-}3}\\ mm^3$ with fewer than 500\ninstances per volume, unable to reveal the complexity in large brain regions\nand restrict the investigation of neuronal structures. In this paper, we have\npushed the task forward to the sub-cubic millimeter scale and curated the NucMM\ndataset with two fully annotated volumes: one $0.1\\ mm^3$ electron microscopy\n(EM) volume containing nearly the entire zebrafish brain with around 170,000\nnuclei; and one $0.25\\ mm^3$ micro-CT (uCT) volume containing part of a mouse\nvisual cortex with about 7,000 nuclei. With two imaging modalities and\nsignificantly increased volume size and instance numbers, we discover a great\ndiversity of neuronal nuclei in appearance and density, introducing new\nchallenges to the field. We also perform a statistical analysis to illustrate\nthose challenges quantitatively. To tackle the challenges, we propose a novel\nhybrid-representation learning model that combines the merits of foreground\nmask, contour map, and signed distance transform to produce high-quality 3D\nmasks. The benchmark comparisons on the NucMM dataset show that our proposed\nmethod significantly outperforms state-of-the-art nuclei segmentation\napproaches. Code and data are available at\nhttps://connectomics-bazaar.github.io/proj/nucMM/index.html.",
          "link": "http://arxiv.org/abs/2107.05840",
          "publishedOn": "2021-07-14T01:41:49.804Z",
          "wordCount": 698,
          "title": "NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale. (arXiv:2107.05840v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fiege_E/0/1/0/all/0/1\">Eric Fiege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houta_S/0/1/0/all/0/1\">Salima Houta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisgin_P/0/1/0/all/0/1\">Pinar Bisgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surges_R/0/1/0/all/0/1\">Rainer Surges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howar_F/0/1/0/all/0/1\">Falk Howar</a>",
          "description": "Documentation of epileptic seizures plays an essential role in planning\nmedical therapy. Solutions for automated epileptic seizure detection can help\nimprove the current problem of incomplete and erroneous manual documentation of\nepileptic seizures. In recent years, a number of wearable sensors have been\ntested for this purpose. However, detecting seizures with subtle symptoms\nremains difficult and current solutions tend to have a high false alarm rate.\nSeizures can also affect the patient's arterial blood pressure, which has not\nyet been studied for detection with sensors. The pulse transit time (PTT)\nprovides a noninvasive estimate of arterial blood pressure. It can be obtained\nby using to two sensors, which are measuring the time differences between\narrivals of the pulse waves. Due to separated time chips a clock drift emerges,\nwhich is strongly influencing the PTT. In this work, we present an algorithm\nwhich responds to alterations in the PTT, considering the clock drift and\nenabling the noninvasive monitoring of blood pressure alterations using\nseparated sensors. Furthermore we investigated whether seizures can be detected\nusing the PTT. Our results indicate that using the algorithm, it is possible to\ndetect seizures with a Random Forest. Using the PTT along with other signals in\na multimodal approach, the detection of seizures with subtle symptoms could\nthereby be improved.",
          "link": "http://arxiv.org/abs/2107.05894",
          "publishedOn": "2021-07-14T01:41:49.785Z",
          "wordCount": 651,
          "title": "Automatic Seizure Detection Using the Pulse Transit Time. (arXiv:2107.05894v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sourya Dipta Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg A. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Saikat Dutta</a>",
          "description": "Deep image relighting allows photo enhancement by illumination-specific\nretouching without human effort and so it is getting much interest lately. Most\nof the existing popular methods available for relighting are run-time intensive\nand memory inefficient. Keeping these issues in mind, we propose the use of\nStacked Deep Multi-Scale Hierarchical Network, which aggregates features from\neach image at different scales. Our solution is differentiable and robust for\ntranslating image illumination setting from input image to target image.\nAdditionally, we have also shown that using a multi-step training approach to\nthis problem with two different loss functions can significantly boost\nperformance and can achieve a high quality reconstruction of a relighted image.",
          "link": "http://arxiv.org/abs/2107.06125",
          "publishedOn": "2021-07-14T01:41:49.778Z",
          "wordCount": 565,
          "title": "MSR-Net: Multi-Scale Relighting Network for One-to-One Relighting. (arXiv:2107.06125v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Webster_R/0/1/0/all/0/1\">Ryan Webster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabin_J/0/1/0/all/0/1\">Julien Rabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1\">Loic Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1\">Frederic Jurie</a>",
          "description": "Recently, generative adversarial networks (GANs) have achieved stunning\nrealism, fooling even human observers. Indeed, the popular tongue-in-cheek\nwebsite {\\small \\url{ this http URL}}, taunts users with\nGAN generated images that seem too real to believe. On the other hand, GANs do\nleak information about their training data, as evidenced by membership attacks\nrecently demonstrated in the literature. In this work, we challenge the\nassumption that GAN faces really are novel creations, by constructing a\nsuccessful membership attack of a new kind. Unlike previous works, our attack\ncan accurately discern samples sharing the same identity as training samples\nwithout being the same samples. We demonstrate the interest of our attack\nacross several popular face datasets and GAN training procedures. Notably, we\nshow that even in the presence of significant dataset diversity, an over\nrepresented person can pose a privacy concern.",
          "link": "http://arxiv.org/abs/2107.06018",
          "publishedOn": "2021-07-14T01:41:49.771Z",
          "wordCount": 584,
          "title": "This Person (Probably) Exists. Identity Membership Attacks Against GAN Generated Faces. (arXiv:2107.06018v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sumedha Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_S/0/1/0/all/0/1\">Stephen Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triantafillou_S/0/1/0/all/0/1\">Sofia Triantafillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Model explainability is essential for the creation of trustworthy Machine\nLearning models in healthcare. An ideal explanation resembles the\ndecision-making process of a domain expert and is expressed using concepts or\nterminology that is meaningful to the clinicians. To provide such an\nexplanation, we first associate the hidden units of the classifier to\nclinically relevant concepts. We take advantage of radiology reports\naccompanying the chest X-ray images to define concepts. We discover sparse\nassociations between concepts and hidden units using a linear sparse logistic\nregression. To ensure that the identified units truly influence the\nclassifier's outcome, we adopt tools from Causal Inference literature and, more\nspecifically, mediation analysis through counterfactual interventions. Finally,\nwe construct a low-depth decision tree to translate all the discovered concepts\ninto a straightforward decision rule, expressed to the radiologist. We\nevaluated our approach on a large chest x-ray dataset, where our model produces\na global explanation consistent with clinical knowledge.",
          "link": "http://arxiv.org/abs/2107.06098",
          "publishedOn": "2021-07-14T01:41:49.763Z",
          "wordCount": 595,
          "title": "Using Causal Analysis for Conceptual Deep Learning Explanation. (arXiv:2107.06098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiangbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_A/0/1/0/all/0/1\">An-Ti Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haro_A/0/1/0/all/0/1\">Antonio Haro</a>",
          "description": "Large-scale product recognition is one of the major applications of computer\nvision and machine learning in the e-commerce domain. Since the number of\nproducts is typically much larger than the number of categories of products,\nimage-based product recognition is often cast as a visual search rather than a\nclassification problem. It is also one of the instances of super fine-grained\nrecognition, where there are many products with slight or subtle visual\ndifferences. It has always been a challenge to create a benchmark dataset for\ntraining and evaluation on various visual search solutions in a real-world\nsetting. This motivated creation of eProduct, a dataset consisting of 2.5\nmillion product images towards accelerating development in the areas of\nself-supervised learning, weakly-supervised learning, and multimodal learning,\nfor fine-grained recognition. We present eProduct as a training set and an\nevaluation set, where the training set contains 1.3M+ listing images with\ntitles and hierarchical category labels, for model development, and the\nevaluation set includes 10,000 query and 1.1 million index images for visual\nsearch evaluation. We will present eProduct's construction steps, provide\nanalysis about its diversity and cover the performance of baseline models\ntrained on it.",
          "link": "http://arxiv.org/abs/2107.05856",
          "publishedOn": "2021-07-14T01:41:49.746Z",
          "wordCount": 648,
          "title": "eProduct: A Million-Scale Visual Search Benchmark to Address Product Recognition Challenges. (arXiv:2107.05856v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leng_G/0/1/0/all/0/1\">Guangjie Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yeku Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>",
          "description": "Empirical works suggest that various semantics emerge in the latent space of\nGenerative Adversarial Networks (GANs) when being trained to generate images.\nTo perform real image editing, it requires an accurate mapping from the real\nimage to the latent space to leveraging these learned semantics, which is\nimportant yet difficult. An in-domain GAN inversion approach is recently\nproposed to constraint the inverted code within the latent space by forcing the\nreconstructed image obtained from the inverted code within the real image\nspace. Empirically, we find that the inverted code by the in-domain GAN can\ndeviate from the latent space significantly. To solve this problem, we propose\na force-in-domain GAN based on the in-domain GAN, which utilizes a\ndiscriminator to force the inverted code within the latent space. The\nforce-in-domain GAN can also be interpreted by a cycle-GAN with slight\nmodification. Extensive experiments show that our force-in-domain GAN not only\nreconstructs the target image at the pixel level, but also align the inverted\ncode with the latent space well for semantic editing.",
          "link": "http://arxiv.org/abs/2107.06050",
          "publishedOn": "2021-07-14T01:41:49.733Z",
          "wordCount": 606,
          "title": "Force-in-domain GAN inversion. (arXiv:2107.06050v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets.",
          "link": "http://arxiv.org/abs/2107.05948",
          "publishedOn": "2021-07-14T01:41:49.726Z",
          "wordCount": 681,
          "title": "On Designing Good Representation Learning Models. (arXiv:2107.05948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1\">Aihua Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zihui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yaqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>",
          "description": "Point cloud upsampling aims to generate dense point clouds from given sparse\nones, which is a challenging task due to the irregular and unordered nature of\npoint sets. To address this issue, we present a novel deep learning-based\nmodel, called PU-Flow,which incorporates normalizing flows and feature\ninterpolation techniques to produce dense points uniformly distributed on the\nunderlying surface. Specifically, we formulate the upsampling process as point\ninterpolation in a latent space, where the interpolation weights are adaptively\nlearned from local geometric context, and exploit the invertible\ncharacteristics of normalizing flows to transform points between Euclidean and\nlatent spaces. We evaluate PU-Flow on a wide range of 3D models with sharp\nfeatures and high-frequency details. Qualitative and quantitative results show\nthat our method outperforms state-of-the-art deep learning-based approaches in\nterms of reconstruction quality, proximity-to-surface accuracy, and computation\nefficiency.",
          "link": "http://arxiv.org/abs/2107.05893",
          "publishedOn": "2021-07-14T01:41:49.718Z",
          "wordCount": 577,
          "title": "PU-Flow: a Point Cloud Upsampling Networkwith Normalizing Flows. (arXiv:2107.05893v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1\">Qirong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Ling Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenming Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1\">Xiuyan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaohua Huang</a>",
          "description": "Micro-expression recognition (\\textbf{MER}) has attracted lots of\nresearchers' attention in a decade. However, occlusion will occur for MER in\nreal-world scenarios. This paper deeply investigates an interesting but\nunexplored challenging issue in MER, \\ie, occlusion MER. First, to research MER\nunder real-world occlusion, synthetic occluded micro-expression databases are\ncreated by using various mask for the community. Second, to suppress the\ninfluence of occlusion, a \\underline{R}egion-inspired \\underline{R}elation\n\\underline{R}easoning \\underline{N}etwork (\\textbf{RRRN}) is proposed to model\nrelations between various facial regions. RRRN consists of a backbone network,\nthe Region-Inspired (\\textbf{RI}) module and Relation Reasoning (\\textbf{RR})\nmodule. More specifically, the backbone network aims at extracting feature\nrepresentations from different facial regions, RI module computing an adaptive\nweight from the region itself based on attention mechanism with respect to the\nunobstructedness and importance for suppressing the influence of occlusion, and\nRR module exploiting the progressive interactions among these regions by\nperforming graph convolutions. Experiments are conducted on handout-database\nevaluation and composite database evaluation tasks of MEGC 2018 protocol.\nExperimental results show that RRRN can significantly explore the importance of\nfacial regions and capture the cooperative complementary relationship of facial\nregions for MER. The results also demonstrate RRRN outperforms the\nstate-of-the-art approaches, especially on occlusion, and RRRN acts more robust\nto occlusion.",
          "link": "http://arxiv.org/abs/2107.05904",
          "publishedOn": "2021-07-14T01:41:49.705Z",
          "wordCount": 656,
          "title": "Region attention and graph embedding network for occlusion objective class-based micro-expression recognition. (arXiv:2107.05904v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>",
          "description": "We propose ST-DETR, a Spatio-Temporal Transformer-based architecture for\nobject detection from a sequence of temporal frames. We treat the temporal\nframes as sequences in both space and time and employ the full attention\nmechanisms to take advantage of the features correlations over both dimensions.\nThis treatment enables us to deal with frames sequence as temporal object\nfeatures traces over every location in the space. We explore two possible\napproaches; the early spatial features aggregation over the temporal dimension,\nand the late temporal aggregation of object query spatial features. Moreover,\nwe propose a novel Temporal Positional Embedding technique to encode the time\nsequence information. To evaluate our approach, we choose the Moving Object\nDetection (MOD)task, since it is a perfect candidate to showcase the importance\nof the temporal dimension. Results show a significant 5% mAP improvement on the\nKITTI MOD dataset over the 1-step spatial baseline.",
          "link": "http://arxiv.org/abs/2107.05887",
          "publishedOn": "2021-07-14T01:41:49.692Z",
          "wordCount": 584,
          "title": "ST-DETR: Spatio-Temporal Object Traces Attention Detection Transformer. (arXiv:2107.05887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>",
          "description": "Background: The deployment of various networks (e.g., Internet of Things\n(IoT) and mobile networks) and databases (e.g., nutrition tables and food\ncompositional databases) in the food system generates massive information silos\ndue to the well-known data harmonization problem. The food knowledge graph\nprovides a unified and standardized conceptual terminology and their\nrelationships in a structured form and thus can transform these information\nsilos across the whole food system to a more reusable globally digitally\nconnected Internet of Food, enabling every stage of the food system from\nfarm-to-fork.\n\nScope and approach: We review the evolution of food knowledge organization,\nfrom food classification, food ontology to food knowledge graphs. We then\ndiscuss the progress in food knowledge graphs from several representative\napplications. We finally discuss the main challenges and future directions.\n\nKey findings and conclusions: Our comprehensive summary of current research\non food knowledge graphs shows that food knowledge graphs play an important\nrole in food-oriented applications, including food search and Question\nAnswering (QA), personalized dietary recommendation, food analysis and\nvisualization, food traceability, and food machinery intelligent manufacturing.\nFuture directions for food knowledge graphs cover several fields such as\nmultimodal food knowledge graphs and food intelligence.",
          "link": "http://arxiv.org/abs/2107.05869",
          "publishedOn": "2021-07-14T01:41:49.683Z",
          "wordCount": 637,
          "title": "Towards Building a Food Knowledge Graph for Internet of Food. (arXiv:2107.05869v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kherchouche_A/0/1/0/all/0/1\">Anouar Kherchouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fezza_S/0/1/0/all/0/1\">Sid Ahmed Fezza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>",
          "description": "Despite the enormous performance of deepneural networks (DNNs), recent\nstudies have shown theirvulnerability to adversarial examples (AEs), i.e.,\ncare-fully perturbed inputs designed to fool the targetedDNN. Currently, the\nliterature is rich with many ef-fective attacks to craft such AEs. Meanwhile,\nmany de-fenses strategies have been developed to mitigate thisvulnerability.\nHowever, these latter showed their effec-tiveness against specific attacks and\ndoes not general-ize well to different attacks. In this paper, we proposea\nframework for defending DNN classifier against ad-versarial samples. The\nproposed method is based on atwo-stage framework involving a separate detector\nanda denoising block. The detector aims to detect AEs bycharacterizing them\nthrough the use of natural scenestatistic (NSS), where we demonstrate that\nthese statis-tical features are altered by the presence of\nadversarialperturbations. The denoiser is based on block matching3D (BM3D)\nfilter fed by an optimum threshold valueestimated by a convolutional neural\nnetwork (CNN) toproject back the samples detected as AEs into theirdata\nmanifold. We conducted a complete evaluation onthree standard datasets namely\nMNIST, CIFAR-10 andTiny-ImageNet. The experimental results show that\ntheproposed defense method outperforms the state-of-the-art defense techniques\nby improving the robustnessagainst a set of attacks under black-box, gray-box\nand white-box settings. The source code is available at:\nhttps://github.com/kherchouche-anouar/2DAE",
          "link": "http://arxiv.org/abs/2107.05780",
          "publishedOn": "2021-07-14T01:41:49.666Z",
          "wordCount": 665,
          "title": "Detect and Defense Against Adversarial Examples in Deep Learning using Natural Scene Statistics and Adaptive Denoising. (arXiv:2107.05780v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiabao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liangli Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1\">Cuizhu Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jupeng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "High-capacity image steganography, aimed at concealing a secret image in a\ncover image, is a technique to preserve sensitive data, e.g., faces and\nfingerprints. Previous methods focus on the security during transmission and\nsubsequently run a risk of privacy leakage after the restoration of secret\nimages at the receiving end. To address this issue, we propose a framework,\ncalled Multitask Identity-Aware Image Steganography (MIAIS), to achieve direct\nrecognition on container images without restoring secret images. The key issue\nof the direct recognition is to preserve identity information of secret images\ninto container images and make container images look similar to cover images at\nthe same time. Thus, we introduce a simple content loss to preserve the\nidentity information, and design a minimax optimization to deal with the\ncontradictory aspects. We demonstrate that the robustness results can be\ntransferred across different cover datasets. In order to be flexible for the\nsecret image restoration in some cases, we incorporate an optional restoration\nnetwork into our method, providing a multitask framework. The experiments under\nthe multitask scenario show the effectiveness of our framework compared with\nother visual information hiding methods and state-of-the-art high-capacity\nimage steganography methods.",
          "link": "http://arxiv.org/abs/2107.05819",
          "publishedOn": "2021-07-14T01:41:49.617Z",
          "wordCount": 640,
          "title": "Multitask Identity-Aware Image Steganography via Minimax Optimization. (arXiv:2107.05819v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Siyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>",
          "description": "Self-Attention has become prevalent in computer vision models. Inspired by\nfully connected Conditional Random Fields (CRFs), we decompose it into local\nand context terms. They correspond to the unary and binary terms in CRF and are\nimplemented by attention mechanisms with projection matrices. We observe that\nthe unary terms only make small contributions to the outputs, and meanwhile\nstandard CNNs that rely solely on the unary terms achieve great performances on\na variety of tasks. Therefore, we propose Locally Enhanced Self-Attention\n(LESA), which enhances the unary term by incorporating it with convolutions,\nand utilizes a fusion module to dynamically couple the unary and binary\noperations. In our experiments, we replace the self-attention modules with\nLESA. The results on ImageNet and COCO show the superiority of LESA over\nconvolution and self-attention baselines for the tasks of image recognition,\nobject detection, and instance segmentation. The code is made publicly\navailable.",
          "link": "http://arxiv.org/abs/2107.05637",
          "publishedOn": "2021-07-14T01:41:49.577Z",
          "wordCount": 588,
          "title": "Locally Enhanced Self-Attention: Rethinking Self-Attention as Local and Context Terms. (arXiv:2107.05637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengsheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1\">Miguel Angel Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1\">Alex Colburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulbricht_D/0/1/0/all/0/1\">Daniel Ulbricht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1\">Qi Shan</a>",
          "description": "We study the problem of novel view synthesis of a scene comprised of 3D\nobjects. We propose a simple yet effective approach that is neither continuous\nnor implicit, challenging recent trends on view synthesis. We demonstrate that\nalthough continuous radiance field representations have gained a lot of\nattention due to their expressive power, our simple approach obtains comparable\nor even better novel view reconstruction quality comparing with\nstate-of-the-art baselines while increasing rendering speed by over 400x. Our\nmodel is trained in a category-agnostic manner and does not require\nscene-specific optimization. Therefore, it is able to generalize novel view\nsynthesis to object categories not seen during training. In addition, we show\nthat with our simple formulation, we can use view synthesis as a\nself-supervision signal for efficient learning of 3D geometry without explicit\n3D supervision.",
          "link": "http://arxiv.org/abs/2107.05775",
          "publishedOn": "2021-07-14T01:41:49.558Z",
          "wordCount": 581,
          "title": "Fast and Explicit Neural View Synthesis. (arXiv:2107.05775v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun%2A_S/0/1/0/all/0/1\">Shuyang Sun*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue%2A_X/0/1/0/all/0/1\">Xiaoyu Yue*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>",
          "description": "Human vision is able to capture the part-whole hierarchical information from\nthe entire scene. This paper presents the Visual Parser (ViP) that explicitly\nconstructs such a hierarchy with transformers. ViP divides visual\nrepresentations into two levels, the part level and the whole level.\nInformation of each part represents a combination of several independent\nvectors within the whole. To model the representations of the two levels, we\nfirst encode the information from the whole into part vectors through an\nattention mechanism, then decode the global information within the part vectors\nback into the whole representation. By iteratively parsing the two levels with\nthe proposed encoder-decoder interaction, the model can gradually refine the\nfeatures on both levels. Experimental results demonstrate that ViP can achieve\nvery competitive performance on three major tasks e.g. classification,\ndetection and instance segmentation. In particular, it can surpass the previous\nstate-of-the-art CNN backbones by a large margin on object detection. The tiny\nmodel of the ViP family with $7.2\\times$ fewer parameters and $10.9\\times$\nfewer FLOPS can perform comparably with the largest model\nResNeXt-101-64$\\times$4d of ResNe(X)t family. Visualization results also\ndemonstrate that the learnt parts are highly informative of the predicting\nclass, making ViP more explainable than previous fundamental architectures.\nCode is available at https://github.com/kevin-ssy/ViP.",
          "link": "http://arxiv.org/abs/2107.05790",
          "publishedOn": "2021-07-14T01:41:49.550Z",
          "wordCount": 642,
          "title": "Visual Parser: Representing Part-whole Hierarchies with Transformers. (arXiv:2107.05790v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>",
          "description": "The technological advancements of deep learning have enabled sophisticated\nface manipulation schemes, raising severe trust issues and security concerns in\nmodern society. Generally speaking, detecting manipulated faces and locating\nthe potentially altered regions are challenging tasks. Herein, we propose a\nconceptually simple but effective method to efficiently detect forged faces in\nan image while simultaneously locating the manipulated regions. The proposed\nscheme relies on a segmentation map that delivers meaningful high-level\nsemantic information clues about the image. Furthermore, a noise map is\nestimated, playing a complementary role in capturing low-level clues and\nsubsequently empowering decision-making. Finally, the features from these two\nmodules are combined to distinguish fake faces. Extensive experiments show that\nthe proposed model achieves state-of-the-art detection accuracy and remarkable\nlocalization performance.",
          "link": "http://arxiv.org/abs/2107.05821",
          "publishedOn": "2021-07-14T01:41:49.543Z",
          "wordCount": 576,
          "title": "Detect and Locate: A Face Anti-Manipulation Approach with Semantic and Noise-level Supervision. (arXiv:2107.05821v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>",
          "description": "Deep neural networks suffer from catastrophic forgetting when learning\nmultiple knowledge sequentially, and a growing number of approaches have been\nproposed to mitigate this problem. Some of these methods achieved considerable\nperformance by associating the flat local minima with forgetting mitigation in\ncontinual learning. However, they inevitably need (1) tedious hyperparameters\ntuning, and (2) additional computational cost. To alleviate these problems, in\nthis paper, we propose a simple yet effective optimization method, called\nAlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we\nconduct gradient descent and ascent alternatively when the network tends to\nconverge at each session of learning new knowledge. Moreover, we theoretically\nprove that such a strategy can encourage the optimization to converge to a flat\nminima. We verify AlterSGD on continual learning benchmark for semantic\nsegmentation and the empirical results show that we can significantly mitigate\nthe forgetting and outperform the state-of-the-art methods with a large margin\nunder challenging continual learning protocols.",
          "link": "http://arxiv.org/abs/2107.05804",
          "publishedOn": "2021-07-14T01:41:49.519Z",
          "wordCount": 605,
          "title": "AlterSGD: Finding Flat Minima for Continual Learning by Alternative Training. (arXiv:2107.05804v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devgon_S/0/1/0/all/0/1\">Shivin Devgon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1\">Jeffrey Ichnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danielczuk_M/0/1/0/all/0/1\">Michael Danielczuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Daniel S. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1\">Ashwin Balakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shirin Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_E/0/1/0/all/0/1\">Eduardo M. C. Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solowjow_E/0/1/0/all/0/1\">Eugen Solowjow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>",
          "description": "In industrial part kitting, 3D objects are inserted into cavities for\ntransportation or subsequent assembly. Kitting is a critical step as it can\ndecrease downstream processing and handling times and enable lower storage and\nshipping costs. We present Kit-Net, a framework for kitting previously unseen\n3D objects into cavities given depth images of both the target cavity and an\nobject held by a gripper in an unknown initial orientation. Kit-Net uses\nself-supervised deep learning and data augmentation to train a convolutional\nneural network (CNN) to robustly estimate 3D rotations between objects and\nmatching concave or convex cavities using a large training dataset of simulated\ndepth images pairs. Kit-Net then uses the trained CNN to implement a controller\nto orient and position novel objects for insertion into novel prismatic and\nconformal 3D cavities. Experiments in simulation suggest that Kit-Net can\norient objects to have a 98.9% average intersection volume between the object\nmesh and that of the target cavity. Physical experiments with industrial\nobjects succeed in 18% of trials using a baseline method and in 63% of trials\nwith Kit-Net. Video, code, and data are available at\nhttps://github.com/BerkeleyAutomation/Kit-Net.",
          "link": "http://arxiv.org/abs/2107.05789",
          "publishedOn": "2021-07-14T01:41:49.511Z",
          "wordCount": 658,
          "title": "Kit-Net: Self-Supervised Learning to Kit Novel 3D Objects into Novel 3D Cavities. (arXiv:2107.05789v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1\">Hawzhin Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1\">Tolulope A. Odetola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1\">Nan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Syed Rafay Hasan</a>",
          "description": "In this paper, dynamic deployment of Convolutional Neural Network (CNN)\narchitecture is proposed utilizing only IoT-level devices. By partitioning and\npipelining the CNN, it horizontally distributes the computation load among\nresource-constrained devices (called horizontal collaboration), which in turn\nincreases the throughput. Through partitioning, we can decrease the computation\nand energy consumption on individual IoT devices and increase the throughput\nwithout sacrificing accuracy. Also, by processing the data at the generation\npoint, data privacy can be achieved. The results show that throughput can be\nincreased by 1.55x to 1.75x for sharing the CNN into two and three\nresource-constrained devices, respectively.",
          "link": "http://arxiv.org/abs/2107.05828",
          "publishedOn": "2021-07-14T01:41:49.503Z",
          "wordCount": 555,
          "title": "Dynamic Distribution of Edge Intelligence at the Node Level for Internet of Things. (arXiv:2107.05828v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toichkin_D/0/1/0/all/0/1\">Dmitry Toichkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1\">Yansong Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qinghai Guo</a>",
          "description": "State-of-the-art artificial neural networks (ANNs) require labelled data or\nfeedback between layers, are often biologically implausible, and are vulnerable\nto adversarial attacks that humans are not susceptible to. On the other hand,\nHebbian learning in winner-take-all (WTA) networks, is unsupervised,\nfeed-forward, and biologically plausible. However, an objective optimization\ntheory for WTA networks has been missing, except under very limiting\nassumptions. Here we derive formally such a theory, based on biologically\nplausible but generic ANN elements. Through Hebbian learning, network\nparameters maintain a Bayesian generative model of the data. There is no\nsupervisory loss function, but the network does minimize cross-entropy between\nits activations and the input distribution. The key is a \"soft\" WTA where there\nis no absolute \"hard\" winner neuron, and a specific type of Hebbian-like\nplasticity of weights and biases. We confirm our theory in practice, where, in\nhandwritten digit (MNIST) recognition, our Hebbian algorithm, SoftHebb,\nminimizes cross-entropy without having access to it, and outperforms the more\nfrequently used, hard-WTA-based method. Strikingly, it even outperforms\nsupervised end-to-end backpropagation, under certain conditions. Specifically,\nin a two-layered network, SoftHebb outperforms backpropagation when the\ntraining dataset is only presented once, when the testing data is noisy, and\nunder gradient-based adversarial attacks. Adversarial attacks that confuse\nSoftHebb are also confusing to the human eye. Finally, the model can generate\ninterpolations of objects from its input distribution.",
          "link": "http://arxiv.org/abs/2107.05747",
          "publishedOn": "2021-07-14T01:41:49.454Z",
          "wordCount": 679,
          "title": "SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all networks. (arXiv:2107.05747v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1\">Ali Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1\">Madhusudhanan Balasubramanian</a>",
          "description": "Dense optical flow estimation is challenging when there are large\ndisplacements in a scene with heterogeneous motion dynamics, occlusion, and\nscene homogeneity. Traditional approaches to handle these challenges include\nhierarchical and multiresolution processing methods. Learning-based optical\nflow methods typically use a multiresolution approach with image warping when a\nbroad range of flow velocities and heterogeneous motion is present. Accuracy of\nsuch coarse-to-fine methods is affected by the ghosting artifacts when images\nare warped across multiple resolutions and by the vanishing problem in smaller\nscene extents with higher motion contrast. Previously, we devised strategies\nfor building compact dense prediction networks guided by the effective\nreceptive field (ERF) characteristics of the network (DDCNet). The DDCNet\ndesign was intentionally simple and compact allowing it to be used as a\nbuilding block for designing more complex yet compact networks. In this work,\nwe extend the DDCNet strategies to handle heterogeneous motion dynamics by\ncascading DDCNet based sub-nets with decreasing extents of their ERF. Our\nDDCNet with multiresolution capability (DDCNet-Multires) is compact without any\nspecialized network layers. We evaluate the performance of the DDCNet-Multires\nnetwork using standard optical flow benchmark datasets. Our experiments\ndemonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and\nprovides optical flow estimates with accuracy comparable to similar lightweight\nlearning-based methods.",
          "link": "http://arxiv.org/abs/2107.05634",
          "publishedOn": "2021-07-14T01:41:49.412Z",
          "wordCount": 664,
          "title": "DDCNet-Multires: Effective Receptive Field Guided Multiresolution CNN for Dense Prediction. (arXiv:2107.05634v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahiner_A/0/1/0/all/0/1\">Arda Sahiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1\">Tolga Ergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1\">Batu Ozturkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartan_B/0/1/0/all/0/1\">Burak Bartan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1\">Morteza Mardani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>",
          "description": "Generative Adversarial Networks (GANs) are commonly used for modeling complex\ndistributions of data. Both the generators and discriminators of GANs are often\nmodeled by neural networks, posing a non-transparent optimization problem which\nis non-convex and non-concave over the generator and discriminator,\nrespectively. Such networks are often heuristically optimized with gradient\ndescent-ascent (GDA), but it is unclear whether the optimization problem\ncontains any saddle points, or whether heuristic methods can find them in\npractice. In this work, we analyze the training of Wasserstein GANs with\ntwo-layer neural network discriminators through the lens of convex duality, and\nfor a variety of generators expose the conditions under which Wasserstein GANs\ncan be solved exactly with convex optimization approaches, or can be\nrepresented as convex-concave games. Using this convex duality interpretation,\nwe further demonstrate the impact of different activation functions of the\ndiscriminator. Our observations are verified with numerical results\ndemonstrating the power of the convex interpretation, with applications in\nprogressive training of convex architectures corresponding to linear generators\nand quadratic-activation discriminators for CelebA image generation. The code\nfor our experiments is available at https://github.com/ardasahiner/ProCoGAN.",
          "link": "http://arxiv.org/abs/2107.05680",
          "publishedOn": "2021-07-14T01:41:49.405Z",
          "wordCount": 662,
          "title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1\">Darshan Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1\">S Balasubramanian</a>",
          "description": "Facial expression recognition (FER) in the wild is crucial for building\nreliable human-computer interactive systems. However, annotations of large\nscale datasets in FER has been a key challenge as these datasets suffer from\nnoise due to various factors like crowd sourcing, subjectivity of annotators,\npoor quality of images, automatic labelling based on key word search etc. Such\nnoisy annotations impede the performance of FER due to the memorization ability\nof deep networks. During early learning stage, deep networks fit on clean data.\nThen, eventually, they start overfitting on noisy labels due to their\nmemorization ability, which limits FER performance. This report presents\nConsensual Collaborative Training (CCT) framework used in our submission to\nexpression recognition track of the Affective Behaviour Analysis in-the-wild\n(ABAW) 2021 competition. CCT co-trains three networks jointly using a convex\ncombination of supervision loss and consistency loss, without making any\nassumption about the noise distribution. A dynamic transition mechanism is used\nto move from supervision loss in early learning to consistency loss for\nconsensus of predictions among networks in the later stage. Co-training reduces\noverall error, and consistency loss prevents overfitting to noisy samples. The\nperformance of the model is validated on challenging Aff-Wild2 dataset for\ncategorical expression classification. Our code is made publicly available at\nhttps://github.com/1980x/ABAW2021DMACS.",
          "link": "http://arxiv.org/abs/2107.05736",
          "publishedOn": "2021-07-14T01:41:49.362Z",
          "wordCount": 679,
          "title": "Affect Expression Behaviour Analysis in the Wild using Consensual Collaborative Training. (arXiv:2107.05736v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>",
          "description": "This paper presents a novel hierarchical Bayesian model for unbiased atlas\nbuilding with subject-specific regularizations of image registration. We\ndevelop an atlas construction process that automatically selects parameters to\ncontrol the smoothness of diffeomorphic transformation according to individual\nimage data. To achieve this, we introduce a hierarchical prior distribution on\nregularization parameters that allows multiple penalties on images with various\ndegrees of geometric transformations. We then treat the regularization\nparameters as latent variables and integrate them out from the model by using\nthe Monte Carlo Expectation Maximization (MCEM) algorithm. Another advantage of\nour algorithm is that it eliminates the need for manual parameter tuning, which\ncan be tedious and infeasible. We demonstrate the effectiveness of our model on\n3D brain MR images. Experimental results show that our model provides a sharper\natlas compared to the current atlas building algorithms with single-penalty\nregularizations. Our code is publicly available at\nhttps://github.com/jw4hv/HierarchicalBayesianAtlasBuild.",
          "link": "http://arxiv.org/abs/2107.05698",
          "publishedOn": "2021-07-14T01:41:49.345Z",
          "wordCount": 616,
          "title": "Bayesian Atlas Building with Hierarchical Priors for Subject-specific Regularization. (arXiv:2107.05698v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>",
          "description": "Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.",
          "link": "http://arxiv.org/abs/2107.05768",
          "publishedOn": "2021-07-14T01:41:49.311Z",
          "wordCount": 664,
          "title": "Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilie_A/0/1/0/all/0/1\">Andrei Ilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanescu_A/0/1/0/all/0/1\">Alin Stefanescu</a>",
          "description": "Recent work has shown how easily white-box adversarial attacks can be applied\nto state-of-the-art image classifiers. However, real-life scenarios resemble\nmore the black-box adversarial conditions, lacking transparency and usually\nimposing natural, hard constraints on the query budget.\n\nWe propose $\\textbf{EvoBA}$, a black-box adversarial attack based on a\nsurprisingly simple evolutionary search strategy. $\\textbf{EvoBA}$ is\nquery-efficient, minimizes $L_0$ adversarial perturbations, and does not\nrequire any form of training.\n\n$\\textbf{EvoBA}$ shows efficiency and efficacy through results that are in\nline with much more complex state-of-the-art black-box attacks such as\n$\\textbf{AutoZOOM}$. It is more query-efficient than $\\textbf{SimBA}$, a simple\nand powerful baseline black-box attack, and has a similar level of complexity.\nTherefore, we propose it both as a new strong baseline for black-box\nadversarial attacks and as a fast and general tool for gaining empirical\ninsight into how robust image classifiers are with respect to $L_0$ adversarial\nperturbations.\n\nThere exist fast and reliable $L_2$ black-box attacks, such as\n$\\textbf{SimBA}$, and $L_{\\infty}$ black-box attacks, such as\n$\\textbf{DeepSearch}$. We propose $\\textbf{EvoBA}$ as a query-efficient $L_0$\nblack-box adversarial attack which, together with the aforementioned methods,\ncan serve as a generic tool to assess the empirical robustness of image\nclassifiers. The main advantages of such methods are that they run fast, are\nquery-efficient, and can easily be integrated in image classifiers development\npipelines.\n\nWhile our attack minimises the $L_0$ adversarial perturbation, we also report\n$L_2$, and notice that we compare favorably to the state-of-the-art $L_2$\nblack-box attack, $\\textbf{AutoZOOM}$, and of the $L_2$ strong baseline,\n$\\textbf{SimBA}$.",
          "link": "http://arxiv.org/abs/2107.05754",
          "publishedOn": "2021-07-14T01:41:49.277Z",
          "wordCount": 694,
          "title": "EvoBA: An Evolution Strategy as a Strong Baseline forBlack-Box Adversarial Attacks. (arXiv:2107.05754v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1\">Ajey Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1\">Nisarg Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Prasenjit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1\">Govind Makharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>",
          "description": "Contrastive Learning (CL) is a recent representation learning approach, which\nencourages inter-class separability and intra-class compactness in learned\nimage representations. Since medical images often contain multiple semantic\nclasses in an image, using CL to learn representations of local features (as\nopposed to global) is important. In this work, we present a novel\nsemi-supervised 2D medical segmentation solution that applies CL on image\npatches, instead of full images. These patches are meaningfully constructed\nusing the semantic information of different classes obtained via pseudo\nlabeling. We also propose a novel consistency regularization (CR) scheme, which\nworks in synergy with CL. It addresses the problem of confirmation bias, and\nencourages better clustering in the feature space. We evaluate our method on\nfour public medical segmentation datasets and a novel histopathology dataset\nthat we introduce. Our method obtains consistent improvements over\nstate-of-the-art semi-supervised segmentation approaches for all datasets.",
          "link": "http://arxiv.org/abs/2106.06801",
          "publishedOn": "2021-07-13T01:59:35.482Z",
          "wordCount": 613,
          "title": "Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaofei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>",
          "description": "Scene understanding based on LiDAR point cloud is an essential task for\nautonomous cars to drive safely, which often employs spherical projection to\nmap 3D point cloud into multi-channel 2D images for semantic segmentation. Most\nexisting methods simply stack different point attributes/modalities (e.g.\ncoordinates, intensity, depth, etc.) as image channels to increase information\ncapacity, but ignore distinct characteristics of point attributes in different\nimage channels. We design FPS-Net, a convolutional fusion network that exploits\nthe uniqueness and discrepancy among the projected image channels for optimal\npoint cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead\nof simply stacking multiple channel images as a single input, we group them\ninto different modalities to first learn modality-specific features separately\nand then map the learned features into a common high-dimensional feature space\nfor pixel-level fusion and learning. Specifically, we design a residual dense\nblock with multiple receptive fields as a building block in the encoder which\npreserves detailed information in each modality and learns hierarchical\nmodality-specific and fused features effectively. In the FPS-Net decoder, we\nuse a recurrent convolution block likewise to hierarchically decode fused\nfeatures into output space for pixel-level classification. Extensive\nexperiments conducted on two widely adopted point cloud datasets show that\nFPS-Net achieves superior semantic segmentation as compared with\nstate-of-the-art projection-based methods. In addition, the proposed modality\nfusion idea is compatible with typical projection-based methods and can be\nincorporated into them with consistent performance improvements.",
          "link": "http://arxiv.org/abs/2103.00738",
          "publishedOn": "2021-07-13T01:59:35.470Z",
          "wordCount": 697,
          "title": "FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation. (arXiv:2103.00738v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.02159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tone_D/0/1/0/all/0/1\">Daiki Tone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1\">Daisuke Iwai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiura_S/0/1/0/all/0/1\">Shinsaku Hiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1\">Kosuke Sato</a>",
          "description": "This paper presents a novel active marker for dynamic projection mapping (PM)\nthat emits a temporal blinking pattern of infrared (IR) light representing its\nID. We used a multi-material three dimensional (3D) printer to fabricate a\nprojection object with optical fibers that can guide IR light from LEDs\nattached on the bottom of the object. The aperture of an optical fiber is\ntypically very small; thus, it is unnoticeable to human observers under\nprojection and can be placed on a strongly curved part of a projection surface.\nIn addition, the working range of our system can be larger than previous\nmarker-based methods as the blinking patterns can theoretically be recognized\nby a camera placed at a wide range of distances from markers. We propose an\nautomatic marker placement algorithm to spread multiple active markers over the\nsurface of a projection object such that its pose can be robustly estimated\nusing captured images from arbitrary directions. We also propose an\noptimization framework for determining the routes of the optical fibers in such\na way that collisions of the fibers can be avoided while minimizing the loss of\nlight intensity in the fibers. Through experiments conducted using three\nfabricated objects containing strongly curved surfaces, we confirmed that the\nproposed method can achieve accurate dynamic PMs in a significantly wide\nworking range.",
          "link": "http://arxiv.org/abs/2002.02159",
          "publishedOn": "2021-07-13T01:59:35.429Z",
          "wordCount": 693,
          "title": "FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers in Dynamic Projection Mapping. (arXiv:2002.02159v1 [cs.GR] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "This paper jointly resolves two problems in vision transformer: i) the\ncomputation of Multi-Head Self-Attention (MHSA) has high computational/space\ncomplexity; ii) recent vision transformer networks are overly tuned for image\nclassification, ignoring the difference between image classification (simple\nscenarios, more similar to NLP) and downstream scene understanding tasks\n(complicated scenarios, rich structural and contextual information). To this\nend, we note that pyramid pooling has been demonstrated to be effective in\nvarious vision tasks owing to its powerful ability in context abstraction, and\nits natural property of spatial invariance is also suitable to address the loss\nof structural information (problem ii)). Hence, we propose to adapt pyramid\npooling to MHSA for alleviating its high requirement on computational resources\n(problem i)). In this way, this pooling-based MHSA can well address the above\ntwo problems and is thus flexible and powerful for downstream scene\nunderstanding tasks. Plugged with our pooling-based MHSA, we build a\ndownstream-task-oriented transformer network, dubbed Pyramid Pooling\nTransformer (P2T). Extensive experiments demonstrate that, when applied P2T as\nthe backbone network, it shows substantial superiority in various downstream\nscene understanding tasks such as semantic segmentation, object detection,\ninstance segmentation, and visual saliency detection, compared to previous CNN-\nand transformer-based networks. The code will be released at\nhttps://github.com/yuhuan-wu/P2T.",
          "link": "http://arxiv.org/abs/2106.12011",
          "publishedOn": "2021-07-13T01:59:35.406Z",
          "wordCount": 678,
          "title": "P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Child_T/0/1/0/all/0/1\">Travers B. Child</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>",
          "description": "Visual Commonsense Reasoning (VCR) predicts an answer with corresponding\nrationale, given a question-image input. VCR is a recently introduced visual\nscene understanding task with a wide range of applications, including visual\nquestion answering, automated vehicle systems, and clinical decision support.\nPrevious approaches to solving the VCR task generally rely on pre-training or\nexploiting memory with long dependency relationship encoded models. However,\nthese approaches suffer from a lack of generalizability and prior knowledge. In\nthis paper we propose a dynamic working memory based cognitive VCR network,\nwhich stores accumulated commonsense between sentences to provide prior\nknowledge for inference. Extensive experiments show that the proposed model\nyields significant improvements over existing methods on the benchmark VCR\ndataset. Moreover, the proposed model provides intuitive interpretation into\nvisual commonsense reasoning. A Python implementation of our mechanism is\npublicly available at https://github.com/tanjatang/DMVCR",
          "link": "http://arxiv.org/abs/2107.01671",
          "publishedOn": "2021-07-13T01:59:35.394Z",
          "wordCount": 601,
          "title": "Cognitive Visual Commonsense Reasoning Using Dynamic Working Memory. (arXiv:2107.01671v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Changshun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcone_Y/0/1/0/all/0/1\">Yli&#xe8;s Falcone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensalem_S/0/1/0/all/0/1\">Saddek Bensalem</a>",
          "description": "Classification neural networks fail to detect inputs that do not fall inside\nthe classes they have been trained for. Runtime monitoring techniques on the\nneuron activation pattern can be used to detect such inputs. We present an\napproach for monitoring classification systems via data abstraction. Data\nabstraction relies on the notion of box with a resolution. Box-based\nabstraction consists in representing a set of values by its minimal and maximal\nvalues in each dimension. We augment boxes with a notion of resolution and\ndefine their clustering coverage, which is intuitively a quantitative metric\nthat indicates the abstraction quality. This allows studying the effect of\ndifferent clustering parameters on the constructed boxes and estimating an\ninterval of sub-optimal parameters. Moreover, we automatically construct\nmonitors that leverage both the correct and incorrect behaviors of a system.\nThis allows checking the size of the monitor abstractions and analyzing the\nseparability of the network. Monitors are obtained by combining the\nsub-monitors of each class of the system placed at some selected layers. Our\nexperiments demonstrate the effectiveness of our clustering coverage estimation\nand show how to assess the effectiveness and precision of monitors according to\nthe selected clustering parameter and monitored layers.",
          "link": "http://arxiv.org/abs/2104.14435",
          "publishedOn": "2021-07-13T01:59:35.388Z",
          "wordCount": 668,
          "title": "Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes. (arXiv:2104.14435v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tingting Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Modern top-performing object detectors depend heavily on backbone networks,\nwhose advances bring consistent performance gains through exploring more\neffective network structures. In this paper, we propose a novel and flexible\nbackbone framework, namely CBNetV2, to better train existing open-sourced\npre-trained backbones under the pre-training fine-tuning protocol. In\nparticular, CBNetV2 architecture groups multiple identical backbones, which are\nconnected through composite connections. Specifically, it integrates the high-\nand low-level features of multiple backbone networks and gradually expands the\nreceptive field to more efficiently perform object detection. We also propose a\nbetter training strategy with assistant supervision for CBNet-based detectors.\nCBNetV2 has strong generalization capabilities for different backbones and head\ndesigns of the detector architecture. Without additional pre-training, CBNetV2\ncan be adapted to various backbones, including manual-based and NAS-based, as\nwell as CNN-based and Transformer-based ones. Experiments provide strong\nevidence showing that composite backbones are more efficient, effective, and\nresource-friendly than wider and deeper networks. CBNetV2 is compatible with\nthe head designs of most mainstream detectors, including one-stage and\ntwo-stage detectors, as well as anchor-based and anchor-free-based ones, and\nsignificantly improve their performances by more than 3.0% AP over the baseline\non COCO. Particularly, under the single-model and single-scale testing\nprotocol, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO\ntest-dev, which is significantly better than the state-of-the-art result (i.e.,\n57.7% box AP and 50.2% mask AP). Code is available at\nhttps://github.com/VDIGPKU/CBNetV2.",
          "link": "http://arxiv.org/abs/2107.00420",
          "publishedOn": "2021-07-13T01:59:35.364Z",
          "wordCount": 719,
          "title": "CBNetV2: A Composite Backbone Network Architecture for Object Detection. (arXiv:2107.00420v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1\">Dimitrios Vytiniotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1\">Grzegorz Swirszcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1\">Viorica Patraucean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.",
          "link": "http://arxiv.org/abs/2106.08318",
          "publishedOn": "2021-07-13T01:59:35.358Z",
          "wordCount": 668,
          "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_M/0/1/0/all/0/1\">Mingzhe Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhengjun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Facial Expression Recognition (FER) in the wild is an extremely challenging\ntask in computer vision due to variant backgrounds, low-quality facial images,\nand the subjectiveness of annotators. These uncertainties make it difficult for\nneural networks to learn robust features on limited-scale datasets. Moreover,\nthe networks can be easily distributed by the above factors and perform\nincorrect decisions. Recently, vision transformer (ViT) and data-efficient\nimage transformers (DeiT) present their significant performance in traditional\nclassification tasks. The self-attention mechanism makes transformers obtain a\nglobal receptive field in the first layer which dramatically enhances the\nfeature extraction capability. In this work, we first propose a novel pure\ntransformer-based mask vision transformer (MVT) for FER in the wild, which\nconsists of two modules: a transformer-based mask generation network (MGN) to\ngenerate a mask that can filter out complex backgrounds and occlusion of face\nimages, and a dynamic relabeling module to rectify incorrect labels in FER\ndatasets in the wild. Extensive experimental results demonstrate that our MVT\noutperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with\n89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable\nresult on AffectNet-8 with 61.40%.",
          "link": "http://arxiv.org/abs/2106.04520",
          "publishedOn": "2021-07-13T01:59:35.352Z",
          "wordCount": 666,
          "title": "MVT: Mask Vision Transformer for Facial Expression Recognition in the wild. (arXiv:2106.04520v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangrui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "Place recognition gives a SLAM system the ability to correct cumulative\nerrors. Unlike images that contain rich texture features, point clouds are\nalmost pure geometric information which makes place recognition based on point\nclouds challenging. Existing works usually encode low-level features such as\ncoordinate, normal, reflection intensity, etc., as local or global descriptors\nto represent scenes. Besides, they often ignore the translation between point\nclouds when matching descriptors. Different from most existing methods, we\nexplore the use of high-level features, namely semantics, to improve the\ndescriptor's representation ability. Also, when matching descriptors, we try to\ncorrect the translation between point clouds to improve accuracy. Concretely,\nwe propose a novel global descriptor, Semantic Scan Context, which explores\nsemantic information to represent scenes more effectively. We also present a\ntwo-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align\nthe point cloud to improve matching performance. Our experiments on the KITTI\ndataset show that our approach outperforms the state-of-the-art methods with a\nlarge margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.",
          "link": "http://arxiv.org/abs/2107.00382",
          "publishedOn": "2021-07-13T01:59:35.345Z",
          "wordCount": 638,
          "title": "SSC: Semantic Scan Context for Large-Scale Place Recognition. (arXiv:2107.00382v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhananjaya_M/0/1/0/all/0/1\">Mahesh M Dhananjaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Autonomous driving is rapidly advancing, and Level 2 functions are becoming a\nstandard feature. One of the foremost outstanding hurdles is to obtain robust\nvisual perception in harsh weather and low light conditions where accuracy\ndegradation is severe. It is critical to have a weather classification model to\ndecrease visual perception confidence during these scenarios. Thus, we have\nbuilt a new dataset for weather (fog, rain, and snow) classification and light\nlevel (bright, moderate, and low) classification. Furthermore, we provide\nstreet type (asphalt, grass, and cobblestone) classification, leading to 9\nlabels. Each image has three labels corresponding to weather, light level, and\nstreet type. We recorded the data utilizing an industrial front camera of RCCC\n(red/clear) format with a resolution of $1024\\times1084$. We collected 15k\nvideo sequences and sampled 60k images. We implement an active learning\nframework to reduce the dataset's redundancy and find the optimal set of frames\nfor training a model. We distilled the 60k images further to 1.1k images, which\nwill be shared publicly after privacy anonymization. There is no public dataset\nfor weather and light level classification focused on autonomous driving to the\nbest of our knowledge. The baseline ResNet18 network used for weather\nclassification achieves state-of-the-art results in two non-automotive weather\nclassification public datasets but significantly lower accuracy on our proposed\ndataset, demonstrating it is not saturated and needs further research.",
          "link": "http://arxiv.org/abs/2104.14042",
          "publishedOn": "2021-07-13T01:59:35.338Z",
          "wordCount": 715,
          "title": "Weather and Light Level Classification for Autonomous Driving: Dataset, Baseline and Active Learning. (arXiv:2104.14042v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianbu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>",
          "description": "The monitoring of coastal wetlands is of great importance to the protection\nof marine and terrestrial ecosystems. However, due to the complex environment,\nsevere vegetation mixture, and difficulty of access, it is impossible to\naccurately classify coastal wetlands and identify their species with\ntraditional classifiers. Despite the integration of multisource remote sensing\ndata for performance enhancement, there are still challenges with acquiring and\nexploiting the complementary merits from multisource data. In this paper, the\nDeepwise Feature Interaction Network (DFINet) is proposed for wetland\nclassification. A depthwise cross attention module is designed to extract\nself-correlation and cross-correlation from multisource feature pairs. In this\nway, meaningful complementary information is emphasized for classification.\nDFINet is optimized by coordinating consistency loss, discrimination loss, and\nclassification loss. Accordingly, DFINet reaches the standard solution-space\nunder the regularity of loss functions, while the spatial consistency and\nfeature discrimination are preserved. Comprehensive experimental results on two\nhyperspectral and multispectral wetland datasets demonstrate that the proposed\nDFINet outperforms other competitive methods in terms of overall accuracy.",
          "link": "http://arxiv.org/abs/2106.06896",
          "publishedOn": "2021-07-13T01:59:35.315Z",
          "wordCount": 656,
          "title": "Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yaya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>",
          "description": "By adding human-imperceptible perturbations to images, DNNs can be easily\nfooled. As one of the mainstream methods, feature space targeted attacks\nperturb images by modulating their intermediate feature maps, for the\ndiscrepancy between the intermediate source and target features is minimized.\nHowever, the current choice of pixel-wise Euclidean Distance to measure the\ndiscrepancy is questionable because it unreasonably imposes a\nspatial-consistency constraint on the source and target features. Intuitively,\nan image can be categorized as \"cat\" no matter the cat is on the left or right\nof the image. To address this issue, we propose to measure this discrepancy\nusing statistic alignment. Specifically, we design two novel approaches called\nPair-wise Alignment Attack and Global-wise Alignment Attack, which attempt to\nmeasure similarities between feature maps by high-order statistics with\ntranslation invariance. Furthermore, we systematically analyze the layer-wise\ntransferability with varied difficulties to obtain highly reliable attacks.\nExtensive experiments verify the effectiveness of our proposed method, and it\noutperforms the state-of-the-art algorithms by a large margin. Our code is\npublicly available at https://github.com/yaya-cheng/PAA-GAA.",
          "link": "http://arxiv.org/abs/2105.11645",
          "publishedOn": "2021-07-13T01:59:35.308Z",
          "wordCount": 637,
          "title": "Feature Space Targeted Attacks by Statistic Alignment. (arXiv:2105.11645v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zamanitajeddin_N/0/1/0/all/0/1\">Neda Zamanitajeddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>",
          "description": "Digitization of histology images and the advent of new computational methods,\nlike deep learning, have helped the automatic grading of colorectal\nadenocarcinoma cancer (CRA). Present automated CRA grading methods, however,\nusually use tiny image patches and thus fail to integrate the entire tissue\nmicro-architecture for grading purposes. To tackle these challenges, we propose\nto use a statistical network analysis method to describe the complex structure\nof the tissue micro-environment by modelling nuclei and their connections as a\nnetwork. We show that by analyzing only the interactions between the cells in a\nnetwork, we can extract highly discriminative statistical features for CRA\ngrading. Unlike other deep learning or convolutional graph-based approaches,\nour method is highly scalable (can be used for cell networks consist of\nmillions of nodes), completely explainable, and computationally inexpensive. We\ncreate cell networks on a broad CRC histology image dataset, experiment with\nour method, and report state-of-the-art performance for the prediction of\nthree-class CRA grading.",
          "link": "http://arxiv.org/abs/2106.15299",
          "publishedOn": "2021-07-13T01:59:35.300Z",
          "wordCount": 624,
          "title": "Cells are Actors: Social Network Analysis with Classical ML for SOTA Histology Image Classification. (arXiv:2106.15299v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07961",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1\">Alan Q. Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+LaViolette_A/0/1/0/all/0/1\">Aaron K. LaViolette</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moon_L/0/1/0/all/0/1\">Leo Moon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chris Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>",
          "description": "Compressed sensing fluorescence microscopy (CS-FM) proposes a scheme whereby\nless measurements are collected during sensing and reconstruction is performed\nto recover the image. Much work has gone into optimizing the sensing and\nreconstruction portions separately. We propose a method of jointly optimizing\nboth sensing and reconstruction end-to-end under a total measurement\nconstraint, enabling learning of the optimal sensing scheme concurrently with\nthe parameters of a neural network-based reconstruction network. We train our\nmodel on a rich dataset of confocal, two-photon, and wide-field microscopy\nimages comprising of a variety of biological samples. We show that our method\noutperforms several baseline sensing schemes and a regularized regression\nreconstruction algorithm.",
          "link": "http://arxiv.org/abs/2105.07961",
          "publishedOn": "2021-07-13T01:59:35.295Z",
          "wordCount": 586,
          "title": "Joint Optimization of Hadamard Sensing and Reconstruction in Compressed Sensing Fluorescence Microscopy. (arXiv:2105.07961v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1\">Michal Uricar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahiaoui_L/0/1/0/all/0/1\">Lucie Yahiaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Manual annotation of soiling on surround view cameras is a very challenging\nand expensive task. The unclear boundary for various soiling categories like\nwater drops or mud particles usually results in a large variance in the\nannotation quality. As a result, the models trained on such poorly annotated\ndata are far from being optimal. In this paper, we focus on handling such noisy\nannotations via pseudo-label driven ensemble model which allow us to quickly\nspot problematic annotations and in most cases also sufficiently fixing them.\nWe train a soiling segmentation model on both noisy and refined labels and\ndemonstrate significant improvements using the refined annotations. It also\nillustrates that it is possible to effectively refine lower cost coarse\nannotations.",
          "link": "http://arxiv.org/abs/2105.07930",
          "publishedOn": "2021-07-13T01:59:35.275Z",
          "wordCount": 601,
          "title": "Ensemble-based Semi-supervised Learning to Improve Noisy Soiling Annotations in Autonomous Driving. (arXiv:2105.07930v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14844",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yucheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>",
          "description": "Low-light imaging on mobile devices is typically challenging due to\ninsufficient incident light coming through the relatively small aperture,\nresulting in a low signal-to-noise ratio. Most of the previous works on\nlow-light image processing focus either only on a single task such as\nillumination adjustment, color enhancement, or noise removal; or on a joint\nillumination adjustment and denoising task that heavily relies on short-long\nexposure image pairs collected from specific camera models, and thus these\napproaches are less practical and generalizable in real-world settings where\ncamera-specific joint enhancement and restoration is required. To tackle this\nproblem, in this paper, we propose a low-light image processing framework that\nperforms joint illumination adjustment, color enhancement, and denoising.\nConsidering the difficulty in model-specific data collection and the ultra-high\ndefinition of the captured images, we design two branches: a coefficient\nestimation branch as well as a joint enhancement and denoising branch. The\ncoefficient estimation branch works in a low-resolution space and predicts the\ncoefficients for enhancement via bilateral learning, whereas the joint\nenhancement and denoising branch works in a full-resolution space and\nprogressively performs joint enhancement and denoising. In contrast to existing\nmethods, our framework does not need to recollect massive data when being\nadapted to another camera model, which significantly reduces the efforts\nrequired to fine-tune our approach for practical usage. Through extensive\nexperiments, we demonstrate its great potential in real-world low-light imaging\napplications when compared with current state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.14844",
          "publishedOn": "2021-07-13T01:59:35.258Z",
          "wordCount": 708,
          "title": "Progressive Joint Low-light Enhancement and Noise Removal for Raw Images. (arXiv:2106.14844v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weina Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>",
          "description": "Being able to explain the prediction to clinical end-users is a necessity to\nleverage the power of AI models for clinical decision support. For medical\nimages, saliency maps are the most common form of explanation. The maps\nhighlight important features for AI model's prediction. Although many saliency\nmap methods have been proposed, it is unknown how well they perform on\nexplaining decisions on multi-modal medical images, where each modality/channel\ncarries distinct clinical meanings of the same underlying biomedical\nphenomenon. Understanding such modality-dependent features is essential for\nclinical users' interpretation of AI decisions. To tackle this clinically\nimportant but technically ignored problem, we propose the MSFI\n(Modality-Specific Feature Importance) metric to examine whether saliency maps\ncan highlight modality-specific important features. MSFI encodes the clinical\nrequirements on modality prioritization and modality-specific feature\nlocalization. Our evaluations on 16 commonly used saliency map methods,\nincluding a clinician user study, show that although most saliency map methods\ncaptured modality importance information in general, most of them failed to\nhighlight modality-specific important features consistently and precisely. The\nevaluation results guide the choices of saliency map methods and provide\ninsights to propose new ones targeting clinical applications.",
          "link": "http://arxiv.org/abs/2107.05047",
          "publishedOn": "2021-07-13T01:59:35.239Z",
          "wordCount": 646,
          "title": "One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images. (arXiv:2107.05047v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuanhao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>",
          "description": "This technical report presents our solution to the HACS Temporal Action\nLocalization Challenge 2021, Weakly-Supervised Learning Track. The goal of\nweakly-supervised temporal action localization is to temporally locate and\nclassify action of interest in untrimmed videos given only video-level labels.\nWe adopt the two-stream consensus network (TSCN) as the main framework in this\nchallenge. The TSCN consists of a two-stream base model training procedure and\na pseudo ground truth learning procedure. The base model training encourages\nthe model to predict reliable predictions based on single modality (i.e., RGB\nor optical flow), based on the fusion of which a pseudo ground truth is\ngenerated and in turn used as supervision to train the base models. On the HACS\nv1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our\nmethod achieves 22.20% on the validation set and 21.68% on the testing set in\nterms of average mAP. Our solution ranked the 2rd in this challenge, and we\nhope our method can serve as a baseline for future academic research.",
          "link": "http://arxiv.org/abs/2106.10829",
          "publishedOn": "2021-07-13T01:59:35.233Z",
          "wordCount": 654,
          "title": "Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track. (arXiv:2106.10829v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1\">Juraj Kabzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kok Seang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1\">Eric Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1\">Alex Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1\">Luke Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1\">Sammy Omari</a>",
          "description": "In this work, we propose the world's first closed-loop ML-based planning\nbenchmark for autonomous driving. While there is a growing body of ML-based\nmotion planners, the lack of established datasets and metrics has limited the\nprogress in this area. Existing benchmarks for autonomous vehicle motion\nprediction have focused on short-term motion forecasting, rather than long-term\nplanning. This has led previous works to use open-loop evaluation with L2-based\nmetrics, which are not suitable for fairly evaluating long-term planning. Our\nbenchmark overcomes these limitations by introducing a large-scale driving\ndataset, lightweight closed-loop simulator, and motion-planning-specific\nmetrics. We provide a high-quality dataset with 1500h of human driving data\nfrom 4 cities across the US and Asia with widely varying traffic patterns\n(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop\nsimulation framework with reactive agents and provide a large set of both\ngeneral and scenario-specific planning metrics. We plan to release the dataset\nat NeurIPS 2021 and organize benchmark challenges starting in early 2022.",
          "link": "http://arxiv.org/abs/2106.11810",
          "publishedOn": "2021-07-13T01:59:35.202Z",
          "wordCount": 650,
          "title": "NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoxiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a novel method for single image depth estimation using surface\nnormal constraints. Existing depth estimation methods either suffer from the\nlack of geometric constraints, or are limited to the difficulty of reliably\ncapturing geometric context, which leads to a bottleneck of depth estimation\nquality. We therefore introduce a simple yet effective method, named Adaptive\nSurface Normal (ASN) constraint, to effectively correlate the depth estimation\nwith geometric consistency. Our key idea is to adaptively determine the\nreliable local geometry from a set of randomly sampled candidates to derive\nsurface normal constraint, for which we measure the consistency of the\ngeometric contextual features. As a result, our method can faithfully\nreconstruct the 3D geometry and is robust to local shape variations, such as\nboundaries, sharp corners and noises. We conduct extensive evaluations and\ncomparisons using public datasets. The experimental results demonstrate our\nmethod outperforms the state-of-the-art methods and has superior efficiency and\nrobustness.",
          "link": "http://arxiv.org/abs/2103.15483",
          "publishedOn": "2021-07-13T01:59:35.196Z",
          "wordCount": 623,
          "title": "Adaptive Surface Normal Constraint for Depth Estimation. (arXiv:2103.15483v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1\">Sambit Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotzig_H/0/1/0/all/0/1\">Heinrich Gotzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1\">Stefan Milz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1\">Patrick Mader</a>",
          "description": "3D object detection based on LiDAR point clouds is a crucial module in\nautonomous driving particularly for long range sensing. Most of the research is\nfocused on achieving higher accuracy and these models are not optimized for\ndeployment on embedded systems from the perspective of latency and power\nefficiency. For high speed driving scenarios, latency is a crucial parameter as\nit provides more time to react to dangerous situations. Typically a voxel or\npoint-cloud based 3D convolution approach is utilized for this module. Firstly,\nthey are inefficient on embedded platforms as they are not suitable for\nefficient parallelization. Secondly, they have a variable runtime due to level\nof sparsity of the scene which is against the determinism needed in a safety\nsystem. In this work, we aim to develop a very low latency algorithm with fixed\nruntime. We propose a novel semantic segmentation architecture as a single\nunified model for object center detection using key points, box predictions and\norientation prediction using binned classification in a simpler Bird's Eye View\n(BEV) 2D representation. The proposed architecture can be trivially extended to\ninclude semantic segmentation classes like road without any additional\ncomputation. The proposed model has a latency of 4 ms on the embedded Nvidia\nXavier platform. The model is 5X faster than other top accuracy models with a\nminimal accuracy degradation of 2% in Average Precision at IoU=0.5 on KITTI\ndataset.",
          "link": "http://arxiv.org/abs/2104.10780",
          "publishedOn": "2021-07-13T01:59:35.184Z",
          "wordCount": 725,
          "title": "BEVDetNet: Bird's Eye View LiDAR Point Cloud based Real-time 3D Object Detection for Autonomous Driving. (arXiv:2104.10780v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1\">Waqas Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_A/0/1/0/all/0/1\">Amir Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Neeraj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RehmanJaved_A/0/1/0/all/0/1\">Abdul RehmanJaved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1\">Thippa Reddy Gadekallu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalil_Z/0/1/0/all/0/1\">Zunera Jalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryvinska_N/0/1/0/all/0/1\">Natalia Kryvinska</a>",
          "description": "Cash payment is still king in several markets, accounting for more than 90\\\nof the payments in almost all the developing countries. The usage of mobile\nphones is pretty ordinary in this present era. Mobile phones have become an\ninseparable friend for many users, serving much more than just communication\ntools. Every subsequent person is heavily relying on them due to multifaceted\nusage and affordability. Every person wants to manage his/her daily\ntransactions and related issues by using his/her mobile phone. With the rise\nand advancements of mobile-specific security, threats are evolving as well. In\nthis paper, we provide a survey of various security models for mobile phones.\nWe explore multiple proposed models of the mobile payment system (MPS), their\ntechnologies and comparisons, payment methods, different security mechanisms\ninvolved in MPS, and provide analysis of the encryption technologies,\nauthentication methods, and firewall in MPS. We also present current challenges\nand future directions of mobile phone security.",
          "link": "http://arxiv.org/abs/2105.12097",
          "publishedOn": "2021-07-13T01:59:35.166Z",
          "wordCount": 634,
          "title": "Security in Next Generation Mobile Payment Systems: A Comprehensive Survey. (arXiv:2105.12097v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1\">Serban Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>",
          "description": "Convolutional neural networks (CNNs) have led to significant improvements in\ntasks involving semantic segmentation of images. CNNs are vulnerable in the\narea of biomedical image segmentation because of distributional gap between two\nsource and target domains with different data modalities which leads to domain\nshift. Domain shift makes data annotations in new modalities necessary because\nmodels must be retrained from scratch. Unsupervised domain adaptation (UDA) is\nproposed to adapt a model to new modalities using solely unlabeled target\ndomain data. Common UDA algorithms require access to data points in the source\ndomain which may not be feasible in medical imaging due to privacy concerns. In\nthis work, we develop an algorithm for UDA in a privacy-constrained setting,\nwhere the source domain data is inaccessible. Our idea is based on encoding the\ninformation from the source samples into a prototypical distribution that is\nused as an intermediate distribution for aligning the target domain\ndistribution with the source domain distribution. We demonstrate the\neffectiveness of our algorithm by comparing it to state-of-the-art medical\nimage semantic segmentation approaches on two medical image semantic\nsegmentation datasets.",
          "link": "http://arxiv.org/abs/2101.00522",
          "publishedOn": "2021-07-13T01:59:35.157Z",
          "wordCount": 659,
          "title": "Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images. (arXiv:2101.00522v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>",
          "description": "Human behavior understanding with unmanned aerial vehicles (UAVs) is of great\nsignificance for a wide range of applications, which simultaneously brings an\nurgent demand of large, challenging, and comprehensive benchmarks for the\ndevelopment and evaluation of UAV-based models. However, existing benchmarks\nhave limitations in terms of the amount of captured data, types of data\nmodalities, categories of provided tasks, and diversities of subjects and\nenvironments. Here we propose a new benchmark - UAVHuman - for human behavior\nunderstanding with UAVs, which contains 67,428 multi-modal video sequences and\n119 subjects for action recognition, 22,476 frames for pose estimation, 41,290\nframes and 1,144 identities for person re-identification, and 22,263 frames for\nattribute recognition. Our dataset was collected by a flying UAV in multiple\nurban and rural districts in both daytime and nighttime over three months,\nhence covering extensive diversities w.r.t subjects, backgrounds,\nilluminations, weathers, occlusions, camera motions, and UAV flying attitudes.\nSuch a comprehensive and challenging benchmark shall be able to promote the\nresearch of UAV-based human behavior understanding, including action\nrecognition, pose estimation, re-identification, and attribute recognition.\nFurthermore, we propose a fisheye-based action recognition method that\nmitigates the distortions in fisheye videos via learning unbounded\ntransformations guided by flat RGB videos. Experiments show the efficacy of our\nmethod on the UAV-Human dataset. The project page:\nhttps://github.com/SUTDCV/UAV-Human",
          "link": "http://arxiv.org/abs/2104.00946",
          "publishedOn": "2021-07-13T01:59:35.149Z",
          "wordCount": 707,
          "title": "UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles. (arXiv:2104.00946v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zipeng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>",
          "description": "Modern change detection (CD) has achieved remarkable success by the powerful\ndiscriminative ability of deep convolutions. However, high-resolution remote\nsensing CD remains challenging due to the complexity of objects in the scene.\nObjects with the same semantic concept may show distinct spectral\ncharacteristics at different times and spatial locations. Most recent CD\npipelines using pure convolutions are still struggling to relate long-range\nconcepts in space-time. Non-local self-attention approaches show promising\nperformance via modeling dense relations among pixels, yet are computationally\ninefficient. Here, we propose a bitemporal image transformer (BIT) to\nefficiently and effectively model contexts within the spatial-temporal domain.\nOur intuition is that the high-level concepts of the change of interest can be\nrepresented by a few visual words, i.e., semantic tokens. To achieve this, we\nexpress the bitemporal image into a few tokens, and use a transformer encoder\nto model contexts in the compact token-based space-time. The learned\ncontext-rich tokens are then feedback to the pixel-space for refining the\noriginal features via a transformer decoder. We incorporate BIT in a deep\nfeature differencing-based CD framework. Extensive experiments on three CD\ndatasets demonstrate the effectiveness and efficiency of the proposed method.\nNotably, our BIT-based model significantly outperforms the purely convolutional\nbaseline using only 3 times lower computational costs and model parameters.\nBased on a naive backbone (ResNet18) without sophisticated structures (e.g.,\nFPN, UNet), our model surpasses several state-of-the-art CD methods, including\nbetter than four recent attention-based methods in terms of efficiency and\naccuracy. Our code is available at https://github.com/justchenhao/BIT\\_CD.",
          "link": "http://arxiv.org/abs/2103.00208",
          "publishedOn": "2021-07-13T01:59:35.142Z",
          "wordCount": 731,
          "title": "Remote Sensing Image Change Detection with Transformers. (arXiv:2103.00208v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1\">Sebastian Monka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1\">Lavdim Halilaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1\">Stefan Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>",
          "description": "Traditional computer vision approaches, based on neural networks (NN), are\ntypically trained on a large amount of image data. By minimizing the\ncross-entropy loss between a prediction and a given class label, the NN and its\nvisual embedding space are learned to fulfill a given task. However, due to the\nsole dependence on the image data distribution of the training domain, these\nmodels tend to fail when applied to a target domain that differs from their\nsource domain. To learn a more robust NN to domain shifts, we propose the\nknowledge graph neural network (KG-NN), a neuro-symbolic approach that\nsupervises the training using image-data-invariant auxiliary knowledge. The\nauxiliary knowledge is first encoded in a knowledge graph with respective\nconcepts and their relationships, which is then transformed into a dense vector\nrepresentation via an embedding method. Using a contrastive loss function,\nKG-NN learns to adapt its visual embedding space and thus its weights according\nto the image-data invariant knowledge graph embedding space. We evaluate KG-NN\non visual transfer learning tasks for classification using the mini-ImageNet\ndataset and its derivatives, as well as road sign recognition datasets from\nGermany and China. The results show that a visual model trained with a\nknowledge graph as a trainer outperforms a model trained with cross-entropy in\nall experiments, in particular when the domain gap increases. Besides better\nperformance and stronger robustness to domain shifts, these KG-NN adapts to\nmultiple datasets and classes without suffering heavily from catastrophic\nforgetting.",
          "link": "http://arxiv.org/abs/2102.08747",
          "publishedOn": "2021-07-13T01:59:35.134Z",
          "wordCount": 728,
          "title": "Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yundong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huiye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiang Hu</a>",
          "description": "Medical image segmentation - the prerequisite of numerous clinical needs -\nhas been significantly prospered by recent advances in convolutional neural\nnetworks (CNNs). However, it exhibits general limitations on modeling explicit\nlong-range relation, and existing cures, resorting to building deep encoders\nalong with aggressive downsampling operations, leads to redundant deepened\nnetworks and loss of localized details. Hence, the segmentation task awaits a\nbetter solution to improve the efficiency of modeling global contexts while\nmaintaining a strong grasp of low-level details. In this paper, we propose a\nnovel parallel-in-branch architecture, TransFuse, to address this challenge.\nTransFuse combines Transformers and CNNs in a parallel style, where both global\ndependency and low-level spatial details can be efficiently captured in a much\nshallower manner. Besides, a novel fusion technique - BiFusion module is\ncreated to efficiently fuse the multi-level features from both branches.\nExtensive experiments demonstrate that TransFuse achieves the newest\nstate-of-the-art results on both 2D and 3D medical image sets including polyp,\nskin lesion, hip, and prostate segmentation, with significant parameter\ndecrease and inference speed improvement.",
          "link": "http://arxiv.org/abs/2102.08005",
          "publishedOn": "2021-07-13T01:59:35.127Z",
          "wordCount": 644,
          "title": "TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation. (arXiv:2102.08005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jian Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuge Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Licong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaona Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huabin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Desheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>",
          "description": "Objective: Breast cancer screening is of great significance in contemporary\nwomen's health prevention. The existing machines embedded in the AI system do\nnot reach the accuracy that clinicians hope. How to make intelligent systems\nmore reliable is a common problem. Methods: 1) Ultrasound image\nsuper-resolution: the SRGAN super-resolution network reduces the unclearness of\nultrasound images caused by the device itself and improves the accuracy and\ngeneralization of the detection model. 2) In response to the needs of medical\nimages, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI\nmodel: based on the respective advantages of different AI models, we employ two\nAI models to determine clinical resuls cross validation. And we accept the same\nresults and refuses others. Results: 1) With the help of the super-resolution\nmodel, the YOLOv4 model and the CenterNet model both increased the mAP score by\n9.6% and 13.8%. 2) Two methods for transforming the target model into a\nclassification model are proposed. And the unified output is in a specified\nformat to facilitate the call of the molti-AI model. 3) In the classification\nevaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%,\nspecificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity\n92.54%), the multi-AI model will refuse to make judgments on 23.55% of the\ninput data. Correspondingly, the performance has been greatly improved to\n95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work\nmakes the AI model more reliable in medical image diagnosis. Significance: 1)\nThe proposed method makes the target detection model more suitable for\ndiagnosing breast ultrasound images. 2) It provides a new idea for artificial\nintelligence in medical diagnosis, which can more conveniently introduce target\ndetection models from other fields to serve medical lesion screening.",
          "link": "http://arxiv.org/abs/2101.02639",
          "publishedOn": "2021-07-13T01:59:35.121Z",
          "wordCount": 774,
          "title": "More Reliable AI Solution: Breast Ultrasound Diagnosis Using Multi-AI Combination. (arXiv:2101.02639v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaojie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Real-time surgical phase recognition is a fundamental task in modern\noperating rooms. Previous works tackle this task relying on architectures\narranged in spatio-temporal order, however, the supportive benefits of\nintermediate spatial features are not considered. In this paper, we introduce,\nfor the first time in surgical workflow analysis, Transformer to reconsider the\nignored complementary effects of spatial and temporal features for accurate\nsurgical phase recognition. Our hybrid embedding aggregation Transformer fuses\ncleverly designed spatial and temporal embeddings by allowing for active\nqueries based on spatial information from temporal embedding sequences. More\nimportantly, our framework processes the hybrid embeddings in parallel to\nachieve a high inference speed. Our method is thoroughly validated on two large\nsurgical video datasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and\noutperforms the state-of-the-art approaches at a processing speed of 91 fps.",
          "link": "http://arxiv.org/abs/2103.09712",
          "publishedOn": "2021-07-13T01:59:35.095Z",
          "wordCount": 616,
          "title": "Trans-SVNet: Accurate Phase Recognition from Surgical Videos via Hybrid Embedding Aggregation Transformer. (arXiv:2103.09712v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Huayi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhimeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1\">Xiaorong Pu</a>",
          "description": "Multi-view clustering is an important research topic due to its capability to\nutilize complementary information from multiple views. However, there are few\nmethods to consider the negative impact caused by certain views with unclear\nclustering structures, resulting in poor multi-view clustering performance. To\naddress this drawback, we propose self-supervised discriminative feature\nlearning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders\nare applied to learn embedded features for each view independently. To leverage\nthe multi-view complementary information, we concatenate all views' embedded\nfeatures to form the global features, which can overcome the negative impact of\nsome views' unclear clustering structures. In a self-supervised manner,\npseudo-labels are obtained to build a unified target distribution to perform\nmulti-view discriminative feature learning. During this process, global\ndiscriminative information can be mined to supervise all views to learn more\ndiscriminative features, which in turn are used to update the target\ndistribution. Besides, this unified target distribution can make SDMVC learn\nconsistent cluster assignments, which accomplishes the clustering consistency\nof multiple views while preserving their features' diversity. Experiments on\nvarious types of multi-view datasets show that SDMVC achieves state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2103.15069",
          "publishedOn": "2021-07-13T01:59:35.075Z",
          "wordCount": 656,
          "title": "Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering. (arXiv:2103.15069v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04708",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jicong Zhang</a>",
          "description": "The success of deep learning methods in medical image segmentation tasks\nusually requires a large amount of labeled data. However, obtaining reliable\nannotations is expensive and time-consuming. Semi-supervised learning has\nattracted much attention in medical image segmentation by taking the advantage\nof unlabeled data which is much easier to acquire. In this paper, we propose a\nnovel dual-task mutual learning framework for semi-supervised medical image\nsegmentation. Our framework can be formulated as an integration of two\nindividual segmentation networks based on two tasks: learning region-based\nshape constraint and learning boundary-based surface mismatch. Different from\nthe one-way transfer between teacher and student networks, an ensemble of\ndual-task students can learn collaboratively and implicitly explore useful\nknowledge from each other during the training process. By jointly learning the\nsegmentation probability maps and signed distance maps of targets, our\nframework can enforce the geometric shape constraint and learn more reliable\ninformation. Experimental results demonstrate that our method achieves\nperformance gains by leveraging unlabeled data and outperforms the\nstate-of-the-art semi-supervised segmentation methods.",
          "link": "http://arxiv.org/abs/2103.04708",
          "publishedOn": "2021-07-13T01:59:35.060Z",
          "wordCount": 631,
          "title": "Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation. (arXiv:2103.04708v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruizhi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1\">Daniel Moyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Miriam Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quigley_K/0/1/0/all/0/1\">Keegan Quigley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkowitz_S/0/1/0/all/0/1\">Seth Berkowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horng_S/0/1/0/all/0/1\">Steven Horng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William M. Wells</a>",
          "description": "We propose and demonstrate a representation learning approach by maximizing\nthe mutual information between local features of images and text. The goal of\nthis approach is to learn useful image representations by taking advantage of\nthe rich information contained in the free text that describes the findings in\nthe image. Our method trains image and text encoders by encouraging the\nresulting representations to exhibit high local mutual information. We make use\nof recent advances in mutual information estimation with neural network\ndiscriminators. We argue that the sum of local mutual information is typically\na lower bound on the global mutual information. Our experimental results in the\ndownstream image classification tasks demonstrate the advantages of using local\nfeatures for image-text representation learning.",
          "link": "http://arxiv.org/abs/2103.04537",
          "publishedOn": "2021-07-13T01:59:35.053Z",
          "wordCount": 615,
          "title": "Multimodal Representation Learning via Maximization of Local Mutual Information. (arXiv:2103.04537v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "Object detection has recently achieved a breakthrough for removing the last\none non-differentiable component in the pipeline, Non-Maximum Suppression\n(NMS), and building up an end-to-end system. However, what makes for its\none-to-one prediction has not been well understood. In this paper, we first\npoint out that one-to-one positive sample assignment is the key factor, while,\none-to-many assignment in previous detectors causes redundant predictions in\ninference. Second, we surprisingly find that even training with one-to-one\nassignment, previous detectors still produce redundant predictions. We identify\nthat classification cost in matching cost is the main ingredient: (1) previous\ndetectors only consider location cost, (2) by additionally introducing\nclassification cost, previous detectors immediately produce one-to-one\nprediction during inference. We introduce the concept of score gap to explore\nthe effect of matching cost. Classification cost enlarges the score gap by\nchoosing positive samples as those of highest score in the training iteration\nand reducing noisy positive samples brought by only location cost. Finally, we\ndemonstrate the advantages of end-to-end object detection on crowded scenes.\nThe code is available at: \\url{https://github.com/PeizeSun/OneNet}.",
          "link": "http://arxiv.org/abs/2012.05780",
          "publishedOn": "2021-07-13T01:59:35.036Z",
          "wordCount": 648,
          "title": "What Makes for End-to-End Object Detection?. (arXiv:2012.05780v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roldao_L/0/1/0/all/0/1\">Luis Roldao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verroust_Blondet_A/0/1/0/all/0/1\">Anne Verroust-Blondet</a>",
          "description": "Semantic Scene Completion (SSC) aims to jointly estimate the complete\ngeometry and semantics of a scene, assuming partial sparse input. In the last\nyears following the multiplication of large-scale 3D datasets, SSC has gained\nsignificant momentum in the research community because it holds unresolved\nchallenges. Specifically, SSC lies in the ambiguous completion of large\nunobserved areas and the weak supervision signal of the ground truth. This led\nto a substantially increasing number of papers on the matter. This survey aims\nto identify, compare and analyze the techniques providing a critical analysis\nof the SSC literature on both methods and datasets. Throughout the paper, we\nprovide an in-depth analysis of the existing works covering all choices made by\nthe authors while highlighting the remaining avenues of research. SSC\nperformance of the SoA on the most popular datasets is also evaluated and\nanalyzed.",
          "link": "http://arxiv.org/abs/2103.07466",
          "publishedOn": "2021-07-13T01:59:35.030Z",
          "wordCount": 616,
          "title": "3D Semantic Scene Completion: a Survey. (arXiv:2103.07466v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desmarais_Y/0/1/0/all/0/1\">Yann Desmarais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottet_D/0/1/0/all/0/1\">Denis Mottet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slangen_P/0/1/0/all/0/1\">Pierre Slangen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesinos_P/0/1/0/all/0/1\">Philippe Montesinos</a>",
          "description": "Human pose estimation is a very active research field, stimulated by its\nimportant applications in robotics, entertainment or health and sports\nsciences, among others. Advances in convolutional networks triggered noticeable\nimprovements in 2D pose estimation, leading modern 3D markerless motion capture\ntechniques to an average error per joint of 20 mm. However, with the\nproliferation of methods, it is becoming increasingly difficult to make an\ninformed choice. Here, we review the leading human pose estimation methods of\nthe past five years, focusing on metrics, benchmarks and method structures. We\npropose a taxonomy based on accuracy, speed and robustness that we use to\nclassify de methods and derive directions for future research.",
          "link": "http://arxiv.org/abs/2010.06449",
          "publishedOn": "2021-07-13T01:59:35.023Z",
          "wordCount": 595,
          "title": "A review of 3D human pose estimation algorithms for markerless motion capture. (arXiv:2010.06449v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Honghong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Caili Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanjun Wang</a>",
          "description": "In spite of the great progress in human motion prediction, it is still a\nchallenging task to predict those aperiodic and complicated motions. We believe\nthat to capture the correlations among human body components is the key to\nunderstand the human motion. In this paper, we propose a novel multiscale graph\nconvolution network (MGCN) to address this problem. Firstly, we design an\nadaptive multiscale interactional encoding module (MIEM) which is composed of\ntwo sub modules: scale transformation module and scale interaction module to\nlearn the human body correlations. Secondly, we apply a coarse-to-fine decoding\nstrategy to decode the motions sequentially. We evaluate our approach on two\nstandard benchmark datasets for human motion prediction: Human3.6M and CMU\nmotion capture dataset. The experiments show that the proposed approach\nachieves the state-of-the-art performance for both short-term and long-term\nprediction especially in those complicated action category.",
          "link": "http://arxiv.org/abs/2103.10674",
          "publishedOn": "2021-07-13T01:59:35.017Z",
          "wordCount": 622,
          "title": "Learning Multiscale Correlations for Human Motion Prediction. (arXiv:2103.10674v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13557",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chao_H/0/1/0/all/0/1\">Hanqing Chao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xuanang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>",
          "description": "The extensive use of medical CT has raised a public concern over the\nradiation dose to the patient. Reducing the radiation dose leads to increased\nCT image noise and artifacts, which can adversely affect not only the\nradiologists judgement but also the performance of downstream medical image\nanalysis tasks. Various low-dose CT denoising methods, especially the recent\ndeep learning based approaches, have produced impressive results. However, the\nexisting denoising methods are all downstream-task-agnostic and neglect the\ndiverse needs of the downstream applications. In this paper, we introduce a\nnovel Task-Oriented Denoising Network (TOD-Net) with a task-oriented loss\nleveraging knowledge from the downstream tasks. Comprehensive empirical\nanalysis shows that the task-oriented loss complements other task agnostic\nlosses by steering the denoiser to enhance the image quality in the task\nrelated regions of interest. Such enhancement in turn brings general boosts on\nthe performance of various methods for the downstream task. The presented work\nmay shed light on the future development of context-aware image denoising\nmethods.",
          "link": "http://arxiv.org/abs/2103.13557",
          "publishedOn": "2021-07-13T01:59:35.010Z",
          "wordCount": 629,
          "title": "Task-Oriented Low-Dose CT Image Denoising. (arXiv:2103.13557v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_H/0/1/0/all/0/1\">Helen M. C. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milot_L/0/1/0/all/0/1\">Laurent Milot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>",
          "description": "Colorectal cancer is one of the most common and lethal cancers and colorectal\ncancer liver metastases (CRLM) is the major cause of death in patients with\ncolorectal cancer. Multifocality occurs frequently in CRLM, but is relatively\nunexplored in CRLM outcome prediction. Most existing clinical and imaging\nbiomarkers do not take the imaging features of all multifocal lesions into\naccount. In this paper, we present an end-to-end autoencoder-based multiple\ninstance neural network (AMINN) for the prediction of survival outcomes in\nmultifocal CRLM patients using radiomic features extracted from\ncontrast-enhanced MRIs. Specifically, we jointly train an autoencoder to\nreconstruct input features and a multiple instance network to make predictions\nby aggregating information from all tumour lesions of a patient. Also, we\nincorporate a two-step normalization technique to improve the training of deep\nneural networks, built on the observation that the distributions of radiomic\nfeatures are almost always severely skewed. Experimental results empirically\nvalidated our hypothesis that incorporating imaging features of all lesions\nimproves outcome prediction for multifocal cancer. The proposed AMINN framework\nachieved an area under the ROC curve (AUC) of 0.70, which is 11.4% higher than\nthe best baseline method. A risk score based on the outputs of AMINN achieved\nsuperior prediction in our multifocal CRLM cohort. The effectiveness of\nincorporating all lesions and applying two-step normalization is demonstrated\nby a series of ablation studies. A Keras implementation of AMINN is released.",
          "link": "http://arxiv.org/abs/2012.06875",
          "publishedOn": "2021-07-13T01:59:34.991Z",
          "wordCount": 712,
          "title": "AMINN: Autoencoder-based Multiple Instance Neural Network Improves Outcome Prediction of Multifocal Liver Metastases. (arXiv:2012.06875v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>",
          "description": "Visual object tracking aims to precisely estimate the bounding box for the\ngiven target, which is a challenging problem due to factors such as deformation\nand occlusion. Many recent trackers adopt the multiple-stage tracking strategy\nto improve the quality of bounding box estimation. These methods first coarsely\nlocate the target and then refine the initial prediction in the following\nstages. However, existing approaches still suffer from limited precision, and\nthe coupling of different stages severely restricts the method's\ntransferability. This work proposes a novel, flexible, and accurate refinement\nmodule called Alpha-Refine (AR), which can significantly improve the base\ntrackers' box estimation quality. By exploring a series of design options, we\nconclude that the key to successful refinement is extracting and maintaining\ndetailed spatial information as much as possible. Following this principle,\nAlpha-Refine adopts a pixel-wise correlation, a corner prediction head, and an\nauxiliary mask head as the core components. Comprehensive experiments on\nTrackingNet, LaSOT, GOT-10K, and VOT2020 benchmarks with multiple base trackers\nshow that our approach significantly improves the base trackers' performance\nwith little extra latency. The proposed Alpha-Refine method leads to a series\nof strengthened trackers, among which the ARSiamRPN (AR strengthened SiamRPNpp)\nand the ARDiMP50 (ARstrengthened DiMP50) achieve good efficiency-precision\ntrade-off, while the ARDiMPsuper (AR strengthened DiMP-super) achieves very\ncompetitive performance at a real-time speed. Code and pretrained models are\navailable at https://github.com/MasterBin-IIAU/AlphaRefine.",
          "link": "http://arxiv.org/abs/2012.06815",
          "publishedOn": "2021-07-13T01:59:34.985Z",
          "wordCount": 713,
          "title": "Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation. (arXiv:2012.06815v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>",
          "description": "Recently, many plug-and-play self-attention modules are proposed to enhance\nthe model generalization by exploiting the internal information of deep\nconvolutional neural networks (CNNs). Previous works lay an emphasis on the\ndesign of attention module for specific functionality, e.g., light-weighted or\ntask-oriented attention. However, they ignore the importance of where to plug\nin the attention module since they connect the modules individually with each\nblock of the entire CNN backbone for granted, leading to incremental\ncomputational cost and number of parameters with the growth of network depth.\nThus, we propose a framework called Efficient Attention Network (EAN) to\nimprove the efficiency for the existing attention modules. In EAN, we leverage\nthe sharing mechanism (Huang et al. 2020) to share the attention module within\nthe backbone and search where to connect the shared attention module via\nreinforcement learning. Finally, we obtain the attention network with sparse\nconnections between the backbone and modules, while (1) maintaining accuracy\n(2) reducing extra parameter increment and (3) accelerating inference.\nExtensive experiments on widely-used benchmarks and popular attention networks\nshow the effectiveness of EAN. Furthermore, we empirically illustrate that our\nEAN has the capacity of transferring to other tasks and capturing the\ninformative features. The code is available at\nhttps://github.com/gbup-group/EAN-efficient-attention-network.",
          "link": "http://arxiv.org/abs/2011.14058",
          "publishedOn": "2021-07-13T01:59:34.979Z",
          "wordCount": 685,
          "title": "Efficient Attention Network: Accelerate Attention by Searching Where to Plug. (arXiv:2011.14058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otsuzuki_T/0/1/0/all/0/1\">Takato Otsuzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Heon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hideaki Hayashi</a>",
          "description": "In convolutional neural network-based character recognition, pooling layers\nplay an important role in dimensionality reduction and deformation\ncompensation. However, their kernel shapes and pooling operations are\nempirically predetermined; typically, a fixed-size square kernel shape and max\npooling operation are used. In this paper, we propose a meta-learning framework\nfor pooling layers. As part of our framework, a parameterized pooling layer is\nproposed in which the kernel shape and pooling operation are trainable using\ntwo parameters, thereby allowing flexible pooling of the input data. We also\npropose a meta-learning algorithm for the parameterized pooling layer, which\nallows us to acquire a suitable pooling layer across multiple tasks. In the\nexperiment, we applied the proposed meta-learning framework to character\nrecognition tasks. The results demonstrate that a pooling layer that is\nsuitable across character recognition tasks was obtained via meta-learning, and\nthe obtained pooling layer improved the performance of the model in both\nfew-shot character recognition and noisy image recognition tasks.",
          "link": "http://arxiv.org/abs/2103.09528",
          "publishedOn": "2021-07-13T01:59:34.972Z",
          "wordCount": 627,
          "title": "Meta-learning of Pooling Layers for Character Recognition. (arXiv:2103.09528v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingquan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_G/0/1/0/all/0/1\">George Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>",
          "description": "Chest X-ray becomes one of the most common medical diagnoses due to its\nnoninvasiveness. The number of chest X-ray images has skyrocketed, but reading\nchest X-rays still have been manually performed by radiologists, which creates\nhuge burnouts and delays. Traditionally, radiomics, as a subfield of radiology\nthat can extract a large number of quantitative features from medical images,\ndemonstrates its potential to facilitate medical imaging diagnosis before the\ndeep learning era. In this paper, we develop an end-to-end framework,\nChexRadiNet, that can utilize the radiomics features to improve the abnormality\nclassification performance. Specifically, ChexRadiNet first applies a\nlight-weight but efficient triplet-attention mechanism to classify the chest\nX-rays and highlight the abnormal regions. Then it uses the generated class\nactivation map to extract radiomic features, which further guides our model to\nlearn more robust image features. After a number of iterations and with the\nhelp of radiomic features, our framework can converge to more accurate image\nregions. We evaluate the ChexRadiNet framework using three public datasets: NIH\nChestX-ray, CheXpert, and MIMIC-CXR. We find that ChexRadiNet outperforms the\nstate-of-the-art on both disease detection (0.843 in AUC) and localization\n(0.679 in T(IoU) = 0.1). We will make the code publicly available at\nhttps://github.com/bionlplab/lung_disease_detection_amia2021, with the hope\nthat this method can facilitate the development of automatic systems with a\nhigher-level understanding of the radiological world.",
          "link": "http://arxiv.org/abs/2011.12506",
          "publishedOn": "2021-07-13T01:59:34.967Z",
          "wordCount": 726,
          "title": "Using Radiomics as Prior Knowledge for Thorax Disease Classification and Localization in Chest X-rays. (arXiv:2011.12506v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bugata_P/0/1/0/all/0/1\">Peter Bugata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drotar_P/0/1/0/all/0/1\">Peter Drotar</a>",
          "description": "Feature selection is important step in machine learning since it has shown to\nimprove prediction accuracy while depressing the curse of dimensionality of\nhigh dimensional data. The neural networks have experienced tremendous success\nin solving many nonlinear learning problems. Here, we propose new\nneural-network based feature selection approach that introduces two constrains,\nthe satisfying of which leads to sparse FS layer. We have performed extensive\nexperiments on synthetic and real world data to evaluate performance of the\nproposed FS. In experiments we focus on the high dimension, low sample size\ndata since those represent the main challenge for feature selection. The\nresults confirm that proposed Feature Selection Based on Sparse Neural Network\nLayer with Normalizing Constraints (SNEL-FS) is able to select the important\nfeatures and yields superior performance compared to other conventional FS\nmethods.",
          "link": "http://arxiv.org/abs/2012.06365",
          "publishedOn": "2021-07-13T01:59:34.949Z",
          "wordCount": 610,
          "title": "Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints. (arXiv:2012.06365v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.11948",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xianhong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arce_G/0/1/0/all/0/1\">Gonzalo R. Arce</a>",
          "description": "Hyperspectral image classification (HIC) is an active research topic in\nremote sensing. Hyperspectral images typically generate large data cubes posing\nbig challenges in data acquisition, storage, transmission and processing. To\novercome these limitations, this paper develops a novel deep learning HIC\napproach based on compressive measurements of coded-aperture snapshot spectral\nimagers (CASSI), without reconstructing the complete hyperspectral data cube. A\nnew kind of deep learning strategy, namely 3D coded convolutional neural\nnetwork (3D-CCNN) is proposed to efficiently solve for the classification\nproblem, where the hardware-based coded aperture is regarded as a pixel-wise\nconnected network layer. An end-to-end training method is developed to jointly\noptimize the network parameters and the coded apertures with periodic\nstructures. The accuracy of classification is effectively improved by\nexploiting the synergy between the deep learning network and coded apertures.\nThe superiority of the proposed method is assessed over the state-of-the-art\nHIC methods on several hyperspectral datasets.",
          "link": "http://arxiv.org/abs/2009.11948",
          "publishedOn": "2021-07-13T01:59:34.943Z",
          "wordCount": 625,
          "title": "Compressive spectral image classification using 3D coded convolutional neural network. (arXiv:2009.11948v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Suncheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuzhuo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1\">Mengyuan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>",
          "description": "Employing clustering strategy to assign unlabeled target images with pseudo\nlabels has become a trend for person re-identification (re-ID) algorithms in\ndomain adaptation. A potential limitation of these clustering-based methods is\nthat they always tend to introduce noisy labels, which will undoubtedly hamper\nthe performance of our re-ID system. To handle this limitation, an intuitive\nsolution is to utilize collaborative training to purify the pseudo label\nquality. However, there exists a challenge that the complementarity of two\nnetworks, which inevitably share a high similarity, becomes weakened gradually\nas training process goes on; worse still, these approaches typically ignore to\nconsider the self-discrepancy of intra-class relations. To address this issue,\nin this paper, we propose a multiple co-teaching framework for domain adaptive\nperson re-ID, opening up a promising direction about self-discrepancy problem\nunder unsupervised condition. On top of that, a mean-teaching mechanism is\nleveraged to enlarge the difference and discover more complementary features.\nComprehensive experiments conducted on several large-scale datasets show that\nour method achieves competitive performance compared with the\nstate-of-the-arts.",
          "link": "http://arxiv.org/abs/2104.02265",
          "publishedOn": "2021-07-13T01:59:34.936Z",
          "wordCount": 636,
          "title": "Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoxiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a novel method for multi-view depth estimation from a single\nvideo, which is a critical task in various applications, such as perception,\nreconstruction and robot navigation. Although previous learning-based methods\nhave demonstrated compelling results, most works estimate depth maps of\nindividual video frames independently, without taking into consideration the\nstrong geometric and temporal coherence among the frames. Moreover, current\nstate-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for\ncost regularization and therefore require high computational cost, thus\nlimiting their deployment in real-world applications. Our method achieves\ntemporally coherent depth estimation results by using a novel Epipolar\nSpatio-Temporal (EST) transformer to explicitly associate geometric and\ntemporal correlation with multiple estimated depth maps. Furthermore, to reduce\nthe computational cost, inspired by recent Mixture-of-Experts models, we design\na compact hybrid network consisting of a 2D context-aware network and a 3D\nmatching network which learn 2D context information and 3D disparity cues\nseparately. Extensive experiments demonstrate that our method achieves higher\naccuracy in depth estimation and significant speedup than the SOTA methods.",
          "link": "http://arxiv.org/abs/2011.13118",
          "publishedOn": "2021-07-13T01:59:34.929Z",
          "wordCount": 646,
          "title": "Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks. (arXiv:2011.13118v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Variational autoencoders (VAEs) provide an effective and simple method for\nmodeling complex distributions. However, training VAEs often requires\nconsiderable hyperparameter tuning to determine the optimal amount of\ninformation retained by the latent variable. We study the impact of calibrated\ndecoders, which learn the uncertainty of the decoding distribution and can\ndetermine this amount of information automatically, on the VAE performance.\nWhile many methods for learning calibrated decoders have been proposed, many of\nthe recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc\nmodifications instead. We perform the first comprehensive comparative analysis\nof calibrated decoder and provide recommendations for simple and effective VAE\ntraining. Our analysis covers a range of image and video datasets and several\nsingle-image and sequential VAE models. We further propose a simple but novel\nmodification to the commonly used Gaussian decoder, which computes the\nprediction variance analytically. We observe empirically that using heuristic\nmodifications is not necessary with our method. Project website is at\nhttps://orybkin.github.io/sigma-vae/",
          "link": "http://arxiv.org/abs/2006.13202",
          "publishedOn": "2021-07-13T01:59:34.921Z",
          "wordCount": 661,
          "title": "Simple and Effective VAE Training with Calibrated Decoders. (arXiv:2006.13202v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaming Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "The past decade has witnessed significant progress on detecting objects in\naerial images that are often distributed with large scale variations and\narbitrary orientations. However most of existing methods rely on heuristically\ndefined anchors with different scales, angles and aspect ratios and usually\nsuffer from severe misalignment between anchor boxes and axis-aligned\nconvolutional features, which leads to the common inconsistency between the\nclassification score and localization accuracy. To address this issue, we\npropose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules:\na Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The\nFAM can generate high-quality anchors with an Anchor Refinement Network and\nadaptively align the convolutional features according to the anchor boxes with\na novel Alignment Convolution. The ODM first adopts active rotating filters to\nencode the orientation information and then produces orientation-sensitive and\norientation-invariant features to alleviate the inconsistency between\nclassification score and localization accuracy. Besides, we further explore the\napproach to detect objects in large-size images, which leads to a better\ntrade-off between speed and accuracy. Extensive experiments demonstrate that\nour method can achieve state-of-the-art performance on two commonly used aerial\nobjects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The\ncode is available at https://github.com/csuhan/s2anet.",
          "link": "http://arxiv.org/abs/2008.09397",
          "publishedOn": "2021-07-13T01:59:34.903Z",
          "wordCount": 675,
          "title": "Align Deep Features for Oriented Object Detection. (arXiv:2008.09397v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>",
          "description": "We present a new learning-based framework to recover vehicle pose in SO(3)\nfrom a single RGB image. In contrast to previous works that map from local\nappearance to observation angles, we explore a progressive approach by\nextracting meaningful Intermediate Geometrical Representations (IGRs) to\nestimate egocentric vehicle orientation. This approach features a deep model\nthat transforms perceived intensities to IGRs, which are mapped to a 3D\nrepresentation encoding object orientation in the camera coordinate system.\nCore problems are what IGRs to use and how to learn them more effectively. We\nanswer the former question by designing IGRs based on an interpolated cuboid\nthat derives from primitive 3D annotation readily. The latter question\nmotivates us to incorporate geometry knowledge with a new loss function based\non a projective invariant. This loss function allows unlabeled data to be used\nin the training stage to improve representation learning. Without additional\nlabels, our system outperforms previous monocular RGB-based methods for joint\nvehicle detection and pose estimation on the KITTI benchmark, achieving\nperformance even comparable to stereo methods. Code and pre-trained models are\navailable at this https URL.",
          "link": "http://arxiv.org/abs/2011.08464",
          "publishedOn": "2021-07-13T01:59:34.897Z",
          "wordCount": 687,
          "title": "Exploring intermediate representation for monocular vehicle pose estimation. (arXiv:2011.08464v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1\">Stephen J. Maybank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Extracting accurate foregrounds from natural images benefits many downstream\napplications such as film production and augmented reality. However, the furry\ncharacteristics and various appearance of the foregrounds, e.g., animal and\nportrait, challenge existing matting methods, which usually require extra user\ninputs such as trimap or scribbles. To resolve these problems, we study the\ndistinct roles of semantics and details for image matting and decompose the\ntask into two parallel sub-tasks: high-level semantic segmentation and\nlow-level details matting. Specifically, we propose a novel Glance and Focus\nMatting network (GFM), which employs a shared encoder and two separate decoders\nto learn both tasks in a collaborative manner for end-to-end natural image\nmatting. Besides, due to the limitation of available natural images in the\nmatting task, previous methods typically adopt composite images for training\nand evaluation, which result in limited generalization ability on real-world\nimages. In this paper, we investigate the domain gap issue between composite\nimages and real-world images systematically by conducting comprehensive\nanalyses of various discrepancies between foreground and background images. We\nfind that a carefully designed composition route RSSN that aims to reduce the\ndiscrepancies can lead to a better model with remarkable generalization\nability. Furthermore, we provide a benchmark containing 2,000 high-resolution\nreal-world animal images and 10,000 portrait images along with their manually\nlabeled alpha mattes to serve as a test bed for evaluating matting model's\ngeneralization ability on real-world images. Comprehensive empirical studies\nhave demonstrated that GFM outperforms state-of-the-art methods and effectively\nreduces the generalization error. The code and the dataset will be released.",
          "link": "http://arxiv.org/abs/2010.16188",
          "publishedOn": "2021-07-13T01:59:34.814Z",
          "wordCount": 749,
          "title": "Bridge Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miro_Nicolau_M/0/1/0/all/0/1\">Miquel Mir&#xf3;-Nicolau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moya_Alcover_B/0/1/0/all/0/1\">Biel Moy&#xe0;-Alcover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Hidalgo_M/0/1/0/all/0/1\">Manuel Gonz&#xe1;lez-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaume_i_Capo_A/0/1/0/all/0/1\">Antoni Jaume-i-Cap&#xf3;</a>",
          "description": "In this paper we propose a method to detect concave points as a first step to\nsegment overlapped objects on images. Given an image of an object cluster we\ncompute the curvature on each point of its contour. Then, we select regions\nwith the highest probability to contain an interest point, that is, regions\nwith higher curvature. Finally we obtain an interest point from each region and\nwe classify them between convex and concave. In order to evaluate the quality\nof the concave point detection algorithm we constructed a synthetic dataset to\nsimulate overlapping objects, providing the position of the concave points as a\nground truth. As a case study, the performance of a well-known application is\nevaluated, such as the splitting of overlapped cells in images of peripheral\nblood smears samples of patients with sickle cell anaemia. We used the proposed\nmethod to detect the concave points in clusters of cells and then we separate\nthis clusters by ellipse fitting. Experimentally we demonstrate that our\nproposal has a better performance than the state-of-the-art.",
          "link": "http://arxiv.org/abs/2008.00997",
          "publishedOn": "2021-07-13T01:59:34.807Z",
          "wordCount": 651,
          "title": "Segmenting overlapped cell clusters in biomedical images by concave point detection. (arXiv:2008.00997v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Di Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harakeh_A/0/1/0/all/0/1\">Ali Harakeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven Waslander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>",
          "description": "Capturing uncertainty in object detection is indispensable for safe\nautonomous driving. In recent years, deep learning has become the de-facto\napproach for object detection, and many probabilistic object detectors have\nbeen proposed. However, there is no summary on uncertainty estimation in deep\nobject detection, and existing methods are not only built with different\nnetwork architectures and uncertainty estimation methods, but also evaluated on\ndifferent datasets with a wide range of evaluation metrics. As a result, a\ncomparison among methods remains challenging, as does the selection of a model\nthat best suits a particular application. This paper aims to alleviate this\nproblem by providing a review and comparative study on existing probabilistic\nobject detection methods for autonomous driving applications. First, we provide\nan overview of generic uncertainty estimation in deep learning, and then\nsystematically survey existing methods and evaluation metrics for probabilistic\nobject detection. Next, we present a strict comparative study for probabilistic\nobject detection based on an image detector and three public autonomous driving\ndatasets. Finally, we present a discussion of the remaining challenges and\nfuture works. Code has been made available at\nhttps://github.com/asharakeh/pod_compare.git",
          "link": "http://arxiv.org/abs/2011.10671",
          "publishedOn": "2021-07-13T01:59:34.800Z",
          "wordCount": 674,
          "title": "A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving. (arXiv:2011.10671v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.13373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaeseok Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yeji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Data augmentation has greatly contributed to improving the performance in\nimage recognition tasks, and a lot of related studies have been conducted.\nHowever, data augmentation on 3D point cloud data has not been much explored.\n3D label has more sophisticated and rich structural information than the 2D\nlabel, so it enables more diverse and effective data augmentation. In this\npaper, we propose part-aware data augmentation (PA-AUG) that can better utilize\nrich information of 3D label to enhance the performance of 3D object detectors.\nPA-AUG divides objects into partitions and stochastically applies five\naugmentation methods to each local region. It is compatible with existing point\ncloud data augmentation methods and can be used universally regardless of the\ndetector's architecture. PA-AUG has improved the performance of\nstate-of-the-art 3D object detector for all classes of the KITTI dataset and\nhas the equivalent effect of increasing the train data by about 2.5$\\times$. We\nalso show that PA-AUG not only increases performance for a given dataset but\nalso is robust to corrupted data. The code is available at\nhttps://github.com/sky77764/pa-aug.pytorch",
          "link": "http://arxiv.org/abs/2007.13373",
          "publishedOn": "2021-07-13T01:59:34.782Z",
          "wordCount": 650,
          "title": "Part-Aware Data Augmentation for 3D Object Detection in Point Cloud. (arXiv:2007.13373v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tete Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mintun_E/0/1/0/all/0/1\">Eric Mintun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1\">Piotr Doll&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1\">Ross Girshick</a>",
          "description": "Vision transformer (ViT) models exhibit substandard optimizability. In\nparticular, they are sensitive to the choice of optimizer (AdamW vs. SGD),\noptimizer hyperparameters, and training schedule length. In comparison, modern\nconvolutional neural networks are far easier to optimize. Why is this the case?\nIn this work, we conjecture that the issue lies with the patchify stem of ViT\nmodels, which is implemented by a stride-p pxp convolution (p=16 by default)\napplied to the input image. This large-kernel plus large-stride convolution\nruns counter to typical design choices of convolutional layers in neural\nnetworks. To test whether this atypical design choice causes an issue, we\nanalyze the optimization behavior of ViT models with their original patchify\nstem versus a simple counterpart where we replace the ViT stem by a small\nnumber of stacked stride-two 3x3 convolutions. While the vast majority of\ncomputation in the two ViT designs is identical, we find that this small change\nin early visual processing results in markedly different training behavior in\nterms of the sensitivity to optimization settings as well as the final model\naccuracy. Using a convolutional stem in ViT dramatically increases optimization\nstability and also improves peak performance (by ~1-2% top-1 accuracy on\nImageNet-1k), while maintaining flops and runtime. The improvement can be\nobserved across the wide spectrum of model complexities (from 1G to 36G flops)\nand dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us\nto recommend using a standard, lightweight convolutional stem for ViT models as\na more robust architectural choice compared to the original ViT model design.",
          "link": "http://arxiv.org/abs/2106.14881",
          "publishedOn": "2021-07-13T01:59:34.770Z",
          "wordCount": 721,
          "title": "Early Convolutions Help Transformers See Better. (arXiv:2106.14881v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.00422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_D/0/1/0/all/0/1\">Deepak Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_V/0/1/0/all/0/1\">Vivek Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulluri_T/0/1/0/all/0/1\">Tarun Pulluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ussa_A/0/1/0/all/0/1\">Andres Ussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_P/0/1/0/all/0/1\">Pradeep Kumar Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_B/0/1/0/all/0/1\">Bharath Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Arindam Basu</a>",
          "description": "Neuromorphic vision sensors (NVS) have been recently explored to tackle\nscenarios where conventional sensors result in high data rate and processing\ntime. This paper presents a hybrid event-frame approach for detecting and\ntracking objects recorded by a stationary neuromorphic sensor, thereby\nexploiting the sparse NVS output in a low-power setting for traffic monitoring.\nSpecifically, we propose a hardware efficient processing pipeline that\noptimizes memory and computational needs. The usage of NVS gives the advantage\nof rejecting background while it has a unique disadvantage of fragmented\nobjects. To exploit the background removal, we propose an event-based binary\nimage creation that signals presence or absence of events in a frame duration.\nThis reduces memory requirement and enables usage of simple algorithms like\nmedian filtering and connected component labeling for denoise and region\nproposal respectively. To overcome the fragmentation issue, a YOLO-inspired\nneural network based detector and classifier to merge fragmented region\nproposals has been proposed. Finally, an overlap based tracker exploiting\noverlap between detections and tracks is proposed with heuristics to overcome\nocclusion. The proposed pipeline is evaluated with more than 5 hours of traffic\nrecording spanning three different locations on two different NVS and\ndemonstrate similar performance. Compared to existing event-based feature\ntrackers, our method provides similar accuracy while needing 6 times less\ncomputes. To the best of our knowledge, this is the first time a stationary NVS\nbased traffic monitoring solution is extensively compared to simultaneously\nrecorded RGB frame-methods while showing tremendous promise by outperforming\nstate-of-the-art deep learning solutions.",
          "link": "http://arxiv.org/abs/2006.00422",
          "publishedOn": "2021-07-13T01:59:34.740Z",
          "wordCount": 740,
          "title": "EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Neuromorphic Vision Sensors. (arXiv:2006.00422v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09646",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this paper, we present a novel network for high resolution video\ngeneration. Our network uses ideas from Wasserstein GANs by enforcing\nk-Lipschitz constraint on the loss term and Conditional GANs using class labels\nfor training and testing. We present Generator and Discriminator network\nlayerwise details along with the combined network architecture, optimization\ndetails and algorithm used in this work. Our network uses a combination of two\nloss terms: mean square pixel loss and an adversarial loss. The datasets used\nfor training and testing our network are UCF101, Golf and Aeroplane Datasets.\nUsing Inception Score and Fr\\'echet Inception Distance as the evaluation\nmetrics, our network outperforms previous state of the art networks on\nunsupervised video generation.",
          "link": "http://arxiv.org/abs/2008.09646",
          "publishedOn": "2021-07-13T01:59:34.734Z",
          "wordCount": 596,
          "title": "HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN. (arXiv:2008.09646v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.00845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoxiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a new learning-based method for multi-frame depth estimation from\na color video, which is a fundamental problem in scene understanding, robot\nnavigation or handheld 3D reconstruction. While recent learning-based methods\nestimate depth at high accuracy, 3D point clouds exported from their depth maps\noften fail to preserve important geometric feature (e.g., corners, edges,\nplanes) of man-made scenes. Widely-used pixel-wise depth errors do not\nspecifically penalize inconsistency on these features. These inaccuracies are\nparticularly severe when subsequent depth reconstructions are accumulated in an\nattempt to scan a full environment with man-made objects with this kind of\nfeatures. Our depth estimation algorithm therefore introduces a Combined Normal\nMap (CNM) constraint, which is designed to better preserve high-curvature\nfeatures and global planar regions. In order to further improve the depth\nestimation accuracy, we introduce a new occlusion-aware strategy that\naggregates initial depth predictions from multiple adjacent views into one\nfinal depth map and one occlusion probability map for the current reference\nview. Our method outperforms the state-of-the-art in terms of depth estimation\naccuracy, and preserves essential geometric features of man-made indoor scenes\nmuch better than other algorithms.",
          "link": "http://arxiv.org/abs/2004.00845",
          "publishedOn": "2021-07-13T01:59:34.728Z",
          "wordCount": 684,
          "title": "Occlusion-Aware Depth Estimation with Adaptive Normal Constraints. (arXiv:2004.00845v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12931",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_T/0/1/0/all/0/1\">Tashin Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabab_N/0/1/0/all/0/1\">Noor Hossain Nuri Sabab</a>",
          "description": "Climate change has been a common interest and the forefront of crucial\npolitical discussion and decision-making for many years. Shallow clouds play a\nsignificant role in understanding the Earth's climate, but they are challenging\nto interpret and represent in a climate model. By classifying these cloud\nstructures, there is a better possibility of understanding the physical\nstructures of the clouds, which would improve the climate model generation,\nresulting in a better prediction of climate change or forecasting weather\nupdate. Clouds organise in many forms, which makes it challenging to build\ntraditional rule-based algorithms to separate cloud features. In this paper,\nclassification of cloud organization patterns was performed using a new\nscaled-up version of Convolutional Neural Network (CNN) named as EfficientNet\nas the encoder and UNet as decoder where they worked as feature extractor and\nreconstructor of fine grained feature map and was used as a classifier, which\nwill help experts to understand how clouds will shape the future climate. By\nusing a segmentation model in a classification task, it was shown that with a\ngood encoder alongside UNet, it is possible to obtain good performance from\nthis dataset. Dice coefficient has been used for the final evaluation metric,\nwhich gave the score of 66.26\\% and 66.02\\% for public and private (test set)\nleaderboard on Kaggle competition respectively.",
          "link": "http://arxiv.org/abs/2009.12931",
          "publishedOn": "2021-07-13T01:59:34.716Z",
          "wordCount": 708,
          "title": "Classification and understanding of cloud structures via satellite images with EfficientUNet. (arXiv:2009.12931v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.13200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Compared with cheap addition operation, multiplication operation is of much\nhigher computation complexity. The widely-used convolutions in deep neural\nnetworks are exactly cross-correlation to measure the similarity between input\nfeature and convolution filters, which involves massive multiplications between\nfloat values. In this paper, we present adder networks (AdderNets) to trade\nthese massive multiplications in deep neural networks, especially convolutional\nneural networks (CNNs), for much cheaper additions to reduce computation costs.\nIn AdderNets, we take the $\\ell_1$-norm distance between filters and input\nfeature as the output response. The influence of this new similarity measure on\nthe optimization of neural network have been thoroughly analyzed. To achieve a\nbetter performance, we develop a special back-propagation approach for\nAdderNets by investigating the full-precision gradient. We then propose an\nadaptive learning rate strategy to enhance the training procedure of AdderNets\naccording to the magnitude of each neuron's gradient. As a result, the proposed\nAdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50\non the ImageNet dataset without any multiplication in convolution layer. The\ncodes are publicly available at: https://github.com/huaweinoah/AdderNet.",
          "link": "http://arxiv.org/abs/1912.13200",
          "publishedOn": "2021-07-13T01:59:34.692Z",
          "wordCount": 699,
          "title": "AdderNet: Do We Really Need Multiplications in Deep Learning?. (arXiv:1912.13200v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.14536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>",
          "description": "It is commonly believed that networks cannot be both accurate and robust,\nthat gaining robustness means losing accuracy. It is also generally believed\nthat, unless making networks larger, network architectural elements would\notherwise matter little in improving adversarial robustness. Here we present\nevidence to challenge these common beliefs by a careful study about adversarial\ntraining. Our key observation is that the widely-used ReLU activation function\nsignificantly weakens adversarial training due to its non-smooth nature. Hence\nwe propose smooth adversarial training (SAT), in which we replace ReLU with its\nsmooth approximations to strengthen adversarial training. The purpose of smooth\nactivation functions in SAT is to allow it to find harder adversarial examples\nand compute better gradient updates during adversarial training.\n\nCompared to standard adversarial training, SAT improves adversarial\nrobustness for \"free\", i.e., no drop in accuracy and no increase in\ncomputational cost. For example, without introducing additional computations,\nSAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while\nalso improving accuracy by 0.9% on ImageNet. SAT also works well with larger\nnetworks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%\nrobustness on ImageNet, outperforming the previous state-of-the-art defense by\n9.5% for accuracy and 11.6% for robustness. Models are available at\nhttps://github.com/cihangxie/SmoothAdversarialTraining.",
          "link": "http://arxiv.org/abs/2006.14536",
          "publishedOn": "2021-07-13T01:59:34.656Z",
          "wordCount": 674,
          "title": "Smooth Adversarial Training. (arXiv:2006.14536v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chenxu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "To encourage AI agents to conduct meaningful Visual Dialogue (VD), the use of\nReinforcement Learning has been proven potential. In Reinforcement Learning, it\nis crucial to represent states and assign rewards based on the action-caused\ntransitions of states. However, the state representation in previous Visual\nDialogue works uses the textual information only and its transitions are\nimplicit. In this paper, we propose Explicit Concerning States (ECS) to\nrepresent what visual contents are concerned at each round and what have been\nconcerned throughout the Visual Dialogue. ECS is modeled from multimodal\ninformation and is represented explicitly. Based on ECS, we formulate two\nintuitive and interpretable rewards to encourage the Visual Dialogue agents to\nconverse on diverse and informative visual information. Experimental results on\nthe VisDial v1.0 dataset show our method enables the Visual Dialogue agents to\ngenerate more visual coherent, less repetitive and more visual informative\ndialogues compared with previous methods, according to multiple automatic\nmetrics, human study and qualitative analysis.",
          "link": "http://arxiv.org/abs/2107.05250",
          "publishedOn": "2021-07-13T01:59:34.647Z",
          "wordCount": 604,
          "title": "Modeling Explicit Concerning States for Reinforcement Learning in Visual Dialogue. (arXiv:2107.05250v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanpeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">He Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yongming Tang</a>",
          "description": "Video super-resolution (VSR) technology excels in reconstructing low-quality\nvideo, avoiding unpleasant blur effect caused by interpolation-based\nalgorithms. However, vast computation complexity and memory occupation hampers\nthe edge of deplorability and the runtime inference in real-life applications,\nespecially for large-scale VSR task. This paper explores the possibility of\nreal-time VSR system and designs an efficient and generic VSR network, termed\nEGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for\ntemporal coherence. In order to pursue faster VSR processing ability up to 4K\nresolution, this paper tries to choose lightweight network structure and\nefficient upsampling method to reduce the computation required by EGVSR network\nunder the guarantee of high visual quality. Besides, we implement the batch\nnormalization computation fusion, convolutional acceleration algorithm and\nother neural network acceleration techniques on the actual hardware platform to\noptimize the inference process of EGVSR network. Finally, our EGVSR achieves\nthe real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the\nmost advanced VSR network at present, we achieve 85.04% reduction of\ncomputation density and 7.92x performance speedups. In terms of visual quality,\nthe proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP,\netc.) on the public test dataset Vid4 and surpasses other state-of-the-art\nmethods in overall performance score. The source code of this project can be\nfound on https://github.com/Thmen/EGVSR.",
          "link": "http://arxiv.org/abs/2107.05307",
          "publishedOn": "2021-07-13T01:59:34.634Z",
          "wordCount": 676,
          "title": "Real-Time Super-Resolution System of 4K-Video Based on Deep Learning. (arXiv:2107.05307v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1\">Ross Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1\">Pujitha Gunaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "With increasing automation in passenger vehicles, the study of safe and\nsmooth occupant-vehicle interaction and control transitions is key. In this\nstudy, we focus on the development of contextual, semantically meaningful\nrepresentations of the driver state, which can then be used to determine the\nappropriate timing and conditions for transfer of control between driver and\nvehicle. To this end, we conduct a large-scale real-world controlled data study\nwhere participants are instructed to take-over control from an autonomous agent\nunder different driving conditions while engaged in a variety of distracting\nactivities. These take-over events are captured using multiple driver-facing\ncameras, which when labelled result in a dataset of control transitions and\ntheir corresponding take-over times (TOTs). We then develop and train TOT\nmodels that operate sequentially on mid to high-level features produced by\ncomputer vision algorithms operating on different driver-facing camera views.\nThe proposed TOT model produces continuous estimates of take-over times without\ndelay, and shows promising qualitative and quantitative results in complex\nreal-world scenarios.",
          "link": "http://arxiv.org/abs/2104.11489",
          "publishedOn": "2021-07-13T01:59:34.627Z",
          "wordCount": 639,
          "title": "Autonomous Vehicles that Alert Humans to Take-Over Controls: Modeling with Real-World Data. (arXiv:2104.11489v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sallab_A/0/1/0/all/0/1\">Ahmad El Sallab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Moving object Detection (MOD) is a critical task in autonomous driving as\nmoving agents around the ego-vehicle need to be accurately detected for safe\ntrajectory planning. It also enables appearance agnostic detection of objects\nbased on motion cues. There are geometric challenges like motion-parallax\nambiguity which makes it a difficult problem. In this work, we aim to leverage\nthe vehicle motion information and feed it into the model to have an adaptation\nmechanism based on ego-motion. The motivation is to enable the model to\nimplicitly perform ego-motion compensation to improve performance. We convert\nthe six degrees of freedom vehicle motion into a pixel-wise tensor which can be\nfed as input to the CNN model. The proposed model using Vehicle Motion Tensor\n(VMT) achieves an absolute improvement of 5.6% in mIoU over the baseline\narchitecture. We also achieve state-of-the-art results on the public\nKITTI_MoSeg_Extended dataset even compared to methods which make use of LiDAR\nand additional input frames. Our model is also lightweight and runs at 85 fps\non a TitanX GPU. Qualitative results are provided in\nhttps://youtu.be/ezbfjti-kTk.",
          "link": "http://arxiv.org/abs/2104.10985",
          "publishedOn": "2021-07-13T01:59:34.609Z",
          "wordCount": 659,
          "title": "VM-MODNet: Vehicle Motion aware Moving Object Detection for Autonomous Driving. (arXiv:2104.10985v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>",
          "description": "We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n\nHaving all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n\nWe solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article. Just like for linear convolution a\nmorphological convolution is specified by a kernel that we train in our\nPDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling\nand ReLUs as they are already subsumed by morphological convolutions.\n\nWe present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.",
          "link": "http://arxiv.org/abs/2001.09046",
          "publishedOn": "2021-07-13T01:59:34.603Z",
          "wordCount": 751,
          "title": "PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loo_S/0/1/0/all/0/1\">Shing Yan Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mashohor_S/0/1/0/all/0/1\">Syamsiah Mashohor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sai Hong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>",
          "description": "In this paper, we propose a dense monocular SLAM system, named\nDeepRelativeFusion, that is capable to recover a globally consistent 3D\nstructure. To this end, we use a visual SLAM algorithm to reliably recover the\ncamera poses and semi-dense depth maps of the keyframes, and then use relative\ndepth prediction to densify the semi-dense depth maps and refine the keyframe\npose-graph. To improve the semi-dense depth maps, we propose an adaptive\nfiltering scheme, which is a structure-preserving weighted average smoothing\nfilter that takes into account the pixel intensity and depth of the\nneighbouring pixels, yielding substantial reconstruction accuracy gain in\ndensification. To perform densification, we introduce two incremental\nimprovements upon the energy minimization framework proposed by DeepFusion: (1)\nan improved cost function, and (2) the use of single-image relative depth\nprediction. After densification, we update the keyframes with two-view\nconsistent optimized semi-dense and dense depth maps to improve pose-graph\noptimization, providing a feedback loop to refine the keyframe poses for\naccurate scene reconstruction. Our system outperforms the state-of-the-art\ndense SLAM systems quantitatively in dense reconstruction accuracy by a large\nmargin.",
          "link": "http://arxiv.org/abs/2006.04047",
          "publishedOn": "2021-07-13T01:59:34.584Z",
          "wordCount": 686,
          "title": "DeepRelativeFusion: Dense Monocular SLAM using Single-Image Relative Depth Prediction. (arXiv:2006.04047v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>",
          "description": "The task of table structure recognition is to recognize the internal\nstructure of a table, which is a key step to make machines understand tables.\nHowever, tabular data in unstructured digital documents, e.g. Portable Document\nFormat (PDF) and images, are difficult to parse into structured\nmachine-readable format, due to complexity and diversity in their structure and\nstyle, especially for complex tables. In this paper, we introduce Split, Embed\nand Merge (SEM), an accurate table structure recognizer. In the first stage, we\nuse the FCN to predict the potential regions of the table row (column)\nseparators, so as to obtain the bounding boxes of the basic grids in the table.\nIn the second stage, we not only extract the visual features corresponding to\neach grid through RoIAlign, but also use the off-the-shelf recognizer and the\nBERT to extract the semantic features. The fused features of both are used to\ncharacterize each table grid. We find that by adding additional semantic\nfeatures to each grid, the ambiguity problem of the table structure from the\nvisual perspective can be solved to a certain extent and achieve higher\nprecision. Finally, we process the merging of these basic grids in a\nself-regression manner. The correspondent merging results is learned by the\nattention maps in attention mechanism. With the proposed method, we can\nrecognize the structure of tables well, even for complex tables. SEM can\nachieve an average F-Measure of $96.9\\%$ on the SciTSR dataset which\noutperforms other methods by a large margin. Extensive experiments on other\npublicly available table structure recognition datasets show that our model\nachieves state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.05214",
          "publishedOn": "2021-07-13T01:59:34.578Z",
          "wordCount": 698,
          "title": "Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.01751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_K/0/1/0/all/0/1\">Keiller Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mura_M/0/1/0/all/0/1\">Mauro Dalla Mura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "The recent impressive results of deep learning-based methods on computer\nvision applications brought fresh air to the research and industrial community.\nThis success is mainly due to the process that allows those methods to learn\ndata-driven features, generally based upon linear operations. However, in some\nscenarios, such operations do not have a good performance because of their\ninherited process that blurs edges, losing notions of corners, borders, and\ngeometry of objects. Overcoming this, non-linear operations, such as\nmorphological ones, may preserve such properties of the objects, being\npreferable and even state-of-the-art in some applications. Encouraged by this,\nin this work, we propose a novel network, called Deep Morphological Network\n(DeepMorphNet), capable of doing non-linear morphological operations while\nperforming the feature learning process by optimizing the structuring elements.\nThe DeepMorphNets can be trained and optimized end-to-end using traditional\nexisting techniques commonly employed in the training of deep learning\napproaches. A systematic evaluation of the proposed algorithm is conducted\nusing two synthetic and two traditional image classification datasets. Results\nshow that the proposed DeepMorphNets is a promising technique that can learn\ndistinct features when compared to the ones learned by current deep learning\nmethods.",
          "link": "http://arxiv.org/abs/1906.01751",
          "publishedOn": "2021-07-13T01:59:34.539Z",
          "wordCount": 661,
          "title": "An Introduction to Deep Morphological Networks. (arXiv:1906.01751v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.04573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>",
          "description": "We introduce a method for simultaneously classifying, segmenting and tracking\nobject instances in a video sequence. Our method, named MaskProp, adapts the\npopular Mask R-CNN to video by adding a mask propagation branch that propagates\nframe-level object instance masks from each video frame to all the other frames\nin a video clip. This allows our system to predict clip-level instance tracks\nwith respect to the object instances segmented in the middle frame of the clip.\nClip-level instance tracks generated densely for each frame in the sequence are\nfinally aggregated to produce video-level object instance segmentation and\nclassification. Our experiments demonstrate that our clip-level instance\nsegmentation makes our approach robust to motion blur and object occlusions in\nvideo. MaskProp achieves the best reported accuracy on the YouTube-VIS dataset,\noutperforming the ICCV 2019 video instance segmentation challenge winner\ndespite being much simpler and using orders of magnitude less labeled data\n(1.3M vs 1B images and 860K vs 14M bounding boxes).",
          "link": "http://arxiv.org/abs/1912.04573",
          "publishedOn": "2021-07-13T01:59:34.522Z",
          "wordCount": 647,
          "title": "Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation. (arXiv:1912.04573v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nabati_R/0/1/0/all/0/1\">Ramin Nabati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_L/0/1/0/all/0/1\">Landon Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hairong Qi</a>",
          "description": "3D multi-object tracking is a crucial component in the perception system of\nautonomous driving vehicles. Tracking all dynamic objects around the vehicle is\nessential for tasks such as obstacle avoidance and path planning. Autonomous\nvehicles are usually equipped with different sensor modalities to improve\naccuracy and reliability. While sensor fusion has been widely used in object\ndetection networks in recent years, most existing multi-object tracking\nalgorithms either rely on a single input modality, or do not fully exploit the\ninformation provided by multiple sensing modalities. In this work, we propose\nan end-to-end network for joint object detection and tracking based on radar\nand camera sensor fusion. Our proposed method uses a center-based radar-camera\nfusion algorithm for object detection and utilizes a greedy algorithm for\nobject association. The proposed greedy algorithm uses the depth, velocity and\n2D displacement of the detected objects to associate them through time. This\nmakes our tracking algorithm very robust to occluded and overlapping objects,\nas the depth and velocity information can help the network in distinguishing\nthem. We evaluate our method on the challenging nuScenes dataset, where it\nachieves 20.0 AMOTA and outperforms all vision-based 3D tracking methods in the\nbenchmark, as well as the baseline LiDAR-based method. Our method is online\nwith a runtime of 35ms per image, making it very suitable for autonomous\ndriving applications.",
          "link": "http://arxiv.org/abs/2107.05150",
          "publishedOn": "2021-07-13T01:59:34.515Z",
          "wordCount": 671,
          "title": "CFTrack: Center-based Radar and Camera Fusion for 3D Multi-Object Tracking. (arXiv:2107.05150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1\">Or Bar-Shira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubstein_A/0/1/0/all/0/1\">Ahuva Grubstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapson_Y/0/1/0/all/0/1\">Yael Rapson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhami_D/0/1/0/all/0/1\">Dror Suhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atar_E/0/1/0/all/0/1\">Eli Atar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peri_Hanania_K/0/1/0/all/0/1\">Keren Peri-Hanania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_R/0/1/0/all/0/1\">Ronnie Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "Breast cancer is the most common malignancy in women. Mammographic findings\nsuch as microcalcifications and masses, as well as morphologic features of\nmasses in sonographic scans, are the main diagnostic targets for tumor\ndetection. However, improved specificity of these imaging modalities is\nrequired. A leading alternative target is neoangiogenesis. When pathological,\nit contributes to the development of numerous types of tumors, and the\nformation of metastases. Hence, demonstrating neoangiogenesis by visualization\nof the microvasculature may be of great importance. Super resolution ultrasound\nlocalization microscopy enables imaging of the microvasculature at the\ncapillary level. Yet, challenges such as long reconstruction time, dependency\non prior knowledge of the system Point Spread Function (PSF), and separability\nof the Ultrasound Contrast Agents (UCAs), need to be addressed for translation\nof super-resolution US into the clinic. In this work we use a deep neural\nnetwork architecture that makes effective use of signal structure to address\nthese challenges. We present in vivo human results of three different breast\nlesions acquired with a clinical US scanner. By leveraging our trained network,\nthe microvasculature structure is recovered in a short time, without prior PSF\nknowledge, and without requiring separability of the UCAs. Each of the\nrecoveries exhibits a different structure that corresponds with the known\nhistological structure. This study demonstrates the feasibility of in vivo\nhuman super resolution, based on a clinical scanner, to increase US specificity\nfor different breast lesions and promotes the use of US in the diagnosis of\nbreast pathologies.",
          "link": "http://arxiv.org/abs/2107.05270",
          "publishedOn": "2021-07-13T01:59:34.505Z",
          "wordCount": 704,
          "title": "Learned super resolution ultrasound for improved breast lesion characterization. (arXiv:2107.05270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>",
          "description": "The proliferation of remote sensing satellites has resulted in a massive\namount of remote sensing images. However, due to human and material resource\nconstraints, the vast majority of remote sensing images remain unlabeled. As a\nresult, it cannot be applied to currently available deep learning methods. To\nfully utilize the remaining unlabeled images, we propose a Geographical\nKnowledge-driven Representation learning method for remote sensing images\n(GeoKR), improving network performance and reduce the demand for annotated\ndata. The global land cover products and geographical location associated with\neach remote sensing image are regarded as geographical knowledge to provide\nsupervision for representation learning and network pre-training. An efficient\npre-training framework is proposed to eliminate the supervision noises caused\nby imaging times and resolutions difference between remote sensing images and\ngeographical knowledge. A large scale pre-training dataset Levir-KR is proposed\nto support network pre-training. It contains 1,431,950 remote sensing images\nfrom Gaofen series satellites with various resolutions. Experimental results\ndemonstrate that our proposed method outperforms ImageNet pre-training and\nself-supervised representation learning methods and significantly reduces the\nburden of data annotation on downstream tasks such as scene classification,\nsemantic segmentation, object detection, and cloud / snow detection. It\ndemonstrates that our proposed method can be used as a novel paradigm for\npre-training neural networks. Codes will be available on\nhttps://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.",
          "link": "http://arxiv.org/abs/2107.05276",
          "publishedOn": "2021-07-13T01:59:34.497Z",
          "wordCount": 659,
          "title": "Geographical Knowledge-driven Representation Learning for Remote Sensing Images. (arXiv:2107.05276v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05121",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saito_N/0/1/0/all/0/1\">Naoki Saito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1\">Yiqun Shao</a>",
          "description": "Extending computational harmonic analysis tools from the classical setting of\nregular lattices to the more general setting of graphs and networks is very\nimportant and much research has been done recently. The Generalized Haar-Walsh\nTransform (GHWT) developed by Irion and Saito (2014) is a multiscale transform\nfor signals on graphs, which is a generalization of the classical Haar and\nWalsh-Hadamard Transforms. We propose the extended Generalized Haar-Walsh\nTransform (eGHWT), which is a generalization of the adapted time-frequency\ntilings of Thiele and Villemoes (1996). The eGHWT examines not only the\nefficiency of graph-domain partitions but also that of \"sequency-domain\"\npartitions simultaneously. Consequently, the eGHWT and its associated\nbest-basis selection algorithm for graph signals significantly improve the\nperformance of the previous GHWT with the similar computational cost, $O(N \\log\nN)$, where $N$ is the number of nodes of an input graph. While the GHWT\nbest-basis algorithm seeks the most suitable orthonormal basis for a given task\namong more than $(1.5)^N$ possible orthonormal bases in $\\mathbb{R}^N$, the\neGHWT best-basis algorithm can find a better one by searching through more than\n$0.618\\cdot(1.84)^N$ possible orthonormal bases in $\\mathbb{R}^N$. This article\ndescribes the details of the eGHWT best-basis algorithm and demonstrates its\nsuperiority using several examples including genuine graph signals as well as\nconventional digital images viewed as graph signals. Furthermore, we also show\nhow the eGHWT can be extended to 2D signals and matrix-form data by viewing\nthem as a tensor product of graphs generated from their columns and rows and\ndemonstrate its effectiveness on applications such as image approximation.",
          "link": "http://arxiv.org/abs/2107.05121",
          "publishedOn": "2021-07-13T01:59:34.479Z",
          "wordCount": 701,
          "title": "eGHWT: The extended Generalized Haar-Walsh Transform. (arXiv:2107.05121v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chrol_Cannon_J/0/1/0/all/0/1\">Joseph Chrol-Cannon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazic_R/0/1/0/all/0/1\">Ranko Lazic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhusoodanan_A/0/1/0/all/0/1\">Adithya Madhusoodanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>",
          "description": "Video activity recognition by deep neural networks is impressive for many\nclasses. However, it falls short of human performance, especially for\nchallenging to discriminate activities. Humans differentiate these complex\nactivities by recognising critical spatio-temporal relations among explicitly\nrecognised objects and parts, for example, an object entering the aperture of a\ncontainer. Deep neural networks can struggle to learn such critical\nrelationships effectively. Therefore we propose a more human-like approach to\nactivity recognition, which interprets a video in sequential temporal phases\nand extracts specific relationships among objects and hands in those phases.\nRandom forest classifiers are learnt from these extracted relationships. We\napply the method to a challenging subset of the something-something dataset and\nachieve a more robust performance against neural network baselines on\nchallenging activities.",
          "link": "http://arxiv.org/abs/2107.05319",
          "publishedOn": "2021-07-13T01:59:34.472Z",
          "wordCount": 562,
          "title": "Human-like Relational Models for Activity Recognition in Video. (arXiv:2107.05319v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Interpretable brain network models for disease prediction are of great value\nfor the advancement of neuroscience. GNNs are promising to model complicated\nnetwork data, but they are prone to overfitting and suffer from poor\ninterpretability, which prevents their usage in decision-critical scenarios\nlike healthcare. To bridge this gap, we propose BrainNNExplainer, an\ninterpretable GNN framework for brain network analysis. It is mainly composed\nof two jointly learned modules: a backbone prediction model that is\nspecifically designed for brain networks and an explanation generator that\nhighlights disease-specific prominent brain network connections. Extensive\nexperimental results with visualizations on two challenging disease prediction\ndatasets demonstrate the unique interpretability and outstanding performance of\nBrainNNExplainer.",
          "link": "http://arxiv.org/abs/2107.05097",
          "publishedOn": "2021-07-13T01:59:34.466Z",
          "wordCount": 600,
          "title": "BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis. (arXiv:2107.05097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ainetter_S/0/1/0/all/0/1\">Stefan Ainetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>",
          "description": "In this work, we introduce a novel, end-to-end trainable CNN-based\narchitecture to deliver high quality results for grasp detection suitable for a\nparallel-plate gripper, and semantic segmentation. Utilizing this, we propose a\nnovel refinement module that takes advantage of previously calculated grasp\ndetection and semantic segmentation and further increases grasp detection\naccuracy. Our proposed network delivers state-of-the-art accuracy on two\npopular grasp dataset, namely Cornell and Jacquard. As additional contribution,\nwe provide a novel dataset extension for the OCID dataset, making it possible\nto evaluate grasp detection in highly challenging scenes. Using this dataset,\nwe show that semantic segmentation can additionally be used to assign grasp\ncandidates to object classes, which can be used to pick specific objects in the\nscene.",
          "link": "http://arxiv.org/abs/2107.05287",
          "publishedOn": "2021-07-13T01:59:34.459Z",
          "wordCount": 568,
          "title": "End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB. (arXiv:2107.05287v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hira_S/0/1/0/all/0/1\">Sanchit Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Ritwik Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Abhinav Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_D/0/1/0/all/0/1\">Daniil Pakhomov</a>",
          "description": "We present an approach to perform supervised action recognition in the dark.\nIn this work, we present our results on the ARID dataset. Most previous works\nonly evaluate performance on large, well illuminated datasets like Kinetics and\nHMDB51. We demonstrate that our work is able to achieve a very low error rate\nwhile being trained on a much smaller dataset of dark videos. We also explore a\nvariety of training and inference strategies including domain transfer\nmethodologies and also propose a simple but useful frame selection strategy.\nOur empirical results demonstrate that we beat previously published baseline\nmodels by 11%.",
          "link": "http://arxiv.org/abs/2107.05202",
          "publishedOn": "2021-07-13T01:59:34.453Z",
          "wordCount": 539,
          "title": "Delta Sampling R-BERT for limited data and low-light action recognition. (arXiv:2107.05202v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Rongkai Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Jiang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_Z/0/1/0/all/0/1\">Zhiyuan Zha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dauwels_J/0/1/0/all/0/1\">Justin Dauwels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>",
          "description": "State-of-the-art image denoisers exploit various types of deep neural\nnetworks via deterministic training. Alternatively, very recent works utilize\ndeep reinforcement learning for restoring images with diverse or unknown\ncorruptions. Though deep reinforcement learning can generate effective policy\nnetworks for operator selection or architecture search in image restoration,\nhow it is connected to the classic deterministic training in solving inverse\nproblems remains unclear. In this work, we propose a novel image denoising\nscheme via Residual Recovery using Reinforcement Learning, dubbed R3L. We show\nthat R3L is equivalent to a deep recurrent neural network that is trained using\na stochastic reward, in contrast to many popular denoisers using supervised\nlearning with deterministic losses. To benchmark the effectiveness of\nreinforcement learning in R3L, we train a recurrent neural network with the\nsame architecture for residual recovery using the deterministic loss, thus to\nanalyze how the two different training strategies affect the denoising\nperformance. With such a unified benchmarking system, we demonstrate that the\nproposed R3L has better generalizability and robustness in image denoising when\nthe estimated noise level varies, comparing to its counterparts using\ndeterministic training, as well as various state-of-the-art image denoising\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.05318",
          "publishedOn": "2021-07-13T01:59:34.448Z",
          "wordCount": 657,
          "title": "R3L: Connecting Deep Reinforcement Learning to Recurrent Neural Networks for Image Denoising via Residual Recovery. (arXiv:2107.05318v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>",
          "description": "We introduce a novel Recurrent Neural Network-based algorithm for future\nvideo feature generation and action anticipation called feature mapping RNN.\nOur novel RNN architecture builds upon three effective principles of machine\nlearning, namely parameter sharing, Radial Basis Function kernels and\nadversarial training. Using only some of the earliest frames of a video, the\nfeature mapping RNN is able to generate future features with a fraction of the\nparameters needed in traditional RNN. By feeding these future features into a\nsimple multi-layer perceptron facilitated with an RBF kernel layer, we are able\nto accurately predict the action in the video. In our experiments, we obtain\n18% improvement on JHMDB-21 dataset, 6% on UCF101-24 and 13% improvement on\nUT-Interaction datasets over prior state-of-the-art for action anticipation.",
          "link": "http://arxiv.org/abs/1911.07806",
          "publishedOn": "2021-07-13T01:59:34.431Z",
          "wordCount": 600,
          "title": "Action Anticipation with RBF Kernelized Feature Mapping RNN. (arXiv:1911.07806v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>",
          "description": "Early action recognition (action prediction) from limited preliminary\nobservations plays a critical role for streaming vision systems that demand\nreal-time inference, as video actions often possess elongated temporal spans\nwhich cause undesired latency. In this study, we address action prediction by\ninvestigating how action patterns evolve over time in a spatial feature space.\nThere are three key components to our system. First, we work with\nintermediate-layer ConvNet features, which allow for abstraction from raw data,\nwhile retaining spatial layout. Second, instead of propagating features per se,\nwe propagate their residuals across time, which allows for a compact\nrepresentation that reduces redundancy. Third, we employ a Kalman filter to\ncombat error build-up and unify across prediction start times. Extensive\nexperimental results on multiple benchmarks show that our approach leads to\ncompetitive performance in action prediction. Notably, we investigate the\nlearned components of our system to shed light on their otherwise opaque\nnatures in two ways. First, we document that our learned feature propagation\nmodule works as a spatial shifting mechanism under convolution to propagate\ncurrent observations into the future. Thus, it captures flow-based image motion\ninformation. Second, the learned Kalman filter adaptively updates prior\nestimation to aid the sequence learning process.",
          "link": "http://arxiv.org/abs/2107.05122",
          "publishedOn": "2021-07-13T01:59:34.425Z",
          "wordCount": 632,
          "title": "Interpretable Deep Feature Propagation for Early Action Recognition. (arXiv:2107.05122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05274",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Bingzhi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yishu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>",
          "description": "With the development of deep encoder-decoder architectures and large-scale\nannotated medical datasets, great progress has been achieved in the development\nof automatic medical image segmentation. Due to the stacking of convolution\nlayers and the consecutive sampling operations, existing standard models\ninevitably encounter the information recession problem of feature\nrepresentations, which fails to fully model the global contextual feature\ndependencies. To overcome the above challenges, this paper proposes a novel\nTransformer based medical image semantic segmentation framework called\nTransAttUnet, in which the multi-level guided attention and multi-scale skip\nconnection are jointly designed to effectively enhance the functionality and\nflexibility of traditional U-shaped architecture. Inspired by Transformer, a\nnovel self-aware attention (SAA) module with both Transformer Self Attention\n(TSA) and Global Spatial Attention (GSA) is incorporated into TransAttUnet to\neffectively learn the non-local interactions between encoder features. In\nparticular, we also establish additional multi-scale skip connections between\ndecoder blocks to aggregate the different semantic-scale upsampling features.\nIn this way, the representation ability of multi-scale context information is\nstrengthened to generate discriminative features. Benefitting from these\ncomplementary components, the proposed TransAttUnet can effectively alleviate\nthe loss of fine details caused by the information recession problem, improving\nthe diagnostic sensitivity and segmentation quality of medical image analysis.\nExtensive experiments on multiple medical image segmentation datasets of\ndifferent imaging demonstrate that our method consistently outperforms the\nstate-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2107.05274",
          "publishedOn": "2021-07-13T01:59:34.417Z",
          "wordCount": 674,
          "title": "TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation. (arXiv:2107.05274v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menghan_H/0/1/0/all/0/1\">Hu Menghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guangtao_Z/0/1/0/all/0/1\">Zhai Guangtao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Ping_Z/0/1/0/all/0/1\">Zhang Xiao-Ping</a>",
          "description": "In recent years, computer-aided diagnosis has become an increasingly popular\ntopic. Methods based on convolutional neural networks have achieved good\nperformance in medical image segmentation and classification. Due to the\nlimitations of the convolution operation, the long-term spatial features are\noften not accurately obtained. Hence, we propose a TransClaw U-Net network\nstructure, which combines the convolution operation with the transformer\noperation in the encoding part. The convolution part is applied for extracting\nthe shallow spatial features to facilitate the recovery of the image resolution\nafter upsampling. The transformer part is used to encode the patches, and the\nself-attention mechanism is used to obtain global information between\nsequences. The decoding part retains the bottom upsampling structure for better\ndetail segmentation performance. The experimental results on Synapse\nMulti-organ Segmentation Datasets show that the performance of TransClaw U-Net\nis better than other network structures. The ablation experiments also prove\nthe generalization performance of TransClaw U-Net.",
          "link": "http://arxiv.org/abs/2107.05188",
          "publishedOn": "2021-07-13T01:59:34.411Z",
          "wordCount": 611,
          "title": "TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation. (arXiv:2107.05188v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05115",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1\">Basit O. Alawode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1\">Mudassir Masood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1\">Tarig Ballal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1\">Tareq Al-Naffouri</a>",
          "description": "In spite of the improvements achieved by the several denoising algorithms\nover the years, many of them still fail at preserving the fine details of the\nimage after denoising. This is as a result of the smooth-out effect they have\non the images. Most neural network-based algorithms have achieved better\nquantitative performance than the classical denoising algorithms. However, they\nalso suffer from qualitative (visual) performance as a result of the smooth-out\neffect. In this paper, we propose an algorithm to address this shortcoming. We\npropose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image\ndenoising. This algorithm performs collaborative denoising of image patches in\nthe sparse domain using a set of optimized neural network models. This results\nin a fast algorithm that is able to excellently obtain a trade-off between\nnoise removal and details preservation. Extensive experiments show that the\nDeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and\nqualitatively (visually) better than many of the state-of-the-art denoising\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.05115",
          "publishedOn": "2021-07-13T01:59:34.405Z",
          "wordCount": 609,
          "title": "Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising. (arXiv:2107.05115v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_C/0/1/0/all/0/1\">Chun Chet Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazaruddin_A/0/1/0/all/0/1\">Akmalul Khairi Bin Nazaruddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeong Khang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lixin Fan</a>",
          "description": "With hundreds of thousands of electronic chip components are being\nmanufactured every day, chip manufacturers have seen an increasing demand in\nseeking a more efficient and effective way of inspecting the quality of printed\ntexts on chip components. The major problem that deters this area of research\nis the lacking of realistic text on chips datasets to act as a strong\nfoundation. Hence, a text on chips dataset, ICText is used as the main target\nfor the proposed Robust Reading Challenge on Integrated Circuit Text Spotting\nand Aesthetic Assessment (RRC-ICText) 2021 to encourage the research on this\nproblem. Throughout the entire competition, we have received a total of 233\nsubmissions from 10 unique teams/individuals. Details of the competition and\nsubmission results are presented in this report.",
          "link": "http://arxiv.org/abs/2107.05279",
          "publishedOn": "2021-07-13T01:59:34.389Z",
          "wordCount": 609,
          "title": "ICDAR 2021 Competition on Integrated Circuit Text Spotting and Aesthetic Assessment. (arXiv:2107.05279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>",
          "description": "Video predictive understanding encompasses a wide range of efforts that are\nconcerned with the anticipation of the unobserved future from the current as\nwell as historical video observations. Action prediction is a major sub-area of\nvideo predictive understanding and is the focus of this review. This sub-area\nhas two major subdivisions: early action recognition and future action\nprediction. Early action recognition is concerned with recognizing an ongoing\naction as soon as possible. Future action prediction is concerned with the\nanticipation of actions that follow those previously observed. In either case,\nthe \\textbf{\\textit{causal}} relationship between the past, current, and\npotential future information is the main focus. Various mathematical tools such\nas Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian\nrecursive filtering are widely adopted jointly with computer vision techniques\nfor these two tasks. However, these approaches face challenges such as the\ncurse of dimensionality, poor generalization, and constraints from\ndomain-specific knowledge. Recently, structures that rely on deep convolutional\nneural networks and recurrent neural networks have been extensively proposed\nfor improving the performance of existing vision tasks, in general, and action\nprediction tasks, in particular. However, they have their own shortcomings, \\eg\nreliance on massive training data and lack of strong theoretical underpinnings.\nIn this survey, we start by introducing the major sub-areas of the broad area\nof video predictive understanding, which recently have received intensive\nattention and proven to have practical value. Next, a thorough review of\nvarious early action recognition and future action prediction algorithms are\nprovided with suitably organized divisions. Finally, we conclude our discussion\nwith future research directions.",
          "link": "http://arxiv.org/abs/2107.05140",
          "publishedOn": "2021-07-13T01:59:34.383Z",
          "wordCount": 698,
          "title": "Review of Video Predictive Understanding: Early ActionRecognition and Future Action Prediction. (arXiv:2107.05140v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05342",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Celik_N/0/1/0/all/0/1\">Numan Celik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Soumya Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braden_B/0/1/0/all/0/1\">Barbara Braden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1\">Jens Rittscher</a>",
          "description": "Gastrointestinal (GI) cancer precursors require frequent monitoring for risk\nstratification of patients. Automated segmentation methods can help to assess\nrisk areas more accurately, and assist in therapeutic procedures or even\nremoval. In clinical practice, addition to the conventional white-light imaging\n(WLI), complimentary modalities such as narrow-band imaging (NBI) and\nfluorescence imaging are used. While, today most segmentation approaches are\nsupervised and only concentrated on a single modality dataset, this work\nexploits to use a target-independent unsupervised domain adaptation (UDA)\ntechnique that is capable to generalize to an unseen target modality. In this\ncontext, we propose a novel UDA-based segmentation method that couples the\nvariational autoencoder and U-Net with a common EfficientNet-B4 backbone, and\nuses a joint loss for latent-space optimization for target samples. We show\nthat our model can generalize to unseen target NBI (target) modality when\ntrained using only WLI (source) modality. Our experiments on both upper and\nlower GI endoscopy data show the effectiveness of our approach compared to\nnaive supervised approach and state-of-the-art UDA segmentation methods.",
          "link": "http://arxiv.org/abs/2107.05342",
          "publishedOn": "2021-07-13T01:59:34.376Z",
          "wordCount": 632,
          "title": "EndoUDA: A modality independent segmentation approach for endoscopy imaging. (arXiv:2107.05342v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangyue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Y. Chai</a>",
          "description": "In this paper, we study the problem of recognizing compositional\nattribute-object concepts within the zero-shot learning (ZSL) framework. We\npropose an episode-based cross-attention (EpiCA) network which combines merits\nof cross-attention mechanism and episode-based training strategy to recognize\nnovel compositional concepts. Firstly, EpiCA bases on cross-attention to\ncorrelate concept-visual information and utilizes the gated pooling layer to\nbuild contextualized representations for both images and concepts. The updated\nrepresentations are used for a more in-depth multi-modal relevance calculation\nfor concept recognition. Secondly, a two-phase episode training strategy,\nespecially the transductive phase, is adopted to utilize unlabeled test\nexamples to alleviate the low-resource learning problem. Experiments on two\nwidely-used zero-shot compositional learning (ZSCL) benchmarks have\ndemonstrated the effectiveness of the model compared with recent approaches on\nboth conventional and generalized ZSCL settings.",
          "link": "http://arxiv.org/abs/2107.05176",
          "publishedOn": "2021-07-13T01:59:34.369Z",
          "wordCount": 560,
          "title": "Zero-Shot Compositional Concept Learning. (arXiv:2107.05176v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sushobhan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhaoyang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_N/0/1/0/all/0/1\">Nathan Matsuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkovich_A/0/1/0/all/0/1\">Andrew Berkovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1\">Oliver Cossairt</a>",
          "description": "Existing Multi-Plane Image (MPI) based view-synthesis methods generate an MPI\naligned with the input view using a fixed number of planes in one forward pass.\nThese methods produce fast, high-quality rendering of novel views, but rely on\nslow and computationally expensive MPI generation methods unsuitable for\nreal-time applications. In addition, most MPI techniques use fixed\ndepth/disparity planes which cannot be modified once the training is complete,\nhence offering very little flexibility at run-time.\n\nWe propose LiveView - a novel MPI generation and rendering technique that\nproduces high-quality view synthesis in real-time. Our method can also offer\nthe flexibility to select scene-dependent MPI planes (number of planes and\nspacing between them) at run-time. LiveView first warps input images to target\nview (target-centered) and then learns to generate a target view centered MPI,\none depth plane at a time (dynamically). The method generates high-quality\nrenderings, while also enabling fast MPI generation and novel view synthesis.\nAs a result, LiveView enables real-time view synthesis applications where an\nMPI needs to be updated frequently based on a video stream of input views. We\ndemonstrate that LiveView improves the quality of view synthesis while being 70\ntimes faster at run-time compared to state-of-the-art MPI-based methods.",
          "link": "http://arxiv.org/abs/2107.05113",
          "publishedOn": "2021-07-13T01:59:34.362Z",
          "wordCount": 638,
          "title": "LiveView: Dynamic Target-Centered MPI for View Synthesis. (arXiv:2107.05113v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05085",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1\">Gorkem Polat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Serinagaoglu_Y/0/1/0/all/0/1\">Yesim Dogrusoz Serinagaoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halici_U/0/1/0/all/0/1\">Ugur Halici</a>",
          "description": "Recent studies have shown that lung cancer screening using annual low-dose\ncomputed tomography (CT) reduces lung cancer mortality by 20% compared to\ntraditional chest radiography. Therefore, CT lung screening has started to be\nused widely all across the world. However, analyzing these images is a serious\nburden for radiologists. The number of slices in a CT scan can be up to 600.\nTherefore, computer-aided-detection (CAD) systems are very important for faster\nand more accurate assessment of the data. In this study, we proposed a\nframework that analyzes CT lung screenings using convolutional neural networks\n(CNNs) to reduce false positives. We trained our model with different volume\nsizes and showed that volume size plays a critical role in the performance of\nthe system. We also used different fusions in order to show their power and\neffect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D\nconvolutional operations applied to 3D data could result in information loss.\nThe proposed framework has been tested on the dataset provided by the LUNA16\nChallenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.",
          "link": "http://arxiv.org/abs/2107.05085",
          "publishedOn": "2021-07-13T01:59:34.347Z",
          "wordCount": 661,
          "title": "Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks. (arXiv:2107.05085v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dromey_B/0/1/0/all/0/1\">Brian Dromey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napolitano_R/0/1/0/all/0/1\">Raffaele Napolitano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peebles_D/0/1/0/all/0/1\">Donald M. Peebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>",
          "description": "During pregnancy, ultrasound examination in the second trimester can assess\nfetal size according to standardized charts. To achieve a reproducible and\naccurate measurement, a sonographer needs to identify three standard 2D planes\nof the fetal anatomy (head, abdomen, femur) and manually mark the key\nanatomical landmarks on the image for accurate biometry and fetal weight\nestimation. This can be a time-consuming operator-dependent task, especially\nfor a trainee sonographer. Computer-assisted techniques can help in automating\nthe fetal biometry computation process. In this paper, we present a unified\nautomated framework for estimating all measurements needed for the fetal weight\nassessment. The proposed framework semantically segments the key fetal\nanatomies using state-of-the-art segmentation models, followed by region\nfitting and scale recovery for the biometry estimation. We present an ablation\nstudy of segmentation algorithms to show their robustness through 4-fold\ncross-validation on a dataset of 349 ultrasound standard plane images from 42\npregnancies. Moreover, we show that the network with the best segmentation\nperformance tends to be more accurate for biometry estimation. Furthermore, we\ndemonstrate that the error between clinically measured and predicted fetal\nbiometry is lower than the permissible error during routine clinical\nmeasurements.",
          "link": "http://arxiv.org/abs/2107.05255",
          "publishedOn": "2021-07-13T01:59:34.341Z",
          "wordCount": 653,
          "title": "AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2107.05255v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1\">Xuan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Relation prediction among entities in images is an important step in scene\ngraph generation (SGG), which further impacts various visual understanding and\nreasoning tasks. Existing SGG frameworks, however, require heavy training yet\nare incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we\nstress that such incapability is due to the lack of commonsense reasoning,i.e.,\nthe ability to associate similar entities and infer similar relations based on\ngeneral understanding of the world. To fill this gap, we propose\nCommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to\nintegrate commonsense knowledge for SGG, especially for zero-shot relation\nprediction. Specifically, we develop novel graph mining pipelines to model the\nneighborhoods and paths around entities in an external commonsense knowledge\ngraph, and integrate them on top of state-of-the-art SGG frameworks. Extensive\nquantitative evaluations and qualitative case studies on both original and\nmanipulated datasets from Visual Genome demonstrate the effectiveness of our\nproposed approach.",
          "link": "http://arxiv.org/abs/2107.05080",
          "publishedOn": "2021-07-13T01:59:34.335Z",
          "wordCount": 608,
          "title": "Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration. (arXiv:2107.05080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-En Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">En-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_P/0/1/0/all/0/1\">Pei-Yung Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Li-Chen Fu</a>",
          "description": "Recently, there has been a panoptic segmentation task combining semantic and\ninstance segmentation, in which the goal is to classify each pixel with the\ncorresponding instance ID. In this work, we propose a solution to tackle the\npanoptic segmentation task. The overall structure combines the bottom-up method\nand the top-down method. Therefore, not only can there be better performance,\nbut also the execution speed can be maintained. The network mainly pays\nattention to the quality of the mask. In the previous work, we can see that the\nuneven contour of the object is more likely to appear, resulting in low-quality\nprediction. Accordingly, we propose enhancement features and corresponding loss\nfunctions for the silhouette of objects and backgrounds to improve the mask.\nMeanwhile, we use the new proposed confidence score to solve the occlusion\nproblem and make the network tend to use higher quality masks as prediction\nresults. To verify our research, we used the COCO dataset and CityScapes\ndataset to do experiments and obtained competitive results with fast inference\ntime.",
          "link": "http://arxiv.org/abs/2107.05093",
          "publishedOn": "2021-07-13T01:59:34.328Z",
          "wordCount": 614,
          "title": "SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network. (arXiv:2107.05093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Joshua Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Chau-Wai Wong</a>",
          "description": "Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory\nfunctionality and is receiving increasing attention during the COVID-19\npandemic. Clinical findings show that it is possible for COVID-19 patients to\nhave significantly low SpO$_2$ before any obvious symptoms. The prevalence of\ncameras has motivated researchers to investigate methods for monitoring SpO$_2$\nusing videos. Most prior schemes involving smartphones are contact-based: They\nrequire a fingertip to cover the phone's camera and the nearby light source to\ncapture re-emitted light from the illuminated tissue. In this paper, we propose\nthe first convolutional neural network based noncontact SpO$_2$ estimation\nscheme using smartphone cameras. The scheme analyzes the videos of a\nparticipant's hand for physiological sensing, which is convenient and\ncomfortable, and can protect their privacy and allow for keeping face masks on.\nWe design our neural network architectures inspired by the optophysiological\nmodels for SpO$_2$ measurement and demonstrate the explainability by\nvisualizing the weights for channel combination. Our proposed models outperform\nthe state-of-the-art model that is designed for contact-based SpO$_2$\nmeasurement, showing the potential of our proposed method to contribute to\npublic health. We also analyze the impact of skin type and the side of a hand\non SpO$_2$ estimation performance.",
          "link": "http://arxiv.org/abs/2107.05087",
          "publishedOn": "2021-07-13T01:59:34.321Z",
          "wordCount": 689,
          "title": "Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+George_B/0/1/0/all/0/1\">Blessen George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K. Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>",
          "description": "Generative adversarial networks (GANs) are very popular to generate realistic\nimages, but they often suffer from the training instability issues and the\nphenomenon of mode loss. In order to attain greater diversity in GAN\nsynthesized data, it is critical to solving the problem of mode loss. Our work\nexplores probabilistic approaches to GAN modelling that could allow us to\ntackle these issues. We present Prb-GANs, a new variation that uses dropout to\ncreate a distribution over the network parameters with the posterior learnt\nusing variational inference. We describe theoretically and validate\nexperimentally using simple and complex datasets the benefits of such an\napproach. We look into further improvements using the concept of uncertainty\nmeasures. Through a set of further modifications to the loss functions for each\nnetwork of the GAN, we are able to get results that show the improvement of GAN\nperformance. Our methods are extremely simple and require very little\nmodification to existing GAN architecture.",
          "link": "http://arxiv.org/abs/2107.05241",
          "publishedOn": "2021-07-13T01:59:34.305Z",
          "wordCount": 597,
          "title": "Prb-GAN: A Probabilistic Framework for GAN Modelling. (arXiv:2107.05241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05334",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1\">Chih-Chung Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Guan-Lin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Mei-Hsuan Wu</a>",
          "description": "With the massive damage in the world caused by Coronavirus Disease 2019\nSARS-CoV-2 (COVID-19), many related research topics have been proposed in the\npast two years. The Chest Computed Tomography (CT) scans are the most valuable\nmaterials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19\nclassification of Chest CT scan is based on a single-slice level, implying that\nthe most critical CT slice should be selected from the original CT scan volume\nmanually. We simultaneously propose 2-D and 3-D models to predict the COVID-19\nof CT scan to tickle this issue. In our 2-D model, we introduce the Deep\nWilcoxon signed-rank test (DWCC) to determine the importance of each slice of a\nCT scan to overcome the issue mentioned previously. Furthermore, a\nConvolutional CT scan-Aware Transformer (CCAT) is proposed to discover the\ncontext of the slices fully. The frame-level feature is extracted from each CT\nslice based on any backbone network and followed by feeding the features to our\nwithin-slice-Transformer (WST) to discover the context information in the pixel\ndimension. The proposed Between-Slice-Transformer (BST) is used to aggregate\nthe extracted spatial-context features of every CT slice. A simple classifier\nis then used to judge whether the Spatio-temporal features are COVID-19 or\nnon-COVID-19. The extensive experiments demonstrated that the proposed CCAT and\nDWCC significantly outperform the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.05334",
          "publishedOn": "2021-07-13T01:59:34.296Z",
          "wordCount": 713,
          "title": "Visual Transformer with Statistical Test for COVID-19 Classification. (arXiv:2107.05334v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_S/0/1/0/all/0/1\">Sho Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaji_S/0/1/0/all/0/1\">Shizuo Kaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawa_K/0/1/0/all/0/1\">Kanabu Nawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imae_T/0/1/0/all/0/1\">Toshikazu Imae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_A/0/1/0/all/0/1\">Atsushi Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamoto_T/0/1/0/all/0/1\">Takahiro Nakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohta_T/0/1/0/all/0/1\">Takeshi Ohta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozawa_Y/0/1/0/all/0/1\">Yuki Nozawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_H/0/1/0/all/0/1\">Hideomi Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haga_A/0/1/0/all/0/1\">Akihiro Haga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_K/0/1/0/all/0/1\">Keiichi Nakagawa</a>",
          "description": "Deep-learning-based image processing has emerged as a valuable tool in recent\nyears owing to its high performance. However, the quality of\ndeep-learning-based methods relies heavily on the amount of training data, and\nthe cost of acquiring a large amount of data is often prohibitive in medical\nfields. Therefore, we performed CT modality conversion based on deep learning\nrequiring only a small number of unsupervised images. The proposed method is\nbased on generative adversarial networks (GANs) with several extensions\ntailored for CT images. This method emphasizes the preservation of the\nstructure in the processed images and reduction in the amount of training data.\nThis method was applied to realize the conversion of mega-voltage computed\ntomography (MVCT) to kilo-voltage computed tomography (kVCT) images. Training\nwas performed using several datasets acquired from patients with head and neck\ncancer. The size of the datasets ranged from 16 slices (for two patients) to\n2745 slices (for 137 patients) of MVCT and 2824 slices of kVCT for 98 patients.\nThe quality of the processed MVCT images was considerably enhanced, and the\nstructural changes in the images were minimized. With an increase in the size\nof training data, the image quality exhibited a satisfactory convergence from a\nfew hundred slices. In addition to statistical and visual evaluations, these\nresults were clinically evaluated by medical doctors in terms of the accuracy\nof contouring. We developed an MVCT to kVCT conversion model based on deep\nlearning, which can be trained using a few hundred unpaired images. The\nstability of the model against the change in the data size was demonstrated.\nThis research promotes the reliable use of deep learning in clinical medicine\nby partially answering the commonly asked questions: \"Is our data enough? How\nmuch data must we prepare?\"",
          "link": "http://arxiv.org/abs/2107.05238",
          "publishedOn": "2021-07-13T01:59:34.288Z",
          "wordCount": 772,
          "title": "Training deep cross-modality conversion models with a small amount of data and its application to MVCT to kVCT conversion. (arXiv:2107.05238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolf_J/0/1/0/all/0/1\">Joerg Christian Wolf</a>",
          "description": "State-of-the-art motor vehicles are able to break for pedestrians in an\nemergency. We investigate what it would take to issue an early warning to the\ndriver so he/she has time to react. We have identified that predicting the\nintention of a pedestrian reliably by position is a particularly hard\nchallenge. This paper describes an early pedestrian warning demonstration\nsystem.",
          "link": "http://arxiv.org/abs/2107.05186",
          "publishedOn": "2021-07-13T01:59:34.281Z",
          "wordCount": 484,
          "title": "Early warning of pedestrians and cyclists. (arXiv:2107.05186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05190",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xinyu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tianlang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1\">Jinchao Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Y/0/1/0/all/0/1\">Yanqing Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1\">Yanlong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_B/0/1/0/all/0/1\">Banging Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_P/0/1/0/all/0/1\">Pengwei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>",
          "description": "Hyperspectral image (HSI) contains both spatial pattern and spectral\ninformation which has been widely used in food safety, remote sensing, and\nmedical detection. However, the acquisition of hyperspectral images is usually\ncostly due to the complicated apparatus for the acquisition of optical\nspectrum. Recently, it has been reported that HSI can be reconstructed from\nsingle RGB image using convolution neural network (CNN) algorithms. Compared\nwith the traditional hyperspectral cameras, the method based on CNN algorithms\nis simple, portable and low cost. In this study, we focused on the influence of\nthe RGB camera spectral sensitivity (CSS) on the HSI. A Xenon lamp incorporated\nwith a monochromator were used as the standard light source to calibrate the\nCSS. And the experimental results show that the CSS plays a significant role in\nthe reconstruction accuracy of an HSI. In addition, we proposed a new HSI\nreconstruction network where the dimensional structure of the original\nhyperspectral datacube was modified by 3D matrix transpose to improve the\nreconstruction accuracy.",
          "link": "http://arxiv.org/abs/2107.05190",
          "publishedOn": "2021-07-13T01:59:34.243Z",
          "wordCount": 616,
          "title": "Deep-learning-based Hyperspectral imaging through a RGB camera. (arXiv:2107.05190v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shuyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xinqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>",
          "description": "The paper describes our proposed methodology for the seven basic expression\nclassification track of Affective Behavior Analysis in-the-wild (ABAW)\nCompetition 2021. In this task, facial expression recognition (FER) methods aim\nto classify the correct expression category from a diverse background, but\nthere are several challenges. First, to adapt the model to in-the-wild\nscenarios, we use the knowledge from pre-trained large-scale face recognition\ndata. Second, we propose an ensemble model with a convolution neural network\n(CNN), a CNN-recurrent neural network (CNN-RNN), and a CNN-Transformer\n(CNN-Transformer), to incorporate both spatial and temporal information. Our\nensemble model achieved F1 as 0.4133, accuracy as 0.6216 and final metric as\n0.4821 on the validation set.",
          "link": "http://arxiv.org/abs/2107.05160",
          "publishedOn": "2021-07-13T01:59:34.224Z",
          "wordCount": 558,
          "title": "Spatial and Temporal Networks for Facial Expression Recognition in the Wild Videos. (arXiv:2107.05160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05023",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lan_P/0/1/0/all/0/1\">Phan Ngoc Lan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_N/0/1/0/all/0/1\">Nguyen Sy An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_D/0/1/0/all/0/1\">Dao Viet Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_D/0/1/0/all/0/1\">Dao Van Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trung_T/0/1/0/all/0/1\">Tran Quang Trung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thuy_N/0/1/0/all/0/1\">Nguyen Thi Thuy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sang_D/0/1/0/all/0/1\">Dinh Viet Sang</a>",
          "description": "Automatic polyp segmentation has proven to be immensely helpful for endoscopy\nprocedures, reducing the missing rate of adenoma detection for endoscopists\nwhile increasing efficiency. However, classifying a polyp as being neoplasm or\nnot and segmenting it at the pixel level is still a challenging task for\ndoctors to perform in a limited time. In this work, we propose a fine-grained\nformulation for the polyp segmentation problem. Our formulation aims to not\nonly segment polyp regions, but also identify those at high risk of malignancy\nwith high accuracy. In addition, we present a UNet-based neural network\narchitecture called NeoUNet, along with a hybrid loss function to solve this\nproblem. Experiments show highly competitive results for NeoUNet on our\nbenchmark dataset compared to existing polyp segmentation models.",
          "link": "http://arxiv.org/abs/2107.05023",
          "publishedOn": "2021-07-13T01:59:34.204Z",
          "wordCount": 584,
          "title": "NeoUNet: Towards accurate colon polyp segmentation and neoplasm detection. (arXiv:2107.05023v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tianxiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>",
          "description": "This study delves into semi-supervised object detection (SSOD) to improve\ndetector performance with additional unlabeled data. State-of-the-art SSOD\nperformance has been achieved recently by self-training, in which training\nsupervision consists of ground truths and pseudo-labels. In current studies, we\nobserve that class imbalance in SSOD severely impedes the effectiveness of\nself-training. To address the class imbalance, we propose adaptive\nclass-rebalancing self-training (ACRST) with a novel memory module called\nCropBank. ACRST adaptively rebalances the training data with foreground\ninstances extracted from the CropBank, thereby alleviating the class imbalance.\nOwing to the high complexity of detection tasks, we observe that both\nself-training and data-rebalancing suffer from noisy pseudo-labels in SSOD.\nTherefore, we propose a novel two-stage filtering algorithm to generate\naccurate pseudo-labels. Our method achieves satisfactory improvements on\nMS-COCO and VOC benchmarks. When using only 1\\% labeled data in MS-COCO, our\nmethod achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP\nimprovement compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.05031",
          "publishedOn": "2021-07-13T01:59:34.197Z",
          "wordCount": 591,
          "title": "Semi-Supervised Object Detection with Adaptive Class-Rebalancing Self-Training. (arXiv:2107.05031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Catak_F/0/1/0/all/0/1\">Ferhat Ozgur Catak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1\">Tao Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Shaukat Ali</a>",
          "description": "Object detection in autonomous cars is commonly based on camera images and\nLidar inputs, which are often used to train prediction models such as deep\nartificial neural networks for decision making for object recognition,\nadjusting speed, etc. A mistake in such decision making can be damaging; thus,\nit is vital to measure the reliability of decisions made by such prediction\nmodels via uncertainty measurement. Uncertainty, in deep learning models, is\noften measured for classification problems. However, deep learning models in\nautonomous driving are often multi-output regression models. Hence, we propose\na novel method called PURE (Prediction sURface uncErtainty) for measuring\nprediction uncertainty of such regression models. We formulate the object\nrecognition problem as a regression model with more than one outputs for\nfinding object locations in a 2-dimensional camera view. For evaluation, we\nmodified three widely-applied object recognition models (i.e., YoLo, SSD300 and\nSSD512) and used the KITTI, Stanford Cars, Berkeley DeepDrive, and NEXET\ndatasets. Results showed the statistically significant negative correlation\nbetween prediction surface uncertainty and prediction accuracy suggesting that\nuncertainty significantly impacts the decisions made by autonomous driving.",
          "link": "http://arxiv.org/abs/2107.04991",
          "publishedOn": "2021-07-13T01:59:34.191Z",
          "wordCount": 640,
          "title": "Prediction Surface Uncertainty Quantification in Object Detection Models for Autonomous Driving. (arXiv:2107.04991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiangzhu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenzhe Liu</a>",
          "description": "In most practical applications, it's common to utilize multiple features from\ndifferent views to represent one object. Among these works, multi-view\nsubspace-based clustering has gained extensive attention from many researchers,\nwhich aims to provide clustering solutions to multi-view data. However, most\nexisting methods fail to take full use of the locality geometric structure and\nsimilarity relationship among samples under the multi-view scenario. To solve\nthese issues, we propose a novel multi-view learning method with locality\nrelationship constraint to explore the problem of multi-view clustering, called\nLocality Relationship Constrained Multi-view Clustering Framework (LRC-MCF).\nLRC-MCF aims to explore the diversity, geometric, consensus and complementary\ninformation among different views, by capturing the locality relationship\ninformation and the common similarity relationships among multiple views.\nMoreover, LRC-MCF takes sufficient consideration to weights of different views\nin finding the common-view locality structure and straightforwardly produce the\nfinal clusters. To effectually reduce the redundancy of the learned\nrepresentations, the low-rank constraint on the common similarity matrix is\nconsidered additionally. To solve the minimization problem of LRC-MCF, an\nAlternating Direction Minimization (ADM) method is provided to iteratively\ncalculate all variables LRC-MCF. Extensive experimental results on seven\nbenchmark multi-view datasets validate the effectiveness of the LRC-MCF method.",
          "link": "http://arxiv.org/abs/2107.05073",
          "publishedOn": "2021-07-13T01:59:34.175Z",
          "wordCount": 631,
          "title": "Locality Relationship Constrained Multi-view Clustering Framework. (arXiv:2107.05073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zheyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiyong Bu</a>",
          "description": "To prevent the spread of coronavirus disease 2019 (COVID-19), preliminary\ntemperature measurement and mask detection in public areas are conducted.\nHowever, the existing temperature measurement methods face the problems of\nsafety and deployment. In this paper, to realize safe and accurate temperature\nmeasurement even when a person's face is partially obscured, we propose a\ncloud-edge-terminal collaborative system with a lightweight infrared\ntemperature measurement model. A binocular camera with an RGB lens and a\nthermal lens is utilized to simultaneously capture image pairs. Then, a mobile\ndetection model based on a multi-task cascaded convolutional network (MTCNN) is\nproposed to realize face alignment and mask detection on the RGB images. For\naccurate temperature measurement, we transform the facial landmarks on the RGB\nimages to the thermal images by an affine transformation and select a more\naccurate temperature measurement area on the forehead. The collected\ninformation is uploaded to the cloud in real time for COVID-19 prevention.\nExperiments show that the detection model is only 6.1M and the average\ndetection speed is 257ms. At a distance of 1m, the error of indoor temperature\nmeasurement is about 3%. That is, the proposed system can realize real-time\ntemperature measurement in public areas.",
          "link": "http://arxiv.org/abs/2107.05078",
          "publishedOn": "2021-07-13T01:59:34.167Z",
          "wordCount": 703,
          "title": "A Cloud-Edge-Terminal Collaborative System for Temperature Measurement in COVID-19 Prevention. (arXiv:2107.05078v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>",
          "description": "The advancement of convolutional neural networks (CNNs) on various vision\napplications has attracted lots of attention. Yet the majority of CNNs are\nunable to satisfy the strict requirement for real-world deployment. To overcome\nthis, the recent popular network pruning is an effective method to reduce the\nredundancy of the models. However, the ranking of filters according to their\n\"importance\" on different pruning criteria may be inconsistent. One filter\ncould be important according to a certain criterion, while it is unnecessary\naccording to another one, which indicates that each criterion is only a partial\nview of the comprehensive \"importance\". From this motivation, we propose a\nnovel framework to integrate the existing filter pruning criteria by exploring\nthe criteria diversity. The proposed framework contains two stages: Criteria\nClustering and Filters Importance Calibration. First, we condense the pruning\ncriteria via layerwise clustering based on the rank of \"importance\" score.\nSecond, within each cluster, we propose a calibration factor to adjust their\nsignificance for each selected blending candidates and search for the optimal\nblending criterion via Evolutionary Algorithm. Quantitative results on the\nCIFAR-100 and ImageNet benchmarks show that our framework outperforms the\nstate-of-the-art baselines, regrading to the compact model performance after\npruning.",
          "link": "http://arxiv.org/abs/2107.05033",
          "publishedOn": "2021-07-13T01:59:34.161Z",
          "wordCount": 642,
          "title": "Blending Pruning Criteria for Convolutional Neural Networks. (arXiv:2107.05033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi-Geng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hui-Chu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>",
          "description": "Visual object localization is the key step in a series of object detection\ntasks. In the literature, high localization accuracy is achieved with the\nmainstream strongly supervised frameworks. However, such methods require\nobject-level annotations and are unable to detect objects of unknown\ncategories. Weakly supervised methods face similar difficulties. In this paper,\na self-paced learning framework is proposed to achieve accurate object\nlocalization on the rank list returned by instance search. The proposed\nframework mines the target instance gradually from the queries and their\ncorresponding top-ranked search results. Since a common instance is shared\nbetween the query and the images in the rank list, the target visual instance\ncan be accurately localized even without knowing what the object category is.\nIn addition to performing localization on instance search, the issue of\nfew-shot object detection is also addressed under the same framework. Superior\nperformance over state-of-the-art methods is observed on both tasks.",
          "link": "http://arxiv.org/abs/2107.05005",
          "publishedOn": "2021-07-13T01:59:34.154Z",
          "wordCount": 590,
          "title": "Towards Accurate Localization by Instance Search. (arXiv:2107.05005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1\">Kenta Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1\">Daisuke Iwai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1\">Kosuke Sato</a>",
          "description": "We propose a novel projector-camera system (ProCams) in which each pixel has\nboth projection and capturing capabilities. Our proposed ProCams solves the\ndifficulty of obtaining precise pixel correspondence between the projector and\nthe camera. We implemented a proof-of-concept ProCams prototype and\ndemonstrated its applicability to a dynamic projection mapping.",
          "link": "http://arxiv.org/abs/2107.05043",
          "publishedOn": "2021-07-13T01:59:34.147Z",
          "wordCount": 517,
          "title": "A Projector-Camera System Using Hybrid Pixels with Projection and Capturing Capabilities. (arXiv:2107.05043v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Atul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajesh_B/0/1/0/all/0/1\">Bulla Rajesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "Plant leaf diseases pose a significant danger to food security and they cause\ndepletion in quality and volume of production. Therefore accurate and timely\ndetection of leaf disease is very important to check the loss of the crops and\nmeet the growing food demand of the people. Conventional techniques depend on\nlab investigation and human skills which are generally costly and inaccessible.\nRecently, Deep Neural Networks have been exceptionally fruitful in image\nclassification. In this research paper, plant leaf disease detection employing\ntransfer learning is explored in the JPEG compressed domain. Here, the JPEG\ncompressed stream consisting of DCT coefficients is, directly fed into the\nNeural Network to improve the efficiency of classification. The experimental\nresults on JPEG compressed leaf dataset demonstrate the efficacy of the\nproposed model.",
          "link": "http://arxiv.org/abs/2107.04813",
          "publishedOn": "2021-07-13T01:59:34.141Z",
          "wordCount": 601,
          "title": "Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique. (arXiv:2107.04813v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu-Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chun-Chieh Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kuo-Chin Fan</a>",
          "description": "The development of lightweight object detectors is essential due to the\nlimited computation resources. To reduce the computation cost, how to generate\nredundant features plays a significant role. This paper proposes a new\nlightweight Convolution method Cross-Stage Lightweight (CSL) Module, to\ngenerate redundant features from cheap operations. In the intermediate\nexpansion stage, we replaced Pointwise Convolution with Depthwise Convolution\nto produce candidate features. The proposed CSL-Module can reduce the\ncomputation cost significantly. Experiments conducted at MS-COCO show that the\nproposed CSL-Module can approximate the fitting ability of Convolution-3x3.\nFinally, we use the module to construct a lightweight detector CSL-YOLO,\nachieving better detection performance with only 43% FLOPs and 52% parameters\nthan Tiny-YOLOv4.",
          "link": "http://arxiv.org/abs/2107.04829",
          "publishedOn": "2021-07-13T01:59:34.135Z",
          "wordCount": 551,
          "title": "CSL-YOLO: A New Lightweight Object Detection System for Edge Computing. (arXiv:2107.04829v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danny Z. Chen</a>",
          "description": "A large labeled dataset is a key to the success of supervised deep learning,\nbut for medical image segmentation, it is highly challenging to obtain\nsufficient annotated images for model training. In many scenarios, unannotated\nimages are abundant and easy to acquire. Self-supervised learning (SSL) has\nshown great potentials in exploiting raw data information and representation\nlearning. In this paper, we propose Hierarchical Self-Supervised Learning\n(HSSL), a new self-supervised framework that boosts medical image segmentation\nby making good use of unannotated data. Unlike the current literature on\ntask-specific self-supervised pretraining followed by supervised fine-tuning,\nwe utilize SSL to learn task-agnostic knowledge from heterogeneous data for\nvarious medical image segmentation tasks. Specifically, we first aggregate a\ndataset from several medical challenges, then pre-train the network in a\nself-supervised manner, and finally fine-tune on labeled data. We develop a new\nloss function by combining contrastive loss and classification loss and\npretrain an encoder-decoder architecture for segmentation tasks. Our extensive\nexperiments show that multi-domain joint pre-training benefits downstream\nsegmentation tasks and outperforms single-domain pre-training significantly.\nCompared to learning from scratch, our new method yields better performance on\nvarious tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotated\ndata). With limited amounts of training data, our method can substantially\nbridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% of\nannotated data).",
          "link": "http://arxiv.org/abs/2107.04886",
          "publishedOn": "2021-07-13T01:59:34.128Z",
          "wordCount": 676,
          "title": "Hierarchical Self-Supervised Learning for Medical Image Segmentation Based on Multi-Domain Data Aggregation. (arXiv:2107.04886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>",
          "description": "Domain adaptation (DA) approaches address domain shift and enable networks to\nbe applied to different scenarios. Although various image DA approaches have\nbeen proposed in recent years, there is limited research towards video DA. This\nis partly due to the complexity in adapting the different modalities of\nfeatures in videos, which includes the correlation features extracted as\nlong-term dependencies of pixels across spatiotemporal dimensions. The\ncorrelation features are highly associated with action classes and proven their\neffectiveness in accurate video feature extraction through the supervised\naction recognition task. Yet correlation features of the same action would\ndiffer across domains due to domain shift. Therefore we propose a novel\nAdversarial Correlation Adaptation Network (ACAN) to align action videos by\naligning pixel correlations. ACAN aims to minimize the distribution of\ncorrelation information, termed as Pixel Correlation Discrepancy (PCD).\nAdditionally, video DA research is also limited by the lack of cross-domain\nvideo datasets with larger domain shifts. We, therefore, introduce a novel\nHMDB-ARID dataset with a larger domain shift caused by a larger statistical\ndifference between domains. This dataset is built in an effort to leverage\ncurrent datasets for dark video classification. Empirical results demonstrate\nthe state-of-the-art performance of our proposed ACAN for both existing and the\nnew video DA datasets.",
          "link": "http://arxiv.org/abs/2107.04932",
          "publishedOn": "2021-07-13T01:59:34.097Z",
          "wordCount": 663,
          "title": "Aligning Correlation Information for Domain Adaptation in Action Recognition. (arXiv:2107.04932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1\">Euijoon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dagan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>",
          "description": "The segmentation of medical images is a fundamental step in automated\nclinical decision support systems. Existing medical image segmentation methods\nbased on supervised deep learning, however, remain problematic because of their\nreliance on large amounts of labelled training data. Although medical imaging\ndata repositories continue to expand, there has not been a commensurate\nincrease in the amount of annotated data. Hence, we propose a new spatial\nguided self-supervised clustering network (SGSCN) for medical image\nsegmentation, where we introduce multiple loss functions designed to aid in\ngrouping image pixels that are spatially connected and have similar feature\nrepresentations. It iteratively learns feature representations and clustering\nassignment of each pixel in an end-to-end fashion from a single image. We also\npropose a context-based consistency loss that better delineates the shape and\nboundaries of image regions. It enforces all the pixels belonging to a cluster\nto be spatially close to the cluster centre. We evaluated our method on 2\npublic medical image datasets and compared it to existing conventional and\nself-supervised clustering methods. Experimental results show that our method\nwas most accurate for medical image segmentation.",
          "link": "http://arxiv.org/abs/2107.04934",
          "publishedOn": "2021-07-13T01:59:34.089Z",
          "wordCount": 632,
          "title": "A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation. (arXiv:2107.04934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young Kyun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>",
          "description": "Face image retrieval, which searches for images of the same identity from the\nquery input face image, is drawing more attention as the size of the image\ndatabase increases rapidly. In order to conduct fast and accurate retrieval, a\ncompact hash code-based methods have been proposed, and recently, deep face\nimage hashing methods with supervised classification training have shown\noutstanding performance. However, classification-based scheme has a\ndisadvantage in that it cannot reveal complex similarities between face images\ninto the hash code learning. In this paper, we attempt to improve the face\nimage retrieval quality by proposing a Similarity Guided Hashing (SGH) method,\nwhich gently considers self and pairwise-similarity simultaneously. SGH employs\nvarious data augmentations designed to explore elaborate similarities between\nface images, solving both intra and inter identity-wise difficulties. Extensive\nexperimental results on the protocols with existing benchmarks and an\nadditionally proposed large scale higher resolution face image dataset\ndemonstrate that our SGH delivers state-of-the-art retrieval performance.",
          "link": "http://arxiv.org/abs/2107.05025",
          "publishedOn": "2021-07-13T01:59:34.078Z",
          "wordCount": 596,
          "title": "Similarity Guided Deep Face Image Retrieval. (arXiv:2107.05025v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essam_M/0/1/0/all/0/1\">Mariam Essam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Maha Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sallab_A/0/1/0/all/0/1\">Ahmad El Sallab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Detection of moving objects is a very important task in autonomous driving\nsystems. After the perception phase, motion planning is typically performed in\nBird's Eye View (BEV) space. This would require projection of objects detected\non the image plane to top view BEV plane. Such a projection is prone to errors\ndue to lack of depth information and noisy mapping in far away areas. CNNs can\nleverage the global context in the scene to project better. In this work, we\nexplore end-to-end Moving Object Detection (MOD) on the BEV map directly using\nmonocular images as input. To the best of our knowledge, such a dataset does\nnot exist and we create an extended KITTI-raw dataset consisting of 12.9k\nimages with annotations of moving object masks in BEV space for five classes.\nThe dataset is intended to be used for class agnostic motion cue based object\ndetection and classes are provided as meta-data for better tuning. We design\nand implement a two-stream RGB and optical flow fusion architecture which\noutputs motion segmentation directly in BEV space. We compare it with inverse\nperspective mapping of state-of-the-art motion segmentation predictions on the\nimage plane. We observe a significant improvement of 13% in mIoU using the\nsimple baseline implementation. This demonstrates the ability to directly learn\nmotion segmentation output in BEV space. Qualitative results of our baseline\nand the dataset annotations can be found in\nhttps://sites.google.com/view/bev-modnet.",
          "link": "http://arxiv.org/abs/2107.04937",
          "publishedOn": "2021-07-13T01:59:34.067Z",
          "wordCount": 699,
          "title": "BEV-MODNet: Monocular Camera based Bird's Eye View Moving Object Detection for Autonomous Driving. (arXiv:2107.04937v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1\">Gaurav Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandok_S/0/1/0/all/0/1\">Shivam Chandok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "A common problem with most zero and few-shot learning approaches is they\nsuffer from bias towards seen classes resulting in sub-optimal performance.\nExisting efforts aim to utilize unlabeled images from unseen classes (i.e\ntransductive zero-shot) during training to enable generalization. However, this\nlimits their use in practical scenarios where data from target unseen classes\nis unavailable or infeasible to collect. In this work, we present a practical\nsetting of inductive zero and few-shot learning, where unlabeled images from\nother out-of-data classes, that do not belong to seen or unseen categories, can\nbe used to improve generalization in any-shot learning. We leverage a\nformulation based on product-of-experts and introduce a new AUD module that\nenables us to use unlabeled samples from out-of-data classes which are usually\neasily available and practically entail no annotation cost. In addition, we\nalso demonstrate the applicability of our model to address a more practical and\nchallenging, Generalized Zero-shot under a limited supervision setting, where\neven base seen classes do not have sufficient annotated samples.",
          "link": "http://arxiv.org/abs/2107.04952",
          "publishedOn": "2021-07-13T01:59:34.057Z",
          "wordCount": 614,
          "title": "Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision. (arXiv:2107.04952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Rose McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>",
          "description": "It is challenging for humans to enable visual knowledge discovery in data\nwith more than 2-3 dimensions with a naked eye. This chapter explores the\nefficiency of discovering predictive machine learning models interactively\nusing new Elliptic Paired coordinates (EPC) visualizations. It is shown that\nEPC are capable to visualize multidimensional data and support visual machine\nlearning with preservation of multidimensional information in 2-D. Relative to\nparallel and radial coordinates, EPC visualization requires only a half of the\nvisual elements for each n-D point. An interactive software system EllipseVis,\nwhich is developed in this work, processes high-dimensional datasets, creates\nEPC visualizations, and produces predictive classification models by\ndiscovering dominance rules in EPC. By using interactive and automatic\nprocesses it discovers zones in EPC with a high dominance of a single class.\nThe EPC methodology has been successful in discovering non-linear predictive\nmodels with high coverage and precision in the computational experiments. This\ncan benefit multiple domains by producing visually appealing dominance rules.\nThis chapter presents results of successful testing the EPC non-linear\nmethodology in experiments using real and simulated data, EPC generalized to\nthe Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of\ncoordinates to optimize the visual discovery, introduction of an alternative\nEPC design and introduction of the concept of incompact machine learning\nmethodology based on EPC/DEPC.",
          "link": "http://arxiv.org/abs/2107.04974",
          "publishedOn": "2021-07-13T01:59:34.020Z",
          "wordCount": 660,
          "title": "Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates. (arXiv:2107.04974v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hadviger_A/0/1/0/all/0/1\">Antea Hadviger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cvisic_I/0/1/0/all/0/1\">Igor Cvi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovic_I/0/1/0/all/0/1\">Ivan Markovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrazic_S/0/1/0/all/0/1\">Sacha Vra&#x17e;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovic_I/0/1/0/all/0/1\">Ivan Petrovi&#x107;</a>",
          "description": "Event-based cameras are biologically inspired sensors that output events,\ni.e., asynchronous pixel-wise brightness changes in the scene. Their high\ndynamic range and temporal resolution of a microsecond makes them more reliable\nthan standard cameras in environments of challenging illumination and in\nhigh-speed scenarios, thus developing odometry algorithms based solely on event\ncameras offers exciting new possibilities for autonomous systems and robots. In\nthis paper, we propose a novel stereo visual odometry method for event cameras\nbased on feature detection and matching with careful feature management, while\npose estimation is done by reprojection error minimization. We evaluate the\nperformance of the proposed method on two publicly available datasets: MVSEC\nsequences captured by an indoor flying drone and DSEC outdoor driving\nsequences. MVSEC offers accurate ground truth from motion capture, while for\nDSEC, which does not offer ground truth, in order to obtain a reference\ntrajectory on the standard camera frames we used our SOFT visual odometry, one\nof the highest ranking algorithms on the KITTI scoreboards. We compared our\nmethod to the ESVO method, which is the first and still the only stereo event\nodometry method, showing on par performance on the MVSEC sequences, while on\nthe DSEC dataset ESVO, unlike our method, was unable to handle outdoor driving\nscenario with default parameters. Furthermore, two important advantages of our\nmethod over ESVO are that it adapts tracking frequency to the asynchronous\nevent rate and does not require initialization.",
          "link": "http://arxiv.org/abs/2107.04921",
          "publishedOn": "2021-07-13T01:59:34.006Z",
          "wordCount": 679,
          "title": "Feature-based Event Stereo Visual Odometry. (arXiv:2107.04921v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04943",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1\">Xiaohong Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jianping Zhang</a>",
          "description": "Compressed sensing (CS) is an efficient method to reconstruct MR image from\nsmall sampled data in $k$-space and accelerate the acquisition of MRI. In this\nwork, we propose a novel deep geometric distillation network which combines the\nmerits of model-based and deep learning-based CS-MRI methods, it can be\ntheoretically guaranteed to improve geometric texture details of a linear\nreconstruction. Firstly, we unfold the model-based CS-MRI optimization problem\ninto two sub-problems that consist of image linear approximation and image\ngeometric compensation. Secondly, geometric compensation sub-problem for\ndistilling lost texture details in approximation stage can be expanded by\nTaylor expansion to design a geometric distillation module fusing features of\ndifferent geometric characteristic domains. Additionally, we use a learnable\nversion with adaptive initialization of the step-length parameter, which allows\nmodel more flexibility that can lead to convergent smoothly. Numerical\nexperiments verify its superiority over other state-of-the-art CS-MRI\nreconstruction approaches. The source code will be available at\n\\url{https://github.com/fanxiaohong/Deep-Geometric-Distillation-Network-for-CS-MRI}",
          "link": "http://arxiv.org/abs/2107.04943",
          "publishedOn": "2021-07-13T01:59:33.999Z",
          "wordCount": 623,
          "title": "Deep Geometric Distillation Network for Compressive Sensing MRI. (arXiv:2107.04943v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04847",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuangzhuang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gay_H/0/1/0/all/0/1\">Hiram Gay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_B/0/1/0/all/0/1\">Baozhou Sun</a>",
          "description": "In radiotherapy planning, manual contouring is labor-intensive and\ntime-consuming. Accurate and robust automated segmentation models improve the\nefficiency and treatment outcome. We aim to develop a novel hybrid deep\nlearning approach, combining convolutional neural networks (CNNs) and the\nself-attention mechanism, for rapid and accurate multi-organ segmentation on\nhead and neck computed tomography (CT) images. Head and neck CT images with\nmanual contours of 115 patients were retrospectively collected and used. We set\nthe training/validation/testing ratio to 81/9/25 and used the 10-fold\ncross-validation strategy to select the best model parameters. The proposed\nhybrid model segmented ten organs-at-risk (OARs) altogether for each case. The\nperformance of the model was evaluated by three metrics, i.e., the Dice\nSimilarity Coefficient (DSC), Hausdorff distance 95% (HD95), and mean surface\ndistance (MSD). We also tested the performance of the model on the Head and\nNeck 2015 challenge dataset and compared it against several state-of-the-art\nautomated segmentation algorithms. The proposed method generated contours that\nclosely resemble the ground truth for ten OARs. Our results of the new Weaving\nAttention U-net demonstrate superior or similar performance on the segmentation\nof head and neck CT images.",
          "link": "http://arxiv.org/abs/2107.04847",
          "publishedOn": "2021-07-13T01:59:33.993Z",
          "wordCount": 660,
          "title": "Weaving Attention U-net: A Novel Hybrid CNN and Attention-based Method for Organs-at-risk Segmentation in Head and Neck CT Images. (arXiv:2107.04847v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotseruba_I/0/1/0/all/0/1\">Iuliia Kotseruba</a>",
          "description": "This work aims to study the dynamic between research in the industry and\nacademia in computer vision. The results are demonstrated on a set of top-5\nvision conferences that are representative of the field. Since data for such\nanalysis was not readily available, significant effort was spent on gathering\nand processing meta-data from the original publications. First, this study\nquantifies the share of industry-sponsored research. Specifically, it shows\nthat the proportion of papers published by industry-affiliated researchers is\nincreasing and that more academics join companies or collaborate with them.\nNext, the possible impact of industry presence is further explored, namely in\nthe distribution of research topics and citation patterns. The results indicate\nthat the distribution of the research topics is similar in industry and\nacademic papers. However, there is a strong preference towards citing industry\npapers. Finally, possible reasons for citation bias, such as code availability\nand influence, are investigated.",
          "link": "http://arxiv.org/abs/2107.04902",
          "publishedOn": "2021-07-13T01:59:33.987Z",
          "wordCount": 585,
          "title": "Industry and Academic Research in Computer Vision. (arXiv:2107.04902v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Periyasamy_A/0/1/0/all/0/1\">Arul Selvam Periyasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1\">Max Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "We present SynPick, a synthetic dataset for dynamic scene understanding in\nbin-picking scenarios. In contrast to existing datasets, our dataset is both\nsituated in a realistic industrial application domain -- inspired by the\nwell-known Amazon Robotics Challenge (ARC) -- and features dynamic scenes with\nauthentic picking actions as chosen by our picking heuristic developed for the\nARC 2017. The dataset is compatible with the popular BOP dataset format. We\ndescribe the dataset generation process in detail, including object arrangement\ngeneration and manipulation simulation using the NVIDIA PhysX physics engine.\nTo cover a large action space, we perform untargeted and targeted picking\nactions, as well as random moving actions. To establish a baseline for object\nperception, a state-of-the-art pose estimation approach is evaluated on the\ndataset. We demonstrate the usefulness of tracking poses during manipulation\ninstead of single-shot estimation even with a naive filtering approach. The\ngenerator source code and dataset are publicly available.",
          "link": "http://arxiv.org/abs/2107.04852",
          "publishedOn": "2021-07-13T01:59:33.969Z",
          "wordCount": 606,
          "title": "SynPick: A Dataset for Dynamic Bin Picking Scene Understanding. (arXiv:2107.04852v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uwimana1_A/0/1/0/all/0/1\">Anisie Uwimana1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>",
          "description": "Deep learning models have become a popular choice for medical image analysis.\nHowever, the poor generalization performance of deep learning models limits\nthem from being deployed in the real world as robustness is critical for\nmedical applications. For instance, the state-of-the-art Convolutional Neural\nNetworks (CNNs) fail to detect adversarial samples or samples drawn\nstatistically far away from the training distribution. In this work, we\nexperimentally evaluate the robustness of a Mahalanobis distance-based\nconfidence score, a simple yet effective method for detecting abnormal input\nsamples, in classifying malaria parasitized cells and uninfected cells. Results\nindicated that the Mahalanobis confidence score detector exhibits improved\nperformance and robustness of deep learning models, and achieves\nstateof-the-art performance on both out-of-distribution (OOD) and adversarial\nsamples.",
          "link": "http://arxiv.org/abs/2107.04882",
          "publishedOn": "2021-07-13T01:59:33.962Z",
          "wordCount": 578,
          "title": "Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis. (arXiv:2107.04882v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhuheir_M/0/1/0/all/0/1\">Marwan Dhuheir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1\">Emna Baccour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabeeh_S/0/1/0/all/0/1\">Sinan Sabeeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1\">Mounir Hamdi</a>",
          "description": "Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention\ndue to their outstanding ability to be used in different sectors and serve in\ndifficult and dangerous areas. Moreover, the advancements in computer vision\nand artificial intelligence have increased the use of UAVs in various\napplications and solutions, such as forest fires detection and borders\nmonitoring. However, using deep neural networks (DNNs) with UAVs introduces\nseveral challenges of processing deeper networks and complex models, which\nrestricts their on-board computation. In this work, we present a strategy\naiming at distributing inference requests to a swarm of resource-constrained\nUAVs that classifies captured images on-board and finds the minimum\ndecision-making latency. We formulate the model as an optimization problem that\nminimizes the latency between acquiring images and making the final decisions.\nThe formulated optimization solution is an NP-hard problem. Hence it is not\nadequate for online resource allocation. Therefore, we introduce an online\nheuristic solution, namely DistInference, to find the layers placement strategy\nthat gives the best latency among the available UAVs. The proposed approach is\ngeneral enough to be used for different low decision-latency applications as\nwell as for all CNN types organized into the pipeline of layers (e.g., VGG) or\nbased on residual blocks (e.g., ResNet).",
          "link": "http://arxiv.org/abs/2107.04648",
          "publishedOn": "2021-07-13T01:59:33.955Z",
          "wordCount": 677,
          "title": "Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs and Convolutional Networks. (arXiv:2107.04648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>",
          "description": "Partial Domain Adaptation (PDA) is a practical and general domain adaptation\nscenario, which relaxes the fully shared label space assumption such that the\nsource label space subsumes the target one. The key challenge of PDA is the\nissue of negative transfer caused by source-only classes. For videos, such\nnegative transfer could be triggered by both spatial and temporal features,\nwhich leads to a more challenging Partial Video Domain Adaptation (PVDA)\nproblem. In this paper, we propose a novel Partial Adversarial Temporal\nAttentive Network (PATAN) to address the PVDA problem by utilizing both spatial\nand temporal features for filtering source-only classes. Besides, PATAN\nconstructs effective overall temporal features by attending to local temporal\nfeatures that contribute more toward the class filtration process. We further\nintroduce new benchmarks to facilitate research on PVDA problems, covering a\nwide range of PVDA scenarios. Empirical results demonstrate the\nstate-of-the-art performance of our proposed PATAN across the multiple PVDA\nbenchmarks.",
          "link": "http://arxiv.org/abs/2107.04941",
          "publishedOn": "2021-07-13T01:59:33.948Z",
          "wordCount": 617,
          "title": "Partial Video Domain Adaptation with Partial Adversarial Temporal Attentive Network. (arXiv:2107.04941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04808",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Miron_R/0/1/0/all/0/1\">Radu Miron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moisii_C/0/1/0/all/0/1\">Cosmin Moisii</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dinu_S/0/1/0/all/0/1\">Sergiu Dinu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breaban_M/0/1/0/all/0/1\">Mihaela Breaban</a>",
          "description": "The paper presents a comparative analysis of three distinct approaches based\non deep learning for COVID-19 detection in chest CTs. The first approach is a\nvolumetric one, involving 3D convolutions, while the other two approaches\nperform at first slice-wise classification and then aggregate the results at\nthe volume level. The experiments are carried on the COV19-CT-DB dataset, with\nthe aim of addressing the challenge raised by the MIA-COV19D Competition within\nICCV 2021. Our best results on the validation subset reach a macro-F1 score of\n0.92, which improves considerably the baseline score of 0.70 set by the\norganizers.",
          "link": "http://arxiv.org/abs/2107.04808",
          "publishedOn": "2021-07-13T01:59:33.940Z",
          "wordCount": 590,
          "title": "COVID Detection in Chest CTs: Improving the Baseline on COV19-CT-DB. (arXiv:2107.04808v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1\">Nikos Makris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J. O&#x27;Donnell</a>",
          "description": "White matter fiber clustering (WMFC) enables parcellation of white matter\ntractography for applications such as disease classification and anatomical\ntract segmentation. However, the lack of ground truth and the ambiguity of\nfiber data (the points along a fiber can equivalently be represented in forward\nor reverse order) pose challenges to this task. We propose a novel WMFC\nframework based on unsupervised deep learning. We solve the unsupervised\nclustering problem as a self-supervised learning task. Specifically, we use a\nconvolutional neural network to learn embeddings of input fibers, using\npairwise fiber distances as pseudo annotations. This enables WMFC that is\ninsensitive to fiber point ordering. In addition, anatomical coherence of fiber\nclusters is improved by incorporating brain anatomical segmentation data. The\nproposed framework enables outlier removal in a natural way by rejecting fibers\nwith low cluster assignment probability. We train and evaluate our method using\n200 datasets from the Human Connectome Project. Results demonstrate superior\nperformance and efficiency of the proposed approach.",
          "link": "http://arxiv.org/abs/2107.04938",
          "publishedOn": "2021-07-13T01:59:33.914Z",
          "wordCount": 623,
          "title": "Deep Fiber Clustering: Anatomically Informed Unsupervised Deep Learning for Fast and Effective White Matter Parcellation. (arXiv:2107.04938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04930",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Teli_M/0/1/0/all/0/1\">Mohammad Nayeem Teli</a>",
          "description": "Hundreds of millions of cases and millions of deaths have occurred worldwide\ndue to COVID-19. The fight against this pandemic is on-going on multiple\nfronts. While vaccinations are picking up speed, there are still billions of\nunvaccinated people. In this fight diagnosis of the disease and isolation of\nthe patients to prevent any spreads play a huge role. Machine Learning\napproaches have assisted the diagnosis of COVID-19 cases by analyzing chest\nX-ray and CT-scan images of patients. In this research we present a simple and\nshallow Convolutional Neural Network based approach, TeliNet, to classify\nCT-scan images of COVID-19 patients. Our results outperform the F1 score of\nVGGNet and the benchmark approaches. Our proposed solution is also more\nlightweight in comparison to the other methods.",
          "link": "http://arxiv.org/abs/2107.04930",
          "publishedOn": "2021-07-13T01:59:33.905Z",
          "wordCount": 631,
          "title": "TeliNet, a simple and shallow Convolution Neural Network (CNN) to Classify CT Scans of COVID-19 patients. (arXiv:2107.04930v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">An-Chieh Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xueting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>",
          "description": "We propose a canonical point autoencoder (CPAE) that predicts dense\ncorrespondences between 3D shapes of the same category. The autoencoder\nperforms two key functions: (a) encoding an arbitrarily ordered point cloud to\na canonical primitive, e.g., a sphere, and (b) decoding the primitive back to\nthe original input instance shape. As being placed in the bottleneck, this\nprimitive plays a key role to map all the unordered point clouds on the\ncanonical surface and to be reconstructed in an ordered fashion. Once trained,\npoints from different shape instances that are mapped to the same locations on\nthe primitive surface are determined to be a pair of correspondence. Our method\ndoes not require any form of annotation or self-supervised part segmentation\nnetwork and can handle unaligned input point clouds. Experimental results on 3D\nsemantic keypoint transfer and part segmentation transfer show that our model\nperforms favorably against state-of-the-art correspondence learning methods.",
          "link": "http://arxiv.org/abs/2107.04867",
          "publishedOn": "2021-07-13T01:59:33.894Z",
          "wordCount": 587,
          "title": "Learning 3D Dense Correspondence via Canonical Point Autoencoder. (arXiv:2107.04867v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zakazov_I/0/1/0/all/0/1\">Ivan Zakazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirokikh_B/0/1/0/all/0/1\">Boris Shirokikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Alexey Chernyavskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>",
          "description": "Domain Adaptation (DA) methods are widely used in medical image segmentation\ntasks to tackle the problem of differently distributed train (source) and test\n(target) data. We consider the supervised DA task with a limited number of\nannotated samples from the target domain. It corresponds to one of the most\nrelevant clinical setups: building a sufficiently accurate model on the minimum\npossible amount of annotated data. Existing methods mostly fine-tune specific\nlayers of the pretrained Convolutional Neural Network (CNN). However, there is\nno consensus on which layers are better to fine-tune, e.g. the first layers for\nimages with low-level domain shift or the deeper layers for images with\nhigh-level domain shift. To this end, we propose SpotTUnet - a CNN architecture\nthat automatically chooses the layers which should be optimally fine-tuned.\nMore specifically, on the target domain, our method additionally learns the\npolicy that indicates whether a specific layer should be fine-tuned or reused\nfrom the pretrained network. We show that our method performs at the same level\nas the best of the nonflexible fine-tuning methods even under the extreme\nscarcity of annotated data. Secondly, we show that SpotTUnet policy provides a\nlayer-wise visualization of the domain shift impact on the network, which could\nbe further used to develop robust domain generalization methods. In order to\nextensively evaluate SpotTUnet performance, we use a publicly available dataset\nof brain MR images (CC359), characterized by explicit domain shift. We release\na reproducible experimental pipeline.",
          "link": "http://arxiv.org/abs/2107.04914",
          "publishedOn": "2021-07-13T01:59:33.886Z",
          "wordCount": 698,
          "title": "Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation. (arXiv:2107.04914v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04823",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhonghua Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jiewei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yijin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_J/0/1/0/all/0/1\">Junyan Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jiong Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>",
          "description": "Optical coherence tomography angiography (OCTA) is a novel non-invasive\nimaging technique that allows visualizations of vasculature and foveal\navascular zone (FAZ) across retinal layers. Clinical researches suggest that\nthe morphology and contour irregularity of FAZ are important biomarkers of\nvarious ocular pathologies. Therefore, precise segmentation of FAZ has great\nclinical interest. Also, there is no existing research reporting that FAZ\nfeatures can improve the performance of deep diagnostic classification\nnetworks. In this paper, we propose a novel multi-level boundary shape and\ndistance aware joint learning framework, named BSDA-Net, for FAZ segmentation\nand diagnostic classification from OCTA images. Two auxiliary branches, namely\nboundary heatmap regression and signed distance map reconstruction branches,\nare constructed in addition to the segmentation branch to improve the\nsegmentation performance, resulting in more accurate FAZ contours and fewer\noutliers. Moreover, both low-level and high-level features from the\naforementioned three branches, including shape, size, boundary, and signed\ndirectional distance map of FAZ, are fused hierarchically with features from\nthe diagnostic classifier. Through extensive experiments, the proposed BSDA-Net\nis found to yield state-of-the-art segmentation and classification results on\nthe OCTA-500, OCTAGON, and FAZID datasets.",
          "link": "http://arxiv.org/abs/2107.04823",
          "publishedOn": "2021-07-13T01:59:33.878Z",
          "wordCount": 658,
          "title": "BSDA-Net: A Boundary Shape and Distance Aware Joint Learning Framework for Segmenting and Classifying OCTA Images. (arXiv:2107.04823v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_F/0/1/0/all/0/1\">Feng Shuang</a>",
          "description": "Monocular depth estimation (MDE) is a fundamental task in many applications\nsuch as scene understanding and reconstruction. However, most of the existing\nmethods rely on accurately labeled datasets. A weakly-supervised framework\nbased on attention nested U-net (ANU) named as ANUW is introduced in this paper\nfor cases with wrong labels. The ANUW is trained end-to-end to convert an input\nsingle RGB image into a depth image. It consists of a dense residual network\nstructure, an adaptive weight channel attention (AWCA) module, a patch second\nnon-local (PSNL) module and a soft label generation method. The dense residual\nnetwork is the main body of the network to encode and decode the input. The\nAWCA module can adaptively adjust the channel weights to extract important\nfeatures. The PSNL module implements the spatial attention mechanism through a\nsecond-order non-local method. The proposed soft label generation method uses\nthe prior knowledge of the dataset to produce soft labels to replace false\nones. The proposed ANUW is trained on a defective monocular depth dataset and\nthe trained model is tested on three public datasets, and the results\ndemonstrate the superiority of ANUW in comparison with the state-of-the-art MDE\nmethods.",
          "link": "http://arxiv.org/abs/2107.04819",
          "publishedOn": "2021-07-13T01:59:33.855Z",
          "wordCount": 636,
          "title": "A Weakly-Supervised Depth Estimation Network Using Attention Mechanism. (arXiv:2107.04819v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_S/0/1/0/all/0/1\">Shea Hagstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pak_H/0/1/0/all/0/1\">Hee Won Pak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_S/0/1/0/all/0/1\">Stephanie Ku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sean Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Myron Brown</a>",
          "description": "Urban 3D modeling from satellite images requires accurate semantic\nsegmentation to delineate urban features, multiple view stereo for 3D\nreconstruction of surface heights, and 3D model fitting to produce compact\nmodels with accurate surface slopes. In this work, we present a cumulative\nassessment metric that succinctly captures error contributions from each of\nthese components. We demonstrate our approach by providing challenging public\ndatasets and extending two open source projects to provide an end-to-end 3D\nmodeling baseline solution to stimulate further research and evaluation with a\npublic leaderboard.",
          "link": "http://arxiv.org/abs/2107.04622",
          "publishedOn": "2021-07-13T01:59:33.849Z",
          "wordCount": 536,
          "title": "Cumulative Assessment for Urban 3D Modeling. (arXiv:2107.04622v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "In this paper, we propose a new continuously learning generative model,\ncalled the Lifelong Twin Generative Adversarial Networks (LT-GANs). LT-GANs\nlearns a sequence of tasks from several databases and its architecture consists\nof three components: two identical generators, namely the Teacher and\nAssistant, and one Discriminator. In order to allow for the LT-GANs to learn\nnew concepts without forgetting, we introduce a new lifelong training approach,\nnamely Lifelong Adversarial Knowledge Distillation (LAKD), which encourages the\nTeacher and Assistant to alternately teach each other, while learning a new\ndatabase. This training approach favours transferring knowledge from a more\nknowledgeable player to another player which knows less information about a\npreviously given task.",
          "link": "http://arxiv.org/abs/2107.04708",
          "publishedOn": "2021-07-13T01:59:33.843Z",
          "wordCount": 551,
          "title": "Lifelong Twin Generative Adversarial Networks. (arXiv:2107.04708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaohua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_X/0/1/0/all/0/1\">Xiuchao Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yangqin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1\">Daniel Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>",
          "description": "Deep neural networks (DNNs) trained on one set of medical images often\nexperience severe performance drop on unseen test images, due to various domain\ndiscrepancy between the training images (source domain) and the test images\n(target domain), which raises a domain adaptation issue. In clinical settings,\nit is difficult to collect enough annotated target domain data in a short\nperiod. Few-shot domain adaptation, i.e., adapting a trained model with a\nhandful of annotations, is highly practical and useful in this case. In this\npaper, we propose a Polymorphic Transformer (Polyformer), which can be\nincorporated into any DNN backbones for few-shot domain adaptation.\nSpecifically, after the polyformer layer is inserted into a model trained on\nthe source domain, it extracts a set of prototype embeddings, which can be\nviewed as a \"basis\" of the source-domain features. On the target domain, the\npolyformer layer adapts by only updating a projection layer which controls the\ninteractions between image features and the prototype embeddings. All other\nmodel weights (except BatchNorm parameters) are frozen during adaptation. Thus,\nthe chance of overfitting the annotations is greatly reduced, and the model can\nperform robustly on the target domain after being trained on a few annotated\nimages. We demonstrate the effectiveness of Polyformer on two medical\nsegmentation tasks (i.e., optic disc/cup segmentation, and polyp segmentation).\nThe source code of Polyformer is released at\nhttps://github.com/askerlee/segtran.",
          "link": "http://arxiv.org/abs/2107.04805",
          "publishedOn": "2021-07-13T01:59:33.836Z",
          "wordCount": 678,
          "title": "Few-Shot Domain Adaptation with Polymorphic Transformers. (arXiv:2107.04805v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shoaib Ahmed Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>",
          "description": "Common neural network architectures are susceptible to attack by adversarial\nsamples. Neural network architectures are commonly thought of as divided into\nlow-level feature extraction layers and high-level classification layers;\nsusceptibility of networks to adversarial samples is often thought of as a\nproblem related to classification rather than feature extraction. We test this\nidea by selectively retraining different portions of VGG and ResNet\narchitectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and\nadversarial data. Our experimental results show that susceptibility to\nadversarial samples is associated with low-level feature extraction layers.\nTherefore, retraining high-level layers is insufficient for achieving\nrobustness. This phenomenon could have two explanations: either, adversarial\nattacks yield outputs from early layers that are indistinguishable from\nfeatures found in the attack classes, or adversarial attacks yield outputs from\nearly layers that differ statistically from features for non-adversarial\nsamples and do not permit consistent classification by subsequent layers. We\ntest this question by large-scale non-linear dimensionality reduction and\ndensity modeling on distributions of feature vectors in hidden layers and find\nthat the feature distributions between non-adversarial and adversarial samples\ndiffer substantially. Our results provide new insights into the statistical\norigins of adversarial samples and possible defenses.",
          "link": "http://arxiv.org/abs/2107.04827",
          "publishedOn": "2021-07-13T01:59:33.829Z",
          "wordCount": 630,
          "title": "Identifying Layers Susceptible to Adversarial Attacks. (arXiv:2107.04827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1\">Wei Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hailan Huang</a>",
          "description": "The seven basic facial expression classifications are a basic way to express\ncomplex human emotions and are an important part of artificial intelligence\nresearch. Based on the traditional Bayesian neural network framework, the\nResNet-18_BNN network constructed in this paper has been improved in the\nfollowing three aspects: (1) A new objective function is proposed, which is\ncomposed of the KL loss of uncertain parameters and the intersection of\nspecific parameters. Entropy loss composition. (2) Aiming at a special\nobjective function, a training scheme for alternately updating these two\nparameters is proposed. (3) Only model the parameters of the last convolution\ngroup. According to experimental analysis, our method achieves an accuracy of\n98.28% on the evaluation set of the Aff-Wild2 database. Compared with the\ntraditional Bayesian Neural Network, our method brings the highest\nclassification accuracy gain.",
          "link": "http://arxiv.org/abs/2107.04834",
          "publishedOn": "2021-07-13T01:59:33.791Z",
          "wordCount": 570,
          "title": "Bayesian Convolutional Neural Networks for Seven Basic Facial Expression Classifications. (arXiv:2107.04834v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_F/0/1/0/all/0/1\">Fangqiu Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingting Jiang</a>",
          "description": "Surgical phase recognition is of particular interest to computer assisted\nsurgery systems, in which the goal is to predict what phase is occurring at\neach frame for a surgery video. Networks with multi-stage architecture have\nbeen widely applied in many computer vision tasks with rich patterns, where a\npredictor stage first outputs initial predictions and an additional refinement\nstage operates on the initial predictions to perform further refinement.\nExisting works show that surgical video contents are well ordered and contain\nrich temporal patterns, making the multi-stage architecture well suited for the\nsurgical phase recognition task. However, we observe that when simply applying\nthe multi-stage architecture to the surgical phase recognition task, the\nend-to-end training manner will make the refinement ability fall short of its\nwishes. To address the problem, we propose a new non end-to-end training\nstrategy and explore different designs of multi-stage architecture for surgical\nphase recognition task. For the non end-to-end training strategy, the\nrefinement stage is trained separately with proposed two types of disturbed\nsequences. Meanwhile, we evaluate three different choices of refinement models\nto show that our analysis and solution are robust to the choices of specific\nmulti-stage models. We conduct experiments on two public benchmarks, the\nM2CAI16 Workflow Challenge, and the Cholec80 dataset. Results show that\nmulti-stage architecture trained with our strategy largely boosts the\nperformance of the current state-of-the-art single-stage model. Code is\navailable at \\url{https://github.com/ChinaYi/casual_tcn}.",
          "link": "http://arxiv.org/abs/2107.04810",
          "publishedOn": "2021-07-13T01:59:33.780Z",
          "wordCount": 679,
          "title": "Not End-to-End: Explore Multi-Stage Architecture for Online Surgical Phase Recognition. (arXiv:2107.04810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1\">Darshan Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1\">S. Balasubramanian</a>",
          "description": "Presence of noise in the labels of large scale facial expression datasets has\nbeen a key challenge towards Facial Expression Recognition (FER) in the wild.\nDuring early learning stage, deep networks fit on clean data. Then, eventually,\nthey start overfitting on noisy labels due to their memorization ability, which\nlimits FER performance. This work proposes an effective training strategy in\nthe presence of noisy labels, called as Consensual Collaborative Training (CCT)\nframework. CCT co-trains three networks jointly using a convex combination of\nsupervision loss and consistency loss, without making any assumption about the\nnoise distribution. A dynamic transition mechanism is used to move from\nsupervision loss in early learning to consistency loss for consensus of\npredictions among networks in the later stage. Inference is done using a single\nnetwork based on a simple knowledge distillation scheme. Effectiveness of the\nproposed framework is demonstrated on synthetic as well as real noisy FER\ndatasets. In addition, a large test subset of around 5K images is annotated\nfrom the FEC dataset using crowd wisdom of 16 different annotators and reliable\nlabels are inferred. CCT is also validated on it. State-of-the-art performance\nis reported on the benchmark FER datasets RAFDB (90.84%) FERPlus (89.99%) and\nAffectNet (66%). Our codes are available at https://github.com/1980x/CCT.",
          "link": "http://arxiv.org/abs/2107.04746",
          "publishedOn": "2021-07-13T01:59:33.759Z",
          "wordCount": 681,
          "title": "Consensual Collaborative Training And Knowledge Distillation Based Facial Expression Recognition Under Noisy Annotations. (arXiv:2107.04746v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ronghang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>",
          "description": "Owing to the remarkable capability of extracting effective graph embeddings,\ngraph convolutional network (GCN) and its variants have been successfully\napplied to a broad range of tasks, such as node classification, link\nprediction, and graph classification. Traditional GCN models suffer from the\nissues of overfitting and oversmoothing, while some recent techniques like\nDropEdge could alleviate these issues and thus enable the development of deep\nGCN. However, training GCN models is non-trivial, as it is sensitive to the\nchoice of hyperparameters such as dropout rate and learning weight decay,\nespecially for deep GCN models. In this paper, we aim to automate the training\nof GCN models through hyperparameter optimization. To be specific, we propose a\nself-tuning GCN approach with an alternate training algorithm, and further\nextend our approach by incorporating the population based training scheme.\nExperimental results on three benchmark datasets demonstrate the effectiveness\nof our approaches on optimizing multi-layer GCN, compared with several\nrepresentative baselines.",
          "link": "http://arxiv.org/abs/2107.04713",
          "publishedOn": "2021-07-13T01:59:33.731Z",
          "wordCount": 600,
          "title": "Automated Graph Learning via Population Based Self-Tuning GCN. (arXiv:2107.04713v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "A unique cognitive capability of humans consists in their ability to acquire\nnew knowledge and skills from a sequence of experiences. Meanwhile, artificial\nintelligence systems are good at learning only the last given task without\nbeing able to remember the databases learnt in the past. We propose a novel\nlifelong learning methodology by employing a Teacher-Student network framework.\nWhile the Student module is trained with a new given database, the Teacher\nmodule would remind the Student about the information learnt in the past. The\nTeacher, implemented by a Generative Adversarial Network (GAN), is trained to\npreserve and replay past knowledge corresponding to the probabilistic\nrepresentations of previously learn databases. Meanwhile, the Student module is\nimplemented by a Variational Autoencoder (VAE) which infers its latent variable\nrepresentation from both the output of the Teacher module as well as from the\nnewly available database. Moreover, the Student module is trained to capture\nboth continuous and discrete underlying data representations across different\ndomains. The proposed lifelong learning framework is applied in supervised,\nsemi-supervised and unsupervised training. The code is available~:\n\\url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}",
          "link": "http://arxiv.org/abs/2107.04689",
          "publishedOn": "2021-07-13T01:59:33.711Z",
          "wordCount": 625,
          "title": "Lifelong Teacher-Student Network Learning. (arXiv:2107.04689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teli_M/0/1/0/all/0/1\">Mohammad Nayeem Teli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seungwon Oh</a>",
          "description": "Due to the vulnerability of deep neural networks to adversarial examples,\nnumerous works on adversarial attacks and defenses have been burgeoning over\nthe past several years. However, there seem to be some conventional views\nregarding adversarial attacks and object detection approaches that most\nresearchers take for granted. In this work, we bring a fresh perspective on\nthose procedures by evaluating the impact of universal perturbations on object\ndetection at a class-level. We apply it to a carefully curated data set related\nto autonomous driving. We use Faster-RCNN object detector on images of five\ndifferent categories: person, car, truck, stop sign and traffic light from the\nCOCO data set, while carefully perturbing the images using Universal Dense\nObject Suppression algorithm. Our results indicate that person, car, traffic\nlight, truck and stop sign are resilient in that order (most to least) to\nuniversal perturbations. To the best of our knowledge, this is the first time\nsuch a ranking has been established which is significant for the security of\nthe data sets pertaining to autonomous vehicles and object detection in\ngeneral.",
          "link": "http://arxiv.org/abs/2107.04749",
          "publishedOn": "2021-07-13T01:59:33.685Z",
          "wordCount": 633,
          "title": "Resilience of Autonomous Vehicle Object Category Detection to Universal Adversarial Perturbations. (arXiv:2107.04749v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Richard Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lihan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huster_T/0/1/0/all/0/1\">Todd Huster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_W/0/1/0/all/0/1\">William Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arleth_S/0/1/0/all/0/1\">Stephen Arleth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Justin Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridge_D/0/1/0/all/0/1\">Devin Ridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_M/0/1/0/all/0/1\">Michael Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Headley_W/0/1/0/all/0/1\">William C. Headley</a>",
          "description": "This paper describes a systematic approach towards building a new family of\nneural networks based on a delay-loop version of a reservoir neural network.\nThe resulting architecture, called Scaled-Time-Attention Robust Edge (STARE)\nnetwork, exploits hyper dimensional space and non-multiply-and-add computation\nto achieve a simpler architecture, which has shallow layers, is simple to\ntrain, and is better suited for Edge applications, such as Internet of Things\n(IoT), over traditional deep neural networks. STARE incorporates new AI\nconcepts such as Attention and Context, and is best suited for temporal feature\nextraction and classification. We demonstrate that STARE is applicable to a\nvariety of applications with improved performance and lower implementation\ncomplexity. In particular, we showed a novel way of applying a dual-loop\nconfiguration to detection and identification of drone vs bird in a counter\nUnmanned Air Systems (UAS) detection application by exploiting both spatial\n(video frame) and temporal (trajectory) information. We also demonstrated that\nthe STARE performance approaches that of a State-of-the-Art deep neural network\nin classifying RF modulations, and outperforms Long Short-term Memory (LSTM) in\na special case of Mackey Glass time series prediction. To demonstrate hardware\nefficiency, we designed and developed an FPGA implementation of the STARE\nalgorithm to demonstrate its low-power and high-throughput operations. In\naddition, we illustrate an efficient structure for integrating a massively\nparallel implementation of the STARE algorithm for ASIC implementation.",
          "link": "http://arxiv.org/abs/2107.04688",
          "publishedOn": "2021-07-13T01:59:33.662Z",
          "wordCount": 683,
          "title": "Scaled-Time-Attention Robust Edge Network. (arXiv:2107.04688v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shuwei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Co-training, extended from self-training, is one of the frameworks for\nsemi-supervised learning. It works at the cost of training extra classifiers,\nwhere the algorithm should be delicately designed to prevent individual\nclassifiers from collapsing into each other. In this paper, we present a simple\nand efficient co-training algorithm, named Multi-Head Co-Training, for\nsemi-supervised image classification. By integrating base learners into a\nmulti-head structure, the model is in a minimal amount of extra parameters.\nEvery classification head in the unified model interacts with its peers through\na \"Weak and Strong Augmentation\" strategy, achieving single-view co-training\nwithout promoting diversity explicitly. The effectiveness of Multi-Head\nCo-Training is demonstrated in an empirical study on standard semi-supervised\nlearning benchmarks.",
          "link": "http://arxiv.org/abs/2107.04795",
          "publishedOn": "2021-07-13T01:59:33.650Z",
          "wordCount": 547,
          "title": "Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huabin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1\">Mengjuan Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>",
          "description": "Few-shot action recognition aims to recognize novel action classes (query)\nusing just a few samples (support). The majority of current approaches follow\nthe metric learning paradigm, which learns to compare the similarity between\nvideos. Recently, it has been observed that directly measuring this similarity\nis not ideal since different action instances may show distinctive temporal\ndistribution, resulting in severe misalignment issues across query and support\nvideos. In this paper, we arrest this problem from two distinct aspects --\naction duration misalignment and motion evolution misalignment. We address them\nsequentially through a Two-stage Temporal Alignment Network (TTAN). The first\nstage performs temporal transformation with the predicted affine warp\nparameters, while the second stage utilizes a cross-attention mechanism to\ncoordinate the features of the support and query to a consistent evolution.\nBesides, we devise a novel multi-shot fusion strategy, which takes the\nmisalignment among support samples into consideration. Ablation studies and\nvisualizations demonstrate the role played by both stages in addressing the\nmisalignment. Extensive experiments on benchmark datasets show the potential of\nthe proposed method in achieving state-of-the-art performance for few-shot\naction recognition.",
          "link": "http://arxiv.org/abs/2107.04782",
          "publishedOn": "2021-07-13T01:59:33.636Z",
          "wordCount": 626,
          "title": "TTAN: Two-Stage Temporal Alignment Network for Few-shot Action Recognition. (arXiv:2107.04782v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "In this paper, we propose an end-to-end lifelong learning mixture of experts.\nEach expert is implemented by a Variational Autoencoder (VAE). The experts in\nthe mixture system are jointly trained by maximizing a mixture of individual\ncomponent evidence lower bounds (MELBO) on the log-likelihood of the given\ntraining samples. The mixing coefficients in the mixture, control the\ncontributions of each expert in the goal representation. These are sampled from\na Dirichlet distribution whose parameters are determined through non-parametric\nestimation during lifelong learning. The model can learn new tasks fast when\nthese are similar to those previously learnt. The proposed Lifelong mixture of\nVAE (L-MVAE) expands its architecture with new components when learning a\ncompletely new task. After the training, our model can automatically determine\nthe relevant expert to be used when fed with new data samples. This mechanism\nbenefits both the memory efficiency and the required computational cost as only\none expert is used during the inference. The L-MVAE inference model is able to\nperform interpolation in the joint latent space across the data domains\nassociated with different tasks and is shown to be efficient for disentangled\nlearning representation.",
          "link": "http://arxiv.org/abs/2107.04694",
          "publishedOn": "2021-07-13T01:59:33.629Z",
          "wordCount": 631,
          "title": "Lifelong Mixture of Variational Autoencoders. (arXiv:2107.04694v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Transformers have demonstrated great potential in computer vision tasks. To\navoid dense computations of self-attentions in high-resolution visual data,\nsome recent Transformer models adopt a hierarchical design, where\nself-attentions are only computed within local windows. This design\nsignificantly improves the efficiency but lacks global feature reasoning in\nearly stages. In this work, we design a multi-path structure of the\nTransformer, which enables local-to-global reasoning at multiple granularities\nin each stage. The proposed framework is computationally efficient and highly\neffective. With a marginal increasement in computational overhead, our model\nachieves notable improvements in both image classification and semantic\nsegmentation. Code is available at https://github.com/ljpadam/LG-Transformer",
          "link": "http://arxiv.org/abs/2107.04735",
          "publishedOn": "2021-07-13T01:59:33.605Z",
          "wordCount": 537,
          "title": "Local-to-Global Self-Attention in Vision Transformers. (arXiv:2107.04735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1\">Bing-Kun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>",
          "description": "Video question answering is a challenging task, which requires agents to be\nable to understand rich video contents and perform spatial-temporal reasoning.\nHowever, existing graph-based methods fail to perform multi-step reasoning\nwell, neglecting two properties of VideoQA: (1) Even for the same video,\ndifferent questions may require different amount of video clips or objects to\ninfer the answer with relational reasoning; (2) During reasoning, appearance\nand motion features have complicated interdependence which are correlated and\ncomplementary to each other. Based on these observations, we propose a\nDual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an\nend-to-end fashion. The first contribution of our DualVGR is the design of an\nexplainable Query Punishment Module, which can filter out irrelevant visual\nfeatures through multiple cycles of reasoning. The second contribution is the\nproposed Video-based Multi-view Graph Attention Network, which captures the\nrelations between appearance and motion features. Our DualVGR network achieves\nstate-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and\ndemonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is\navailable at https://github.com/MMIR/DualVGR-VideoQA.",
          "link": "http://arxiv.org/abs/2107.04768",
          "publishedOn": "2021-07-13T01:59:33.598Z",
          "wordCount": 628,
          "title": "DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering. (arXiv:2107.04768v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "Learning disentangled and interpretable representations is an important step\ntowards accomplishing comprehensive data representations on the manifold. In\nthis paper, we propose a novel representation learning algorithm which combines\nthe inference abilities of Variational Autoencoders (VAE) with the\ngeneralization capability of Generative Adversarial Networks (GAN). The\nproposed model, called InfoVAEGAN, consists of three networks~: Encoder,\nGenerator and Discriminator. InfoVAEGAN aims to jointly learn discrete and\ncontinuous interpretable representations in an unsupervised manner by using two\ndifferent data-free log-likelihood functions onto the variables sampled from\nthe generator's distribution. We propose a two-stage algorithm for optimizing\nthe inference network separately from the generator training. Moreover, we\nenforce the learning of interpretable representations through the maximization\nof the mutual information between the existing latent variables and those\ncreated through generative and inference processes.",
          "link": "http://arxiv.org/abs/2107.04705",
          "publishedOn": "2021-07-13T01:59:33.590Z",
          "wordCount": 585,
          "title": "InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood. (arXiv:2107.04705v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1\">Mayur R. Parate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhurchandi_K/0/1/0/all/0/1\">Kishor M. Bhurchandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Ashwin G. Kothari</a>",
          "description": "Intelligent resident surveillance is one of the most essential smart\ncommunity services. The increasing demand for security needs surveillance\nsystems to be able to detect anomalies in surveillance scenes. Employing\nhigh-capacity computational devices for intelligent surveillance in residential\nsocieties is costly and not feasible. Therefore, we propose anomaly detection\nfor intelligent surveillance using CPU-only edge devices. A modular framework\nto capture object-level inferences and tracking is developed. To cope with\npartial occlusions, posture deformations, and complex scenes we employed\nfeature encoding and trajectory associations. Elements of the anomaly detection\nframework are optimized to run on CPU-only edge devices with sufficient FPS.\nThe experimental results indicate the proposed method is feasible and achieves\nsatisfactory results in real-life scenarios.",
          "link": "http://arxiv.org/abs/2107.04767",
          "publishedOn": "2021-07-13T01:59:33.576Z",
          "wordCount": 573,
          "title": "Anomaly Detection in Residential Video Surveillance on Edge Devices in IoT Framework. (arXiv:2107.04767v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wight_C/0/1/0/all/0/1\">Colby Wight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akers_S/0/1/0/all/0/1\">Sarah Akers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1\">Scott Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Woongjo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosink_L/0/1/0/all/0/1\">Luke Gosink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurrus_E/0/1/0/all/0/1\">Elizabeth Jurrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kappagantula_K/0/1/0/all/0/1\">Keerti Kappagantula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1\">Tegan H. Emerson</a>",
          "description": "As both machine learning models and the datasets on which they are evaluated\nhave grown in size and complexity, the practice of using a few summary\nstatistics to understand model performance has become increasingly problematic.\nThis is particularly true in real-world scenarios where understanding model\nfailure on certain subpopulations of the data is of critical importance. In\nthis paper we propose a topological framework for evaluating machine learning\nmodels in which a dataset is treated as a \"space\" on which a model operates.\nThis provides us with a principled way to organize information about model\nperformance at both the global level (over the entire test set) and also the\nlocal level (on specific subpopulations). Finally, we describe a topological\ndata structure, presheaves, which offer a convenient way to store and analyze\nmodel performance between different subpopulations.",
          "link": "http://arxiv.org/abs/2107.04714",
          "publishedOn": "2021-07-13T01:59:33.558Z",
          "wordCount": 598,
          "title": "A Topological-Framework to Improve Analysis of Machine Learning Model Performance. (arXiv:2107.04714v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1\">Ali Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1\">Madhusudhanan Balasubramanian</a>",
          "description": "Dense pixel matching problems such as optical flow and disparity estimation\nare among the most challenging tasks in computer vision. Recently, several deep\nlearning methods designed for these problems have been successful. A\nsufficiently larger effective receptive field (ERF) and a higher resolution of\nspatial features within a network are essential for providing higher-resolution\ndense estimates. In this work, we present a systemic approach to design network\narchitectures that can provide a larger receptive field while maintaining a\nhigher spatial feature resolution. To achieve a larger ERF, we utilized dilated\nconvolutional layers. By aggressively increasing dilation rates in the deeper\nlayers, we were able to achieve a sufficiently larger ERF with a significantly\nfewer number of trainable parameters. We used optical flow estimation problem\nas the primary benchmark to illustrate our network design strategy. The\nbenchmark results (Sintel, KITTI, and Middlebury) indicate that our compact\nnetworks can achieve comparable performance in the class of lightweight\nnetworks.",
          "link": "http://arxiv.org/abs/2107.04715",
          "publishedOn": "2021-07-13T01:59:33.551Z",
          "wordCount": 597,
          "title": "DDCNet: Deep Dilated Convolutional Neural Network for Dense Prediction. (arXiv:2107.04715v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baoru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuch_D/0/1/0/all/0/1\">David Tuch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_K/0/1/0/all/0/1\">Kunal Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1\">Stamatia Giannarou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elson_D/0/1/0/all/0/1\">Daniel S. Elson</a>",
          "description": "Dense depth estimation and 3D reconstruction of a surgical scene are crucial\nsteps in computer assisted surgery. Recent work has shown that depth estimation\nfrom a stereo images pair could be solved with convolutional neural networks.\nHowever, most recent depth estimation models were trained on datasets with\nper-pixel ground truth. Such data is especially rare for laparoscopic imaging,\nmaking it hard to apply supervised depth estimation to real surgical\napplications. To overcome this limitation, we propose SADepth, a new\nself-supervised depth estimation method based on Generative Adversarial\nNetworks. It consists of an encoder-decoder generator and a discriminator to\nincorporate geometry constraints during training. Multi-scale outputs from the\ngenerator help to solve the local minima caused by the photometric reprojection\nloss, while the adversarial learning improves the framework generation quality.\nExtensive experiments on two public datasets show that SADepth outperforms\nrecent state-of-the-art unsupervised methods by a large margin, and reduces the\ngap between supervised and unsupervised depth estimation in laparoscopic\nimages.",
          "link": "http://arxiv.org/abs/2107.04644",
          "publishedOn": "2021-07-13T01:59:33.541Z",
          "wordCount": 607,
          "title": "Self-Supervised Generative Adversarial Network for Depth Estimation in Laparoscopic Images. (arXiv:2107.04644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04721",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1\">Shuyun Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_Z/0/1/0/all/0/1\">Ziming Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granley_J/0/1/0/all/0/1\">Jacob Granley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beyeler_M/0/1/0/all/0/1\">Michael Beyeler</a>",
          "description": "Fundus photography has routinely been used to document the presence and\nseverity of retinal degenerative diseases such as age-related macular\ndegeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical\npractice, for which the fovea and optic disc (OD) are important retinal\nlandmarks. However, the occurrence of lesions, drusen, and other retinal\nabnormalities during retinal degeneration severely complicates automatic\nlandmark detection and segmentation. Here we propose HBA-U-Net: a U-Net\nbackbone enriched with hierarchical bottleneck attention. The network consists\nof a novel bottleneck attention block that combines and refines self-attention,\nchannel attention, and relative-position attention to highlight retinal\nabnormalities that may be important for fovea and OD segmentation in the\ndegenerated retina. HBA-U-Net achieved state-of-the-art results on fovea\ndetection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of\n25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for\nAMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:\nED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for\nlandmark detection in the presence of a variety of retinal degenerative\ndiseases.",
          "link": "http://arxiv.org/abs/2107.04721",
          "publishedOn": "2021-07-13T01:59:33.534Z",
          "wordCount": 641,
          "title": "U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina. (arXiv:2107.04721v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_G/0/1/0/all/0/1\">Gaurav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>",
          "description": "Generating future frames given a few context (or past) frames is a\nchallenging task. It requires modeling the temporal coherence of videos and\nmulti-modality in terms of diversity in the potential future states. Current\nvariational approaches for video generation tend to marginalize over\nmulti-modal future outcomes. Instead, we propose to explicitly model the\nmulti-modality in the future outcomes and leverage it to sample diverse\nfutures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to\nlearn priors on future states given the past and maintains a probability\ndistribution over possible futures given a particular sample. In addition, we\nleverage the changes in this distribution over time to control the sampling of\ndiverse future states by estimating the end of ongoing sequences. That is, we\nuse the variance of GP over the output function space to trigger a change in an\naction sequence. We achieve state-of-the-art results on diverse future frame\ngeneration in terms of reconstruction quality and diversity of the generated\nsequences.",
          "link": "http://arxiv.org/abs/2107.04619",
          "publishedOn": "2021-07-13T01:59:33.493Z",
          "wordCount": 612,
          "title": "Diverse Video Generation using a Gaussian Process Trigger. (arXiv:2107.04619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasiri_S/0/1/0/all/0/1\">Seyed-Mahdi Nasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>",
          "description": "Triangulation refers to the problem of finding a 3D point from its 2D\nprojections on multiple camera images. For solving this problem, it is the\ncommon practice to use so-called optimal triangulation method, which we call\nthe L2 method in this paper. But, the method can be optimal only if we assume\nno uncertainty in the camera parameters. Through extensive comparison on\nsynthetic and real data, we observed that the L2 method is actually not the\nbest choice when there is uncertainty in the camera parameters. Interestingly,\nit can be observed that the simple mid-point method outperforms other methods.\nApart from its high performance, the mid-point method has a simple closed\nformed solution for multiple camera images while the L2 method is hard to be\nused for more than two camera images. Therefore, in contrast to the common\npractice, we argue that the simple mid-point method should be used in\nstructure-from-motion applications where there is uncertainty in camera\nparameters.",
          "link": "http://arxiv.org/abs/2107.04618",
          "publishedOn": "2021-07-13T01:59:33.475Z",
          "wordCount": 595,
          "title": "Optimal Triangulation Method is Not Really Optimal. (arXiv:2107.04618v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Siming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenpei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vouga_E/0/1/0/all/0/1\">Etienne Vouga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>",
          "description": "This paper introduces HPNet, a novel deep-learning approach for segmenting a\n3D shape represented as a point cloud into primitive patches. The key to deep\nprimitive segmentation is learning a feature representation that can separate\npoints of different primitives. Unlike utilizing a single feature\nrepresentation, HPNet leverages hybrid representations that combine one learned\nsemantic descriptor, two spectral descriptors derived from predicted geometric\nparameters, as well as an adjacency matrix that encodes sharp edges. Moreover,\ninstead of merely concatenating the descriptors, HPNet optimally combines\nhybrid representations by learning combination weights. This weighting module\nbuilds on the entropy of input features. The output primitive segmentation is\nobtained from a mean-shift clustering module. Experimental results on benchmark\ndatasets ANSI and ABCParts show that HPNet leads to significant performance\ngains from baseline approaches.",
          "link": "http://arxiv.org/abs/2105.10620",
          "publishedOn": "2021-07-12T01:55:17.072Z",
          "wordCount": 612,
          "title": "HPNet: Deep Primitive Segmentation Using Hybrid Representations. (arXiv:2105.10620v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_H/0/1/0/all/0/1\">Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chensu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigel_C/0/1/0/all/0/1\">Carlie S. Sigel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doukas_M/0/1/0/all/0/1\">Michael Doukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alpert_L/0/1/0/all/0/1\">Lindsay Alpert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarnagin_W/0/1/0/all/0/1\">William R. Jarnagin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_A/0/1/0/all/0/1\">Amber Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_T/0/1/0/all/0/1\">Thomas J. Fuchs</a>",
          "description": "Histopathology-based survival modelling has two major hurdles. Firstly, a\nwell-performing survival model has minimal clinical application if it does not\ncontribute to the stratification of a cancer patient cohort into different risk\ngroups, preferably driven by histologic morphologies. In the clinical setting,\nindividuals are not given specific prognostic predictions, but are rather\npredicted to lie within a risk group which has a general survival trend. Thus,\nIt is imperative that a survival model produces well-stratified risk groups.\nSecondly, until now, survival modelling was done in a two-stage approach\n(encoding and aggregation). The massive amount of pixels in digitized whole\nslide images were never utilized to their fullest extent due to technological\nconstraints on data processing, forcing decoupled learning. EPIC-Survival\nbridges encoding and aggregation into an end-to-end survival modelling\napproach, while introducing stratification boosting to encourage the model to\nnot only optimize ranking, but also to discriminate between risk groups. In\nthis study we show that EPIC-Survival performs better than other approaches in\nmodelling intrahepatic cholangiocarcinoma, a historically difficult cancer to\nmodel. Further, we show that stratification boosting improves further improves\nmodel performance, resulting in a concordance-index of 0.880 on a held-out test\nset. Finally, we were able to identify specific histologic differences, not\ncommonly sought out in ICC, between low and high risk groups.",
          "link": "http://arxiv.org/abs/2101.11085",
          "publishedOn": "2021-07-12T01:55:17.065Z",
          "wordCount": 716,
          "title": "EPIC-Survival: End-to-end Part Inferred Clustering for Survival Analysis, Featuring Prognostic Stratification Boosting. (arXiv:2101.11085v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Graphically-rich applications such as games are ubiquitous with attractive\nvisual effects of Graphical User Interface (GUI) that offers a bridge between\nsoftware applications and end-users. However, various types of graphical\nglitches may arise from such GUI complexity and have become one of the main\ncomponent of software compatibility issues. Our study on bug reports from game\ndevelopment teams in NetEase Inc. indicates that graphical glitches frequently\noccur during the GUI rendering and severely degrade the quality of\ngraphically-rich applications such as video games. Existing automated testing\ntechniques for such applications focus mainly on generating various GUI test\nsequences and check whether the test sequences can cause crashes. These\ntechniques require constant human attention to captures non-crashing bugs such\nas bugs causing graphical glitches. In this paper, we present the first step in\nautomating the test oracle for detecting non-crashing bugs in graphically-rich\napplications. Specifically, we propose \\texttt{GLIB} based on a code-based data\naugmentation technique to detect game GUI glitches. We perform an evaluation of\n\\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the\nresult shows that \\texttt{GLIB} can achieve 100\\% precision and 99.5\\% recall\nin detecting non-crashing bugs such as game GUI glitches. Practical application\nof \\texttt{GLIB} on another 14 real-world games (without bug reports) further\ndemonstrates that \\texttt{GLIB} can effectively uncover GUI glitches, with 48\nof 53 bugs reported by \\texttt{GLIB} having been confirmed and fixed so far.",
          "link": "http://arxiv.org/abs/2106.10507",
          "publishedOn": "2021-07-12T01:55:17.040Z",
          "wordCount": 718,
          "title": "GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Wonjong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_G/0/1/0/all/0/1\">Gwangjin Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yucheol Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>",
          "description": "We present a caricature generation framework based on shape and style\nmanipulation using StyleGAN. Our framework, dubbed StyleCariGAN, automatically\ncreates a realistic and detailed caricature from an input photo with optional\ncontrols on shape exaggeration degree and color stylization type. The key\ncomponent of our method is shape exaggeration blocks that are used for\nmodulating coarse layer feature maps of StyleGAN to produce desirable\ncaricature shape exaggerations. We first build a layer-mixed StyleGAN for\nphoto-to-caricature style conversion by swapping fine layers of the StyleGAN\nfor photos to the corresponding layers of the StyleGAN trained to generate\ncaricatures. Given an input photo, the layer-mixed model produces detailed\ncolor stylization for a caricature but without shape exaggerations. We then\nappend shape exaggeration blocks to the coarse layers of the layer-mixed model\nand train the blocks to create shape exaggerations while preserving the\ncharacteristic appearances of the input. Experimental results show that our\nStyleCariGAN generates realistic and detailed caricatures compared to the\ncurrent state-of-the-art methods. We demonstrate StyleCariGAN also supports\nother StyleGAN-based image manipulations, such as facial expression control.",
          "link": "http://arxiv.org/abs/2107.04331",
          "publishedOn": "2021-07-12T01:55:16.992Z",
          "wordCount": 651,
          "title": "StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation. (arXiv:2107.04331v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chui_J/0/1/0/all/0/1\">Jason Chui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klenk_S/0/1/0/all/0/1\">Simon Klenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>",
          "description": "We propose a novel method for continuous-time feature tracking in event\ncameras. To this end, we track features by aligning events along an estimated\ntrajectory in space-time such that the projection on the image plane results in\nmaximally sharp event patch images. The trajectory is parameterized by $n^{th}$\norder B-splines, which are continuous up to $(n-2)^{th}$ derivative. In\ncontrast to previous work, we optimize the curve parameters in a sliding window\nfashion. On a public dataset we experimentally confirm that the proposed\nsliding-window B-spline optimization leads to longer and more accurate feature\ntracks than in previous work.",
          "link": "http://arxiv.org/abs/2107.04536",
          "publishedOn": "2021-07-12T01:55:16.966Z",
          "wordCount": 542,
          "title": "Event-Based Feature Tracking in Continuous Time with Sliding Window Optimization. (arXiv:2107.04536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1\">Pak-Hei Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1\">Ana I.L. Namburete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>",
          "description": "The objective of this work is to segment any arbitrary structures of interest\n(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D\nsegmentation). We show that high accuracy can be achieved by simply propagating\nthe 2D slice segmentation with an affinity matrix between consecutive slices,\nwhich can be learnt in a self-supervised manner, namely slice reconstruction.\nSpecifically, we compare the proposed framework, termed as Sli2Vol, with\nsupervised approaches and two other unsupervised/ self-supervised slice\nregistration approaches, on 8 public datasets (both CT and MRI scans), spanning\n9 different SOIs. Without any parameter-tuning, the same model achieves\nsuperior performance with Dice scores (0-100 scale) of over 80 for most of the\nbenchmarks, including the ones that are unseen during training. Our results\nshow generalizability of the proposed approach across data from different\nmachines and with different SOIs: a major use case of semi-automatic\nsegmentation methods where fully supervised approaches would normally struggle.\nThe source code will be made publicly available at\nhttps://github.com/pakheiyeung/Sli2Vol.",
          "link": "http://arxiv.org/abs/2105.12722",
          "publishedOn": "2021-07-12T01:55:16.942Z",
          "wordCount": 653,
          "title": "Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satnam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schicker_D/0/1/0/all/0/1\">Doris Schicker</a>",
          "description": "We propose to use a ResNet-18 architecture that was pre-trained on the FER+\ndataset for tackling the problem of affective behavior analysis in-the-wild\n(ABAW) for classification of the seven basic expressions, namely, neutral,\nanger, disgust, fear, happiness, sadness and surprise. As part of the second\nworkshop and competition on affective behavior analysis in-the-wild (ABAW2), a\ndatabase consisting of 564 videos with around 2.8M frames is provided along\nwith labels for these seven basic expressions. We resampled the dataset to\ncounter class-imbalances by under-sampling the over-represented classes and\nover-sampling the under-represented classes along with class-wise weights. To\navoid overfitting we performed data-augmentation and used L2 regularisation.\nOur classifier reaches an ABAW2 score of 0.4 and therefore exceeds the baseline\nresults provided by the hosts of the competition.",
          "link": "http://arxiv.org/abs/2107.04569",
          "publishedOn": "2021-07-12T01:55:16.935Z",
          "wordCount": 554,
          "title": "Seven Basic Expression Recognition Using ResNet-18. (arXiv:2107.04569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Eng-Jon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husain_S/0/1/0/all/0/1\">Sameed Husain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1\">Miroslaw Bober</a>",
          "description": "The process of aggregation is ubiquitous in almost all deep nets models. It\nfunctions as an important mechanism for consolidating deep features into a more\ncompact representation, whilst increasing robustness to overfitting and\nproviding spatial invariance in deep nets. In particular, the proximity of\nglobal aggregation layers to the output layers of DNNs mean that aggregated\nfeatures have a direct influence on the performance of a deep net. A better\nunderstanding of this relationship can be obtained using information theoretic\nmethods. However, this requires the knowledge of the distributions of the\nactivations of aggregation layers. To achieve this, we propose a novel\nmathematical formulation for analytically modelling the probability\ndistributions of output values of layers involved with deep feature\naggregation. An important outcome is our ability to analytically predict the\nKL-divergence of output nodes in a DNN. We also experimentally verify our\ntheoretical predictions against empirical observations across a range of\ndifferent classification tasks and datasets.",
          "link": "http://arxiv.org/abs/2107.04458",
          "publishedOn": "2021-07-12T01:55:16.929Z",
          "wordCount": 594,
          "title": "Understanding the Distributions of Aggregation Layers in Deep Neural Networks. (arXiv:2107.04458v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_Y/0/1/0/all/0/1\">Yennie Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauba_P/0/1/0/all/0/1\">Paulius Rauba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachtel_G/0/1/0/all/0/1\">Gal Wachtel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruining Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xingjian Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broestl_N/0/1/0/all/0/1\">Noah Broestl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doff_Sotta_M/0/1/0/all/0/1\">Martin Doff-Sotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>",
          "description": "Hateful memes pose a unique challenge for current machine learning systems\nbecause their message is derived from both text- and visual-modalities. To this\neffect, Facebook released the Hateful Memes Challenge, a dataset of memes with\npre-extracted text captions, but it is unclear whether these synthetic examples\ngeneralize to `memes in the wild'. In this paper, we collect hateful and\nnon-hateful memes from Pinterest to evaluate out-of-sample performance on\nmodels pre-trained on the Facebook dataset. We find that memes in the wild\ndiffer in two key aspects: 1) Captions must be extracted via OCR, injecting\nnoise and diminishing performance of multimodal models, and 2) Memes are more\ndiverse than `traditional memes', including screenshots of conversations or\ntext on a plain background. This paper thus serves as a reality check for the\ncurrent benchmark of hateful meme detection and its applicability for detecting\nreal world hate.",
          "link": "http://arxiv.org/abs/2107.04313",
          "publishedOn": "2021-07-12T01:55:16.922Z",
          "wordCount": 611,
          "title": "Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset. (arXiv:2107.04313v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04537",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With the advent of advancements in deep learning approaches, such as deep\nconvolution neural network, residual neural network, adversarial network; U-Net\narchitectures are most widely utilized in biomedical image segmentation to\naddress the automation in identification and detection of the target regions or\nsub-regions. In recent studies, U-Net based approaches have illustrated\nstate-of-the-art performance in different applications for the development of\ncomputer-aided diagnosis systems for early diagnosis and treatment of diseases\nsuch as brain tumor, lung cancer, alzheimer, breast cancer, etc. This article\ncontributes to present the success of these approaches by describing the U-Net\nframework, followed by the comprehensive analysis of the U-Net variants for\ndifferent medical imaging or modalities such as magnetic resonance imaging,\nX-ray, computerized tomography/computerized axial tomography, ultrasound,\npositron emission tomography, etc. Besides, this article also highlights the\ncontribution of U-Net based frameworks in the on-going pandemic, severe acute\nrespiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.",
          "link": "http://arxiv.org/abs/2107.04537",
          "publishedOn": "2021-07-12T01:55:16.869Z",
          "wordCount": 643,
          "title": "Modality specific U-Net variants for biomedical image segmentation: A survey. (arXiv:2107.04537v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1\">Francisco Eiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>",
          "description": "Randomized smoothing has recently emerged as an effective tool that enables\ncertification of deep neural network classifiers at scale. All prior art on\nrandomized smoothing has focused on isotropic $\\ell_p$ certification, which has\nthe advantage of yielding certificates that can be easily compared among\nisotropic methods via $\\ell_p$-norm radius. However, isotropic certification\nlimits the region that can be certified around an input to worst-case\nadversaries, \\ie it cannot reason about other \"close\", potentially large,\nconstant prediction safe regions. To alleviate this issue, (i) we theoretically\nextend the isotropic randomized smoothing $\\ell_1$ and $\\ell_2$ certificates to\ntheir generalized anisotropic counterparts following a simplified analysis.\nMoreover, (ii) we propose evaluation metrics allowing for the comparison of\ngeneral certificates - a certificate is superior to another if it certifies a\nsuperset region - with the quantification of each certificate through the\nvolume of the certified region. We introduce ANCER, a practical framework for\nobtaining anisotropic certificates for a given test set sample via volume\nmaximization. Our empirical results demonstrate that ANCER achieves\nstate-of-the-art $\\ell_1$ and $\\ell_2$ certified accuracy on both CIFAR-10 and\nImageNet at multiple radii, while certifying substantially larger regions in\nterms of volume, thus highlighting the benefits of moving away from isotropic\nanalysis. Code used in our experiments is available in\nhttps://github.com/MotasemAlfarra/ANCER.",
          "link": "http://arxiv.org/abs/2107.04570",
          "publishedOn": "2021-07-12T01:55:16.713Z",
          "wordCount": 670,
          "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuy C. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tuan N. Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1\">Nam LH. Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chuong H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_M/0/1/0/all/0/1\">Masayuki Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamanaka_M/0/1/0/all/0/1\">Masao Yamanaka</a>",
          "description": "Video Instance Segmentation (VIS) is a multi-task problem performing\ndetection, segmentation, and tracking simultaneously. Extended from image set\napplications, video data additionally induces the temporal information, which,\nif handled appropriately, is very useful to identify and predict object\nmotions. In this work, we design a unified model to mutually learn these tasks.\nSpecifically, we propose two modules, named Temporally Correlated Instance\nSegmentation (TCIS) and Bidirectional Tracking (BiTrack), to take the benefit\nof the temporal correlation between the object's instance masks across adjacent\nframes. On the other hand, video data is often redundant due to the frame's\noverlap. Our analysis shows that this problem is particularly severe for the\nYoutubeVOS-VIS2021 data. Therefore, we propose a Multi-Source Data (MSD)\ntraining mechanism to compensate for the data deficiency. By combining these\ntechniques with a bag of tricks, the network performance is significantly\nboosted compared to the baseline, and outperforms other methods by a\nconsiderable margin on the YoutubeVOS-VIS 2019 and 2021 datasets.",
          "link": "http://arxiv.org/abs/2106.06649",
          "publishedOn": "2021-07-12T01:55:16.668Z",
          "wordCount": 637,
          "title": "1st Place Solution for YouTubeVOS Challenge 2021:Video Instance Segmentation. (arXiv:2106.06649v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Asmit Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehan_P/0/1/0/all/0/1\">Paras Mehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Divyanshu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1\">Rohan Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>",
          "description": "Wearing masks is a useful protection method against COVID-19, which has\ncaused widespread economic and social impact worldwide. Across the globe,\ngovernments have put mandates for the use of face masks, which have received\nboth positive and negative reaction. Online social media provides an exciting\nplatform to study the use of masks and analyze underlying mask-wearing\npatterns. In this article, we analyze 2.04 million social media images for six\nUS cities. An increase in masks worn in images is seen as the COVID-19 cases\nrose, particularly when their respective states imposed strict regulations. We\nalso found a decrease in the posting of group pictures as stay-at-home laws\nwere put into place. Furthermore, mask compliance in the Black Lives Matter\nprotest was analyzed, eliciting that 40% of the people in group photos wore\nmasks, and 45% of them wore the masks with a fit score of greater than 80%. We\nintroduce two new datasets, VAriety MAsks - Classification (VAMA-C) and VAriety\nMAsks - Segmentation (VAMA-S), for mask detection and mask fit analysis tasks,\nrespectively. For the analysis, we create two frameworks, face mask detector\n(for classifying masked and unmasked faces) and mask fit analyzer (a semantic\nsegmentation based model to calculate a mask-fit score). The face mask detector\nachieved a classification accuracy of 98%, and the semantic segmentation model\nfor the mask fit analyzer achieved an Intersection Over Union (IOU) score of\n98%. We conclude that such a framework can be used to evaluate the\neffectiveness of such public health strategies using social media platforms in\ntimes of pandemic.",
          "link": "http://arxiv.org/abs/2011.00052",
          "publishedOn": "2021-07-12T01:55:16.587Z",
          "wordCount": 782,
          "title": "(Un)Masked COVID-19 Trends from Social Media. (arXiv:2011.00052v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnewar_A/0/1/0/all/0/1\">Animesh Karnewar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1\">Lam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>",
          "description": "We propose a new cascaded architecture for novel view synthesis, called\nRGBD-Net, which consists of two core components: a hierarchical depth\nregression network and a depth-aware generator network. The former one predicts\ndepth maps of the target views by using adaptive depth scaling, while the\nlatter one leverages the predicted depths and renders spatially and temporally\nconsistent target images. In the experimental evaluation on standard datasets,\nRGBD-Net not only outperforms the state-of-the-art by a clear margin, but it\nalso generalizes well to new scenes without per-scene optimization. Moreover,\nwe show that RGBD-Net can be optionally trained without depth supervision while\nstill retaining high-quality rendering. Thanks to the depth regression network,\nRGBD-Net can be also used for creating dense 3D point clouds that are more\naccurate than those produced by some state-of-the-art multi-view stereo\nmethods.",
          "link": "http://arxiv.org/abs/2011.14398",
          "publishedOn": "2021-07-12T01:55:16.568Z",
          "wordCount": 625,
          "title": "RGBD-Net: Predicting color and depth images for novel views synthesis. (arXiv:2011.14398v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xitong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaodong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>",
          "description": "One central question for video action recognition is how to model motion. In\nthis paper, we present hierarchical contrastive motion learning, a new\nself-supervised learning framework to extract effective motion representations\nfrom raw video frames. Our approach progressively learns a hierarchy of motion\nfeatures that correspond to different abstraction levels in a network. This\nhierarchical design bridges the semantic gap between low-level motion cues and\nhigh-level recognition tasks, and promotes the fusion of appearance and motion\ninformation at multiple levels. At each level, an explicit motion\nself-supervision is provided via contrastive learning to enforce the motion\nfeatures at the current level to predict the future ones at the previous level.\nThus, the motion features at higher levels are trained to gradually capture\nsemantic dynamics and evolve more discriminative for action recognition. Our\nmotion learning module is lightweight and flexible to be embedded into various\nbackbone networks. Extensive experiments on four benchmarks show that the\nproposed approach consistently achieves superior results.",
          "link": "http://arxiv.org/abs/2007.10321",
          "publishedOn": "2021-07-12T01:55:16.561Z",
          "wordCount": 642,
          "title": "Hierarchical Contrastive Motion Learning for Video Action Recognition. (arXiv:2007.10321v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04288",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dewei Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malone_J/0/1/0/all/0/1\">Joseph D. Malone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atay_Y/0/1/0/all/0/1\">Yigit Atay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Y/0/1/0/all/0/1\">Yuankai K. Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1\">Ipek Oguz</a>",
          "description": "Optical coherence tomography (OCT) is a prevalent imaging technique for\nretina. However, it is affected by multiplicative speckle noise that can\ndegrade the visibility of essential anatomical structures, including blood\nvessels and tissue layers. Although averaging repeated B-scan frames can\nsignificantly improve the signal-to-noise-ratio (SNR), this requires longer\nacquisition time, which can introduce motion artifacts and cause discomfort to\npatients. In this study, we propose a learning-based method that exploits\ninformation from the single-frame noisy B-scan and a pseudo-modality that is\ncreated with the aid of the self-fusion method. The pseudo-modality provides\ngood SNR for layers that are barely perceptible in the noisy B-scan but can\nover-smooth fine features such as small vessels. By using a fusion network,\ndesired features from each modality can be combined, and the weight of their\ncontribution is adjustable. Evaluated by intensity-based and structural\nmetrics, the result shows that our method can effectively suppress the speckle\nnoise and enhance the contrast between retina layers while the overall\nstructure and small blood vessels are preserved. Compared to the single\nmodality network, our method improves the structural similarity with low noise\nB-scan from 0.559 +\\- 0.033 to 0.576 +\\- 0.031.",
          "link": "http://arxiv.org/abs/2107.04288",
          "publishedOn": "2021-07-12T01:55:16.553Z",
          "wordCount": 651,
          "title": "Retinal OCT Denoising with Pseudo-Multimodal Fusion Network. (arXiv:2107.04288v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04440",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>",
          "description": "Image registration aims to establish spatial correspondence across pairs, or\ngroups of images, and is a cornerstone of medical image computing and\ncomputer-assisted-interventions. Currently, most deep learning-based\nregistration methods assume that the desired deformation fields are globally\nsmooth and continuous, which is not always valid for real-world scenarios,\nespecially in medical image registration (e.g. cardiac imaging and abdominal\nimaging). Such a global constraint can lead to artefacts and increased errors\nat discontinuous tissue interfaces. To tackle this issue, we propose a\nweakly-supervised Deep Discontinuity-preserving Image Registration network\n(DDIR), to obtain better registration performance and realistic deformation\nfields. We demonstrate that our method achieves significant improvements in\nregistration accuracy and predicts more realistic deformations, in registration\nexperiments on cardiac magnetic resonance (MR) images from UK Biobank Imaging\nStudy (UKBB), than state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.04440",
          "publishedOn": "2021-07-12T01:55:16.522Z",
          "wordCount": 579,
          "title": "A Deep Discontinuity-Preserving Image Registration Network. (arXiv:2107.04440v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daza_L/0/1/0/all/0/1\">Laura Daza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan C. P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>",
          "description": "The reliability of Deep Learning systems depends on their accuracy but also\non their robustness against adversarial perturbations to the input data.\nSeveral attacks and defenses have been proposed to improve the performance of\nDeep Neural Networks under the presence of adversarial noise in the natural\nimage domain. However, robustness in computer-aided diagnosis for volumetric\ndata has only been explored for specific tasks and with limited attacks. We\npropose a new framework to assess the robustness of general medical image\nsegmentation systems. Our contributions are two-fold: (i) we propose a new\nbenchmark to evaluate robustness in the context of the Medical Segmentation\nDecathlon (MSD) by extending the recent AutoAttack natural image classification\nframework to the domain of volumetric data segmentation, and (ii) we present a\nnovel lattice architecture for RObust Generic medical image segmentation (ROG).\nOur results show that ROG is capable of generalizing across different tasks of\nthe MSD and largely surpasses the state-of-the-art under sophisticated\nadversarial attacks.",
          "link": "http://arxiv.org/abs/2107.04263",
          "publishedOn": "2021-07-12T01:55:16.489Z",
          "wordCount": 596,
          "title": "Towards Robust General Medical Image Segmentation. (arXiv:2107.04263v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1\">Mohammad Javad Shafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_E/0/1/0/all/0/1\">Ellick Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "The fine-grained relationship between form and function with respect to deep\nneural network architecture design and hardware-specific acceleration is one\narea that is not well studied in the research literature, with form often\ndictated by accuracy as opposed to hardware function. In this study, a\ncomprehensive empirical exploration is conducted to investigate the impact of\ndeep neural network architecture design on the degree of inference speedup that\ncan be achieved via hardware-specific acceleration. More specifically, we\nempirically study the impact of a variety of commonly used macro-architecture\ndesign patterns across different architectural depths through the lens of\nOpenVINO microprocessor-specific and GPU-specific acceleration. Experimental\nresults showed that while leveraging hardware-specific acceleration achieved an\naverage inference speed-up of 380%, the degree of inference speed-up varied\ndrastically depending on the macro-architecture design pattern, with the\ngreatest speedup achieved on the depthwise bottleneck convolution design\npattern at 550%. Furthermore, we conduct an in-depth exploration of the\ncorrelation between FLOPs requirement, level 3 cache efficacy, and network\nlatency with increasing architectural depth and width. Finally, we analyze the\ninference time reductions using hardware-specific acceleration when compared to\nnative deep learning frameworks across a wide variety of hand-crafted deep\nconvolutional neural network architecture designs as well as ones found via\nneural architecture search strategies. We found that the DARTS-derived\narchitecture to benefit from the greatest improvement from hardware-specific\nsoftware acceleration (1200%) while the depthwise bottleneck convolution-based\nMobileNet-V2 to have the lowest overall inference time of around 2.4 ms.",
          "link": "http://arxiv.org/abs/2107.04144",
          "publishedOn": "2021-07-12T01:55:16.449Z",
          "wordCount": 709,
          "title": "Does Form Follow Function? An Empirical Exploration of the Impact of Deep Neural Network Architecture Design on Hardware-Specific Acceleration. (arXiv:2107.04144v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04062",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zettler_N/0/1/0/all/0/1\">Nico Zettler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mastmeyer_A/0/1/0/all/0/1\">Andre Mastmeyer</a>",
          "description": "A two-step concept for 3D segmentation on 5 abdominal organs inside\nvolumetric CT images is presented. First each relevant organ's volume of\ninterest is extracted as bounding box. The extracted volume acts as input for a\nsecond stage, wherein two compared U-Nets with different architectural\ndimensions re-construct an organ segmentation as label mask. In this work, we\nfocus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results\nindicate Dice improvements of about 6\\% at maximum. In this study to our\nsurprise, liver and kidneys for instance were tackled significantly better\nusing the faster and GPU-memory saving 2D U-Nets. For other abdominal key\norgans, there were no significant differences, but we observe highly\nsignificant advantages for the 2D U-Net in terms of GPU computational efforts\nfor all organs under study.",
          "link": "http://arxiv.org/abs/2107.04062",
          "publishedOn": "2021-07-12T01:55:16.420Z",
          "wordCount": 610,
          "title": "Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT images. (arXiv:2107.04062v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04055",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1\">Haibo Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>",
          "description": "In this paper, a 3D-RegNet-based neural network is proposed for diagnosing\nthe physical condition of patients with coronavirus (Covid-19) infection. In\nthe application of clinical medicine, lung CT images are utilized by\npractitioners to determine whether a patient is infected with coronavirus.\nHowever, there are some laybacks can be considered regarding to this diagnostic\nmethod, such as time consuming and low accuracy. As a relatively large organ of\nhuman body, important spatial features would be lost if the lungs were\ndiagnosed utilizing two dimensional slice image. Therefore, in this paper, a\ndeep learning model with 3D image was designed. The 3D image as input data was\ncomprised of two-dimensional pulmonary image sequence and from which relevant\ncoronavirus infection 3D features were extracted and classified. The results\nshow that the test set of the 3D model, the result: f1 score of 0.8379 and AUC\nvalue of 0.8807 have been achieved.",
          "link": "http://arxiv.org/abs/2107.04055",
          "publishedOn": "2021-07-12T01:55:16.363Z",
          "wordCount": 649,
          "title": "3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image. (arXiv:2107.04055v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weichuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuefang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhe Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changming Sun</a>",
          "description": "Metric-based few-shot fine-grained image classification (FSFGIC) aims to\nlearn a transferable feature embedding network by estimating the similarities\nbetween query images and support classes from very few examples. In this work,\nwe propose, for the first time, to introduce the non-linear data projection\nconcept into the design of FSFGIC architecture in order to address the limited\nsample problem in few-shot learning and at the same time to increase the\ndiscriminability of the model for fine-grained image classification.\nSpecifically, we first design a feature re-abstraction embedding network that\nhas the ability to not only obtain the required semantic features for effective\nmetric learning but also re-enhance such features with finer details from input\nimages. Then the descriptors of the query images and the support classes are\nprojected into different non-linear spaces in our proposed similarity metric\nlearning network to learn discriminative projection factors. This design can\neffectively operate in the challenging and restricted condition of a FSFGIC\ntask for making the distance between the samples within the same class smaller\nand the distance between samples from different classes larger and for reducing\nthe coupling relationship between samples from different categories.\nFurthermore, a novel similarity measure based on the proposed non-linear data\nproject is presented for evaluating the relationships of feature information\nbetween a query image and a support set. It is worth to note that our proposed\narchitecture can be easily embedded into any episodic training mechanisms for\nend-to-end training from scratch. Extensive experiments on FSFGIC tasks\ndemonstrate the superiority of the proposed methods over the state-of-the-art\nbenchmarks.",
          "link": "http://arxiv.org/abs/2106.06988",
          "publishedOn": "2021-07-12T01:55:15.679Z",
          "wordCount": 743,
          "title": "NDPNet: A novel non-linear data projection network for few-shot fine-grained image classification. (arXiv:2106.06988v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1\">Adil Kaan Akan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vural_F/0/1/0/all/0/1\">Fatos T. Yarman Vural</a>",
          "description": "In this study, we introduce a measure for machine perception, inspired by the\nconcept of Just Noticeable Difference (JND) of human perception. Based on this\nmeasure, we suggest an adversarial image generation algorithm, which\niteratively distorts an image by an additive noise until the machine learning\nmodel detects the change in the image by outputting a false label. The amount\nof noise added to the original image is defined as the gradient of the cost\nfunction of the machine learning model. This cost function explicitly minimizes\nthe amount of perturbation applied on the input image and it is regularized by\nbounded range and total variation functions to assure perceptual similarity of\nthe adversarial image to the input. We evaluate the adversarial images\ngenerated by our algorithm both qualitatively and quantitatively on CIFAR10,\nImageNet, and MS COCO datasets. Our experiments on image classification and\nobject detection tasks show that adversarial images generated by our method are\nboth more successful in deceiving the recognition/detection model and less\nperturbed compared to the images generated by the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.08079",
          "publishedOn": "2021-07-12T01:55:15.661Z",
          "wordCount": 665,
          "title": "Just Noticeable Difference for Machine Perception and Generation of Regularized Adversarial Images with Minimal Perturbation. (arXiv:2102.08079v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>",
          "description": "Visual attention is one of the most significant characteristics for selecting\nand understanding the visual redundancy of the external world. Complex scenes\ninclude enormous redundancy. The human vision system cannot process all\ninformation simultaneously, due to the visual information bottleneck. The human\nvisual system mainly focuses on dominant parts of scenes, in order to reduce\nthe redundant input of visual information. This is commonly known as visual\nattention prediction or visual saliency map prediction. This paper proposes a\nnew psychophysical saliency prediction architecture, WECSF, inspired by\nmulti-channel model of visual cortex functioning in humans. The model consists\nof opponent color channels, a wavelet transform and wavelet energy map, and a\ncontrast sensitivity function for extracting low-level image features and\nproviding maximum approximation to the human visual system. In this paper, the\nproposed model is evaluated using several data sets, including the MIT1003,\nMIT300, TORONTO, SID4VAM, and UCF Sports data sets, in order to demonstrate its\nefficiency. We also quantitatively and qualitatively compare the saliency\nprediction performance with that of other state-of-the-art models. Our model\nachieved stable and very good performance. Additionally, Fourier and\nspectral-inspired saliency prediction models outperformed other\nstate-of-the-art non-neural networks (and even deep neural network) models on\npsychophysical synthetic images. Finally, the proposed model can also be\napplied to spatial-temporal saliency prediction and achieved superior\nperformance in the evaluation.",
          "link": "http://arxiv.org/abs/2011.04076",
          "publishedOn": "2021-07-12T01:55:15.637Z",
          "wordCount": 746,
          "title": "A Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v10 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kwonjoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>",
          "description": "Recently, Vision Transformers (ViTs) have shown competitive performance on\nimage recognition while requiring less vision-specific inductive biases. In\nthis paper, we investigate if such observation can be extended to image\ngeneration. To this end, we integrate the ViT architecture into generative\nadversarial networks (GANs). We observe that existing regularization methods\nfor GANs interact poorly with self-attention, causing serious instability\nduring training. To resolve this issue, we introduce novel regularization\ntechniques for training GANs with ViTs. Empirically, our approach, named\nViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2\non CIFAR-10, CelebA, and LSUN bedroom datasets.",
          "link": "http://arxiv.org/abs/2107.04589",
          "publishedOn": "2021-07-12T01:55:15.629Z",
          "wordCount": 541,
          "title": "ViTGAN: Training GANs with Vision Transformers. (arXiv:2107.04589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hanselmann_N/0/1/0/all/0/1\">Niklas Hanselmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nick Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortelt_B/0/1/0/all/0/1\">Benedikt Ortelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>",
          "description": "In order to handle the challenges of autonomous driving, deep learning has\nproven to be crucial in tackling increasingly complex tasks, such as 3D\ndetection or instance segmentation. State-of-the-art approaches for image-based\ndetection tasks tackle this complexity by operating in a cascaded fashion: they\nfirst extract a 2D bounding box based on which additional attributes, e.g.\ninstance masks, are inferred. While these methods perform well, a key challenge\nremains the lack of accurate and cheap annotations for the growing variety of\ntasks. Synthetic data presents a promising solution but, despite the effort in\ndomain adaptation research, the gap between synthetic and real data remains an\nopen problem. In this work, we propose a weakly supervised domain adaptation\nsetting which exploits the structure of cascaded detection tasks. In\nparticular, we learn to infer the attributes solely from the source domain\nwhile leveraging 2D bounding boxes as weak labels in both domains to explain\nthe domain shift. We further encourage domain-invariant features through\nclass-wise feature alignment using ground-truth class information, which is not\navailable in the unsupervised setting. As our experiments demonstrate, the\napproach is competitive with fully supervised settings while outperforming\nunsupervised adaptation approaches by a large margin.",
          "link": "http://arxiv.org/abs/2107.04523",
          "publishedOn": "2021-07-12T01:55:15.609Z",
          "wordCount": 639,
          "title": "Learning Cascaded Detection Tasks with Weakly-Supervised Domain Adaptation. (arXiv:2107.04523v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fontanel_D/0/1/0/all/0/1\">Dario Fontanel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1\">Fabio Cermelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>",
          "description": "Robotic visual systems operating in the wild must act in unconstrained\nscenarios, under different environmental conditions while facing a variety of\nsemantic concepts, including unknown ones. To this end, recent works tried to\nempower visual object recognition methods with the capability to i) detect\nunseen concepts and ii) extended their knowledge over time, as images of new\nsemantic classes arrive. This setting, called Open World Recognition (OWR), has\nthe goal to produce systems capable of breaking the semantic limits present in\nthe initial training set. However, this training set imposes to the system not\nonly its own semantic limits, but also environmental ones, due to its bias\ntoward certain acquisition conditions that do not necessarily reflect the high\nvariability of the real-world. This discrepancy between training and test\ndistribution is called domain-shift. This work investigates whether OWR\nalgorithms are effective under domain-shift, presenting the first benchmark\nsetup for assessing fairly the performances of OWR algorithms, with and without\ndomain-shift. We then use this benchmark to conduct analyses in various\nscenarios, showing how existing OWR algorithms indeed suffer a severe\nperformance degradation when train and test distributions differ. Our analysis\nshows that this degradation is only slightly mitigated by coupling OWR with\ndomain generalization techniques, indicating that the mere plug-and-play of\nexisting algorithms is not enough to recognize new and unknown categories in\nunseen domains. Our results clearly point toward open issues and future\nresearch directions, that need to be investigated for building robot visual\nsystems able to function reliably under these challenging yet very real\nconditions. Code available at\nhttps://github.com/DarioFontanel/OWR-VisualDomains",
          "link": "http://arxiv.org/abs/2107.04461",
          "publishedOn": "2021-07-12T01:55:15.571Z",
          "wordCount": 710,
          "title": "On the Challenges of Open World Recognitionunder Shifting Visual Domains. (arXiv:2107.04461v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizvi_H/0/1/0/all/0/1\">Hasan Rizvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "In the present study, we propose to implement a new framework for estimating\ngenerative models via an adversarial process to extend an existing GAN\nframework and develop a white-box controllable image cartoonization, which can\ngenerate high-quality cartooned images/videos from real-world photos and\nvideos. The learning purposes of our system are based on three distinct\nrepresentations: surface representation, structure representation, and texture\nrepresentation. The surface representation refers to the smooth surface of the\nimages. The structure representation relates to the sparse colour blocks and\ncompresses generic content. The texture representation shows the texture,\ncurves, and features in cartoon images. Generative Adversarial Network (GAN)\nframework decomposes the images into different representations and learns from\nthem to generate cartoon images. This decomposition makes the framework more\ncontrollable and flexible which allows users to make changes based on the\nrequired output. This approach overcomes any previous system in terms of\nmaintaining clarity, colours, textures, shapes of images yet showing the\ncharacteristics of cartoon images.",
          "link": "http://arxiv.org/abs/2107.04551",
          "publishedOn": "2021-07-12T01:55:15.539Z",
          "wordCount": 609,
          "title": "White-Box Cartoonization Using An Extended GAN Framework. (arXiv:2107.04551v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Long Hoang Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thao Minh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>",
          "description": "Video question answering (Video QA) presents a powerful testbed for\nhuman-like intelligent behaviors. The task demands new capabilities to\nintegrate video processing, language understanding, binding abstract linguistic\nconcepts to concrete visual artifacts, and deliberative reasoning over\nspacetime. Neural networks offer a promising approach to reach this potential\nthrough learning from examples rather than handcrafting features and rules.\nHowever, neural networks are predominantly feature-based - they map data to\nunstructured vectorial representation and thus can fall into the trap of\nexploiting shortcuts through surface statistics instead of true systematic\nreasoning seen in symbolic systems. To tackle this issue, we advocate for\nobject-centric representation as a basis for constructing spatio-temporal\nstructures from videos, essentially bridging the semantic gap between low-level\npattern recognition and high-level symbolic algebra. To this end, we propose a\nnew query-guided representation framework to turn a video into an evolving\nrelational graph of objects, whose features and interactions are dynamically\nand conditionally inferred. The object lives are then summarized into resumes,\nlending naturally for deliberative relational reasoning that produces an answer\nto the query. The framework is evaluated on major Video QA datasets,\ndemonstrating clear benefits of the object-centric approach to video reasoning.",
          "link": "http://arxiv.org/abs/2104.05166",
          "publishedOn": "2021-07-12T01:55:15.524Z",
          "wordCount": 673,
          "title": "Object-Centric Representation Learning for Video Question Answering. (arXiv:2104.05166v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.00862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xin Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>",
          "description": "In person re-identification (Re-ID), supervised methods usually need a large\namount of expensive label information, while unsupervised ones are still unable\nto deliver satisfactory identification performance. In this paper, we introduce\na novel person Re-ID task called unsupervised cross-camera person Re-ID, which\nonly needs the within-camera (intra-camera) label information but not\ncross-camera (inter-camera) labels which are more expensive to obtain. In\nreal-world applications, the intra-camera label information can be easily\ncaptured by tracking algorithms or few manual annotations. In this situation,\nthe main challenge becomes the distribution discrepancy across different camera\nviews, caused by the various body pose, occlusion, image resolution,\nillumination conditions, and background noises in different cameras. To address\nthis situation, we propose a novel Adversarial Camera Alignment Network (ACAN)\nfor unsupervised cross-camera person Re-ID. It consists of the camera-alignment\ntask and the supervised within-camera learning task. To achieve the camera\nalignment, we develop a Multi-Camera Adversarial Learning (MCAL) to map images\nof different cameras into a shared subspace. Particularly, we investigate two\ndifferent schemes, including the existing GRL (i.e., gradient reversal layer)\nscheme and the proposed scheme called \"other camera equiprobability\" (OCE), to\nconduct the multi-camera adversarial task. Based on this shared subspace, we\nthen leverage the within-camera labels to train the network. Extensive\nexperiments on five large-scale datasets demonstrate the superiority of ACAN\nover multiple state-of-the-art unsupervised methods that take advantage of\nlabeled source domains and generated images by GAN-based models. In particular,\nwe verify that the proposed multi-camera adversarial task does contribute to\nthe significant improvement.",
          "link": "http://arxiv.org/abs/1908.00862",
          "publishedOn": "2021-07-12T01:55:15.517Z",
          "wordCount": 736,
          "title": "Adversarial Camera Alignment Network for Unsupervised Cross-camera Person Re-identification. (arXiv:1908.00862v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1\">Tobias Uelwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1\">Felix Michels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candido_O/0/1/0/all/0/1\">Oliver De Candido</a>",
          "description": "Given the increasing threat of adversarial attacks on deep neural networks\n(DNNs), research on efficient detection methods is more important than ever. In\nthis work, we take a closer look at adversarial attack detection based on the\nclass scores of an already trained classification model. We propose to train a\nsupport vector machine (SVM) on the class scores to detect adversarial\nexamples. Our method is able to detect adversarial examples generated by\nvarious attacks, and can be easily adopted to a plethora of deep classification\nmodels. We show that our approach yields an improved detection rate compared to\nan existing method, whilst being easy to implement. We perform an extensive\nempirical analysis on different deep classification models, investigating\nvarious state-of-the-art adversarial attacks. Moreover, we observe that our\nproposed method is better at detecting a combination of adversarial attacks.\nThis work indicates the potential of detecting various adversarial attacks\nsimply by using the class scores of an already trained classification model.",
          "link": "http://arxiv.org/abs/2107.04435",
          "publishedOn": "2021-07-12T01:55:15.509Z",
          "wordCount": 616,
          "title": "Learning to Detect Adversarial Examples Based on Class Scores. (arXiv:2107.04435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.469 for valence and 0.649 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-07-12T01:55:15.486Z",
          "wordCount": 628,
          "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.12056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jensen_H/0/1/0/all/0/1\">Henrik Gr&#xf8;nholt Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauze_F/0/1/0/all/0/1\">Fran&#xe7;ois Lauze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>",
          "description": "We present an information-theoretic approach to the registration of images\nwith directional information, and especially for diffusion-Weighted Images\n(DWI), with explicit optimization over the directional scale. We call it\nLocally Orderless Registration with Directions (LORD). We focus on normalized\nmutual information as a robust information-theoretic similarity measure for\nDWI. The framework is an extension of the LOR-DWI density-based hierarchical\nscale-space model that varies and optimizes the integration, spatial,\ndirectional, and intensity scales. As affine transformations are insufficient\nfor inter-subject registration, we extend the model to non-rigid deformations.\nWe illustrate that the proposed model deforms orientation distribution\nfunctions (ODFs) correctly and is capable of handling the classic complex\nchallenges in DWI-registrations, such as the registration of fiber-crossings\nalong with kissing, fanning, and interleaving fibers. Our experimental results\nclearly illustrate a novel promising regularizing effect, that comes from the\nnonlinear orientation-based cost function. We show the properties of the\ndifferent image scales and, we show that including orientational information in\nour model makes the model better at retrieving deformations in contrast to\nstandard scalar-based registration.",
          "link": "http://arxiv.org/abs/1905.12056",
          "publishedOn": "2021-07-12T01:55:15.471Z",
          "wordCount": 660,
          "title": "Information-Theoretic Registration with Explicit Reorientation of Diffusion-Weighted Images. (arXiv:1905.12056v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yubin Yang</a>",
          "description": "This paper presents an efficient multi-scale vision Transformer, called ResT,\nthat capably served as a general-purpose backbone for image recognition. Unlike\nexisting Transformer methods, which employ standard Transformer blocks to\ntackle raw images with a fixed resolution, our ResT have several advantages:\n(1) A memory-efficient multi-head self-attention is built, which compresses the\nmemory by a simple depth-wise convolution, and projects the interaction across\nthe attention-heads dimension while keeping the diversity ability of\nmulti-heads; (2) Position encoding is constructed as spatial attention, which\nis more flexible and can tackle with input images of arbitrary size without\ninterpolation or fine-tune; (3) Instead of the straightforward tokenization at\nthe beginning of each stage, we design the patch embedding as a stack of\noverlapping convolution operation with stride on the 2D-reshaped token map. We\ncomprehensively validate ResT on image classification and downstream tasks.\nExperimental results show that the proposed ResT can outperform the recently\nstate-of-the-art backbones by a large margin, demonstrating the potential of\nResT as strong backbones. The code and models will be made publicly available\nat https://github.com/wofmanaf/ResT.",
          "link": "http://arxiv.org/abs/2105.13677",
          "publishedOn": "2021-07-12T01:55:15.452Z",
          "wordCount": 676,
          "title": "ResT: An Efficient Transformer for Visual Recognition. (arXiv:2105.13677v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yudong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>",
          "description": "Recovering a 3D head model including the complete face and hair regions is\nstill a challenging problem in computer vision and graphics. In this paper, we\nconsider this problem with a few multi-view portrait images as input. Previous\nmulti-view stereo methods, either based on the optimization strategies or deep\nlearning techniques, suffer from low-frequency geometric structures such as\nunclear head structures and inaccurate reconstruction in hair regions. To\ntackle this problem, we propose a prior-guided implicit neural rendering\nnetwork. Specifically, we model the head geometry with a learnable signed\ndistance field (SDF) and optimize it via an implicit differentiable renderer\nwith the guidance of some human head priors, including the facial prior\nknowledge, head semantic segmentation information and 2D hair orientation maps.\nThe utilization of these priors can improve the reconstruction accuracy and\nrobustness, leading to a high-quality integrated 3D head model. Extensive\nablation studies and comparisons with state-of-the-art methods demonstrate that\nour method could produce high-fidelity 3D head geometries with the guidance of\nthese priors.",
          "link": "http://arxiv.org/abs/2107.04277",
          "publishedOn": "2021-07-12T01:55:15.445Z",
          "wordCount": 596,
          "title": "Prior-Guided Multi-View 3D Head Reconstruction. (arXiv:2107.04277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tung-I Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Hsiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_J/0/1/0/all/0/1\">Jia-Fong Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "While recent progress has significantly boosted few-shot classification (FSC)\nperformance, few-shot object detection (FSOD) remains challenging for modern\nlearning systems. Existing FSOD systems follow FSC approaches, ignoring the\nissues of spatial misalignment and vagueness in class representations, and\nconsequently result in low performance. Observing this, we propose a novel\nDual-Awareness Attention (DAnA) mechanism that can adaptively generate\nquery-position-aware (QPA) support features and guide the detection networks\nprecisely. The generated QPA features represent local information of a support\nimage conditioned on a given region of the query. By taking the spatial\nrelationships across different images into consideration, our approach\nconspicuously outperforms previous FSOD methods (+6.9 AP relatively) and\nachieves remarkable results even under a challenging cross-dataset evaluation\nsetting. Furthermore, the proposed DAnA component is flexible and adaptable to\nmultiple existing object detection frameworks. By equipping DAnA, conventional\nobject detection models, Faster R-CNN and RetinaNet, which are not designed\nexplicitly for few-shot learning, reach state-of-the-art performance in FSOD\ntasks.",
          "link": "http://arxiv.org/abs/2102.12152",
          "publishedOn": "2021-07-12T01:55:15.430Z",
          "wordCount": 632,
          "title": "Dual-Awareness Attention for Few-Shot Object Detection. (arXiv:2102.12152v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.10109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1\">Daniele Cattaneo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1\">Matteo Vaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballardini_A/0/1/0/all/0/1\">Augusto Luis Ballardini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontana_S/0/1/0/all/0/1\">Simone Fontana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorrenti_D/0/1/0/all/0/1\">Domenico Giorgio Sorrenti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>",
          "description": "In this paper we present CMRNet, a realtime approach based on a Convolutional\nNeural Network to localize an RGB image of a scene in a map built from LiDAR\ndata. Our network is not trained in the working area, i.e. CMRNet does not\nlearn the map. Instead it learns to match an image to the map. We validate our\napproach on the KITTI dataset, processing each frame independently without any\ntracking procedure. CMRNet achieves 0.27m and 1.07deg median localization\naccuracy on the sequence 00 of the odometry dataset, starting from a rough pose\nestimate displaced up to 3.5m and 17deg. To the best of our knowledge this is\nthe first CNN-based approach that learns to match images from a monocular\ncamera to a given, preexisting 3D LiDAR-map.",
          "link": "http://arxiv.org/abs/1906.10109",
          "publishedOn": "2021-07-12T01:55:15.415Z",
          "wordCount": 639,
          "title": "CMRNet: Camera to LiDAR-Map Registration. (arXiv:1906.10109v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.00909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lingfei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>",
          "description": "Transfer learning (TL) tries to utilize data or knowledge from one or more\nsource domains to facilitate the learning in a target domain. It is\nparticularly useful when the target domain has few or no labeled data, due to\nannotation expense, privacy concerns, etc. Unfortunately, the effectiveness of\nTL is not always guaranteed. Negative transfer (NT), i.e., the source domain\ndata/knowledge cause reduced learning performance in the target domain, has\nbeen a long-standing and challenging problem in TL. Various approaches to\nhandle NT have been proposed in the literature. However, this filed lacks a\nsystematic survey on the formalization of NT, their factors and the algorithms\nthat handle NT. This paper proposes to fill this gap. First, the definition of\nnegative transfer is considered and a taxonomy of the factors are discussed.\nThen, near fifty representative approaches for handling NT are categorized and\nreviewed, from four perspectives: secure transfer, domain similarity\nestimation, distant transfer and negative transfer mitigation. NT in related\nfields, e.g., multi-task learning, lifelong learning, and adversarial attacks\nare also discussed.",
          "link": "http://arxiv.org/abs/2009.00909",
          "publishedOn": "2021-07-12T01:55:15.408Z",
          "wordCount": 645,
          "title": "A Survey on Negative Transfer. (arXiv:2009.00909v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1\">Claudia P&#xe9;rez-D&#x27;Arpino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1\">Lyne P. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael E. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present iGibson, a novel simulation environment to develop robotic\nsolutions for interactive tasks in large-scale realistic scenes. Our\nenvironment contains 15 fully interactive home-sized scenes with 108 rooms\npopulated with rigid and articulated objects. The scenes are replicas of\nreal-world homes, with distribution and the layout of objects aligned to those\nof the real world. iGibson integrates several key features to facilitate the\nstudy of interactive tasks: i) generation of high-quality virtual sensor\nsignals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain\nrandomization to change the materials of the objects (both visual and physical)\nand/or their shapes, iii) integrated sampling-based motion planners to generate\ncollision-free trajectories for robot bases and arms, and iv) intuitive\nhuman-iGibson interface that enables efficient collection of human\ndemonstrations. Through experiments, we show that the full interactivity of the\nscenes enables agents to learn useful visual representations that accelerate\nthe training of downstream manipulation tasks. We also show that iGibson\nfeatures enable the generalization of navigation agents, and that the\nhuman-iGibson interface and integrated motion planners facilitate efficient\nimitation learning of human demonstrated (mobile) manipulation behaviors.\niGibson is open-source, equipped with comprehensive examples and documentation.\nFor more information, visit our project website:\nthis http URL",
          "link": "http://arxiv.org/abs/2012.02924",
          "publishedOn": "2021-07-12T01:55:15.395Z",
          "wordCount": 722,
          "title": "iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1\">Raunak Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi Hong</a>",
          "description": "We introduce a neural network framework, utilizing adversarial learning to\npartition an image into two cuts, with one cut falling into a reference\ndistribution provided by the user. This concept tackles the task of\nunsupervised anomaly segmentation, which has attracted increasing attention in\nrecent years due to their broad applications in tasks with unlabelled data.\nThis Adversarial-based Selective Cutting network (ASC-Net) bridges the two\ndomains of cluster-based deep learning methods and adversarial-based\nanomaly/novelty detection algorithms. We evaluate this unsupervised learning\nmodel on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and\nMS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN\nfamily, our model demonstrates tremendous performance gains in unsupervised\nanomaly segmentation tasks. Although there is still room to further improve\nperformance compared to supervised learning algorithms, the promising\nexperimental results shed light on building an unsupervised learning algorithm\nusing user-defined knowledge.",
          "link": "http://arxiv.org/abs/2103.03664",
          "publishedOn": "2021-07-12T01:55:15.344Z",
          "wordCount": 619,
          "title": "ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation. (arXiv:2103.03664v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schafer_A/0/1/0/all/0/1\">Alexander Sch&#xe4;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isomura_T/0/1/0/all/0/1\">Tomoko Isomura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_G/0/1/0/all/0/1\">Gerd Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1\">Katsumi Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>",
          "description": "Eye contact between individuals is particularly important for understanding\nhuman behaviour. To further investigate the importance of eye contact in social\ninteractions, portable eye tracking technology seems to be a natural choice.\nHowever, the analysis of available data can become quite complex. Scientists\nneed data that is calculated quickly and accurately. Additionally, the relevant\ndata must be automatically separated to save time. In this work, we propose a\ntool called MutualEyeContact which excels in those tasks and can help\nscientists to understand the importance of (mutual) eye contact in social\ninteractions. We combine state-of-the-art eye tracking with face recognition\nbased on machine learning and provide a tool for analysis and visualization of\nsocial interaction sessions. This work is a joint collaboration of computer\nscientists and cognitive scientists. It combines the fields of social and\nbehavioural science with computer vision and deep learning.",
          "link": "http://arxiv.org/abs/2107.04476",
          "publishedOn": "2021-07-12T01:55:15.326Z",
          "wordCount": 586,
          "title": "MutualEyeContact: A conversation analysis tool with focus on eye contact. (arXiv:2107.04476v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaohao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Long Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1\">Mingsheng Shang</a>",
          "description": "Deep neural networks often suffer from poor performance or even training\nfailure due to the ill-conditioned problem, the vanishing/exploding gradient\nproblem, and the saddle point problem. In this paper, a novel method by acting\nthe gradient activation function (GAF) on the gradient is proposed to handle\nthese challenges. Intuitively, the GAF enlarges the tiny gradients and\nrestricts the large gradient. Theoretically, this paper gives conditions that\nthe GAF needs to meet, and on this basis, proves that the GAF alleviates the\nproblems mentioned above. In addition, this paper proves that the convergence\nrate of SGD with the GAF is faster than that without the GAF under some\nassumptions. Furthermore, experiments on CIFAR, ImageNet, and PASCAL visual\nobject classes confirm the GAF's effectiveness. The experimental results also\ndemonstrate that the proposed method is able to be adopted in various deep\nneural networks to improve their performance. The source code is publicly\navailable at\nhttps://github.com/LongJin-lab/Activated-Gradients-for-Deep-Neural-Networks.",
          "link": "http://arxiv.org/abs/2107.04228",
          "publishedOn": "2021-07-12T01:55:15.306Z",
          "wordCount": 594,
          "title": "Activated Gradients for Deep Neural Networks. (arXiv:2107.04228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.11385",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "We propose a new embedding method, named Quantile-Quantile Embedding (QQE),\nfor distribution transformation and manifold embedding with the ability to\nchoose the embedding distribution. QQE, which uses the concept of\nquantile-quantile plot from visual statistical tests, can transform the\ndistribution of data to any theoretical desired distribution or empirical\nreference sample. Moreover, QQE gives the user a choice of embedding\ndistribution in embedding the manifold of data into the low dimensional\nembedding space. It can also be used for modifying the embedding distribution\nof other dimensionality reduction methods, such as PCA, t-SNE, and deep metric\nlearning, for better representation or visualization of data. We propose QQE in\nboth unsupervised and supervised forms. QQE can also transform a distribution\nto either an exact reference distribution or its shape. We show that QQE allows\nfor better discrimination of classes in some cases. Our experiments on\ndifferent synthetic and image datasets show the effectiveness of the proposed\nembedding method.",
          "link": "http://arxiv.org/abs/2006.11385",
          "publishedOn": "2021-07-12T01:55:15.297Z",
          "wordCount": 656,
          "title": "Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution. (arXiv:2006.11385v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovskikh_A/0/1/0/all/0/1\">Anastasia Sosnovskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seong Jae Hwang</a>",
          "description": "Application of deep neural networks to medical imaging tasks has in some\nsense become commonplace. Still, a \"thorn in the side\" of the deep learning\nmovement is the argument that deep networks are prone to overfitting and are\nthus unable to generalize well when datasets are small (as is common in medical\nimaging tasks). One way to bolster confidence is to provide mathematical\nguarantees, or bounds, on network performance after training which explicitly\nquantify the possibility of overfitting. In this work, we explore recent\nadvances using the PAC-Bayesian framework to provide bounds on generalization\nerror for large (stochastic) networks. While previous efforts focus on\nclassification in larger natural image datasets (e.g., MNIST and CIFAR-10), we\napply these techniques to both classification and segmentation in a smaller\nmedical imagining dataset: the ISIC 2018 challenge set. We observe the\nresultant bounds are competitive compared to a simpler baseline, while also\nbeing more explainable and alleviating the need for holdout sets.",
          "link": "http://arxiv.org/abs/2104.05600",
          "publishedOn": "2021-07-12T01:55:15.289Z",
          "wordCount": 637,
          "title": "PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in Medical Imaging. (arXiv:2104.05600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haoxian Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>",
          "description": "Differentiable architecture search is prevalent in the field of NAS because\nof its simplicity and efficiency, where two paradigms, multi-path algorithms\nand single-path methods, are dominated. Multi-path framework (e.g. DARTS) is\nintuitive but suffers from memory usage and training collapse. Single-path\nmethods (e.g.GDAS and ProxylessNAS) mitigate the memory issue and shrink the\ngap between searching and evaluation but sacrifice the performance. In this\npaper, we propose a conceptually simple yet efficient method to bridge these\ntwo paradigms, referred as Mutually-aware Sub-Graphs Differentiable\nArchitecture Search (MSG-DAS). The core of our framework is a differentiable\nGumbel-TopK sampler that produces multiple mutually exclusive single-path\nsub-graphs. To alleviate the severer skip-connect issue brought by multiple\nsub-graphs setting, we propose a Dropblock-Identity module to stabilize the\noptimization. To make best use of the available models (super-net and\nsub-graphs), we introduce a memory-efficient super-net guidance distillation to\nimprove training. The proposed framework strikes a balance between flexible\nmemory usage and searching quality. We demonstrate the effectiveness of our\nmethods on ImageNet and CIFAR10, where the searched models show a comparable\nperformance as the most recent approaches.",
          "link": "http://arxiv.org/abs/2107.04324",
          "publishedOn": "2021-07-12T01:55:15.269Z",
          "wordCount": 609,
          "title": "Mutually-aware Sub-Graphs Differentiable Architecture Search. (arXiv:2107.04324v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>",
          "description": "Annotating user interfaces (UIs) that involves localization and\nclassification of meaningful UI elements on a screen is a critical step for\nmany mobile applications such as screen readers and voice control of devices.\nAnnotating object icons, such as menu, search, and arrow backward, is\nespecially challenging due to the lack of explicit labels on screens, their\nsimilarity to pictures, and their diverse shapes. Existing studies either use\nview hierarchy or pixel based methods to tackle the task. Pixel based\napproaches are more popular as view hierarchy features on mobile platforms are\noften incomplete or inaccurate, however it leaves out instructional information\nin the view hierarchy such as resource-ids or content descriptions. We propose\na novel deep learning based multi-modal approach that combines the benefits of\nboth pixel and view hierarchy features as well as leverages the\nstate-of-the-art object detection techniques. In order to demonstrate the\nutility provided, we create a high quality UI dataset by manually annotating\nthe most commonly used 29 icons in Rico, a large scale mobile design dataset\nconsisting of 72k UI screenshots. The experimental results indicate the\neffectiveness of our multi-modal approach. Our model not only outperforms a\nwidely used object classification baseline but also pixel based object\ndetection models. Our study sheds light on how to combine view hierarchy with\npixel features for annotating UI elements.",
          "link": "http://arxiv.org/abs/2107.04452",
          "publishedOn": "2021-07-12T01:55:15.243Z",
          "wordCount": 663,
          "title": "Multimodal Icon Annotation For Mobile Applications. (arXiv:2107.04452v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shikun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiaqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Ping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>",
          "description": "The reasonable definition of semantic interpretability presents the core\nchallenge in explainable AI. This paper proposes a method to modify a\ntraditional convolutional neural network (CNN) into an interpretable\ncompositional CNN, in order to learn filters that encode meaningful visual\npatterns in intermediate convolutional layers. In a compositional CNN, each\nfilter is supposed to consistently represent a specific compositional object\npart or image region with a clear meaning. The compositional CNN learns from\nimage labels for classification without any annotations of parts or regions for\nsupervision. Our method can be broadly applied to different types of CNNs.\nExperiments have demonstrated the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2107.04474",
          "publishedOn": "2021-07-12T01:55:15.236Z",
          "wordCount": 543,
          "title": "Interpretable Compositional Convolutional Neural Networks. (arXiv:2107.04474v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinrui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hengtao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuanang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hanqing Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turkbey_B/0/1/0/all/0/1\">Baris Turkbey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_B/0/1/0/all/0/1\">Bradford J. Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>",
          "description": "Prostate cancer biopsy benefits from accurate fusion of transrectal\nultrasound (TRUS) and magnetic resonance (MR) images. In the past few years,\nconvolutional neural networks (CNNs) have been proved powerful in extracting\nimage features crucial for image registration. However, challenging\napplications and recent advances in computer vision suggest that CNNs are quite\nlimited in its ability to understand spatial correspondence between features, a\ntask in which the self-attention mechanism excels. This paper aims to develop a\nself-attention mechanism specifically for cross-modal image registration. Our\nproposed cross-modal attention block effectively maps each of the features in\none volume to all features in the corresponding volume. Our experimental\nresults demonstrate that a CNN network designed with the cross-modal attention\nblock embedded outperforms an advanced CNN network 10 times of its size. We\nalso incorporated visualization techniques to improve the interpretability of\nour network. The source code of our work is available at\nhttps://github.com/DIAL-RPI/Attention-Reg .",
          "link": "http://arxiv.org/abs/2107.04548",
          "publishedOn": "2021-07-12T01:55:15.229Z",
          "wordCount": 600,
          "title": "Cross-modal Attention for MRI and Ultrasound Volume Registration. (arXiv:2107.04548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.14512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kaizhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jacky Y. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1\">Oluwasanmi Koyejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Knowledge transferability, or transfer learning, has been widely adopted to\nallow a pre-trained model in the source domain to be effectively adapted to\ndownstream tasks in the target domain. It is thus important to explore and\nunderstand the factors affecting knowledge transferability. In this paper, as\nthe first work, we analyze and demonstrate the connections between knowledge\ntransferability and another important phenomenon--adversarial transferability,\n\\emph{i.e.}, adversarial examples generated against one model can be\ntransferred to attack other models. Our theoretical studies show that\nadversarial transferability indicates knowledge transferability and vice versa.\nMoreover, based on the theoretical insights, we propose two practical\nadversarial transferability metrics to characterize this process, serving as\nbidirectional indicators between adversarial and knowledge transferability. We\nconduct extensive experiments for different scenarios on diverse datasets,\nshowing a positive correlation between adversarial transferability and\nknowledge transferability. Our findings will shed light on future research\nabout effective knowledge transfer learning and adversarial transferability\nanalyses.",
          "link": "http://arxiv.org/abs/2006.14512",
          "publishedOn": "2021-07-12T01:55:15.215Z",
          "wordCount": 656,
          "title": "Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1\">Maksim Siniukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1\">Anastasia Antsiferova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1\">Dmitriy Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>",
          "description": "Video quality measurement plays a critical role in the development of video\nprocessing applications. In this paper, we show how popular quality metrics\nVMAF and its tuning-resistant version VMAF NEG can be artificially increased by\nvideo preprocessing. We propose a pipeline for tuning parameters of processing\nalgorithms that allows increasing VMAF by up to 218.8%. A subjective comparison\nof preprocessed videos showed that with the majority of methods visual quality\ndrops down or stays unchanged. We show that VMAF NEG scores can also be\nincreased by some preprocessing methods by up to 23.6%.",
          "link": "http://arxiv.org/abs/2107.04510",
          "publishedOn": "2021-07-12T01:55:15.207Z",
          "wordCount": 535,
          "title": "Hacking VMAF and VMAF NEG: metrics vulnerability to different preprocessing. (arXiv:2107.04510v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Riedlinger_T/0/1/0/all/0/1\">Tobias Riedlinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1\">Marius Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>",
          "description": "Reliable epistemic uncertainty estimation is an essential component for\nbackend applications of deep object detectors in safety-critical environments.\nModern network architectures tend to give poorly calibrated confidences with\nlimited predictive power. Here, we introduce novel gradient-based uncertainty\nmetrics and investigate them for different object detection architectures.\nExperiments on the MS COCO, PASCAL VOC and the KITTI dataset show significant\nimprovements in true positive / false positive discrimination and prediction of\nintersection over union as compared to network confidence. We also find\nimprovement over Monte-Carlo dropout uncertainty metrics and further\nsignificant boosts by aggregating different sources of uncertainty metrics.The\nresulting uncertainty models generate well-calibrated confidences in all\ninstances. Furthermore, we implement our uncertainty quantification models into\nobject detection pipelines as a means to discern true against false\npredictions, replacing the ordinary score-threshold-based decision rule. In our\nexperiments, we achieve a significant boost in detection performance in terms\nof mean average precision. With respect to computational complexity, we find\nthat computing gradient uncertainty metrics results in floating point operation\ncounts similar to those of Monte-Carlo dropout.",
          "link": "http://arxiv.org/abs/2107.04517",
          "publishedOn": "2021-07-12T01:55:15.179Z",
          "wordCount": 618,
          "title": "Gradient-Based Quantification of Epistemic Uncertainty for Deep Object Detectors. (arXiv:2107.04517v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1\">Nuri Benbarka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_J/0/1/0/all/0/1\">Jona Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>",
          "description": "Multi-object tracking is a critical component in autonomous navigation, as it\nprovides valuable information for decision-making. Many researchers tackled the\n3D multi-object tracking task by filtering out the frame-by-frame 3D\ndetections; however, their focus was mainly on finding useful features or\nproper matching metrics. Our work focuses on a neglected part of the tracking\nsystem: score refinement and tracklet termination. We show that manipulating\nthe scores depending on time consistency while terminating the tracklets\ndepending on the tracklet score improves tracking results. We do this by\nincreasing the matched tracklets' score with score update functions and\ndecreasing the unmatched tracklets' score. Compared to count-based methods, our\nmethod consistently produces better AMOTA and MOTA scores when utilizing\nvarious detectors and filtering algorithms on different datasets. The\nimprovements in AMOTA score went up to 1.83 and 2.96 in MOTA. We also used our\nmethod as a late-fusion ensembling method, and it performed better than\nvoting-based ensemble methods by a solid margin. It achieved an AMOTA score of\n67.6 on nuScenes test evaluation, which is comparable to other state-of-the-art\ntrackers. Code is publicly available at:\n\\url{https://github.com/cogsys-tuebingen/CBMOT}.",
          "link": "http://arxiv.org/abs/2107.04327",
          "publishedOn": "2021-07-12T01:55:15.158Z",
          "wordCount": 624,
          "title": "Score refinement for confidence-based 3D multi-object tracking. (arXiv:2107.04327v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1\">In Hwa Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Multiplex immunofluorescence and immunohistochemistry benefit patients by\nallowing cancer pathologists to identify several proteins expressed on the\nsurface of cells, enabling cell classification, better understanding of the\ntumour micro-environment, more accurate diagnoses, prognoses, and tailored\nimmunotherapy based on the immune status of individual patients. However, they\nare expensive and time consuming processes which require complex staining and\nimaging techniques by expert technicians. Hoechst staining is much cheaper and\neasier to perform, but is not typically used in this case as it binds to DNA\nrather than to the proteins targeted by immunofluorescent techniques, and it\nwas not previously thought possible to differentiate cells expressing these\nproteins based only on DNA morphology. In this work we show otherwise, training\na deep convolutional neural network to identify cells expressing three proteins\n(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with\ngreater than 90% precision and recall, from Hoechst 33342 stained tissue only.\nOur model learns previously unknown morphological features associated with\nexpression of these proteins which can be used to accurately differentiate\nlymphocyte subtypes for use in key prognostic metrics such as assessment of\nimmune cell infiltration,and thereby predict and improve patient outcomes\nwithout the need for costly multiplex immunofluorescence.",
          "link": "http://arxiv.org/abs/2107.04388",
          "publishedOn": "2021-07-12T01:55:15.150Z",
          "wordCount": 654,
          "title": "Hoechst Is All You Need: LymphocyteClassification with Deep Learning. (arXiv:2107.04388v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wang_S/0/1/0/all/0/1\">Song wang</a>",
          "description": "Image inpainting aims to restore the missing regions and make the recovery\nresults identical to the originally complete image, which is different from the\ncommon generative task emphasizing the naturalness of generated images.\nNevertheless, existing works usually regard it as a pure generation problem and\nemploy cutting-edge generative techniques to address it. The generative\nnetworks fill the main missing parts with realistic contents but usually\ndistort the local structures. In this paper, we formulate image inpainting as a\nmix of two problems, i.e., predictive filtering and deep generation. Predictive\nfiltering is good at preserving local structures and removing artifacts but\nfalls short to complete the large missing regions. The deep generative network\ncan fill the numerous missing pixels based on the understanding of the whole\nscene but hardly restores the details identical to the original ones. To make\nuse of their respective advantages, we propose the joint predictive filtering\nand generative network (JPGNet) that contains three branches: predictive\nfiltering & uncertainty network (PFUNet), deep generative network, and\nuncertainty-aware fusion network (UAFNet). The PFUNet can adaptively predict\npixel-wise kernels for filtering-based inpainting according to the input image\nand output an uncertainty map. This map indicates the pixels should be\nprocessed by filtering or generative networks, which is further fed to the\nUAFNet for a smart combination between filtering and generative results. Note\nthat, our method as a novel framework for the image inpainting problem can\nbenefit any existing generation-based methods. We validate our method on three\npublic datasets, i.e., Dunhuang, Places2, and CelebA, and demonstrate that our\nmethod can enhance three state-of-the-art generative methods (i.e., StructFlow,\nEdgeConnect, and RFRNet) significantly with the slightly extra time cost.",
          "link": "http://arxiv.org/abs/2107.04281",
          "publishedOn": "2021-07-12T01:55:15.142Z",
          "wordCount": 728,
          "title": "JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting. (arXiv:2107.04281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naber_F/0/1/0/all/0/1\">Floris Naber</a>",
          "description": "Semantic segmentation models only perform well on the domain they are trained\non and datasets for training are scarce and often have a small label-spaces,\nbecause the pixel level annotations required are expensive to make. Thus\ntraining models on multiple existing domains is desired to increase the output\nlabel-space. Current research shows that there is potential to improve accuracy\nacross datasets by using multi-domain training, but this has not yet been\nsuccessfully extended to datasets of three different non-overlapping domains\nwithout manual labelling. In this paper a method for this is proposed for the\ndatasets Cityscapes, SUIM and SUN RGB-D, by creating a label-space that spans\nall classes of the datasets. Duplicate classes are merged and discrepant\ngranularity is solved by keeping classes separate. Results show that accuracy\nof the multi-domain model has higher accuracy than all baseline models\ntogether, if hardware performance is equalized, as resources are not limitless,\nshowing that models benefit from additional data even from domains that have\nnothing in common.",
          "link": "http://arxiv.org/abs/2107.04326",
          "publishedOn": "2021-07-12T01:55:15.133Z",
          "wordCount": 605,
          "title": "Semantic Segmentation on Multiple Visual Domains. (arXiv:2107.04326v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiqun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lichang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Sampling, grouping, and aggregation are three important components in the\nmulti-scale analysis of point clouds. In this paper, we present a novel\ndata-driven sampler learning strategy for point-wise analysis tasks. Unlike the\nwidely used sampling technique, Farthest Point Sampling (FPS), we propose to\nlearn sampling and downstream applications jointly. Our key insight is that\nuniform sampling methods like FPS are not always optimal for different tasks:\nsampling more points around boundary areas can make the point-wise\nclassification easier for segmentation. Towards the end, we propose a novel\nsampler learning strategy that learns sampling point displacement supervised by\ntask-related ground truth information and can be trained jointly with the\nunderlying tasks. We further demonstrate our methods in various point-wise\nanalysis architectures, including semantic part segmentation, point cloud\ncompletion, and keypoint detection. Our experiments show that jointly learning\nof the sampler and task brings remarkable improvement over previous baseline\nmethods.",
          "link": "http://arxiv.org/abs/2107.04291",
          "publishedOn": "2021-07-12T01:55:15.108Z",
          "wordCount": 593,
          "title": "Beyond Farthest Point Sampling in Point-Wise Analysis. (arXiv:2107.04291v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04306",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1\">Wenting Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Y/0/1/0/all/0/1\">Yicheng Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Changmiao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Automatic segmentation of hepatocellular carcinoma (HCC)in Digital\nSubtraction Angiography (DSA) videos can assist radiologistsin efficient\ndiagnosis of HCC and accurate evaluation of tumors in clinical practice. Few\nstudies have investigated HCC segmentation from DSAvideos. It shows great\nchallenging due to motion artifacts in filming, ambiguous boundaries of tumor\nregions and high similarity in imaging toother anatomical tissues. In this\npaper, we raise the problem of HCCsegmentation in DSA videos, and build our own\nDSA dataset. We alsopropose a novel segmentation network called DSA-LTDNet,\nincluding asegmentation sub-network, a temporal difference learning (TDL)\nmoduleand a liver region segmentation (LRS) sub-network for providing\nadditional guidance. DSA-LTDNet is preferable for learning the latent\nmotioninformation from DSA videos proactively and boosting segmentation\nperformance. All of experiments are conducted on our self-collected\ndataset.Experimental results show that DSA-LTDNet increases the DICE scoreby\nnearly 4% compared to the U-Net baseline.",
          "link": "http://arxiv.org/abs/2107.04306",
          "publishedOn": "2021-07-12T01:55:15.101Z",
          "wordCount": 599,
          "title": "Hepatocellular Carcinoma Segmentation fromDigital Subtraction Angiography Videos usingLearnable Temporal Difference. (arXiv:2107.04306v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>",
          "description": "Affective Behavior Analysis is an important part in human?computer\ninteraction. Existing successful affective behavior analysis method such as\nTSAV[9] suffer from challenge of incomplete labeled datasets. To boost its\nperformance, this paper presents a multi-task mean teacher model for\nsemi?supervised Affective Behavior Analysis to learn from missing labels and\nexploring the learning of multiple correlated task simultaneously. To be\nspecific, we first utilize TSAV as baseline model to simultaneously recognize\nthe three tasks. We have modified the preprocessing method of rendering mask to\nprovide better semantics information. After that, we extended TSAV model to\nsemi-supervised model using mean teacher, which allow it to be benefited from\nunlabeled data. Experimental results on validation datasets show that our\nmethod achieves better performance than TSAV model, which verifies that the\nproposed network can effectively learn additional unlabeled data to boost the\naffective behavior analysis performance.",
          "link": "http://arxiv.org/abs/2107.04225",
          "publishedOn": "2021-07-12T01:55:15.089Z",
          "wordCount": 581,
          "title": "A Multi-task Mean Teacher for Semi-supervised Facial Affective Behavior Analysis. (arXiv:2107.04225v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04282",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dewei Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Larson_K/0/1/0/all/0/1\">Kathleen E. Larson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Y/0/1/0/all/0/1\">Yuankai K. Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1\">Ipek Oguz</a>",
          "description": "Optical coherence tomography (OCT) is a non-invasive imaging technique widely\nused for ophthalmology. It can be extended to OCT angiography (OCT-A), which\nreveals the retinal vasculature with improved contrast. Recent deep learning\nalgorithms produced promising vascular segmentation results; however, 3D\nretinal vessel segmentation remains difficult due to the lack of manually\nannotated training data. We propose a learning-based method that is only\nsupervised by a self-synthesized modality named local intensity fusion (LIF).\nLIF is a capillary-enhanced volume computed directly from the input OCT-A. We\nthen construct the local intensity fusion encoder (LIFE) to map a given OCT-A\nvolume and its LIF counterpart to a shared latent space. The latent space of\nLIFE has the same dimensions as the input data and it contains features common\nto both modalities. By binarizing this latent space, we obtain a volumetric\nvessel segmentation. Our method is evaluated in a human fovea OCT-A and three\nzebrafish OCT-A volumes with manual labels. It yields a Dice score of 0.7736 on\nhuman data and 0.8594 +/- 0.0275 on zebrafish data, a dramatic improvement over\nexisting unsupervised algorithms.",
          "link": "http://arxiv.org/abs/2107.04282",
          "publishedOn": "2021-07-12T01:55:15.082Z",
          "wordCount": 649,
          "title": "LIFE: A Generalizable Autodidactic Pipeline for 3D OCT-A Vessel Segmentation. (arXiv:2107.04282v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04099",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liew_A/0/1/0/all/0/1\">Andrea Liew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chun Cheng Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lan_B/0/1/0/all/0/1\">Boon Leong Lan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_M/0/1/0/all/0/1\">Maxine Tan</a>",
          "description": "Convolutional neural networks (CNNs) have been used quite successfully for\nsemantic segmentation of brain tumors. However, current CNNs and attention\nmechanisms are stochastic in nature and neglect the morphological indicators\nused by radiologists to manually annotate regions of interest. In this paper,\nwe introduce a channel and spatial wise asymmetric attention (CASPIAN) by\nleveraging the inherent structure of tumors to detect regions of saliency. To\ndemonstrate the efficacy of our proposed layer, we integrate this into a\nwell-established convolutional neural network (CNN) architecture to achieve\nhigher Dice scores, with less GPU resources. Also, we investigate the inclusion\nof auxiliary multiscale and multiplanar attention branches to increase the\nspatial context crucial in semantic segmentation tasks. The resulting\narchitecture is the new CASPIANET++, which achieves Dice Scores of 91.19% whole\ntumor, 87.6% for tumor core and 81.03% for enhancing tumor. Furthermore, driven\nby the scarcity of brain tumor data, we investigate the Noisy Student method\nfor segmentation tasks. Our new Noisy Student Curriculum Learning paradigm,\nwhich infuses noise incrementally to increase the complexity of the training\nimages exposed to the network, further boosts the enhancing tumor region to\n81.53%. Additional validation performed on the BraTS2020 data shows that the\nNoisy Student Curriculum Learning method works well without any additional\ntraining or finetuning.",
          "link": "http://arxiv.org/abs/2107.04099",
          "publishedOn": "2021-07-12T01:55:15.075Z",
          "wordCount": 673,
          "title": "CASPIANET++: A Multidimensional Channel-Spatial Asymmetric Attention Network with Noisy Student Curriculum Learning Paradigm for Brain Tumor Segmentation. (arXiv:2107.04099v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Milan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1\">Mausoom Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Hiresh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>",
          "description": "Document structure extraction has been a widely researched area for decades.\nRecent work in this direction has been deep learning-based, mostly focusing on\nextracting structure using fully convolution NN through semantic segmentation.\nIn this work, we present a novel multi-modal approach for form structure\nextraction. Given simple elements such as textruns and widgets, we extract\nhigher-order structures such as TextBlocks, Text Fields, Choice Fields, and\nChoice Groups, which are essential for information collection in forms. To\nachieve this, we obtain a local image patch around each low-level element\n(reference) by identifying candidate elements closest to it. We process textual\nand spatial representation of candidates sequentially through a BiLSTM to\nobtain context-aware representations and fuse them with image patch features\nobtained by processing it through a CNN. Subsequently, the sequential decoder\ntakes this fused feature vector to predict the association type between\nreference and candidates. These predicted associations are utilized to\ndetermine larger structures through connected components analysis. Experimental\nresults show the effectiveness of our approach achieving a recall of 90.29%,\n73.80%, 83.12%, and 52.72% for the above structures, respectively,\noutperforming semantic segmentation baselines significantly. We show the\nefficacy of our method through ablations, comparing it against using individual\nmodalities. We also introduce our new rich human-annotated Forms Dataset.",
          "link": "http://arxiv.org/abs/2107.04396",
          "publishedOn": "2021-07-12T01:55:15.055Z",
          "wordCount": 655,
          "title": "Multi-Modal Association based Grouping for Form Structure Extraction. (arXiv:2107.04396v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "We show that differentially private stochastic gradient descent (DP-SGD) can\nyield poorly calibrated, overconfident deep learning models. This represents a\nserious issue for safety-critical applications, e.g. in medical diagnosis. We\nhighlight and exploit parallels between stochastic gradient Langevin dynamics,\na scalable Bayesian inference technique for training deep neural networks, and\nDP-SGD, in order to train differentially private, Bayesian neural networks with\nminor adjustments to the original (DP-SGD) algorithm. Our approach provides\nconsiderably more reliable uncertainty estimates than DP-SGD, as demonstrated\nempirically by a reduction in expected calibration error (MNIST $\\sim{5}$-fold,\nPediatric Pneumonia Dataset $\\sim{2}$-fold).",
          "link": "http://arxiv.org/abs/2107.04296",
          "publishedOn": "2021-07-12T01:55:15.047Z",
          "wordCount": 563,
          "title": "Differentially private training of neural networks with Langevin dynamics forcalibrated predictive uncertainty. (arXiv:2107.04296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yilin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuyou Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>",
          "description": "The ability to perceive the environments in different ways is essential to\nrobotic research. This involves the analysis of both 2D and 3D data sources. We\npresent a large scale urban scene dataset associated with a handy simulator\nbased on Unreal Engine 4 and AirSim, which consists of both man-made and\nreal-world reconstruction scenes in different scales, referred to as\nUrbanScene3D. Unlike previous works that purely based on 2D information or\nman-made 3D CAD models, UrbanScene3D contains both compact man-made models and\ndetailed real-world models reconstructed by aerial images. Each building has\nbeen manually extracted from the entire scene model and then has been assigned\nwith a unique label, forming an instance segmentation map. The provided 3D\nground-truth textured models with instance segmentation labels in UrbanScene3D\nallow users to obtain all kinds of data they would like to have: instance\nsegmentation map, depth map in arbitrary resolution, 3D point cloud/mesh in\nboth visible and invisible places, etc. In addition, with the help of AirSim,\nusers can also simulate the robots (cars/drones)to test a variety of autonomous\ntasks in the proposed city environment. Please refer to our paper and\nwebsite(https://vcc.tech/UrbanScene3D/) for further details and applications.",
          "link": "http://arxiv.org/abs/2107.04286",
          "publishedOn": "2021-07-12T01:55:15.039Z",
          "wordCount": 637,
          "title": "UrbanScene3D: A Large Scale Urban Scene Dataset and Simulator. (arXiv:2107.04286v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sanket Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1\">Pau Riba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>",
          "description": "One of the major prerequisites for any deep learning approach is the\navailability of large-scale training data. When dealing with scanned document\nimages in real world scenarios, the principal information of its content is\nstored in the layout itself. In this work, we have proposed an automated deep\ngenerative model using Graph Neural Networks (GNNs) to generate synthetic data\nwith highly variable and plausible document layouts that can be used to train\ndocument interpretation systems, in this case, specially in digital mailroom\napplications. It is also the first graph-based approach for document layout\ngeneration task experimented on administrative document images, in this case,\ninvoices.",
          "link": "http://arxiv.org/abs/2107.04357",
          "publishedOn": "2021-07-12T01:55:15.032Z",
          "wordCount": 549,
          "title": "Graph-based Deep Generative Modelling for Document Layout Generation. (arXiv:2107.04357v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Siyue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jimin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">BingFeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Eng Gee Lim</a>",
          "description": "Video object segmentation, aiming to segment the foreground objects given the\nannotation of the first frame, has been attracting increasing attentions. Many\nstate-of-the-art approaches have achieved great performance by relying on\nonline model updating or mask-propagation techniques. However, most online\nmodels require high computational cost due to model fine-tuning during\ninference. Most mask-propagation based models are faster but with relatively\nlow performance due to failure to adapt to object appearance variation. In this\npaper, we are aiming to design a new model to make a good balance between speed\nand performance. We propose a model, called NPMCA-net, which directly localizes\nforeground objects based on mask-propagation and non-local technique by\nmatching pixels in reference and target frames. Since we bring in information\nof both first and previous frames, our network is robust to large object\nappearance variation, and can better adapt to occlusions. Extensive experiments\nshow that our approach can achieve a new state-of-the-art performance with a\nfast speed at the same time (86.5% IoU on DAVIS-2016 and 72.2% IoU on\nDAVIS-2017, with speed of 0.11s per frame) under the same level comparison.\nSource code is available at https://github.com/siyueyu/NPMCA-net.",
          "link": "http://arxiv.org/abs/2107.04279",
          "publishedOn": "2021-07-12T01:55:15.025Z",
          "wordCount": 629,
          "title": "Fast Pixel-Matching for Video Object Segmentation. (arXiv:2107.04279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaowu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jihao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lei Huang</a>",
          "description": "Deep convolutional neural networks (CNNs) with a large number of parameters\nrequires huge computational resources, which has limited the application of\nCNNs on resources constrained appliances. Decomposition-based methods,\ntherefore, have been utilized to compress CNNs in recent years. However, since\nthe compression factor and performance are negatively correlated, the\nstate-of-the-art works either suffer from severe performance degradation or\nhave limited low compression factors. To overcome these problems, unlike\nprevious works compressing layers separately, we propose to compress CNNs and\nalleviate performance degradation via joint matrix decomposition. The idea is\ninspired by the fact that there are lots of repeated modules in CNNs, and by\nprojecting weights with the same structures into the same subspace, networks\ncan be further compressed and even accelerated. In particular, three joint\nmatrix decomposition schemes are developed, and the corresponding optimization\napproaches based on Singular Values Decomposition are proposed. Extensive\nexperiments are conducted across three challenging compact CNNs and 3 benchmark\ndata sets to demonstrate the superior performance of our proposed algorithms.\nAs a result, our methods can compress the size of ResNet-34 by 22x with\nslighter accuracy degradation compared with several state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.04386",
          "publishedOn": "2021-07-12T01:55:15.018Z",
          "wordCount": 640,
          "title": "Joint Matrix Decomposition for Deep Convolutional Neural Networks Compression. (arXiv:2107.04386v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04220",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Goswami_M/0/1/0/all/0/1\">Mayank Goswami</a>",
          "description": "Relatively abundant availability of medical imaging data has provided\nsignificant support in the development and testing of Neural Network based\nimage processing methods. Clinicians often face issues in selecting suitable\nimage processing algorithm for medical imaging data. A strategy for the\nselection of a proper model is presented here. The training data set comprises\noptical coherence tomography (OCT) and angiography (OCT-A) images of 50 mice\neyes with more than 100 days follow-up. The data contains images from treated\nand untreated mouse eyes. Four deep learning variants are tested for automatic\n(a) differentiation of tumor region with healthy retinal layer and (b)\nsegmentation of 3D ocular tumor volumes. Exhaustive sensitivity analysis of\ndeep learning models is performed with respect to the number of training and\ntesting images using 8 eight performance indices to study accuracy,\nreliability/reproducibility, and speed. U-net with UVgg16 is best for malign\ntumor data set with treatment (having considerable variation) and U-net with\nInception backbone for benign tumor data (with minor variation). Loss value and\nroot mean square error (R.M.S.E.) are found most and least sensitive\nperformance indices, respectively. The performance (via indices) is found to be\nexponentially improving regarding a number of training images. The segmented\nOCT-Angiography data shows that neovascularization drives the tumor volume.\nImage analysis shows that photodynamic imaging-assisted tumor treatment\nprotocol is transforming an aggressively growing tumor into a cyst. An\nempirical expression is obtained to help medical professionals to choose a\nparticular model given the number of images and types of characteristics. We\nrecommend that the presented exercise should be taken as standard practice\nbefore employing a particular deep learning model for biomedical image\nanalysis.",
          "link": "http://arxiv.org/abs/2107.04220",
          "publishedOn": "2021-07-12T01:55:15.000Z",
          "wordCount": 726,
          "title": "Deep Learning models for benign and malign Ocular Tumor Growth Estimation. (arXiv:2107.04220v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1\">Muhammad Ali Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Ammar Ali Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Ansar Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_R/0/1/0/all/0/1\">Rana Hammad Raza</a>",
          "description": "Image Super Resolution (SR) finds applications in areas where images need to\nbe closely inspected by the observer to extract enhanced information. One such\nfocused application is an offline forensic analysis of surveillance feeds. Due\nto the limitations of camera hardware, camera pose, limited bandwidth, varying\nillumination conditions, and occlusions, the quality of the surveillance feed\nis significantly degraded at times, thereby compromising monitoring of\nbehavior, activities, and other sporadic information in the scene. For the\nproposed research work, we have inspected the effectiveness of four\nconventional yet effective SR algorithms and three deep learning-based SR\nalgorithms to seek the finest method that executes well in a surveillance\nenvironment with limited training data op-tions. These algorithms generate an\nenhanced resolution output image from a sin-gle low-resolution (LR) input\nimage. For performance analysis, a subset of 220 images from six surveillance\ndatasets has been used, consisting of individuals with varying distances from\nthe camera, changing illumination conditions, and complex backgrounds. The\nperformance of these algorithms has been evaluated and compared using both\nqualitative and quantitative metrics. These SR algo-rithms have also been\ncompared based on face detection accuracy. By analyzing and comparing the\nperformance of all the algorithms, a Convolutional Neural Network (CNN) based\nSR technique using an external dictionary proved to be best by achieving robust\nface detection accuracy and scoring optimal quantitative metric results under\ndifferent surveillance conditions. This is because the CNN layers progressively\nlearn more complex features using an external dictionary.",
          "link": "http://arxiv.org/abs/2107.04133",
          "publishedOn": "2021-07-12T01:55:14.993Z",
          "wordCount": 704,
          "title": "Effectiveness of State-of-the-Art Super Resolution Algorithms in Surveillance Environment. (arXiv:2107.04133v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenggong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Juan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weilong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruomeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhilei Liu</a>",
          "description": "This paper describes an approach to the facial action unit (AU) detection. In\nthis work, we present our submission to the Field Affective Behavior Analysis\n(ABAW) 2021 competition. The proposed method uses the pre-trained JAA model as\nthe feature extractor, and extracts global features, face alignment features\nand AU local features on the basis of multi-scale features. We take the AU\nlocal features as the input of the graph convolution to further consider the\ncorrelation between AU, and finally use the fused features to classify AU. The\ndetected accuracy was evaluated by 0.5*accuracy + 0.5*F1. Our model achieves\n0.674 on the challenging Aff-Wild2 database.",
          "link": "http://arxiv.org/abs/2107.04389",
          "publishedOn": "2021-07-12T01:55:14.983Z",
          "wordCount": 548,
          "title": "Action Unit Detection with Joint Adaptive Attention and Graph Relation. (arXiv:2107.04389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zichen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>",
          "description": "Unsupervised deep learning has recently demonstrated the promise to produce\nhigh-quality samples. While it has tremendous potential to promote the image\ncolorization task, the performance is limited owing to the manifold hypothesis\nin machine learning. This study presents a novel scheme that exploiting the\nscore-based generative model in wavelet domain to address the issue. By taking\nadvantage of the multi-scale and multi-channel representation via wavelet\ntransform, the proposed model learns the priors from stacked wavelet\ncoefficient components, thus learns the image characteristics under coarse and\ndetail frequency spectrums jointly and effectively. Moreover, such a highly\nflexible generative model without adversarial optimization can execute\ncolorization tasks better under dual consistency terms in wavelet domain,\nnamely data-consistency and structure-consistency. Specifically, in the\ntraining phase, a set of multi-channel tensors consisting of wavelet\ncoefficients are used as the input to train the network by denoising score\nmatching. In the test phase, samples are iteratively generated via annealed\nLangevin dynamics with data and structure consistencies. Experiments\ndemonstrated remarkable improvements of the proposed model on colorization\nquality, particularly on colorization robustness and diversity.",
          "link": "http://arxiv.org/abs/2107.04261",
          "publishedOn": "2021-07-12T01:55:14.977Z",
          "wordCount": 613,
          "title": "Wavelet Transform-assisted Adaptive Generative Modeling for Colorization. (arXiv:2107.04261v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borkman_S/0/1/0/all/0/1\">Steve Borkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1\">Adam Crespi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakad_S/0/1/0/all/0/1\">Saurav Dhakad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1\">Sujoy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1\">Jonathan Hogins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhang_Y/0/1/0/all/0/1\">You-Cyuan Jhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalzadeh_M/0/1/0/all/0/1\">Mohsen Kamalzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_S/0/1/0/all/0/1\">Steven Leal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisi_P/0/1/0/all/0/1\">Pete Parisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_C/0/1/0/all/0/1\">Cesar Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1\">Wesley Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thaman_A/0/1/0/all/0/1\">Alex Thaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warren_S/0/1/0/all/0/1\">Samuel Warren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1\">Nupur Yadav</a>",
          "description": "We introduce the Unity Perception package which aims to simplify and\naccelerate the process of generating synthetic datasets for computer vision\ntasks by offering an easy-to-use and highly customizable toolset. This\nopen-source package extends the Unity Editor and engine components to generate\nperfectly annotated examples for several common computer vision tasks.\nAdditionally, it offers an extensible Randomization framework that lets the\nuser quickly construct and configure randomized simulation parameters in order\nto introduce variation into the generated datasets. We provide an overview of\nthe provided tools and how they work, and demonstrate the value of the\ngenerated synthetic datasets by training a 2D object detection model. The model\ntrained with mostly synthetic data outperforms the model trained using only\nreal data.",
          "link": "http://arxiv.org/abs/2107.04259",
          "publishedOn": "2021-07-12T01:55:14.970Z",
          "wordCount": 579,
          "title": "Unity Perception: Generate Synthetic Data for Computer Vision. (arXiv:2107.04259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1\">Mustafa Shukor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1\">Bharath Bhushan Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>",
          "description": "Generative adversarial networks (GANs) have proven to be surprisingly\nefficient for image editing by inverting and manipulating the latent code\ncorresponding to a natural image. This property emerges from the disentangled\nnature of the latent space. In this paper, we identify two geometric\nlimitations of such latent space: (a) euclidean distances differ from image\nperceptual distance, and (b) disentanglement is not optimal and facial\nattribute separation using linear model is a limiting hypothesis. We thus\npropose a new method to learn a proxy latent representation using normalizing\nflows to remedy these limitations, and show that this leads to a more efficient\nspace for face image editing.",
          "link": "http://arxiv.org/abs/2107.04481",
          "publishedOn": "2021-07-12T01:55:14.951Z",
          "wordCount": 546,
          "title": "Semantic and Geometric Unfolding of StyleGAN Latent Space. (arXiv:2107.04481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1\">Manh Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beurton_Aimar_M/0/1/0/all/0/1\">Marie Beurton-Aimar</a>",
          "description": "In this work, we introduce our submission to the 2nd Affective Behavior\nAnalysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning\nmodel on multi-databases to perform two tasks: seven basic facial expressions\nprediction and valence-arousal estimation. Since these databases do not\ncontains labels for all the two tasks, we have applied the distillation\nknowledge technique to train two networks: one teacher and one student model.\nThe student model will be trained using both ground truth labels and soft\nlabels derived from the pretrained teacher model. During the training, we add\none more task, which is the combination of the two mentioned tasks, for better\nexploiting inter-task correlations. We also exploit the sharing videos between\nthe two tasks of the AffWild2 database that is used in the competition, to\nfurther improve the performance of the network. Experiment results shows that\nthe network have achieved promising results on the validation set of the\nAffWild2 database. Code and pretrained model are publicly available at\nhttps://github.com/glmanhtu/multitask-abaw-2021",
          "link": "http://arxiv.org/abs/2107.04127",
          "publishedOn": "2021-07-12T01:55:14.944Z",
          "wordCount": 595,
          "title": "Multitask Multi-database Emotion Recognition. (arXiv:2107.04127v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yue Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoqiang Xu</a>",
          "description": "Analyzing human affect is vital for human-computer interaction systems. Most\nmethods are developed in restricted scenarios which are not practical for\nin-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021\nContest provides a benchmark for this in-the-wild problem. In this paper, we\nintroduce a multi-modal and multi-task learning method by using both visual and\naudio information. We use both AU and expression annotations to train the model\nand apply a sequence model to further extract associations between video\nframes. We achieve an AU score of 0.712 and an expression score of 0.477 on the\nvalidation set. These results demonstrate the effectiveness of our approach in\nimproving model performance.",
          "link": "http://arxiv.org/abs/2107.04187",
          "publishedOn": "2021-07-12T01:55:14.938Z",
          "wordCount": 558,
          "title": "A Multi-modal and Multi-task Learning Method for Action Unit and Expression Recognition. (arXiv:2107.04187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1\">Venkatesh K Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>",
          "description": "Adaptation of a classifier to new domains is one of the challenging problems\nin machine learning. This has been addressed using many deep and non-deep\nlearning based methods. Among the methodologies used, that of adversarial\nlearning is widely applied to solve many deep learning problems along with\ndomain adaptation. These methods are based on a discriminator that ensures\nsource and target distributions are close. However, here we suggest that rather\nthan using a point estimate obtaining by a single discriminator, it would be\nuseful if a distribution based on ensembles of discriminators could be used to\nbridge this gap. This could be achieved using multiple classifiers or using\ntraditional ensemble methods. In contrast, we suggest that a Monte Carlo\ndropout based ensemble discriminator could suffice to obtain the distribution\nbased discriminator. Specifically, we propose a curriculum based dropout\ndiscriminator that gradually increases the variance of the sample based\ndistribution and the corresponding reverse gradients are used to align the\nsource and target feature representations. An ensemble of discriminators helps\nthe model to learn the data distribution efficiently. It also provides a better\ngradient estimates to train the feature extractor. The detailed results and\nthorough ablation analysis show that our model outperforms state-of-the-art\nresults.",
          "link": "http://arxiv.org/abs/2107.04231",
          "publishedOn": "2021-07-12T01:55:14.929Z",
          "wordCount": 652,
          "title": "Exploring Dropout Discriminator for Domain Adaptation. (arXiv:2107.04231v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongxiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yichao Xiong</a>",
          "description": "State-of-the-art temporal action detectors to date are based on two-stream\ninput including RGB frames and optical flow. Although combining RGB frames and\noptical flow boosts performance significantly, optical flow is a hand-designed\nrepresentation which not only requires heavy computation, but also makes it\nmethodologically unsatisfactory that two-stream methods are often not learned\nend-to-end jointly with the flow. In this paper, we argue that optical flow is\ndispensable in high-accuracy temporal action detection and image level data\naugmentation (ILDA) is the key solution to avoid performance degradation when\noptical flow is removed. To evaluate the effectiveness of ILDA, we design a\nsimple yet efficient one-stage temporal action detector based on single RGB\nstream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has\ncomparable accuracy with all existing state-of-the-art two-stream detectors\nwhile surpassing the inference speed of previous methods by a large margin and\nthe inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is\navailable at \\url{https://github.com/Media-Smart/vedatad}.",
          "link": "http://arxiv.org/abs/2107.04362",
          "publishedOn": "2021-07-12T01:55:14.920Z",
          "wordCount": 604,
          "title": "RGB Stream Is Enough for Temporal Action Detection. (arXiv:2107.04362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuan-Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>",
          "description": "In many applications of computer graphics, art and design, it is desirable\nfor a user to provide intuitive non-image input, such as text, sketch, stroke,\ngraph or layout, and have a computer system automatically generate\nphoto-realistic images that adhere to the input content. While classic works\nthat allow such automatic image content generation have followed a framework of\nimage retrieval and composition, recent advances in deep generative models such\nas generative adversarial networks (GANs), variational autoencoders (VAEs), and\nflow-based methods have enabled more powerful and versatile image generation\ntasks. This paper reviews recent works for image synthesis given intuitive user\ninput, covering advances in input versatility, image generation methodology,\nbenchmark datasets, and evaluation metrics. This motivates new perspectives on\ninput representation and interactivity, cross pollination between major image\ngeneration paradigms, and evaluation and comparison of generation methods.",
          "link": "http://arxiv.org/abs/2107.04240",
          "publishedOn": "2021-07-12T01:55:14.900Z",
          "wordCount": 603,
          "title": "Deep Image Synthesis from Intuitive User Input: A Review and Perspectives. (arXiv:2107.04240v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thinh_P/0/1/0/all/0/1\">Phan Tran Dac Thinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1\">Hoang Manh Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hyung-Jeong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Hyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Guee-Sang Lee</a>",
          "description": "The task of predicting affective information in the wild such as seven basic\nemotions or action units from human faces has gradually become more interesting\ndue to the accessibility and availability of massive annotated datasets. In\nthis study, we propose a method that utilizes the association between seven\nbasic emotions and twelve action units from the AffWild2 dataset. The method\nbased on the architecture of ResNet50 involves the multi-task learning\ntechnique for the incomplete labels of the two tasks. By combining the\nknowledge for two correlated tasks, both performances are improved by a large\nmargin compared to those with the model employing only one kind of label.",
          "link": "http://arxiv.org/abs/2107.04192",
          "publishedOn": "2021-07-12T01:55:14.891Z",
          "wordCount": 552,
          "title": "Emotion Recognition with Incomplete Labels Using Modified Multi-task Learning Technique. (arXiv:2107.04192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1\">Jacob Donley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tourbabin_V/0/1/0/all/0/1\">Vladimir Tourbabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jung-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broyles_M/0/1/0/all/0/1\">Mark Broyles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1\">Vamsi Krishna Ithapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehra_R/0/1/0/all/0/1\">Ravish Mehra</a>",
          "description": "Augmented Reality (AR) as a platform has the potential to facilitate the\nreduction of the cocktail party effect. Future AR headsets could potentially\nleverage information from an array of sensors spanning many different\nmodalities. Training and testing signal processing and machine learning\nalgorithms on tasks such as beam-forming and speech enhancement require high\nquality representative data. To the best of the author's knowledge, as of\npublication there are no available datasets that contain synchronized\negocentric multi-channel audio and video with dynamic movement and\nconversations in a noisy environment. In this work, we describe, evaluate and\nrelease a dataset that contains over 5 hours of multi-modal data useful for\ntraining and testing algorithms for the application of improving conversations\nfor an AR glasses wearer. We provide speech intelligibility, quality and\nsignal-to-noise ratio improvement results for a baseline method and show\nimprovements across all tested metrics. The dataset we are releasing contains\nAR glasses egocentric multi-channel microphone array audio, wide field-of-view\nRGB video, speech source pose, headset microphone audio, annotated voice\nactivity, speech transcriptions, head bounding boxes, target of speech and\nsource identification labels. We have created and are releasing this dataset to\nfacilitate research in multi-modal AR solutions to the cocktail party problem.",
          "link": "http://arxiv.org/abs/2107.04174",
          "publishedOn": "2021-07-12T01:55:14.881Z",
          "wordCount": 683,
          "title": "EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments. (arXiv:2107.04174v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1\">Ekdeep Singh Lubana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hidenori Tanaka</a>",
          "description": "Inspired by BatchNorm, there has been an explosion of normalization layers\nfor deep neural networks (DNNs). However, these alternative normalization\nlayers have seen minimal use, partially due to a lack of guiding principles\nthat can help identify when these layers can serve as a replacement for\nBatchNorm. To address this problem, we take a theoretical approach,\ngeneralizing the known beneficial mechanisms of BatchNorm to several recently\nproposed normalization techniques. Our generalized theory leads to the\nfollowing set of principles: (i) similar to BatchNorm, activations-based\nnormalization layers can prevent exponential growth of activations in ResNets,\nbut parametric layers require explicit remedies; (ii) use of GroupNorm can\nensure informative forward propagation, with different samples being assigned\ndissimilar activations, but increasing group size results in increasingly\nindistinguishable activations for different samples, explaining slow\nconvergence speed in models with LayerNorm; (iii) small group sizes result in\nlarge gradient norm in earlier layers, hence explaining training instability\nissues in Instance Normalization and illustrating a speed-stability tradeoff in\nGroupNorm. Overall, our analysis reveals a unified set of mechanisms that\nunderpin the success of normalization methods in deep learning, providing us\nwith a compass to systematically explore the vast design space of DNN\nnormalization layers.",
          "link": "http://arxiv.org/abs/2106.05956",
          "publishedOn": "2021-07-09T01:58:27.809Z",
          "wordCount": 669,
          "title": "Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning. (arXiv:2106.05956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>",
          "description": "Humans have a strong intuitive understanding of the 3D environment around us.\nThe mental model of the physics in our brain applies to objects of different\nmaterials and enables us to perform a wide range of manipulation tasks that are\nfar beyond the reach of current robots. In this work, we desire to learn models\nfor dynamic 3D scenes purely from 2D visual observations. Our model combines\nNeural Radiance Fields (NeRF) and time contrastive learning with an\nautoencoding framework, which learns viewpoint-invariant 3D-aware scene\nrepresentations. We show that a dynamics model, constructed over the learned\nrepresentation space, enables visuomotor control for challenging manipulation\ntasks involving both rigid bodies and fluids, where the target is specified in\na viewpoint different from what the robot operates on. When coupled with an\nauto-decoding framework, it can even support goal specification from camera\nviewpoints that are outside the training distribution. We further demonstrate\nthe richness of the learned 3D dynamics model by performing future prediction\nand novel view synthesis. Finally, we provide detailed ablation studies\nregarding different system designs and qualitative analysis of the learned\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.04004",
          "publishedOn": "2021-07-09T01:58:27.789Z",
          "wordCount": 632,
          "title": "3D Neural Scene Representations for Visuomotor Control. (arXiv:2107.04004v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1\">Corey Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1\">Pierre Sermanet</a>",
          "description": "Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image -- something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. \"open the drawer...now pick up the block...now\npress the green button...\"). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io",
          "link": "http://arxiv.org/abs/2005.07648",
          "publishedOn": "2021-07-09T01:58:27.761Z",
          "wordCount": 735,
          "title": "Language Conditioned Imitation Learning over Unstructured Data. (arXiv:2005.07648v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+sun_W/0/1/0/all/0/1\">Wei sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>",
          "description": "To improve the viewer's quality of experience and optimize processing systems\nin computer graphics applications, the 3D quality assessment (3D-QA) has become\nan important task in the multimedia area. Point cloud and mesh are the two most\nwidely used electronic representation formats of 3D models, the quality of\nwhich is quite sensitive to operations like simplification and compression.\nTherefore, many studies concerning point cloud quality assessment (PCQA) and\nmesh quality assessment (MQA) have been carried out to measure the visual\nquality degradations caused by lossy operations. However, a large part of\nprevious studies utilizes full-reference (FR) metrics, which means they may\nfail to predict the accurate quality level of 3D models when the reference 3D\nmodel is not available. Furthermore, limited numbers of 3D-QA metrics are\ncarried out to take color features into consideration, which significantly\nrestricts the effectiveness and scope of application. In many quality\nassessment studies, natural scene statistics (NSS) have shown a good ability to\nquantify the distortion of natural scenes to statistical parameters. Therefore,\nwe propose an NSS-based no-reference quality assessment metric for colored 3D\nmodels. In this paper, quality-aware features are extracted from the aspects of\ncolor and geometry directly from the 3D models. Then the statistic parameters\nare estimated using different distribution models to describe the\ncharacteristic of the 3D models. Our method is mainly validated on the colored\npoint cloud quality assessment database (SJTU-PCQA) and the colored mesh\nquality assessment database (CMDM). The experimental results show that the\nproposed method outperforms all the state-of-art NR 3D-QA metrics and obtains\nan acceptable gap with the state-of-art FR 3D-QA metrics.",
          "link": "http://arxiv.org/abs/2107.02041",
          "publishedOn": "2021-07-09T01:58:27.712Z",
          "wordCount": 746,
          "title": "No-Reference Quality Assessment for Colored Point Cloud and Mesh Based on Natural Scene Statistics. (arXiv:2107.02041v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; Martins</a>",
          "description": "Visual attention mechanisms are widely used in multimodal tasks, as visual\nquestion answering (VQA). One drawback of softmax-based attention mechanisms is\nthat they assign some probability mass to all image regions, regardless of\ntheir adjacency structure and of their relevance to the text. In this paper, to\nbetter link the image structure with the text, we replace the traditional\nsoftmax attention mechanism with two alternative sparsity-promoting\ntransformations: sparsemax, which is able to select only the relevant regions\n(assigning zero weight to the rest), and a newly proposed Total-Variation\nSparse Attention (TVmax), which further encourages the joint selection of\nadjacent spatial locations. Experiments in VQA show gains in accuracy as well\nas higher similarity to human attention, which suggests better\ninterpretability.",
          "link": "http://arxiv.org/abs/2002.05556",
          "publishedOn": "2021-07-09T01:58:27.706Z",
          "wordCount": 587,
          "title": "Sparse and Structured Visual Attention. (arXiv:2002.05556v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Luyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanning Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huangjing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pheng_P/0/1/0/all/0/1\">Pheng-Ann Pheng</a>",
          "description": "Chest X-ray (CXR) is the most typical diagnostic X-ray examination for\nscreening various thoracic diseases. Automatically localizing lesions from CXR\nis promising for alleviating radiologists' reading burden. However, CXR\ndatasets are often with massive image-level annotations and scarce lesion-level\nannotations, and more often, without annotations. Thus far, unifying different\nsupervision granularities to develop thoracic disease detection algorithms has\nnot been comprehensively addressed. In this paper, we present OXnet, the first\ndeep omni-supervised thoracic disease detection network to our best knowledge\nthat uses as much available supervision as possible for CXR diagnosis. We first\nintroduce supervised learning via a one-stage detection model. Then, we inject\na global classification head to the detection model and propose dual attention\nalignment to guide the global gradient to the local detection branch, which\nenables learning lesion detection from image-level annotations. We also impose\nintra-class compactness and inter-class separability with global prototype\nalignment to further enhance the global information learning. Moreover, we\nleverage a soft focal loss to distill the soft pseudo-labels of unlabeled data\ngenerated by a teacher model. Extensive experiments on a large-scale chest\nX-ray dataset show the proposed OXnet outperforms competitive methods with\nsignificant margins. Further, we investigate omni-supervision under various\nannotation granularities and corroborate OXnet is a promising choice to\nmitigate the plight of annotation shortage for medical image diagnosis.",
          "link": "http://arxiv.org/abs/2104.03218",
          "publishedOn": "2021-07-09T01:58:27.692Z",
          "wordCount": 696,
          "title": "OXnet: Omni-supervised Thoracic Disease Detection from Chest X-rays. (arXiv:2104.03218v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Manli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangzhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guoxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jingyuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Heng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baogui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weihao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zongzheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yueqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yida Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuqing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1\">Danyang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chuhao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Multi-modal pre-training models have been intensively explored to bridge\nvision and language in recent years. However, most of them explicitly model the\ncross-modal interaction between image-text pairs, by assuming that there exists\nstrong semantic correlation between the text and image modalities. Since this\nstrong assumption is often invalid in real-world scenarios, we choose to\nimplicitly model the cross-modal correlation for large-scale multi-modal\npre-training, which is the focus of the Chinese project `WenLan' led by our\nteam. Specifically, with the weak correlation assumption over image-text pairs,\nwe propose a two-tower pre-training model called BriVL within the cross-modal\ncontrastive learning framework. Unlike OpenAI CLIP that adopts a simple\ncontrastive learning method, we devise a more advanced algorithm by adapting\nthe latest method MoCo into the cross-modal scenario. By building a large\nqueue-based dictionary, our BriVL can incorporate more negative samples in\nlimited GPU resources. We further construct a large Chinese multi-source\nimage-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.\nExtensive experiments demonstrate that the pre-trained BriVL model outperforms\nboth UNITER and OpenAI CLIP on various downstream tasks.",
          "link": "http://arxiv.org/abs/2103.06561",
          "publishedOn": "2021-07-09T01:58:27.613Z",
          "wordCount": 761,
          "title": "WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training. (arXiv:2103.06561v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>",
          "description": "Shortage of annotated data is one of the greatest bottlenecks in biomedical\nimage analysis. Meta learning studies how learning systems can increase in\nefficiency through experience and could thus evolve as an important concept to\novercome data sparsity. However, the core capability of meta learning-based\napproaches is the identification of similar previous tasks given a new task - a\nchallenge largely unexplored in the biomedical imaging domain. In this paper,\nwe address the problem of quantifying task similarity with a concept that we\nrefer to as task fingerprinting. The concept involves converting a given task,\nrepresented by imaging data and corresponding labels, to a fixed-length vector\nrepresentation. In fingerprint space, different tasks can be directly compared\nirrespective of their data set sizes, types of labels or specific resolutions.\nAn initial feasibility study in the field of surgical data science (SDS) with\n26 classification tasks from various medical and non-medical domains suggests\nthat task fingerprinting could be leveraged for both (1) selecting appropriate\ndata sets for pretraining and (2) selecting appropriate architectures for a new\ntask. Task fingerprinting could thus become an important tool for meta learning\nin SDS and other fields of biomedical image analysis.",
          "link": "http://arxiv.org/abs/2107.03949",
          "publishedOn": "2021-07-09T01:58:27.588Z",
          "wordCount": 639,
          "title": "Task Fingerprinting for Meta Learning in Biomedical Image Analysis. (arXiv:2107.03949v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xuejing Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Ganning Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaitai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>",
          "description": "An explainable, efficient and lightweight method for texture generation,\ncalled TGHop (an acronym of Texture Generation PixelHop), is proposed in this\nwork. Although synthesis of visually pleasant texture can be achieved by deep\nneural networks, the associated models are large in size, difficult to explain\nin theory, and computationally expensive in training. In contrast, TGHop is\nsmall in its model size, mathematically transparent, efficient in training and\ninference, and able to generate high quality texture. Given an exemplary\ntexture, TGHop first crops many sample patches out of it to form a collection\nof sample patches called the source. Then, it analyzes pixel statistics of\nsamples from the source and obtains a sequence of fine-to-coarse subspaces for\nthese patches by using the PixelHop++ framework. To generate texture patches\nwith TGHop, we begin with the coarsest subspace, which is called the core, and\nattempt to generate samples in each subspace by following the distribution of\nreal samples. Finally, texture patches are stitched to form texture images of a\nlarge size. It is demonstrated by experimental results that TGHop can generate\ntexture images of superior quality with a small model size and at a fast speed.",
          "link": "http://arxiv.org/abs/2107.04020",
          "publishedOn": "2021-07-09T01:58:27.515Z",
          "wordCount": 644,
          "title": "TGHop: An Explainable, Efficient and Lightweight Method for Texture Generation. (arXiv:2107.04020v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>",
          "description": "We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and\nclassification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,\noutperforming VQ-VAE-2.",
          "link": "http://arxiv.org/abs/2106.15282",
          "publishedOn": "2021-07-09T01:58:27.118Z",
          "wordCount": 624,
          "title": "Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsai%2A_F/0/1/0/all/0/1\">Fu-Jen Tsai*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng%2A_Y/0/1/0/all/0/1\">Yan-Tsung Peng*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1\">Chung-Chi Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>",
          "description": "Image motion blur usually results from moving objects or camera shakes. Such\nblur is generally directional and non-uniform. Previous research efforts\nattempt to solve non-uniform blur by using self-recurrent multi-scale or\nmulti-patch architectures accompanying with self-attention. However, using\nself-recurrent frameworks typically leads to a longer inference time, while\ninter-pixel or inter-channel self-attention may cause excessive memory usage.\nThis paper proposes blur-aware attention networks (BANet) that accomplish\naccurate and efficient deblurring via a single forward pass. Our BANet utilizes\nregion-based self-attention with multi-kernel strip pooling to disentangle blur\npatterns of different degrees and with cascaded parallel dilated convolution to\naggregate multi-scale content features. Extensive experimental results on the\nGoPro and HIDE benchmarks demonstrate that the proposed BANet performs\nfavorably against the state-of-the-art in blurred image restoration and can\nprovide deblurred results in real-time.",
          "link": "http://arxiv.org/abs/2101.07518",
          "publishedOn": "2021-07-09T01:58:27.061Z",
          "wordCount": 597,
          "title": "BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring. (arXiv:2101.07518v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vacher_J/0/1/0/all/0/1\">Jonathan Vacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_C/0/1/0/all/0/1\">Claire Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coen_Cagli_R/0/1/0/all/0/1\">Ruben Coen-Cagli</a>",
          "description": "Probabilistic finite mixture models are widely used for unsupervised\nclustering. These models can often be improved by adapting them to the topology\nof the data. For instance, in order to classify spatially adjacent data points\nsimilarly, it is common to introduce a Laplacian constraint on the posterior\nprobability that each data point belongs to a class. Alternatively, the mixing\nprobabilities can be treated as free parameters, while assuming Gauss-Markov or\nmore complex priors to regularize those mixing probabilities. However, these\napproaches are constrained by the shape of the prior and often lead to\ncomplicated or intractable inference. Here, we propose a new parametrization of\nthe Dirichlet distribution to flexibly regularize the mixing probabilities of\nover-parametrized mixture distributions. Using the Expectation-Maximization\nalgorithm, we show that our approach allows us to define any linear update rule\nfor the mixing probabilities, including spatial smoothing regularization as a\nspecial case. We then show that this flexible design can be extended to share\nclass information between multiple mixture models. We apply our algorithm to\nartificial and natural image segmentation tasks, and we provide quantitative\nand qualitative comparison of the performance of Gaussian and Student-t\nmixtures on the Berkeley Segmentation Dataset. We also demonstrate how to\npropagate class information across the layers of deep convolutional neural\nnetworks in a probabilistically optimal way, suggesting a new interpretation\nfor feedback signals in biological visual systems. Our flexible approach can be\neasily generalized to adapt probabilistic mixture models to arbitrary data\ntopologies.",
          "link": "http://arxiv.org/abs/1905.10629",
          "publishedOn": "2021-07-09T01:58:26.998Z",
          "wordCount": 731,
          "title": "Flexibly Regularized Mixture Models and Application to Image Segmentation. (arXiv:1905.10629v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03442",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hamghalam_M/0/1/0/all/0/1\">Mohammad Hamghalam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simpson_A/0/1/0/all/0/1\">Amber L. Simpson</a>",
          "description": "In large studies involving multi protocol Magnetic Resonance Imaging (MRI),\nit can occur to miss one or more sub-modalities for a given patient owing to\npoor quality (e.g. imaging artifacts), failed acquisitions, or hallway\ninterrupted imaging examinations. In some cases, certain protocols are\nunavailable due to limited scan time or to retrospectively harmonise the\nimaging protocols of two independent studies. Missing image modalities pose a\nchallenge to segmentation frameworks as complementary information contributed\nby the missing scans is then lost. In this paper, we propose a novel model,\nMulti-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute\none or more missing sub-modalities for a patient scan. MGP-VAE can leverage the\nGaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the\nsubjects/patients and sub-modalities correlations. Instead of designing one\nnetwork for each possible subset of present sub-modalities or using frameworks\nto mix feature maps, missing data can be generated from a single model based on\nall the available samples. We show the applicability of MGP-VAE on brain tumor\nsegmentation where either, two, or three of four sub-modalities may be missing.\nOur experiments against competitive segmentation baselines with missing\nsub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE\nmodel for segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.03442",
          "publishedOn": "2021-07-09T01:58:26.878Z",
          "wordCount": 666,
          "title": "Modality Completion via Gaussian Process Prior Variational Autoencoders for Multi-Modal Glioma Segmentation. (arXiv:2107.03442v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haixu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhiyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "This paper tackles video prediction from a new dimension of predicting\nspacetime-varying motions that are incessantly changing across both space and\ntime. Prior methods mainly capture the temporal state transitions but overlook\nthe complex spatiotemporal variations of the motion itself, making them\ndifficult to adapt to ever-changing motions. We observe that physical world\nmotions can be decomposed into transient variation and motion trend, while the\nlatter can be regarded as the accumulation of previous motions. Thus,\nsimultaneously capturing the transient variation and the motion trend is the\nkey to make spacetime-varying motions more predictable. Based on these\nobservations, we propose the MotionRNN framework, which can capture the complex\nvariations within motions and adapt to spacetime-varying scenarios. MotionRNN\nhas two main contributions. The first is that we design the MotionGRU unit,\nwhich can model the transient variation and motion trend in a unified way. The\nsecond is that we apply the MotionGRU to RNN-based predictive models and\nindicate a new flexible video prediction architecture with a Motion Highway\nthat can significantly improve the ability to predict changeable motions and\navoid motion vanishing for stacked multiple-layer predictive models. With high\nflexibility, this framework can adapt to a series of models for deterministic\nspatiotemporal prediction. Our MotionRNN can yield significant improvements on\nthree challenging benchmarks for video prediction with spacetime-varying\nmotions.",
          "link": "http://arxiv.org/abs/2103.02243",
          "publishedOn": "2021-07-09T01:58:26.854Z",
          "wordCount": 694,
          "title": "MotionRNN: A Flexible Model for Video Prediction with Spacetime-Varying Motions. (arXiv:2103.02243v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keipour_A/0/1/0/all/0/1\">Azarakhsh Keipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Guilherme A. S. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>",
          "description": "We propose a new algorithm for real-time detection and tracking of elliptic\npatterns suitable for real-world robotics applications. The method fits\nellipses to each contour in the image frame and rejects ellipses that do not\nyield a good fit. The resulting detection and tracking method is lightweight\nenough to be used on robots' resource-limited onboard computers, can deal with\nlighting variations and detect the pattern even when the view is partial. The\nmethod is tested on an example application of an autonomous UAV landing on a\nfast-moving vehicle to show its performance indoors, outdoors, and in\nsimulation on a real-world robotics task. The comparison with other well-known\nellipse detection methods shows that our proposed algorithm outperforms other\nmethods with the F1 score of 0.981 on a dataset with over 1500 frames. The\nvideos of experiments, the source codes, and the collected dataset are provided\nwith the paper at https://theairlab.org/landing-on-vehicle .",
          "link": "http://arxiv.org/abs/2102.12670",
          "publishedOn": "2021-07-09T01:58:26.846Z",
          "wordCount": 623,
          "title": "Real-Time Ellipse Detection for Robotics Applications. (arXiv:2102.12670v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10553",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1\">Luyang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1\">Yongjie Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanning Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vardhanabhuti_V/0/1/0/all/0/1\">Varut Vardhanabhuti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Mingxiang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Deep learning has demonstrated radiograph screening performances that are\ncomparable or superior to radiologists. However, recent studies show that deep\nmodels for thoracic disease classification usually show degraded performance\nwhen applied to external data. Such phenomena can be categorized into shortcut\nlearning, where the deep models learn unintended decision rules that can fit\nthe identically distributed training and test set but fail to generalize to\nother distributions. A natural way to alleviate this defect is explicitly\nindicating the lesions and focusing the model on learning the intended\nfeatures. In this paper, we conduct extensive retrospective experiments to\ncompare a popular thoracic disease classification model, CheXNet, and a\nthoracic lesion detection model, CheXDet. We first showed that the two models\nachieved similar image-level classification performance on the internal test\nset with no significant differences under many scenarios. Meanwhile, we found\nincorporating external training data even led to performance degradation for\nCheXNet. Then, we compared the models' internal performance on the lesion\nlocalization task and showed that CheXDet achieved significantly better\nperformance than CheXNet even when given 80% less training data. By further\nvisualizing the models' decision-making regions, we revealed that CheXNet\nlearned patterns other than the target lesions, demonstrating its shortcut\nlearning defect. Moreover, CheXDet achieved significantly better external\nperformance than CheXNet on both the image-level classification task and the\nlesion localization task. Our findings suggest improving annotation granularity\nfor training deep learning systems as a promising way to elevate future deep\nlearning-based diagnosis systems for clinical usage.",
          "link": "http://arxiv.org/abs/2104.10553",
          "publishedOn": "2021-07-09T01:58:26.823Z",
          "wordCount": 738,
          "title": "Rethinking annotation granularity for overcoming deep shortcut learning: A retrospective study on chest radiographs. (arXiv:2104.10553v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17171",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pohjonen_J/0/1/0/all/0/1\">Joona Pohjonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sturenberg_C/0/1/0/all/0/1\">Carolin St&#xfc;renberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rannikko_A/0/1/0/all/0/1\">Antti Rannikko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirtti_T/0/1/0/all/0/1\">Tuomas Mirtti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pitkanen_E/0/1/0/all/0/1\">Esa Pitk&#xe4;nen</a>",
          "description": "Many current neural networks for medical imaging generalise poorly to data\nunseen during training. Such behaviour can be caused by networks overfitting\neasy-to-learn, or statistically dominant, features while disregarding other\npotentially informative features. For example, indistinguishable differences in\nthe sharpness of the images from two different scanners can degrade the\nperformance of the network significantly. All neural networks intended for\nclinical practice need to be robust to variation in data caused by differences\nin imaging equipment, sample preparation and patient populations.\n\nTo address these challenges, we evaluate the utility of spectral decoupling\nas an implicit bias mitigation method. Spectral decoupling encourages the\nneural network to learn more features by simply regularising the networks'\nunnormalised prediction scores with an L2 penalty, thus having no added\ncomputational costs.\n\nWe show that spectral decoupling allows training neural networks on datasets\nwith strong spurious correlations. Networks trained without spectral decoupling\ndo not learn the original task and appear to make false predictions based on\nthe spurious correlations. Spectral decoupling also increases networks'\nrobustness for data distribution shifts. To validate our findings, we train\nnetworks with and without spectral decoupling to detect prostate cancer tissue\nslides and COVID-19 in chest radiographs. Networks trained with spectral\ndecoupling achieve substantially higher performance on all evaluation datasets.\n\nOur results show that spectral decoupling helps with generalisation issues\nassociated with neural networks. We recommend using spectral decoupling as an\nimplicit bias mitigation method in any neural network intended for clinical\nuse.",
          "link": "http://arxiv.org/abs/2103.17171",
          "publishedOn": "2021-07-09T01:58:26.798Z",
          "wordCount": 784,
          "title": "Spectral decoupling allows training transferable neural networks in medical imaging. (arXiv:2103.17171v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yu-Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhen-Duo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin-Shun Xu</a>",
          "description": "With the rapid development of social websites, recent years have witnessed an\nexplosive growth of social images with user-provided tags which continuously\narrive in a streaming fashion. Due to the fast query speed and low storage\ncost, hashing-based methods for image search have attracted increasing\nattention. However, existing hashing methods for social image retrieval are\nbased on batch mode which violates the nature of social images, i.e., social\nimages are usually generated periodically or collected in a stream fashion.\nAlthough there exist many online image hashing methods, they either adopt\nunsupervised learning which ignore the relevant tags, or are designed in the\nsupervised manner which needs high-quality labels. In this paper, to overcome\nthe above limitations, we propose a new method named Weakly-supervised Online\nHashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak\nsupervision by considering the semantics of tags and removing the noise.\nBesides, We develop a discrete online optimization algorithm for WOH, which is\nefficient and scalable. Extensive experiments conducted on two real-world\ndatasets demonstrate the superiority of WOH compared with several\nstate-of-the-art hashing baselines.",
          "link": "http://arxiv.org/abs/2009.07436",
          "publishedOn": "2021-07-09T01:58:26.790Z",
          "wordCount": 646,
          "title": "Weakly-Supervised Online Hashing. (arXiv:2009.07436v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fursa_I/0/1/0/all/0/1\">Ivan Fursa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fandi_E/0/1/0/all/0/1\">Elias Fandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_V/0/1/0/all/0/1\">Valentina Musat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culley_J/0/1/0/all/0/1\">Jacob Culley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gil_E/0/1/0/all/0/1\">Enric Gil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1\">Izzeddin Teeti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilous_L/0/1/0/all/0/1\">Louise Bilous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sluis_I/0/1/0/all/0/1\">Isaac Vander Sluis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rast_A/0/1/0/all/0/1\">Alexander Rast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1\">Andrew Bradley</a>",
          "description": "Autonomous vehicles rely heavily upon their perception subsystems to see the\nenvironment in which they operate. Unfortunately, the effect of variable\nweather conditions presents a significant challenge to object detection\nalgorithms, and thus it is imperative to test the vehicle extensively in all\nconditions which it may experience. However, development of robust autonomous\nvehicle subsystems requires repeatable, controlled testing - while real weather\nis unpredictable and cannot be scheduled. Real-world testing in adverse\nconditions is an expensive and time-consuming task, often requiring access to\nspecialist facilities. Simulation is commonly relied upon as a substitute, with\nincreasingly visually realistic representations of the real-world being\ndeveloped. In the context of the complete autonomous vehicle control pipeline,\nsubsystems downstream of perception need to be tested with accurate recreations\nof the perception system output, rather than focusing on subjective visual\nrealism of the input - whether in simulation or the real world. This study\ndevelops the untapped potential of a lightweight weather augmentation method in\nan autonomous racing vehicle - focusing not on visual accuracy, but rather the\neffect upon perception subsystem performance in real time. With minimal\nadjustment, the prototype developed in this study can replicate the effects of\nwater droplets on the camera lens, and fading light conditions. This approach\nintroduces a latency of less than 8 ms using compute hardware well suited to\nbeing carried in the vehicle - rendering it ideal for real-time implementation\nthat can be run during experiments in simulation, and augmented reality testing\nin the real world.",
          "link": "http://arxiv.org/abs/2103.02760",
          "publishedOn": "2021-07-09T01:58:26.770Z",
          "wordCount": 752,
          "title": "Worsening Perception: Real-time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions. (arXiv:2103.02760v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ancha_S/0/1/0/all/0/1\">Siddharth Ancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_G/0/1/0/all/0/1\">Gaurav Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1\">Srinivasa G. Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>",
          "description": "To safely navigate unknown environments, robots must accurately perceive\ndynamic obstacles. Instead of directly measuring the scene depth with a LiDAR\nsensor, we explore the use of a much cheaper and higher resolution sensor:\nprogrammable light curtains. Light curtains are controllable depth sensors that\nsense only along a surface that a user selects. We use light curtains to\nestimate the safety envelope of a scene: a hypothetical surface that separates\nthe robot from all obstacles. We show that generating light curtains that sense\nrandom locations (from a particular distribution) can quickly discover the\nsafety envelope for scenes with unknown objects. Importantly, we produce\ntheoretical safety guarantees on the probability of detecting an obstacle using\nrandom curtains. We combine random curtains with a machine learning based model\nthat forecasts and tracks the motion of the safety envelope efficiently. Our\nmethod accurately estimates safety envelopes while providing probabilistic\nsafety guarantees that can be used to certify the efficacy of a robot\nperception system to detect and avoid dynamic obstacles. We evaluate our\napproach in a simulated urban driving environment and a real-world environment\nwith moving pedestrians using a light curtain device and show that we can\nestimate safety envelopes efficiently and effectively. Project website:\nhttps://siddancha.github.io/projects/active-safety-envelopes-with-guarantees",
          "link": "http://arxiv.org/abs/2107.04000",
          "publishedOn": "2021-07-09T01:58:26.762Z",
          "wordCount": 665,
          "title": "Active Safety Envelopes using Light Curtains with Probabilistic Guarantees. (arXiv:2107.04000v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuanfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>",
          "description": "Precise localization of polyp is crucial for early cancer screening in\ngastrointestinal endoscopy. Videos given by endoscopy bring both richer\ncontextual information as well as more challenges than still images. The\ncamera-moving situation, instead of the common camera-fixed-object-moving one,\nleads to significant background variation between frames. Severe internal\nartifacts (e.g. water flow in the human body, specular reflection by tissues)\ncan make the quality of adjacent frames vary considerately. These factors\nhinder a video-based model to effectively aggregate features from neighborhood\nframes and give better predictions. In this paper, we present Spatial-Temporal\nFeature Transformation (STFT), a multi-frame collaborative framework to address\nthese issues. Spatially, STFT mitigates inter-frame variations in the\ncamera-moving situation with feature alignment by proposal-guided deformable\nconvolutions. Temporally, STFT proposes a channel-aware attention module to\nsimultaneously estimate the quality and correlation of adjacent frames for\nadaptive feature aggregation. Empirical studies and superior results\ndemonstrate the effectiveness and stability of our method. For example, STFT\nimproves the still image baseline FCOS by 10.6% and 20.6% on the comprehensive\nF1-score of the polyp localization task in CVC-Clinic and ASUMayo datasets,\nrespectively, and outperforms the state-of-the-art video-based method by 3.6%\nand 8.0%, respectively. Code is available at\n\\url{https://github.com/lingyunwu14/STFT}.",
          "link": "http://arxiv.org/abs/2107.03609",
          "publishedOn": "2021-07-09T01:58:26.755Z",
          "wordCount": 648,
          "title": "Multi-frame Collaboration for Effective Endoscopic Video Polyp Detection via Spatial-Temporal Feature Transformation. (arXiv:2107.03609v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03648",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Temburwar_S/0/1/0/all/0/1\">Shrikant Temburwar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajesh_B/0/1/0/all/0/1\">Bulla Rajesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "Content-based image retrieval (CBIR) systems on pixel domain use low-level\nfeatures, such as colour, texture and shape, to retrieve images. In this\ncontext, two types of image representations i.e. local and global image\nfeatures have been studied in the literature. Extracting these features from\npixel images and comparing them with images from the database is very\ntime-consuming. Therefore, in recent years, there has been some effort to\naccomplish image analysis directly in the compressed domain with lesser\ncomputations. Furthermore, most of the images in our daily transactions are\nstored in the JPEG compressed format. Therefore, it would be ideal if we could\nretrieve features directly from the partially decoded or compressed data and\nuse them for retrieval. Here, we propose a unified model for image retrieval\nwhich takes DCT coefficients as input and efficiently extracts global and local\nfeatures directly in the JPEG compressed domain for accurate image retrieval.\nThe experimental findings indicate that our proposed model performed similarly\nto the current DELG model which takes RGB features as an input with reference\nto mean average precision while having a faster training and retrieval speed.",
          "link": "http://arxiv.org/abs/2107.03648",
          "publishedOn": "2021-07-09T01:58:26.748Z",
          "wordCount": 636,
          "title": "Deep Learning Based Image Retrieval in the JPEG Compressed Domain. (arXiv:2107.03648v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huazhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "We propose to address quadrupedal locomotion tasks using Reinforcement\nLearning (RL) with a Transformer-based model that learns to combine\nproprioceptive information and high-dimensional depth sensor inputs. While\nlearning-based locomotion has made great advances using RL, most methods still\nrely on domain randomization for training blind agents that generalize to\nchallenging terrains. Our key insight is that proprioceptive states only offer\ncontact measurements for immediate reaction, whereas an agent equipped with\nvisual sensory observations can learn to proactively maneuver environments with\nobstacles and uneven terrain by anticipating changes in the environment many\nsteps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL\nmethod for quadrupedal locomotion that leverages a Transformer-based model for\nfusing proprioceptive states and visual observations. We evaluate our method in\nchallenging simulated environments with different obstacles and uneven terrain.\nWe show that our method obtains significant improvements over policies with\nonly proprioceptive state inputs, and that Transformer-based models further\nimprove generalization across environments. Our project page with videos is at\nhttps://RchalYang.github.io/LocoTransformer .",
          "link": "http://arxiv.org/abs/2107.03996",
          "publishedOn": "2021-07-09T01:58:26.708Z",
          "wordCount": 619,
          "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Regatti_J/0/1/0/all/0/1\">Jayanth Reddy Regatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Aniket Anand Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manavoglu_E/0/1/0/all/0/1\">Eren Manavoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_U/0/1/0/all/0/1\">Urun Dogan</a>",
          "description": "Recent advances in deep clustering and unsupervised representation learning\nare based on the idea that different views of an input image (generated through\ndata augmentation techniques) must either be closer in the representation\nspace, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL)\nis one such representation learning algorithm that has achieved\nstate-of-the-art results in self-supervised image classification on ImageNet\nunder the linear evaluation protocol. However, the utility of the learnt\nfeatures of BYOL to perform clustering is not explored. In this work, we study\nthe clustering ability of BYOL and observe that features learnt using BYOL may\nnot be optimal for clustering. We propose a novel consensus clustering based\nloss function, and train BYOL with the proposed loss in an end-to-end way that\nimproves the clustering ability and outperforms similar clustering based\nmethods on some popular computer vision datasets.",
          "link": "http://arxiv.org/abs/2010.01245",
          "publishedOn": "2021-07-09T01:58:26.540Z",
          "wordCount": 623,
          "title": "Consensus Clustering With Unsupervised Representation Learning. (arXiv:2010.01245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dezhao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Most of the existing video self-supervised methods mainly leverage temporal\nsignals of videos, ignoring that the semantics of moving objects and\nenvironmental information are all critical for video-related tasks. In this\npaper, we propose a novel self-supervised method for video representation\nlearning, referred to as Video 3D Sampling (V3S). In order to sufficiently\nutilize the information (spatial and temporal) provided in videos, we\npre-process a video from three dimensions (width, height, time). As a result,\nwe can leverage the spatial information (the size of objects), temporal\ninformation (the direction and magnitude of motions) as our learning target. In\nour implementation, we combine the sampling of the three dimensions and propose\nthe scale and projection transformations in space and time respectively. The\nexperimental results show that, when applied to action recognition, video\nretrieval and action similarity labeling, our approach improves the\nstate-of-the-arts with significant margins.",
          "link": "http://arxiv.org/abs/2107.03578",
          "publishedOn": "2021-07-09T01:58:26.456Z",
          "wordCount": 587,
          "title": "Video 3D Sampling for Self-supervised Representation Learning. (arXiv:2107.03578v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lucas_L/0/1/0/all/0/1\">Luis Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomas_D/0/1/0/all/0/1\">David Tomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Rodriguez_J/0/1/0/all/0/1\">Jose Garcia-Rodriguez</a>",
          "description": "One of the main issues related to unsupervised machine learning is the cost\nof processing and extracting useful information from large datasets. In this\nwork, we propose a classifier ensemble based on the transferable learning\ncapabilities of the CLIP neural network architecture in multimodal environments\n(image and text) from social media. For this purpose, we used the InstaNY100K\ndataset and proposed a validation approach based on sampling techniques. Our\nexperiments, based on image classification tasks according to the labels of the\nPlaces dataset, are performed by first considering only the visual part, and\nthen adding the associated texts as support. The results obtained demonstrated\nthat trained neural networks such as CLIP can be successfully applied to image\nclassification with little fine-tuning, and considering the associated texts to\nthe images can help to improve the accuracy depending on the goal. The results\ndemonstrated what seems to be a promising research direction.",
          "link": "http://arxiv.org/abs/2107.03751",
          "publishedOn": "2021-07-09T01:58:26.406Z",
          "wordCount": 608,
          "title": "Exploiting the relationship between visual and textual features in social networks for image classification with zero-shot deep learning. (arXiv:2107.03751v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hong-Xia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">I-Hsuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_L/0/1/0/all/0/1\">Ling Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wen-Huang Cheng</a>",
          "description": "In this work, we describe our method for tackling the valence-arousal\nestimation challenge from ABAW2 ICCV-2021 Competition. The competition\norganizers provide an in-the-wild Aff-Wild2 dataset for participants to analyze\naffective behavior in real-life settings. We use a two stream model to learn\nemotion features from appearance and action respectively. To solve data\nimbalanced problem, we apply label distribution smoothing (LDS) to re-weight\nlabels. Our proposed method achieves Concordance Correlation Coefficient (CCC)\nof 0.591 and 0.617 for valence and arousal on the validation set of Aff-wild2\ndataset.",
          "link": "http://arxiv.org/abs/2107.03891",
          "publishedOn": "2021-07-09T01:58:26.398Z",
          "wordCount": 533,
          "title": "Technical Report for Valence-Arousal Estimation in ABAW2 Challenge. (arXiv:2107.03891v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_N/0/1/0/all/0/1\">Noriaki Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takagi_Y/0/1/0/all/0/1\">Yusuke Takagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masuda_H/0/1/0/all/0/1\">Hiroki Masuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyoshi_H/0/1/0/all/0/1\">Hiroaki Miyoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohno_K/0/1/0/all/0/1\">Kei Kohno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagaishi_M/0/1/0/all/0/1\">Miharu Nagaishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1\">Kensaku Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_M/0/1/0/all/0/1\">Mai Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_T/0/1/0/all/0/1\">Takuya Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamoto_K/0/1/0/all/0/1\">Keisuke Kawamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_K/0/1/0/all/0/1\">Kyohei Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moritsubo_M/0/1/0/all/0/1\">Mayuko Moritsubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Kanako Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimasaki_Y/0/1/0/all/0/1\">Yasumasa Shimasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogura_Y/0/1/0/all/0/1\">Yusuke Ogura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imamoto_T/0/1/0/all/0/1\">Teppei Imamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishina_T/0/1/0/all/0/1\">Tatsuzo Mishina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohshima_K/0/1/0/all/0/1\">Koichi Ohshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hontani_H/0/1/0/all/0/1\">Hidekata Hontani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_I/0/1/0/all/0/1\">Ichiro Takeuchi</a>",
          "description": "In the present study, we propose a novel case-based similar image retrieval\n(SIR) method for hematoxylin and eosin (H&E)-stained histopathological images\nof malignant lymphoma. When a whole slide image (WSI) is used as an input\nquery, it is desirable to be able to retrieve similar cases by focusing on\nimage patches in pathologically important regions such as tumor cells. To\naddress this problem, we employ attention-based multiple instance learning,\nwhich enables us to focus on tumor-specific regions when the similarity between\ncases is computed. Moreover, we employ contrastive distance metric learning to\nincorporate immunohistochemical (IHC) staining patterns as useful supervised\ninformation for defining appropriate similarity between heterogeneous malignant\nlymphoma cases. In the experiment with 249 malignant lymphoma patients, we\nconfirmed that the proposed method exhibited higher evaluation measures than\nthe baseline case-based SIR methods. Furthermore, the subjective evaluation by\npathologists revealed that our similarity measure using IHC staining patterns\nis appropriate for representing the similarity of H&E-stained tissue images for\nmalignant lymphoma.",
          "link": "http://arxiv.org/abs/2107.03602",
          "publishedOn": "2021-07-09T01:58:26.367Z",
          "wordCount": 652,
          "title": "Case-based similar image retrieval for weakly annotated large histopathological images of malignant lymphoma using deep metric learning. (arXiv:2107.03602v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sibendu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kunal Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1\">Giuseppe Coviello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1\">Murugan Sankaradas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1\">Oliver Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Y. Charlie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat T. Chakradhar</a>",
          "description": "Complex sensors like video cameras include tens of configurable parameters,\nwhich can be set by end-users to customize the sensors to specific application\nscenarios. Although parameter settings significantly affect the quality of the\nsensor output and the accuracy of insights derived from sensor data, most\nend-users use a fixed parameter setting because they lack the skill or\nunderstanding to appropriately configure these parameters. We propose CamTuner,\nwhich is a system to automatically, and dynamically adapt the complex sensor to\nchanging environments. CamTuner includes two key components. First, a bespoke\nanalytics quality estimator, which is a deep-learning model to automatically\nand continuously estimate the quality of insights from an analytics unit as the\nenvironment around a sensor change. Second, a reinforcement learning (RL)\nmodule, which reacts to the changes in quality, and automatically adjusts the\ncamera parameters to enhance the accuracy of insights. We improve the training\ntime of the RL module by an order of magnitude by designing virtual models to\nmimic essential behavior of the camera: we design virtual knobs that can be set\nto different values to mimic the effects of assigning different values to the\ncamera's configurable parameters, and we design a virtual camera model that\nmimics the output from a video camera at different times of the day. These\nvirtual models significantly accelerate training because (a) frame rates from a\nreal camera are limited to 25-30 fps while the virtual models enable processing\nat 300 fps, (b) we do not have to wait until the real camera sees different\nenvironments, which could take weeks or months, and (c) virtual knobs can be\nupdated instantly, while it can take 200-500 ms to change the camera parameter\nsettings. Our dynamic tuning approach results in up to 12% improvement in the\naccuracy of insights from several video analytics tasks.",
          "link": "http://arxiv.org/abs/2107.03964",
          "publishedOn": "2021-07-09T01:58:26.350Z",
          "wordCount": 751,
          "title": "CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics. (arXiv:2107.03964v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1906.09744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_K/0/1/0/all/0/1\">Kai Ming Ting</a>",
          "description": "This paper presents a new insight into improving the performance of\nStochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of\nGaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.\nFirst, the use of Isolation kernel in t-SNE overcomes the drawback of\nmisrepresenting some structures in the data, which often occurs when Gaussian\nkernel is applied in t-SNE. This is because Gaussian kernel determines each\nlocal bandwidth based on one local point only, while Isolation kernel is\nderived directly from the data based on space partitioning. Second, the use of\nIsolation kernel yields a more efficient similarity computation because\ndata-dependent Isolation kernel has only one parameter that needs to be tuned.\nIn contrast, the use of data-independent Gaussian kernel increases the\ncomputational cost by determining n bandwidths for a dataset of n points. As\nthe root cause of these deficiencies in t-SNE is Gaussian kernel, we show that\nsimply replacing Gaussian kernel with Isolation kernel in t-SNE significantly\nimproves the quality of the final visualisation output (without creating\nmisrepresented structures) and removes one key obstacle that prevents t-SNE\nfrom processing large datasets. Moreover, Isolation kernel enables t-SNE to\ndeal with large-scale datasets in less runtime without trading off accuracy,\nunlike existing methods in speeding up t-SNE.",
          "link": "http://arxiv.org/abs/1906.09744",
          "publishedOn": "2021-07-09T01:58:26.343Z",
          "wordCount": 696,
          "title": "Improving the Effectiveness and Efficiency of Stochastic Neighbour Embedding with Isolation Kernel. (arXiv:1906.09744v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.06519",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1\">Ganna Platonova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1\">Dalibor Stys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1\">Pavel Soucek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1\">Kirill Lonhus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1\">Jan Valenta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1\">Renata Rychtarikova</a>",
          "description": "The most realistic information about the transparent sample such as a live\ncell can be obtained only using bright-field light microscopy. At\nhigh-intensity pulsing LED illumination, we captured a primary\n12-bit-per-channel (bpc) response from an observed sample using a bright-field\nmicroscope equipped with a high-resolution (4872x3248) image sensor. In order\nto suppress data distortions originating from the light interactions with\nelements in the optical path, poor sensor reproduction (geometrical defects of\nthe camera sensor and some peculiarities of sensor sensitivity), we propose a\nspectroscopic approach for the correction of this uncompressed 12-bpc data by\nsimultaneous calibration of all parts of the experimental arrangement.\nMoreover, the final intensities of the corrected images are proportional to the\nphoton fluxes detected by a camera sensor. It can be visualized in 8-bpc\nintensity depth after the Least Information Loss compression [Lect. Notes\nBioinform. 9656, 527 (2016)].",
          "link": "http://arxiv.org/abs/1903.06519",
          "publishedOn": "2021-07-09T01:58:26.336Z",
          "wordCount": 632,
          "title": "Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03642",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_N/0/1/0/all/0/1\">Ningyuan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1\">Jiayan Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jiangjian Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Chengbin Peng</a>",
          "description": "PSNR and SSIM are the most widely used metrics in super-resolution problems,\nbecause they are easy to use and can evaluate the similarities between\ngenerated images and reference images. However, single image super-resolution\nis an ill-posed problem, there are multiple corresponding high-resolution\nimages for the same low-resolution image. The similarities can't totally\nreflect the restoration effect. The perceptual quality of generated images is\nalso important, but PSNR and SSIM do not reflect perceptual quality well. To\nsolve the problem, we proposed a method called regional differential\ninformation entropy to measure both of the similarities and perceptual quality.\nTo overcome the problem that traditional image information entropy can't\nreflect the structure information, we proposed to measure every region's\ninformation entropy with sliding window. Considering that the human visual\nsystem is more sensitive to the brightness difference at low brightness, we\ntake $\\gamma$ quantization rather than linear quantization. To accelerate the\nmethod, we reorganized the calculation procedure of information entropy with a\nneural network. Through experiments on our IQA dataset and PIPAL, this paper\nproves that RDIE can better quantify perceptual quality of images especially\nGAN-based images.",
          "link": "http://arxiv.org/abs/2107.03642",
          "publishedOn": "2021-07-09T01:58:26.316Z",
          "wordCount": 643,
          "title": "Regional Differential Information Entropy for Super-Resolution Image Quality Assessment. (arXiv:2107.03642v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Longyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sham_C/0/1/0/all/0/1\">Chiu-Wing Sham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1\">Chun Yan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xinchao Zhong</a>",
          "description": "Extracting and analyzing iris textures for biometric recognition has been\nextensively studied. As the transition of iris recognition from lab technology\nto nation-scale applications, most systems are facing high complexity in either\ntime or space, leading to unfitness for embedded devices. In this paper, the\nproposed design includes a minimal set of computer vision modules and\nmulti-mode QC-LDPC decoder which can alleviate variability and noise caused by\niris acquisition and follow-up process. Several classes of QC-LDPC code from\nIEEE 802.16 are tested for the validity of accuracy improvement. Some of the\ncodes mentioned above are used for further QC-LDPC decoder quantization,\nvalidation and comparison to each other. We show that we can apply Dynamic\nPartial Reconfiguration technology to implement the multi-mode QC-LDPC decoder\nfor the iris recognition system. The results show that the implementation is\npower-efficient and good for edge applications.",
          "link": "http://arxiv.org/abs/2107.03688",
          "publishedOn": "2021-07-09T01:58:26.305Z",
          "wordCount": 590,
          "title": "An Embedded Iris Recognition System Optimization using Dynamically ReconfigurableDecoder with LDPC Codes. (arXiv:2107.03688v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vakhitov_A/0/1/0/all/0/1\">Alexander Vakhitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colomina_L/0/1/0/all/0/1\">Luis Ferraz Colomina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>",
          "description": "Perspective-n-Point-and-Line (P$n$PL) algorithms aim at fast, accurate, and\nrobust camera localization with respect to a 3D model from 2D-3D feature\ncorrespondences, being a major part of modern robotic and AR/VR systems.\nCurrent point-based pose estimation methods use only 2D feature detection\nuncertainties, and the line-based methods do not take uncertainties into\naccount. In our setup, both 3D coordinates and 2D projections of the features\nare considered uncertain. We propose PnP(L) solvers based on EPnP and DLS for\nthe uncertainty-aware pose estimation. We also modify motion-only bundle\nadjustment to take 3D uncertainties into account. We perform exhaustive\nsynthetic and real experiments on two different visual odometry datasets. The\nnew PnP(L) methods outperform the state-of-the-art on real data in isolation,\nshowing an increase in mean translation accuracy by 18% on a representative\nsubset of KITTI, while the new uncertain refinement improves pose accuracy for\nmost of the solvers, e.g. decreasing mean translation error for the EPnP by 16%\ncompared to the standard refinement on the same dataset. The code is available\nat https://alexandervakhitov.github.io/uncertain-pnp/.",
          "link": "http://arxiv.org/abs/2107.03890",
          "publishedOn": "2021-07-09T01:58:26.298Z",
          "wordCount": 613,
          "title": "Uncertainty-Aware Camera Pose Estimation from Points and Lines. (arXiv:2107.03890v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jetchev_N/0/1/0/all/0/1\">Nikolay Jetchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_G/0/1/0/all/0/1\">G&#xf6;khan Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bracher_C/0/1/0/all/0/1\">Christian Bracher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollgraf_R/0/1/0/all/0/1\">Roland Vollgraf</a>",
          "description": "Attention is a general reasoning mechanism than can flexibly deal with image\ninformation, but its memory requirements had made it so far impractical for\nhigh resolution image generation. We present Grid Partitioned Attention (GPA),\na new approximate attention algorithm that leverages a sparse inductive bias\nfor higher computational and memory efficiency in image domains: queries attend\nonly to few keys, spatially close queries attend to close keys due to\ncorrelations. Our paper introduces the new attention layer, analyzes its\ncomplexity and how the trade-off between memory usage and model power can be\ntuned by the hyper-parameters.We will show how such attention enables novel\ndeep learning architectures with copying modules that are especially useful for\nconditional image generation tasks like pose morphing. Our contributions are\n(i) algorithm and code1of the novel GPA layer, (ii) a novel deep\nattention-copying architecture, and (iii) new state-of-the art experimental\nresults in human pose morphing generation benchmarks.",
          "link": "http://arxiv.org/abs/2107.03742",
          "publishedOn": "2021-07-09T01:58:26.290Z",
          "wordCount": 607,
          "title": "Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation. (arXiv:2107.03742v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benkner_M/0/1/0/all/0/1\">Marcel Seelbach Benkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>",
          "description": "Matching problems on 3D shapes and images are challenging as they are\nfrequently formulated as combinatorial quadratic assignment problems (QAPs)\nwith permutation matrix constraints, which are NP-hard. In this work, we\naddress such problems with emerging quantum computing technology and propose\nseveral reformulations of QAPs as unconstrained problems suitable for efficient\nexecution on quantum hardware. We investigate several ways to inject\npermutation matrix constraints in a quadratic unconstrained binary optimization\nproblem which can be mapped to quantum hardware. We focus on obtaining a\nsufficient spectral gap, which further increases the probability to measure\noptimal solutions and valid permutation matrices in a single run. We perform\nour experiments on the quantum computer D-Wave 2000Q (2^11 qubits, adiabatic).\nDespite the observed discrepancy between simulated adiabatic quantum computing\nand execution on real quantum hardware, our reformulation of permutation matrix\nconstraints increases the robustness of the numerical computations over other\npenalty approaches in our experiments. The proposed algorithm has the potential\nto scale to higher dimensions on future quantum computing architectures, which\nopens up multiple new directions for solving matching problems in 3D computer\nvision and graphics.",
          "link": "http://arxiv.org/abs/2107.04032",
          "publishedOn": "2021-07-09T01:58:26.284Z",
          "wordCount": 638,
          "title": "Adiabatic Quantum Graph Matching with Permutation Matrix Constraints. (arXiv:2107.04032v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oishi_S/0/1/0/all/0/1\">Shuji Oishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koide_K/0/1/0/all/0/1\">Kenji Koide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokozuka_M/0/1/0/all/0/1\">Masashi Yokozuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banno_A/0/1/0/all/0/1\">Atsuhiko Banno</a>",
          "description": "This study presents a framework for capturing human attention in the\nspatio-temporal domain using eye-tracking glasses. Attention mapping is a key\ntechnology for human perceptual activity analysis or Human-Robot Interaction\n(HRI) to support human visual cognition; however, measuring human attention in\ndynamic environments is challenging owing to the difficulty in localizing the\nsubject and dealing with moving objects. To address this, we present a\ncomprehensive framework, 4D Attention, for unified gaze mapping onto static and\ndynamic objects. Specifically, we estimate the glasses pose by leveraging a\nloose coupling of direct visual localization and Inertial Measurement Unit\n(IMU) values. Further, by installing reconstruction components into our\nframework, dynamic objects not captured in the 3D environment map are\ninstantiated based on the input images. Finally, a scene rendering component\nsynthesizes a first-person view with identification (ID) textures and performs\ndirect 2D-3D gaze association. Quantitative evaluations showed the\neffectiveness of our framework. Additionally, we demonstrated the applications\nof 4D Attention through experiments in real situations.",
          "link": "http://arxiv.org/abs/2107.03606",
          "publishedOn": "2021-07-09T01:58:26.261Z",
          "wordCount": 597,
          "title": "4D Attention: Comprehensive Framework for Spatio-Temporal Gaze Mapping. (arXiv:2107.03606v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2005.08307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertugli_A/0/1/0/all/0/1\">Alessia Bertugli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coscia_P/0/1/0/all/0/1\">Pasquale Coscia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Anticipating human motion in crowded scenarios is essential for developing\nintelligent transportation systems, social-aware robots and advanced video\nsurveillance applications. A key component of this task is represented by the\ninherently multi-modal nature of human paths which makes socially acceptable\nmultiple futures when human interactions are involved. To this end, we propose\na generative architecture for multi-future trajectory predictions based on\nConditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning\nmainly relies on prior belief maps, representing most likely moving directions\nand forcing the model to consider past observed dynamics in generating future\npositions. Human interactions are modeled with a graph-based attention\nmechanism enabling an online attentive hidden state refinement of the recurrent\nestimation. To corroborate our model, we perform extensive experiments on\npublicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS\nSportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its\neffectiveness in crowded scenes compared to several state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2005.08307",
          "publishedOn": "2021-07-09T01:58:26.255Z",
          "wordCount": 628,
          "title": "AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction. (arXiv:2005.08307v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1\">Martin Knoche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1\">Stefan H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>",
          "description": "Face recognition approaches often rely on equal image resolution for\nverification faces on two images. However, in practical applications, those\nimage resolutions are usually not in the same range due to different image\ncapture mechanisms or sources. In this work, we first analyze the impact of\nimage resolutions on the face verification performance with a state-of-the-art\nface recognition model. For images, synthetically reduced to $5\\, \\times 5\\,\n\\mathrm{px}$ resolution, the verification performance drops from $99.23\\%$\nincreasingly down to almost $55\\%$. Especially, for cross-resolution image\npairs (one high- and one low-resolution image), the verification accuracy\ndecreases even further. We investigate this behavior more in-depth by looking\nat the feature distances for every 2-image test pair. To tackle this problem,\nwe propose the following two methods: 1) Train a state-of-the-art\nface-recognition model straightforward with $50\\%$ low-resolution images\ndirectly within each batch. \\\\ 2) Train a siamese-network structure and adding\na cosine distance feature loss between high- and low-resolution features. Both\nmethods show an improvement for cross-resolution scenarios and can increase the\naccuracy at very low resolution to approximately $70\\%$. However, a\ndisadvantage is that a specific model needs to be trained for every\nresolution-pair ...",
          "link": "http://arxiv.org/abs/2107.03769",
          "publishedOn": "2021-07-09T01:58:26.248Z",
          "wordCount": 636,
          "title": "Image Resolution Susceptibility of Face Recognition Models. (arXiv:2107.03769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03987",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dingjie Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakravorti_S/0/1/0/all/0/1\">Srijata Chakravorti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noble_J/0/1/0/all/0/1\">Jack H. Noble</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawant_B/0/1/0/all/0/1\">Be-noit M. Dawant</a>",
          "description": "We propose an atlas-based method to segment the intracochlear anatomy (ICA)\nin the post-implantation CT (Post-CT) images of cochlear implant (CI)\nrecipients that preserves the point-to-point correspondence between the meshes\nin the atlas and the segmented volumes. To solve this problem, which is\nchallenging because of the strong artifacts produced by the implant, we use a\npair of co-trained deep networks that generate dense deformation fields (DDFs)\nin opposite directions. One network is tasked with registering an atlas image\nto the Post-CT images and the other network is tasked with registering the\nPost-CT images to the atlas image. The networks are trained using loss\nfunctions based on voxel-wise labels, image content, fiducial registration\nerror, and cycle-consistency constraint. The segmentation of the ICA in the\nPost-CT images is subsequently obtained by transferring the predefined\nsegmentation meshes of the ICA in the atlas image to the Post-CT images using\nthe corresponding DDFs generated by the trained registration networks. Our\nmodel can learn the underlying geometric features of the ICA even though they\nare obscured by the metal artifacts. We show that our end-to-end network\nproduces results that are comparable to the current state of the art (SOTA)\nthat relies on a two-steps approach that first uses conditional generative\nadversarial networks to synthesize artifact-free images from the Post-CT images\nand then uses an active shape model-based method to segment the ICA in the\nsynthetic images. Our method requires a fraction of the time needed by the\nSOTA, which is important for end-user acceptance.",
          "link": "http://arxiv.org/abs/2107.03987",
          "publishedOn": "2021-07-09T01:58:26.241Z",
          "wordCount": 727,
          "title": "Atlas-Based Segmentation of Intracochlear Anatomy in Metal Artifact Affected CT Images of the Ear with Co-trained Deep Neural Networks. (arXiv:2107.03987v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03846",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1\">Doaa Emam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1\">Nada Mufti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1\">Frederic Guffens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1\">Thomas Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demaerel_P/0/1/0/all/0/1\">Philippe Demaerel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1\">Andrew Melbourne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jam Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>",
          "description": "Deep neural networks have increased the accuracy of automatic segmentation,\nhowever, their accuracy depends on the availability of a large number of fully\nsegmented images. Methods to train deep neural networks using images for which\nsome, but not all, regions of interest are segmented are necessary to make\nbetter use of partially annotated datasets. In this paper, we propose the first\naxiomatic definition of label-set loss functions that are the loss functions\nthat can handle partially segmented images. We prove that there is one and only\none method to convert a classical loss function for fully segmented images into\na proper label-set loss function. Our theory also allows us to define the\nleaf-Dice loss, a label-set generalization of the Dice loss particularly suited\nfor partial supervision with only missing labels. Using the leaf-Dice loss, we\nset a new state of the art in partially supervised learning for fetal brain 3D\nMRI segmentation. We achieve a deep neural network able to segment white\nmatter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter,\ndeep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of\nanatomically normal fetuses or with open spina bifida. Our implementation of\nthe proposed label-set loss functions is available at\nhttps://github.com/LucasFidon/label-set-loss-functions",
          "link": "http://arxiv.org/abs/2107.03846",
          "publishedOn": "2021-07-09T01:58:26.233Z",
          "wordCount": 687,
          "title": "Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation. (arXiv:2107.03846v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>",
          "description": "Online continual learning aims to learn from a non-IID stream of data from a\nnumber of different tasks, where the learner is only allowed to consider data\nonce. Methods are typically allowed to use a limited buffer to store some of\nthe images in the stream. Recently, it was found that feature replay, where an\nintermediate layer representation of the image is stored (or generated) leads\nto superior results than image replay, while requiring less memory. Quantized\nexemplars can further reduce the memory usage. However, a drawback of these\nmethods is that they use a fixed (or very intransigent) backbone network. This\nsignificantly limits the learning of representations that can discriminate\nbetween all tasks. To address this problem, we propose an auxiliary classifier\nauto-encoder (ACAE) module for feature replay at intermediate layers with high\ncompression rates. The reduced memory footprint per image allows us to save\nmore exemplars for replay. In our experiments, we conduct task-agnostic\nevaluation under online continual learning setting and get state-of-the-art\nperformance on ImageNet-Subset, CIFAR100 and CIFAR10 dataset.",
          "link": "http://arxiv.org/abs/2105.08595",
          "publishedOn": "2021-07-09T01:58:26.213Z",
          "wordCount": 644,
          "title": "ACAE-REMIND for Online Continual Learning with Compressed Feature Replay. (arXiv:2105.08595v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1\">Yonghao Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Video-based human pose estimation (HPE) is a vital yet challenging task.\nWhile deep learning methods have made significant progress for the HPE, most\napproaches to this task detect each joint independently, damaging the pose\nstructural information. In this paper, unlike the prior methods, we propose a\nRelation-based Pose Semantics Transfer Network (RPSTN) to locate joints\nassociatively. Specifically, we design a lightweight joint relation extractor\n(JRE) to model the pose structural features and associatively generate heatmaps\nfor joints by modeling the relation between any two joints heuristically\ninstead of building each joint heatmap independently. Actually, the proposed\nJRE module models the spatial configuration of human poses through the\nrelationship between any two joints. Moreover, considering the temporal\nsemantic continuity of videos, the pose semantic information in the current\nframe is beneficial for guiding the location of joints in the next frame.\nTherefore, we use the idea of knowledge reuse to propagate the pose semantic\ninformation between consecutive frames. In this way, the proposed RPSTN\ncaptures temporal dynamics of poses. On the one hand, the JRE module can infer\ninvisible joints according to the relationship between the invisible joints and\nother visible joints in space. On the other hand, in the time, the propose\nmodel can transfer the pose semantic features from the non-occluded frame to\nthe occluded frame to locate occluded joints. Therefore, our method is robust\nto the occlusion and achieves state-of-the-art results on the two challenging\ndatasets, which demonstrates its effectiveness for video-based human pose\nestimation. We will release the code and models publicly.",
          "link": "http://arxiv.org/abs/2107.03591",
          "publishedOn": "2021-07-09T01:58:26.206Z",
          "wordCount": 691,
          "title": "Relation-Based Associative Joint Location for Human Pose Estimation in Videos. (arXiv:2107.03591v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1\">Yannis Panagakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1\">Jean Kossaifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1\">Grigorios G. Chrysos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oldfield_J/0/1/0/all/0/1\">James Oldfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolaou_M/0/1/0/all/0/1\">Mihalis A. Nicolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>",
          "description": "Tensors, or multidimensional arrays, are data structures that can naturally\nrepresent visual data of multiple dimensions. Inherently able to efficiently\ncapture structured, latent semantic spaces and high-order interactions, tensors\nhave a long history of applications in a wide span of computer vision problems.\nWith the advent of the deep learning paradigm shift in computer vision, tensors\nhave become even more fundamental. Indeed, essential ingredients in modern deep\nlearning architectures, such as convolutions and attention mechanisms, can\nreadily be considered as tensor mappings. In effect, tensor methods are\nincreasingly finding significant applications in deep learning, including the\ndesign of memory and compute efficient network architectures, improving\nrobustness to random noise and adversarial attacks, and aiding the theoretical\nunderstanding of deep networks.\n\nThis article provides an in-depth and practical review of tensors and tensor\nmethods in the context of representation learning and deep learning, with a\nparticular focus on visual data analysis and computer vision applications.\nConcretely, besides fundamental work in tensor-based visual data analysis\nmethods, we focus on recent developments that have brought on a gradual\nincrease of tensor methods, especially in deep learning architectures, and\ntheir implications in computer vision applications. To further enable the\nnewcomer to grasp such concepts quickly, we provide companion Python notebooks,\ncovering key aspects of the paper and implementing them, step-by-step with\nTensorLy.",
          "link": "http://arxiv.org/abs/2107.03436",
          "publishedOn": "2021-07-09T01:58:26.199Z",
          "wordCount": 670,
          "title": "Tensor Methods in Computer Vision and Deep Learning. (arXiv:2107.03436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangal_A/0/1/0/all/0/1\">Ayushe Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Peeyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_S/0/1/0/all/0/1\">Sunita Kumari</a>",
          "description": "In the following paper, we have combined the various basic functionalities\nprovided by the NumPy library and OpenCv library, which is an open source for\nComputer Vision applications, like conversion of colored images to grayscale,\ncalculating threshold, finding contours and using those contour points to take\nperspective transform of the image inputted by the user, using Python version\n3.7. Additional features include cropping, rotating and saving as well. All\nthese functions and features, when implemented step by step, results in a\ncomplete scanning application. The applied procedure involves the following\nsteps: Finding contours, applying Perspective transform and brightening the\nimage, Adaptive Thresholding and applying filters for noise cancellation, and\nRotation features and perspective transform for a special cropping algorithm.\nThe described technique is implemented on various samples.",
          "link": "http://arxiv.org/abs/2107.03700",
          "publishedOn": "2021-07-09T01:58:26.192Z",
          "wordCount": 562,
          "title": "Complete Scanning Application Using OpenCv. (arXiv:2107.03700v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>",
          "description": "Affective Analysis is not a single task, and the valence-arousal value,\nexpression class and action unit can be predicted at the same time. Previous\nresearches failed to take them as a whole task or ignore the entanglement and\nhierarchical relation of this three facial attributes. We propose a novel model\nnamed feature pyramid networks for multi-task affect analysis. The hierarchical\nfeatures are extracted to predict three labels and we apply teacher-student\ntraining strategy to learn from pretrained single-task models. Extensive\nexperiment results demonstrate the proposed model outperform other models. The\ncode and model are available for research purposes at\n$\\href{https://github.com/ryanhe312/ABAW2-FPNMAA}{\\text{this link}}$.",
          "link": "http://arxiv.org/abs/2107.03670",
          "publishedOn": "2021-07-09T01:58:26.184Z",
          "wordCount": 540,
          "title": "Feature Pyramid Network for Multi-task Affective Analysis. (arXiv:2107.03670v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Subhranil Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bathula_D/0/1/0/all/0/1\">Deepti R. Bathula</a>",
          "description": "Different categories of visual stimuli activate different responses in the\nhuman brain. These signals can be captured with EEG for utilization in\napplications such as Brain-Computer Interface (BCI). However, accurate\nclassification of single-trial data is challenging due to low signal-to-noise\nratio of EEG. This work introduces an EEG-ConvTranformer network that is based\non multi-headed self-attention. Unlike other transformers, the model\nincorporates self-attention to capture inter-region interactions. It further\nextends to adjunct convolutional filters with multi-head attention as a single\nmodule to learn temporal patterns. Experimental results demonstrate that\nEEG-ConvTransformer achieves improved classification accuracy over the\nstate-of-the-art techniques across five different visual stimuli classification\ntasks. Finally, quantitative analysis of inter-head diversity also shows low\nsimilarity in representational subspaces, emphasizing the implicit diversity of\nmulti-head attention.",
          "link": "http://arxiv.org/abs/2107.03983",
          "publishedOn": "2021-07-09T01:58:26.147Z",
          "wordCount": 569,
          "title": "EEG-ConvTransformer for Single-Trial EEG based Visual Stimuli Classification. (arXiv:2107.03983v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaoliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wulong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>",
          "description": "Shift neural networks reduce computation complexity by removing expensive\nmultiplication operations and quantizing continuous weights into low-bit\ndiscrete values, which are fast and energy efficient compared to conventional\nneural networks. However, existing shift networks are sensitive to the weight\ninitialization, and also yield a degraded performance caused by vanishing\ngradient and weight sign freezing problem. To address these issues, we propose\nS low-bit re-parameterization, a novel technique for training low-bit shift\nnetworks. Our method decomposes a discrete parameter in a sign-sparse-shift\n3-fold manner. In this way, it efficiently learns a low-bit network with a\nweight dynamics similar to full-precision networks and insensitive to weight\ninitialization. Our proposed training method pushes the boundaries of shift\nneural networks and shows 3-bit shift networks out-performs their\nfull-precision counterparts in terms of top-1 accuracy on ImageNet.",
          "link": "http://arxiv.org/abs/2107.03453",
          "publishedOn": "2021-07-09T01:58:26.141Z",
          "wordCount": 581,
          "title": "$S^3$: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks. (arXiv:2107.03453v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>",
          "description": "Many existing deep neural networks (DNNs) for 3D point cloud semantic\nsegmentation require a large amount of fully labeled training data. However,\nmanually assigning point-level labels on the complex scenes is time-consuming.\nWhile unlabeled point clouds can be easily obtained from sensors or\nreconstruction, we propose a superpoint constrained semi-supervised\nsegmentation network for 3D point clouds, named as SCSS-Net. Specifically, we\nuse the pseudo labels predicted from unlabeled point clouds for self-training,\nand the superpoints produced by geometry-based and color-based Region Growing\nalgorithms are combined to modify and delete pseudo labels with low confidence.\nAdditionally, we propose an edge prediction module to constrain the features\nfrom edge points of geometry and color. A superpoint feature aggregation module\nand superpoint feature consistency loss functions are introduced to smooth the\npoint features in each superpoint. Extensive experimental results on two 3D\npublic indoor datasets demonstrate that our method can achieve better\nperformance than some state-of-the-art point cloud segmentation networks and\nsome popular semi-supervised segmentation methods with few labeled scenes.",
          "link": "http://arxiv.org/abs/2107.03601",
          "publishedOn": "2021-07-09T01:58:26.133Z",
          "wordCount": 604,
          "title": "SCSS-Net: Superpoint Constrained Semi-supervised Segmentation Network for 3D Indoor Scenes. (arXiv:2107.03601v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antoniadis_P/0/1/0/all/0/1\">Panagiotis Antoniadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_I/0/1/0/all/0/1\">Ioannis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filntisis_P/0/1/0/all/0/1\">Panagiotis P. Filntisis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>",
          "description": "In this work we tackle the task of video-based audio-visual emotion\nrecognition, within the premises of the 2nd Workshop and Competition on\nAffective Behavior Analysis in-the-wild (ABAW). Standard methodologies that\nrely solely on the extraction of facial features often fall short of accurate\nemotion prediction in cases where the aforementioned source of affective\ninformation is inaccessible due to head/body orientation, low resolution and\npoor illumination. We aspire to alleviate this problem by leveraging bodily as\nwell as contextual features, as part of a broader emotion recognition\nframework. A standard CNN-RNN cascade constitutes the backbone of our proposed\nmodel for sequence-to-sequence (seq2seq) learning. Apart from learning through\nthe \\textit{RGB} input modality, we construct an aural stream which operates on\nsequences of extracted mel-spectrograms. Our extensive experiments on the\nchallenging and newly assembled Affect-in-the-wild-2 (Aff-Wild2) dataset verify\nthe superiority of our methods over existing approaches, while by properly\nincorporating all of the aforementioned modules in a network ensemble, we\nmanage to surpass the previous best published recognition scores, in the\nofficial validation set. All the code was implemented using\nPyTorch\\footnote{\\url{https://pytorch.org/}} and is publicly\navailable\\footnote{\\url{https://github.com/PanosAntoniadis/NTUA-ABAW2021}}.",
          "link": "http://arxiv.org/abs/2107.03465",
          "publishedOn": "2021-07-09T01:58:26.124Z",
          "wordCount": 654,
          "title": "An audiovisual and contextual approach for categorical and continuous emotion recognition in-the-wild. (arXiv:2107.03465v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jian Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Houjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>",
          "description": "Pedestrian attribute recognition aims to assign multiple attributes to one\npedestrian image captured by a video surveillance camera. Although numerous\nmethods are proposed and make tremendous progress, we argue that it is time to\nstep back and analyze the status quo of the area. We review and rethink the\nrecent progress from three perspectives. First, given that there is no explicit\nand complete definition of pedestrian attribute recognition, we formally define\nand distinguish pedestrian attribute recognition from other similar tasks.\nSecond, based on the proposed definition, we expose the limitations of the\nexisting datasets, which violate the academic norm and are inconsistent with\nthe essential requirement of practical industry application. Thus, we propose\ntwo datasets, PETA\\textsubscript{$ZS$} and RAP\\textsubscript{$ZS$}, constructed\nfollowing the zero-shot settings on pedestrian identity. In addition, we also\nintroduce several realistic criteria for future pedestrian attribute dataset\nconstruction. Finally, we reimplement existing state-of-the-art methods and\nintroduce a strong baseline method to give reliable evaluations and fair\ncomparisons. Experiments are conducted on four existing datasets and two\nproposed datasets to measure progress on pedestrian attribute recognition.",
          "link": "http://arxiv.org/abs/2107.03576",
          "publishedOn": "2021-07-09T01:58:26.056Z",
          "wordCount": 635,
          "title": "Rethinking of Pedestrian Attribute Recognition: A Reliable Evaluation under Zero-Shot Pedestrian Identity Setting. (arXiv:2107.03576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03904",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liang_S/0/1/0/all/0/1\">Shuang Liang</a>",
          "description": "In this paper, we present a hybrid deep learning framework named CTNet which\ncombines convolutional neural network and transformer together for the\ndetection of COVID-19 via 3D chest CT images. It consists of a CNN feature\nextractor module with SE attention to extract sufficient features from CT\nscans, together with a transformer model to model the discriminative features\nof the 3D CT scans. Compared to previous works, CTNet provides an effective and\nefficient method to perform COVID-19 diagnosis via 3D CT scans with data\nresampling strategy. Advanced results on a large and public benchmarks,\nCOV19-CT-DB database was achieved by the proposed CTNet, over the\nstate-of-the-art baseline approachproposed together with the dataset.",
          "link": "http://arxiv.org/abs/2107.03904",
          "publishedOn": "2021-07-09T01:58:26.034Z",
          "wordCount": 611,
          "title": "A hybrid deep learning framework for Covid-19 detection via 3D Chest CT Images. (arXiv:2107.03904v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oh_G/0/1/0/all/0/1\">Geesung Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1\">Euiseok Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sejoon Lim</a>",
          "description": "Among human affective behavior research, facial expression recognition\nresearch is improving in performance along with the development of deep\nlearning. However, for improved performance, not only past images but also\nfuture images should be used along with corresponding facial images, but there\nare obstacles to the application of this technique to real-time environments.\nIn this paper, we propose the causal affect prediction network (CAPNet), which\nuses only past facial images to predict corresponding affective valence and\narousal. We train CAPNet to learn causal inference between past images and\ncorresponding affective valence and arousal through supervised learning by\npairing the sequence of past images with the current label using the Aff-Wild2\ndataset. We show through experiments that the well-trained CAPNet outperforms\nthe baseline of the second challenge of the Affective Behavior Analysis\nin-the-wild (ABAW2) Competition by predicting affective valence and arousal\nonly with past facial images one-third of a second earlier. Therefore, in\nreal-time application, CAPNet can reliably predict affective valence and\narousal only with past data.",
          "link": "http://arxiv.org/abs/2107.03886",
          "publishedOn": "2021-07-09T01:58:26.022Z",
          "wordCount": 606,
          "title": "Causal affect prediction model using a facial image sequence. (arXiv:2107.03886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_D/0/1/0/all/0/1\">Dige Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>",
          "description": "In practice, the problems encountered in Neural Architecture Search (NAS)\ntraining are not simple problems, but often a series of difficult combinations\n(wrong compensation estimation, curse of dimension, overfitting, high\ncomplexity, etc.). In this paper, we propose a framework to decouple network\nstructure from operator search space, and use two BOHBs to search\nalternatively. Considering that activation function and initialization are also\nimportant parts of neural network, the generalization ability of the model will\nbe affected. We introduce an activation function and an initialization method\ndomain, and add them into the operator search space to form a generalized\nsearch space, so as to improve the generalization ability of the child model.\nWe then trained a GCN-based predictor using feedback from the child model. This\ncan not only improve the search efficiency, but also solve the problem of\ndimension curse. Next, unlike other NAS studies, we used predictors to analyze\nthe stability of different network structures. Finally, we applied our\nframework to neural structure search and achieved significant improvements on\nmultiple datasets.",
          "link": "http://arxiv.org/abs/2103.11820",
          "publishedOn": "2021-07-09T01:58:25.953Z",
          "wordCount": 676,
          "title": "GPNAS: A Neural Network Architecture Search Framework Based on Graphical Predictor. (arXiv:2103.11820v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinhyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1\">Yunze Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>",
          "description": "Point clouds and RGB images are naturally complementary modalities for 3D\nvisual understanding - the former provides sparse but accurate locations of\npoints on objects, while the latter contains dense color and texture\ninformation. Despite this potential for close sensor fusion, many methods train\ntwo models in isolation and use simple feature concatenation to represent 3D\nsensor data. This separated training scheme results in potentially sub-optimal\nperformance and prevents 3D tasks from being used to benefit 2D tasks that are\noften useful on their own. To provide a more integrated approach, we propose a\nnovel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box\nproposals to improve 2D segmentation predictions, which are then used to\nfurther refine the 3D boxes. We show that including a 2D network between two\nstages of 3D modules significantly improves both 2D and 3D task performance.\nMoreover, to prevent the 3D module from over-relying on the overfitted 2D\npredictions, we propose a dual-head 2D segmentation training and inference\nscheme, allowing the 2nd 3D module to learn to interpret imperfect 2D\nsegmentation predictions. Evaluating our model on the challenging SUN RGB-D\ndataset, we improve upon state-of-the-art results of both single modality and\nfusion networks by a large margin ($\\textbf{+3.8}$ mAP@0.5). Code will be\nreleased $\\href{https://github.com/Divadi/MTC_RCNN}{\\text{here.}}$",
          "link": "http://arxiv.org/abs/2107.04013",
          "publishedOn": "2021-07-09T01:58:25.882Z",
          "wordCount": 656,
          "title": "Multi-Modality Task Cascade for 3D Object Detection. (arXiv:2107.04013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1\">Gabriel Tjio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>",
          "description": "Convolutional neural networks may perform poorly when the test and train data\nare from different domains. While this problem can be mitigated by using the\ntarget domain data to align the source and target domain feature\nrepresentations, the target domain data may be unavailable due to privacy\nconcerns. Consequently, there is a need for methods that generalize well\nwithout access to target domain data during training. In this work, we propose\nan adversarial hallucination approach, which combines a class-wise\nhallucination module and a semantic segmentation module. Since the segmentation\nperformance varies across different classes, we design a semantic-conditioned\nstyle hallucination layer to adaptively stylize each class. The classwise\nstylization parameters are generated from the semantic knowledge in the\nsegmentation probability maps of the source domain image. Both modules compete\nadversarially, with the hallucination module generating increasingly\n'difficult' style images to challenge the segmentation module. In response, the\nsegmentation module improves its performance as it is trained with generated\nsamples at an appropriate class-wise difficulty level. Experiments on state of\nthe art domain adaptation work demonstrate the efficacy of our proposed method\nwhen no target domain data are available for training.",
          "link": "http://arxiv.org/abs/2106.04144",
          "publishedOn": "2021-07-09T01:58:25.857Z",
          "wordCount": 666,
          "title": "Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaghani_J/0/1/0/all/0/1\">Javad Khaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanei_Yakhdan_H/0/1/0/all/0/1\">Hossein Ghanei-Yakhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>",
          "description": "A strong visual object tracker nowadays relies on its well-crafted modules,\nwhich typically consist of manually-designed network architectures to deliver\nhigh-quality tracking results. Not surprisingly, the manual design process\nbecomes a particularly challenging barrier, as it demands sufficient prior\nexperience, enormous effort, intuition and perhaps some good luck. Meanwhile,\nneural architecture search has gaining grounds in practical applications such\nas image segmentation, as a promising method in tackling the issue of automated\nsearch of feasible network structures. In this work, we propose a novel\ncell-level differentiable architecture search mechanism to automate the network\ndesign of the tracking module, aiming to adapt backbone features to the\nobjective of a tracking network during offline training. The proposed approach\nis simple, efficient, and with no need to stack a series of modules to\nconstruct a network. Our approach is easy to be incorporated into existing\ntrackers, which is empirically validated using different differentiable\narchitecture search-based methods and tracking objectives. Extensive\nexperimental evaluations demonstrate the superior performance of our approach\nover five commonly-used benchmarks. Meanwhile, our automated searching process\ntakes 41 (18) hours for the second (first) order DARTS method on the\nTrackingNet dataset.",
          "link": "http://arxiv.org/abs/2107.03463",
          "publishedOn": "2021-07-09T01:58:25.837Z",
          "wordCount": 652,
          "title": "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search. (arXiv:2107.03463v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03887",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Savioli_N/0/1/0/all/0/1\">Nicolo Savioli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ORegan_D/0/1/0/all/0/1\">Declan O&#x27;Regan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cook_S/0/1/0/all/0/1\">Stuart Cook</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>",
          "description": "In cardiac magnetic resonance (CMR) imaging, a 3D high-resolution\nsegmentation of the heart is essential for detailed description of its\nanatomical structures. However, due to the limit of acquisition duration and\nrespiratory/cardiac motion, stacks of multi-slice 2D images are acquired in\nclinical routine. The segmentation of these images provides a low-resolution\nrepresentation of cardiac anatomy, which may contain artefacts caused by\nmotion. Here we propose a novel latent optimisation framework that jointly\nperforms motion correction and super resolution for cardiac image\nsegmentations. Given a low-resolution segmentation as input, the framework\naccounts for inter-slice motion in cardiac MR imaging and super-resolves the\ninput into a high-resolution segmentation consistent with input. A multi-view\nloss is incorporated to leverage information from both short-axis view and\nlong-axis view of cardiac imaging. To solve the inverse problem, iterative\noptimisation is performed in a latent space, which ensures the anatomical\nplausibility. This alleviates the need of paired low-resolution and\nhigh-resolution images for supervised learning. Experiments on two cardiac MR\ndatasets show that the proposed framework achieves high performance, comparable\nto state-of-the-art super-resolution approaches and with better cross-domain\ngeneralisability and anatomical plausibility.",
          "link": "http://arxiv.org/abs/2107.03887",
          "publishedOn": "2021-07-09T01:58:25.761Z",
          "wordCount": 665,
          "title": "Joint Motion Correction and Super Resolution for Cardiac Segmentation via Latent Optimisation. (arXiv:2107.03887v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Du Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>",
          "description": "Video transformers have recently emerged as a competitive alternative to 3D\nCNNs for video understanding. However, due to their large number of parameters\nand reduced inductive biases, these models require supervised pretraining on\nlarge-scale image datasets to achieve top performance. In this paper, we\nempirically demonstrate that self-supervised pretraining of video transformers\non video-only datasets can lead to action recognition results that are on par\nor better than those obtained with supervised pretraining on large-scale image\ndatasets, even massive ones such as ImageNet-21K. Since transformer-based\nmodels are effective at capturing dependencies over extended temporal spans, we\npropose a simple learning procedure that forces the model to match a long-term\nview to a short-term view of the same video. Our approach, named Long-Short\nTemporal Contrastive Learning (LSTCL), enables video transformers to learn an\neffective clip-level representation by predicting temporal context captured\nfrom a longer temporal extent. To demonstrate the generality of our findings,\nwe implement and validate our approach under three different self-supervised\ncontrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct\nvideo-transformer architectures, including an improved variant of the Swin\nTransformer augmented with space-time attention. We conduct a thorough ablation\nstudy and show that LSTCL achieves competitive performance on multiple video\nbenchmarks and represents a convincing alternative to supervised image-based\npretraining.",
          "link": "http://arxiv.org/abs/2106.09212",
          "publishedOn": "2021-07-09T01:58:25.751Z",
          "wordCount": 681,
          "title": "Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yikang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhao Zhong</a>",
          "description": "In this paper, we propose a Collaboration of Experts (CoE) framework to pool\ntogether the expertise of multiple networks towards a common aim. Each expert\nis an individual network with expertise on a unique portion of the dataset,\nwhich enhances the collective capacity. Given a sample, an expert is selected\nby the delegator, which simultaneously outputs a rough prediction to support\nearly termination. To fulfill this framework, we propose three modules to impel\neach model to play its role, namely weight generation module (WGM), label\ngeneration module (LGM) and variance calculation module (VCM). Our method\nachieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy\nwith 194M FLOPs. Combined with PWLU activation function and CondConv, CoE\nfurther achieves the accuracy of 80.0% with only 100M FLOPs for the first time.\nMore importantly, our method is hardware friendly and achieves a 3-6x speedup\ncompared with some existing conditional computation approaches.",
          "link": "http://arxiv.org/abs/2107.03815",
          "publishedOn": "2021-07-09T01:58:25.744Z",
          "wordCount": 594,
          "title": "Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zunhu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>",
          "description": "Automatic affective recognition has been an important research topic in human\ncomputer interaction (HCI) area. With recent development of deep learning\ntechniques and large scale in-the-wild annotated datasets, the facial emotion\nanalysis is now aimed at challenges in the real world settings. In this paper,\nwe introduce our submission to the 2nd Affective Behavior Analysis in-the-wild\n(ABAW2) Competition. In dealing with different emotion representations,\nincluding Categorical Emotions (CE), Action Units (AU), and Valence Arousal\n(VA), we propose a multi-task streaming network by a heuristic that the three\nrepresentations are intrinsically associated with each other. Besides, we\nleverage an advanced facial expression embedding as prior knowledge, which is\ncapable of capturing identity-invariant expression features while preserving\nthe expression similarities, to aid the down-streaming recognition tasks. The\nextensive quantitative evaluations as well as ablation studies on the Aff-Wild2\ndataset prove the effectiveness of our proposed prior aided streaming network\napproach.",
          "link": "http://arxiv.org/abs/2107.03708",
          "publishedOn": "2021-07-09T01:58:25.717Z",
          "wordCount": 596,
          "title": "Prior Aided Streaming Network for Multi-task Affective Recognitionat the 2nd ABAW2 Competition. (arXiv:2107.03708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1\">Byeongjoon Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_W/0/1/0/all/0/1\">Wonjun Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">David Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>",
          "description": "Pedestrians are exposed to risk of death or serious injuries on roads,\nespecially unsignalized crosswalks, for a variety of reasons. To date, an\nextensive variety of studies have reported on vision based traffic safety\nsystem. However, many studies required manual inspection of the volumes of\ntraffic video to reliably obtain traffic related objects behavioral factors. In\nthis paper, we propose an automated and simpler system for effectively\nextracting object behavioral features from video sensors deployed on the road.\nWe conduct basic statistical analysis on these features, and show how they can\nbe useful for monitoring the traffic behavior on the road. We confirm the\nfeasibility of the proposed system by applying our prototype to two\nunsignalized crosswalks in Osan city, South Korea. To conclude, we compare\nbehaviors of vehicles and pedestrians in those two areas by simple statistical\nanalysis. This study demonstrates the potential for a network of connected\nvideo sensors to provide actionable data for smart cities to improve pedestrian\nsafety in dangerous road environments.",
          "link": "http://arxiv.org/abs/2107.03554",
          "publishedOn": "2021-07-09T01:58:25.710Z",
          "wordCount": 620,
          "title": "Automated Object Behavioral Feature Extraction for Potential Risk Analysis based on Video Sensor. (arXiv:2107.03554v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asam_M/0/1/0/all/0/1\">Muhammad Asam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1\">Tauseef Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahoora_U/0/1/0/all/0/1\">Umme Zahoora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>",
          "description": "Malicious activities in cyberspace have gone further than simply hacking\nmachines and spreading viruses. It has become a challenge for a nations\nsurvival and hence has evolved to cyber warfare. Malware is a key component of\ncyber-crime, and its analysis is the first line of defence against attack. This\nwork proposes a novel deep boosted hybrid learning-based malware classification\nframework and named as Deep boosted Feature Space-based Malware classification\n(DFS-MC). In the proposed framework, the discrimination power is enhanced by\nfusing the feature spaces of the best performing customized CNN architectures\nmodels and its discrimination by an SVM for classification. The discrimination\ncapacity of the proposed classification framework is assessed by comparing it\nagainst the standard customized CNNs. The customized CNN models are implemented\nin two ways: softmax classifier and deep hybrid learning-based malware\nclassification. In the hybrid learning, Deep features are extracted from\ncustomized CNN architectures and fed into the conventional machine learning\nclassifier to improve the classification performance. We also introduced the\nconcept of transfer learning in a customized CNN architecture based malware\nclassification framework through fine-tuning. The performance of the proposed\nmalware classification approaches are validated on the MalImg malware dataset\nusing the hold-out cross-validation technique. Experimental comparisons were\nconducted by employing innovative, customized CNN, trained from scratch and\nfine-tuning the customized CNN using transfer learning. The proposed\nclassification framework DFS-MC showed improved results, Accuracy: 98.61%,\nF-score: 0.96, Precision: 0.96, and Recall: 0.96.",
          "link": "http://arxiv.org/abs/2107.04008",
          "publishedOn": "2021-07-09T01:58:25.703Z",
          "wordCount": 677,
          "title": "Malware Classification Using Deep Boosted Learning. (arXiv:2107.04008v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1\">Pengxiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Human motion prediction is essential for tasks such as human motion analysis\nand human-robot interactions. Most existing approaches have been proposed to\nrealize motion prediction. However, they ignore an important task, the\nevaluation of the quality of the predicted result. It is far more enough for\ncurrent approaches in actual scenarios because people can't know how to\ninteract with the machine without the evaluation of prediction, and unreliable\npredictions may mislead the machine to harm the human. Hence, we propose an\nuncertainty-aware framework for human motion prediction (UA-HMP). Concretely,\nwe first design an uncertainty-aware predictor through Gaussian modeling to\nachieve the value and the uncertainty of predicted motion. Then, an\nuncertainty-guided learning scheme is proposed to quantitate the uncertainty\nand reduce the negative effect of the noisy samples during optimization for\nbetter performance. Our proposed framework is easily combined with current SOTA\nbaselines to overcome their weakness in uncertainty modeling with slight\nparameters increment. Extensive experiments also show that they can achieve\nbetter performance in both short and long-term predictions in H3.6M, CMU-Mocap.",
          "link": "http://arxiv.org/abs/2107.03575",
          "publishedOn": "2021-07-09T01:58:25.692Z",
          "wordCount": 597,
          "title": "Uncertainty-aware Human Motion Prediction. (arXiv:2107.03575v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhaoyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingfu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>",
          "description": "Crowd counting is critical for numerous video surveillance scenarios. One of\nthe main issues in this task is how to handle the dramatic scale variations of\npedestrians caused by the perspective effect. To address this issue, this paper\nproposes a novel convolution neural network-based crowd counting method, termed\nPerspective-guided Fractional-Dilation Network (PFDNet). By modeling the\ncontinuous scale variations, the proposed PFDNet is able to select the proper\nfractional dilation kernels for adapting to different spatial locations. It\nsignificantly improves the flexibility of the state-of-the-arts that only\nconsider the discrete representative scales. In addition, by avoiding the\nmulti-scale or multi-column architecture that used in other methods, it is\ncomputationally more efficient. In practice, the proposed PFDNet is constructed\nby stacking multiple Perspective-guided Fractional-Dilation Convolutions (PFC)\non a VGG16-BN backbone. By introducing a novel generalized dilation convolution\noperation, the PFC can handle fractional dilation ratios in the spatial domain\nunder the guidance of perspective annotations, achieving continuous scales\nmodeling of pedestrians. To deal with the problem of unavailable perspective\ninformation in some cases, we further introduce an effective perspective\nestimation branch to the proposed PFDNet, which can be trained in either\nsupervised or weakly-supervised setting once the branch has been pre-trained.\nExtensive experiments show that the proposed PFDNet outperforms\nstate-of-the-art methods on ShanghaiTech A, ShanghaiTech B, WorldExpo'10,\nUCF-QNRF, UCF_CC_50 and TRANCOS dataset, achieving MAE 53.8, 6.5, 6.8, 84.3,\n205.8, and 3.06 respectively.",
          "link": "http://arxiv.org/abs/2107.03665",
          "publishedOn": "2021-07-09T01:58:25.685Z",
          "wordCount": 674,
          "title": "Crowd Counting via Perspective-Guided Fractional-Dilation Convolution. (arXiv:2107.03665v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03651",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bar_David_D/0/1/0/all/0/1\">Daniel Bar-David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bar_David_L/0/1/0/all/0/1\">Laura Bar-David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shapira_Y/0/1/0/all/0/1\">Yinon Shapira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leibu_R/0/1/0/all/0/1\">Rina Leibu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dori_D/0/1/0/all/0/1\">Dalia Dori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schneor_R/0/1/0/all/0/1\">Ronit Schneor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischer_A/0/1/0/all/0/1\">Anath Fischer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soudry_S/0/1/0/all/0/1\">Shiri Soudry</a>",
          "description": "To explore the clinical validity of elastic deformation of optical coherence\ntomography (OCT) images for data augmentation in the development of\ndeep-learning model for detection of diabetic macular edema (DME).",
          "link": "http://arxiv.org/abs/2107.03651",
          "publishedOn": "2021-07-09T01:58:25.650Z",
          "wordCount": 508,
          "title": "Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?. (arXiv:2107.03651v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopru_B/0/1/0/all/0/1\">Berkay K&#xf6;pr&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erzin_E/0/1/0/all/0/1\">Engin Erzin</a>",
          "description": "Increasing volume of user-generated human-centric video content and their\napplications, such as video retrieval and browsing, require compact\nrepresentations that are addressed by the video summarization literature.\nCurrent supervised studies formulate video summarization as a\nsequence-to-sequence learning problem and the existing solutions often neglect\nthe surge of human-centric view, which inherently contains affective content.\nIn this study, we investigate the affective-information enriched supervised\nvideo summarization task for human-centric videos. First, we train a visual\ninput-driven state-of-the-art continuous emotion recognition model (CER-NET) on\nthe RECOLA dataset to estimate emotional attributes. Then, we integrate the\nestimated emotional attributes and the high-level representations from the\nCER-NET with the visual information to define the proposed affective video\nsummarization architectures (AVSUM). In addition, we investigate the use of\nattention to improve the AVSUM architectures and propose two new architectures\nbased on temporal attention (TA-AVSUM) and spatial attention (SA-AVSUM). We\nconduct video summarization experiments on the TvSum database. The proposed\nAVSUM-GRU architecture with an early fusion of high level GRU embeddings and\nthe temporal attention based TA-AVSUM architecture attain competitive video\nsummarization performances by bringing strong performance improvements for the\nhuman-centric videos compared to the state-of-the-art in terms of F-score and\nself-defined face recall metrics.",
          "link": "http://arxiv.org/abs/2107.03783",
          "publishedOn": "2021-07-09T01:58:25.642Z",
          "wordCount": 642,
          "title": "Use of Affective Visual Information for Summarization of Human-Centric Videos. (arXiv:2107.03783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.05244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial robustness of deep neural networks has been actively\ninvestigated. However, most existing defense approaches are limited to a\nspecific type of adversarial perturbations. Specifically, they often fail to\noffer resistance to multiple attack types simultaneously, i.e., they lack\nmulti-perturbation robustness. Furthermore, compared to image recognition\nproblems, the adversarial robustness of video recognition models is relatively\nunexplored. While several studies have proposed how to generate adversarial\nvideos, only a handful of approaches about the defense strategies have been\npublished in the literature. In this paper, we propose one of the first defense\nstrategies against multiple types of adversarial videos for video recognition.\nThe proposed method, referred to as MultiBN, performs adversarial training on\nmultiple adversarial video types using multiple independent batch normalization\n(BN) layers with a learning-based BN selection module. With a multiple BN\nstructure, each BN brach is responsible for learning the distribution of a\nsingle perturbation type and thus provides more precise distribution\nestimations. This mechanism benefits dealing with multiple perturbation types.\nThe BN selection module detects the attack type of an input video and sends it\nto the corresponding BN branch, making MultiBN fully automatic and allow\nend-to-end training. Compared to present adversarial training approaches, the\nproposed MultiBN exhibits stronger multi-perturbation robustness against\ndifferent and even unforeseen adversarial video types, ranging from Lp-bounded\nattacks and physically realizable attacks. This holds true on different\ndatasets and target models. Moreover, we conduct an extensive analysis to study\nthe properties of the multiple BN structure.",
          "link": "http://arxiv.org/abs/2009.05244",
          "publishedOn": "2021-07-09T01:58:25.621Z",
          "wordCount": 710,
          "title": "Defending Against Multiple and Unforeseen Adversarial Videos. (arXiv:2009.05244v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Long Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wangbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Conventional salient object detection models cannot differentiate the\nimportance of different salient objects. Recently, two works have been proposed\nto detect saliency ranking by assigning different degrees of saliency to\ndifferent objects. However, one of these models cannot differentiate object\ninstances and the other focuses more on sequential attention shift order\ninference. In this paper, we investigate a practical problem setting that\nrequires simultaneously segment salient instances and infer their relative\nsaliency rank order. We present a novel unified model as the first end-to-end\nsolution, where an improved Mask R-CNN is first used to segment salient\ninstances and a saliency ranking branch is then added to infer the relative\nsaliency. For relative saliency ranking, we build a new graph reasoning module\nby combining four graphs to incorporate the instance interaction relation,\nlocal contrast, global contrast, and a high-level semantic prior, respectively.\nA novel loss function is also proposed to effectively train the saliency\nranking branch. Besides, a new dataset and an evaluation metric are proposed\nfor this task, aiming at pushing forward this field of research. Finally,\nexperimental results demonstrate that our proposed model is more effective than\nprevious methods. We also show an example of its practical usage on adaptive\nimage retargeting.",
          "link": "http://arxiv.org/abs/2107.03824",
          "publishedOn": "2021-07-09T01:58:25.607Z",
          "wordCount": 643,
          "title": "Instance-Level Relative Saliency Ranking with Graph Reasoning. (arXiv:2107.03824v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jeffrey Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Serena Yeung</a>",
          "description": "Creating representations of shapes that are invari-ant to isometric or\nalmost-isometric transforma-tions has long been an area of interest in shape\nanal-ysis, since enforcing invariance allows the learningof more effective and\nrobust shape representations.Most existing invariant shape representations\narehandcrafted, and previous work on learning shaperepresentations do not focus\non producing invariantrepresentations. To solve the problem of\nlearningunsupervised invariant shape representations, weuse contrastive\nlearning, which produces discrimi-native representations through learning\ninvarianceto user-specified data augmentations. To producerepresentations that\nare specifically isometry andalmost-isometry invariant, we propose new\ndataaugmentations that randomly sample these transfor-mations. We show\nexperimentally that our methodoutperforms previous unsupervised learning\nap-proaches in both effectiveness and robustness.",
          "link": "http://arxiv.org/abs/2107.03552",
          "publishedOn": "2021-07-09T01:58:25.592Z",
          "wordCount": 541,
          "title": "Staying in Shape: Learning Invariant Shape Representations using Contrastive Learning. (arXiv:2107.03552v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wangyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from the human's visual and intuitive perspective. We\ntake the first step to bridge the gap by proposing a deep learning-based\ntechnique to automatically classify road networks into four classes on a visual\nbasis. The method is implemented by generating an image of the street network\n(Colored Road Hierarchy Diagram), which we introduce in this paper, and\nclassifying it using a deep convolutional neural network (ResNet-34). The model\nachieves an overall classification accuracy of 0.875. Nine cities around the\nworld are selected as the study areas with their road networks acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: we apply\nour method in a case study of urban vitality prediction. An advanced tree-based\nregression model (LightGBM) is for the first time designated to establish the\nrelationship between morphological indices and vitality indicators. The effect\nof road network classification is found to be small but positively associated\nwith urban vitality. This work expands the toolkit of quantitative urban\nmorphology study with new techniques, supporting further studies in the future.",
          "link": "http://arxiv.org/abs/2105.09908",
          "publishedOn": "2021-07-09T01:58:25.584Z",
          "wordCount": 726,
          "title": "Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zipeng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>",
          "description": "Successful real-world deployment of legged robots would require them to adapt\nin real-time to unseen scenarios like changing terrains, changing payloads,\nwear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to\nsolve this problem of real-time online adaptation in quadruped robots. RMA\nconsists of two components: a base policy and an adaptation module. The\ncombination of these components enables the robot to adapt to novel situations\nin fractions of a second. RMA is trained completely in simulation without using\nany domain knowledge like reference trajectories or predefined foot trajectory\ngenerators and is deployed on the A1 robot without any fine-tuning. We train\nRMA on a varied terrain generator using bioenergetics-inspired rewards and\ndeploy it on a variety of difficult terrains including rocky, slippery,\ndeformable surfaces in environments with grass, long vegetation, concrete,\npebbles, stairs, sand, etc. RMA shows state-of-the-art performance across\ndiverse real-world as well as simulation experiments. Video results at\nhttps://ashish-kmr.github.io/rma-legged-robots/",
          "link": "http://arxiv.org/abs/2107.04034",
          "publishedOn": "2021-07-09T01:58:25.577Z",
          "wordCount": 606,
          "title": "RMA: Rapid Motor Adaptation for Legged Robots. (arXiv:2107.04034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_R/0/1/0/all/0/1\">Robin Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_G/0/1/0/all/0/1\">Guillaume Michel</a>",
          "description": "Pruning seeks to design lightweight architectures by removing redundant\nweights in overparameterized networks. Most of the existing techniques first\nremove structured sub-networks (filters, channels,...) and then fine-tune the\nresulting networks to maintain a high accuracy. However, removing a whole\nstructure is a strong topological prior and recovering the accuracy, with\nfine-tuning, is highly cumbersome. In this paper, we introduce an \"end-to-end\"\nlightweight network design that achieves training and pruning simultaneously\nwithout fine-tuning. The design principle of our method relies on\nreparametrization that learns not only the weights but also the topological\nstructure of the lightweight sub-network. This reparametrization acts as a\nprior (or regularizer) that defines pruning masks implicitly from the weights\nof the underlying network, without increasing the number of training\nparameters. Sparsity is induced with a budget loss that provides an accurate\npruning. Extensive experiments conducted on the CIFAR10 and the TinyImageNet\ndatasets, using standard architectures (namely Conv4, VGG19 and ResNet18), show\ncompelling results without fine-tuning.",
          "link": "http://arxiv.org/abs/2107.03909",
          "publishedOn": "2021-07-09T01:58:25.561Z",
          "wordCount": 598,
          "title": "Weight Reparametrization for Budget-Aware Network Pruning. (arXiv:2107.03909v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lofqvist_M/0/1/0/all/0/1\">Martina Lofqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1\">Jos&#xe9; Cano</a>",
          "description": "There is a proliferation in the number of satellites launched each year,\nresulting in downlinking of terabytes of data each day. The data received by\nground stations is often unprocessed, making this an expensive process\nconsidering the large data sizes and that not all of the data is useful. This,\ncoupled with the increasing demand for real-time data processing, has led to a\ngrowing need for on-orbit processing solutions. In this work, we investigate\nthe performance of CNN-based object detectors on constrained devices by\napplying different image compression techniques to satellite data. We examine\nthe capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;\nlow-power, high-performance computers, with integrated GPUs, small enough to\nfit on-board a nanosatellite. We take a closer look at object detection\nnetworks, including the Single Shot MultiBox Detector (SSD) and Region-based\nFully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a\nLarge Scale Dataset for Object Detection in Aerial Images. The performance is\nmeasured in terms of execution time, memory consumption, and accuracy, and are\ncompared against a baseline containing a server with two powerful GPUs. The\nresults show that by applying image compression techniques, we are able to\nimprove the execution time and memory consumption, achieving a fully runnable\ndataset. A lossless compression technique achieves roughly a 10% reduction in\nexecution time and about a 3% reduction in memory consumption, with no impact\non the accuracy. While a lossy compression technique improves the execution\ntime by up to 144% and the memory consumption is reduced by as much as 97%.\nHowever, it has a significant impact on accuracy, varying depending on the\ncompression ratio. Thus the application and ratio of these compression\ntechniques may differ depending on the required level of accuracy for a\nparticular task.",
          "link": "http://arxiv.org/abs/2107.03774",
          "publishedOn": "2021-07-09T01:58:25.507Z",
          "wordCount": 777,
          "title": "Optimizing Data Processing in Space for Object Detection in Satellite Imagery. (arXiv:2107.03774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Guerrero_C/0/1/0/all/0/1\">Carmina P&#xe9;rez-Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_A/0/1/0/all/0/1\">Adriana Palacios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_C/0/1/0/all/0/1\">Christian Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1\">Miguel Gonzalez-Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_Morales_L/0/1/0/all/0/1\">Luis Eduardo Falc&#xf3;n-Morales</a>",
          "description": "Risk assessment is relevant in any workplace, however there is a degree of\nunpredictability when dealing with flammable or hazardous materials so that\ndetection of fire accidents by itself may not be enough. An example of this is\nthe impingement of jet fires, where the heat fluxes of the flame could reach\nnearby equipment and dramatically increase the probability of a domino effect\nwith catastrophic results. Because of this, the characterization of such fire\naccidents is important from a risk management point of view. One such\ncharacterization would be the segmentation of different radiation zones within\nthe flame, so this paper presents an exploratory research regarding several\ntraditional computer vision and Deep Learning segmentation approaches to solve\nthis specific problem. A data set of propane jet fires is used to train and\nevaluate the different approaches and given the difference in the distribution\nof the zones and background of the images, different loss functions, that seek\nto alleviate data imbalance, are also explored. Additionally, different metrics\nare correlated to a manual ranking performed by experts to make an evaluation\nthat closely resembles the expert's criteria. The Hausdorff Distance and\nAdjsted Random Index were the metrics with the highest correlation and the best\nresults were obtained from the UNet architecture with a Weighted Cross-Entropy\nLoss. These results can be used in future research to extract more geometric\ninformation from the segmentation masks or could even be implemented on other\ntypes of fire accidents.",
          "link": "http://arxiv.org/abs/2107.03461",
          "publishedOn": "2021-07-09T01:58:25.403Z",
          "wordCount": 701,
          "title": "Comparing ML based Segmentation Models on Jet Fire Radiation Zone. (arXiv:2107.03461v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>",
          "description": "As the data scale grows, deep recognition models often suffer from\nlong-tailed data distributions due to the heavy imbalanced sample number across\ncategories. Indeed, real-world data usually exhibit some similarity relation\namong different categories (e.g., pigeons and sparrows), called category\nsimilarity in this work. It is doubly difficult when the imbalance occurs\nbetween such categories with similar appearances. However, existing solutions\nmainly focus on the sample number to re-balance data distribution. In this\nwork, we systematically investigate the essence of the long-tailed problem from\na unified perspective. Specifically, we demonstrate that long-tailed\nrecognition suffers from both sample number and category similarity.\nIntuitively, using a toy example, we first show that sample number is not the\nunique influence factor for performance dropping of long-tailed recognition.\nTheoretically, we demonstrate that (1) category similarity, as an inevitable\nfactor, would also influence the model learning under long-tailed distribution\nvia similar samples, (2) using more discriminative representation methods\n(e.g., self-supervised learning) for similarity reduction, the classifier bias\ncan be further alleviated with greatly improved performance. Extensive\nexperiments on several long-tailed datasets verify the rationality of our\ntheoretical analysis, and show that based on existing state-of-the-arts\n(SOTAs), the performance could be further improved by similarity reduction. Our\ninvestigations highlight the essence behind the long-tailed problem, and claim\nseveral feasible directions for future work.",
          "link": "http://arxiv.org/abs/2107.03758",
          "publishedOn": "2021-07-09T01:58:25.295Z",
          "wordCount": 653,
          "title": "Investigate the Essence of Long-Tailed Recognition from a Unified Perspective. (arXiv:2107.03758v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuaiqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "Optical flow estimation is a fundamental problem of computer vision and has\nmany applications in the fields of robot learning and autonomous driving. This\npaper reveals novel geometric laws of optical flow based on the insight and\ndetailed definition of non-occlusion. Then, two novel loss functions are\nproposed for the unsupervised learning of optical flow based on the geometric\nlaws of non-occlusion. Specifically, after the occlusion part of the images are\nmasked, the flowing process of pixels is carefully considered and geometric\nconstraints are conducted based on the geometric laws of optical flow. First,\nneighboring pixels in the first frame will not intersect during the pixel\ndisplacement to the second frame. Secondly, when the cluster containing\nadjacent four pixels in the first frame moves to the second frame, no other\npixels will flow into the quadrilateral formed by them. According to the two\ngeometrical constraints, the optical flow non-intersection loss and the optical\nflow non-blocking loss in the non-occlusion regions are proposed. Two loss\nfunctions punish the irregular and inexact optical flows in the non-occlusion\nregions. The experiments on datasets demonstrated that the proposed\nunsupervised losses of optical flow based on the geometric laws in\nnon-occlusion regions make the estimated optical flow more refined in detail,\nand improve the performance of unsupervised learning of optical flow. In\naddition, the experiments training on synthetic data and evaluating on real\ndata show that the generalization ability of optical flow network is improved\nby our proposed unsupervised approach.",
          "link": "http://arxiv.org/abs/2107.03610",
          "publishedOn": "2021-07-09T01:58:25.283Z",
          "wordCount": 690,
          "title": "NccFlow: Unsupervised Learning of Optical Flow With Non-occlusion from Geometry. (arXiv:2107.03610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1\">Ricard Durall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1\">Stanislav Frolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>",
          "description": "Transformer models have recently attracted much interest from computer vision\nresearchers and have since been successfully employed for several problems\ntraditionally addressed with convolutional neural networks. At the same time,\nimage synthesis using generative adversarial networks (GANs) has drastically\nimproved over the last few years. The recently proposed TransGAN is the first\nGAN using only transformer-based architectures and achieves competitive results\nwhen compared to convolutional GANs. However, since transformers are\ndata-hungry architectures, TransGAN requires data augmentation, an auxiliary\nsuper-resolution task during training, and a masking prior to guide the\nself-attention mechanism. In this paper, we study the combination of a\ntransformer-based generator and convolutional discriminator and successfully\nremove the need of the aforementioned required design choices. We evaluate our\napproach by conducting a benchmark of well-known CNN discriminators, ablate the\nsize of the transformer-based generator, and show that combining both\narchitectural elements into a hybrid model leads to better results.\nFurthermore, we investigate the frequency spectrum properties of generated\nimages and observe that our model retains the benefits of an attention based\ngenerator.",
          "link": "http://arxiv.org/abs/2105.10189",
          "publishedOn": "2021-07-09T01:58:25.268Z",
          "wordCount": 633,
          "title": "Combining Transformer Generators with Convolutional Discriminators. (arXiv:2105.10189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ningyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jiayan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yaojun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiangjian Xiao</a>",
          "description": "Angular measurements is essential to make a resonable treatment for Hallux\nvalgus (HV), a common forefoot deformity. However, it still depends on manual\nlabeling and measurement, which is time-consuming and sometimes unreliable.\nAutomating this process is a thing of concern. However, it lack of dataset and\nthe keypoints based method which made a great success in pose estimation is not\nsuitable for this field.To solve the problems, we made a dataset and developed\nan algorithm based on deep learning and linear regression. It shows great\nfitting ability to the ground truth.",
          "link": "http://arxiv.org/abs/2107.03640",
          "publishedOn": "2021-07-09T01:58:25.202Z",
          "wordCount": 552,
          "title": "A Dataset and Method for Hallux Valgus Angle Estimation Based on Deep Learing. (arXiv:2107.03640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03225",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yanwen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1\">Luyang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Huangjing Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "The novel coronavirus disease 2019 (COVID-19) characterized by atypical\npneumonia has caused millions of deaths worldwide. Automatically segmenting\nlesions from chest Computed Tomography (CT) is a promising way to assist\ndoctors in COVID-19 screening, treatment planning, and follow-up monitoring.\nHowever, voxel-wise annotations are extremely expert-demanding and scarce,\nespecially when it comes to novel diseases, while an abundance of unlabeled\ndata could be available. To tackle the challenge of limited annotations, in\nthis paper, we propose an uncertainty-guided dual-consistency learning network\n(UDC-Net) for semi-supervised COVID-19 lesion segmentation from CT images.\nSpecifically, we present a dual-consistency learning scheme that simultaneously\nimposes image transformation equivalence and feature perturbation invariance to\neffectively harness the knowledge from unlabeled data. We then quantify the\nsegmentation uncertainty in two forms and employ them together to guide the\nconsistency regularization for more reliable unsupervised learning. Extensive\nexperiments showed that our proposed UDC-Net improves the fully supervised\nmethod by 6.3% in Dice and outperforms other competitive semi-supervised\napproaches by significant margins, demonstrating high potential in real-world\nclinical practice.",
          "link": "http://arxiv.org/abs/2104.03225",
          "publishedOn": "2021-07-09T01:58:25.194Z",
          "wordCount": 704,
          "title": "Dual-Consistency Semi-Supervised Learning with Uncertainty Quantification for COVID-19 Lesion Segmentation from CT Images. (arXiv:2104.03225v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Junha Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.",
          "link": "http://arxiv.org/abs/2107.03438",
          "publishedOn": "2021-07-09T01:58:25.164Z",
          "wordCount": 614,
          "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1\">Lie Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yelin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiufen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>",
          "description": "In medical image segmentation, it is difficult to mark ambiguous areas\naccurately with binary masks, especially when dealing with small lesions.\nTherefore, it is a challenge for radiologists to reach a consensus by using\nbinary masks under the condition of multiple annotations. However, these areas\nmay contain anatomical structures that are conducive to diagnosis. Uncertainty\nis introduced to study these situations. Nevertheless, the uncertainty is\nusually measured by the variances between predictions in a multiple trial way.\nIt is not intuitive, and there is no exact correspondence in the image.\nInspired by image matting, we introduce matting as a soft segmentation method\nand a new perspective to deal with and represent uncertain regions into medical\nscenes, namely medical matting. More specifically, because there is no\navailable medical matting dataset, we first labeled two medical datasets with\nalpha matte. Secondly, the matting method applied to the natural image is not\nsuitable for the medical scene, so we propose a new architecture to generate\nbinary masks and alpha matte in a row. Thirdly, the uncertainty map is\nintroduced to highlight the ambiguous regions from the binary results and\nimprove the matting performance. Evaluated on these datasets, the proposed\nmodel outperformed state-of-the-art matting algorithms by a large margin, and\nalpha matte is proved to be a more efficient labeling form than a binary mask.",
          "link": "http://arxiv.org/abs/2106.09887",
          "publishedOn": "2021-07-09T01:58:25.128Z",
          "wordCount": 710,
          "title": "Medical Matting: A New Perspective on Medical Segmentation with Uncertainty. (arXiv:2106.09887v2 [cs.CV] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.06131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kammerer_L/0/1/0/all/0/1\">Lukas Kammerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1\">Michael Kommenda</a>",
          "description": "We describe a method for the identification of models for dynamical systems\nfrom observational data. The method is based on the concept of symbolic\nregression and uses genetic programming to evolve a system of ordinary\ndifferential equations (ODE). The novelty is that we add a step of\ngradient-based optimization of the ODE parameters. For this we calculate the\nsensitivities of the solution to the initial value problem (IVP) using\nautomatic differentiation. The proposed approach is tested on a set of 19\nproblem instances taken from the literature which includes datasets from\nsimulated systems as well as datasets captured from mechanical systems. We find\nthat gradient-based optimization of parameters improves predictive accuracy of\nthe models. The best results are obtained when we first fit the individual\nequations to the numeric differences and then subsequently fine-tune the\nidentified parameter values by fitting the IVP solution to the observed\nvariable values.",
          "link": "http://arxiv.org/abs/2107.06131",
          "publishedOn": "2021-07-14T01:41:52.131Z",
          "wordCount": 612,
          "title": "Identification of Dynamical Systems using Symbolic Regression. (arXiv:2107.06131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1\">Tolulope A. Odetola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Syed Rafay Hasan</a>",
          "description": "Security of inference phase deployment of Convolutional neural network (CNN)\ninto resource constrained embedded systems (e.g. low end FPGAs) is a growing\nresearch area. Using secure practices, third party FPGA designers can be\nprovided with no knowledge of initial and final classification layers. In this\nwork, we demonstrate that hardware intrinsic attack (HIA) in such a \"secure\"\ndesign is still possible. Proposed HIA is inserted inside mathematical\noperations of individual layers of CNN, which propagates erroneous operations\nin all the subsequent CNN layers that lead to misclassification. The attack is\nnon-periodic and completely random, hence it becomes difficult to detect. Five\ndifferent attack scenarios with respect to each CNN layer are designed and\nevaluated based on the overhead resources and the rate of triggering in\ncomparison to the original implementation. Our results for two CNN\narchitectures show that in all the attack scenarios, additional latency is\nnegligible (<0.61%), increment in DSP, LUT, FF is also less than 2.36%. Three\nattack scenarios do not require any additional BRAM resources, while in two\nscenarios BRAM increases, which compensates with the corresponding decrease in\nFF and LUTs. To the authors' best knowledge this work is the first to address\nthe hardware intrinsic CNN attack with the attacker does not have knowledge of\nthe full CNN.",
          "link": "http://arxiv.org/abs/2103.09327",
          "publishedOn": "2021-07-14T01:41:52.125Z",
          "wordCount": 702,
          "title": "SoWaF: Shuffling of Weights and Feature Maps: A Novel Hardware Intrinsic Attack (HIA) on Convolutional Neural Network (CNN). (arXiv:2103.09327v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09907",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liao_L/0/1/0/all/0/1\">Luofeng Liao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fu_Z/0/1/0/all/0/1\">Zuyue Fu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1\">Mladen Kolar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>",
          "description": "In offline reinforcement learning (RL) an optimal policy is learnt solely\nfrom a priori collected observational data. However, in observational data,\nactions are often confounded by unobserved variables. Instrumental variables\n(IVs), in the context of RL, are the variables whose influence on the state\nvariables are all mediated through the action. When a valid instrument is\npresent, we can recover the confounded transition dynamics through\nobservational data. We study a confounded Markov decision process where the\ntransition dynamics admit an additive nonlinear functional form. Using IVs, we\nderive a conditional moment restriction (CMR) through which we can identify\ntransition dynamics based on observational data. We propose a provably\nefficient IV-aided Value Iteration (IVVI) algorithm based on a primal-dual\nreformulation of CMR. To the best of our knowledge, this is the first provably\nefficient algorithm for instrument-aided offline RL.",
          "link": "http://arxiv.org/abs/2102.09907",
          "publishedOn": "2021-07-14T01:41:52.107Z",
          "wordCount": 598,
          "title": "Instrumental Variable Value Iteration for Causal Offline Reinforcement Learning. (arXiv:2102.09907v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.00773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>",
          "description": "This paper analyzes challenges in cloze-style reading comprehension on\nmultiparty dialogue and suggests two new tasks for more comprehensive\npredictions of personal entities in daily conversations. We first demonstrate\nthat there are substantial limitations to the evaluation methods of previous\nwork, namely that randomized assignment of samples to training and test data\nsubstantially decreases the complexity of cloze-style reading comprehension.\nAccording to our analysis, replacing the random data split with a chronological\ndata split reduces test accuracy on previous single-variable passage completion\ntask from 72\\% to 34\\%, that leaves much more room to improve. Our proposed\ntasks extend the previous single-variable passage completion task by replacing\nmore character mentions with variables. Several deep learning models are\ndeveloped to validate these three tasks. A thorough error analysis is provided\nto understand the challenges and guide the future direction of this research.",
          "link": "http://arxiv.org/abs/1911.00773",
          "publishedOn": "2021-07-14T01:41:52.100Z",
          "wordCount": 611,
          "title": "Design and Challenges of Cloze-Style Reading Comprehension Tasks on Multiparty Dialogue. (arXiv:1911.00773v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1\">Mathieu Blondel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berthet_Q/0/1/0/all/0/1\">Quentin Berthet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frostig_R/0/1/0/all/0/1\">Roy Frostig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyer_S/0/1/0/all/0/1\">Stephan Hoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llinares_Lopez_F/0/1/0/all/0/1\">Felipe Llinares-L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1\">Fabian Pedregosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vert_J/0/1/0/all/0/1\">Jean-Philippe Vert</a>",
          "description": "Automatic differentiation (autodiff) has revolutionized machine learning. It\nallows expressing complex computations by composing elementary ones in creative\nways and removes the burden of computing their derivatives by hand. More\nrecently, differentiation of optimization problem solutions has attracted\nwidespread attention with applications such as optimization as a layer, and in\nbi-level problems such as hyper-parameter optimization and meta-learning.\nHowever, the formulas for these derivatives often involve case-by-case tedious\nmathematical derivations. In this paper, we propose a unified, efficient and\nmodular approach for implicit differentiation of optimization problems. In our\napproach, the user defines (in Python in the case of our implementation) a\nfunction $F$ capturing the optimality conditions of the problem to be\ndifferentiated. Once this is done, we leverage autodiff of $F$ and implicit\ndifferentiation to automatically differentiate the optimization problem. Our\napproach thus combines the benefits of implicit differentiation and autodiff.\nIt is efficient as it can be added on top of any state-of-the-art solver and\nmodular as the optimality condition specification is decoupled from the\nimplicit differentiation mechanism. We show that seemingly simple principles\nallow to recover many recently proposed implicit differentiation methods and\ncreate new ones easily. We demonstrate the ease of formulating and solving\nbi-level optimization problems using our framework. We also showcase an\napplication to the sensitivity analysis of molecular dynamics.",
          "link": "http://arxiv.org/abs/2105.15183",
          "publishedOn": "2021-07-14T01:41:52.043Z",
          "wordCount": 699,
          "title": "Efficient and Modular Implicit Differentiation. (arXiv:2105.15183v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00138",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grodecki_K/0/1/0/all/0/1\">Kajetan Grodecki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Killekar_A/0/1/0/all/0/1\">Aditya Killekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1\">Andrew Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cadet_S/0/1/0/all/0/1\">Sebastien Cadet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McElhinney_P/0/1/0/all/0/1\">Priscilla McElhinney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razipour_A/0/1/0/all/0/1\">Aryabod Razipour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Cato Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pressman_B/0/1/0/all/0/1\">Barry D. Pressman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Julien_P/0/1/0/all/0/1\">Peter Julien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simon_J/0/1/0/all/0/1\">Judit Simon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maurovich_Horvat_P/0/1/0/all/0/1\">Pal Maurovich-Horvat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaibazzi_N/0/1/0/all/0/1\">Nicola Gaibazzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thakur_U/0/1/0/all/0/1\">Udit Thakur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mancini_E/0/1/0/all/0/1\">Elisabetta Mancini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agalbato_C/0/1/0/all/0/1\">Cecilia Agalbato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munechika_J/0/1/0/all/0/1\">Jiro Munechika</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matsumoto_H/0/1/0/all/0/1\">Hidenari Matsumoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mene_R/0/1/0/all/0/1\">Roberto Men&#xe8;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parati_G/0/1/0/all/0/1\">Gianfranco Parati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cernigliaro_F/0/1/0/all/0/1\">Franco Cernigliaro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nerlekar_N/0/1/0/all/0/1\">Nitesh Nerlekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torlasco_C/0/1/0/all/0/1\">Camilla Torlasco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pontone_G/0/1/0/all/0/1\">Gianluca Pontone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_D/0/1/0/all/0/1\">Damini Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slomka_P/0/1/0/all/0/1\">Piotr J. Slomka</a>",
          "description": "Quantitative lung measures derived from computed tomography (CT) have been\ndemonstrated to improve prognostication in coronavirus disease (COVID-19)\npatients, but are not part of the clinical routine since required manual\nsegmentation of lung lesions is prohibitively time-consuming. We propose a new\nfully automated deep learning framework for rapid quantification and\ndifferentiation between lung lesions in COVID-19 pneumonia from both contrast\nand non-contrast CT images using convolutional Long Short-Term Memory\n(ConvLSTM) networks. Utilizing the expert annotations, model training was\nperformed 5 times with separate hold-out sets using 5-fold cross-validation to\nsegment ground-glass opacity and high opacity (including consolidation and\npleural effusion). The performance of the method was evaluated on CT data sets\nfrom 197 patients with positive reverse transcription polymerase chain reaction\ntest result for SARS-CoV-2. Strong agreement between expert manual and\nautomatic segmentation was obtained for lung lesions with a Dice score\ncoefficient of 0.876 $\\pm$ 0.005; excellent correlations of 0.978 and 0.981 for\nground-glass opacity and high opacity volumes. In the external validation set\nof 67 patients, there was dice score coefficient of 0.767 $\\pm$ 0.009 as well\nas excellent correlations of 0.989 and 0.996 for ground-glass opacity and high\nopacity volumes. Computations for a CT scan comprising 120 slices were\nperformed under 2 seconds on a personal computer equipped with NVIDIA Titan RTX\ngraphics processing unit. Therefore, our deep learning-based method allows\nrapid fully-automated quantitative measurement of pneumonia burden from CT and\nmay generate results with an accuracy similar to the expert readers.",
          "link": "http://arxiv.org/abs/2104.00138",
          "publishedOn": "2021-07-14T01:41:52.030Z",
          "wordCount": 824,
          "title": "Rapid quantification of COVID-19 pneumonia burden from computed tomography with convolutional LSTM networks. (arXiv:2104.00138v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yunwen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenhuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1\">Yiming Ying</a>",
          "description": "Many machine learning problems can be formulated as minimax problems such as\nGenerative Adversarial Networks (GANs), AUC maximization and robust estimation,\nto mention but a few. A substantial amount of studies are devoted to studying\nthe convergence behavior of their stochastic gradient-type algorithms. In\ncontrast, there is relatively little work on their generalization, i.e., how\nthe learning models built from training examples would behave on test examples.\nIn this paper, we provide a comprehensive generalization analysis of stochastic\ngradient methods for minimax problems under both convex-concave and\nnonconvex-nonconcave cases through the lens of algorithmic stability. We\nestablish a quantitative connection between stability and several\ngeneralization measures both in expectation and with high probability. For the\nconvex-concave setting, our stability analysis shows that stochastic gradient\ndescent ascent attains optimal generalization bounds for both smooth and\nnonsmooth minimax problems. We also establish generalization bounds for both\nweakly-convex-weakly-concave and gradient-dominated problems.",
          "link": "http://arxiv.org/abs/2105.03793",
          "publishedOn": "2021-07-14T01:41:52.022Z",
          "wordCount": 623,
          "title": "Stability and Generalization of Stochastic Gradient Methods for Minimax Problems. (arXiv:2105.03793v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1\">Maxim Rakhuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We introduce T-Basis, a novel concept for a compact representation of a set\nof tensors, each of an arbitrary shape, which is often seen in Neural Networks.\nEach of the tensors in the set is modeled using Tensor Rings, though the\nconcept applies to other Tensor Networks. Owing its name to the T-shape of\nnodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally\nshaped three-dimensional tensors, used to represent Tensor Ring nodes. Such\nrepresentation allows us to parameterize the tensor set with a small number of\nparameters (coefficients of the T-Basis tensors), scaling logarithmically with\neach tensor's size in the set and linearly with the dimensionality of T-Basis.\nWe evaluate the proposed approach on the task of neural network compression and\ndemonstrate that it reaches high compression rates at acceptable performance\ndrops. Finally, we analyze memory and operation requirements of the compressed\nnetworks and conclude that T-Basis networks are equally well suited for\ntraining and inference in resource-constrained environments and usage on the\nedge devices.",
          "link": "http://arxiv.org/abs/2007.06631",
          "publishedOn": "2021-07-14T01:41:52.014Z",
          "wordCount": 650,
          "title": "T-Basis: a Compact Representation for Neural Networks. (arXiv:2007.06631v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lemkhenter_A/0/1/0/all/0/1\">Abdelhak Lemkhenter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1\">Adam Bielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_A/0/1/0/all/0/1\">Alp Eren Sari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "We introduce Kernel Density Discrimination GAN (KDD GAN), a novel method for\ngenerative adversarial learning. KDD GAN formulates the training as a\nlikelihood ratio optimization problem where the data distributions are written\nexplicitly via (local) Kernel Density Estimates (KDE). This is inspired by the\nrecent progress in contrastive learning and its relation to KDE. We define the\nKDEs directly in feature space and forgo the requirement of invertibility of\nthe kernel feature mappings. In our approach, features are no longer optimized\nfor linear separability, as in the original GAN formulation, but for the more\ngeneral discrimination of distributions in the feature space. We analyze the\ngradient of our loss with respect to the feature representation and show that\nit is better behaved than that of the original hinge loss. We perform\nexperiments with the proposed KDE-based loss, used either as a training loss or\na regularization term, on both CIFAR10 and scaled versions of ImageNet. We use\nBigGAN/SA-GAN as a backbone and baseline, since our focus is not to design the\narchitecture of the networks. We show a boost in the quality of generated\nsamples with respect to FID from 10% to 40% compared to the baseline. Code will\nbe made available.",
          "link": "http://arxiv.org/abs/2107.06197",
          "publishedOn": "2021-07-14T01:41:51.995Z",
          "wordCount": 638,
          "title": "Generative Adversarial Learning via Kernel Density Discrimination. (arXiv:2107.06197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Daniel Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oort_C/0/1/0/all/0/1\">Colin Van Oort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1\">Jonathan Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wshah_S/0/1/0/all/0/1\">Safwan Wshah</a>",
          "description": "Geo-localizing static objects from street images is challenging but also very\nimportant for road asset mapping and autonomous driving. In this paper we\npresent a two-stage framework that detects and geolocalizes traffic signs from\nlow frame rate street videos. Our proposed system uses a modified version of\nRetinaNet (GPS-RetinaNet), which predicts a positional offset for each sign\nrelative to the camera, in addition to performing the standard classification\nand bounding box regression. Candidate sign detections from GPS-RetinaNet are\ncondensed into geolocalized signs by our custom tracker, which consists of a\nlearned metric network and a variant of the Hungarian Algorithm. Our metric\nnetwork estimates the similarity between pairs of detections, then the\nHungarian Algorithm matches detections across images using the similarity\nscores provided by the metric network. Our models were trained using an updated\nversion of the ARTS dataset, which contains 25,544 images and 47.589 sign\nannotations ~\\cite{arts}. The proposed dataset covers a diverse set of\nenvironments gathered from a broad selection of roads. Each annotaiton contains\na sign class label, its geospatial location, an assembly label, a side of road\nindicator, and unique identifiers that aid in the evaluation. This dataset will\nsupport future progress in the field, and the proposed system demonstrates how\nto take advantage of some of the unique characteristics of a realistic\ngeolocalization dataset.",
          "link": "http://arxiv.org/abs/2107.06257",
          "publishedOn": "2021-07-14T01:41:51.983Z",
          "wordCount": 677,
          "title": "Object Tracking and Geo-localization from Street Images. (arXiv:2107.06257v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anari_N/0/1/0/all/0/1\">Nima Anari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thuy-Duong Vuong</a>",
          "description": "We show a connection between sampling and optimization on discrete domains.\nFor a family of distributions $\\mu$ defined on size $k$ subsets of a ground set\nof elements that is closed under external fields, we show that rapid mixing of\nnatural local random walks implies the existence of simple approximation\nalgorithms to find $\\max \\mu(\\cdot)$. More precisely we show that if\n(multi-step) down-up random walks have spectral gap at least inverse\npolynomially large in $k$, then (multi-step) local search can find $\\max\n\\mu(\\cdot)$ within a factor of $k^{O(k)}$. As the main application of our\nresult, we show a simple nearly-optimal $k^{O(k)}$-factor approximation\nalgorithm for MAP inference on nonsymmetric DPPs. This is the first nontrivial\nmultiplicative approximation for finding the largest size $k$ principal minor\nof a square (not-necessarily-symmetric) matrix $L$ with $L+L^\\intercal\\succeq\n0$.\n\nWe establish the connection between sampling and optimization by showing that\nan exchange inequality, a concept rooted in discrete convex analysis, can be\nderived from fast mixing of local random walks. We further connect exchange\ninequalities with composable core-sets for optimization, generalizing recent\nresults on composable core-sets for DPP maximization to arbitrary distributions\nthat satisfy either the strongly Rayleigh property or that have a log-concave\ngenerating polynomial.",
          "link": "http://arxiv.org/abs/2102.05347",
          "publishedOn": "2021-07-14T01:41:51.976Z",
          "wordCount": 677,
          "title": "From Sampling to Optimization on Discrete Domains withApplications to Determinant Maximization. (arXiv:2102.05347v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07671",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Simovic_P/0/1/0/all/0/1\">Petra Posedel &#x160;imovi&#x107;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Horvatic_D/0/1/0/all/0/1\">Davor Horvatic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_E/0/1/0/all/0/1\">Edward W. Sun</a>",
          "description": "Using big data to analyze consumer behavior can provide effective\ndecision-making tools for preventing customer attrition (churn) in customer\nrelationship management (CRM). Focusing on a CRM dataset with several different\ncategories of factors that impact customer heterogeneity (i.e., usage of\nself-care service channels, duration of service, and responsiveness to\nmarketing actions), we provide new predictive analytics of customer churn rate\nbased on a machine learning method that enhances the classification of logistic\nregression by adding a mixed penalty term. The proposed penalized logistic\nregression can prevent overfitting when dealing with big data and minimize the\nloss function when balancing the cost from the median (absolute value) and mean\n(squared value) regularization. We show the analytical properties of the\nproposed method and its computational advantage in this research. In addition,\nwe investigate the performance of the proposed method with a CRM data set (that\nhas a large number of features) under different settings by efficiently\neliminating the disturbance of (1) least important features and (2) sensitivity\nfrom the minority (churn) class. Our empirical results confirm the expected\nperformance of the proposed method in full compliance with the common\nclassification criteria (i.e., accuracy, precision, and recall) for evaluating\nmachine learning methods.",
          "link": "http://arxiv.org/abs/2105.07671",
          "publishedOn": "2021-07-14T01:41:51.968Z",
          "wordCount": 694,
          "title": "Classifying variety of customer's online engagement for churn prediction with mixed-penalty logistic regression. (arXiv:2105.07671v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Bowen Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eismann_S/0/1/0/all/0/1\">Stephan Eismann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_P/0/1/0/all/0/1\">Pratham N. Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Ron O. Dror</a>",
          "description": "Representing and reasoning about 3D structures of macromolecules is emerging\nas a distinct challenge in machine learning. Here, we extend recent work on\ngeometric vector perceptrons and apply equivariant graph neural networks to a\nwide range of tasks from structural biology. Our method outperforms all\nreference architectures on three out of eight tasks in the ATOM3D benchmark, is\ntied for first on two others, and is competitive with equivariant networks\nusing higher-order representations and spherical harmonic convolutions. In\naddition, we demonstrate that transfer learning can further improve performance\non certain downstream tasks. Code is available at\nhttps://github.com/drorlab/gvp-pytorch.",
          "link": "http://arxiv.org/abs/2106.03843",
          "publishedOn": "2021-07-14T01:41:51.961Z",
          "wordCount": 572,
          "title": "Equivariant Graph Neural Networks for 3D Macromolecular Structure. (arXiv:2106.03843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rossenbach_N/0/1/0/all/0/1\">Nick Rossenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilmes_B/0/1/0/all/0/1\">Benedikt Hilmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "Recent publications on automatic-speech-recognition (ASR) have a strong focus\non attention encoder-decoder (AED) architectures which tend to suffer from\nover-fitting in low resource scenarios. One solution to tackle this issue is to\ngenerate synthetic data with a trained text-to-speech system (TTS) if\nadditional text is available. This was successfully applied in many\npublications with AED systems, but only very limited in the context of other\nASR architectures. We investigate the effect of varying pre-processing, the\nspeaker embedding and input encoding of the TTS system w.r.t. the effectiveness\nof the synthesized data for AED-ASR training. Additionally, we also consider\ninternal language model subtraction for the first time, resulting in up to 38%\nrelative improvement. We compare the AED results to a state-of-the-art hybrid\nASR system, a monophone based system using\nconnectionist-temporal-classification (CTC) and a monotonic transducer based\nsystem. We show that for the later systems the addition of synthetic data has\nno relevant effect, but they still outperform the AED systems on\nLibriSpeech-100h. We achieve a final word-error-rate of 3.3%/10.0% with a\nhybrid system on the clean/noisy test-sets, surpassing any previous\nstate-of-the-art systems on Librispeech-100h that do not include unlabeled\naudio data.",
          "link": "http://arxiv.org/abs/2104.05379",
          "publishedOn": "2021-07-14T01:41:51.943Z",
          "wordCount": 681,
          "title": "Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures. (arXiv:2104.05379v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheinker_A/0/1/0/all/0/1\">Alexander Scheinker</a>",
          "description": "Machine learning (ML) tools such as encoder-decoder convolutional neural\nnetworks (CNN) can represent incredibly complex nonlinear functions which map\nbetween combinations of images and scalars. For example, CNNs can be used to\nmap combinations of accelerator parameters and images which are 2D projections\nof the 6D phase space distributions of charged particle beams as they are\ntransported between various particle accelerator locations. Despite their\nstrengths, applying ML to time-varying systems, or systems with shifting\ndistributions, is an open problem, especially for large systems for which\ncollecting new data for re-training is impractical or interrupts operations.\nParticle accelerators are one example of large time-varying systems for which\ncollecting detailed training data requires lengthy dedicated beam measurements\nwhich may no longer be available during regular operations. We present a\nrecently developed method of adaptive ML for time-varying systems. Our approach\nis to map very high (N>100k) dimensional inputs (a combination of scalar\nparameters and images) into the low dimensional (N~2) latent space at the\noutput of the encoder section of an encoder-decoder CNN. We then actively tune\nthe low dimensional latent space-based representation of complex system\ndynamics by the addition of an adaptively tuned feedback vector directly before\nthe decoder sections builds back up to our image-based high-dimensional phase\nspace density representations. This method allows us to learn correlations\nwithin and to quickly tune the characteristics of incredibly high parameter\nsystems and to track their evolution in real time based on feedback without\nmassive new data sets for re-training.",
          "link": "http://arxiv.org/abs/2107.06207",
          "publishedOn": "2021-07-14T01:41:51.936Z",
          "wordCount": 689,
          "title": "Adaptive Machine Learning for Time-Varying Systems: Low Dimensional Latent Space Tuning. (arXiv:2107.06207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05767",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Toledo_C/0/1/0/all/0/1\">Carmen Melo Toledo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bassedon_G/0/1/0/all/0/1\">Guilherme Mendes Bassedon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ferreira_J/0/1/0/all/0/1\">Jonathan Batista Ferreira</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gianvechio_L/0/1/0/all/0/1\">Lucka de Godoy Gianvechio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guatimosim_C/0/1/0/all/0/1\">Carlos Guatimosim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1\">Renato Vicente</a>",
          "description": "Student's grade retention is a key issue faced by many education systems,\nespecially those in developing countries. In this paper, we seek to gauge the\nrelevance of students' personality traits in predicting grade retention in\nBrazil. For that, we used data collected in 2012 and 2017, in the city of\nSertaozinho, countryside of the state of Sao Paulo, Brazil. The surveys taken\nin Sertaozinho included several socioeconomic questions, standardized tests,\nand a personality test. Moreover, students were in grades 4, 5, and 6 in 2012.\nOur approach was based on training machine learning models on the surveys' data\nto predict grade retention between 2012 and 2017 using information from 2012 or\nbefore, and then using some strategies to quantify personality traits'\npredictive power. We concluded that, besides proving to be fairly better than a\nrandom classifier when isolated, personality traits contribute to prediction\neven when using socioeconomic variables and standardized tests results.",
          "link": "http://arxiv.org/abs/2107.05767",
          "publishedOn": "2021-07-14T01:41:51.922Z",
          "wordCount": 606,
          "title": "Effects of personality traits in predicting grade retention of Brazilian students. (arXiv:2107.05767v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harris_K/0/1/0/all/0/1\">Keegan Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Daniel Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1\">Logan Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1\">Hoda Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>",
          "description": "Machine Learning algorithms often prompt individuals to strategically modify\ntheir observable attributes to receive more favorable predictions. As a result,\nthe distribution the predictive model is trained on may differ from the one it\noperates on in deployment. While such distribution shifts, in general, hinder\naccurate predictions, our work identifies a unique opportunity associated with\nshifts due to strategic responses: We show that we can use strategic responses\neffectively to recover causal relationships between the observable features and\noutcomes we wish to predict. More specifically, we study a game-theoretic model\nin which a principal deploys a sequence of models to predict an outcome of\ninterest (e.g., college GPA) for a sequence of strategic agents (e.g., college\napplicants). In response, strategic agents invest efforts and modify their\nfeatures for better predictions. In such settings, unobserved confounding\nvariables can influence both an agent's observable features (e.g., high school\nrecords) and outcomes. Therefore, standard regression methods generally produce\nbiased estimators. In order to address this issue, our work establishes a novel\nconnection between strategic responses to machine learning models and\ninstrumental variable (IV) regression, by observing that the sequence of\ndeployed models can be viewed as an instrument that affects agents' observable\nfeatures but does not directly influence their outcomes. Therefore, two-stage\nleast squares (2SLS) regression can recover the causal relationships between\nobservable features and outcomes. Beyond causal recovery, we can build on our\n2SLS method to address two additional relevant optimization objectives: agent\noutcome maximization and predictive risk minimization. Finally, our numerical\nsimulations on semi-synthetic data show that our methods significantly\noutperform OLS regression in causal relationship estimation.",
          "link": "http://arxiv.org/abs/2107.05762",
          "publishedOn": "2021-07-14T01:41:51.912Z",
          "wordCount": 703,
          "title": "Strategic Instrumental Variable Regression: Recovering Causal Relationships From Strategic Responses. (arXiv:2107.05762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zappella_G/0/1/0/all/0/1\">Giovanni Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salinas_D/0/1/0/all/0/1\">David Salinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1\">C&#xe9;dric Archambeau</a>",
          "description": "In this work we consider the problem of repeated hyperparameter and neural\narchitecture search (HNAS). We propose an extension of Successive Halving that\nis able to leverage information gained in previous HNAS problems with the goal\nof saving computational resources. We empirically demonstrate that our solution\nis able to drastically decrease costs while maintaining accuracy and being\nrobust to negative transfer. Our method is significantly simpler than competing\ntransfer learning approaches, setting a new baseline for transfer learning in\nHNAS.",
          "link": "http://arxiv.org/abs/2103.16111",
          "publishedOn": "2021-07-14T01:41:51.905Z",
          "wordCount": 550,
          "title": "A resource-efficient method for repeated HPO and NAS problems. (arXiv:2103.16111v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.08962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaza_K/0/1/0/all/0/1\">Kesav Kaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meshram_R/0/1/0/all/0/1\">Rahul Meshram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1\">Varun Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merchant_S/0/1/0/all/0/1\">S.N.Merchant</a>",
          "description": "This paper studies a class of constrained restless multi-armed bandits\n(CRMAB). The constraints are in the form of time varying set of actions (set of\navailable arms). This variation can be either stochastic or semi-deterministic.\nGiven a set of arms, a fixed number of them can be chosen to be played in each\ndecision interval. The play of each arm yields a state dependent reward. The\ncurrent states of arms are partially observable through binary feedback signals\nfrom arms that are played. The current availability of arms is fully\nobservable. The objective is to maximize long term cumulative reward. The\nuncertainty about future availability of arms along with partial state\ninformation makes this objective challenging. Applications for CRMAB abound in\nthe domain of cyber-physical systems. First, this optimization problem is\nanalyzed using Whittle's index policy. To this end, a constrained restless\nsingle-armed bandit is studied. It is shown to admit a threshold-type optimal\npolicy and is also indexable. An algorithm to compute Whittle's index is\npresented. An alternate solution method with lower complexity is also presented\nin the form of an online rollout policy. Further, upper bounds on the value\nfunction are derived in order to estimate the degree of sub-optimality of\nvarious solutions. The simulation study compares the performance of Whittle's\nindex, online rollout, myopic and modified Whittle's index policies.",
          "link": "http://arxiv.org/abs/1904.08962",
          "publishedOn": "2021-07-14T01:41:51.886Z",
          "wordCount": 713,
          "title": "Constrained Restless Bandits for Dynamic Scheduling in Cyber-Physical Systems. (arXiv:1904.08962v4 [cs.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10643",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1\">Jie Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gallego_B/0/1/0/all/0/1\">Blanca Gallego</a>",
          "description": "Causal inference in longitudinal observational health data often requires the\naccurate estimation of treatment effects on time-to-event outcomes in the\npresence of time-varying covariates. To tackle this sequential treatment effect\nestimation problem, we have developed a causal dynamic survival (CDS) model\nthat uses the potential outcomes framework with the recurrent sub-networks with\nrandom seed ensembles to estimate the difference in survival curves of its\nconfidence interval. Using simulated survival datasets, the CDS model has shown\ngood causal effect estimation performance across scenarios of sample dimension,\nevent rate, confounding and overlapping. However, increasing the sample size is\nnot effective to alleviate the adverse impact from high level of confounding.\nIn two large clinical cohort studies, our model identified the expected\nconditional average treatment effect and detected individual effect\nheterogeneity over time and patient subgroups. CDS provides individualised\nabsolute treatment effect estimations to improve clinical decisions.",
          "link": "http://arxiv.org/abs/2101.10643",
          "publishedOn": "2021-07-14T01:41:51.880Z",
          "wordCount": 652,
          "title": "Casual Inference using Deep Bayesian Dynamic Survival Model (CDS). (arXiv:2101.10643v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1\">Idan Achituve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1\">Aviv Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yemini_Y/0/1/0/all/0/1\">Yochai Yemini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>",
          "description": "Gaussian processes (GPs) are non-parametric, flexible, models that work well\nin many tasks. Combining GPs with deep learning methods via deep kernel\nlearning (DKL) is especially compelling due to the strong representational\npower induced by the network. However, inference in GPs, whether with or\nwithout DKL, can be computationally challenging on large datasets. Here, we\npropose GP-Tree, a novel method for multi-class classification with Gaussian\nprocesses and DKL. We develop a tree-based hierarchical model in which each\ninternal node of the tree fits a GP to the data using the P\\'olya Gamma\naugmentation scheme. As a result, our method scales well with both the number\nof classes and data size. We demonstrate the effectiveness of our method\nagainst other Gaussian process training baselines, and we show how our general\nGP approach achieves improved accuracy on standard incremental few-shot\nlearning benchmarks.",
          "link": "http://arxiv.org/abs/2102.07868",
          "publishedOn": "2021-07-14T01:41:51.873Z",
          "wordCount": 618,
          "title": "GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning. (arXiv:2102.07868v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.10516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruizhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Bo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1\">Greg Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehrmann_A/0/1/0/all/0/1\">Andreas Lehrmann</a>",
          "description": "Normalizing flows transform a simple base distribution into a complex target\ndistribution and have proved to be powerful models for data generation and\ndensity estimation. In this work, we propose a novel type of normalizing flow\ndriven by a differential deformation of the Wiener process. As a result, we\nobtain a rich time series model whose observable process inherits many of the\nappealing properties of its base process, such as efficient computation of\nlikelihoods and marginals. Furthermore, our continuous treatment provides a\nnatural framework for irregular time series with an independent arrival\nprocess, including straightforward interpolation. We illustrate the desirable\nproperties of the proposed model on popular stochastic processes and\ndemonstrate its superior flexibility to variational RNN and latent ODE\nbaselines in a series of experiments on synthetic and real-world data.",
          "link": "http://arxiv.org/abs/2002.10516",
          "publishedOn": "2021-07-14T01:41:51.866Z",
          "wordCount": 624,
          "title": "Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows. (arXiv:2002.10516v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duval_A/0/1/0/all/0/1\">Alexandre Duval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1\">Fragkiskos D. Malliaros</a>",
          "description": "Graph Neural Networks (GNNs) achieve significant performance for various\nlearning tasks on geometric data due to the incorporation of graph structure\ninto the learning of node representations, which renders their comprehension\nchallenging. In this paper, we first propose a unified framework satisfied by\nmost existing GNN explainers. Then, we introduce GraphSVX, a post hoc local\nmodel-agnostic explanation method specifically designed for GNNs. GraphSVX is a\ndecomposition technique that captures the \"fair\" contribution of each feature\nand node towards the explained prediction by constructing a surrogate model on\na perturbed dataset. It extends to graphs and ultimately provides as\nexplanation the Shapley Values from game theory. Experiments on real-world and\nsynthetic datasets demonstrate that GraphSVX achieves state-of-the-art\nperformance compared to baseline models while presenting core theoretical and\nhuman-centric properties.",
          "link": "http://arxiv.org/abs/2104.10482",
          "publishedOn": "2021-07-14T01:41:51.858Z",
          "wordCount": 586,
          "title": "GraphSVX: Shapley Value Explanations for Graph Neural Networks. (arXiv:2104.10482v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguntola_I/0/1/0/all/0/1\">Ini Oguntola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_D/0/1/0/all/0/1\">Dana Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1\">Katia Sycara</a>",
          "description": "When developing AI systems that interact with humans, it is essential to\ndesign both a system that can understand humans, and a system that humans can\nunderstand. Most deep network based agent-modeling approaches are 1) not\ninterpretable and 2) only model external behavior, ignoring internal mental\nstates, which potentially limits their capability for assistance,\ninterventions, discovering false beliefs, etc. To this end, we develop an\ninterpretable modular neural framework for modeling the intentions of other\nobserved entities. We demonstrate the efficacy of our approach with experiments\non data from human participants on a search and rescue task in Minecraft, and\nshow that incorporating interpretability can significantly increase predictive\nperformance under the right conditions.",
          "link": "http://arxiv.org/abs/2104.02938",
          "publishedOn": "2021-07-14T01:41:51.840Z",
          "wordCount": 574,
          "title": "Deep Interpretable Models of Theory of Mind. (arXiv:2104.02938v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Busk_J/0/1/0/all/0/1\">Jonas Busk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorgensen_P/0/1/0/all/0/1\">Peter Bj&#xf8;rn J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmik_A/0/1/0/all/0/1\">Arghya Bhowmik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Mikkel N. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegge_T/0/1/0/all/0/1\">Tejs Vegge</a>",
          "description": "Data-driven methods based on machine learning have the potential to\naccelerate analysis of atomic structures. However, machine learning models can\nproduce overconfident predictions and it is therefore crucial to detect and\nhandle uncertainty carefully. Here, we extend a message passing neural network\ndesigned specifically for predicting properties of molecules and materials with\na calibrated probabilistic predictive distribution. The method presented in\nthis paper differs from the previous work by considering both aleatoric and\nepistemic uncertainty in a unified framework, and by re-calibrating the\npredictive distribution on unseen data. Through computer experiments, we show\nthat our approach results in accurate models for predicting molecular formation\nenergies with calibrated uncertainty in and out of the training data\ndistribution on two public molecular benchmark datasets, QM9 and PC9. The\nproposed method provides a general framework for training and evaluating neural\nnetwork ensemble models that are able to produce accurate predictions of\nproperties of molecules with calibrated uncertainty.",
          "link": "http://arxiv.org/abs/2107.06068",
          "publishedOn": "2021-07-14T01:41:51.833Z",
          "wordCount": 605,
          "title": "Calibrated Uncertainty for Molecular Property Prediction using Ensembles of Message Passing Neural Networks. (arXiv:2107.06068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1\">Mineto Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1\">Hiroki Matsutani</a>",
          "description": "Currently there has been increasing demand for real-time training on\nresource-limited IoT devices such as smart sensors, which realizes standalone\nonline adaptation for streaming data without data transfers to remote servers.\nOS-ELM (Online Sequential Extreme Learning Machine) has been one of promising\nneural-network-based online algorithms for on-chip learning because it can\nperform online training at low computational cost and is easy to implement as a\ndigital circuit. Existing OS-ELM digital circuits employ fixed-point data\nformat and the bit-widths are often manually tuned, however, this may cause\noverflow or underflow which can lead to unexpected behavior of the circuit. For\non-chip learning systems, an overflow/underflow-free design has a great impact\nsince online training is continuously performed and the intervals of\nintermediate variables will dynamically change as time goes by. In this paper,\nwe propose an overflow/underflow-free bit-width optimization method for\nfixed-point digital circuits of OS-ELM. Experimental results show that our\nmethod realizes overflow/underflow-free OS-ELM digital circuits with 1.0x -\n1.5x more area cost compared to the baseline simulation method where overflow\nor underflow can happen.",
          "link": "http://arxiv.org/abs/2103.09791",
          "publishedOn": "2021-07-14T01:41:51.827Z",
          "wordCount": 636,
          "title": "An Overflow/Underflow-Free Fixed-Point Bit-Width Optimization Method for OS-ELM Digital Circuit. (arXiv:2103.09791v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sah_R/0/1/0/all/0/1\">Ramesh Kumar Sah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1\">Hassan Ghasemzadeh</a>",
          "description": "Stress detection and monitoring is an active area of research with important\nimplications for the personal, professional, and social health of an\nindividual. Current approaches for affective state classification use\ntraditional machine learning algorithms with features computed from multiple\nsensor modalities. These methods are data-intensive and rely on hand-crafted\nfeatures which impede the practical applicability of these sensor systems in\ndaily lives. To overcome these shortcomings, we propose a novel Convolutional\nNeural Network (CNN) based stress detection and classification framework\nwithout any feature computation using data from only one sensor modality. Our\nmethod is competitive and outperforms current state-of-the-art techniques and\nachieves a classification accuracy of $92.85\\%$ and an $f1$ score of $0.89$.\nThrough our leave-one-subject-out analysis, we also show the importance of\npersonalizing stress models.",
          "link": "http://arxiv.org/abs/2107.05666",
          "publishedOn": "2021-07-14T01:41:51.820Z",
          "wordCount": 577,
          "title": "Stress Classification and Personalization: Getting the most out of the least. (arXiv:2107.05666v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Dibya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahme_J/0/1/0/all/0/1\">Jad Rahme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aviral Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amy Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_R/0/1/0/all/0/1\">Ryan P. Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Generalization is a central challenge for the deployment of reinforcement\nlearning (RL) systems in the real world. In this paper, we show that the\nsequential structure of the RL problem necessitates new approaches to\ngeneralization beyond the well-studied techniques used in supervised learning.\nWhile supervised learning methods can generalize effectively without explicitly\naccounting for epistemic uncertainty, we show that, perhaps surprisingly, this\nis not the case in RL. We show that generalization to unseen test conditions\nfrom a limited number of training conditions induces implicit partial\nobservability, effectively turning even fully-observed MDPs into POMDPs.\nInformed by this observation, we recast the problem of generalization in RL as\nsolving the induced partially observed Markov decision process, which we call\nthe epistemic POMDP. We demonstrate the failure modes of algorithms that do not\nappropriately handle this partial observability, and suggest a simple\nensemble-based technique for approximately solving the partially observed\nproblem. Empirically, we demonstrate that our simple algorithm derived from the\nepistemic POMDP achieves significant gains in generalization over current\nmethods on the Procgen benchmark suite.",
          "link": "http://arxiv.org/abs/2107.06277",
          "publishedOn": "2021-07-14T01:41:51.813Z",
          "wordCount": 632,
          "title": "Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability. (arXiv:2107.06277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiusheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1\">Nikhil Bhendawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Ting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1\">Desheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1\">Bingyu Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>",
          "description": "Transformer-based models have made tremendous impacts in natural language\ngeneration. However the inference speed is a bottleneck due to large model size\nand intensive computing involved in auto-regressive decoding process. We\ndevelop FastSeq framework to accelerate sequence generation without accuracy\nloss. The proposed optimization techniques include an attention cache\noptimization, an efficient algorithm for detecting repeated n-grams, and an\nasynchronous generation pipeline with parallel I/O. These optimizations are\ngeneral enough to be applicable to Transformer-based models (e.g., T5, GPT2,\nand UniLM). Our benchmark results on a set of widely used and diverse models\ndemonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use\nwith a simple one-line code change. The source code is available at\nhttps://github.com/microsoft/fastseq.",
          "link": "http://arxiv.org/abs/2106.04718",
          "publishedOn": "2021-07-14T01:41:51.795Z",
          "wordCount": 595,
          "title": "FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06181",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gecgel_S/0/1/0/all/0/1\">Selen Gecgel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurt_G/0/1/0/all/0/1\">Gunes Karabulut Kurt</a>",
          "description": "Towards sixth-generation networks (6G), satellite communication systems,\nespecially based on Low Earth Orbit (LEO) networks, become promising due to\ntheir unique and comprehensive capabilities. These advantages are accompanied\nby a variety of challenges such as security vulnerabilities, management of\nhybrid systems, and high mobility. In this paper, firstly, a security\ndeficiency in the physical layer is addressed with a conceptual framework,\nconsidering the cyber-physical nature of the satellite systems, highlighting\nthe potential attacks. Secondly, a learning-driven detection scheme is\nproposed, and the lightweight convolutional neural network (CNN) is designed.\nThe performance of the designed CNN architecture is compared with a prevalent\nmachine learning algorithm, support vector machine (SVM). The results show that\ndeficiency attacks against the satellite systems can be detected by employing\nthe proposed scheme.",
          "link": "http://arxiv.org/abs/2107.06181",
          "publishedOn": "2021-07-14T01:41:51.788Z",
          "wordCount": 574,
          "title": "Intermittent Jamming against Telemetry and Telecommand of Satellite Systems and A Learning-driven Detection Strategy. (arXiv:2107.06181v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1\">Syed Muhammad Arsalan Bashir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mahrukh Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yilong Niu</a>",
          "description": "Image super-resolution (SR) is one of the vital image processing methods that\nimprove the resolution of an image in the field of computer vision. In the last\ntwo decades, significant progress has been made in the field of\nsuper-resolution, especially by utilizing deep learning methods. This survey is\nan effort to provide a detailed survey of recent progress in single-image\nsuper-resolution in the perspective of deep learning while also informing about\nthe initial classical methods used for image super-resolution. The survey\nclassifies the image SR methods into four categories, i.e., classical methods,\nsupervised learning-based methods, unsupervised learning-based methods, and\ndomain-specific SR methods. We also introduce the problem of SR to provide\nintuition about image quality metrics, available reference datasets, and SR\nchallenges. Deep learning-based approaches of SR are evaluated using a\nreference dataset. Some of the reviewed state-of-the-art image SR methods\ninclude the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN),\nmultiscale residual network (MSRN), meta residual dense network (Meta-RDN),\nrecurrent back-projection network (RBPN), second-order attention network (SAN),\nSR feedback network (SRFBN) and the wavelet-based residual attention network\n(WRAN). Finally, this survey is concluded with future directions and trends in\nSR and open problems in SR to be addressed by the researchers.",
          "link": "http://arxiv.org/abs/2102.09351",
          "publishedOn": "2021-07-14T01:41:51.781Z",
          "wordCount": 699,
          "title": "A Comprehensive Review of Deep Learning-based Single Image Super-resolution. (arXiv:2102.09351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>",
          "description": "The SARS-CoV-2 virus and COVID-19 disease have posed unprecedented and\noverwhelming challenges and opportunities to data and domain-driven modeling.\nThis paper makes a comprehensive review of the challenges, tasks, methods, gaps\nand opportunities on modeling COVID-19 problems and data. It constructs a\nresearch landscape of COVID-19 modeling, and further categorizes, compares and\ndiscusses the related work on modeling COVID-19 epidemic transmission processes\nand dynamics, case identification and tracing, infection diagnosis and trends,\nmedical treatments, non-pharmaceutical intervention effect, drug and vaccine\ndevelopment, psychological, economic and social impact, and misinformation,\netc. The modeling methods involve mathematical and statistical models,\ndomain-driven modeling by epidemiological compartmental models, medical and\nbiomedical analysis, data-driven learning by shallow and deep machine learning,\nsimulation systems, social science methods, and hybrid methods.",
          "link": "http://arxiv.org/abs/2104.12556",
          "publishedOn": "2021-07-14T01:41:51.774Z",
          "wordCount": 626,
          "title": "COVID-19 Modeling: A Review. (arXiv:2104.12556v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01874",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bramburger_J/0/1/0/all/0/1\">Jason J. Bramburger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Brunton_S/0/1/0/all/0/1\">Steven L. Brunton</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kutz_J/0/1/0/all/0/1\">J. Nathan Kutz</a>",
          "description": "Despite many of the most common chaotic dynamical systems being continuous in\ntime, it is through discrete time mappings that much of the understanding of\nchaos is formed. Henri Poincar\\'e first made this connection by tracking\nconsecutive iterations of the continuous flow with a lower-dimensional,\ntransverse subspace. The mapping that iterates the dynamics through consecutive\nintersections of the flow with the subspace is now referred to as a Poincar\\'e\nmap, and it is the primary method available for interpreting and classifying\nchaotic dynamics. Unfortunately, in all but the simplest systems, an explicit\nform for such a mapping remains outstanding. This work proposes a method for\nobtaining explicit Poincar\\'e mappings by using deep learning to construct an\ninvertible coordinate transformation into a conjugate representation where the\ndynamics are governed by a relatively simple chaotic mapping. The invertible\nchange of variable is based on an autoencoder, which allows for dimensionality\nreduction, and has the advantage of classifying chaotic systems using the\nequivalence relation of topological conjugacies. Indeed, the enforcement of\ntopological conjugacies is the critical neural network regularization for\nlearning the coordinate and dynamics pairing. We provide expository\napplications of the method to low-dimensional systems such as the R\\\"ossler and\nLorenz systems, while also demonstrating the utility of the method on\ninfinite-dimensional systems, such as the Kuramoto--Sivashinsky equation.",
          "link": "http://arxiv.org/abs/2104.01874",
          "publishedOn": "2021-07-14T01:41:51.767Z",
          "wordCount": 662,
          "title": "Deep Learning of Conjugate Mappings. (arXiv:2104.01874v2 [math.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bressan_M/0/1/0/all/0/1\">Marco Bressan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1\">Nicol&#xf2; Cesa-Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1\">Silvio Lattanzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudice_A/0/1/0/all/0/1\">Andrea Paudice</a>",
          "description": "We investigate the problem of exact cluster recovery using oracle queries.\nPrevious results show that clusters in Euclidean spaces that are convex and\nseparated with a margin can be reconstructed exactly using only $O(\\log n)$\nsame-cluster queries, where $n$ is the number of input points. In this work, we\nstudy this problem in the more challenging non-convex setting. We introduce a\nstructural characterization of clusters, called $(\\beta,\\gamma)$-convexity,\nthat can be applied to any finite set of points equipped with a metric (or even\na semimetric, as the triangle inequality is not needed). Using\n$(\\beta,\\gamma)$-convexity, we can translate natural density properties of\nclusters (which include, for instance, clusters that are strongly non-convex in\n$\\mathbb{R}^d$) into a graph-theoretic notion of convexity. By exploiting this\nconvexity notion, we design a deterministic algorithm that recovers\n$(\\beta,\\gamma)$-convex clusters using $O(k^2 \\log n + k^2\n(6/\\beta\\gamma)^{dens(X)})$ same-cluster queries, where $k$ is the number of\nclusters and $dens(X)$ is the density dimension of the semimetric. We show that\nan exponential dependence on the density dimension is necessary, and we also\nshow that, if we are allowed to make $O(k^2 + k\\log n)$ additional queries to a\n\"cluster separation\" oracle, then we can recover clusters that have different\nand arbitrary scales, even when the scale of each cluster is unknown.",
          "link": "http://arxiv.org/abs/2102.00504",
          "publishedOn": "2021-07-14T01:41:51.747Z",
          "wordCount": 694,
          "title": "Exact Recovery of Clusters in Finite Metric Spaces Using Oracle Queries. (arXiv:2102.00504v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao-Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "Modern keyboards allow a musician to play multiple instruments at the same\ntime by assigning zones -- fixed pitch ranges of the keyboard -- to different\ninstruments. In this paper, we aim to further extend this idea and examine the\nfeasibility of automatic instrumentation -- dynamically assigning instruments\nto notes in solo music during performance. In addition to the online,\nreal-time-capable setting for performative use cases, automatic instrumentation\ncan also find applications in assistive composing tools in an offline setting.\nDue to the lack of paired data of original solo music and their full\narrangements, we approach automatic instrumentation by learning to separate\nparts (e.g., voices, instruments and tracks) from their mixture in symbolic\nmultitrack music, assuming that the mixture is to be played on a keyboard. We\nframe the task of part separation as a sequential multi-class classification\nproblem and adopt machine learning to map sequences of notes into sequences of\npart labels. To examine the effectiveness of our proposed models, we conduct a\ncomprehensive empirical evaluation over four diverse datasets of different\ngenres and ensembles -- Bach chorales, string quartets, game music and pop\nmusic. Our experiments show that the proposed models outperform various\nbaselines. We also demonstrate the potential for our proposed models to produce\nalternative convincing instrumentations for an existing arrangement by\nseparating its mixture into parts. All source code and audio samples can be\nfound at https://salu133445.github.io/arranger/ .",
          "link": "http://arxiv.org/abs/2107.05916",
          "publishedOn": "2021-07-14T01:41:51.741Z",
          "wordCount": 691,
          "title": "Towards Automatic Instrumentation by Learning to Separate Parts in Symbolic Multitrack Music. (arXiv:2107.05916v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Revay_M/0/1/0/all/0/1\">Max Revay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manchester_I/0/1/0/all/0/1\">Ian R. Manchester</a>",
          "description": "This paper introduces recurrent equilibrium networks (RENs), a new class of\nnonlinear dynamical models for applications in machine learning, system\nidentification and control. The new model class has ``built in'' guarantees of\nstability and robustness: all models in the class are contracting - a strong\nform of nonlinear stability - and models can satisfy prescribed incremental\nintegral quadratic constraints (IQC), including Lipschitz bounds and\nincremental passivity. RENs are otherwise very flexible: they can represent all\nstable linear systems, all previously-known sets of contracting recurrent\nneural networks and echo state networks, all deep feedforward neural networks,\nand all stable Wiener/Hammerstein models. RENs are parameterized directly by a\nvector in R^N, i.e. stability and robustness are ensured without parameter\nconstraints, which simplifies learning since generic methods for unconstrained\noptimization can be used. The performance and robustness of the new model set\nis evaluated on benchmark nonlinear system identification problems, and the\npaper also presents applications in data-driven nonlinear observer design and\ncontrol with stability guarantees.",
          "link": "http://arxiv.org/abs/2104.05942",
          "publishedOn": "2021-07-14T01:41:51.734Z",
          "wordCount": 653,
          "title": "Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness. (arXiv:2104.05942v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhui/0/1/0/all/0/1\">Minhui</a> (Jason)Xue, <a href=\"http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1\">Stjepan Picek</a>",
          "description": "Backdoor attacks represent a serious threat to neural network models. A\nbackdoored model will misclassify the trigger-embedded inputs into an\nattacker-chosen target label while performing normally on other benign inputs.\nThere are already numerous works on backdoor attacks on neural networks, but\nonly a few works consider graph neural networks (GNNs). As such, there is no\nintensive research on explaining the impact of trigger injecting position on\nthe performance of backdoor attacks on GNNs.\n\nTo bridge this gap, we conduct an experimental investigation on the\nperformance of backdoor attacks on GNNs. We apply two powerful GNN\nexplainability approaches to select the optimal trigger injecting position to\nachieve two attacker objectives -- high attack success rate and low clean\naccuracy drop. Our empirical results on benchmark datasets and state-of-the-art\nneural network models demonstrate the proposed method's effectiveness in\nselecting trigger injecting position for backdoor attacks on GNNs. For\ninstance, on the node classification task, the backdoor attack with trigger\ninjecting position selected by GraphLIME reaches over $84 \\%$ attack success\nrate with less than $2.5 \\%$ accuracy drop",
          "link": "http://arxiv.org/abs/2104.03674",
          "publishedOn": "2021-07-14T01:41:51.727Z",
          "wordCount": 640,
          "title": "Explainability-based Backdoor Attacks Against Graph Neural Networks. (arXiv:2104.03674v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of obese human mesh recovery, i.e., fitting a\nparametric human mesh to images of obese people. Despite obese person mesh\nfitting being an important problem with numerous applications (e.g.,\nhealthcare), much recent progress in mesh recovery has been restricted to\nimages of non-obese people. In this work, we identify this crucial gap in the\ncurrent literature by presenting and discussing limitations of existing\nalgorithms. Next, we present a simple baseline to address this problem that is\nscalable and can be easily used in conjunction with existing algorithms to\nimprove their performance. Finally, we present a generalized human mesh\noptimization algorithm that substantially improves the performance of existing\nmethods on both obese person images as well as community-standard benchmark\ndatasets. A key innovation of this technique is that it does not rely on\nsupervision from expensive-to-create mesh parameters. Instead, starting from\nwidely and cheaply available 2D keypoints annotations, our method automatically\ngenerates mesh parameters that can in turn be used to re-train and fine-tune\nany existing mesh estimation algorithm. This way, we show our method acts as a\ndrop-in to improve the performance of a wide variety of contemporary mesh\nestimation methods. We conduct extensive experiments on multiple datasets\ncomprising both standard and obese person images and demonstrate the efficacy\nof our proposed techniques.",
          "link": "http://arxiv.org/abs/2107.06239",
          "publishedOn": "2021-07-14T01:41:51.720Z",
          "wordCount": 677,
          "title": "Everybody Is Unique: Towards Unbiased Human Mesh Recovery. (arXiv:2107.06239v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "This paper aims to provide understandings for the effect of an\nover-parameterized model, e.g. a deep neural network, memorizing\ninstance-dependent noisy labels. We first quantify the harms caused by\nmemorizing noisy instances, and show the disparate impacts of noisy labels for\nsample instances with different representation frequencies. We then analyze how\nseveral popular solutions for learning with noisy labels mitigate this harm at\nthe instance level. Our analysis reveals that existing approaches lead to\ndisparate treatments when handling noisy instances. While higher-frequency\ninstances often enjoy a high probability of an improvement by applying these\nsolutions, lower-frequency instances do not. Our analysis reveals new\nunderstandings for when these approaches work, and provides theoretical\njustifications for previously reported empirical observations. This observation\nrequires us to rethink the distribution of label noise across instances and\ncalls for different treatments for instances in different regimes.",
          "link": "http://arxiv.org/abs/2102.05336",
          "publishedOn": "2021-07-14T01:41:51.701Z",
          "wordCount": 606,
          "title": "Understanding Instance-Level Label Noise: Disparate Impacts and Treatments. (arXiv:2102.05336v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">Mohit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1\">Bernhard A. Moser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1\">Lukas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freudenthaler_B/0/1/0/all/0/1\">Bernhard Freudenthaler</a>",
          "description": "Guidelines and principles of trustworthy AI should be adhered to in practice\nduring the development of AI systems. This work suggests a novel information\ntheoretic trustworthy AI framework based on the hypothesis that information\ntheory enables taking into account the ethical AI principles during the\ndevelopment of machine learning and deep learning models via providing a way to\nstudy and optimize the inherent tradeoffs between trustworthy AI principles.\nUnder the proposed framework, a unified approach to ``privacy-preserving\ninterpretable and transferable learning'' is considered to introduce the\ninformation theoretic measures for privacy-leakage, interpretability, and\ntransferability. A technique based on variational optimization, employing\n\\emph{conditionally deep autoencoders}, is developed for practically\ncalculating the defined information theoretic measures for privacy-leakage,\ninterpretability, and transferability.",
          "link": "http://arxiv.org/abs/2106.06046",
          "publishedOn": "2021-07-14T01:41:51.694Z",
          "wordCount": 618,
          "title": "Information Theoretic Evaluation of Privacy-Leakage, Interpretability, and Transferability for a Novel Trustworthy AI Framework. (arXiv:2106.06046v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhouzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1\">Kun Feng</a>",
          "description": "While the beta-VAE family is aiming to find disentangled representations and\nacquire human-interpretable generative factors, like what an ICA (from the\nlinear domain) does, we propose Full Encoder, a novel unified autoencoder\nframework as a correspondence to PCA in the non-linear domain. The idea is to\ntrain an autoencoder with one latent variable first, then involve more latent\nvariables progressively to refine the reconstruction results. The Full Encoder\nis also a latent variable predictive model that the latent variables acquired\nare stable and robust, as they always learn the same representation regardless\nof the network initial states. Full Encoder can be used to determine the\ndegrees of freedom in a simple non-linear system and can be useful for data\ncompression or anomaly detection. Full Encoder can also be combined with the\nbeta-VAE framework to sort out the importance of the generative factors,\nproviding more insights for non-linear system analysis. These qualities will\nmake FE useful for analyzing real-life industrial non-linear systems. To\nvalidate, we created a toy dataset with a custom-made non-linear system to test\nit and compare its properties to those of VAE and beta-VAE's.",
          "link": "http://arxiv.org/abs/2103.14082",
          "publishedOn": "2021-07-14T01:41:51.688Z",
          "wordCount": 636,
          "title": "Learning Stable Representations with Full Encoder. (arXiv:2103.14082v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rim_D/0/1/0/all/0/1\">Daniela N. Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_I/0/1/0/all/0/1\">Inseon Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>",
          "description": "Recent achievements in end-to-end deep learning have encouraged the\nexploration of tasks dealing with highly structured data with unified deep\nnetwork models. Having such models for compressing audio signals has been\nchallenging since it requires discrete representations that are not easy to\ntrain with end-to-end backpropagation. In this paper, we present an end-to-end\ndeep learning approach that combines recurrent neural networks (RNNs) within\nthe training strategy of variational autoencoders (VAEs) with a binary\nrepresentation of the latent space. We apply a reparametrization trick for the\nBernoulli distribution for the discrete representations, which allows smooth\nbackpropagation. In addition, our approach allows the separation of the encoder\nand decoder, which is necessary for compression tasks. To our best knowledge,\nthis is the first end-to-end learning for a single audio compression model with\nRNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.",
          "link": "http://arxiv.org/abs/2105.11681",
          "publishedOn": "2021-07-14T01:41:51.682Z",
          "wordCount": 611,
          "title": "Deep Neural Networks and End-to-End Learning for Audio Compression. (arXiv:2105.11681v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jiangeng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaoze Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Mengling Feng</a>",
          "description": "In this paper, we propose a deep residual network-based method, namely the\nDiCOVA-Net, to identify COVID-19 infected patients based on the acoustic\nrecording of their coughs. Since there are far more healthy people than\ninfected patients, this classification problem faces the challenge of\nimbalanced data. To improve the model's ability to recognize minority class\n(the infected patients), we introduce data augmentation and cost-sensitive\nmethods into our model. Besides, considering the particularity of this task, we\ndeploy some fine-tuning techniques to adjust the pre-training ResNet50.\nFurthermore, to improve the model's generalizability, we use ensemble learning\nto integrate prediction results from multiple base classifiers generated using\ndifferent random seeds. To evaluate the proposed DiCOVA-Net's performance, we\nconducted experiments with the DiCOVA challenge dataset. The results show that\nour method has achieved 85.43\\% in AUC, among the top of all competing teams.",
          "link": "http://arxiv.org/abs/2107.06126",
          "publishedOn": "2021-07-14T01:41:51.675Z",
          "wordCount": 637,
          "title": "DiCOVA-Net: Diagnosing COVID-19 using Acoustics based on Deep Residual Network for the DiCOVA Challenge 2021. (arXiv:2107.06126v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Chen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfrommer_T/0/1/0/all/0/1\">Timo Pfrommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1\">Marius Kloft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Maja Rudolph</a>",
          "description": "Data transformations (e.g. rotations, reflections, and cropping) play an\nimportant role in self-supervised learning. Typically, images are transformed\ninto different views, and neural networks trained on tasks involving these\nviews produce useful feature representations for downstream tasks, including\nanomaly detection. However, for anomaly detection beyond image data, it is\noften unclear which transformations to use. Here we present a simple end-to-end\nprocedure for anomaly detection with learnable transformations. The key idea is\nto embed the transformed data into a semantic space such that the transformed\ndata still resemble their untransformed form, while different transformations\nare easily distinguishable. Extensive experiments on time series demonstrate\nthat our proposed method outperforms existing approaches in the one-vs.-rest\nsetting and is competitive in the more challenging n-vs.-rest anomaly detection\ntask. On tabular datasets from the medical and cyber-security domains, our\nmethod learns domain-specific transformations and detects anomalies more\naccurately than previous work.",
          "link": "http://arxiv.org/abs/2103.16440",
          "publishedOn": "2021-07-14T01:41:51.658Z",
          "wordCount": 636,
          "title": "Neural Transformation Learning for Deep Anomaly Detection Beyond Images. (arXiv:2103.16440v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yatong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiaheng Wei</a>",
          "description": "We formulate the problem of induced domain adaptation (IDA) when the\nunderlying distribution/domain shift is introduced by the model being deployed.\nOur formulation is motivated by applications where the deployed machine\nlearning models interact with human agents, and will ultimately face responsive\nand interactive data distributions. We formalize the discussions of the\ntransferability of learning in our IDA setting by studying how the model\ntrained on the available source distribution (data) would translate to the\nperformance on the induced domain. We provide both upper bounds for the\nperformance gap due to the induced domain shift, as well as lower bound for the\ntrade-offs a classifier has to suffer on either the source training\ndistribution or the induced target distribution. We provide further\ninstantiated analysis for two popular domain adaptation settings with covariate\nshift and label shift. We highlight some key properties of IDA, as well as\ncomputational and learning challenges.",
          "link": "http://arxiv.org/abs/2107.05911",
          "publishedOn": "2021-07-14T01:41:51.651Z",
          "wordCount": 577,
          "title": "Induced Domain Adaptation. (arXiv:2107.05911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bozkir_E/0/1/0/all/0/1\">Efe Bozkir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1\">Ali Burak &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1\">Mete Akg&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1\">Nico Pfeifer</a>",
          "description": "Eye tracking is handled as one of the key technologies for applications that\nassess and evaluate human attention, behavior, and biometrics, especially using\ngaze, pupillary, and blink behaviors. One of the challenges with regard to the\nsocial acceptance of eye tracking technology is however the preserving of\nsensitive and personal information. To tackle this challenge, we employ a\nprivacy-preserving framework based on randomized encoding to train a Support\nVector Regression model using synthetic eye images privately to estimate the\nhuman gaze. During the computation, none of the parties learn about the data or\nthe result that any other party has. Furthermore, the party that trains the\nmodel cannot reconstruct pupil, blinks or visual scanpath. The experimental\nresults show that our privacy-preserving framework is capable of working in\nreal-time, with the same accuracy as compared to non-private version and could\nbe extended to other eye tracking related problems.",
          "link": "http://arxiv.org/abs/1911.07936",
          "publishedOn": "2021-07-14T01:41:51.644Z",
          "wordCount": 688,
          "title": "Privacy Preserving Gaze Estimation using Synthetic Images via a Randomized Encoding Based Framework. (arXiv:1911.07936v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigner_C/0/1/0/all/0/1\">Christina Aigner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>",
          "description": "Deep Neural Networks (DNNs) have an enormous potential to learn from complex\nbiomedical data. In particular, DNNs have been used to seamlessly fuse\nheterogeneous information from neuroanatomy, genetics, biomarkers, and\nneuropsychological tests for highly accurate Alzheimer's disease diagnosis. On\nthe other hand, their black-box nature is still a barrier for the adoption of\nsuch a system in the clinic, where interpretability is absolutely essential. We\npropose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for\nexplaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of\nthe neuroanatomy and tabular biomarkers. Our explanations are based on the\nShapley value, which is the unique method that satisfies all fundamental axioms\nfor local explanations previously established in the literature. Thus, SVEHNN\nhas many desirable characteristics that previous work on interpretability for\nmedical decision making is lacking. To avoid the exponential time complexity of\nthe Shapley value, we propose to transform a given DNN into a Lightweight\nProbabilistic Deep Network without re-training, thus achieving a complexity\nonly quadratic in the number of features. In our experiments on synthetic and\nreal data, we show that we can closely approximate the exact Shapley value with\na dramatically reduced runtime and can reveal the hidden knowledge the network\nhas learned from the data.",
          "link": "http://arxiv.org/abs/2107.05997",
          "publishedOn": "2021-07-14T01:41:51.636Z",
          "wordCount": 669,
          "title": "Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from Heterogeneous Data. (arXiv:2107.05997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cruttwell_G/0/1/0/all/0/1\">G.S.H. Cruttwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavranovic_B/0/1/0/all/0/1\">Bruno Gavranovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghani_N/0/1/0/all/0/1\">Neil Ghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_P/0/1/0/all/0/1\">Paul Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanasi_F/0/1/0/all/0/1\">Fabio Zanasi</a>",
          "description": "We propose a categorical semantics of gradient-based machine learning\nalgorithms in terms of lenses, parametrised maps, and reverse derivative\ncategories. This foundation provides a powerful explanatory and unifying\nframework: it encompasses a variety of gradient descent algorithms such as\nADAM, AdaGrad, and Nesterov momentum, as well as a variety of loss functions\nsuch as as MSE and Softmax cross-entropy, shedding new light on their\nsimilarities and differences. Our approach to gradient-based learning has\nexamples generalising beyond the familiar continuous domains (modelled in\ncategories of smooth maps) and can be realized in the discrete setting of\nboolean circuits. Finally, we demonstrate the practical significance of our\nframework with an implementation in Python.",
          "link": "http://arxiv.org/abs/2103.01931",
          "publishedOn": "2021-07-14T01:41:51.629Z",
          "wordCount": 575,
          "title": "Categorical Foundations of Gradient-Based Learning. (arXiv:2103.01931v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jianyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojima_I/0/1/0/all/0/1\">Iwao Ojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fusheng Wang</a>",
          "description": "Artificial intelligence (AI) has been transforming the practice of drug\ndiscovery in the past decade. Various AI techniques have been used in a wide\nrange of applications, such as virtual screening and drug design. In this\nsurvey, we first give an overview on drug discovery and discuss related\napplications, which can be reduced to two major tasks, i.e., molecular property\nprediction and molecule generation. We then discuss common data resources,\nmolecule representations and benchmark platforms. Furthermore, to summarize the\nprogress of AI in drug discovery, we present the relevant AI techniques\nincluding model architectures and learning paradigms in the papers surveyed. We\nexpect that this survey will serve as a guide for researchers who are\ninterested in working at the interface of artificial intelligence and drug\ndiscovery. We also provide a GitHub repository\n(https://github.com/dengjianyuan/Survey_AI_Drug_Discovery) with the collection\nof papers and codes, if applicable, as a learning resource, which is regularly\nupdated.",
          "link": "http://arxiv.org/abs/2106.05386",
          "publishedOn": "2021-07-14T01:41:51.613Z",
          "wordCount": 630,
          "title": "Artificial Intelligence in Drug Discovery: Applications and Techniques. (arXiv:2106.05386v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.01129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nabi_S/0/1/0/all/0/1\">Sareh Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassif_H/0/1/0/all/0/1\">Houssam Nassif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joseph Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamani_H/0/1/0/all/0/1\">Hamed Mamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imbens_G/0/1/0/all/0/1\">Guido Imbens</a>",
          "description": "Adding domain knowledge to a learning system is known to improve results. In\nmulti-parameter Bayesian frameworks, such knowledge is incorporated as a prior.\nOn the other hand, various model parameters can have different learning rates\nin real-world problems, especially with skewed data. Two often-faced challenges\nin Operation Management and Management Science applications are the absence of\ninformative priors, and the inability to control parameter learning rates. In\nthis study, we propose a hierarchical Empirical Bayes approach that addresses\nboth challenges, and that can generalize to any Bayesian framework. Our method\nlearns empirical meta-priors from the data itself and uses them to decouple the\nlearning rates of first-order and second-order features (or any other given\nfeature grouping) in a Generalized Linear Model. As the first-order features\nare likely to have a more pronounced effect on the outcome, focusing on\nlearning first-order weights first is likely to improve performance and\nconvergence time. Our Empirical Bayes method clamps features in each group\ntogether and uses the deployed model's observed data to empirically compute a\nhierarchical prior in hindsight. We report theoretical results for the\nunbiasedness, strong consistency, and optimal frequentist cumulative regret\nproperties of our meta-prior variance estimator. We apply our method to a\nstandard supervised learning optimization problem, as well as an online\ncombinatorial optimization problem in a contextual bandit setting implemented\nin an Amazon production system. Both during simulations and live experiments,\nour method shows marked improvements, especially in cases of small traffic. Our\nfindings are promising, as optimizing over sparse data is often a challenge.",
          "link": "http://arxiv.org/abs/2002.01129",
          "publishedOn": "2021-07-14T01:41:51.606Z",
          "wordCount": 741,
          "title": "Bayesian Meta-Prior Learning Using Empirical Bayes. (arXiv:2002.01129v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foti_S/0/1/0/all/0/1\">Simone Foti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1\">Bongjin Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowrick_T/0/1/0/all/0/1\">Thomas Dowrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalhinho_J/0/1/0/all/0/1\">Joao Ramalhinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allam_M/0/1/0/all/0/1\">Moustafa Allam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1\">Brian Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>",
          "description": "In this work we propose a method based on geometric deep learning to predict\nthe complete surface of the liver, given a partial point cloud of the organ\nobtained during the surgical laparoscopic procedure. We introduce a new data\naugmentation technique that randomly perturbs shapes in their frequency domain\nto compensate the limited size of our dataset. The core of our method is a\nvariational autoencoder (VAE) that is trained to learn a latent space for\ncomplete shapes of the liver. At inference time, the generative part of the\nmodel is embedded in an optimisation procedure where the latent representation\nis iteratively updated to generate a model that matches the intraoperative\npartial point cloud. The effect of this optimisation is a progressive non-rigid\ndeformation of the initially generated shape. Our method is qualitatively\nevaluated on real data and quantitatively evaluated on synthetic data. We\ncompared with a state-of-the-art rigid registration algorithm, that our method\noutperformed in visible areas.",
          "link": "http://arxiv.org/abs/2009.03871",
          "publishedOn": "2021-07-14T01:41:51.600Z",
          "wordCount": 640,
          "title": "Intraoperative Liver Surface Completion with Graph Convolutional VAE. (arXiv:2009.03871v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yeong-Dae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jinho Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byoungjip Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1\">Iljoo Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1\">Youngjune Gwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Seungjai Min</a>",
          "description": "In neural combinatorial optimization (CO), reinforcement learning (RL) can\nturn a deep neural net into a fast, powerful heuristic solver of NP-hard\nproblems. This approach has a great potential in practical applications because\nit allows near-optimal solutions to be found without expert guides armed with\nsubstantial domain knowledge. We introduce Policy Optimization with Multiple\nOptima (POMO), an end-to-end approach for building such a heuristic solver.\nPOMO is applicable to a wide range of CO problems. It is designed to exploit\nthe symmetries in the representation of a CO solution. POMO uses a modified\nREINFORCE algorithm that forces diverse rollouts towards all optimal solutions.\nEmpirically, the low-variance baseline of POMO makes RL training fast and\nstable, and it is more resistant to local minima compared to previous\napproaches. We also introduce a new augmentation-based inference method, which\naccompanies POMO nicely. We demonstrate the effectiveness of POMO by solving\nthree popular NP-hard problems, namely, traveling salesman (TSP), capacitated\nvehicle routing (CVRP), and 0-1 knapsack (KP). For all three, our solver based\non POMO shows a significant improvement in performance over all recent learned\nheuristics. In particular, we achieve the optimality gap of 0.14% with TSP100\nwhile reducing inference time by more than an order of magnitude.",
          "link": "http://arxiv.org/abs/2010.16011",
          "publishedOn": "2021-07-14T01:41:51.593Z",
          "wordCount": 682,
          "title": "POMO: Policy Optimization with Multiple Optima for Reinforcement Learning. (arXiv:2010.16011v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Saurabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhanson_J/0/1/0/all/0/1\">Joshua Zhanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisotto_E/0/1/0/all/0/1\">Emilio Parisotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Adarsh Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1\">Sivaraman Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>",
          "description": "Modern policy gradient algorithms such as Proximal Policy Optimization (PPO)\nrely on an arsenal of heuristics, including loss clipping and gradient\nclipping, to ensure successful learning. These heuristics are reminiscent of\ntechniques from robust statistics, commonly used for estimation in outlier-rich\n(``heavy-tailed'') regimes. In this paper, we present a detailed empirical\nstudy to characterize the heavy-tailed nature of the gradients of the PPO\nsurrogate reward function. We demonstrate that the gradients, especially for\nthe actor network, exhibit pronounced heavy-tailedness and that it increases as\nthe agent's policy diverges from the behavioral policy (i.e., as the agent goes\nfurther off policy). Further examination implicates the likelihood ratios and\nadvantages in the surrogate reward as the main sources of the observed\nheavy-tailedness. We then highlight issues arising due to the heavy-tailed\nnature of the gradients. In this light, we study the effects of the standard\nPPO clipping heuristics, demonstrating that these tricks primarily serve to\noffset heavy-tailedness in gradients. Thus motivated, we propose incorporating\nGMOM, a high-dimensional robust estimator, into PPO as a substitute for three\nclipping tricks. Despite requiring less hyperparameter tuning, our method\nmatches the performance of PPO (with all heuristics enabled) on a battery of\nMuJoCo continuous control tasks.",
          "link": "http://arxiv.org/abs/2102.10264",
          "publishedOn": "2021-07-14T01:41:51.586Z",
          "wordCount": 680,
          "title": "On Proximal Policy Optimization's Heavy-tailed Gradients. (arXiv:2102.10264v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Soumya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>",
          "description": "We propose ${\\tt AdaTS}$, a Thompson sampling algorithm that adapts\nsequentially to bandit tasks that it interacts with. The key idea in ${\\tt\nAdaTS}$ is to adapt to an unknown task prior distribution by maintaining a\ndistribution over its parameters. When solving a bandit task, that uncertainty\nis marginalized out and properly accounted for. ${\\tt AdaTS}$ is a\nfully-Bayesian algorithm that can be implemented efficiently in several classes\nof bandit problems. We derive upper bounds on its Bayes regret that quantify\nthe loss due to not knowing the task prior, and show that it is small. Our\ntheory is supported by experiments, where ${\\tt AdaTS}$ outperforms prior\nalgorithms and works well even in challenging real-world problems.",
          "link": "http://arxiv.org/abs/2107.06196",
          "publishedOn": "2021-07-14T01:41:51.568Z",
          "wordCount": 546,
          "title": "No Regrets for Learning the Prior in Bandits. (arXiv:2107.06196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1\">Christos Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1\">Kiran Vodrahalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_M/0/1/0/all/0/1\">Mihalis Yannakakis</a>",
          "description": "On-line firms deploy suites of software platforms, where each platform is\ndesigned to interact with users during a certain activity, such as browsing,\nchatting, socializing, emailing, driving, etc. The economic and incentive\nstructure of this exchange, as well as its algorithmic nature, have not been\nexplored to our knowledge. We model this interaction as a Stackelberg game\nbetween a Designer and one or more Agents. We model an Agent as a Markov chain\nwhose states are activities; we assume that the Agent's utility is a linear\nfunction of the steady-state distribution of this chain. The Designer may\ndesign a platform for each of these activities/states; if a platform is adopted\nby the Agent, the transition probabilities of the Markov chain are affected,\nand so is the objective of the Agent. The Designer's utility is a linear\nfunction of the steady state probabilities of the accessible states minus the\ndevelopment cost of the platforms. The underlying optimization problem of the\nAgent -- how to choose the states for which to adopt the platform -- is an MDP.\nIf this MDP has a simple yet plausible structure (the transition probabilities\nfrom one state to another only depend on the target state and the recurrent\nprobability of the current state) the Agent's problem can be solved by a greedy\nalgorithm. The Designer's optimization problem (designing a custom suite for\nthe Agent so as to optimize, through the Agent's optimum reaction, the\nDesigner's revenue), is NP-hard to approximate within any finite ratio;\nhowever, the special case, while still NP-hard, has an FPTAS. These results\ngeneralize from a single Agent to a distribution of Agents with finite support,\nas well as to the setting where the Designer must find the best response to the\nexisting strategies of other Designers. We discuss other implications of our\nresults and directions of future research.",
          "link": "http://arxiv.org/abs/2009.06117",
          "publishedOn": "2021-07-14T01:41:51.560Z",
          "wordCount": 783,
          "title": "The Platform Design Problem. (arXiv:2009.06117v2 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Jui Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>",
          "description": "In recent times, BERT based transformer models have become an inseparable\npart of the 'tech stack' of text processing models. Similar progress is being\nobserved in the speech domain with a multitude of models observing\nstate-of-the-art results by using audio transformer models to encode speech.\nThis begs the question of what are these audio transformer models learning.\nMoreover, although the standard methodology is to choose the last layer\nembedding for any downstream task, but is it the optimal choice? We try to\nanswer these questions for the two recent audio transformer models, Mockingjay\nand wave2vec2.0. We compare them on a comprehensive set of language delivery\nand structure features including audio, fluency and pronunciation features.\nAdditionally, we probe the audio models' understanding of textual surface,\nsyntax, and semantic features and compare them to BERT. We do this over\nexhaustive settings for native, non-native, synthetic, read and spontaneous\nspeech datasets",
          "link": "http://arxiv.org/abs/2101.00387",
          "publishedOn": "2021-07-14T01:41:51.552Z",
          "wordCount": 641,
          "title": "What all do audio transformer models hear? Probing Acoustic Representations for Language Delivery and its Structure. (arXiv:2101.00387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06104",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tajini_B/0/1/0/all/0/1\">Badr Tajini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Richard_H/0/1/0/all/0/1\">Hugo Richard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thirion_B/0/1/0/all/0/1\">Bertrand Thirion</a>",
          "description": "Advances in computational cognitive neuroimaging research are related to the\navailability of large amounts of labeled brain imaging data, but such data are\nscarce and expensive to generate. While powerful data generation mechanisms,\nsuch as Generative Adversarial Networks (GANs), have been designed in the last\ndecade for computer vision, such improvements have not yet carried over to\nbrain imaging. A likely reason is that GANs training is ill-suited to the\nnoisy, high-dimensional and small-sample data available in functional\nneuroimaging.In this paper, we introduce Conditional Independent Components\nAnalysis (Conditional ICA): a fast functional Magnetic Resonance Imaging (fMRI)\ndata augmentation technique, that leverages abundant resting-state data to\ncreate images by sampling from an ICA decomposition. We then propose a\nmechanism to condition the generator on classes observed with few samples. We\nfirst show that the generative mechanism is successful at synthesizing data\nindistinguishable from observations, and that it yields gains in classification\naccuracy in brain decoding problems. In particular it outperforms GANs while\nbeing much easier to optimize and interpret. Lastly, Conditional ICA enhances\nclassification accuracy in eight datasets without further parameters tuning.",
          "link": "http://arxiv.org/abs/2107.06104",
          "publishedOn": "2021-07-14T01:41:51.545Z",
          "wordCount": 631,
          "title": "Functional Magnetic Resonance Imaging data augmentation through conditional ICA. (arXiv:2107.06104v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.13988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hangfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "Algorithmic stability is a key characteristic to ensure the generalization\nability of a learning algorithm. Among different notions of stability,\n\\emph{uniform stability} is arguably the most popular one, which yields\nexponential generalization bounds. However, uniform stability only considers\nthe worst-case loss change (or so-called sensitivity) by removing a single data\npoint, which is distribution-independent and therefore undesirable. There are\nmany cases that the worst-case sensitivity of the loss is much larger than the\naverage sensitivity taken over the single data point that is removed,\nespecially in some advanced models such as random feature models or neural\nnetworks. Many previous works try to mitigate the distribution independent\nissue by proposing weaker notions of stability, however, they either only yield\npolynomial bounds or the bounds derived do not vanish as sample size goes to\ninfinity. Given that, we propose \\emph{locally elastic stability} as a weaker\nand distribution-dependent stability notion, which still yields exponential\ngeneralization bounds. We further demonstrate that locally elastic stability\nimplies tighter generalization bounds than those derived based on uniform\nstability in many situations by revisiting the examples of bounded support\nvector machines, regularized least square regressions, and stochastic gradient\ndescent.",
          "link": "http://arxiv.org/abs/2010.13988",
          "publishedOn": "2021-07-14T01:41:51.538Z",
          "wordCount": 660,
          "title": "Toward Better Generalization Bounds with Locally Elastic Stability. (arXiv:2010.13988v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yidong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>",
          "description": "Machine learning systems generally assume that the training and testing\ndistributions are the same. To this end, a key requirement is to develop models\nthat can generalize to unseen distributions. Domain generalization (DG), i.e.,\nout-of-distribution generalization, has attracted increasing interests in\nrecent years. Domain generalization deals with a challenging setting where one\nor several different but related domain(s) are given, and the goal is to learn\na model that can generalize to an unseen test domain. Great progress has been\nmade in the area of domain generalization for years. This paper presents the\nfirst review of recent advances in this area. First, we provide a formal\ndefinition of domain generalization and discuss several related fields. We then\nthoroughly review the theories related to domain generalization and carefully\nanalyze the theory behind generalization. We categorize recent algorithms into\nthree classes: data manipulation, representation learning, and learning\nstrategy, and present several popular algorithms in detail for each category.\nThird, we introduce the commonly used datasets and applications. Finally, we\nsummarize existing literature and present some potential research topics for\nthe future.",
          "link": "http://arxiv.org/abs/2103.03097",
          "publishedOn": "2021-07-14T01:41:51.520Z",
          "wordCount": 690,
          "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.08579",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lyu_X/0/1/0/all/0/1\">Xiong Lyu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ludkovski_M/0/1/0/all/0/1\">Mike Ludkovski</a>",
          "description": "We develop adaptive replicated designs for Gaussian process metamodels of\nstochastic experiments. Adaptive batching is a natural extension of sequential\ndesign heuristics with the benefit of replication growing as response features\nare learned, inputs concentrate, and the metamodeling overhead rises. Motivated\nby the problem of learning the level set of the mean simulator response we\ndevelop four novel schemes: Multi-Level Batching (MLB), Ratchet Batching (RB),\nAdaptive Batched Stepwise Uncertainty Reduction (ABSUR), Adaptive Design with\nStepwise Allocation (ADSA) and Deterministic Design with Stepwise Allocation\n(DDSA). Our algorithms simultaneously (MLB, RB and ABSUR) or sequentially (ADSA\nand DDSA) determine the sequential design inputs and the respective number of\nreplicates. Illustrations using synthetic examples and an application in\nquantitative finance (Bermudan option pricing via Regression Monte Carlo) show\nthat adaptive batching brings significant computational speed-ups with minimal\nloss of modeling fidelity.",
          "link": "http://arxiv.org/abs/2003.08579",
          "publishedOn": "2021-07-14T01:41:51.513Z",
          "wordCount": 600,
          "title": "Adaptive Batching for Gaussian Process Surrogates with Application in Noisy Level Set Estimation. (arXiv:2003.08579v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Mai Lan Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1\">Volker Blanz</a>",
          "description": "Discriminative features play an important role in image and object\nclassification and also in other fields of research such as semi-supervised\nlearning, fine-grained classification, out of distribution detection. Inspired\nby Linear Discriminant Analysis (LDA), we propose an optimization called Neural\nDiscriminant Analysis (NDA) for Deep Convolutional Neural Networks (DCNNs). NDA\ntransforms deep features to become more discriminative and, therefore, improves\nthe performances in various tasks. Our proposed optimization has two primary\ngoals for inter- and intra-class variances. The first one is to minimize\nvariances within each individual class. The second goal is to maximize pairwise\ndistances between features coming from different classes. We evaluate our NDA\noptimization in different research fields: general supervised classification,\nfine-grained classification, semi-supervised learning, and out of distribution\ndetection. We achieve performance improvements in all the fields compared to\nbaseline methods that do not use NDA. Besides, using NDA, we also surpass the\nstate of the art on the four tasks on various testing datasets.",
          "link": "http://arxiv.org/abs/2107.06209",
          "publishedOn": "2021-07-14T01:41:51.484Z",
          "wordCount": 602,
          "title": "Learning a Discriminant Latent Space with Neural Discriminant Analysis. (arXiv:2107.06209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05834",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoxiao Sun</a>",
          "description": "The divide-and-conquer method has been widely used for estimating large-scale\nkernel ridge regression estimates. Unfortunately, when the response variable is\nhighly skewed, the divide-and-conquer kernel ridge regression (dacKRR) may\noverlook the underrepresented region and result in unacceptable results. We\ndevelop a novel response-adaptive partition strategy to overcome the\nlimitation. In particular, we propose to allocate the replicates of some\ncarefully identified informative observations to multiple nodes (local\nprocessors). The idea is analogous to the popular oversampling technique.\nAlthough such a technique has been widely used for addressing discrete label\nskewness, extending it to the dacKRR setting is nontrivial. We provide both\ntheoretical and practical guidance on how to effectively over-sample the\nobservations under the dacKRR setting. Furthermore, we show the proposed\nestimate has a smaller asymptotic mean squared error (AMSE) than that of the\nclassical dacKRR estimate under mild conditions. Our theoretical findings are\nsupported by both simulated and real-data analyses.",
          "link": "http://arxiv.org/abs/2107.05834",
          "publishedOn": "2021-07-14T01:41:51.474Z",
          "wordCount": 579,
          "title": "Oversampling Divide-and-conquer for Response-skewed Kernel Ridge Regression. (arXiv:2107.05834v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.13620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sohini Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shalmali Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As predictive models are increasingly being deployed in high-stakes decision\nmaking (e.g., loan approvals), there has been growing interest in post hoc\ntechniques which provide recourse to affected individuals. These techniques\ngenerate recourses under the assumption that the underlying predictive model\ndoes not change. However, in practice, models are often regularly updated for a\nvariety of reasons (e.g., dataset shifts), thereby rendering previously\nprescribed recourses ineffective. To address this problem, we propose a novel\nframework, RObust Algorithmic Recourse (ROAR), that leverages adversarial\ntraining for finding recourses that are robust to model shifts. To the best of\nour knowledge, this work proposes the first solution to this critical problem.\nWe also carry out detailed theoretical analysis which underscores the\nimportance of constructing recourses that are robust to model shifts: 1) we\nderive a lower bound on the probability of invalidation of recourses generated\nby existing approaches which are not robust to model shifts. 2) we prove that\nthe additional cost incurred due to the robust recourses output by our\nframework is bounded. Experimental evaluation on multiple synthetic and\nreal-world datasets demonstrates the efficacy of the proposed framework and\nsupports our theoretical findings.",
          "link": "http://arxiv.org/abs/2102.13620",
          "publishedOn": "2021-07-14T01:41:51.451Z",
          "wordCount": 646,
          "title": "Towards Robust and Reliable Algorithmic Recourse. (arXiv:2102.13620v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yecheng Jason Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>",
          "description": "Many reinforcement learning (RL) problems in practice are offline, learning\npurely from observational data. A key challenge is how to ensure the learned\npolicy is safe, which requires quantifying the risk associated with different\nactions. In the online setting, distributional RL algorithms do so by learning\nthe distribution over returns (i.e., cumulative rewards) instead of the\nexpected return; beyond quantifying risk, they have also been shown to learn\nbetter representations for planning. We propose Conservative Offline\nDistributional Actor Critic (CODAC), an offline RL algorithm suitable for both\nrisk-neutral and risk-averse domains. CODAC adapts distributional RL to the\noffline setting by penalizing the predicted quantiles of the return for\nout-of-distribution actions. We prove that CODAC learns a conservative return\ndistribution -- in particular, for finite MDPs, CODAC converges to an uniform\nlower bound on the quantiles of the return distribution; our proof relies on a\nnovel analysis of the distributional Bellman operator. In our experiments, on\ntwo challenging robot navigation tasks, CODAC successfully learns risk-averse\npolicies using offline data collected purely from risk-neutral agents.\nFurthermore, CODAC is state-of-the-art on the D4RL MuJoCo benchmark in terms of\nboth expected and risk-sensitive performance.",
          "link": "http://arxiv.org/abs/2107.06106",
          "publishedOn": "2021-07-14T01:41:51.371Z",
          "wordCount": 614,
          "title": "Conservative Offline Distributional Reinforcement Learning. (arXiv:2107.06106v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.02627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verdoja_F/0/1/0/all/0/1\">Francesco Verdoja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>",
          "description": "Among the various options to estimate uncertainty in deep neural networks,\nMonte-Carlo dropout is widely popular for its simplicity and effectiveness.\nHowever the quality of the uncertainty estimated through this method varies and\nchoices in architecture design and in training procedures have to be carefully\nconsidered and tested to obtain satisfactory results. In this paper we present\na study offering a different point of view on the behavior of Monte-Carlo\ndropout, which enables us to observe a few interesting properties of the\ntechnique to keep in mind when considering its use for uncertainty estimation.",
          "link": "http://arxiv.org/abs/2008.02627",
          "publishedOn": "2021-07-14T01:41:51.354Z",
          "wordCount": 564,
          "title": "Notes on the Behavior of MC Dropout. (arXiv:2008.02627v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets.",
          "link": "http://arxiv.org/abs/2107.05948",
          "publishedOn": "2021-07-14T01:41:51.347Z",
          "wordCount": 681,
          "title": "On Designing Good Representation Learning Models. (arXiv:2107.05948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11619",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kehoe_A/0/1/0/all/0/1\">Aidan Kehoe</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wittek_P/0/1/0/all/0/1\">Peter Wittek</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xue_Y/0/1/0/all/0/1\">Yanbo Xue</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pozas_Kerstjens_A/0/1/0/all/0/1\">Alejandro Pozas-Kerstjens</a>",
          "description": "We provide a robust defence to adversarial attacks on discriminative\nalgorithms. Neural networks are naturally vulnerable to small, tailored\nperturbations in the input data that lead to wrong predictions. On the\ncontrary, generative models attempt to learn the distribution underlying a\ndataset, making them inherently more robust to small perturbations. We use\nBoltzmann machines for discrimination purposes as attack-resistant classifiers,\nand compare them against standard state-of-the-art adversarial defences. We\nfind improvements ranging from 5% to 72% against attacks with Boltzmann\nmachines on the MNIST dataset. We furthermore complement the training with\nquantum-enhanced sampling from the D-Wave 2000Q annealer, finding results\ncomparable with classical techniques and with marginal improvements in some\ncases. These results underline the relevance of probabilistic methods in\nconstructing neural networks and highlight a novel scenario of practical\nrelevance where quantum computers, even with limited hardware capabilites,\ncould provide advantages over classical computers. This work is dedicated to\nthe memory of Peter Wittek.",
          "link": "http://arxiv.org/abs/2012.11619",
          "publishedOn": "2021-07-14T01:41:51.338Z",
          "wordCount": 639,
          "title": "Defence against adversarial attacks using classical and quantum-enhanced Boltzmann machines. (arXiv:2012.11619v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1\">Ramon Manzorro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1\">Joshua L. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Binh Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1\">Dev Yashpal Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1\">Eero P. Simoncelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteson_D/0/1/0/all/0/1\">David S. Matteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1\">Peter A. Crozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>",
          "description": "Denoising is a fundamental challenge in scientific imaging. Deep\nconvolutional neural networks (CNNs) provide the current state of the art in\ndenoising natural images, where they produce impressive results. However, their\npotential has barely been explored in the context of scientific imaging.\nDenoising CNNs are typically trained on real natural images artificially\ncorrupted with simulated noise. In contrast, in scientific applications,\nnoiseless ground-truth images are usually not available. To address this issue,\nwe propose a simulation-based denoising (SBD) framework, in which CNNs are\ntrained on simulated images. We test the framework on data obtained from\ntransmission electron microscopy (TEM), an imaging technique with widespread\napplications in material science, biology, and medicine. SBD outperforms\nexisting techniques by a wide margin on a simulated benchmark dataset, as well\nas on real data. Apart from the denoised images, SBD generates likelihood maps\nto visualize the agreement between the structure of the denoised image and the\nobserved data. Our results reveal shortcomings of state-of-the-art denoising\narchitectures, such as their small field-of-view: substantially increasing the\nfield-of-view of the CNNs allows them to exploit non-local periodic patterns in\nthe data, which is crucial at high noise levels. In addition, we analyze the\ngeneralization capability of SBD, demonstrating that the trained networks are\nrobust to variations of imaging parameters and of the underlying signal\nstructure. Finally, we release the first publicly available benchmark dataset\nof TEM images, containing 18,000 examples.",
          "link": "http://arxiv.org/abs/2010.12970",
          "publishedOn": "2021-07-14T01:41:51.321Z",
          "wordCount": 750,
          "title": "Deep Denoising For Scientific Discovery: A Case Study In Electron Microscopy. (arXiv:2010.12970v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06264",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Kanishk/0/1/0/all/0/1\">Kanishk</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nandal_T/0/1/0/all/0/1\">Tanishk Nandal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tyagi_P/0/1/0/all/0/1\">Prince Tyagi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_R/0/1/0/all/0/1\">Raj Kumar Singh</a>",
          "description": "Autoencoders and generative neural network models have recently gained\npopularity in fluid mechanics due to their spontaneity and low processing time\ninstead of high fidelity CFD simulations. Auto encoders are used as model order\nreduction tools in applications of fluid mechanics by compressing input\nhigh-dimensional data using an encoder to map the input space into a\nlower-dimensional latent space. Whereas, generative models such as Variational\nAuto-encoders (VAEs) and Generative Adversarial Networks (GANs) are proving to\nbe effective in generating solutions to chaotic models with high 'randomness'\nsuch as turbulent flows. In this study, forced isotropic turbulence flow is\ngenerated by parameterizing into some basic statistical characteristics. The\nmodels trained on pre-simulated data from dependencies on these characteristics\nand the flow generation is then affected by varying these parameters. The\nlatent vectors pushed along the generator models like the decoders and\ngenerators contain independent entries which can be used to create different\noutputs with similar properties. The use of neural network-based architecture\nremoves the need for dependency on the classical mesh-based Navier-Stoke\nequation estimation which is prominent in many CFD softwares.",
          "link": "http://arxiv.org/abs/2107.06264",
          "publishedOn": "2021-07-14T01:41:51.313Z",
          "wordCount": 630,
          "title": "Parameterization of Forced Isotropic Turbulent Flow using Autoencoders and Generative Adversarial Networks. (arXiv:2107.06264v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1\">Shubham Dhayarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1\">V.K. Viekash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.",
          "link": "http://arxiv.org/abs/2107.06212",
          "publishedOn": "2021-07-14T01:41:51.306Z",
          "wordCount": 684,
          "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ehsan_M/0/1/0/all/0/1\">Md Amimul Ehsan</a>",
          "description": "Electricity generation from burning fossil fuels is one of the major\ncontributors to global warming. Renewable energy sources are a viable\nalternative to produce electrical energy and to reduce the emission from the\npower industry. These energy sources are the building blocks of green energy,\nwhich all have different characteristics. Their availabilities are also\ndiverse, depending on geographical locations and other parameters. Low\nimplementation cost and distributed availability all over the world uplifts\ntheir popularity exponentially. Therefore, it has unlocked opportunities for\nconsumers to produce electricity locally and use it on-site, which reduces\ndependency on centralized utility companies. The research considers two main\nobjectives: the prediction of wind speed that simplifies wind farm planning and\nfeasibility study. Secondly, the need to understand the dependency structure of\nthe wind speeds of multiple distant locations. To address the first objective,\ntwelve artificial intelligence algorithms were used for wind speed prediction\nfrom collected meteorological parameters. The model performances were compared\nto determine the wind speed prediction accuracy. The results show a deep\nlearning approach, long short-term memory (LSTM) outperforms other models with\nthe highest accuracy of 97.8%. For dependency, a multivariate cumulative\ndistribution function, Copula, was used to find the joint distribution of two\nor more distant location wind speeds, followed by a case study. We found that\nthe appropriate copula family and the parameters vary based on the distance in\nbetween. For the case study, Joe-Frank (BB8) copula shows an efficient joint\ndistribution fit for a wind speed pair with a standard error of 0.0094.\nFinally, some insights about the uncertainty aspects of wind speed dependency\nwere addressed.",
          "link": "http://arxiv.org/abs/2107.06182",
          "publishedOn": "2021-07-14T01:41:51.299Z",
          "wordCount": 740,
          "title": "Predictive models for wind speed using artificial intelligence and copula. (arXiv:2107.06182v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_D/0/1/0/all/0/1\">Dimitris Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Swayambhoo Jain</a>",
          "description": "Despite many modern applications of Deep Neural Networks (DNNs), the large\nnumber of parameters in the hidden layers makes them unattractive for\ndeployment on devices with storage capacity constraints. In this paper we\npropose a Data-Driven Low-rank (DDLR) method to reduce the number of parameters\nof pretrained DNNs and expedite inference by imposing low-rank structure on the\nfully connected layers, while controlling for the overall accuracy and without\nrequiring any retraining. We pose the problem as finding the lowest rank\napproximation of each fully connected layer with given performance guarantees\nand relax it to a tractable convex optimization problem. We show that it is\npossible to significantly reduce the number of parameters in common DNN\narchitectures with only a small reduction in classification accuracy. We\ncompare DDLR with Net-Trim, which is another data-driven DNN compression\ntechnique based on sparsity and show that DDLR consistently produces more\ncompressed neural networks while maintaining higher accuracy.",
          "link": "http://arxiv.org/abs/2107.05787",
          "publishedOn": "2021-07-14T01:41:51.293Z",
          "wordCount": 573,
          "title": "Data-Driven Low-Rank Neural Network Compression. (arXiv:2107.05787v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_M/0/1/0/all/0/1\">Mike A. Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>",
          "description": "While deep learning has revolutionized research and applications in NLP and\ncomputer vision, this has not yet been the case for behavioral modeling and\nbehavioral health applications. This is because the domain's datasets are\nsmaller, have heterogeneous datatypes, and typically exhibit a large degree of\nmissingness. Therefore, off-the-shelf deep learning models require significant,\noften prohibitive, adaptation. Accordingly, many research applications still\nrely on manually coded features with boosted tree models, sometimes with\ntask-specific features handcrafted by experts. Here, we address these\nchallenges by providing a neural architecture framework for mobile sensing data\nthat can learn generalizable feature representations from time series and\ndemonstrates the feasibility of transfer learning on small data domains through\nfinetuning. This architecture combines benefits from CNN and Trans-former\narchitectures to (1) enable better prediction performance by learning directly\nfrom raw minute-level sensor data without the need for handcrafted features by\nup to 0.33 ROC AUC, and (2) use pretraining to outperform simpler neural models\nand boosted decision trees with data from as few a dozen participants.",
          "link": "http://arxiv.org/abs/2107.06097",
          "publishedOn": "2021-07-14T01:41:51.286Z",
          "wordCount": 611,
          "title": "Transformer-Based Behavioral Representation Learning Enables Transfer Learning for Mobile Sensing in Small Datasets. (arXiv:2107.06097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.11117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carletti_M/0/1/0/all/0/1\">Mattia Carletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terzi_M/0/1/0/all/0/1\">Matteo Terzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susto_G/0/1/0/all/0/1\">Gian Antonio Susto</a>",
          "description": "Anomaly Detection is an unsupervised learning task aimed at detecting\nanomalous behaviours with respect to historical data. In particular,\nmultivariate Anomaly Detection has an important role in many applications\nthanks to the capability of summarizing the status of a complex system or\nobserved phenomenon with a single indicator (typically called `Anomaly Score')\nand thanks to the unsupervised nature of the task that does not require human\ntagging. The Isolation Forest is one of the most commonly adopted algorithms in\nthe field of Anomaly Detection, due to its proven effectiveness and low\ncomputational complexity. A major problem affecting Isolation Forest is\nrepresented by the lack of interpretability, an effect of the inherent\nrandomness governing the splits performed by the Isolation Trees, the building\nblocks of the Isolation Forest. In this paper we propose effective, yet\ncomputationally inexpensive, methods to define feature importance scores at\nboth global and local level for the Isolation Forest. Moreover, we define a\nprocedure to perform unsupervised feature selection for Anomaly Detection\nproblems based on our interpretability method; such procedure also serves the\npurpose of tackling the challenging task of feature importance evaluation in\nunsupervised anomaly detection. We assess the performance on several synthetic\nand real-world datasets, including comparisons against state-of-the-art\ninterpretability techniques, and make the code publicly available to enhance\nreproducibility and foster research in the field.",
          "link": "http://arxiv.org/abs/2007.11117",
          "publishedOn": "2021-07-14T01:41:51.280Z",
          "wordCount": 697,
          "title": "Interpretable Anomaly Detection with DIFFI: Depth-based Isolation Forest Feature Importance. (arXiv:2007.11117v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01724",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1\">Bingyan Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yan_Y/0/1/0/all/0/1\">Yuling Yan</a>",
          "description": "We investigate the effectiveness of convex relaxation and nonconvex\noptimization in solving bilinear systems of equations under two different\ndesigns (i.e.$~$a sort of random Fourier design and Gaussian design). Despite\nthe wide applicability, the theoretical understanding about these two paradigms\nremains largely inadequate in the presence of random noise. The current paper\nmakes two contributions by demonstrating that: (1) a two-stage nonconvex\nalgorithm attains minimax-optimal accuracy within a logarithmic number of\niterations. (2) convex relaxation also achieves minimax-optimal statistical\naccuracy vis-\\`a-vis random noise. Both results significantly improve upon the\nstate-of-the-art theoretical guarantees.",
          "link": "http://arxiv.org/abs/2008.01724",
          "publishedOn": "2021-07-14T01:41:51.273Z",
          "wordCount": 576,
          "title": "Convex and Nonconvex Optimization Are Both Minimax-Optimal for Noisy Blind Deconvolution under Random Designs. (arXiv:2008.01724v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Chuanqiang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_H/0/1/0/all/0/1\">Huiyun Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>",
          "description": "In recent years, the fast rise in number of studies on graph neural network\n(GNN) has put it from the theories research to reality application stage.\nDespite the encouraging performance achieved by GNN, less attention has been\npaid to the privacy-preserving training and inference over distributed graph\ndata in the related literature. Due to the particularity of graph structure, it\nis challenging to extend the existing private learning framework to GNN.\nMotivated by the idea of split learning, we propose a \\textbf{S}erver\n\\textbf{A}ided \\textbf{P}rivacy-preserving \\textbf{GNN} (SAPGNN) for the node\nlevel task on horizontally partitioned cross-silo scenario. It offers a natural\nextension of centralized GNN to isolated graph with max/min pooling\naggregation, while guaranteeing that all the private data involved in\ncomputation still stays at local data holders. To further enhancing the data\nprivacy, a secure pooling aggregation mechanism is proposed. Theoretical and\nexperimental results show that the proposed model achieves the same accuracy as\nthe one learned over the combined data.",
          "link": "http://arxiv.org/abs/2107.05917",
          "publishedOn": "2021-07-14T01:41:51.266Z",
          "wordCount": 596,
          "title": "Towards Representation Identical Privacy-Preserving Graph Neural Network via Split Learning. (arXiv:2107.05917v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "We study model-based offline Reinforcement Learning with general function\napproximation. We present an algorithm named Constrained Pessimistic Policy\nOptimization (CPPO) which leverages a general function class and uses a\nconstraint to encode pessimism. Under the assumption that the ground truth\nmodel belongs to our function class, CPPO can learn with the offline data only\nproviding partial coverage, i.e., it can learn a policy that completes against\nany policy that is covered by the offline data, in polynomial sample complexity\nwith respect to the statistical complexity of the function class. We then\ndemonstrate that this algorithmic framework can be applied to many specialized\nMarkov Decision Processes where the additional structural assumptions can\nfurther refine the concept of partial coverage. One notable example is low-rank\nMDP with representation learning where the partial coverage is defined using\nthe concept of relative condition number measured by the underlying unknown\nground truth feature representation. Finally, we introduce and study the\nBayesian setting in offline RL. The key benefit of Bayesian offline RL is that\nalgorithmically, we do not need to explicitly construct pessimism or reward\npenalty which could be hard beyond models with linear structures. We present a\nposterior sampling-based incremental policy optimization algorithm (PS-PO)\nwhich proceeds by iteratively sampling a model from the posterior distribution\nand performing one-step incremental policy optimization inside the sampled\nmodel. Theoretically, in expectation with respect to the prior distribution,\nPS-PO can learn a near optimal policy under partial coverage with polynomial\nsample complexity.",
          "link": "http://arxiv.org/abs/2107.06226",
          "publishedOn": "2021-07-14T01:41:51.239Z",
          "wordCount": 687,
          "title": "Pessimistic Model-based Offline RL: PAC Bounds and Posterior Sampling under Partial Coverage. (arXiv:2107.06226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breen_J/0/1/0/all/0/1\">Joe Breen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M. Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merwe_J/0/1/0/all/0/1\">Jacobus Van der Merwe</a>",
          "description": "Network traffic classification that is widely applicable and highly accurate\nis valuable for many network security and management tasks. A flexible and\neasily configurable classification framework is ideal, as it can be customized\nfor use in a wide variety of networks. In this paper, we propose a highly\nconfigurable and flexible machine learning traffic classification method that\nrelies only on statistics of sequences of packets to distinguish known, or\napproved, traffic from unknown traffic. Our method is based on likelihood\nestimation, provides a measure of certainty for classification decisions, and\ncan classify traffic at adjustable certainty levels. Our classification method\ncan also be applied in different classification scenarios, each prioritizing a\ndifferent classification goal. We demonstrate how our classification scheme and\nall its configurations perform well on real-world traffic from a high\nperformance computing network environment.",
          "link": "http://arxiv.org/abs/2107.06080",
          "publishedOn": "2021-07-14T01:41:51.227Z",
          "wordCount": 586,
          "title": "Practical and Configurable Network Traffic Classification Using Probabilistic Machine Learning. (arXiv:2107.06080v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06008",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Samuel_R/0/1/0/all/0/1\">Rikli Samuel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nico_B/0/1/0/all/0/1\">Bigler Daniel Nico</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moritz_P/0/1/0/all/0/1\">Pfenninger Moritz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Joerg_O/0/1/0/all/0/1\">Osterrieder Joerg</a>",
          "description": "Modeling financial time series is challenging due to their high volatility\nand unexpected happenings on the market. Most financial models and algorithms\ntrying to fill the lack of historical financial time series struggle to perform\nand are highly vulnerable to overfitting. As an alternative, we introduce in\nthis paper a deep neural network called the WGAN-GP, a data-driven model that\nfocuses on sample generation. The WGAN-GP consists of a generator and\ndiscriminator function which utilize an LSTM architecture. The WGAN-GP is\nsupposed to learn the underlying structure of the input data, which in our\ncase, is the Bitcoin. Bitcoin is unique in its behavior; the prices fluctuate\nwhat makes guessing the price trend hardly impossible. Through adversarial\ntraining, the WGAN-GP should learn the underlying structure of the bitcoin and\ngenerate very similar samples of the bitcoin distribution. The generated\nsynthetic time series are visually indistinguishable from the real data. But\nthe numerical results show that the generated data were close to the real data\ndistribution but distinguishable. The model mainly shows a stable learning\nbehavior. However, the model has space for optimization, which could be\nachieved by adjusting the hyperparameters.",
          "link": "http://arxiv.org/abs/2107.06008",
          "publishedOn": "2021-07-14T01:41:51.219Z",
          "wordCount": 629,
          "title": "Wasserstein GAN: Deep Generation applied on Bitcoins financial time series. (arXiv:2107.06008v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengsheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1\">Miguel Angel Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1\">Alex Colburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulbricht_D/0/1/0/all/0/1\">Daniel Ulbricht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1\">Qi Shan</a>",
          "description": "We study the problem of novel view synthesis of a scene comprised of 3D\nobjects. We propose a simple yet effective approach that is neither continuous\nnor implicit, challenging recent trends on view synthesis. We demonstrate that\nalthough continuous radiance field representations have gained a lot of\nattention due to their expressive power, our simple approach obtains comparable\nor even better novel view reconstruction quality comparing with\nstate-of-the-art baselines while increasing rendering speed by over 400x. Our\nmodel is trained in a category-agnostic manner and does not require\nscene-specific optimization. Therefore, it is able to generalize novel view\nsynthesis to object categories not seen during training. In addition, we show\nthat with our simple formulation, we can use view synthesis as a\nself-supervision signal for efficient learning of 3D geometry without explicit\n3D supervision.",
          "link": "http://arxiv.org/abs/2107.05775",
          "publishedOn": "2021-07-14T01:41:51.190Z",
          "wordCount": 581,
          "title": "Fast and Explicit Neural View Synthesis. (arXiv:2107.05775v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11436",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>",
          "description": "Despite remarkable success in a variety of applications, it is well-known\nthat deep learning can fail catastrophically when presented with\nout-of-distribution data. Toward addressing this challenge, we consider the\ndomain generalization problem, wherein predictors are trained using data drawn\nfrom a family of related training domains and then evaluated on a distinct and\nunseen test domain. We show that under a natural model of data generation and a\nconcomitant invariance condition, the domain generalization problem is\nequivalent to an infinite-dimensional constrained statistical learning problem;\nthis problem forms the basis of our approach, which we call Model-Based Domain\nGeneralization. Due to the inherent challenges in solving constrained\noptimization problems in deep learning, we exploit nonconvex duality theory to\ndevelop unconstrained relaxations of this statistical problem with tight bounds\non the duality gap. Based on this theoretical motivation, we propose a novel\ndomain generalization algorithm with convergence guarantees. In our\nexperiments, we report improvements of up to 30 percentage points over\nstate-of-the-art domain generalization baselines on several benchmarks\nincluding ColoredMNIST, Camelyon17-WILDS, FMoW-WILDS, and PACS.",
          "link": "http://arxiv.org/abs/2102.11436",
          "publishedOn": "2021-07-14T01:41:51.174Z",
          "wordCount": 625,
          "title": "Model-Based Domain Generalization. (arXiv:2102.11436v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03594",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zadorozhnyi_O/0/1/0/all/0/1\">Oleksandr Zadorozhnyi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gaillard_P/0/1/0/all/0/1\">Pierre Gaillard</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gerschinovitz_S/0/1/0/all/0/1\">Sebastien Gerschinovitz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "In this work we investigate the variation of the online kernelized ridge\nregression algorithm in the setting of $d-$dimensional adversarial\nnonparametric regression. We derive the regret upper bounds on the classes of\nSobolev spaces $W_{p}^{\\beta}(\\mathcal{X})$, $p\\geq 2, \\beta>\\frac{d}{p}$. The\nupper bounds are supported by the minimax regret analysis, which reveals that\nin the cases $\\beta> \\frac{d}{2}$ or $p=\\infty$ these rates are (essentially)\noptimal. Finally, we compare the performance of the kernelized ridge regression\nforecaster to the known non-parametric forecasters in terms of the regret rates\nand their computational complexity as well as to the excess risk rates in the\nsetting of statistical (i.i.d.) nonparametric regression.",
          "link": "http://arxiv.org/abs/2102.03594",
          "publishedOn": "2021-07-14T01:41:51.159Z",
          "wordCount": 566,
          "title": "Online nonparametric regression with Sobolev kernels. (arXiv:2102.03594v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziel_F/0/1/0/all/0/1\">Florian Ziel</a>",
          "description": "We present a winning method of the IEEE DataPort Competition on Day-Ahead\nElectricity Demand Forecasting: Post-COVID Paradigm. The day-ahead load\nforecasting approach is based on online forecast combination of multiple point\nprediction models. It contains four steps: i) data cleaning and preprocessing,\nii) a holiday adjustment procedure, iii) training of individual forecasting\nmodels, iv) forecast combination by smoothed Bernstein Online Aggregation\n(BOA). The approach is flexible and can quickly adopt to new energy system\nsituations as they occurred during and after COVID-19 shutdowns. The pool of\nindividual prediction models ranges from rather simple time series models to\nsophisticated models like generalized additive models (GAMs) and\nhigh-dimensional linear models estimated by lasso. They incorporate\nautoregressive, calendar and weather effects efficiently. All steps contain\nnovel concepts that contribute to the excellent forecasting performance of the\nproposed method. This holds particularly for the holiday adjustment procedure\nand the fully adaptive smoothed BOA approach.",
          "link": "http://arxiv.org/abs/2107.06268",
          "publishedOn": "2021-07-14T01:41:51.151Z",
          "wordCount": 654,
          "title": "Smoothed Bernstein Online Aggregation for Day-Ahead Electricity Demand Forecasting. (arXiv:2107.06268v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wen-Sheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>",
          "description": "We present Retrieve in Style (RIS), an unsupervised framework for\nfine-grained facial feature transfer and retrieval on real images. Recent work\nshows that it is possible to learn a catalog that allows local semantic\ntransfers of facial features on generated images by capitalizing on the\ndisentanglement property of the StyleGAN latent space. RIS improves existing\nart on: 1) feature disentanglement and allows for challenging transfers (i.e.,\nhair and pose) that were not shown possible in SoTA methods. 2) eliminating the\nneed for per-image hyperparameter tuning, and for computing a catalog over a\nlarge batch of images. 3) enabling face retrieval using the proposed facial\nfeatures (e.g., eyes), and to our best knowledge, is the first work to retrieve\nface images at the fine-grained level. 4) robustness and natural application to\nreal images. Our qualitative and quantitative analyses show RIS achieves both\nhigh-fidelity feature transfers and accurate fine-grained retrievals on real\nimages. We discuss the responsible application of RIS.",
          "link": "http://arxiv.org/abs/2107.06256",
          "publishedOn": "2021-07-14T01:41:51.144Z",
          "wordCount": 607,
          "title": "Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval. (arXiv:2107.06256v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junkun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Anpeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Runze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>",
          "description": "Instrumental variables (IVs), sources of treatment randomization that are\nconditionally independent of the outcome, play an important role in causal\ninference with unobserved confounders. However, the existing IV-based\ncounterfactual prediction methods need well-predefined IVs, while it's an art\nrather than science to find valid IVs in many real-world scenes. Moreover, the\npredefined hand-made IVs could be weak or erroneous by violating the conditions\nof valid IVs. These thorny facts hinder the application of the IV-based\ncounterfactual prediction methods. In this paper, we propose a novel Automatic\nInstrumental Variable decomposition (AutoIV) algorithm to automatically\ngenerate representations serving the role of IVs from observed variables (IV\ncandidates). Specifically, we let the learned IV representations satisfy the\nrelevance condition with the treatment and exclusion condition with the outcome\nvia mutual information maximization and minimization constraints, respectively.\nWe also learn confounder representations by encouraging them to be relevant to\nboth the treatment and the outcome. The IV and confounder representations\ncompete for the information with their constraints in an adversarial game,\nwhich allows us to get valid IV representations for IV-based counterfactual\nprediction. Extensive experiments demonstrate that our method generates valid\nIV representations for accurate IV-based counterfactual prediction.",
          "link": "http://arxiv.org/abs/2107.05884",
          "publishedOn": "2021-07-14T01:41:51.115Z",
          "wordCount": 638,
          "title": "Auto IV: Counterfactual Prediction via Automatic Instrumental Variable Decomposition. (arXiv:2107.05884v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04891",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Subhro Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>",
          "description": "This paper considers online optimal control with affine constraints on the\nstates and actions under linear dynamics with bounded random disturbances. The\nsystem dynamics and constraints are assumed to be known and time-invariant but\nthe convex stage cost functions change adversarially. To solve this problem, we\npropose Online Gradient Descent with Buffer Zones (OGD-BZ). Theoretically, we\nshow that OGD-BZ with proper parameters can guarantee the system to satisfy all\nthe constraints despite any admissible disturbances. Further, we investigate\nthe policy regret of OGD-BZ, which compares OGD-BZ's performance with the\nperformance of the optimal linear policy in hindsight. We show that OGD-BZ can\nachieve a policy regret upper bound that is the square root of the horizon\nlength multiplied by some logarithmic terms of the horizon length under proper\nalgorithm parameters.",
          "link": "http://arxiv.org/abs/2010.04891",
          "publishedOn": "2021-07-14T01:41:51.104Z",
          "wordCount": 593,
          "title": "Online Optimal Control with Affine Constraints. (arXiv:2010.04891v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Priya_S/0/1/0/all/0/1\">Shruti Priya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhadra_S/0/1/0/all/0/1\">Shubhankar Bhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimalakonda_S/0/1/0/all/0/1\">Sridhar Chimalakonda</a>",
          "description": "Today, Machine Learning (ML) is of a great importance to society due to the\navailability of huge data and high computational resources. This ultimately led\nto the introduction of ML concepts at multiple levels of education including\nK-12 students to promote computational thinking. However, teaching these\nconcepts to K-12 through traditional methodologies such as video lectures and\nbooks is challenging. Many studies in the literature have reported that using\ninteractive environments such as games to teach computational thinking and\nprogramming improves retention capacity and motivation among students.\nTherefore, introducing ML concepts using a game might enhance students'\nunderstanding of the subject and motivate them to learn further. However, we\nare not aware of any existing game which explicitly focuses on introducing ML\nconcepts to students using game play. Hence, in this paper, we propose\nML-Quest, a 3D video game to provide conceptual overview of three ML concepts:\nSupervised Learning, Gradient Descent and K-Nearest Neighbor (KNN)\nClassification. The crux of the game is to introduce the definition and working\nof these concepts, which we call conceptual overview, in a simulated scenario\nwithout overwhelming students with the intricacies of ML. The game has been\npredominantly evaluated for its usefulness and player experience using the\nTechnology Acceptance Model (TAM) model with the help of 23 higher-secondary\nschool students. The survey result shows that around 70% of the participants\neither agree or strongly agree that the ML-Quest is quite interactive and\nuseful in introducing them to ML concepts.",
          "link": "http://arxiv.org/abs/2107.06206",
          "publishedOn": "2021-07-14T01:41:51.097Z",
          "wordCount": 693,
          "title": "ML-Quest: A Game for Introducing Machine Learning Concepts to K-12 Students. (arXiv:2107.06206v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11046",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bauer_M/0/1/0/all/0/1\">Matthias Bauer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mnih_A/0/1/0/all/0/1\">Andriy Mnih</a>",
          "description": "Efficient low-variance gradient estimation enabled by the reparameterization\ntrick (RT) has been essential to the success of variational autoencoders.\nDoubly-reparameterized gradients (DReGs) improve on the RT for multi-sample\nvariational bounds by applying reparameterization a second time for an\nadditional reduction in variance. Here, we develop two generalizations of the\nDReGs estimator and show that they can be used to train conditional and\nhierarchical VAEs on image modelling tasks more effectively. First, we extend\nthe estimator to hierarchical models with several stochastic layers by showing\nhow to treat additional score function terms due to the hierarchical\nvariational posterior. We then generalize DReGs to score functions of arbitrary\ndistributions instead of just those of the sampling distribution, which makes\nthe estimator applicable to the parameters of the prior in addition to those of\nthe posterior.",
          "link": "http://arxiv.org/abs/2101.11046",
          "publishedOn": "2021-07-14T01:41:51.090Z",
          "wordCount": 584,
          "title": "Generalized Doubly Reparameterized Gradient Estimators. (arXiv:2101.11046v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06174",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Juyong Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_Y/0/1/0/all/0/1\">Youngsang Cho</a>",
          "description": "As the volatility of electricity demand increases owing to climate change and\nelectrification, the importance of accurate peak load forecasting is\nincreasing. Traditional peak load forecasting has been conducted through time\nseries-based models; however, recently, new models based on machine or deep\nlearning are being introduced. This study performs a comparative analysis to\ndetermine the most accurate peak load-forecasting model for Korea, by comparing\nthe performance of time series, machine learning, and hybrid models. Seasonal\nautoregressive integrated moving average with exogenous variables (SARIMAX) is\nused for the time series model. Artificial neural network (ANN), support vector\nregression (SVR), and long short-term memory (LSTM) are used for the machine\nlearning models. SARIMAX-ANN, SARIMAX-SVR, and SARIMAX-LSTM are used for the\nhybrid models. The results indicate that the hybrid models exhibit significant\nimprovement over the SARIMAX model. The LSTM-based models outperformed the\nothers; the single and hybrid LSTM models did not exhibit a significant\nperformance difference. In the case of Korea's highest peak load in 2019, the\npredictive power of the LSTM model proved to be greater than that of the\nSARIMAX-LSTM model. The LSTM, SARIMAX-SVR, and SARIMAX-LSTM models outperformed\nthe current time series-based forecasting model used in Korea. Thus, Korea's\npeak load-forecasting performance can be improved by including machine learning\nor hybrid models.",
          "link": "http://arxiv.org/abs/2107.06174",
          "publishedOn": "2021-07-14T01:41:51.082Z",
          "wordCount": 660,
          "title": "National-scale electricity peak load forecasting: Traditional, machine learning, or hybrid model?. (arXiv:2107.06174v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafar_H/0/1/0/all/0/1\">Hammad Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utkovski_Z/0/1/0/all/0/1\">Zoran Utkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasparick_M/0/1/0/all/0/1\">Martin Kasparick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_S/0/1/0/all/0/1\">Slawomir Stanczak</a>",
          "description": "This paper addresses the problem of decentralized spectrum sharing in\nvehicle-to-everything (V2X) communication networks. The aim is to provide\nresource-efficient coexistence of vehicle-to-infrastructure(V2I) and\nvehicle-to-vehicle(V2V) links. A recent work on the topic proposes a\nmulti-agent reinforcement learning (MARL) approach based on deep Q-learning,\nwhich leverages a fingerprint-based deep Q-network (DQN) architecture. This\nwork considers an extension of this framework by combining Double Q-learning\n(via Double DQN) and transfer learning. The motivation behind is that Double\nQ-learning can alleviate the problem of overestimation of the action values\npresent in conventional Q-learning, while transfer learning can leverage\nknowledge acquired by an expert model to accelerate learning in the MARL\nsetting. The proposed algorithm is evaluated in a realistic V2X setting, with\nsynthetic data generated based on a geometry-based propagation model that\nincorporates location-specific geographical descriptors of the simulated\nenvironment(outlines of buildings, foliage, and vehicles). The advantages of\nthe proposed approach are demonstrated via numerical simulations.",
          "link": "http://arxiv.org/abs/2107.06195",
          "publishedOn": "2021-07-14T01:41:51.075Z",
          "wordCount": 614,
          "title": "Transfer Learning in Multi-Agent Reinforcement Learning with Double Q-Networks for Distributed Resource Sharing in V2X Communication. (arXiv:2107.06195v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoxin Liu</a>",
          "description": "Domain generalization (DG) aims to help models trained on a set of source\ndomains generalize better on unseen target domains. The performances of current\nDG methods largely rely on sufficient labeled data, which however are usually\ncostly or unavailable. While unlabeled data are far more accessible, we seek to\nexplore how unsupervised learning can help deep models generalizes across\ndomains. Specifically, we study a novel generalization problem called\nunsupervised domain generalization, which aims to learn generalizable models\nwith unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised\nLearning (DIUL) method to cope with the significant and misleading\nheterogeneity within unlabeled data and severe distribution shifts between\nsource and target data. Surprisingly we observe that DIUL can not only\ncounterbalance the scarcity of labeled data but also further strengthen the\ngeneralization ability of models when the labeled data are sufficient. As a\npretraining approach, DIUL shows superior to ImageNet pretraining protocol even\nwhen the available data are unlabeled and of a greatly smaller amount compared\nto ImageNet. Extensive experiments clearly demonstrate the effectiveness of our\nmethod compared with state-of-the-art unsupervised learning counterparts.",
          "link": "http://arxiv.org/abs/2107.06219",
          "publishedOn": "2021-07-14T01:41:51.056Z",
          "wordCount": 625,
          "title": "Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization. (arXiv:2107.06219v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Mai Lan Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1\">Volker Blanz</a>",
          "description": "We propose a simple modification from a fixed margin triplet loss to an\nadaptive margin triplet loss. While the original triplet loss is used widely in\nclassification problems such as face recognition, face re-identification and\nfine-grained similarity, our proposed loss is well suited for rating datasets\nin which the ratings are continuous values. In contrast to original triplet\nloss where we have to sample data carefully, in out method, we can generate\ntriplets using the whole dataset, and the optimization can still converge\nwithout frequently running into a model collapsing issue. The adaptive margins\nonly need to be computed once before the training, which is much less expensive\nthan generating triplets after every epoch as in the fixed margin case. Besides\nsubstantially improved training stability (the proposed model never collapsed\nin our experiments compared to a couple of times that the training collapsed on\nexisting triplet loss), we achieved slightly better performance than the\noriginal triplet loss on various rating datasets and network architectures.",
          "link": "http://arxiv.org/abs/2107.06187",
          "publishedOn": "2021-07-14T01:41:51.047Z",
          "wordCount": 599,
          "title": "Deep Ranking with Adaptive Margin Triplet Loss. (arXiv:2107.06187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leng_G/0/1/0/all/0/1\">Guangjie Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yeku Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>",
          "description": "Empirical works suggest that various semantics emerge in the latent space of\nGenerative Adversarial Networks (GANs) when being trained to generate images.\nTo perform real image editing, it requires an accurate mapping from the real\nimage to the latent space to leveraging these learned semantics, which is\nimportant yet difficult. An in-domain GAN inversion approach is recently\nproposed to constraint the inverted code within the latent space by forcing the\nreconstructed image obtained from the inverted code within the real image\nspace. Empirically, we find that the inverted code by the in-domain GAN can\ndeviate from the latent space significantly. To solve this problem, we propose\na force-in-domain GAN based on the in-domain GAN, which utilizes a\ndiscriminator to force the inverted code within the latent space. The\nforce-in-domain GAN can also be interpreted by a cycle-GAN with slight\nmodification. Extensive experiments show that our force-in-domain GAN not only\nreconstructs the target image at the pixel level, but also align the inverted\ncode with the latent space well for semantic editing.",
          "link": "http://arxiv.org/abs/2107.06050",
          "publishedOn": "2021-07-14T01:41:51.041Z",
          "wordCount": 606,
          "title": "Force-in-domain GAN inversion. (arXiv:2107.06050v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davis_Q/0/1/0/all/0/1\">Q. Tyrell Davis</a>",
          "description": "This paper is both an introduction and an invitation. It is an introduction\nto CARLE, a Life-like cellular automata simulator and reinforcement learning\nenvironment. It is also an invitation to Carle's Game, a challenge in\nopen-ended machine exploration and creativity. Inducing machine agents to excel\nat creating interesting patterns across multiple cellular automata universes is\na substantial challenge, and approaching this challenge is likely to require\ncontributions from the fields of artificial life, AI, machine learning, and\ncomplexity, at multiple levels of interest. Carle's Game is based on machine\nagent interaction with CARLE, a Cellular Automata Reinforcement Learning\nEnvironment. CARLE is flexible, capable of simulating any of the 262,144\ndifferent rules defining Life-like cellular automaton universes. CARLE is also\nfast and can simulate automata universes at a rate of tens of thousands of\nsteps per second through a combination of vectorization and GPU acceleration.\nFinally, CARLE is simple. Compared to high-fidelity physics simulators and\nvideo games designed for human players, CARLE's two-dimensional grid world\noffers a discrete, deterministic, and atomic universal playground, despite its\ncomplexity. In combination with CARLE, Carle's Game offers an initial set of\nagent policies, learning and meta-learning algorithms, and reward wrappers that\ncan be tailored to encourage exploration or specific tasks.",
          "link": "http://arxiv.org/abs/2107.05786",
          "publishedOn": "2021-07-14T01:41:51.034Z",
          "wordCount": 652,
          "title": "Carle's Game: An Open-Ended Challenge in Exploratory Machine Creativity. (arXiv:2107.05786v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Olivan_C/0/1/0/all/0/1\">Carlos Hernandez-Olivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltran_J/0/1/0/all/0/1\">Jose R. Beltran</a>",
          "description": "The aim of this work is to define a model based on deep learning that is able\nto identify different instrument timbres with as few parameters as possible.\nFor this purpose, we have worked with classical orchestral instruments played\nwith different dynamics, which are part of a few instrument families and which\nplay notes in the same pitch range. It has been possible to assess the ability\nto classify instruments by timbre even if the instruments are playing the same\nnote with the same intensity. The network employed uses a multi-head attention\nmechanism, with 8 heads and a dense network at the output taking as input the\nlog-mel magnitude spectrograms of the sound samples. This network allows the\nidentification of 20 instrument classes of the classical orchestra, achieving\nan overall F$_1$ value of 0.62. An analysis of the weights of the attention\nlayer has been performed and the confusion matrix of the model is presented,\nallowing us to assess the ability of the proposed architecture to distinguish\ntimbre and to establish the aspects on which future work should focus.",
          "link": "http://arxiv.org/abs/2107.06231",
          "publishedOn": "2021-07-14T01:41:51.027Z",
          "wordCount": 622,
          "title": "Timbre Classification of Musical Instruments with a Deep Learning Multi-Head Attention-Based Model. (arXiv:2107.06231v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belghazi_M/0/1/0/all/0/1\">Mohamed Ishmael Belghazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Paz_D/0/1/0/all/0/1\">David Lopez-Paz</a>",
          "description": "Being uncertain when facing the unknown is key to intelligent decision\nmaking. However, machine learning algorithms lack reliable estimates about\ntheir predictive uncertainty. This leads to wrong and overly-confident\ndecisions when encountering classes unseen during training. Despite the\nimportance of equipping classifiers with uncertainty estimates ready for the\nreal world, prior work has focused on small datasets and little or no class\ndiscrepancy between training and testing data. To close this gap, we introduce\nUIMNET: a realistic, ImageNet-scale test-bed to evaluate predictive uncertainty\nestimates for deep image classifiers. Our benchmark provides implementations of\neight state-of-the-art algorithms, six uncertainty measures, four in-domain\nmetrics, three out-domain metrics, and a fully automated pipeline to train,\ncalibrate, ensemble, select, and evaluate models. Our test-bed is open-source\nand all of our results are reproducible from a fixed commit in our repository.\nAdding new datasets, algorithms, measures, or metrics is a matter of a few\nlines of code-in so hoping that UIMNET becomes a stepping stone towards\nrealistic, rigorous, and reproducible research in uncertainty estimation. Our\nresults show that ensembles of ERM classifiers as well as single MIMO\nclassifiers are the two best alternatives currently available to measure\nuncertainty about both in-domain and out-domain classes.",
          "link": "http://arxiv.org/abs/2107.06217",
          "publishedOn": "2021-07-14T01:41:51.008Z",
          "wordCount": 630,
          "title": "What classifiers know what they don't?. (arXiv:2107.06217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quinones_M/0/1/0/all/0/1\">Miguel Paredes Qui&#xf1;ones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zortea_M/0/1/0/all/0/1\">Maciel Zortea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_L/0/1/0/all/0/1\">Leonardo S. A. Martins</a>",
          "description": "Streamflow forecasting is key to effectively managing water resources and\npreparing for the occurrence of natural calamities being exacerbated by climate\nchange. Here we use the concept of fast and slow flow components to create a\nnew mass-conserving Long Short-Term Memory (LSTM) neural network model. It uses\nhydrometeorological time series and catchment attributes to predict daily river\ndischarges. Preliminary results evidence improvement in skills for different\nscores compared to the recent literature.",
          "link": "http://arxiv.org/abs/2107.06057",
          "publishedOn": "2021-07-14T01:41:50.959Z",
          "wordCount": 528,
          "title": "Fast-Slow Streamflow Model Using Mass-Conserving LSTM. (arXiv:2107.06057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06115",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhenning Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guohui Zhang</a>",
          "description": "Inefficient traffic signal control methods may cause numerous problems, such\nas traffic congestion and waste of energy. Reinforcement learning (RL) is a\ntrending data-driven approach for adaptive traffic signal control in complex\nurban traffic networks. Although the development of deep neural networks (DNN)\nfurther enhances its learning capability, there are still some challenges in\napplying deep RLs to transportation networks with multiple signalized\nintersections, including non-stationarity environment, exploration-exploitation\ndilemma, multi-agent training schemes, continuous action spaces, etc. In order\nto address these issues, this paper first proposes a multi-agent deep\ndeterministic policy gradient (MADDPG) method by extending the actor-critic\npolicy gradient algorithms. MADDPG has a centralized learning and decentralized\nexecution paradigm in which critics use additional information to streamline\nthe training process, while actors act on their own local observations. The\nmodel is evaluated via simulation on the Simulation of Urban MObility (SUMO)\nplatform. Model comparison results show the efficiency of the proposed\nalgorithm in controlling traffic lights.",
          "link": "http://arxiv.org/abs/2107.06115",
          "publishedOn": "2021-07-14T01:41:50.951Z",
          "wordCount": 599,
          "title": "A Deep Reinforcement Learning Approach for Traffic Signal Control Optimization. (arXiv:2107.06115v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06099",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1\">Haiyang Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhou_G/0/1/0/all/0/1\">Guangyu Zhou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_S/0/1/0/all/0/1\">Siqi Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jiang_J/0/1/0/all/0/1\">Jyun-Yu Jiang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Motivation: Predicting Drug-Target Interaction (DTI) is a well-studied topic\nin bioinformatics due to its relevance in the fields of proteomics and\npharmaceutical research. Although many machine learning methods have been\nsuccessfully applied in this task, few of them aim at leveraging the inherent\nheterogeneous graph structure in the DTI network to address the challenge. For\nbetter learning and interpreting the DTI topological structure and the\nsimilarity, it is desirable to have methods specifically for predicting\ninteractions from the graph structure.\n\nResults: We present an end-to-end framework, DTI-GAT (Drug-Target Interaction\nprediction with Graph Attention networks) for DTI predictions. DTI-GAT\nincorporates a deep neural network architecture that operates on\ngraph-structured data with the attention mechanism, which leverages both the\ninteraction patterns and the features of drug and protein sequences. DTI-GAT\nfacilitates the interpretation of the DTI topological structure by assigning\ndifferent attention weights to each node with the self-attention mechanism.\nExperimental evaluations show that DTI-GAT outperforms various state-of-the-art\nsystems on the binary DTI prediction problem. Moreover, the independent study\nresults further demonstrate that our model can be generalized better than other\nconventional methods.\n\nAvailability: The source code and all datasets are available at\nhttps://github.com/Haiyang-W/DTI-GRAPH",
          "link": "http://arxiv.org/abs/2107.06099",
          "publishedOn": "2021-07-14T01:41:50.945Z",
          "wordCount": 632,
          "title": "Drug-Target Interaction Prediction with Graph Attention networks. (arXiv:2107.06099v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_C/0/1/0/all/0/1\">Chikara Nakamura</a>",
          "description": "Extreme value theory (EVT) is a statistical tool for analysis of extreme\nevents. It has a strong theoretical background, however, we need to choose\nhyper-parameters\n\nto apply EVT. In recent studies of machine learning, techniques of choosing\nhyper-parameters have been well-studied. In this paper, we propose a new method\nof choosing hyper-parameters in EVT based on machine learning techniques. We\nalso experiment our method to real-world data and show good usability of our\nmethod.",
          "link": "http://arxiv.org/abs/2107.06074",
          "publishedOn": "2021-07-14T01:41:50.938Z",
          "wordCount": 508,
          "title": "On Choice of Hyper-parameter in Extreme Value Theory based on Machine Learning Techniques. (arXiv:2107.06074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahim Shahriar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1\">Mueeze Al Mushabbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>",
          "description": "Chatbots are intelligent software built to be used as a replacement for human\ninteraction. However, existing studies typically do not provide enough support\nfor low-resource languages like Bangla. Moreover, due to the increasing\npopularity of social media, we can also see the rise of interactions in Bangla\ntransliteration (mostly in English) among the native Bangla speakers. In this\npaper, we propose a novel approach to build a Bangla chatbot aimed to be used\nas a business assistant which can communicate in Bangla and Bangla\nTransliteration in English with high confidence consistently. Since annotated\ndata was not available for this purpose, we had to work on the whole machine\nlearning life cycle (data preparation, machine learning modeling, and model\ndeployment) using Rasa Open Source Framework, fastText embeddings, Polyglot\nembeddings, Flask, and other systems as building blocks. While working with the\nskewed annotated dataset, we try out different setups and pipelines to evaluate\nwhich works best and provide possible reasoning behind the observed results.\nFinally, we present a pipeline for intent classification and entity extraction\nwhich achieves reasonable performance (accuracy: 83.02\\%, precision: 80.82\\%,\nrecall: 83.02\\%, F1-score: 80\\%).",
          "link": "http://arxiv.org/abs/2107.05541",
          "publishedOn": "2021-07-14T01:41:50.921Z",
          "wordCount": 652,
          "title": "End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05847",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Binder_M/0/1/0/all/0/1\">Martin Binder</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Lang_M/0/1/0/all/0/1\">Michel Lang</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Pielok_T/0/1/0/all/0/1\">Tobias Pielok</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Richter_J/0/1/0/all/0/1\">Jakob Richter</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Coors_S/0/1/0/all/0/1\">Stefan Coors</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Thomas_J/0/1/0/all/0/1\">Janek Thomas</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Ullmann_T/0/1/0/all/0/1\">Theresa Ullmann</a> (2), <a href=\"http://arxiv.org/find/stat/1/au:+Becker_M/0/1/0/all/0/1\">Marc Becker</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Boulesteix_A/0/1/0/all/0/1\">Anne-Laure Boulesteix</a> (2), <a href=\"http://arxiv.org/find/stat/1/au:+Deng_D/0/1/0/all/0/1\">Difan Deng</a> (3), <a href=\"http://arxiv.org/find/stat/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a> (3) ((1) Department of Statistics, Ludwig Maximilian University Munich, (2) Institute for Medical Information Processing, Biometry and Epidemiology, Ludwig Maximilian University Munich, (3) Institute for Information Processing, Leibniz University Hannover)",
          "description": "Most machine learning algorithms are configured by one or several\nhyperparameters that must be carefully chosen and often considerably impact\nperformance. To avoid a time consuming and unreproducible manual\ntrial-and-error process to find well-performing hyperparameter configurations,\nvarious automatic hyperparameter optimization (HPO) methods, e.g., based on\nresampling error estimation for supervised machine learning, can be employed.\nAfter introducing HPO from a general perspective, this paper reviews important\nHPO methods such as grid or random search, evolutionary algorithms, Bayesian\noptimization, Hyperband and racing. It gives practical recommendations\nregarding important choices to be made when conducting HPO, including the HPO\nalgorithms themselves, performance evaluation, how to combine HPO with ML\npipelines, runtime improvements, and parallelization.",
          "link": "http://arxiv.org/abs/2107.05847",
          "publishedOn": "2021-07-14T01:41:50.914Z",
          "wordCount": 654,
          "title": "Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges. (arXiv:2107.05847v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Han Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Feng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1\">Marcus Eng Hock Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yilin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chee_M/0/1/0/all/0/1\">Marcel Lucas Chee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_S/0/1/0/all/0/1\">Seyed Ehsan Saffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1\">Hairil Rizal Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_B/0/1/0/all/0/1\">Benjamin Alan Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1\">Bibhas Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>",
          "description": "Background: Medical decision-making impacts both individual and public\nhealth. Clinical scores are commonly used among a wide variety of\ndecision-making models for determining the degree of disease deterioration at\nthe bedside. AutoScore was proposed as a useful clinical score generator based\non machine learning and a generalized linear model. Its current framework,\nhowever, still leaves room for improvement when addressing unbalanced data of\nrare events. Methods: Using machine intelligence approaches, we developed\nAutoScore-Imbalance, which comprises three components: training dataset\noptimization, sample weight optimization, and adjusted AutoScore. All scoring\nmodels were evaluated on the basis of their area under the curve (AUC) in the\nreceiver operating characteristic analysis and balanced accuracy (i.e., mean\nvalue of sensitivity and specificity). By utilizing a publicly accessible\ndataset from Beth Israel Deaconess Medical Center, we assessed the proposed\nmodel and baseline approaches in the prediction of inpatient mortality.\nResults: AutoScore-Imbalance outperformed baselines in terms of AUC and\nbalanced accuracy. The nine-variable AutoScore-Imbalance sub-model achieved the\nhighest AUC of 0.786 (0.732-0.839) while the eleven-variable original AutoScore\nobtained an AUC of 0.723 (0.663-0.783), and the logistic regression with 21\nvariables obtained an AUC of 0.743 (0.685-0.800). The AutoScore-Imbalance\nsub-model (using down-sampling algorithm) yielded an AUC of 0. 0.771\n(0.718-0.823) with only five variables, demonstrating a good balance between\nperformance and variable sparsity. Conclusions: The AutoScore-Imbalance tool\nhas the potential to be applied to highly unbalanced datasets to gain further\ninsight into rare medical events and to facilitate real-world clinical\ndecision-making.",
          "link": "http://arxiv.org/abs/2107.06039",
          "publishedOn": "2021-07-14T01:41:50.902Z",
          "wordCount": 703,
          "title": "AutoScore-Imbalance: An interpretable machine learning tool for development of clinical scores with rare events data. (arXiv:2107.06039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Derakhshani_M/0/1/0/all/0/1\">Mohammad Mahdi Derakhshani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "This paper introduces kernel continual learning, a simple but effective\nvariant of continual learning that leverages the non-parametric nature of\nkernel methods to tackle catastrophic forgetting. We deploy an episodic memory\nunit that stores a subset of samples for each task to learn task-specific\nclassifiers based on kernel ridge regression. This does not require memory\nreplay and systematically avoids task interference in the classifiers. We\nfurther introduce variational random features to learn a data-driven kernel for\neach task. To do so, we formulate kernel continual learning as a variational\ninference problem, where a random Fourier basis is incorporated as the latent\nvariable. The variational posterior distribution over the random Fourier basis\nis inferred from the coreset of each task. In this way, we are able to generate\nmore informative kernels specific to each task, and, more importantly, the\ncoreset size can be reduced to achieve more compact memory, resulting in more\nefficient continual learning based on episodic memory. Extensive evaluation on\nfour benchmarks demonstrates the effectiveness and promise of kernels for\ncontinual learning.",
          "link": "http://arxiv.org/abs/2107.05757",
          "publishedOn": "2021-07-14T01:41:50.895Z",
          "wordCount": 604,
          "title": "Kernel Continual Learning. (arXiv:2107.05757v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Riehle_D/0/1/0/all/0/1\">Dirk Riehle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harutyunyan_N/0/1/0/all/0/1\">Nikolay Harutyunyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barcomb_A/0/1/0/all/0/1\">Ann Barcomb</a>",
          "description": "Pattern discovery, the process of discovering previously unrecognized\npatterns, is often performed as an ad-hoc process with little resulting\ncertainty in the quality of the proposed patterns. Pattern validation, the\nprocess of validating the accuracy of proposed patterns, remains dominated by\nthe simple heuristic of \"the rule of three\". This article shows how to use\nestablished scientific research methods for the purpose of pattern discovery\nand validation. We present a specific approach, called the handbook method,\nthat uses the qualitative survey, action research, and case study research for\npattern discovery and evaluation, and we discuss the underlying principle of\nusing scientific methods in general. We evaluate the handbook method using\nthree exploratory studies and demonstrate its usefulness.",
          "link": "http://arxiv.org/abs/2107.06065",
          "publishedOn": "2021-07-14T01:41:50.883Z",
          "wordCount": 550,
          "title": "Pattern Discovery and Validation Using Scientific Research Methods. (arXiv:2107.06065v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05849",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1\">Avishek Ghosh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sayak Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>",
          "description": "We address the problem of model selection for the finite horizon episodic\nReinforcement Learning (RL) problem where the transition kernel $P^*$ belongs\nto a family of models $\\mathcal{P}^*$ with finite metric entropy. In the model\nselection framework, instead of $\\mathcal{P}^*$, we are given $M$ nested\nfamilies of transition kernels $\\cP_1 \\subset \\cP_2 \\subset \\ldots \\subset\n\\cP_M$. We propose and analyze a novel algorithm, namely \\emph{Adaptive\nReinforcement Learning (General)} (\\texttt{ARL-GEN}) that adapts to the\nsmallest such family where the true transition kernel $P^*$ lies.\n\\texttt{ARL-GEN} uses the Upper Confidence Reinforcement Learning\n(\\texttt{UCRL}) algorithm with value targeted regression as a blackbox and puts\na model selection module at the beginning of each epoch. Under a mild\nseparability assumption on the model classes, we show that \\texttt{ARL-GEN}\nobtains a regret of\n$\\Tilde{\\mathcal{O}}(d_{\\mathcal{E}}^*H^2+\\sqrt{d_{\\mathcal{E}}^* \\mathbb{M}^*\nH^2 T})$, with high probability, where $H$ is the horizon length, $T$ is the\ntotal number of steps, $d_{\\mathcal{E}}^*$ is the Eluder dimension and\n$\\mathbb{M}^*$ is the metric entropy corresponding to $\\mathcal{P}^*$. Note\nthat this regret scaling matches that of an oracle that knows $\\mathcal{P}^*$\nin advance. We show that the cost of model selection for \\texttt{ARL-GEN} is an\nadditive term in the regret having a weak dependence on $T$. Subsequently, we\nremove the separability assumption and consider the setup of linear mixture\nMDPs, where the transition kernel $P^*$ has a linear function approximation.\nWith this low rank structure, we propose novel adaptive algorithms for model\nselection, and obtain (order-wise) regret identical to that of an oracle with\nknowledge of the true model class.",
          "link": "http://arxiv.org/abs/2107.05849",
          "publishedOn": "2021-07-14T01:41:50.861Z",
          "wordCount": 707,
          "title": "Model Selection with Near Optimal Rates for Reinforcement Learning with General Model Classes. (arXiv:2107.05849v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilie_A/0/1/0/all/0/1\">Andrei Ilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanescu_A/0/1/0/all/0/1\">Alin Stefanescu</a>",
          "description": "Recent work has shown how easily white-box adversarial attacks can be applied\nto state-of-the-art image classifiers. However, real-life scenarios resemble\nmore the black-box adversarial conditions, lacking transparency and usually\nimposing natural, hard constraints on the query budget.\n\nWe propose $\\textbf{EvoBA}$, a black-box adversarial attack based on a\nsurprisingly simple evolutionary search strategy. $\\textbf{EvoBA}$ is\nquery-efficient, minimizes $L_0$ adversarial perturbations, and does not\nrequire any form of training.\n\n$\\textbf{EvoBA}$ shows efficiency and efficacy through results that are in\nline with much more complex state-of-the-art black-box attacks such as\n$\\textbf{AutoZOOM}$. It is more query-efficient than $\\textbf{SimBA}$, a simple\nand powerful baseline black-box attack, and has a similar level of complexity.\nTherefore, we propose it both as a new strong baseline for black-box\nadversarial attacks and as a fast and general tool for gaining empirical\ninsight into how robust image classifiers are with respect to $L_0$ adversarial\nperturbations.\n\nThere exist fast and reliable $L_2$ black-box attacks, such as\n$\\textbf{SimBA}$, and $L_{\\infty}$ black-box attacks, such as\n$\\textbf{DeepSearch}$. We propose $\\textbf{EvoBA}$ as a query-efficient $L_0$\nblack-box adversarial attack which, together with the aforementioned methods,\ncan serve as a generic tool to assess the empirical robustness of image\nclassifiers. The main advantages of such methods are that they run fast, are\nquery-efficient, and can easily be integrated in image classifiers development\npipelines.\n\nWhile our attack minimises the $L_0$ adversarial perturbation, we also report\n$L_2$, and notice that we compare favorably to the state-of-the-art $L_2$\nblack-box attack, $\\textbf{AutoZOOM}$, and of the $L_2$ strong baseline,\n$\\textbf{SimBA}$.",
          "link": "http://arxiv.org/abs/2107.05754",
          "publishedOn": "2021-07-14T01:41:50.854Z",
          "wordCount": 694,
          "title": "EvoBA: An Evolution Strategy as a Strong Baseline forBlack-Box Adversarial Attacks. (arXiv:2107.05754v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogert_K/0/1/0/all/0/1\">Kenneth Bogert</a> (University of North Carolina Asheville), <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1\">Prashant Doshi</a> (University of Georgia)",
          "description": "Robots learning from observations in the real world using inverse\nreinforcement learning (IRL) may encounter objects or agents in the\nenvironment, other than the expert, that cause nuisance observations during the\ndemonstration. These confounding elements are typically removed in\nfully-controlled environments such as virtual simulations or lab settings. When\ncomplete removal is impossible the nuisance observations must be filtered out.\nHowever, identifying the source of observations when large amounts of\nobservations are made is difficult. To address this, we present a hierarchical\nBayesian model that incorporates both the expert's and the confounding\nelements' observations thereby explicitly modeling the diverse observations a\nrobot may receive. We extend an existing IRL algorithm originally designed to\nwork under partial occlusion of the expert to consider the diverse\nobservations. In a simulated robotic sorting domain containing both occlusion\nand confounding elements, we demonstrate the model's effectiveness. In\nparticular, our technique outperforms several other comparative methods, second\nonly to having perfect knowledge of the subject's trajectory.",
          "link": "http://arxiv.org/abs/2107.05818",
          "publishedOn": "2021-07-14T01:41:50.847Z",
          "wordCount": 611,
          "title": "A Hierarchical Bayesian model for Inverse RL in Partially-Controlled Environments. (arXiv:2107.05818v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1\">Rianne van den Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalchbrenner_N/0/1/0/all/0/1\">Nal Kalchbrenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1\">Hanie Sedghi</a>",
          "description": "We focus on the problem of domain adaptation when the goal is shifting the\nmodel towards the target distribution, rather than learning domain invariant\nrepresentations. It has been shown that under the following two assumptions:\n(a) access to samples from intermediate distributions, and (b) samples being\nannotated with the amount of change from the source distribution, self-training\ncan be successfully applied on gradually shifted samples to adapt the model\ntoward the target distribution. We hypothesize having (a) is enough to enable\niterative self-training to slowly adapt the model to the target distribution,\nby making use of an implicit curriculum. In the case where (a) does not hold,\nwe observe that iterative self-training falls short. We propose GIFT, a method\nthat creates virtual samples from intermediate distributions by interpolating\nrepresentations of examples from source and target domains. We evaluate an\niterative-self-training method on datasets with natural distribution shifts,\nand show that when applied on top of other domain adaptation methods, it\nimproves the performance of the model on the target dataset. We run an analysis\non a synthetic dataset to show that in the presence of (a)\niterative-self-training naturally forms a curriculum of samples. Furthermore,\nwe show that when (a) does not hold, GIFT performs better than iterative\nself-training.",
          "link": "http://arxiv.org/abs/2106.06080",
          "publishedOn": "2021-07-14T01:41:50.840Z",
          "wordCount": 685,
          "title": "Gradual Domain Adaptation in the Wild:When Intermediate Distributions are Absent. (arXiv:2106.06080v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhuheir_M/0/1/0/all/0/1\">Marwan Dhuheir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1\">Abdullatif Albaseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1\">Emna Baccour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1\">Mohamed Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1\">Mounir Hamdi</a>",
          "description": "Recognizing the patient's emotions using deep learning techniques has\nattracted significant attention recently due to technological advancements.\nAutomatically identifying the emotions can help build smart healthcare centers\nthat can detect depression and stress among the patients in order to start the\nmedication early. Using advanced technology to identify emotions is one of the\nmost exciting topics as it defines the relationships between humans and\nmachines. Machines learned how to predict emotions by adopting various methods.\nIn this survey, we present recent research in the field of using neural\nnetworks to recognize emotions. We focus on studying emotions' recognition from\nspeech, facial expressions, and audio-visual input and show the different\ntechniques of deploying these algorithms in the real world. These three emotion\nrecognition techniques can be used as a surveillance system in healthcare\ncenters to monitor patients. We conclude the survey with a presentation of the\nchallenges and the related future work to provide an insight into the\napplications of using emotion recognition.",
          "link": "http://arxiv.org/abs/2107.05989",
          "publishedOn": "2021-07-14T01:41:50.832Z",
          "wordCount": 629,
          "title": "Emotion Recognition for Healthcare Surveillance Systems Using Neural Networks: A Survey. (arXiv:2107.05989v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "It is well noted that coordinate based MLPs benefit greatly -- in terms of\npreserving high-frequency information -- through the encoding of coordinate\npositions as an array of Fourier features. Hitherto, the rationale for the\neffectiveness of these positional encodings has been solely studied through a\nFourier lens. In this paper, we strive to broaden this understanding by showing\nthat alternative non-Fourier embedding functions can indeed be used for\npositional encoding. Moreover, we show that their performance is entirely\ndetermined by a trade-off between the stable rank of the embedded matrix and\nthe distance preservation between embedded coordinates. We further establish\nthat the now ubiquitous Fourier feature mapping of position is a special case\nthat fulfills these conditions. Consequently, we present a more general theory\nto analyze positional encoding in terms of shifted basis functions. To this\nend, we develop the necessary theoretical formulae and empirically verify that\nour theoretical claims hold in practice. Codes available at\nhttps://github.com/osiriszjq/Rethinking-positional-encoding.",
          "link": "http://arxiv.org/abs/2107.02561",
          "publishedOn": "2021-07-14T01:41:50.813Z",
          "wordCount": 603,
          "title": "Rethinking Positional Encoding. (arXiv:2107.02561v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianshen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azam_N/0/1/0/all/0/1\">Naveed Ahmed Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_K/0/1/0/all/0/1\">Kazuya Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagamochi_H/0/1/0/all/0/1\">Hiroshi Nagamochi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akutsu_T/0/1/0/all/0/1\">Tatsuya Akutsu</a>",
          "description": "Recently a novel framework has been proposed for designing the molecular\nstructure of chemical compounds using both artificial neural networks (ANNs)\nand mixed integer linear programming (MILP). In the framework, we first define\na feature vector $f(C)$ of a chemical graph $C$ and construct an ANN that maps\n$x=f(C)$ to a predicted value $\\eta(x)$ of a chemical property $\\pi$ to $C$.\nAfter this, we formulate an MILP that simulates the computation process of\n$f(C)$ from $C$ and that of $\\eta(x)$ from $x$. Given a target value $y^*$ of\nthe chemical property $\\pi$, we infer a chemical graph $C^\\dagger$ such that\n$\\eta(f(C^\\dagger))=y^*$ by solving the MILP. In this paper, we use linear\nregression to construct a prediction function $\\eta$ instead of ANNs. For this,\nwe derive an MILP formulation that simulates the computation process of a\nprediction function by linear regression. The results of computational\nexperiments suggest our method can infer chemical graphs with around up to 50\nnon-hydrogen atoms.",
          "link": "http://arxiv.org/abs/2107.02381",
          "publishedOn": "2021-07-14T01:41:50.806Z",
          "wordCount": 624,
          "title": "An Inverse QSAR Method Based on Linear Regression and Integer Programming. (arXiv:2107.02381v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiajun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Changnan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>",
          "description": "Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning\n(DRL) via combining deep learning (DL) with reinforcement learning (RL), which\nhas noticed that the distribution of the acquired data would change during the\ntraining process. DQN found this property might cause instability for training,\nso it proposed effective methods to handle the downside of the property.\nInstead of focusing on the unfavourable aspects, we find it critical for RL to\nease the gap between the estimated data distribution and the ground truth data\ndistribution while supervised learning (SL) fails to do so. From this new\nperspective, we extend the basic paradigm of RL called the Generalized Policy\nIteration (GPI) into a more generalized version, which is called the\nGeneralized Data Distribution Iteration (GDI). We see massive RL algorithms and\ntechniques can be unified into the GDI paradigm, which can be considered as one\nof the special cases of GDI. We provide theoretical proof of why GDI is better\nthan GPI and how it works. Several practical algorithms based on GDI have been\nproposed to verify the effectiveness and extensiveness of it. Empirical\nexperiments prove our state-of-the-art (SOTA) performance on Arcade Learning\nEnvironment (ALE), wherein our algorithm has achieved 9620.98% mean human\nnormalized score (HNS), 1146.39% median HNS and 22 human world record\nbreakthroughs (HWRB) using only 200 training frames. Our work aims to lead the\nRL research to step into the journey of conquering the human world records and\nseek real superhuman agents on both performance and efficiency.",
          "link": "http://arxiv.org/abs/2106.06232",
          "publishedOn": "2021-07-14T01:41:50.795Z",
          "wordCount": 728,
          "title": "GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hesen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "Data augmentation is a commonly used approach to improving the generalization\nof deep learning models. Recent works show that learned data augmentation\npolicies can achieve better generalization than hand-crafted ones. However,\nmost of these works use unified augmentation policies for all samples in a\ndataset, which is observed not necessarily beneficial for all labels in\nmulti-label classification tasks, i.e., some policies may have negative impacts\non some labels while benefitting the others. To tackle this problem, we propose\na novel Label-Based AutoAugmentation (LB-Aug) method for multi-label scenarios,\nwhere augmentation policies are generated with respect to labels by an\naugmentation-policy network. The policies are learned via reinforcement\nlearning using policy gradient methods, providing a mapping from instance\nlabels to their optimal augmentation policies. Numerical experiments show that\nour LB-Aug outperforms previous state-of-the-art augmentation methods by large\nmargins in multiple benchmarks on image and video classification.",
          "link": "http://arxiv.org/abs/2107.05384",
          "publishedOn": "2021-07-14T01:41:50.787Z",
          "wordCount": 598,
          "title": "Fine-Grained AutoAugmentation for Multi-Label Classification. (arXiv:2107.05384v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kingma_D/0/1/0/all/0/1\">Diederik P. Kingma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1\">Ben Poole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>",
          "description": "Diffusion-based generative models have demonstrated a capacity for\nperceptually impressive synthesis, but can they also be great likelihood-based\nmodels? We answer this in the affirmative, and introduce a family of\ndiffusion-based generative models that obtain state-of-the-art likelihoods on\nstandard image density estimation benchmarks. Unlike other diffusion-based\nmodels, our method allows for efficient optimization of the noise schedule\njointly with the rest of the model. We show that the variational lower bound\n(VLB) simplifies to a remarkably short expression in terms of the\nsignal-to-noise ratio of the diffused data, thereby improving our theoretical\nunderstanding of this model class. Using this insight, we prove an equivalence\nbetween several models proposed in the literature. In addition, we show that\nthe continuous-time VLB is invariant to the noise schedule, except for the\nsignal-to-noise ratio at its endpoints. This enables us to learn a noise\nschedule that minimizes the variance of the resulting VLB estimator, leading to\nfaster optimization. Combining these advances with architectural improvements,\nwe obtain state-of-the-art likelihoods on image density estimation benchmarks,\noutperforming autoregressive models that have dominated these benchmarks for\nmany years, with often significantly faster optimization. In addition, we show\nhow to turn the model into a bits-back compression scheme, and demonstrate\nlossless compression rates close to the theoretical optimum.",
          "link": "http://arxiv.org/abs/2107.00630",
          "publishedOn": "2021-07-14T01:41:50.779Z",
          "wordCount": 653,
          "title": "Variational Diffusion Models. (arXiv:2107.00630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1\">Pierre Marza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1\">Laetitia Matignon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1\">Olivier Simonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>",
          "description": "In the context of visual navigation, the capacity to map a novel environment\nis necessary for an agent to exploit its observation history in the considered\nplace and efficiently reach known goals. This ability can be associated with\nspatial reasoning, where an agent is able to perceive spatial relationships and\nregularities, and discover object affordances. In classical Reinforcement\nLearning (RL) setups, this capacity is learned from reward alone. We introduce\nsupplementary supervision in the form of auxiliary tasks designed to favor the\nemergence of spatial perception capabilities in agents trained for a\ngoal-reaching downstream objective. We show that learning to estimate metrics\nquantifying the spatial relationships between an agent at a given location and\na goal to reach has a high positive impact in Multi-Object Navigation settings.\nOur method significantly improves the performance of different baseline agents,\nthat either build an explicit or implicit representation of the environment,\neven matching the performance of incomparable oracle agents taking ground-truth\nmaps as input.",
          "link": "http://arxiv.org/abs/2107.06011",
          "publishedOn": "2021-07-14T01:41:50.760Z",
          "wordCount": 608,
          "title": "Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junhyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1\">Byungyoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Charmgil Hong</a>",
          "description": "We present Multi-Scale Label Dependence Relation Networks (MSDN), a novel\napproach to multi-label classification (MLC) using 1-dimensional convolution\nkernels to learn label dependencies at multi-scale. Modern multi-label\nclassifiers have been adopting recurrent neural networks (RNNs) as a memory\nstructure to capture and exploit label dependency relations. The RNN-based MLC\nmodels however tend to introduce a very large number of parameters that may\ncause under-/over-fitting problems. The proposed method uses the 1-dimensional\nconvolutional neural network (1D-CNN) to serve the same purpose in a more\nefficient manner. By training a model with multiple kernel sizes, the method is\nable to learn the dependency relations among labels at multiple scales, while\nit uses a drastically smaller number of parameters. With public benchmark\ndatasets, we demonstrate that our model can achieve better accuracies with much\nsmaller number of model parameters compared to RNN-based MLC models.",
          "link": "http://arxiv.org/abs/2107.05941",
          "publishedOn": "2021-07-14T01:41:50.754Z",
          "wordCount": 576,
          "title": "Multi-Scale Label Relation Learning for Multi-Label Classification Using 1-Dimensional Convolutional Neural Networks. (arXiv:2107.05941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>",
          "description": "Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.",
          "link": "http://arxiv.org/abs/2107.05768",
          "publishedOn": "2021-07-14T01:41:50.747Z",
          "wordCount": 664,
          "title": "Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+We_S/0/1/0/all/0/1\">Shi-jie We</a>",
          "description": "This paper presents the participation of the MiniTrue team in the FinSim-3\nshared task on learning semantic similarities for the financial domain in\nEnglish language. Our approach combines contextual embeddings learned by\ntransformer-based language models with network structures embeddings extracted\non external knowledge sources, to create more meaningful representations of\nfinancial domain entities and terms. For this, two BERT based language models\nand a knowledge graph embedding model are used. Besides, we propose a voting\nfunction to joint three basic models for the final inference. Experimental\nresults show that the model with the knowledge graph embeddings has achieved a\nsuperior result than these models with only contextual embeddings.\nNevertheless, we also observe that our voting function brings an extra benefit\nto the final system.",
          "link": "http://arxiv.org/abs/2107.05885",
          "publishedOn": "2021-07-14T01:41:50.741Z",
          "wordCount": 566,
          "title": "Exploiting Network Structures to Improve Semantic Representation for the Financial Domain. (arXiv:2107.05885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sumedha Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_S/0/1/0/all/0/1\">Stephen Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triantafillou_S/0/1/0/all/0/1\">Sofia Triantafillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Model explainability is essential for the creation of trustworthy Machine\nLearning models in healthcare. An ideal explanation resembles the\ndecision-making process of a domain expert and is expressed using concepts or\nterminology that is meaningful to the clinicians. To provide such an\nexplanation, we first associate the hidden units of the classifier to\nclinically relevant concepts. We take advantage of radiology reports\naccompanying the chest X-ray images to define concepts. We discover sparse\nassociations between concepts and hidden units using a linear sparse logistic\nregression. To ensure that the identified units truly influence the\nclassifier's outcome, we adopt tools from Causal Inference literature and, more\nspecifically, mediation analysis through counterfactual interventions. Finally,\nwe construct a low-depth decision tree to translate all the discovered concepts\ninto a straightforward decision rule, expressed to the radiologist. We\nevaluated our approach on a large chest x-ray dataset, where our model produces\na global explanation consistent with clinical knowledge.",
          "link": "http://arxiv.org/abs/2107.06098",
          "publishedOn": "2021-07-14T01:41:50.734Z",
          "wordCount": 595,
          "title": "Using Causal Analysis for Conceptual Deep Learning Explanation. (arXiv:2107.06098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05975",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gotkowski_K/0/1/0/all/0/1\">Karol Gotkowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bucher_A/0/1/0/all/0/1\">Andreas Bucher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischbach_R/0/1/0/all/0/1\">Ricarda Fischbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaltenborn_I/0/1/0/all/0/1\">Isabel Kaltenborn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Automatic segmentation of lung lesions in computer tomography has the\npotential to ease the burden of clinicians during the Covid-19 pandemic. Yet\npredictive deep learning models are not trusted in the clinical routine due to\nfailing silently in out-of-distribution (OOD) data. We propose a lightweight\nOOD detection method that exploits the Mahalanobis distance in the feature\nspace. The proposed approach can be seamlessly integrated into state-of-the-art\nsegmentation pipelines without requiring changes in model architecture or\ntraining procedure, and can therefore be used to assess the suitability of\npre-trained models to new data. We validate our method with a patch-based\nnnU-Net architecture trained with a multi-institutional dataset and find that\nit effectively detects samples that the model segments incorrectly.",
          "link": "http://arxiv.org/abs/2107.05975",
          "publishedOn": "2021-07-14T01:41:50.727Z",
          "wordCount": 617,
          "title": "Detecting when pre-trained nnU-Net models fail silently for Covid-19. (arXiv:2107.05975v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>",
          "description": "In this paper, we answer the question when inserting label noise (less\ninformative labels) can instead return us more accurate and fair models. We are\nprimarily inspired by two observations that 1) increasing a certain class of\ninstances' label noise to balance the noise rates (increasing-to-balancing)\nresults in an easier learning problem; 2) Increasing-to-balancing improves\nfairness guarantees against label bias. In this paper, we will first quantify\nthe trade-offs introduced by increasing a certain group of instances' label\nnoise rate w.r.t. the learning difficulties and performance guarantees. We\nanalytically demonstrate when such an increase proves to be beneficial, in\nterms of either improved generalization errors or the fairness guarantees. Then\nwe present a method to leverage our idea of inserting label noise for the task\nof learning with noisy labels, either without or with a fairness constraint.\nThe primary technical challenge we face is due to the fact that we would not\nknow which data instances are suffering from higher noise, and we would not\nhave the ground truth labels to verify any possible hypothesis. We propose a\ndetection method that informs us which group of labels might suffer from higher\nnoise, without using ground truth information. We formally establish the\neffectiveness of the proposed solution and demonstrate it with extensive\nexperiments.",
          "link": "http://arxiv.org/abs/2107.05913",
          "publishedOn": "2021-07-14T01:41:50.708Z",
          "wordCount": 652,
          "title": "Can Less be More? When Increasing-to-Balancing Label Noise Rates Considered Beneficial. (arXiv:2107.05913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Angelica Louren&#xe7;o Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1\">Marcos Eduardo Valle</a>",
          "description": "This paper presents a hybrid morphological neural network for regression\ntasks called linear dilation-erosion regression ($\\ell$-DER). In few words, an\n$\\ell$-DER model is given by a convex combination of the composition of linear\nand elementary morphological operators. As a result, they yield continuous\npiecewise linear functions and, thus, are universal approximators. Apart from\nintroducing the $\\ell$-DER models, we present three approaches for training\nthese models: one based on stochastic descent gradient and two based on the\ndifference of convex programming problems. Finally, we evaluate the performance\nof the $\\ell$-DER model using 14 regression tasks. Although the approach based\non SDG revealed faster than the other two, the $\\ell$-DER trained using a\ndisciplined convex-concave programming problem outperformed the others in terms\nof the least mean absolute error score.",
          "link": "http://arxiv.org/abs/2107.05682",
          "publishedOn": "2021-07-14T01:41:50.701Z",
          "wordCount": 597,
          "title": "Least-Squares Linear Dilation-Erosion Regressor Trained using Stochastic Descent Gradient or the Difference of Convex Methods. (arXiv:2107.05682v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1\">Dylan J. Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentile_C/0/1/0/all/0/1\">Claudio Gentile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmert_J/0/1/0/all/0/1\">Julian Zimmert</a>",
          "description": "A major research direction in contextual bandits is to develop algorithms\nthat are computationally efficient, yet support flexible, general-purpose\nfunction approximation. Algorithms based on modeling rewards have shown strong\nempirical performance, but typically require a well-specified model, and can\nfail when this assumption does not hold. Can we design algorithms that are\nefficient and flexible, yet degrade gracefully in the face of model\nmisspecification? We introduce a new family of oracle-efficient algorithms for\n$\\varepsilon$-misspecified contextual bandits that adapt to unknown model\nmisspecification -- both for finite and infinite action settings. Given access\nto an online oracle for square loss regression, our algorithm attains optimal\nregret and -- in particular -- optimal dependence on the misspecification\nlevel, with no prior knowledge. Specializing to linear contextual bandits with\ninfinite actions in $d$ dimensions, we obtain the first algorithm that achieves\nthe optimal $O(d\\sqrt{T} + \\varepsilon\\sqrt{d}T)$ regret bound for unknown\nmisspecification level $\\varepsilon$.\n\nOn a conceptual level, our results are enabled by a new optimization-based\nperspective on the regression oracle reduction framework of Foster and Rakhlin,\nwhich we anticipate will find broader use.",
          "link": "http://arxiv.org/abs/2107.05745",
          "publishedOn": "2021-07-14T01:41:50.694Z",
          "wordCount": 618,
          "title": "Adapting to Misspecification in Contextual Bandits. (arXiv:2107.05745v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hirn_J/0/1/0/all/0/1\">J. Hirn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1\">J. E. Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesinos_Navarro_A/0/1/0/all/0/1\">A. Montesinos-Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Martin_R/0/1/0/all/0/1\">R. Sanchez-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanz_V/0/1/0/all/0/1\">V. Sanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verdu_M/0/1/0/all/0/1\">M. Verd&#xfa;</a>",
          "description": "1. Deciphering coexistence patterns is a current challenge to understanding\ndiversity maintenance, especially in rich communities where the complexity of\nthese patterns is magnified through indirect interactions that prevent their\napproximation with classical experimental approaches. 2. We explore\ncutting-edge Machine Learning techniques called Generative Artificial\nIntelligence (GenAI) to decipher species coexistence patterns in vegetation\npatches, training generative adversarial networks (GAN) and variational\nAutoEncoders (VAE) that are then used to unravel some of the mechanisms behind\ncommunity assemblage. 3. The GAN accurately reproduces the species composition\nof real patches as well as the affinity of plant species to different soil\ntypes, and the VAE also reaches a high level of accuracy, above 99%. Using the\nartificially generated patches, we found that high order interactions tend to\nsuppress the positive effects of low order interactions. Finally, by\nreconstructing successional trajectories we could identify the pioneer species\nwith larger potential to generate a high diversity of distinct patches in terms\nof species composition. 4. Understanding the complexity of species coexistence\npatterns in diverse ecological communities requires new approaches beyond\nheuristic rules. Generative Artificial Intelligence can be a powerful tool to\nthis end as it allows to overcome the inherent dimensionality of this\nchallenge.",
          "link": "http://arxiv.org/abs/2107.06020",
          "publishedOn": "2021-07-14T01:41:50.687Z",
          "wordCount": 649,
          "title": "A Deep Generative Artificial Intelligence system to decipher species coexistence patterns. (arXiv:2107.06020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amor_M/0/1/0/all/0/1\">M. Ben Amor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stier_J/0/1/0/all/0/1\">J. Stier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1\">M. Granitzer</a>",
          "description": "Deep learning models have been shown to be vulnerable to adversarial attacks.\nThis perception led to analyzing deep learning models not only from the\nperspective of their performance measures but also their robustness to certain\ntypes of adversarial attacks. We take another step forward in relating the\narchitectural structure of neural networks from a graph theoretic perspective\nto their robustness. We aim to investigate any existing correlations between\ngraph theoretic properties and the robustness of Sparse Neural Networks. Our\nhypothesis is, that graph theoretic properties as a prior of neural network\nstructures are related to their robustness. To answer to this hypothesis, we\ndesigned an empirical study with neural network models obtained through random\ngraphs used as sparse structural priors for the networks. We additionally\ninvestigated the evaluation of a randomly pruned fully connected network as a\npoint of reference.\n\nWe found that robustness measures are independent of initialization methods\nbut show weak correlations with graph properties: higher graph densities\ncorrelate with lower robustness, but higher average path lengths and average\nnode eccentricities show negative correlations with robustness measures. We\nhope to motivate further empirical and analytical research to tightening an\nanswer to our hypothesis.",
          "link": "http://arxiv.org/abs/2107.06158",
          "publishedOn": "2021-07-14T01:41:50.669Z",
          "wordCount": 642,
          "title": "Correlation Analysis between the Robustness of Sparse Neural Networks and their Random Hidden Structural Priors. (arXiv:2107.06158v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuangbin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Wenwei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuxin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>",
          "description": "Logs have been an imperative resource to ensure the reliability and\ncontinuity of many software systems, especially large-scale distributed\nsystems. They faithfully record runtime information to facilitate system\ntroubleshooting and behavior understanding. Due to the large scale and\ncomplexity of modern software systems, the volume of logs has reached an\nunprecedented level. Consequently, for log-based anomaly detection,\nconventional methods of manual inspection or even traditional machine\nlearning-based methods become impractical, which serve as a catalyst for the\nrapid development of deep learning-based solutions. However, there is currently\na lack of rigorous comparison among the representative log-based anomaly\ndetectors which resort to neural network models. Moreover, the\nre-implementation process demands non-trivial efforts and bias can be easily\nintroduced. To better understand the characteristics of different anomaly\ndetectors, in this paper, we provide a comprehensive review and evaluation on\nfive popular models used by six state-of-the-art methods. Particularly, four of\nthe selected methods are unsupervised and the remaining two are supervised.\nThese methods are evaluated with two publicly-available log datasets, which\ncontain nearly 16 millions log messages and 0.4 million anomaly instances in\ntotal. We believe our work can serve as a basis in this field and contribute to\nthe future academic researches and industrial applications.",
          "link": "http://arxiv.org/abs/2107.05908",
          "publishedOn": "2021-07-14T01:41:50.663Z",
          "wordCount": 644,
          "title": "Experience Report: Deep Learning-based System Log Analysis for Anomaly Detection. (arXiv:2107.05908v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05709",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Morales_G/0/1/0/all/0/1\">Guillermo B. Morales</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Munoz_M/0/1/0/all/0/1\">Miguel A. Mu&#xf1;oz</a>",
          "description": "Shedding light onto how biological systems represent, process and store\ninformation in noisy environments is a key and challenging goal. A stimulating,\nthough controversial, hypothesis poses that operating in dynamical regimes near\nthe edge of a phase transition, i.e. at criticality or the \"edge of chaos\", can\nprovide information-processing living systems with important operational\nadvantages, creating, e.g., an optimal trade-off between robustness and\nflexibility. Here, we elaborate on a recent theoretical result, which\nestablishes that the spectrum of covariance matrices of neural networks\nrepresenting complex inputs in a robust way needs to decay as a power-law of\nthe rank, with an exponent close to unity, a result that has been indeed\nexperimentally verified in neurons of the mouse visual cortex. Aimed at\nunderstanding and mimicking these results, we construct an artificial neural\nnetwork and train it to classify images. Remarkably, we find that the best\nperformance in such a task is obtained when the network operates near the\ncritical point, at which the eigenspectrum of the covariance matrix follows the\nvery same statistics as actual neurons do. Thus, we conclude that operating\nnear criticality can also have -- besides the usually alleged virtues -- the\nadvantage of allowing for flexible, robust and efficient input representations.",
          "link": "http://arxiv.org/abs/2107.05709",
          "publishedOn": "2021-07-14T01:41:50.656Z",
          "wordCount": 662,
          "title": "Optimal input representation in neural systems at the edge of chaos. (arXiv:2107.05709v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1\">Manuel W&#xfc;thrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmaier_F/0/1/0/all/0/1\">Felix Widmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>",
          "description": "Learning data representations that are useful for various downstream tasks is\na cornerstone of artificial intelligence. While existing methods are typically\nevaluated on downstream tasks such as classification or generative image\nquality, we propose to assess representations through their usefulness in\ndownstream control tasks, such as reaching or pushing objects. By training over\n10,000 reinforcement learning policies, we extensively evaluate to what extent\ndifferent representation properties affect out-of-distribution (OOD)\ngeneralization. Finally, we demonstrate zero-shot transfer of these policies\nfrom simulation to the real world, without any domain randomization or\nfine-tuning. This paper aims to establish the first systematic characterization\nof the usefulness of learned representations for real-world OOD downstream\ntasks.",
          "link": "http://arxiv.org/abs/2107.05686",
          "publishedOn": "2021-07-14T01:41:50.650Z",
          "wordCount": 557,
          "title": "Representation Learning for Out-Of-Distribution Generalization in Reinforcement Learning. (arXiv:2107.05686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05990",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolf_T/0/1/0/all/0/1\">Tom Nuno Wolf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>",
          "description": "Prior work on diagnosing Alzheimer's disease from magnetic resonance images\nof the brain established that convolutional neural networks (CNNs) can leverage\nthe high-dimensional image information for classifying patients. However,\nlittle research focused on how these models can utilize the usually\nlow-dimensional tabular information, such as patient demographics or laboratory\nmeasurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a\ngeneral-purpose module for CNNs that dynamically rescales and shifts the\nfeature maps of a convolutional layer, conditional on a patient's tabular\nclinical information. We show that DAFT is highly effective in combining 3D\nimage and tabular information for diagnosis and time-to-dementia prediction,\nwhere it outperforms competing CNNs with a mean balanced accuracy of 0.622 and\nmean c-index of 0.748, respectively. Our extensive ablation study provides\nvaluable insights into the architectural properties of DAFT. Our implementation\nis available at https://github.com/ai-med/DAFT.",
          "link": "http://arxiv.org/abs/2107.05990",
          "publishedOn": "2021-07-14T01:41:50.643Z",
          "wordCount": 614,
          "title": "Combining 3D Image and Tabular Data via the Dynamic Affine Feature Map Transform. (arXiv:2107.05990v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Atreya Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bocquet_M/0/1/0/all/0/1\">Marc Bocquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirtzlin_T/0/1/0/all/0/1\">Tifenn Hirtzlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laborieux_A/0/1/0/all/0/1\">Axel Laborieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">Jacques-Olivier Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowak_E/0/1/0/all/0/1\">Etienne Nowak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vianello_E/0/1/0/all/0/1\">Elisa Vianello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portal_J/0/1/0/all/0/1\">Jean-Michel Portal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Querlioz_D/0/1/0/all/0/1\">Damien Querlioz</a>",
          "description": "The implementation of current deep learning training algorithms is\npower-hungry, owing to data transfer between memory and logic units.\nOxide-based RRAMs are outstanding candidates to implement in-memory computing,\nwhich is less power-intensive. Their weak RESET regime, is particularly\nattractive for learning, as it allows tuning the resistance of the devices with\nremarkable endurance. However, the resistive change behavior in this regime\nsuffers many fluctuations and is particularly challenging to model, especially\nin a way compatible with tools used for simulating deep learning. In this work,\nwe present a model of the weak RESET process in hafnium oxide RRAM and\nintegrate this model within the PyTorch deep learning framework. Validated on\nexperiments on a hybrid CMOS/RRAM technology, our model reproduces both the\nnoisy progressive behavior and the device-to-device (D2D) variability. We use\nthis tool to train Binarized Neural Networks for the MNIST handwritten digit\nrecognition task and the CIFAR-10 object classification task. We simulate our\nmodel with and without various aspects of device imperfections to understand\ntheir impact on the training process and identify that the D2D variability is\nthe most detrimental aspect. The framework can be used in the same manner for\nother types of memories to identify the device imperfections that cause the\nmost degradation, which can, in turn, be used to optimize the devices to reduce\nthe impact of these imperfections.",
          "link": "http://arxiv.org/abs/2107.06064",
          "publishedOn": "2021-07-14T01:41:50.637Z",
          "wordCount": 679,
          "title": "Model of the Weak Reset Process in HfOx Resistive Memory for Deep Learning Frameworks. (arXiv:2107.06064v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torabi_F/0/1/0/all/0/1\">Faraz Torabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>",
          "description": "A longstanding goal of artificial intelligence is to create artificial agents\ncapable of learning to perform tasks that require sequential decision making.\nImportantly, while it is the artificial agent that learns and acts, it is still\nup to humans to specify the particular task to be performed. Classical\ntask-specification approaches typically involve humans providing stationary\nreward functions or explicit demonstrations of the desired tasks. However,\nthere has recently been a great deal of research energy invested in exploring\nalternative ways in which humans may guide learning agents that may, e.g., be\nmore suitable for certain tasks or require less human effort. This survey\nprovides a high-level overview of five recent machine learning frameworks that\nprimarily rely on human guidance apart from pre-specified reward functions or\nconventional, step-by-step action demonstrations. We review the motivation,\nassumptions, and implementation of each framework, and we discuss possible\nfuture research directions.",
          "link": "http://arxiv.org/abs/2107.05825",
          "publishedOn": "2021-07-14T01:41:50.621Z",
          "wordCount": 601,
          "title": "Recent Advances in Leveraging Human Guidance for Sequential Decision-Making Tasks. (arXiv:2107.05825v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05984",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Moreno_Pino_F/0/1/0/all/0/1\">Fernando Moreno-Pino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Olmos_P/0/1/0/all/0/1\">Pablo M. Olmos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Artes_Rodriguez_A/0/1/0/all/0/1\">Antonio Art&#xe9;s-Rodr&#xed;guez</a>",
          "description": "Time series forecasting is an important problem across many domains, playing\na crucial role in multiple real-world applications. In this paper, we propose a\nforecasting architecture that combines deep autoregressive models with a\nSpectral Attention (SA) module, which merges global and local frequency domain\ninformation in the model's embedded space. By characterizing in the spectral\ndomain the embedding of the time series as occurrences of a random process, our\nmethod can identify global trends and seasonality patterns. Two spectral\nattention models, global and local to the time series, integrate this\ninformation within the forecast and perform spectral filtering to remove time\nseries's noise. The proposed architecture has a number of useful properties: it\ncan be effectively incorporated into well-know forecast architectures,\nrequiring a low number of parameters and producing interpretable results that\nimprove forecasting accuracy. We test the Spectral Attention Autoregressive\nModel (SAAM) on several well-know forecast datasets, consistently demonstrating\nthat our model compares favorably to state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.05984",
          "publishedOn": "2021-07-14T01:41:50.615Z",
          "wordCount": 588,
          "title": "Deep Autoregressive Models with Spectral Attention. (arXiv:2107.05984v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larsen_B/0/1/0/all/0/1\">Brett W. Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_N/0/1/0/all/0/1\">Nic Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>",
          "description": "A variety of recent works, spanning pruning, lottery tickets, and training\nwithin random subspaces, have shown that deep neural networks can be trained\nusing far fewer degrees of freedom than the total number of parameters. We\nexplain this phenomenon by first examining the success probability of hitting a\ntraining loss sub-level set when training within a random subspace of a given\ntraining dimensionality. We find a sharp phase transition in the success\nprobability from $0$ to $1$ as the training dimension surpasses a threshold.\nThis threshold training dimension increases as the desired final loss\ndecreases, but decreases as the initial loss decreases. We then theoretically\nexplain the origin of this phase transition, and its dependence on\ninitialization and final desired loss, in terms of precise properties of the\nhigh dimensional geometry of the loss landscape. In particular, we show via\nGordon's escape theorem, that the training dimension plus the Gaussian width of\nthe desired loss sub-level set, projected onto a unit sphere surrounding the\ninitialization, must exceed the total number of parameters for the success\nprobability to be large. In several architectures and datasets, we measure the\nthreshold training dimension as a function of initialization and demonstrate\nthat it is a small fraction of the total number of parameters, thereby\nimplying, by our theory, that successful training with so few dimensions is\npossible precisely because the Gaussian width of low loss sub-level sets is\nvery large. Moreover, this threshold training dimension provides a strong null\nmodel for assessing the efficacy of more sophisticated ways to reduce training\ndegrees of freedom, including lottery tickets as well a more optimal method we\nintroduce: lottery subspaces.",
          "link": "http://arxiv.org/abs/2107.05802",
          "publishedOn": "2021-07-14T01:41:50.608Z",
          "wordCount": 724,
          "title": "How many degrees of freedom do we need to train deep networks: a loss landscape perspective. (arXiv:2107.05802v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Dan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>",
          "description": "The Graph Convolutional Networks (GCNs) proposed by Kipf and Welling are\neffective models for semi-supervised learning, but facing the obstacle of\nover-smoothing, which will weaken the representation ability of GCNs. Recently\nsome works are proposed to tackle with above limitation by randomly perturbing\ngraph topology or feature matrix to generate data augmentations as input for\ntraining. However, these operations have to pay the price of information\nstructure integrity breaking, and inevitably sacrifice information\nstochastically from original graph. In this paper, we introduce a novel graph\nentropy definition as an quantitative index to evaluate feature information\ndiffusion among a graph. Under considerations of preserving graph entropy, we\npropose an effective strategy to generate perturbed training data using a\nstochastic mechanism but guaranteeing graph topology integrity and with only a\nsmall amount of graph entropy decaying. Extensive experiments have been\nconducted on real-world datasets and the results verify the effectiveness of\nour proposed method in improving semi-supervised node classification accuracy\ncompared with a surge of baselines. Beyond that, our proposed approach\nsignificantly enhances the robustness and generalization ability of GCNs during\nthe training process.",
          "link": "http://arxiv.org/abs/2107.06048",
          "publishedOn": "2021-07-14T01:41:50.581Z",
          "wordCount": 613,
          "title": "A Graph Data Augmentation Strategy with Entropy Preserving. (arXiv:2107.06048v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korshunova_I/0/1/0/all/0/1\">Iryna Korshunova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1\">David Stutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1\">Alexander A. Alemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiles_O/0/1/0/all/0/1\">Olivia Wiles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1\">Sven Gowal</a>",
          "description": "We study the adversarial robustness of information bottleneck models for\nclassification. Previous works showed that the robustness of models trained\nwith information bottlenecks can improve upon adversarial training. Our\nevaluation under a diverse range of white-box $l_{\\infty}$ attacks suggests\nthat information bottlenecks alone are not a strong defense strategy, and that\nprevious results were likely influenced by gradient obfuscation.",
          "link": "http://arxiv.org/abs/2107.05712",
          "publishedOn": "2021-07-14T01:41:50.574Z",
          "wordCount": 498,
          "title": "A Closer Look at the Adversarial Robustness of Information Bottleneck Models. (arXiv:2107.05712v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1\">Toshinori Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takamitsu Matsubara</a>",
          "description": "In this paper, we propose cautious policy programming (CPP), a novel\nvalue-based reinforcement learning (RL) algorithm that can ensure monotonic\npolicy improvement during learning. Based on the nature of entropy-regularized\nRL, we derive a new entropy regularization-aware lower bound of policy\nimprovement that only requires estimating the expected policy advantage\nfunction. CPP leverages this lower bound as a criterion for adjusting the\ndegree of a policy update for alleviating policy oscillation. Different from\nsimilar algorithms that are mostly theory-oriented, we also propose a novel\ninterpolation scheme that makes CPP better scale in high dimensional control\nproblems. We demonstrate that the proposed algorithm can trade o? performance\nand stability in both didactic classic control problems and challenging\nhigh-dimensional Atari games.",
          "link": "http://arxiv.org/abs/2107.05798",
          "publishedOn": "2021-07-14T01:41:50.568Z",
          "wordCount": 571,
          "title": "Cautious Policy Programming: Exploiting KL Regularization in Monotonic Policy Improvement for Reinforcement Learning. (arXiv:2107.05798v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05719",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhao_S/0/1/0/all/0/1\">Shengjia Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kim_M/0/1/0/all/0/1\">Michael P. Kim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sahoo_R/0/1/0/all/0/1\">Roshni Sahoo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "When facing uncertainty, decision-makers want predictions they can trust. A\nmachine learning provider can convey confidence to decision-makers by\nguaranteeing their predictions are distribution calibrated -- amongst the\ninputs that receive a predicted class probabilities vector $q$, the actual\ndistribution over classes is $q$. For multi-class prediction problems, however,\nachieving distribution calibration tends to be infeasible, requiring sample\ncomplexity exponential in the number of classes $C$. In this work, we introduce\na new notion -- \\emph{decision calibration} -- that requires the predicted\ndistribution and true distribution to be ``indistinguishable'' to a set of\ndownstream decision-makers. When all possible decision makers are under\nconsideration, decision calibration is the same as distribution calibration.\nHowever, when we only consider decision makers choosing between a bounded\nnumber of actions (e.g. polynomial in $C$), our main result shows that\ndecisions calibration becomes feasible -- we design a recalibration algorithm\nthat requires sample complexity polynomial in the number of actions and the\nnumber of classes. We validate our recalibration algorithm empirically:\ncompared to existing methods, decision calibration improves decision-making on\nskin lesion and ImageNet classification with modern neural network predictors.",
          "link": "http://arxiv.org/abs/2107.05719",
          "publishedOn": "2021-07-14T01:41:50.562Z",
          "wordCount": 628,
          "title": "Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration. (arXiv:2107.05719v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>",
          "description": "Deep neural networks suffer from catastrophic forgetting when learning\nmultiple knowledge sequentially, and a growing number of approaches have been\nproposed to mitigate this problem. Some of these methods achieved considerable\nperformance by associating the flat local minima with forgetting mitigation in\ncontinual learning. However, they inevitably need (1) tedious hyperparameters\ntuning, and (2) additional computational cost. To alleviate these problems, in\nthis paper, we propose a simple yet effective optimization method, called\nAlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we\nconduct gradient descent and ascent alternatively when the network tends to\nconverge at each session of learning new knowledge. Moreover, we theoretically\nprove that such a strategy can encourage the optimization to converge to a flat\nminima. We verify AlterSGD on continual learning benchmark for semantic\nsegmentation and the empirical results show that we can significantly mitigate\nthe forgetting and outperform the state-of-the-art methods with a large margin\nunder challenging continual learning protocols.",
          "link": "http://arxiv.org/abs/2107.05804",
          "publishedOn": "2021-07-14T01:41:50.556Z",
          "wordCount": 605,
          "title": "AlterSGD: Finding Flat Minima for Continual Learning by Alternative Training. (arXiv:2107.05804v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05991",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gholipoor_N/0/1/0/all/0/1\">Narges Gholipoor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nouruzi_A/0/1/0/all/0/1\">Ali Nouruzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salarhosseini_S/0/1/0/all/0/1\">Shima Salarhosseini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Javan_M/0/1/0/all/0/1\">Mohammad Reza Javan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mokari_N/0/1/0/all/0/1\">Nader Mokari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorswieck_E/0/1/0/all/0/1\">Eduard A. Jorswieck</a>",
          "description": "In this paper, we propose a joint radio and core resource allocation\nframework for NFV-enabled networks. In the proposed system model, the goal is\nto maximize energy efficiency (EE), by guaranteeing end-to-end (E2E) quality of\nservice (QoS) for different service types. To this end, we formulate an\noptimization problem in which power and spectrum resources are allocated in the\nradio part. In the core part, the chaining, placement, and scheduling of\nfunctions are performed to ensure the QoS of all users. This joint optimization\nproblem is modeled as a Markov decision process (MDP), considering time-varying\ncharacteristics of the available resources and wireless channels. A soft\nactor-critic deep reinforcement learning (SAC-DRL) algorithm based on the\nmaximum entropy framework is subsequently utilized to solve the above MDP.\nNumerical results reveal that the proposed joint approach based on the SAC-DRL\nalgorithm could significantly reduce energy consumption compared to the case in\nwhich R-RA and NFV-RA problems are optimized separately.",
          "link": "http://arxiv.org/abs/2107.05991",
          "publishedOn": "2021-07-14T01:41:50.549Z",
          "wordCount": 623,
          "title": "Learning based E2E Energy Efficient in Joint Radio and NFV Resource Allocation for 5G and Beyond Networks. (arXiv:2107.05991v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1\">Umang Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_I/0/1/0/all/0/1\">Isabel Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Bilal Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "As the complexity of machine learning (ML) models increases, resulting in a\nlack of prediction explainability, several methods have been developed to\nexplain a model's behavior in terms of the training data points that most\ninfluence the model. However, these methods tend to mark outliers as highly\ninfluential points, limiting the insights that practitioners can draw from\npoints that are not representative of the training data. In this work, we take\na step towards finding influential training points that also represent the\ntraining data well. We first review methods for assigning importance scores to\ntraining points. Given importance scores, we propose a method to select a set\nof DIVerse INfluEntial (DIVINE) training points as a useful explanation of\nmodel behavior. As practitioners might not only be interested in finding data\npoints influential with respect to model accuracy, but also with respect to\nother important metrics, we show how to evaluate training data points on the\nbasis of group fairness. Our method can identify unfairness-inducing training\npoints, which can be removed to improve fairness outcomes. Our quantitative\nexperiments and user studies show that visualizing DIVINE points helps\npractitioners understand and explain model behavior better than earlier\napproaches.",
          "link": "http://arxiv.org/abs/2107.05978",
          "publishedOn": "2021-07-14T01:41:50.531Z",
          "wordCount": 647,
          "title": "DIVINE: Diverse Influential Training Points for Data Visualization and Model Refinement. (arXiv:2107.05978v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>",
          "description": "Active learning is the iterative construction of a classification model\nthrough targeted labeling, enabling significant labeling cost savings. As most\nresearch on active learning has been carried out before transformer-based\nlanguage models (\"transformers\") became popular, despite its practical\nimportance, comparably few papers have investigated how transformers can be\ncombined with active learning to date. This can be attributed to the fact that\nusing state-of-the-art query strategies for transformers induces a prohibitive\nruntime overhead, which effectively cancels out, or even outweighs\naforementioned cost savings. In this paper, we revisit uncertainty-based query\nstrategies, which had been largely outperformed before, but are particularly\nsuited in the context of fine-tuning transformers. In an extensive evaluation\non five widely used text classification benchmarks, we show that considerable\nimprovements of up to 14.4 percentage points in area under the learning curve\nare achieved, as well as a final accuracy close to the state of the art for all\nbut one benchmark, using only between 0.4% and 15% of the training data.",
          "link": "http://arxiv.org/abs/2107.05687",
          "publishedOn": "2021-07-14T01:41:50.525Z",
          "wordCount": 599,
          "title": "Uncertainty-based Query Strategies for Active Learning with Transformers. (arXiv:2107.05687v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1\">Yicheng Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1\">Xaq Pitkow</a>",
          "description": "Probabilistic graphical models provide a powerful tool to describe complex\nstatistical structure, with many real-world applications in science and\nengineering from controlling robotic arms to understanding neuronal\ncomputations. A major challenge for these graphical models is that inferences\nsuch as marginalization are intractable for general graphs. These inferences\nare often approximated by a distributed message-passing algorithm such as\nBelief Propagation, which does not always perform well on graphs with cycles,\nnor can it always be easily specified for complex continuous probability\ndistributions. Such difficulties arise frequently in expressive graphical\nmodels that include intractable higher-order interactions. In this paper we\nconstruct iterative message-passing algorithms using Graph Neural Networks\ndefined on factor graphs to achieve fast approximate inference on graphical\nmodels that involve many-variable interactions. Experimental results on several\nfamilies of graphical models demonstrate the out-of-distribution generalization\ncapability of our method to different sized graphs, and indicate the domain in\nwhich our method gains advantage over Belief Propagation.",
          "link": "http://arxiv.org/abs/2107.05729",
          "publishedOn": "2021-07-14T01:41:50.518Z",
          "wordCount": 594,
          "title": "Generalization of graph network inferences in higher-order probabilistic graphical models. (arXiv:2107.05729v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>",
          "description": "The Jeffreys divergence is a renown symmetrization of the statistical\nKullback-Leibler divergence which is often used in machine learning, signal\nprocessing, and information sciences. Since the Jeffreys divergence between the\nubiquitous Gaussian Mixture Models are not available in closed-form, many\ntechniques with various pros and cons have been proposed in the literature to\neither (i) estimate, (ii) approximate, or (iii) lower and upper bound this\ndivergence. In this work, we propose a simple yet fast heuristic to approximate\nthe Jeffreys divergence between two GMMs of arbitrary number of components. The\nheuristic relies on converting GMMs into pairs of dually parameterized\nprobability densities belonging to exponential families. In particular, we\nconsider Polynomial Exponential Densities, and design a goodness-of-fit\ncriterion to measure the dissimilarity between a GMM and a PED which is a\ngeneralization of the Hyv\\\"arinen divergence. This criterion allows one to\nselect the orders of the PEDs to approximate the GMMs. We demonstrate\nexperimentally that the computational time of our heuristic improves over the\nstochastic Monte Carlo estimation baseline by several orders of magnitude while\napproximating reasonably well the Jeffreys divergence, specially when the\nunivariate mixtures have a small number of modes.",
          "link": "http://arxiv.org/abs/2107.05901",
          "publishedOn": "2021-07-14T01:41:50.501Z",
          "wordCount": 639,
          "title": "Fast approximations of the Jeffreys divergence between univariate Gaussian mixture models via exponential polynomial densities. (arXiv:2107.05901v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Osa_T/0/1/0/all/0/1\">Takayuki Osa</a>",
          "description": "The objective function used in trajectory optimization is often non-convex\nand can have an infinite set of local optima. In such cases, there are diverse\nsolutions to perform a given task. Although there are a few methods to find\nmultiple solutions for motion planning, they are limited to generating a finite\nset of solutions. To address this issue, we presents an optimization method\nthat learns an infinite set of solutions in trajectory optimization. In our\nframework, diverse solutions are obtained by learning latent representations of\nsolutions. Our approach can be interpreted as training a deep generative model\nof collision-free trajectories for motion planning. The experimental results\nindicate that the trained model represents an infinite set of homotopic\nsolutions for motion planning problems.",
          "link": "http://arxiv.org/abs/2107.05842",
          "publishedOn": "2021-07-14T01:41:50.495Z",
          "wordCount": 564,
          "title": "Motion Planning by Learning the Solution Manifold in Trajectory Optimization. (arXiv:2107.05842v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chiheon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Saehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwoong Kim</a>",
          "description": "Large-batch training has been essential in leveraging large-scale datasets\nand models in deep learning. While it is computationally beneficial to use\nlarge batch sizes, it often requires a specially designed learning rate (LR)\nschedule to achieve a comparable level of performance as in smaller batch\ntraining. Especially, when the number of training epochs is constrained, the\nuse of a large LR and a warmup strategy is critical in the final performance of\nlarge-batch training due to the reduced number of updating steps. In this work,\nwe propose an automated LR scheduling algorithm which is effective for neural\nnetwork training with a large batch size under the given epoch budget. In\nspecific, the whole schedule consists of two phases: adaptive warmup and\npredefined decay, where the LR is increased until the training loss no longer\ndecreases and decreased to zero until the end of training. Here, whether the\ntraining loss has reached the minimum value is robustly checked with Gaussian\nprocess smoothing in an online manner with a low computational burden. Coupled\nwith adaptive stochastic optimizers such as AdamP and LAMB, the proposed\nscheduler successfully adjusts the LRs without cumbersome hyperparameter tuning\nand achieves comparable or better performances than tuned baselines on various\nimage classification benchmarks and architectures with a wide range of batch\nsizes.",
          "link": "http://arxiv.org/abs/2107.05855",
          "publishedOn": "2021-07-14T01:41:50.489Z",
          "wordCount": 659,
          "title": "Automated Learning Rate Scheduler for Large-batch Training. (arXiv:2107.05855v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parsaeefard_S/0/1/0/all/0/1\">Saeedeh Parsaeefard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_Garcia_A/0/1/0/all/0/1\">Alberto Leon-Garcia</a>",
          "description": "6G networks will greatly expand the support for data-oriented, autonomous\napplications for over the top (OTT) and networking use cases. The success of\nthese use cases will depend on the availability of big data sets which is not\npractical in many real scenarios due to the highly dynamic behavior of systems\nand the cost of data collection procedures. Transfer learning (TL) is a\npromising approach to deal with these challenges through the sharing of\nknowledge among diverse learning algorithms. with TL, the learning rate and\nlearning accuracy can be considerably improved. However, there are\nimplementation challenges to efficiently deploy and utilize TL in 6G. In this\npaper, we initiate this discussion by providing some performance metrics to\nmeasure the TL success. Then, we show how infrastructure, application,\nmanagement, and training planes of 6G can be adapted to handle TL. We provide\nexamples of TL in 6G and highlight the spatio-temporal features of data in 6G\nthat can lead to efficient TL. By simulation results, we demonstrate how\ntransferring the quantized neural network weights between two use cases can\nmake a trade-off between overheads and performance and attain more efficient TL\nin 6G. We also provide a list of future research directions in TL for 6G.",
          "link": "http://arxiv.org/abs/2107.05728",
          "publishedOn": "2021-07-14T01:41:50.482Z",
          "wordCount": 634,
          "title": "Toward Efficient Transfer Learning in 6G. (arXiv:2107.05728v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05707",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Herath_S/0/1/0/all/0/1\">Sumudu Herath</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xiao_X/0/1/0/all/0/1\">Xiao Xiao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cirak_F/0/1/0/all/0/1\">Fehmi Cirak</a>",
          "description": "Knitting is an effective technique for producing complex three-dimensional\nsurfaces owing to the inherent flexibility of interlooped yarns and recent\nadvances in manufacturing providing better control of local stitch patterns.\nFully yarn-level modelling of large-scale knitted membranes is not feasible.\nTherefore, we consider a two-scale homogenisation approach and model the\nmembrane as a Kirchhoff-Love shell on the macroscale and as Euler-Bernoulli\nrods on the microscale. The governing equations for both the shell and the rod\nare discretised with cubic B-spline basis functions. The solution of the\nnonlinear microscale problem requires a significant amount of time due to the\nlarge deformations and the enforcement of contact constraints, rendering\nconventional online computational homogenisation approaches infeasible. To\nsidestep this problem, we use a pre-trained statistical Gaussian Process\nRegression (GPR) model to map the macroscale deformations to macroscale\nstresses. During the offline learning phase, the GPR model is trained by\nsolving the microscale problem for a sufficiently rich set of deformation\nstates obtained by either uniform or Sobol sampling. The trained GPR model\nencodes the nonlinearities and anisotropies present in the microscale and\nserves as a material model for the macroscale Kirchhoff-Love shell. After\nverifying and validating the different components of the proposed approach, we\nintroduce several examples involving membranes subjected to tension and shear\nto demonstrate its versatility and good performance.",
          "link": "http://arxiv.org/abs/2107.05707",
          "publishedOn": "2021-07-14T01:41:50.476Z",
          "wordCount": 656,
          "title": "Computational modelling and data-driven homogenisation of knitted membranes. (arXiv:2107.05707v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1\">Rodrigo Castellon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We demonstrate that language models pre-trained on codified\n(discretely-encoded) music audio learn representations that are useful for\ndownstream MIR tasks. Specifically, we explore representations from Jukebox\n(Dhariwal et al. 2020): a music generation system containing a language model\ntrained on codified audio from 1M songs. To determine if Jukebox's\nrepresentations contain useful information for MIR, we use them as input\nfeatures to train shallow models on several MIR tasks. Relative to\nrepresentations from conventional MIR models which are pre-trained on tagging,\nwe find that using representations from Jukebox as input features yields 30%\nstronger performance on average across four MIR tasks: tagging, genre\nclassification, emotion recognition, and key detection. For key detection, we\nobserve that representations from Jukebox are considerably stronger than those\nfrom models pre-trained on tagging, suggesting that pre-training via codified\naudio language modeling may address blind spots in conventional approaches. We\ninterpret the strength of Jukebox's representations as evidence that modeling\naudio instead of tags provides richer representations for MIR.",
          "link": "http://arxiv.org/abs/2107.05677",
          "publishedOn": "2021-07-14T01:41:50.461Z",
          "wordCount": 621,
          "title": "Codified audio language modeling learns useful representations for music information retrieval. (arXiv:2107.05677v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1\">Ali Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1\">Madhusudhanan Balasubramanian</a>",
          "description": "Dense optical flow estimation is challenging when there are large\ndisplacements in a scene with heterogeneous motion dynamics, occlusion, and\nscene homogeneity. Traditional approaches to handle these challenges include\nhierarchical and multiresolution processing methods. Learning-based optical\nflow methods typically use a multiresolution approach with image warping when a\nbroad range of flow velocities and heterogeneous motion is present. Accuracy of\nsuch coarse-to-fine methods is affected by the ghosting artifacts when images\nare warped across multiple resolutions and by the vanishing problem in smaller\nscene extents with higher motion contrast. Previously, we devised strategies\nfor building compact dense prediction networks guided by the effective\nreceptive field (ERF) characteristics of the network (DDCNet). The DDCNet\ndesign was intentionally simple and compact allowing it to be used as a\nbuilding block for designing more complex yet compact networks. In this work,\nwe extend the DDCNet strategies to handle heterogeneous motion dynamics by\ncascading DDCNet based sub-nets with decreasing extents of their ERF. Our\nDDCNet with multiresolution capability (DDCNet-Multires) is compact without any\nspecialized network layers. We evaluate the performance of the DDCNet-Multires\nnetwork using standard optical flow benchmark datasets. Our experiments\ndemonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and\nprovides optical flow estimates with accuracy comparable to similar lightweight\nlearning-based methods.",
          "link": "http://arxiv.org/abs/2107.05634",
          "publishedOn": "2021-07-14T01:41:50.421Z",
          "wordCount": 664,
          "title": "DDCNet-Multires: Effective Receptive Field Guided Multiresolution CNN for Dense Prediction. (arXiv:2107.05634v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toichkin_D/0/1/0/all/0/1\">Dmitry Toichkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1\">Yansong Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qinghai Guo</a>",
          "description": "State-of-the-art artificial neural networks (ANNs) require labelled data or\nfeedback between layers, are often biologically implausible, and are vulnerable\nto adversarial attacks that humans are not susceptible to. On the other hand,\nHebbian learning in winner-take-all (WTA) networks, is unsupervised,\nfeed-forward, and biologically plausible. However, an objective optimization\ntheory for WTA networks has been missing, except under very limiting\nassumptions. Here we derive formally such a theory, based on biologically\nplausible but generic ANN elements. Through Hebbian learning, network\nparameters maintain a Bayesian generative model of the data. There is no\nsupervisory loss function, but the network does minimize cross-entropy between\nits activations and the input distribution. The key is a \"soft\" WTA where there\nis no absolute \"hard\" winner neuron, and a specific type of Hebbian-like\nplasticity of weights and biases. We confirm our theory in practice, where, in\nhandwritten digit (MNIST) recognition, our Hebbian algorithm, SoftHebb,\nminimizes cross-entropy without having access to it, and outperforms the more\nfrequently used, hard-WTA-based method. Strikingly, it even outperforms\nsupervised end-to-end backpropagation, under certain conditions. Specifically,\nin a two-layered network, SoftHebb outperforms backpropagation when the\ntraining dataset is only presented once, when the testing data is noisy, and\nunder gradient-based adversarial attacks. Adversarial attacks that confuse\nSoftHebb are also confusing to the human eye. Finally, the model can generate\ninterpolations of objects from its input distribution.",
          "link": "http://arxiv.org/abs/2107.05747",
          "publishedOn": "2021-07-14T01:41:50.403Z",
          "wordCount": 679,
          "title": "SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all networks. (arXiv:2107.05747v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05675",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gunlu_O/0/1/0/all/0/1\">Onur G&#xfc;nl&#xfc;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaefer_R/0/1/0/all/0/1\">Rafael F. Schaefer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "We consider a secret key agreement problem in which noisy physical unclonable\nfunction (PUF) outputs facilitate reliable, secure, and private key agreement\nwith the help of public, noiseless, and authenticated storage. PUF outputs are\nhighly correlated, so transform coding methods have been combined with scalar\nquantizers to extract uncorrelated bit sequences with reliability guarantees.\nFor PUF circuits with continuous-valued outputs, the models for transformed\noutputs are made more realistic by replacing the fitted distributions with\ncorresponding truncated ones. The state-of-the-art PUF methods that provide\nreliability guarantees to each extracted bit are shown to be inadequate to\nguarantee the same reliability level for all PUF outputs. Thus, a quality of\nservice parameter is introduced to control the percentage of PUF outputs for\nwhich a target reliability level can be guaranteed. A public ring oscillator\n(RO) output dataset is used to illustrate that a truncated Gaussian\ndistribution can be fitted to transformed RO outputs that are inputs to uniform\nscalar quantizers such that reliability guarantees can be provided for each bit\nextracted from any PUF device under additive Gaussian noise components by\neliminating a small subset of PUF outputs. Furthermore, we conversely show that\nit is not possible to provide such reliability guarantees without eliminating\nany PUF output if no extra secrecy and privacy leakage is allowed.",
          "link": "http://arxiv.org/abs/2107.05675",
          "publishedOn": "2021-07-14T01:41:50.350Z",
          "wordCount": 669,
          "title": "Quality of Service Guarantees for Physical Unclonable Functions. (arXiv:2107.05675v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahiner_A/0/1/0/all/0/1\">Arda Sahiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1\">Tolga Ergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1\">Batu Ozturkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartan_B/0/1/0/all/0/1\">Burak Bartan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1\">Morteza Mardani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>",
          "description": "Generative Adversarial Networks (GANs) are commonly used for modeling complex\ndistributions of data. Both the generators and discriminators of GANs are often\nmodeled by neural networks, posing a non-transparent optimization problem which\nis non-convex and non-concave over the generator and discriminator,\nrespectively. Such networks are often heuristically optimized with gradient\ndescent-ascent (GDA), but it is unclear whether the optimization problem\ncontains any saddle points, or whether heuristic methods can find them in\npractice. In this work, we analyze the training of Wasserstein GANs with\ntwo-layer neural network discriminators through the lens of convex duality, and\nfor a variety of generators expose the conditions under which Wasserstein GANs\ncan be solved exactly with convex optimization approaches, or can be\nrepresented as convex-concave games. Using this convex duality interpretation,\nwe further demonstrate the impact of different activation functions of the\ndiscriminator. Our observations are verified with numerical results\ndemonstrating the power of the convex interpretation, with applications in\nprogressive training of convex architectures corresponding to linear generators\nand quadratic-activation discriminators for CelebA image generation. The code\nfor our experiments is available at https://github.com/ardasahiner/ProCoGAN.",
          "link": "http://arxiv.org/abs/2107.05680",
          "publishedOn": "2021-07-14T01:41:50.330Z",
          "wordCount": 662,
          "title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naylor_M/0/1/0/all/0/1\">Mitchell Naylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+French_C/0/1/0/all/0/1\">Christi French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terker_S/0/1/0/all/0/1\">Samantha Terker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_U/0/1/0/all/0/1\">Uday Kamath</a>",
          "description": "The healthcare domain is one of the most exciting application areas for\nmachine learning, but a lack of model transparency contributes to a lag in\nadoption within the industry. In this work, we explore the current art of\nexplainability and interpretability within a case study in clinical text\nclassification, using a task of mortality prediction within MIMIC-III clinical\nnotes. We demonstrate various visualization techniques for fully interpretable\nmethods as well as model-agnostic post hoc attributions, and we provide a\ngeneralized method for evaluating the quality of explanations using infidelity\nand local Lipschitz across model types from logistic regression to BERT\nvariants. With these metrics, we introduce a framework through which\npractitioners and researchers can assess the frontier between a model's\npredictive performance and the quality of its available explanations. We make\nour code available to encourage continued refinement of these methods.",
          "link": "http://arxiv.org/abs/2107.05693",
          "publishedOn": "2021-07-14T01:41:50.323Z",
          "wordCount": 598,
          "title": "Quantifying Explainability in NLP and Analyzing Algorithms for Performance-Explainability Tradeoff. (arXiv:2107.05693v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05630",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dinsdale_N/0/1/0/all/0/1\">Nicola K Dinsdale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bluemke_E/0/1/0/all/0/1\">Emma Bluemke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sundaresan_V/0/1/0/all/0/1\">Vaanathi Sundaresan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smith_S/0/1/0/all/0/1\">Stephen Smith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namburete_A/0/1/0/all/0/1\">Ana IL Namburete</a>",
          "description": "The combination of deep learning image analysis methods and large-scale\nimaging datasets offers many opportunities to imaging neuroscience and\nepidemiology. However, despite the success of deep learning when applied to\nmany neuroimaging tasks, there remain barriers to the clinical translation of\nlarge-scale datasets and processing tools. Here, we explore the main challenges\nand the approaches that have been explored to overcome them. We focus on issues\nrelating to data availability, interpretability, evaluation and logistical\nchallenges, and discuss the challenges we believe are still to be overcome to\nenable the full success of big data deep learning approaches to be experienced\noutside of the research field.",
          "link": "http://arxiv.org/abs/2107.05630",
          "publishedOn": "2021-07-14T01:41:50.309Z",
          "wordCount": 560,
          "title": "Challenges for machine learning in clinical translation of big data imaging studies. (arXiv:2107.05630v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiangzhu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chonghui Guo</a>",
          "description": "In recent years, multi-view learning technologies for various applications\nhave attracted a surge of interest. Due to more compatible and complementary\ninformation from multiple views, existing multi-view methods could achieve more\npromising performance than conventional single-view methods in most situations.\nHowever, there are still no sufficient researches on the unified framework in\nexisting multi-view works. Meanwhile, how to efficiently integrate multi-view\ninformation is still full of challenges. In this paper, we propose a novel\nmulti-view learning framework, which aims to leverage most existing graph\nembedding works into a unified formula via introducing the graph consensus\nterm. In particular, our method explores the graph structure in each view\nindependently to preserve the diversity property of graph embedding methods.\nMeanwhile, we choose heterogeneous graphs to construct the graph consensus term\nto explore the correlations among multiple views jointly. To this end, the\ndiversity and complementary information among different views could be\nsimultaneously considered. Furthermore, the proposed framework is utilized to\nimplement the multi-view extension of Locality Linear Embedding, named\nMulti-view Locality Linear Embedding (MvLLE), which could be efficiently solved\nby applying the alternating optimization strategy. Empirical validations\nconducted on six benchmark datasets can show the effectiveness of our proposed\nmethod.",
          "link": "http://arxiv.org/abs/2105.11781",
          "publishedOn": "2021-07-13T01:59:37.467Z",
          "wordCount": 659,
          "title": "A unified framework based on graph consensus term for multi-view learning. (arXiv:2105.11781v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shichen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_K/0/1/0/all/0/1\">Kenric P. Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kon_M/0/1/0/all/0/1\">Mark A. Kon</a>",
          "description": "We present a coupled Variational Auto-Encoder (VAE) method that improves the\naccuracy and robustness of the probabilistic inferences on represented data.\nThe new method models the dependency between input feature vectors (images) and\nweighs the outliers with a higher penalty by generalizing the original loss\nfunction to the coupled entropy function, using the principles of nonlinear\nstatistical coupling. We evaluate the performance of the coupled VAE model\nusing the MNIST dataset. Compared with the traditional VAE algorithm, the\noutput images generated by the coupled VAE method are clearer and less blurry.\nThe visualization of the input images embedded in 2D latent variable space\nprovides a deeper insight into the structure of new model with coupled loss\nfunction: the latent variable has a smaller deviation and a more compact latent\nspace generates the output values. We analyze the histogram of the likelihoods\nof the input images using the generalized mean, which measures the model's\naccuracy as a function of the relative risk. The neutral accuracy, which is the\ngeometric mean and is consistent with a measure of the Shannon cross-entropy,\nis improved. The robust accuracy, measured by the -2/3 generalized mean, is\nalso improved.",
          "link": "http://arxiv.org/abs/1906.00536",
          "publishedOn": "2021-07-13T01:59:37.461Z",
          "wordCount": 688,
          "title": "Coupled VAE: Improved Accuracy and Robustness of a Variational Autoencoder. (arXiv:1906.00536v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Che Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>",
          "description": "There are two major classes of natural language grammar -- the dependency\ngrammar that models one-to-one correspondences between words and the\nconstituency grammar that models the assembly of one or several corresponded\nwords. While previous unsupervised parsing methods mostly focus on only\ninducing one class of grammars, we introduce a novel model, StructFormer, that\ncan simultaneously induce dependency and constituency structure. To achieve\nthis, we propose a new parsing framework that can jointly generate a\nconstituency tree and dependency graph. Then we integrate the induced\ndependency relations into the transformer, in a differentiable manner, through\na novel dependency-constrained self-attention mechanism. Experimental results\nshow that our model can achieve strong results on unsupervised constituency\nparsing, unsupervised dependency parsing, and masked language modeling at the\nsame time.",
          "link": "http://arxiv.org/abs/2012.00857",
          "publishedOn": "2021-07-13T01:59:37.448Z",
          "wordCount": 626,
          "title": "StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling. (arXiv:2012.00857v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07405",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1\">Wu Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>",
          "description": "Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank\ncovariances) is computationally challenging due to difficult Fisher-matrix\ncomputations. We address this issue by using \\emph{local-parameter coordinates}\nto obtain a flexible and efficient NGD method that works well for a\nwide-variety of structured parameterizations. We show four applications where\nour method (1) generalizes the exponential natural evolutionary strategy, (2)\nrecovers existing Newton-like algorithms, (3) yields new structured\nsecond-order algorithms, and (4) gives new algorithms to learn covariances of\nGaussian and Wishart-based distributions. We show results on a range of\nproblems from deep learning, variational inference, and evolution strategies.\nOur work opens a new direction for scalable structured geometric methods.",
          "link": "http://arxiv.org/abs/2102.07405",
          "publishedOn": "2021-07-13T01:59:37.442Z",
          "wordCount": 604,
          "title": "Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v6 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_D/0/1/0/all/0/1\">Debmalya Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medya_S/0/1/0/all/0/1\">Sourav Medya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzzi_B/0/1/0/all/0/1\">Brian Uzzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>",
          "description": "Graph Neural Networks (GNNs), a generalization of deep neural networks on\ngraph data have been widely used in various domains, ranging from drug\ndiscovery to recommender systems. However, GNNs on such applications are\nlimited when there are few available samples. Meta-learning has been an\nimportant framework to address the lack of samples in machine learning, and in\nrecent years, researchers have started to apply meta-learning to GNNs. In this\nwork, we provide a comprehensive survey of different meta-learning approaches\ninvolving GNNs on various graph problems showing the power of using these two\napproaches together. We categorize the literature based on proposed\narchitectures, shared representations, and applications. Finally, we discuss\nseveral exciting future research directions and open problems.",
          "link": "http://arxiv.org/abs/2103.00137",
          "publishedOn": "2021-07-13T01:59:37.436Z",
          "wordCount": 579,
          "title": "Meta-Learning with Graph Neural Networks: Methods and Applications. (arXiv:2103.00137v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanhua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiwen Xu</a>",
          "description": "Content feed, a type of product that recommends a sequence of items for users\nto browse and engage with, has gained tremendous popularity among social media\nplatforms. In this paper, we propose to study the diversity problem in such a\nscenario from an item sequence perspective using time series analysis\ntechniques. We derive a method called sliding spectrum decomposition (SSD) that\ncaptures users' perception of diversity in browsing a long item sequence. We\nalso share our experiences in designing and implementing a suitable item\nembedding method for accurate similarity measurement under long tail effect.\nCombined together, they are now fully implemented and deployed in Xiaohongshu\nApp's production recommender system that serves the main Explore Feed product\nfor tens of millions of users every day. We demonstrate the effectiveness and\nefficiency of the method through theoretical analysis, offline experiments and\nonline A/B tests.",
          "link": "http://arxiv.org/abs/2107.05204",
          "publishedOn": "2021-07-13T01:59:37.430Z",
          "wordCount": 596,
          "title": "Sliding Spectrum Decomposition for Diversified Recommendation. (arXiv:2107.05204v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+George_B/0/1/0/all/0/1\">Blessen George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K. Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>",
          "description": "Generative adversarial networks (GANs) are very popular to generate realistic\nimages, but they often suffer from the training instability issues and the\nphenomenon of mode loss. In order to attain greater diversity in GAN\nsynthesized data, it is critical to solving the problem of mode loss. Our work\nexplores probabilistic approaches to GAN modelling that could allow us to\ntackle these issues. We present Prb-GANs, a new variation that uses dropout to\ncreate a distribution over the network parameters with the posterior learnt\nusing variational inference. We describe theoretically and validate\nexperimentally using simple and complex datasets the benefits of such an\napproach. We look into further improvements using the concept of uncertainty\nmeasures. Through a set of further modifications to the loss functions for each\nnetwork of the GAN, we are able to get results that show the improvement of GAN\nperformance. Our methods are extremely simple and require very little\nmodification to existing GAN architecture.",
          "link": "http://arxiv.org/abs/2107.05241",
          "publishedOn": "2021-07-13T01:59:37.424Z",
          "wordCount": 597,
          "title": "Prb-GAN: A Probabilistic Framework for GAN Modelling. (arXiv:2107.05241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01863",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Mesuga_R/0/1/0/all/0/1\">Reymond Mesuga</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Bayanay_B/0/1/0/all/0/1\">Brian James Bayanay</a>",
          "description": "LIGO is considered the most sensitive and complicated gravitational\nexperiment ever built. Its main objective is to detect the gravitational wave\nfrom the strongest events in the universe by observing if the length of its\n4-kilometer arms change by a distance 10,000 times smaller than the diameter of\na proton. Due to its sensitivity, LIGO is prone to the disturbance of external\nnoises which affects the data being collected to detect the gravitational wave.\nThese noises are commonly called by the LIGO community as glitches. The\nobjective of this study is to evaluate the effeciency of various deep trasnfer\nlearning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch\nwaveform in gravitational wave data. The accuracy achieved by the said models\nare 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models\nachieved fairly high accuracy, it is observed that all of the model suffered\nfrom the lack of data for certain classes which is the main concern found in\nthe experiment.",
          "link": "http://arxiv.org/abs/2107.01863",
          "publishedOn": "2021-07-13T01:59:37.417Z",
          "wordCount": 650,
          "title": "On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data. (arXiv:2107.01863v2 [gr-qc] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06671",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1\">Thanh Nguyen-Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1\">Hung Tran-The</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "We study the statistical theory of offline reinforcement learning (RL) with\ndeep ReLU network function approximation. We analyze a variant of fitted-Q\niteration (FQI) algorithm under a new dynamic condition that we call Besov\ndynamic closure, which encompasses the conditions from prior analyses for deep\nneural network function approximation. Under Besov dynamic closure, we prove\nthat the FQI-type algorithm enjoys the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( \\kappa^{1 + d/\\alpha} \\cdot \\epsilon^{-2 -\n2d/\\alpha} \\right)$ where $\\kappa$ is a distribution shift measure, $d$ is the\ndimensionality of the state-action space, $\\alpha$ is the (possibly fractional)\nsmoothness parameter of the underlying MDP, and $\\epsilon$ is a user-specified\nprecision. This is an improvement over the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( K \\cdot \\kappa^{2 + d/\\alpha} \\cdot \\epsilon^{-2 -\nd/\\alpha} \\right)$ in the prior result [Yang et al., 2019] where $K$ is an\nalgorithmic iteration number which is arbitrarily large in practice.\nImportantly, our sample complexity is obtained under the new general dynamic\ncondition and a data-dependent structure where the latter is either ignored in\nprior algorithms or improperly handled by prior analyses. This is the first\ncomprehensive analysis for offline RL with deep ReLU network function\napproximation under a general setting.",
          "link": "http://arxiv.org/abs/2103.06671",
          "publishedOn": "2021-07-13T01:59:37.410Z",
          "wordCount": 674,
          "title": "Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks. (arXiv:2103.06671v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1\">Sneha Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithal_V/0/1/0/all/0/1\">Varun Mithal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polatkan_G/0/1/0/all/0/1\">Gungor Polatkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1\">Rohan Ramanath</a>",
          "description": "Attention Model has now become an important concept in neural networks that\nhas been researched within diverse application domains. This survey provides a\nstructured and comprehensive overview of the developments in modeling\nattention. In particular, we propose a taxonomy which groups existing\ntechniques into coherent categories. We review salient neural architectures in\nwhich attention has been incorporated, and discuss applications in which\nmodeling attention has shown a significant impact. We also describe how\nattention has been used to improve the interpretability of neural networks.\nFinally, we discuss some future research directions in attention. We hope this\nsurvey will provide a succinct introduction to attention models and guide\npractitioners while developing approaches for their applications.",
          "link": "http://arxiv.org/abs/1904.02874",
          "publishedOn": "2021-07-13T01:59:37.392Z",
          "wordCount": 593,
          "title": "An Attentive Survey of Attention Models. (arXiv:1904.02874v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>",
          "description": "We present a new learning-based framework to recover vehicle pose in SO(3)\nfrom a single RGB image. In contrast to previous works that map from local\nappearance to observation angles, we explore a progressive approach by\nextracting meaningful Intermediate Geometrical Representations (IGRs) to\nestimate egocentric vehicle orientation. This approach features a deep model\nthat transforms perceived intensities to IGRs, which are mapped to a 3D\nrepresentation encoding object orientation in the camera coordinate system.\nCore problems are what IGRs to use and how to learn them more effectively. We\nanswer the former question by designing IGRs based on an interpolated cuboid\nthat derives from primitive 3D annotation readily. The latter question\nmotivates us to incorporate geometry knowledge with a new loss function based\non a projective invariant. This loss function allows unlabeled data to be used\nin the training stage to improve representation learning. Without additional\nlabels, our system outperforms previous monocular RGB-based methods for joint\nvehicle detection and pose estimation on the KITTI benchmark, achieving\nperformance even comparable to stereo methods. Code and pre-trained models are\navailable at this https URL.",
          "link": "http://arxiv.org/abs/2011.08464",
          "publishedOn": "2021-07-13T01:59:37.383Z",
          "wordCount": 687,
          "title": "Exploring intermediate representation for monocular vehicle pose estimation. (arXiv:2011.08464v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Penco_L/0/1/0/all/0/1\">Luigi Penco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1\">Jean-Baptiste Mouret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivaldi_S/0/1/0/all/0/1\">Serena Ivaldi</a>",
          "description": "Humanoid robots could be versatile and intuitive human avatars that operate\nremotely in inaccessible places: the robot could reproduce in the remote\nlocation the movements of an operator equipped with a wearable motion capture\ndevice while sending visual feedback to the operator. While substantial\nprogress has been made on transferring (\"retargeting\") human motions to\nhumanoid robots, a major problem preventing the deployment of such systems in\nreal applications is the presence of communication delays between the human\ninput and the feedback from the robot: even a few hundred milliseconds of delay\ncan irreversibly disturb the operator, let alone a few seconds. To overcome\nthese delays, we introduce a system in which a humanoid robot executes commands\nbefore it actually receives them, so that the visual feedback appears to be\nsynchronized to the operator, whereas the robot executed the commands in the\npast. To do so, the robot continuously predicts future commands by querying a\nmachine learning model that is trained on past trajectories and conditioned on\nthe last received commands. In our experiments, an operator was able to\nsuccessfully control a humanoid robot (32 degrees of freedom) with stochastic\ndelays up to 2 seconds in several whole-body manipulation tasks, including\nreaching different targets, picking up, and placing a box at distinct\nlocations.",
          "link": "http://arxiv.org/abs/2107.01281",
          "publishedOn": "2021-07-13T01:59:37.375Z",
          "wordCount": 658,
          "title": "Prescient teleoperation of humanoid robots. (arXiv:2107.01281v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Atul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajesh_B/0/1/0/all/0/1\">Bulla Rajesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>",
          "description": "Plant leaf diseases pose a significant danger to food security and they cause\ndepletion in quality and volume of production. Therefore accurate and timely\ndetection of leaf disease is very important to check the loss of the crops and\nmeet the growing food demand of the people. Conventional techniques depend on\nlab investigation and human skills which are generally costly and inaccessible.\nRecently, Deep Neural Networks have been exceptionally fruitful in image\nclassification. In this research paper, plant leaf disease detection employing\ntransfer learning is explored in the JPEG compressed domain. Here, the JPEG\ncompressed stream consisting of DCT coefficients is, directly fed into the\nNeural Network to improve the efficiency of classification. The experimental\nresults on JPEG compressed leaf dataset demonstrate the efficacy of the\nproposed model.",
          "link": "http://arxiv.org/abs/2107.04813",
          "publishedOn": "2021-07-13T01:59:37.352Z",
          "wordCount": 601,
          "title": "Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique. (arXiv:2107.04813v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunzong Xu</a>",
          "description": "We consider the general (stochastic) contextual bandit problem under the\nrealizability assumption, i.e., the expected reward, as a function of contexts\nand actions, belongs to a general function class $\\mathcal{F}$. We design a\nfast and simple algorithm that achieves the statistically optimal regret with\nonly ${O}(\\log T)$ calls to an offline regression oracle across all $T$ rounds.\nThe number of oracle calls can be further reduced to $O(\\log\\log T)$ if $T$ is\nknown in advance. Our results provide the first universal and optimal reduction\nfrom contextual bandits to offline regression, solving an important open\nproblem in the contextual bandit literature. A direct consequence of our\nresults is that any advances in offline regression immediately translate to\ncontextual bandits, statistically and computationally. This leads to faster\nalgorithms and improved regret guarantees for broader classes of contextual\nbandit problems.",
          "link": "http://arxiv.org/abs/2003.12699",
          "publishedOn": "2021-07-13T01:59:37.345Z",
          "wordCount": 647,
          "title": "Bypassing the Monster: A Faster and Simpler Optimal Algorithm for Contextual Bandits under Realizability. (arXiv:2003.12699v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yutao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deyi Li</a>",
          "description": "Most of the existing deep learning-based sequential recommendation approaches\nutilize the recurrent neural network architecture or self-attention to model\nthe sequential patterns and temporal influence among a user's historical\nbehavior and learn the user's preference at a specific time. However, these\nmethods have two main drawbacks. First, they focus on modeling users' dynamic\nstates from a user-centric perspective and always neglect the dynamics of items\nover time. Second, most of them deal with only the first-order user-item\ninteractions and do not consider the high-order connectivity between users and\nitems, which has recently been proved helpful for the sequential\nrecommendation. To address the above problems, in this article, we attempt to\nmodel user-item interactions by a bipartite graph structure and propose a new\nrecommendation approach based on a Position-enhanced and Time-aware Graph\nConvolutional Network (PTGCN) for the sequential recommendation. PTGCN models\nthe sequential patterns and temporal dynamics between user-item interactions by\ndefining a position-enhanced and time-aware graph convolution operation and\nlearning the dynamic representations of users and items simultaneously on the\nbipartite graph with a self-attention aggregator. Also, it realizes the\nhigh-order connectivity between users and items by stacking multi-layer graph\nconvolutions. To demonstrate the effectiveness of PTGCN, we carried out a\ncomprehensive evaluation of PTGCN on three real-world datasets of different\nsizes compared with a few competitive baselines. Experimental results indicate\nthat PTGCN outperforms several state-of-the-art models in terms of two\ncommonly-used evaluation metrics for ranking.",
          "link": "http://arxiv.org/abs/2107.05235",
          "publishedOn": "2021-07-13T01:59:37.330Z",
          "wordCount": 686,
          "title": "Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations. (arXiv:2107.05235v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.11645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsung-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosca_J/0/1/0/all/0/1\">Justinian Rosca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1\">Peter J. Ramadge</a>",
          "description": "We consider the problem of reinforcement learning when provided with (1) a\nbaseline control policy and (2) a set of constraints that the learner must\nsatisfy. The baseline policy can arise from demonstration data or a teacher\nagent and may provide useful cues for learning, but it might also be\nsub-optimal for the task at hand, and is not guaranteed to satisfy the\nspecified constraints, which might encode safety, fairness or other\napplication-specific requirements. In order to safely learn from baseline\npolicies, we propose an iterative policy optimization algorithm that alternates\nbetween maximizing expected return on the task, minimizing distance to the\nbaseline policy, and projecting the policy onto the constraint-satisfying set.\nWe analyze our algorithm theoretically and provide a finite-time convergence\nguarantee. In our experiments on five different control tasks, our algorithm\nconsistently outperforms several state-of-the-art baselines, achieving 10 times\nfewer constraint violations and 40% higher reward on average.",
          "link": "http://arxiv.org/abs/2006.11645",
          "publishedOn": "2021-07-13T01:59:37.315Z",
          "wordCount": 636,
          "title": "Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies. (arXiv:2006.11645v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menghan_H/0/1/0/all/0/1\">Hu Menghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guangtao_Z/0/1/0/all/0/1\">Zhai Guangtao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Ping_Z/0/1/0/all/0/1\">Zhang Xiao-Ping</a>",
          "description": "In recent years, computer-aided diagnosis has become an increasingly popular\ntopic. Methods based on convolutional neural networks have achieved good\nperformance in medical image segmentation and classification. Due to the\nlimitations of the convolution operation, the long-term spatial features are\noften not accurately obtained. Hence, we propose a TransClaw U-Net network\nstructure, which combines the convolution operation with the transformer\noperation in the encoding part. The convolution part is applied for extracting\nthe shallow spatial features to facilitate the recovery of the image resolution\nafter upsampling. The transformer part is used to encode the patches, and the\nself-attention mechanism is used to obtain global information between\nsequences. The decoding part retains the bottom upsampling structure for better\ndetail segmentation performance. The experimental results on Synapse\nMulti-organ Segmentation Datasets show that the performance of TransClaw U-Net\nis better than other network structures. The ablation experiments also prove\nthe generalization performance of TransClaw U-Net.",
          "link": "http://arxiv.org/abs/2107.05188",
          "publishedOn": "2021-07-13T01:59:37.309Z",
          "wordCount": 611,
          "title": "TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation. (arXiv:2107.05188v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04721",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1\">Shuyun Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_Z/0/1/0/all/0/1\">Ziming Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granley_J/0/1/0/all/0/1\">Jacob Granley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beyeler_M/0/1/0/all/0/1\">Michael Beyeler</a>",
          "description": "Fundus photography has routinely been used to document the presence and\nseverity of retinal degenerative diseases such as age-related macular\ndegeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical\npractice, for which the fovea and optic disc (OD) are important retinal\nlandmarks. However, the occurrence of lesions, drusen, and other retinal\nabnormalities during retinal degeneration severely complicates automatic\nlandmark detection and segmentation. Here we propose HBA-U-Net: a U-Net\nbackbone enriched with hierarchical bottleneck attention. The network consists\nof a novel bottleneck attention block that combines and refines self-attention,\nchannel attention, and relative-position attention to highlight retinal\nabnormalities that may be important for fovea and OD segmentation in the\ndegenerated retina. HBA-U-Net achieved state-of-the-art results on fovea\ndetection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of\n25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for\nAMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:\nED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for\nlandmark detection in the presence of a variety of retinal degenerative\ndiseases.",
          "link": "http://arxiv.org/abs/2107.04721",
          "publishedOn": "2021-07-13T01:59:37.292Z",
          "wordCount": 641,
          "title": "U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina. (arXiv:2107.04721v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14473",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1\">Tim De Ryck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>",
          "description": "Physics informed neural networks approximate solutions of PDEs by minimizing\npointwise residuals. We derive rigorous bounds on the error, incurred by PINNs\nin approximating the solutions of a large class of linear parabolic PDEs,\nnamely Kolmogorov equations that include the heat equation and Black-Scholes\nequation of option pricing, as examples. We construct neural networks, whose\nPINN residual (generalization error) can be made as small as desired. We also\nprove that the total $L^2$-error can be bounded by the generalization error,\nwhich in turn is bounded in terms of the training error, provided that a\nsufficient number of randomly chosen training (collocation) points is used.\nMoreover, we prove that the size of the PINNs and the number of training\nsamples only grow polynomially with the underlying dimension, enabling PINNs to\novercome the curse of dimensionality in this context. These results enable us\nto provide a comprehensive error analysis for PINNs in approximating Kolmogorov\nPDEs.",
          "link": "http://arxiv.org/abs/2106.14473",
          "publishedOn": "2021-07-13T01:59:37.287Z",
          "wordCount": 617,
          "title": "Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs. (arXiv:2106.14473v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shota Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Learning from positive and unlabeled (PU) data is an important problem in\nvarious applications. Most of the recent approaches for PU classification\nassume that the class-prior (the ratio of positive samples) in the training\nunlabeled dataset is identical to that of the test data, which does not hold in\nmany practical cases. In addition, we usually do not know the class-priors of\nthe training and test data, thus we have no clue on how to train a classifier\nwithout them. To address these problems, we propose a novel PU classification\nmethod based on density ratio estimation. A notable advantage of our proposed\nmethod is that it does not require the class-priors in the training phase;\nclass-prior shift is incorporated only in the test phase. We theoretically\njustify our proposed method and experimentally demonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2107.05045",
          "publishedOn": "2021-07-13T01:59:37.281Z",
          "wordCount": 576,
          "title": "Positive-Unlabeled Classification under Class-Prior Shift: A Prior-invariant Approach Based on Density Ratio Estimation. (arXiv:2107.05045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Starting from childhood, the human brain restructures and rewires throughout\nlife. Characterizing such complex brain development requires effective analysis\nof longitudinal and multi-modal neuroimaging data. Here, we propose such an\nanalysis approach named Longitudinal Correlation Analysis (LCA). LCA couples\nthe data of two modalities by first reducing the input from each modality to a\nlatent representation based on autoencoders. A self-supervised strategy then\nrelates the two latent spaces by jointly disentangling two directions, one in\neach space, such that the longitudinal changes in latent representations along\nthose directions are maximally correlated between modalities. We applied LCA to\nanalyze the longitudinal T1-weighted and diffusion-weighted MRIs of 679 youths\nfrom the National Consortium on Alcohol and Neurodevelopment in Adolescence.\nUnlike existing approaches that focus on either cross-sectional or single-modal\nmodeling, LCA successfully unraveled coupled macrostructural and\nmicrostructural brain development from morphological and diffusivity features\nextracted from the data. A retesting of LCA on raw 3D image volumes of those\nsubjects successfully replicated the findings from the feature-based analysis.\nLastly, the developmental effects revealed by LCA were inline with the current\nunderstanding of maturational patterns of the adolescent brain.",
          "link": "http://arxiv.org/abs/2107.04724",
          "publishedOn": "2021-07-13T01:59:37.276Z",
          "wordCount": 615,
          "title": "Longitudinal Correlation Analysis for Decoding Multi-Modal Brain Development. (arXiv:2107.04724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yi-Hui Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1\">I-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chin-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1\">Joann Ching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "This paper presents an attempt to employ the mask language modeling approach\nof BERT to pre-train a 12-layer Transformer model over 4,166 pieces of\npolyphonic piano MIDI files for tackling a number of symbolic-domain\ndiscriminative music understanding tasks. These include two note-level\nclassification tasks, i.e., melody extraction and velocity prediction, as well\nas two sequence-level classification tasks, i.e., composer classification and\nemotion classification. We find that, given a pre-trained Transformer, our\nmodels outperform recurrent neural network based baselines with less than 10\nepochs of fine-tuning. Ablation studies show that the pre-training remains\neffective even if none of the MIDI data of the downstream tasks are seen at the\npre-training stage, and that freezing the self-attention layers of the\nTransformer at the fine-tuning stage slightly degrades performance. All the\nfive datasets employed in this work are publicly available, as well as\ncheckpoints of our pre-trained and fine-tuned models. As such, our research can\nbe taken as a benchmark for symbolic-domain music understanding.",
          "link": "http://arxiv.org/abs/2107.05223",
          "publishedOn": "2021-07-13T01:59:37.270Z",
          "wordCount": 603,
          "title": "MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding. (arXiv:2107.05223v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1\">Toshinori Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takamitsu Matsubara</a>",
          "description": "The oscillating performance of off-policy learning and persisting errors in\nthe actor-critic (AC) setting call for algorithms that can conservatively learn\nto suit the stability-critical applications better. In this paper, we propose a\nnovel off-policy AC algorithm cautious actor-critic (CAC). The name cautious\ncomes from the doubly conservative nature that we exploit the classic policy\ninterpolation from conservative policy iteration for the actor and the\nentropy-regularization of conservative value iteration for the critic. Our key\nobservation is the entropy-regularized critic facilitates and simplifies the\nunwieldy interpolated actor update while still ensuring robust policy\nimprovement. We compare CAC to state-of-the-art AC methods on a set of\nchallenging continuous control problems and demonstrate that CAC achieves\ncomparable performance while significantly stabilizes learning.",
          "link": "http://arxiv.org/abs/2107.05217",
          "publishedOn": "2021-07-13T01:59:37.253Z",
          "wordCount": 544,
          "title": "Cautious Actor-Critic. (arXiv:2107.05217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devic_S/0/1/0/all/0/1\">Siddartha Devic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juba_B/0/1/0/all/0/1\">Brendan Juba</a>",
          "description": "Many reinforcement learning (RL) environments in practice feature enormous\nstate spaces that may be described compactly by a \"factored\" structure, that\nmay be modeled by Factored Markov Decision Processes (FMDPs). We present the\nfirst polynomial-time algorithm for RL with FMDPs that does not rely on an\noracle planner, and instead of requiring a linear transition model, only\nrequires a linear value function with a suitable local basis with respect to\nthe factorization. With this assumption, we can solve FMDPs in polynomial time\nby constructing an efficient separation oracle for convex optimization.\nImportantly, and in contrast to prior work, we do not assume that the\ntransitions on various factors are independent.",
          "link": "http://arxiv.org/abs/2107.05187",
          "publishedOn": "2021-07-13T01:59:37.247Z",
          "wordCount": 549,
          "title": "Polynomial Time Reinforcement Learning in Correlated FMDPs with Linear Value Functions. (arXiv:2107.05187v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.03745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1\">Mohamad H. Danesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anurag Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1\">Alan Fern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorram_S/0/1/0/all/0/1\">Saeed Khorram</a>",
          "description": "We introduce an approach for understanding control policies represented as\nrecurrent neural networks. Recent work has approached this problem by\ntransforming such recurrent policy networks into finite-state machines (FSM)\nand then analyzing the equivalent minimized FSM. While this led to interesting\ninsights, the minimization process can obscure a deeper understanding of a\nmachine's operation by merging states that are semantically distinct. To\naddress this issue, we introduce an analysis approach that starts with an\nunminimized FSM and applies more-interpretable reductions that preserve the key\ndecision points of the policy. We also contribute an attention tool to attain a\ndeeper understanding of the role of observations in the decisions. Our case\nstudies on 7 Atari games and 3 control benchmarks demonstrate that the approach\ncan reveal insights that have not been previously noticed.",
          "link": "http://arxiv.org/abs/2006.03745",
          "publishedOn": "2021-07-13T01:59:37.241Z",
          "wordCount": 607,
          "title": "Re-understanding Finite-State Representations of Recurrent Policy Networks. (arXiv:2006.03745v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05201",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Lin_H/0/1/0/all/0/1\">Hengxu Lin</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "Modeling and managing portfolio risk is perhaps the most important step to\nachieve growing and preserving investment performance. Within the modern\nportfolio construction framework that built on Markowitz's theory, the\ncovariance matrix of stock returns is required to model the portfolio risk.\nTraditional approaches to estimate the covariance matrix are based on human\ndesigned risk factors, which often requires tremendous time and effort to\ndesign better risk factors to improve the covariance estimation. In this work,\nwe formulate the quest of mining risk factors as a learning problem and propose\na deep learning solution to effectively \"design\" risk factors with neural\nnetworks. The learning objective is carefully set to ensure the learned risk\nfactors are effective in explaining stock returns as well as have desired\northogonality and stability. Our experiments on the stock market data\ndemonstrate the effectiveness of the proposed method: our method can obtain\n$1.9\\%$ higher explained variance measured by $R^2$ and also reduce the risk of\na global minimum variance portfolio. Incremental analysis further supports our\ndesign of both the architecture and the learning objective.",
          "link": "http://arxiv.org/abs/2107.05201",
          "publishedOn": "2021-07-13T01:59:37.235Z",
          "wordCount": 630,
          "title": "Deep Risk Model: A Deep Learning Solution for Mining Latent Risk Factors to Improve Covariance Matrix Estimation. (arXiv:2107.05201v1 [q-fin.RM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1\">Harikrishna Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>",
          "description": "Many modern machine learning applications come with complex and nuanced\ndesign goals such as minimizing the worst-case error, satisfying a given\nprecision or recall target, or enforcing group-fairness constraints. Popular\ntechniques for optimizing such non-decomposable objectives reduce the problem\ninto a sequence of cost-sensitive learning tasks, each of which is then solved\nby re-weighting the training loss with example-specific costs. We point out\nthat the standard approach of re-weighting the loss to incorporate label costs\ncan produce unsatisfactory results when used to train over-parameterized\nmodels. As a remedy, we propose new cost-sensitive losses that extend the\nclassical idea of logit adjustment to handle more general cost matrices. Our\nlosses are calibrated, and can be further improved with distilled labels from a\nteacher model. Through experiments on benchmark image datasets, we showcase the\neffectiveness of our approach in training ResNet models with common robust and\nconstrained optimization objectives.",
          "link": "http://arxiv.org/abs/2107.04641",
          "publishedOn": "2021-07-13T01:59:37.229Z",
          "wordCount": 574,
          "title": "Training Over-parameterized Models with Non-decomposable Objectives. (arXiv:2107.04641v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ly_R/0/1/0/all/0/1\">Racine Ly</a>",
          "description": "The improvement of computers' capacities, advancements in algorithmic\ntechniques, and the significant increase of available data have enabled the\nrecent developments of Artificial Intelligence (AI) technology. One of its\nbranches, called Machine Learning (ML), has shown strong capacities in\nmimicking characteristics attributed to human intelligence, such as vision,\nspeech, and problem-solving. However, as previous technological revolutions\nsuggest, their most significant impacts could be mostly expected on other\nsectors that were not traditional users of that technology. The agricultural\nsector is vital for African economies; improving yields, mitigating losses, and\neffective management of natural resources are crucial in a climate change era.\nMachine Learning is a technology with an added value in making predictions,\nhence the potential to reduce uncertainties and risk across sectors, in this\ncase, the agricultural sector. The purpose of this paper is to contextualize\nand discuss barriers to ML-based solutions for African agriculture. In the\nsecond section, we provided an overview of ML technology from a historical and\ntechnical perspective and its main driving force. In the third section, we\nprovided a brief review of the current use of ML in agriculture. Finally, in\nsection 4, we discuss ML growing interest in Africa and the potential barriers\nto creating and using ML-based solutions in the agricultural sector.",
          "link": "http://arxiv.org/abs/2107.05101",
          "publishedOn": "2021-07-13T01:59:37.223Z",
          "wordCount": 682,
          "title": "Machine Learning Challenges and Opportunities in the African Agricultural Sector -- A General Perspective. (arXiv:2107.05101v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhijie Sasha Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christenson_L/0/1/0/all/0/1\">Lauren Christenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulton_L/0/1/0/all/0/1\">Lawrence Fulton</a>",
          "description": "Social media has become an essential channel for posting disaster-related\ninformation, which provide governments and relief agencies real-time data for\nbetter disaster management. However, research in this field has not received\nsufficient attention and extracting useful information is still challenging.\nThis paper aims to improve disaster relief efficiency via mining and analyzing\nsocial media data like public attitudes towards disaster response and public\ndemands for targeted relief supplies during different types of disasters. We\nfocus on different natural disasters based on properties such as types,\ndurations, and damages, which contains a total of 41,993 tweets. In this paper,\npublic perception is assessed qualitatively by manually classified tweets,\nwhich contain information like the demand for targeted relief supplies,\nsatisfactions of disaster response, and public fear. Public attitudes to\nnatural disasters are studied via a quantitative analysis using eight machine\nlearning models. To better provide decision-makers with the appropriate model,\nthe comparison of machine learning models based on computational time and\nprediction accuracy is conducted. The change of public opinion during different\nnatural disasters and the evolution of people's behavior of using social media\nfor disaster relief in the face of the identical type of natural disasters as\nTwitter continues to evolve are studied. The results in this paper demonstrate\nthe feasibility and validation of the proposed research approach and provide\nrelief agencies with insights into better disaster management.",
          "link": "http://arxiv.org/abs/2005.07019",
          "publishedOn": "2021-07-13T01:59:37.204Z",
          "wordCount": 737,
          "title": "Social Media Information Sharing for Natural Disaster Response. (arXiv:2005.07019v5 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05342",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Celik_N/0/1/0/all/0/1\">Numan Celik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Soumya Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braden_B/0/1/0/all/0/1\">Barbara Braden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1\">Jens Rittscher</a>",
          "description": "Gastrointestinal (GI) cancer precursors require frequent monitoring for risk\nstratification of patients. Automated segmentation methods can help to assess\nrisk areas more accurately, and assist in therapeutic procedures or even\nremoval. In clinical practice, addition to the conventional white-light imaging\n(WLI), complimentary modalities such as narrow-band imaging (NBI) and\nfluorescence imaging are used. While, today most segmentation approaches are\nsupervised and only concentrated on a single modality dataset, this work\nexploits to use a target-independent unsupervised domain adaptation (UDA)\ntechnique that is capable to generalize to an unseen target modality. In this\ncontext, we propose a novel UDA-based segmentation method that couples the\nvariational autoencoder and U-Net with a common EfficientNet-B4 backbone, and\nuses a joint loss for latent-space optimization for target samples. We show\nthat our model can generalize to unseen target NBI (target) modality when\ntrained using only WLI (source) modality. Our experiments on both upper and\nlower GI endoscopy data show the effectiveness of our approach compared to\nnaive supervised approach and state-of-the-art UDA segmentation methods.",
          "link": "http://arxiv.org/abs/2107.05342",
          "publishedOn": "2021-07-13T01:59:37.198Z",
          "wordCount": 632,
          "title": "EndoUDA: A modality independent segmentation approach for endoscopy imaging. (arXiv:2107.05342v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Umer_M/0/1/0/all/0/1\">Muhammad Azmi Umer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_C/0/1/0/all/0/1\">Chuadhry Mujeeb Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jilani_M/0/1/0/all/0/1\">Muhammad Taha Jilani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Aditya P. Mathur</a>",
          "description": "Adversarial learning is used to test the robustness of machine learning\nalgorithms under attack and create attacks that deceive the anomaly detection\nmethods in Industrial Control System (ICS). Given that security assessment of\nan ICS demands that an exhaustive set of possible attack patterns is studied,\nin this work, we propose an association rule mining-based attack generation\ntechnique. The technique has been implemented using data from a secure Water\nTreatment plant. The proposed technique was able to generate more than 300,000\nattack patterns constituting a vast majority of new attack vectors which were\nnot seen before. Automatically generated attacks improve our understanding of\nthe potential attacks and enable the design of robust attack detection\ntechniques.",
          "link": "http://arxiv.org/abs/2107.05127",
          "publishedOn": "2021-07-13T01:59:37.192Z",
          "wordCount": 570,
          "title": "Attack Rules: An Adversarial Approach to Generate Attacks for Industrial Control Systems using Machine Learning. (arXiv:2107.05127v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/1807.11398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bengs_V/0/1/0/all/0/1\">Viktor Bengs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busa_Fekete_R/0/1/0/all/0/1\">Robert Busa-Fekete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesaoudi_Paul_A/0/1/0/all/0/1\">Adil El Mesaoudi-Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "In machine learning, the notion of multi-armed bandits refers to a class of\nonline learning problems, in which an agent is supposed to simultaneously\nexplore and exploit a given set of choice alternatives in the course of a\nsequential decision process. In the standard setting, the agent learns from\nstochastic feedback in the form of real-valued rewards. In many applications,\nhowever, numerical reward signals are not readily available -- instead, only\nweaker information is provided, in particular relative preferences in the form\nof qualitative comparisons between pairs of alternatives. This observation has\nmotivated the study of variants of the multi-armed bandit problem, in which\nmore general representations are used both for the type of feedback to learn\nfrom and the target of prediction. The aim of this paper is to provide a survey\nof the state of the art in this field, referred to as preference-based\nmulti-armed bandits or dueling bandits. To this end, we provide an overview of\nproblems that have been considered in the literature as well as methods for\ntackling them. Our taxonomy is mainly based on the assumptions made by these\nmethods about the data-generating process and, related to this, the properties\nof the preference-based feedback.",
          "link": "http://arxiv.org/abs/1807.11398",
          "publishedOn": "2021-07-13T01:59:37.185Z",
          "wordCount": 678,
          "title": "Preference-based Online Learning with Dueling Bandits: A Survey. (arXiv:1807.11398v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.12322",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salomone_R/0/1/0/all/0/1\">Robert Salomone</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roosta_F/0/1/0/all/0/1\">Fred Roosta</a>",
          "description": "For sampling from a log-concave density, we study implicit integrators\nresulting from $\\theta$-method discretization of the overdamped Langevin\ndiffusion stochastic differential equation. Theoretical and algorithmic\nproperties of the resulting sampling methods for $ \\theta \\in [0,1] $ and a\nrange of step sizes are established. Our results generalize and extend prior\nworks in several directions. In particular, for $\\theta\\ge1/2$, we prove\ngeometric ergodicity and stability of the resulting methods for all step sizes.\nWe show that obtaining subsequent samples amounts to solving a strongly-convex\noptimization problem, which is readily achievable using one of numerous\nexisting methods. Numerical examples supporting our theoretical analysis are\nalso presented.",
          "link": "http://arxiv.org/abs/1903.12322",
          "publishedOn": "2021-07-13T01:59:37.168Z",
          "wordCount": 556,
          "title": "Implicit Langevin Algorithms for Sampling From Log-concave Densities. (arXiv:1903.12322v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>",
          "description": "We propose a semi-supervised learning method for building end-to-end rich\ntranscription-style automatic speech recognition (RT-ASR) systems from\nsmall-scale rich transcription-style and large-scale common transcription-style\ndatasets. In spontaneous speech tasks, various speech phenomena such as\nfillers, word fragments, laughter and coughs, etc. are often included. While\ncommon transcriptions do not give special awareness to these phenomena, rich\ntranscriptions explicitly convert them into special phenomenon tokens as well\nas textual tokens. In previous studies, the textual and phenomenon tokens were\nsimultaneously estimated in an end-to-end manner. However, it is difficult to\nbuild accurate RT-ASR systems because large-scale rich transcription-style\ndatasets are often unavailable. To solve this problem, our training method uses\na limited rich transcription-style dataset and common transcription-style\ndataset simultaneously. The Key process in our semi-supervised learning is to\nconvert the common transcription-style dataset into a pseudo-rich\ntranscription-style dataset. To this end, we introduce style tokens which\ncontrol phenomenon tokens are generated or not into transformer-based\nautoregressive modeling. We use this modeling for generating the pseudo-rich\ntranscription-style datasets and for building RT-ASR system from the pseudo and\noriginal datasets. Our experiments on spontaneous ASR tasks showed the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.05382",
          "publishedOn": "2021-07-13T01:59:37.161Z",
          "wordCount": 648,
          "title": "End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning. (arXiv:2107.05382v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weina Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>",
          "description": "Being able to explain the prediction to clinical end-users is a necessity to\nleverage the power of AI models for clinical decision support. For medical\nimages, saliency maps are the most common form of explanation. The maps\nhighlight important features for AI model's prediction. Although many saliency\nmap methods have been proposed, it is unknown how well they perform on\nexplaining decisions on multi-modal medical images, where each modality/channel\ncarries distinct clinical meanings of the same underlying biomedical\nphenomenon. Understanding such modality-dependent features is essential for\nclinical users' interpretation of AI decisions. To tackle this clinically\nimportant but technically ignored problem, we propose the MSFI\n(Modality-Specific Feature Importance) metric to examine whether saliency maps\ncan highlight modality-specific important features. MSFI encodes the clinical\nrequirements on modality prioritization and modality-specific feature\nlocalization. Our evaluations on 16 commonly used saliency map methods,\nincluding a clinician user study, show that although most saliency map methods\ncaptured modality importance information in general, most of them failed to\nhighlight modality-specific important features consistently and precisely. The\nevaluation results guide the choices of saliency map methods and provide\ninsights to propose new ones targeting clinical applications.",
          "link": "http://arxiv.org/abs/2107.05047",
          "publishedOn": "2021-07-13T01:59:37.155Z",
          "wordCount": 646,
          "title": "One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images. (arXiv:2107.05047v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2001.02492",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_W/0/1/0/all/0/1\">Wenqing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1\">Chuhan Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jabari_S/0/1/0/all/0/1\">Saif Eddin Jabari</a>",
          "description": "This paper addresses the problem of short-term traffic prediction for\nsignalized traffic operations management. Specifically, we focus on predicting\nsensor states in high-resolution (second-by-second). This contrasts with\ntraditional traffic forecasting problems, which have focused on predicting\naggregated traffic variables, typically over intervals that are no shorter than\n5 minutes. Our contributions can be summarized as offering three insights:\nfirst, we show how the prediction problem can be modeled as a matrix completion\nproblem. Second, we employ a block-coordinate descent algorithm and demonstrate\nthat the algorithm converges in sub-linear time to a block coordinate-wise\noptimizer. This allows us to capitalize on the \"bigness\" of high-resolution\ndata in a computationally feasible way. Third, we develop an ensemble learning\n(or adaptive boosting) approach to reduce the training error to within any\narbitrary error threshold. The latter utilizes past days so that the boosting\ncan be interpreted as capturing periodic patterns in the data. The performance\nof the proposed method is analyzed theoretically and tested empirically using\nboth simulated data and a real-world high-resolution traffic dataset from Abu\nDhabi, UAE. Our experimental results show that the proposed method outperforms\nother state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2001.02492",
          "publishedOn": "2021-07-13T01:59:37.149Z",
          "wordCount": 664,
          "title": "Nonlinear Traffic Prediction as a Matrix Completion Problem with Ensemble Learning. (arXiv:2001.02492v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+YinchuanLi/0/1/0/all/0/1\">YinchuanLi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XiaofengLiu/0/1/0/all/0/1\">XiaofengLiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XuZhang/0/1/0/all/0/1\">XuZhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YunfengShao/0/1/0/all/0/1\">YunfengShao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+QingWang/0/1/0/all/0/1\">QingWang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YanhuiGeng/0/1/0/all/0/1\">YanhuiGeng</a>",
          "description": "Federated Learning (FL) is a collaborative machine learning technique to\ntrain a global model without obtaining clients' private data. The main\nchallenges in FL are statistical diversity among clients, limited computing\ncapability among client equipments and the excessive communication overhead and\nlong latency between server and clients. To address these problems,\n\nwe propose a novel personalized federated learning via maximizing correlation\npFedMac), and further extend it to sparse and hierarchical models. By\nminimizing loss functions including the properties of an approximated L1-norm\nand the hierarchical correlation, the performance on statistical diversity data\nis improved and the communicational and computational loads required in the\nnetwork are reduced. Theoretical proofs show that pFedMac performs better than\nthe L2-norm distance based personalization methods. Experimentally, we\ndemonstrate the benefits of this sparse hierarchical personalization\narchitecture compared with the state-of-the-art personalization methods and\ntheir extensions (e.g. pFedMac achieves 99.75% accuracy on MNIST and 87.27%\naccuracy on Synthetic under heterogeneous and non-i.i.d data distributions)",
          "link": "http://arxiv.org/abs/2107.05330",
          "publishedOn": "2021-07-13T01:59:37.143Z",
          "wordCount": 592,
          "title": "Personalized Federated Learning via Maximizing Correlation with Sparse and Hierarchical Extensions. (arXiv:2107.05330v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>",
          "description": "We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n\nHaving all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n\nWe solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article. Just like for linear convolution a\nmorphological convolution is specified by a kernel that we train in our\nPDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling\nand ReLUs as they are already subsumed by morphological convolutions.\n\nWe present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.",
          "link": "http://arxiv.org/abs/2001.09046",
          "publishedOn": "2021-07-13T01:59:37.138Z",
          "wordCount": 751,
          "title": "PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianwen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianwei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shenghuan He</a>",
          "description": "In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.",
          "link": "http://arxiv.org/abs/2107.05377",
          "publishedOn": "2021-07-13T01:59:37.103Z",
          "wordCount": 580,
          "title": "A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1\">Ilja Kuzborskij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>",
          "description": "We explore the ability of overparameterized shallow neural networks to learn\nLipschitz regression functions with and without label noise when trained by\nGradient Descent (GD). To avoid the problem that in the presence of noisy\nlabels, neural networks trained to nearly zero training error are inconsistent\non this class, we propose an early stopping rule that allows us to show optimal\nrates. This provides an alternative to the result of Hu et al. (2021) who\nstudied the performance of $\\ell 2$ -regularized GD for training shallow\nnetworks in nonparametric regression which fully relied on the infinite-width\nnetwork (Neural Tangent Kernel (NTK)) approximation. Here we present a simpler\nanalysis which is based on a partitioning argument of the input space (as in\nthe case of 1-nearest-neighbor rule) coupled with the fact that trained neural\nnetworks are smooth with respect to their inputs when trained by GD. In the\nnoise-free case the proof does not rely on any kernelization and can be\nregarded as a finite-width result. In the case of label noise, by slightly\nmodifying the proof, the noise is controlled using a technique of Yao, Rosasco,\nand Caponnetto (2007).",
          "link": "http://arxiv.org/abs/2107.05341",
          "publishedOn": "2021-07-13T01:59:37.083Z",
          "wordCount": 633,
          "title": "Nonparametric Regression with Shallow Overparameterized Neural Networks Trained by GD with Early Stopping. (arXiv:2107.05341v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Anish Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rudrajit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1\">Sujay Sanghavi</a>",
          "description": "In this paper we study test time decoding; an ubiquitous step in almost all\nsequential text generation task spanning across a wide array of natural\nlanguage processing (NLP) problems. Our main contribution is to develop a\ncontinuous relaxation framework for the combinatorial NP-hard decoding problem\nand propose Disco - an efficient algorithm based on standard first order\ngradient based. We provide tight analysis and show that our proposed algorithm\nlinearly converges to within $\\epsilon$ neighborhood of the optima. Finally, we\nperform preliminary experiments on the task of adversarial text generation and\nshow superior performance of Disco over several popular decoding approaches.",
          "link": "http://arxiv.org/abs/2107.05380",
          "publishedOn": "2021-07-13T01:59:37.077Z",
          "wordCount": 552,
          "title": "DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation. (arXiv:2107.05380v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2002.04276",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Woznica_K/0/1/0/all/0/1\">Katarzyna Wo&#x17a;nica</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Biecek_P/0/1/0/all/0/1\">Przemys&#x142;aw Biecek</a>",
          "description": "Meta-learning is a field that aims at discovering how different machine\nlearning algorithms perform on a wide range of predictive tasks. Such knowledge\nspeeds up the hyperparameter tuning or feature engineering. With the use of\nsurrogate models various aspects of the predictive task such as meta-features,\nlandmarker models e.t.c. are used to predict the expected performance. State of\nthe art approaches are focused on searching for the best meta-model but do not\nexplain how these different aspects contribute to its performance. However, to\nbuild a new generation of meta-models we need a deeper understanding of the\nimportance and effect of meta-features on the model tunability. In this paper,\nwe propose techniques developed for eXplainable Artificial Intelligence (XAI)\nto examine and extract knowledge from black-box surrogate models. To our\nknowledge, this is the first paper that shows how post-hoc explainability can\nbe used to improve the meta-learning.",
          "link": "http://arxiv.org/abs/2002.04276",
          "publishedOn": "2021-07-13T01:59:37.071Z",
          "wordCount": 583,
          "title": "Towards explainable meta-learning. (arXiv:2002.04276v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1\">Kenneth Enevoldsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielbo_K/0/1/0/all/0/1\">Kristoffer Nielbo</a>",
          "description": "Danish natural language processing (NLP) has in recent years obtained\nconsiderable improvements with the addition of multiple new datasets and\nmodels. However, at present, there is no coherent framework for applying\nstate-of-the-art models for Danish. We present DaCy: a unified framework for\nDanish NLP built on SpaCy. DaCy uses efficient multitask models which obtain\nstate-of-the-art performance on named entity recognition, part-of-speech\ntagging, and dependency parsing. DaCy contains tools for easy integration of\nexisting models such as for polarity, emotion, or subjectivity detection. In\naddition, we conduct a series of tests for biases and robustness of Danish NLP\npipelines through augmentation of the test set of DaNE. DaCy large compares\nfavorably and is especially robust to long input lengths and spelling\nvariations and errors. All models except DaCy large display significant biases\nrelated to ethnicity while only Polyglot shows a significant gender bias. We\nargue that for languages with limited benchmark sets, data augmentation can be\nparticularly useful for obtaining more realistic and fine-grained performance\nestimates. We provide a series of augmenters as a first step towards a more\nthorough evaluation of language models for low and medium resource languages\nand encourage further development.",
          "link": "http://arxiv.org/abs/2107.05295",
          "publishedOn": "2021-07-13T01:59:37.054Z",
          "wordCount": 632,
          "title": "DaCy: A Unified Framework for Danish NLP. (arXiv:2107.05295v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05320",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Peleg_A/0/1/0/all/0/1\">Amit Peleg</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pearl_N/0/1/0/all/0/1\">Naama Pearl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meir_R/0/1/0/all/0/1\">Ron Meir</a>",
          "description": "Fully Bayesian approaches to sequential decision-making assume that problem\nparameters are generated from a known prior, while in practice, such\ninformation is often lacking, and needs to be estimated through learning. This\nproblem is exacerbated in decision-making setups with partial information,\nwhere using a misspecified prior may lead to poor exploration and inferior\nperformance. In this work we prove, in the context of stochastic linear bandits\nand Gaussian priors, that as long as the prior estimate is sufficiently close\nto the true prior, the performance of an algorithm that uses the misspecified\nprior is close to that of the algorithm that uses the true prior. Next, we\naddress the task of learning the prior through metalearning, where a learner\nupdates its estimate of the prior across multiple task instances in order to\nimprove performance on future tasks. The estimated prior is then updated within\neach task based on incoming observations, while actions are selected in order\nto maximize expected reward. In this work we apply this scheme within a linear\nbandit setting, and provide algorithms and regret bounds, demonstrating its\neffectiveness, as compared to an algorithm that knows the correct prior. Our\nresults hold for a broad class of algorithms, including, for example, Thompson\nSampling and Information Directed Sampling.",
          "link": "http://arxiv.org/abs/2107.05320",
          "publishedOn": "2021-07-13T01:59:37.048Z",
          "wordCount": 642,
          "title": "Metalearning Linear Bandits by Prior Update. (arXiv:2107.05320v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1805.08845",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Muandet_K/0/1/0/all/0/1\">Krikamol Muandet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kanagawa_M/0/1/0/all/0/1\">Motonobu Kanagawa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saengkyongam_S/0/1/0/all/0/1\">Sorawit Saengkyongam</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Marukatat_S/0/1/0/all/0/1\">Sanparith Marukatat</a>",
          "description": "Counterfactual inference has become a ubiquitous tool in online\nadvertisement, recommendation systems, medical diagnosis, and econometrics.\nAccurate modeling of outcome distributions associated with different\ninterventions -- known as counterfactual distributions -- is crucial for the\nsuccess of these applications. In this work, we propose to model counterfactual\ndistributions using a novel Hilbert space representation called counterfactual\nmean embedding (CME). The CME embeds the associated counterfactual distribution\ninto a reproducing kernel Hilbert space (RKHS) endowed with a positive definite\nkernel, which allows us to perform causal inference over the entire landscape\nof the counterfactual distribution. Based on this representation, we propose a\ndistributional treatment effect (DTE) that can quantify the causal effect over\nentire outcome distributions. Our approach is nonparametric as the CME can be\nestimated under the unconfoundedness assumption from observational data without\nrequiring any parametric assumption about the underlying distributions. We also\nestablish a rate of convergence of the proposed estimator which depends on the\nsmoothness of the conditional mean and the Radon-Nikodym derivative of the\nunderlying marginal distributions. Furthermore, our framework allows for more\ncomplex outcomes such as images, sequences, and graphs. Our experimental\nresults on synthetic data and off-policy evaluation tasks demonstrate the\nadvantages of the proposed estimator.",
          "link": "http://arxiv.org/abs/1805.08845",
          "publishedOn": "2021-07-13T01:59:37.042Z",
          "wordCount": 663,
          "title": "Counterfactual Mean Embeddings. (arXiv:1805.08845v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1\">Or Bar-Shira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubstein_A/0/1/0/all/0/1\">Ahuva Grubstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapson_Y/0/1/0/all/0/1\">Yael Rapson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhami_D/0/1/0/all/0/1\">Dror Suhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atar_E/0/1/0/all/0/1\">Eli Atar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peri_Hanania_K/0/1/0/all/0/1\">Keren Peri-Hanania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_R/0/1/0/all/0/1\">Ronnie Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "Breast cancer is the most common malignancy in women. Mammographic findings\nsuch as microcalcifications and masses, as well as morphologic features of\nmasses in sonographic scans, are the main diagnostic targets for tumor\ndetection. However, improved specificity of these imaging modalities is\nrequired. A leading alternative target is neoangiogenesis. When pathological,\nit contributes to the development of numerous types of tumors, and the\nformation of metastases. Hence, demonstrating neoangiogenesis by visualization\nof the microvasculature may be of great importance. Super resolution ultrasound\nlocalization microscopy enables imaging of the microvasculature at the\ncapillary level. Yet, challenges such as long reconstruction time, dependency\non prior knowledge of the system Point Spread Function (PSF), and separability\nof the Ultrasound Contrast Agents (UCAs), need to be addressed for translation\nof super-resolution US into the clinic. In this work we use a deep neural\nnetwork architecture that makes effective use of signal structure to address\nthese challenges. We present in vivo human results of three different breast\nlesions acquired with a clinical US scanner. By leveraging our trained network,\nthe microvasculature structure is recovered in a short time, without prior PSF\nknowledge, and without requiring separability of the UCAs. Each of the\nrecoveries exhibits a different structure that corresponds with the known\nhistological structure. This study demonstrates the feasibility of in vivo\nhuman super resolution, based on a clinical scanner, to increase US specificity\nfor different breast lesions and promotes the use of US in the diagnosis of\nbreast pathologies.",
          "link": "http://arxiv.org/abs/2107.05270",
          "publishedOn": "2021-07-13T01:59:37.035Z",
          "wordCount": 704,
          "title": "Learned super resolution ultrasound for improved breast lesion characterization. (arXiv:2107.05270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1\">Enzo Tartaglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiandrotti_A/0/1/0/all/0/1\">Attilio Fiandrotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cagnazzo_M/0/1/0/all/0/1\">Marco Cagnazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangetto_M/0/1/0/all/0/1\">Marco Grangetto</a>",
          "description": "We formulate the entropy of a quantized artificial neural network as a\ndifferentiable function that can be plugged as a regularization term into the\ncost function minimized by gradient descent. Our formulation scales efficiently\nbeyond the first order and is agnostic of the quantization scheme. The network\ncan then be trained to minimize the entropy of the quantized parameters, so\nthat they can be optimally compressed via entropy coding. We experiment with\nour entropy formulation at quantizing and compressing well-known network\narchitectures over multiple datasets. Our approach compares favorably over\nsimilar methods, enjoying the benefits of higher order entropy estimate,\nshowing flexibility towards non-uniform quantization (we use Lloyd-max\nquantization), scalability towards any entropy order to be minimized and\nefficiency in terms of compression. We show that HEMP is able to work in\nsynergy with other approaches aiming at pruning or quantizing the model itself,\ndelivering significant benefits in terms of storage size compressibility\nwithout harming the model's performance.",
          "link": "http://arxiv.org/abs/2107.05298",
          "publishedOn": "2021-07-13T01:59:37.029Z",
          "wordCount": 602,
          "title": "HEMP: High-order Entropy Minimization for neural network comPression. (arXiv:2107.05298v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingshi Chen</a>",
          "description": "Transformer is the state of the art model for many language and visual tasks.\nIn this paper, we give a deep analysis of its multi-head self-attention (MHSA)\nmodule and find that: 1) Each token is a random variable in high dimensional\nfeature space. 2) After layer normalization, these variables are mapped to\npoints on the hyper-sphere. 3) The update of these tokens is a Brownian motion.\nThe Brownian motion has special properties, its second order item should not be\nignored. So we present a new second-order optimizer(an iterative K-FAC\nalgorithm) for the MHSA module.\n\nIn some short words: All tokens are mapped to high dimension hyper-sphere.\nThe Scaled Dot-Product Attention\n$softmax(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}})$ is just the Markov\ntransition matrix for the random walking on the sphere. And the deep learning\nprocess would learn proper kernel function to get proper positions of these\ntokens. The training process in the MHSA module corresponds to a Brownian\nmotion worthy of further study.",
          "link": "http://arxiv.org/abs/2107.05264",
          "publishedOn": "2021-07-13T01:59:37.013Z",
          "wordCount": 582,
          "title": "The Brownian motion in the transformer model. (arXiv:2107.05264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiacheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wensi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songze Li</a>",
          "description": "We propose OmniLytics, a blockchain-based secure data trading marketplace for\nmachine learning applications. Utilizing OmniLytics, many distributed data\nowners can contribute their private data to collectively train a ML model\nrequested by some model owners, and get compensated for data contribution.\nOmniLytics enables such model training while simultaneously providing 1) model\nsecurity against curious data owners; 2) data security against curious model\nand data owners; 3) resilience to malicious data owners who provide faulty\nresults to poison model training; and 4) resilience to malicious model owner\nwho intents to evade the payment. OmniLytics is implemented as a smart contract\non the Ethereum blockchain to guarantee the atomicity of payment. In\nOmniLytics, a model owner publishes encrypted initial model on the contract,\nover which the participating data owners compute gradients using their private\ndata, and securely aggregate the gradients through the contract. Finally, the\ncontract reimburses the data owners, and the model owner decrypts the\naggregated model update. We implement a working prototype of OmniLytics on\nEthereum, and perform extensive experiments to measure its gas cost and\nexecution time under various parameter combinations, demonstrating its high\ncomputation and cost efficiency and strong practicality.",
          "link": "http://arxiv.org/abs/2107.05252",
          "publishedOn": "2021-07-13T01:59:37.006Z",
          "wordCount": 668,
          "title": "OmniLytics: A Blockchain-based Secure Data Market for Decentralized Machine Learning. (arXiv:2107.05252v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Joshua Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Chau-Wai Wong</a>",
          "description": "Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory\nfunctionality and is receiving increasing attention during the COVID-19\npandemic. Clinical findings show that it is possible for COVID-19 patients to\nhave significantly low SpO$_2$ before any obvious symptoms. The prevalence of\ncameras has motivated researchers to investigate methods for monitoring SpO$_2$\nusing videos. Most prior schemes involving smartphones are contact-based: They\nrequire a fingertip to cover the phone's camera and the nearby light source to\ncapture re-emitted light from the illuminated tissue. In this paper, we propose\nthe first convolutional neural network based noncontact SpO$_2$ estimation\nscheme using smartphone cameras. The scheme analyzes the videos of a\nparticipant's hand for physiological sensing, which is convenient and\ncomfortable, and can protect their privacy and allow for keeping face masks on.\nWe design our neural network architectures inspired by the optophysiological\nmodels for SpO$_2$ measurement and demonstrate the explainability by\nvisualizing the weights for channel combination. Our proposed models outperform\nthe state-of-the-art model that is designed for contact-based SpO$_2$\nmeasurement, showing the potential of our proposed method to contribute to\npublic health. We also analyze the impact of skin type and the side of a hand\non SpO$_2$ estimation performance.",
          "link": "http://arxiv.org/abs/2107.05087",
          "publishedOn": "2021-07-13T01:59:37.000Z",
          "wordCount": 689,
          "title": "Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05085",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1\">Gorkem Polat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Serinagaoglu_Y/0/1/0/all/0/1\">Yesim Dogrusoz Serinagaoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halici_U/0/1/0/all/0/1\">Ugur Halici</a>",
          "description": "Recent studies have shown that lung cancer screening using annual low-dose\ncomputed tomography (CT) reduces lung cancer mortality by 20% compared to\ntraditional chest radiography. Therefore, CT lung screening has started to be\nused widely all across the world. However, analyzing these images is a serious\nburden for radiologists. The number of slices in a CT scan can be up to 600.\nTherefore, computer-aided-detection (CAD) systems are very important for faster\nand more accurate assessment of the data. In this study, we proposed a\nframework that analyzes CT lung screenings using convolutional neural networks\n(CNNs) to reduce false positives. We trained our model with different volume\nsizes and showed that volume size plays a critical role in the performance of\nthe system. We also used different fusions in order to show their power and\neffect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D\nconvolutional operations applied to 3D data could result in information loss.\nThe proposed framework has been tested on the dataset provided by the LUNA16\nChallenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.",
          "link": "http://arxiv.org/abs/2107.05085",
          "publishedOn": "2021-07-13T01:59:36.993Z",
          "wordCount": 661,
          "title": "Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks. (arXiv:2107.05085v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaze_R/0/1/0/all/0/1\">Rahul Vaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanawal_M/0/1/0/all/0/1\">Manjesh K. Hanawal</a>",
          "description": "We consider a continuous-time multi-arm bandit problem (CTMAB), where the\nlearner can sample arms any number of times in a given interval and obtain a\nrandom reward from each sample, however, increasing the frequency of sampling\nincurs an additive penalty/cost. Thus, there is a tradeoff between obtaining\nlarge reward and incurring sampling cost as a function of the sampling\nfrequency. The goal is to design a learning algorithm that minimizes regret,\nthat is defined as the difference of the payoff of the oracle policy and that\nof the learning algorithm. CTMAB is fundamentally different than the usual\nmulti-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial\nin CTMAB, since the optimal sampling frequency depends on the mean of the arm,\nwhich needs to be estimated. We first establish lower bounds on the regret\nachievable with any algorithm and then propose algorithms that achieve the\nlower bound up to logarithmic factors. For the single-arm case, we show that\nthe lower bound on the regret is $\\Omega((\\log T)^2/\\mu)$, where $\\mu$ is the\nmean of the arm, and $T$ is the time horizon. For the multiple arms case, we\nshow that the lower bound on the regret is $\\Omega((\\log T)^2 \\mu/\\Delta^2)$,\nwhere $\\mu$ now represents the mean of the best arm, and $\\Delta$ is the\ndifference of the mean of the best and the second-best arm. We then propose an\nalgorithm that achieves the bound up to constant terms.",
          "link": "http://arxiv.org/abs/2107.05289",
          "publishedOn": "2021-07-13T01:59:36.982Z",
          "wordCount": 670,
          "title": "Continuous Time Bandits With Sampling Costs. (arXiv:2107.05289v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Soham Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_Y/0/1/0/all/0/1\">Yash Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1\">Aditya Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shevade_S/0/1/0/all/0/1\">Shirish Shevade</a>",
          "description": "Machine-Learning-as-a-Service providers expose machine learning (ML) models\nthrough application programming interfaces (APIs) to developers. Recent work\nhas shown that attackers can exploit these APIs to extract good approximations\nof such ML models, by querying them with samples of their choosing. We propose\nVarDetect, a stateful monitor that tracks the distribution of queries made by\nusers of such a service, to detect model extraction attacks. Harnessing the\nlatent distributions learned by a modified variational autoencoder, VarDetect\nrobustly separates three types of attacker samples from benign samples, and\nsuccessfully raises an alarm for each. Further, with VarDetect deployed as an\nautomated defense mechanism, the extracted substitute models are found to\nexhibit poor performance and transferability, as intended. Finally, we\ndemonstrate that even adaptive attackers with prior knowledge of the deployment\nof VarDetect, are detected by it.",
          "link": "http://arxiv.org/abs/2107.05166",
          "publishedOn": "2021-07-13T01:59:36.966Z",
          "wordCount": 574,
          "title": "Stateful Detection of Model Extraction Attacks. (arXiv:2107.05166v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1\">Mohamad H Danesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1\">Alan Fern</a>",
          "description": "We study the problem of out-of-distribution dynamics (OODD) detection, which\ninvolves detecting when the dynamics of a temporal process change compared to\nthe training-distribution dynamics. This is relevant to applications in\ncontrol, reinforcement learning (RL), and multi-variate time-series, where\nchanges to test time dynamics can impact the performance of learning\ncontrollers/predictors in unknown ways. This problem is particularly important\nin the context of deep RL, where learned controllers often overfit to the\ntraining environment. Currently, however, there is a lack of established OODD\nbenchmarks for the types of environments commonly used in RL research. Our\nfirst contribution is to design a set of OODD benchmarks derived from common RL\nenvironments with varying types and intensities of OODD. Our second\ncontribution is to design a strong OODD baseline approach based on recurrent\nimplicit quantile networks (RIQNs), which monitors autoregressive prediction\nerrors for OODD detection. Our final contribution is to evaluate the RIQN\napproach on the benchmarks to provide baseline results for future comparison.",
          "link": "http://arxiv.org/abs/2107.04982",
          "publishedOn": "2021-07-13T01:59:36.960Z",
          "wordCount": 603,
          "title": "Out-of-Distribution Dynamics Detection: RL-Relevant Benchmarks and Results. (arXiv:2107.04982v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Domingo_Enrich_C/0/1/0/all/0/1\">Carles Domingo-Enrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bietti_A/0/1/0/all/0/1\">Alberto Bietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrie_M/0/1/0/all/0/1\">Marylou Gabri&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1\">Eric Vanden-Eijnden</a>",
          "description": "Energy-based models (EBMs) are generative models that are usually trained via\nmaximum likelihood estimation. This approach becomes challenging in generic\nsituations where the trained energy is nonconvex, due to the need to sample the\nGibbs distribution associated with this energy. Using general Fenchel duality\nresults, we derive variational principles dual to maximum likelihood EBMs with\nshallow overparametrized neural network energies, both in the active (aka\nfeature-learning) and lazy regimes. In the active regime, this dual formulation\nleads to a training algorithm in which one updates concurrently the particles\nin the sample space and the neurons in the parameter space of the energy. We\nalso consider a variant of this algorithm in which the particles are sometimes\nrestarted at random samples drawn from the data set, and show that performing\nthese restarts at every iteration step corresponds to score matching training.\nUsing intermediate parameter setups in our dual algorithm thereby gives a way\nto interpolate between maximum likelihood and score matching training. These\nresults are illustrated in simple numerical experiments.",
          "link": "http://arxiv.org/abs/2107.05134",
          "publishedOn": "2021-07-13T01:59:36.927Z",
          "wordCount": 616,
          "title": "Dual Training of Energy-Based Models with Overparametrized Shallow Neural Networks. (arXiv:2107.05134v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zakazov_I/0/1/0/all/0/1\">Ivan Zakazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirokikh_B/0/1/0/all/0/1\">Boris Shirokikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Alexey Chernyavskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>",
          "description": "Domain Adaptation (DA) methods are widely used in medical image segmentation\ntasks to tackle the problem of differently distributed train (source) and test\n(target) data. We consider the supervised DA task with a limited number of\nannotated samples from the target domain. It corresponds to one of the most\nrelevant clinical setups: building a sufficiently accurate model on the minimum\npossible amount of annotated data. Existing methods mostly fine-tune specific\nlayers of the pretrained Convolutional Neural Network (CNN). However, there is\nno consensus on which layers are better to fine-tune, e.g. the first layers for\nimages with low-level domain shift or the deeper layers for images with\nhigh-level domain shift. To this end, we propose SpotTUnet - a CNN architecture\nthat automatically chooses the layers which should be optimally fine-tuned.\nMore specifically, on the target domain, our method additionally learns the\npolicy that indicates whether a specific layer should be fine-tuned or reused\nfrom the pretrained network. We show that our method performs at the same level\nas the best of the nonflexible fine-tuning methods even under the extreme\nscarcity of annotated data. Secondly, we show that SpotTUnet policy provides a\nlayer-wise visualization of the domain shift impact on the network, which could\nbe further used to develop robust domain generalization methods. In order to\nextensively evaluate SpotTUnet performance, we use a publicly available dataset\nof brain MR images (CC359), characterized by explicit domain shift. We release\na reproducible experimental pipeline.",
          "link": "http://arxiv.org/abs/2107.04914",
          "publishedOn": "2021-07-13T01:59:36.921Z",
          "wordCount": 698,
          "title": "Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation. (arXiv:2107.04914v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaobo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shuo Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Haikun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>",
          "description": "Estimating the kernel mean in a reproducing kernel Hilbert space is a\ncritical component in many kernel learning algorithms. Given a finite sample,\nthe standard estimate of the target kernel mean is the empirical average.\nPrevious works have shown that better estimators can be constructed by\nshrinkage methods. In this work, we propose to corrupt data examples with noise\nfrom known distributions and present a new kernel mean estimator, called the\nmarginalized kernel mean estimator, which estimates kernel mean under the\ncorrupted distribution. Theoretically, we show that the marginalized kernel\nmean estimator introduces implicit regularization in kernel mean estimation.\nEmpirically, we show on a variety of datasets that the marginalized kernel mean\nestimator obtains much lower estimation error than the existing estimators.",
          "link": "http://arxiv.org/abs/2107.04855",
          "publishedOn": "2021-07-13T01:59:36.915Z",
          "wordCount": 561,
          "title": "Kernel Mean Estimation by Marginalized Corrupted Distributions. (arXiv:2107.04855v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1\">Lijun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hengshu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>",
          "description": "Real estate appraisal refers to the process of developing an unbiased opinion\nfor real property's market value, which plays a vital role in decision-making\nfor various players in the marketplace (e.g., real estate agents, appraisers,\nlenders, and buyers). However, it is a nontrivial task for accurate real estate\nappraisal because of three major challenges: (1) The complicated influencing\nfactors for property value; (2) The asynchronously spatiotemporal dependencies\namong real estate transactions; (3) The diversified correlations between\nresidential communities. To this end, we propose a Multi-Task Hierarchical\nGraph Representation Learning (MugRep) framework for accurate real estate\nappraisal. Specifically, by acquiring and integrating multi-source urban data,\nwe first construct a rich feature set to comprehensively profile the real\nestate from multiple perspectives (e.g., geographical distribution, human\nmobility distribution, and resident demographics distribution). Then, an\nevolving real estate transaction graph and a corresponding event graph\nconvolution module are proposed to incorporate asynchronously spatiotemporal\ndependencies among real estate transactions. Moreover, to further incorporate\nvaluable knowledge from the view of residential communities, we devise a\nhierarchical heterogeneous community graph convolution module to capture\ndiversified correlations between residential communities. Finally, an urban\ndistrict partitioned multi-task learning module is introduced to generate\ndifferently distributed value opinions for real estate. Extensive experiments\non two real-world datasets demonstrate the effectiveness of MugRep and its\ncomponents and features.",
          "link": "http://arxiv.org/abs/2107.05180",
          "publishedOn": "2021-07-13T01:59:36.898Z",
          "wordCount": 668,
          "title": "MugRep: A Multi-Task Hierarchical Graph Representation Learning Framework for Real Estate Appraisal. (arXiv:2107.05180v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-En Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">En-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_P/0/1/0/all/0/1\">Pei-Yung Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Li-Chen Fu</a>",
          "description": "Recently, there has been a panoptic segmentation task combining semantic and\ninstance segmentation, in which the goal is to classify each pixel with the\ncorresponding instance ID. In this work, we propose a solution to tackle the\npanoptic segmentation task. The overall structure combines the bottom-up method\nand the top-down method. Therefore, not only can there be better performance,\nbut also the execution speed can be maintained. The network mainly pays\nattention to the quality of the mask. In the previous work, we can see that the\nuneven contour of the object is more likely to appear, resulting in low-quality\nprediction. Accordingly, we propose enhancement features and corresponding loss\nfunctions for the silhouette of objects and backgrounds to improve the mask.\nMeanwhile, we use the new proposed confidence score to solve the occlusion\nproblem and make the network tend to use higher quality masks as prediction\nresults. To verify our research, we used the COCO dataset and CityScapes\ndataset to do experiments and obtained competitive results with fast inference\ntime.",
          "link": "http://arxiv.org/abs/2107.05093",
          "publishedOn": "2021-07-13T01:59:36.891Z",
          "wordCount": 614,
          "title": "SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network. (arXiv:2107.05093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1\">Qiyou Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghauch_H/0/1/0/all/0/1\">Hadi Ghauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>",
          "description": "Data representation techniques have made a substantial contribution to\nadvancing data processing and machine learning (ML). Improving predictive power\nwas the focus of previous representation techniques, which unfortunately\nperform rather poorly on the interpretability in terms of extracting underlying\ninsights of the data. Recently, Kolmogorov model (KM) was studied, which is an\ninterpretable and predictable representation approach to learning the\nunderlying probabilistic structure of a set of random variables. The existing\nKM learning algorithms using semi-definite relaxation with randomization\n(SDRwR) or discrete monotonic optimization (DMO) have, however, limited utility\nto big data applications because they do not scale well computationally. In\nthis paper, we propose a computationally scalable KM learning algorithm, based\non the regularized dual optimization combined with enhanced gradient descent\n(GD) method. To make our method more scalable to large-dimensional problems, we\npropose two acceleration schemes, namely, eigenvalue decomposition (EVD)\nelimination strategy and proximal EVD algorithm. Furthermore, a thresholding\ntechnique by exploiting the approximation error analysis and leveraging the\nnormalized Minkowski $\\ell_1$-norm and its bounds, is provided for the\nselection of the number of iterations of the proximal EVD algorithm. When\napplied to big data applications, it is demonstrated that the proposed method\ncan achieve compatible training/prediction performance with significantly\nreduced computational complexity; roughly two orders of magnitude improvement\nin terms of the time overhead, compared to the existing KM learning algorithms.\nFurthermore, it is shown that the accuracy of logical relation mining for\ninterpretability by using the proposed KM learning algorithm exceeds $80\\%$.",
          "link": "http://arxiv.org/abs/2107.05011",
          "publishedOn": "2021-07-13T01:59:36.873Z",
          "wordCount": 691,
          "title": "Dual Optimization for Kolmogorov Model Learning Using Enhanced Gradient Descent. (arXiv:2107.05011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wight_C/0/1/0/all/0/1\">Colby Wight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akers_S/0/1/0/all/0/1\">Sarah Akers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1\">Scott Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Woongjo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosink_L/0/1/0/all/0/1\">Luke Gosink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurrus_E/0/1/0/all/0/1\">Elizabeth Jurrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kappagantula_K/0/1/0/all/0/1\">Keerti Kappagantula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1\">Tegan H. Emerson</a>",
          "description": "As both machine learning models and the datasets on which they are evaluated\nhave grown in size and complexity, the practice of using a few summary\nstatistics to understand model performance has become increasingly problematic.\nThis is particularly true in real-world scenarios where understanding model\nfailure on certain subpopulations of the data is of critical importance. In\nthis paper we propose a topological framework for evaluating machine learning\nmodels in which a dataset is treated as a \"space\" on which a model operates.\nThis provides us with a principled way to organize information about model\nperformance at both the global level (over the entire test set) and also the\nlocal level (on specific subpopulations). Finally, we describe a topological\ndata structure, presheaves, which offer a convenient way to store and analyze\nmodel performance between different subpopulations.",
          "link": "http://arxiv.org/abs/2107.04714",
          "publishedOn": "2021-07-13T01:59:36.868Z",
          "wordCount": 598,
          "title": "A Topological-Framework to Improve Analysis of Machine Learning Model Performance. (arXiv:2107.04714v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Shalini Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_J/0/1/0/all/0/1\">Jaideep Srivastava</a>",
          "description": "Many machine learning models have been built to tackle information overload\nissues on Massive Open Online Courses (MOOC) platforms. These models rely on\nlearning powerful representations of MOOC entities. However, they suffer from\nthe problem of scarce expert label data. To overcome this problem, we propose\nto learn pre-trained representations of MOOC entities using abundant unlabeled\ndata from the structure of MOOCs which can directly be applied to the\ndownstream tasks. While existing pre-training methods have been successful in\nNLP areas as they learn powerful textual representation, their models do not\nleverage the richer information about MOOC entities. This richer information\nincludes the graph relationship between the lectures, concepts, and courses\nalong with the domain knowledge about the complexity of a concept. We develop\nMOOCRep, a novel method based on Transformer language model trained with two\npre-training objectives : 1) graph-based objective to capture the powerful\nsignal of entities and relations that exist in the graph, and 2)\ndomain-oriented objective to effectively incorporate the complexity level of\nconcepts. Our experiments reveal that MOOCRep's embeddings outperform\nstate-of-the-art representation learning methods on two tasks important for\neducation community, concept pre-requisite prediction and lecture\nrecommendation.",
          "link": "http://arxiv.org/abs/2107.05154",
          "publishedOn": "2021-07-13T01:59:36.861Z",
          "wordCount": 622,
          "title": "MOOCRep: A Unified Pre-trained Embedding of MOOC Entities. (arXiv:2107.05154v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haodong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yabo Chu</a>",
          "description": "Social-aware recommendation approaches have been recognized as an effective\nway to solve the data sparsity issue of traditional recommender systems. The\nassumption behind is that the knowledge in social user-user connections can be\nshared and transferred to the domain of user-item interactions, whereby to help\nlearn user preferences. However, most existing approaches merely adopt the\nfirst-order connections among users during transfer learning, ignoring those\nconnections in higher orders. We argue that better recommendation performance\ncan also benefit from high-order social relations. In this paper, we propose a\nnovel Propagation-aware Transfer Learning Network (PTLN) based on the\npropagation of social relations. We aim to better mine the sharing knowledge\nhidden in social networks and thus further improve recommendation performance.\nSpecifically, we explore social influence in two aspects: (a) higher-order\nfriends have been taken into consideration by order bias; (b) different friends\nin the same order will have distinct importance for recommendation by an\nattention mechanism. Besides, we design a novel regularization to bridge the\ngap between social relations and user-item interactions. We conduct extensive\nexperiments on two real-world datasets and beat other counterparts in terms of\nranking accuracy, especially for the cold-start users with few historical\ninteractions.",
          "link": "http://arxiv.org/abs/2107.04846",
          "publishedOn": "2021-07-13T01:59:36.843Z",
          "wordCount": 625,
          "title": "Propagation-aware Social Recommendation by Transfer Learning. (arXiv:2107.04846v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05007",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Meldgaard_S/0/1/0/all/0/1\">S&#xf8;ren Ager Meldgaard</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mortensen_H/0/1/0/all/0/1\">Henrik Lund Mortensen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Christiansen_M/0/1/0/all/0/1\">Mads-Peter V. Christiansen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Noe_F/0/1/0/all/0/1\">Frank No&#xe9;</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hammer_B/0/1/0/all/0/1\">Bj&#xf8;rk Hammer</a>",
          "description": "Chemical space is routinely explored by machine learning methods to discover\ninteresting molecules, before time-consuming experimental synthesizing is\nattempted. However, these methods often rely on a graph representation,\nignoring 3D information necessary for determining the stability of the\nmolecules. We propose a reinforcement learning approach for generating\nmolecules in cartesian coordinates allowing for quantum chemical prediction of\nthe stability. To improve sample-efficiency we learn basic chemical rules from\nimitation learning on the GDB-11 database to create an initial model applicable\nfor all stoichiometries. We then deploy multiple copies of the model\nconditioned on a specific stoichiometry in a reinforcement learning setting.\nThe models correctly identify low energy molecules in the database and produce\nnovel isomers not found in the training set. Finally, we apply the model to\nlarger molecules to show how reinforcement learning further refines the\nimitation learning model in domains far from the training data.",
          "link": "http://arxiv.org/abs/2107.05007",
          "publishedOn": "2021-07-13T01:59:36.837Z",
          "wordCount": 588,
          "title": "Generating stable molecules using imitation and reinforcement learning. (arXiv:2107.05007v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mehdi Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1\">Max Berrendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thost_V/0/1/0/all/0/1\">Veronika Thost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>",
          "description": "For many years, link prediction on knowledge graphs (KGs) has been a purely\ntransductive task, not allowing for reasoning on unseen entities. Recently,\nincreasing efforts are put into exploring semi- and fully inductive scenarios,\nenabling inference over unseen and emerging entities. Still, all these\napproaches only consider triple-based \\glspl{kg}, whereas their richer\ncounterparts, hyper-relational KGs (e.g., Wikidata), have not yet been properly\nstudied. In this work, we classify different inductive settings and study the\nbenefits of employing hyper-relational KGs on a wide range of semi- and fully\ninductive link prediction tasks powered by recent advancements in graph neural\nnetworks. Our experiments on a novel set of benchmarks show that qualifiers\nover typed edges can lead to performance improvements of 6% of absolute gains\n(for the Hits@10 metric) compared to triple-only baselines. Our code is\navailable at \\url{https://github.com/mali-git/hyper_relational_ilp}.",
          "link": "http://arxiv.org/abs/2107.04894",
          "publishedOn": "2021-07-13T01:59:36.831Z",
          "wordCount": 572,
          "title": "Improving Inductive Link Prediction Using Hyper-Relational Facts. (arXiv:2107.04894v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1\">Kin Wai Cheuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>",
          "description": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.",
          "link": "http://arxiv.org/abs/2107.04954",
          "publishedOn": "2021-07-13T01:59:36.826Z",
          "wordCount": 612,
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1\">Gabriel de Souza P. Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1\">Sara Rabhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ak_R/0/1/0/all/0/1\">Ronay Ak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md Yasin Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1\">Even Oldridge</a>",
          "description": "Session-based recommendation is an important task for e-commerce services,\nwhere a large number of users browse anonymously or may have very distinct\ninterests for different sessions. In this paper we present one of the winning\nsolutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce\nData Challenge. Our solution was inspired by NLP techniques and consists of an\nensemble of two Transformer architectures - Transformer-XL and XLNet - trained\nwith autoregressive and autoencoding approaches. To leverage most of the rich\ndataset made available for the competition, we describe how we prepared\nmulti-model features by combining tabular events with textual and image\nvectors. We also present a model prediction analysis to better understand the\neffectiveness of our architectures for the session-based recommendation.",
          "link": "http://arxiv.org/abs/2107.05124",
          "publishedOn": "2021-07-13T01:59:36.819Z",
          "wordCount": 593,
          "title": "Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. (arXiv:2107.05124v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04766",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kang_L/0/1/0/all/0/1\">Lican Kang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1\">Yanyan Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1\">Youzhou Zhou</a>",
          "description": "Schr\\\"{o}dinger-F\\\"{o}llmer sampler (SFS) is a novel and efficient approach\nfor sampling from possibly unnormalized distributions without ergodicity. SFS\nis based on the Euler-Maruyama discretization of Schr\\\"{o}dinger-F\\\"{o}llmer\ndiffusion process $$\\mathrm{d} X_{t}=-\\nabla U\\left(X_t, t\\right) \\mathrm{d}\nt+\\mathrm{d} B_{t}, \\quad t \\in[0,1],\\quad X_0=0$$ on the unit interval, which\ntransports the degenerate distribution at time zero to the target distribution\nat time one. In \\cite{sfs21}, the consistency of SFS is established under a\nrestricted assumption that %the drift term $b(x,t)$ the potential $U(x,t)$ is\nuniformly (on $t$) strongly %concave convex (on $x$). In this paper we provide\na nonasymptotic error bound of SFS in Wasserstein distance under some smooth\nand bounded conditions on the density ratio of the target distribution over the\nstandard normal distribution, but without requiring the strongly convexity of\nthe potential.",
          "link": "http://arxiv.org/abs/2107.04766",
          "publishedOn": "2021-07-13T01:59:36.803Z",
          "wordCount": 568,
          "title": "Convergence Analysis of Schr{\\\"o}dinger-F{\\\"o}llmer Sampler without Convexity. (arXiv:2107.04766v1 [stat.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vomvas_M/0/1/0/all/0/1\">Marinos Vomvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_Huu_T/0/1/0/all/0/1\">Triet Vo-Huu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noubir_G/0/1/0/all/0/1\">Guevara Noubir</a>",
          "description": "RF emissions detection, classification, and spectro-temporal localization are\ncrucial not only for tasks relating to understanding, managing, and protecting\nthe RF spectrum, but also for safety and security applications such as\ndetecting intruding drones or jammers. Achieving this goal for wideband\nspectrum and in real-time performance is a challenging problem. We present\nWRIST, a Wideband, Real-time RF Identification system with Spectro-Temporal\ndetection, framework and system. Our resulting deep learning model is capable\nto detect, classify, and precisely locate RF emissions in time and frequency\nusing RF samples of 100 MHz spectrum in real-time (over 6Gbps incoming I&Q\nstreams). Such capabilities are made feasible by leveraging a deep-learning\nbased one-stage object detection framework, and transfer learning to a\nmulti-channel image-based RF signals representation. We also introduce an\niterative training approach which leverages synthesized and augmented RF data\nto efficiently build large labelled datasets of RF emissions (SPREAD). WRIST\ndetector achieves 90 mean Average Precision even in extremely congested\nenvironment in the wild. WRIST model classifies five technologies (Bluetooth,\nLightbridge, Wi-Fi, XPD, and ZigBee) and is easily extendable to others. We are\nmaking our curated and annotated dataset available to the whole community. It\nconsists of nearly 1 million fully labelled RF emissions collected from various\noff-the-shelf wireless radios in a range of environments and spanning the five\nclasses of emissions.",
          "link": "http://arxiv.org/abs/2107.05114",
          "publishedOn": "2021-07-13T01:59:36.796Z",
          "wordCount": 654,
          "title": "Spectro-Temporal RF Identification using Deep Learning. (arXiv:2107.05114v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04831",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pfitzinger_J/0/1/0/all/0/1\">Johann Pfitzinger</a>",
          "description": "Prediction tasks with high-dimensional nonorthogonal predictor sets pose a\nchallenge for least squares based fitting procedures. A large and productive\nliterature exists, discussing various regularized approaches to improving the\nout-of-sample robustness of parameter estimates. This paper proposes a novel\ncluster-based regularization - the hierarchical feature regression (HFR) -,\nwhich mobilizes insights from the domains of machine learning and graph theory\nto estimate parameters along a supervised hierarchical representation of the\npredictor set, shrinking parameters towards group targets. The method is\ninnovative in its ability to estimate optimal compositions of predictor groups,\nas well as the group targets endogenously. The HFR can be viewed as a\nsupervised factor regression, with the strength of shrinkage governed by a\npenalty on the extent of idiosyncratic variation captured in the fitting\nprocess. The method demonstrates good predictive accuracy and versatility,\noutperforming a panel of benchmark regularized estimators across a diverse set\nof simulated regression tasks, including dense, sparse and grouped data\ngenerating processes. An application to the prediction of economic growth is\nused to illustrate the HFR's effectiveness in an empirical setting, with\nfavorable comparisons to several frequentist and Bayesian alternatives.",
          "link": "http://arxiv.org/abs/2107.04831",
          "publishedOn": "2021-07-13T01:59:36.790Z",
          "wordCount": 613,
          "title": "Cluster Regularization via a Hierarchical Feature Regression. (arXiv:2107.04831v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Ye Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shao-Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sheng-Jun Huang</a>",
          "description": "Traditional supervised learning requires ground truth labels for the training\ndata, whose collection can be difficult in many cases. Recently, crowdsourcing\nhas established itself as an efficient labeling solution through resorting to\nnon-expert crowds. To reduce the labeling error effects, one common practice is\nto distribute each instance to multiple workers, whereas each worker only\nannotates a subset of data, resulting in the {\\it sparse annotation}\nphenomenon. In this paper, we note that when meeting with class-imbalance,\ni.e., when the ground truth labels are {\\it class-imbalanced}, the sparse\nannotations are prone to be skewly distributed, which thus can severely bias\nthe learning algorithm. To combat this issue, we propose one self-training\nbased approach named {\\it Self-Crowd} by progressively adding confident\npseudo-annotations and rebalancing the annotation distribution. Specifically,\nwe propose one distribution aware confidence measure to select confident\npseudo-annotations, which adopts the resampling strategy to oversample the\nminority annotations and undersample the majority annotations. On one\nreal-world crowdsourcing image classification task, we show that the proposed\nmethod yields more balanced annotations throughout training than the\ndistribution agnostic methods and substantially improves the learning\nperformance at different annotation sparsity levels.",
          "link": "http://arxiv.org/abs/2107.05039",
          "publishedOn": "2021-07-13T01:59:36.783Z",
          "wordCount": 616,
          "title": "Learning from Crowds with Sparse and Imbalanced Annotations. (arXiv:2107.05039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_B/0/1/0/all/0/1\">Ben Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully\ncausal approach to neural audio synthesis which operates directly in the\nwaveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU\ninference. The NEWT uses time-distributed multilayer perceptrons with periodic\nactivations to implicitly learn nonlinear transfer functions that encode the\ncharacteristics of a target timbre. Once trained, a NEWT can produce complex\ntimbral evolutions by simple affine transformations of its input and output\nsignals. We paired the NEWT with a differentiable noise synthesiser and reverb\nand found it capable of generating realistic musical instrument performances\nwith only 260k total model parameters, conditioned on F0 and loudness features.\nWe compared our method to state-of-the-art benchmarks with a multi-stimulus\nlistening test and the Fr\\'echet Audio Distance and found it performed\ncompetitively across the tested timbral domains. Our method significantly\noutperformed the benchmarks in terms of generation speed, and achieved\nreal-time performance on a consumer CPU, both with and without FastNEWT,\nsuggesting it is a viable basis for future creative sound design tools.",
          "link": "http://arxiv.org/abs/2107.05050",
          "publishedOn": "2021-07-13T01:59:36.777Z",
          "wordCount": 614,
          "title": "Neural Waveshaping Synthesis. (arXiv:2107.05050v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Rose McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>",
          "description": "It is challenging for humans to enable visual knowledge discovery in data\nwith more than 2-3 dimensions with a naked eye. This chapter explores the\nefficiency of discovering predictive machine learning models interactively\nusing new Elliptic Paired coordinates (EPC) visualizations. It is shown that\nEPC are capable to visualize multidimensional data and support visual machine\nlearning with preservation of multidimensional information in 2-D. Relative to\nparallel and radial coordinates, EPC visualization requires only a half of the\nvisual elements for each n-D point. An interactive software system EllipseVis,\nwhich is developed in this work, processes high-dimensional datasets, creates\nEPC visualizations, and produces predictive classification models by\ndiscovering dominance rules in EPC. By using interactive and automatic\nprocesses it discovers zones in EPC with a high dominance of a single class.\nThe EPC methodology has been successful in discovering non-linear predictive\nmodels with high coverage and precision in the computational experiments. This\ncan benefit multiple domains by producing visually appealing dominance rules.\nThis chapter presents results of successful testing the EPC non-linear\nmethodology in experiments using real and simulated data, EPC generalized to\nthe Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of\ncoordinates to optimize the visual discovery, introduction of an alternative\nEPC design and introduction of the concept of incompact machine learning\nmethodology based on EPC/DEPC.",
          "link": "http://arxiv.org/abs/2107.04974",
          "publishedOn": "2021-07-13T01:59:36.771Z",
          "wordCount": 660,
          "title": "Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates. (arXiv:2107.04974v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yunsong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stearrett_R/0/1/0/all/0/1\">Ryan Stearrett</a>",
          "description": "A cross-benchmark has been done on three critical aspects, data imputing,\nfeature selection and regression algorithms, for machine learning based\nchemical vapor deposition (CVD) virtual metrology (VM). The result reveals that\nlinear feature selection regression algorithm would extensively under-fit the\nVM data. Data imputing is also necessary to achieve a higher prediction\naccuracy as the data availability is only ~70% when optimal accuracy is\nobtained. This work suggests a nonlinear feature selection and regression\nalgorithm combined with nearest data imputing algorithm would provide a\nprediction accuracy as high as 0.7. This would lead to 70% reduced CVD\nprocessing variation, which is believed to will lead to reduced frequency of\nphysical metrology as well as more reliable mass-produced wafer with improved\nquality.",
          "link": "http://arxiv.org/abs/2107.05071",
          "publishedOn": "2021-07-13T01:59:36.754Z",
          "wordCount": 557,
          "title": "Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process. (arXiv:2107.05071v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>",
          "description": "The advancement of convolutional neural networks (CNNs) on various vision\napplications has attracted lots of attention. Yet the majority of CNNs are\nunable to satisfy the strict requirement for real-world deployment. To overcome\nthis, the recent popular network pruning is an effective method to reduce the\nredundancy of the models. However, the ranking of filters according to their\n\"importance\" on different pruning criteria may be inconsistent. One filter\ncould be important according to a certain criterion, while it is unnecessary\naccording to another one, which indicates that each criterion is only a partial\nview of the comprehensive \"importance\". From this motivation, we propose a\nnovel framework to integrate the existing filter pruning criteria by exploring\nthe criteria diversity. The proposed framework contains two stages: Criteria\nClustering and Filters Importance Calibration. First, we condense the pruning\ncriteria via layerwise clustering based on the rank of \"importance\" score.\nSecond, within each cluster, we propose a calibration factor to adjust their\nsignificance for each selected blending candidates and search for the optimal\nblending criterion via Evolutionary Algorithm. Quantitative results on the\nCIFAR-100 and ImageNet benchmarks show that our framework outperforms the\nstate-of-the-art baselines, regrading to the compact model performance after\npruning.",
          "link": "http://arxiv.org/abs/2107.05033",
          "publishedOn": "2021-07-13T01:59:36.746Z",
          "wordCount": 642,
          "title": "Blending Pruning Criteria for Convolutional Neural Networks. (arXiv:2107.05033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kousaridas_A/0/1/0/all/0/1\">Apostolos Kousaridas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_R/0/1/0/all/0/1\">Ramya Panthangi Manjunath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdomo_J/0/1/0/all/0/1\">Jose Mauricio Perdomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zielinski_E/0/1/0/all/0/1\">Ernst Zielinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitz_S/0/1/0/all/0/1\">Steffen Schmitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfadler_A/0/1/0/all/0/1\">Andreas Pfadler</a>",
          "description": "5G communication system can support the demanding quality-of-service (QoS)\nrequirements of many advanced vehicle-to-everything (V2X) use cases. However,\nthe safe and efficient driving, especially of automated vehicles, may be\naffected by sudden changes of the provided QoS. For that reason, the prediction\nof the QoS changes and the early notification of these predicted changes to the\nvehicles have been recently enabled by 5G communication systems. This solution\nenables the vehicles to avoid or mitigate the effect of sudden QoS changes at\nthe application level. This article describes how QoS prediction could be\ngenerated by a 5G communication system and delivered to a V2X application. The\ntele-operated driving use case is used as an example to analyze the feasibility\nof a QoS prediction scheme. Useful recommendations for the development of a QoS\nprediction solution are provided, while open research topics are identified.",
          "link": "http://arxiv.org/abs/2107.05000",
          "publishedOn": "2021-07-13T01:59:36.707Z",
          "wordCount": 603,
          "title": "QoS Prediction for 5G Connected and Automated Driving. (arXiv:2107.05000v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1\">Satyen Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1\">Ayush Sekhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1\">Karthik Sridharan</a>",
          "description": "Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the\nmethod of choice for learning with large over-parameterized models. A popular\ntheory for explaining why SGD works well in practice is that the algorithm has\nan implicit regularization that biases its output towards a good solution.\nPerhaps the theoretically most well understood learning setting for SGD is that\nof Stochastic Convex Optimization (SCO), where it is well known that SGD learns\nat a rate of $O(1/\\sqrt{n})$, where $n$ is the number of samples. In this\npaper, we consider the problem of SCO and explore the role of implicit\nregularization, batch size and multiple epochs for SGD. Our main contributions\nare threefold:\n\n(a) We show that for any regularizer, there is an SCO problem for which\nRegularized Empirical Risk Minimzation fails to learn. This automatically rules\nout any implicit regularization based explanation for the success of SGD.\n\n(b) We provide a separation between SGD and learning via Gradient Descent on\nempirical loss (GD) in terms of sample complexity. We show that there is an SCO\nproblem such that GD with any step size and number of iterations can only learn\nat a suboptimal rate: at least $\\widetilde{\\Omega}(1/n^{5/12})$.\n\n(c) We present a multi-epoch variant of SGD commonly used in practice. We\nprove that this algorithm is at least as good as single pass SGD in the worst\ncase. However, for certain SCO problems, taking multiple passes over the\ndataset can significantly outperform single pass SGD.\n\nWe extend our results to the general learning setting by showing a problem\nwhich is learnable for any data distribution, and for this problem, SGD is\nstrictly better than RERM for any regularization function. We conclude by\ndiscussing the implications of our results for deep learning, and show a\nseparation between SGD and ERM for two layer diagonal neural networks.",
          "link": "http://arxiv.org/abs/2107.05074",
          "publishedOn": "2021-07-13T01:59:36.660Z",
          "wordCount": 735,
          "title": "SGD: The Role of Implicit Regularization, Batch-size and Multiple-epochs. (arXiv:2107.05074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaozhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_E/0/1/0/all/0/1\">Ensheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Recently, deep learning methods have become mainstream in code search since\nthey do better at capturing semantic correlations between code snippets and\nsearch queries and have promising performance. However, code snippets have\ndiverse information from different dimensions, such as business logic, specific\nalgorithm, and hardware communication, so it is hard for a single code\nrepresentation module to cover all the perspectives. On the other hand, as a\nspecific query may focus on one or several perspectives, it is difficult for a\nsingle query representation module to represent different user intents. In this\npaper, we propose MuCoS, a multi-model ensemble learning architecture for\nsemantic code search. It combines several individual learners, each of which\nemphasizes a specific perspective of code snippets. We train the individual\nlearners on different datasets which contain different perspectives of code\ninformation, and we use a data augmentation strategy to get these different\ndatasets. Then we ensemble the learners to capture comprehensive features of\ncode snippets.",
          "link": "http://arxiv.org/abs/2107.04773",
          "publishedOn": "2021-07-13T01:59:36.653Z",
          "wordCount": 612,
          "title": "Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning for Semantic Code Search. (arXiv:2107.04773v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04973",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shankar_R/0/1/0/all/0/1\">Ravi Shankar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Venkataraman_A/0/1/0/all/0/1\">Archana Venkataraman</a>",
          "description": "We propose the first method to adaptively modify the duration of a given\nspeech signal. Our approach uses a Bayesian framework to define a latent\nattention map that links frames of the input and target utterances. We train a\nmasked convolutional encoder-decoder network to produce this attention map via\na stochastic version of the mean absolute error loss function; our model also\npredicts the length of the target speech signal using the encoder embeddings.\nThe predicted length determines the number of steps for the decoder operation.\nDuring inference, we generate the attention map as a proxy for the similarity\nmatrix between the given input speech and an unknown target speech signal.\nUsing this similarity matrix, we compute a warping path of alignment between\nthe two signals. Our experiments demonstrate that this adaptive framework\nproduces similar results to dynamic time warping, which relies on a known\ntarget signal, on both voice conversion and emotion conversion tasks. We also\nshow that our technique results in a high quality of generated speech that is\non par with state-of-the-art vocoders.",
          "link": "http://arxiv.org/abs/2107.04973",
          "publishedOn": "2021-07-13T01:59:36.645Z",
          "wordCount": 625,
          "title": "A Deep-Bayesian Framework for Adaptive Speech Duration Modification. (arXiv:2107.04973v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yuanyi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jian Peng</a>",
          "description": "The control variates (CV) method is widely used in policy gradient estimation\nto reduce the variance of the gradient estimators in practice. A control\nvariate is applied by subtracting a baseline function from the state-action\nvalue estimates. Then the variance-reduced policy gradient presumably leads to\nhigher learning efficiency. Recent research on control variates with deep\nneural net policies mainly focuses on scalar-valued baseline functions. The\neffect of vector-valued baselines is under-explored. This paper investigates\nvariance reduction with coordinate-wise and layer-wise control variates\nconstructed from vector-valued baselines for neural net policies. We present\nexperimental evidence suggesting that lower variance can be obtained with such\nbaselines than with the conventional scalar-valued baseline. We demonstrate how\nto equip the popular Proximal Policy Optimization (PPO) algorithm with these\nnew control variates. We show that the resulting algorithm with proper\nregularization can achieve higher sample efficiency than scalar control\nvariates in continuous control benchmarks.",
          "link": "http://arxiv.org/abs/2107.04987",
          "publishedOn": "2021-07-13T01:59:36.639Z",
          "wordCount": 580,
          "title": "Coordinate-wise Control Variates for Deep Policy Gradients. (arXiv:2107.04987v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05001",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nisimov_S/0/1/0/all/0/1\">Shami Nisimov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gurwicz_Y/0/1/0/all/0/1\">Yaniv Gurwicz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rohekar_R/0/1/0/all/0/1\">Raanan Y. Rohekar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Novik_G/0/1/0/all/0/1\">Gal Novik</a>",
          "description": "Causal discovery from observational data is an important tool in many\nbranches of science. Under certain assumptions it allows scientists to explain\nphenomena, predict, and make decisions. In the large sample limit, sound and\ncomplete causal discovery algorithms have been previously introduced, where a\ndirected acyclic graph (DAG), or its equivalence class, representing causal\nrelations is searched. However, in real-world cases, only finite training data\nis available, which limits the power of statistical tests used by these\nalgorithms, leading to errors in the inferred causal model. This is commonly\naddressed by devising a strategy for using as few as possible statistical\ntests. In this paper, we introduce such a strategy in the form of a recursive\nwrapper for existing constraint-based causal discovery algorithms, which\npreserves soundness and completeness. It recursively clusters the observed\nvariables using the normalized min-cut criterion from the outset, and uses a\nbaseline causal discovery algorithm during backtracking for learning local\nsub-graphs. It then combines them and ensures completeness. By an ablation\nstudy, using synthetic data, and by common real-world benchmarks, we\ndemonstrate that our approach requires significantly fewer statistical tests,\nlearns more accurate graphs, and requires shorter run-times than the baseline\nalgorithm.",
          "link": "http://arxiv.org/abs/2107.05001",
          "publishedOn": "2021-07-13T01:59:36.633Z",
          "wordCount": 657,
          "title": "Improving Efficiency and Accuracy of Causal Discovery Using a Hierarchical Wrapper. (arXiv:2107.05001v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1\">Rohan Taori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1\">Shiori Sagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "For machine learning systems to be reliable, we must understand their\nperformance in unseen, out-of-distribution environments. In this paper, we\nempirically show that out-of-distribution performance is strongly correlated\nwith in-distribution performance for a wide range of models and distribution\nshifts. Specifically, we demonstrate strong correlations between\nin-distribution and out-of-distribution performance on variants of CIFAR-10 &\nImageNet, a synthetic pose estimation task derived from YCB objects, satellite\nimagery classification in FMoW-WILDS, and wildlife classification in\niWildCam-WILDS. The strong correlations hold across model architectures,\nhyperparameters, training set size, and training duration, and are more precise\nthan what is expected from existing domain adaptation theory. To complete the\npicture, we also investigate cases where the correlation is weaker, for\ninstance some synthetic distribution shifts from CIFAR-10-C and the tissue\nclassification dataset Camelyon17-WILDS. Finally, we provide a candidate theory\nbased on a Gaussian data model that shows how changes in the data covariance\narising from distribution shift can affect the observed correlations.",
          "link": "http://arxiv.org/abs/2107.04649",
          "publishedOn": "2021-07-13T01:59:36.627Z",
          "wordCount": 614,
          "title": "Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. (arXiv:2107.04649v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kouagou_N/0/1/0/all/0/1\">N&#x27;Dah Jean Kouagou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1\">Stefan Heindorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1\">Caglar Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>",
          "description": "Concept learning approaches based on refinement operators explore partially\nordered solution spaces to compute concepts, which are used as binary\nclassification models for individuals. However, the refinement trees spanned by\nthese approaches can easily grow to millions of nodes for complex learning\nproblems. This leads to refinement-based approaches often failing to detect\noptimal concepts efficiently. In this paper, we propose a supervised machine\nlearning approach for learning concept lengths, which allows predicting the\nlength of the target concept and therefore facilitates the reduction of the\nsearch space during concept learning. To achieve this goal, we compare four\nneural architectures and evaluate them on four benchmark knowledge\ngraphs--Carcinogenesis, Mutagenesis, Semantic Bible, Family Benchmark. Our\nevaluation results suggest that recurrent neural network architectures perform\nbest at concept length prediction with an F-measure of up to 92%. We show that\nintegrating our concept length predictor into the CELOE (Class Expression\nLearner for Ontology Engineering) algorithm improves CELOE's runtime by a\nfactor of up to 13.4 without any significant changes to the quality of the\nresults it generates. For reproducibility, we provide our implementation in the\npublic GitHub repository at\nhttps://github.com/ConceptLengthLearner/ReproducibilityRepo",
          "link": "http://arxiv.org/abs/2107.04911",
          "publishedOn": "2021-07-13T01:59:36.620Z",
          "wordCount": 634,
          "title": "Prediction of concept lengths for fast concept learning in description logics. (arXiv:2107.04911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1\">Florian Tambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giulio Antoniol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>",
          "description": "Deep Neural Networks (DNN) applications are increasingly becoming a part of\nour everyday life, from medical applications to autonomous cars. Traditional\nvalidation of DNN relies on accuracy measures, however, the existence of\nadversarial examples has highlighted the limitations of these accuracy\nmeasures, raising concerns especially when DNN are integrated into\nsafety-critical systems. In this paper, we present HOMRS, an approach to boost\nmetamorphic testing by automatically building a small optimized set of high\norder metamorphic relations from an initial set of elementary metamorphic\nrelations. HOMRS' backbone is a multi-objective search; it exploits ideas drawn\nfrom traditional systems testing such as code coverage, test case, and path\ndiversity. We applied HOMRS to LeNet5 DNN with MNIST dataset and we report\nevidence that it builds a small but effective set of high order transformations\nachieving a 95% kill ratio. Five raters manually labeled a pool of images\nbefore and after high order transformation; Fleiss' Kappa and statistical tests\nconfirmed that they are metamorphic properties. HOMRS built-in relations are\nalso effective to confront adversarial or out-of-distribution examples; HOMRS\ndetected 92% of randomly sampled out-of-distribution images. HOMRS\ntransformations are also suitable for online real-time use.",
          "link": "http://arxiv.org/abs/2107.04863",
          "publishedOn": "2021-07-13T01:59:36.603Z",
          "wordCount": 632,
          "title": "HOMRS: High Order Metamorphic Relations Selector for Deep Neural Networks. (arXiv:2107.04863v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zonghan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>",
          "description": "Graph convolutional networks are becoming indispensable for deep learning\nfrom graph-structured data. Most of the existing graph convolutional networks\nshare two big shortcomings. First, they are essentially low-pass filters, thus\nthe potentially useful middle and high frequency band of graph signals are\nignored. Second, the bandwidth of existing graph convolutional filters is\nfixed. Parameters of a graph convolutional filter only transform the graph\ninputs without changing the curvature of a graph convolutional filter function.\nIn reality, we are uncertain about whether we should retain or cut off the\nfrequency at a certain point unless we have expert domain knowledge. In this\npaper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture\nthe full spectrum of graph signals and automatically update the bandwidth of\ngraph convolutional filters. While it is based on graph spectral theory, our\nAutoGCN is also localized in space and has a spatial form. Experimental results\nshow that AutoGCN achieves significant improvement over baseline methods which\nonly work as low-pass filters.",
          "link": "http://arxiv.org/abs/2107.04755",
          "publishedOn": "2021-07-13T01:59:36.593Z",
          "wordCount": 603,
          "title": "Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic Filtering. (arXiv:2107.04755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lantao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhangjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "Multi-agent imitation learning aims to train multiple agents to perform tasks\nfrom demonstrations by learning a mapping between observations and actions,\nwhich is essential for understanding physical, social, and team-play systems.\nHowever, most existing works on modeling multi-agent interactions typically\nassume that agents make independent decisions based on their observations,\nignoring the complex dependence among agents. In this paper, we propose to use\ncopula, a powerful statistical tool for capturing dependence among random\nvariables, to explicitly model the correlation and coordination in multi-agent\nsystems. Our proposed model is able to separately learn marginals that capture\nthe local behavioral patterns of each individual agent, as well as a copula\nfunction that solely and fully captures the dependence structure among agents.\nExtensive experiments on synthetic and real-world datasets show that our model\noutperforms state-of-the-art baselines across various scenarios in the action\nprediction task, and is able to generate new trajectories close to expert\ndemonstrations.",
          "link": "http://arxiv.org/abs/2107.04750",
          "publishedOn": "2021-07-13T01:59:36.585Z",
          "wordCount": 584,
          "title": "Multi-Agent Imitation Learning with Copulas. (arXiv:2107.04750v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uwimana1_A/0/1/0/all/0/1\">Anisie Uwimana1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>",
          "description": "Deep learning models have become a popular choice for medical image analysis.\nHowever, the poor generalization performance of deep learning models limits\nthem from being deployed in the real world as robustness is critical for\nmedical applications. For instance, the state-of-the-art Convolutional Neural\nNetworks (CNNs) fail to detect adversarial samples or samples drawn\nstatistically far away from the training distribution. In this work, we\nexperimentally evaluate the robustness of a Mahalanobis distance-based\nconfidence score, a simple yet effective method for detecting abnormal input\nsamples, in classifying malaria parasitized cells and uninfected cells. Results\nindicated that the Mahalanobis confidence score detector exhibits improved\nperformance and robustness of deep learning models, and achieves\nstateof-the-art performance on both out-of-distribution (OOD) and adversarial\nsamples.",
          "link": "http://arxiv.org/abs/2107.04882",
          "publishedOn": "2021-07-13T01:59:36.576Z",
          "wordCount": 578,
          "title": "Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis. (arXiv:2107.04882v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_A/0/1/0/all/0/1\">Albert Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1\">Ashwin Balakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>",
          "description": "Reinforcement learning (RL) algorithms have shown impressive success in\nexploring high-dimensional environments to learn complex, long-horizon tasks,\nbut can often exhibit unsafe behaviors and require extensive environment\ninteraction when exploration is unconstrained. A promising strategy for safe\nlearning in dynamically uncertain environments is requiring that the agent can\nrobustly return to states where task success (and therefore safety) can be\nguaranteed. While this approach has been successful in low-dimensions,\nenforcing this constraint in environments with high-dimensional state spaces,\nsuch as images, is challenging. We present Latent Space Safe Sets (LS3), which\nextends this strategy to iterative, long-horizon tasks with image observations\nby using suboptimal demonstrations and a learned dynamics model to restrict\nexploration to the neighborhood of a learned Safe Set where task completion is\nlikely. We evaluate LS3 on 4 domains, including a challenging sequential\npushing task in simulation and a physical cable routing task. We find that LS3\ncan use prior task successes to restrict exploration and learn more efficiently\nthan prior algorithms while satisfying constraints. See\nhttps://tinyurl.com/latent-ss for code and supplementary material.",
          "link": "http://arxiv.org/abs/2107.04775",
          "publishedOn": "2021-07-13T01:59:36.558Z",
          "wordCount": 633,
          "title": "LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Iterative Tasks. (arXiv:2107.04775v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "A unique cognitive capability of humans consists in their ability to acquire\nnew knowledge and skills from a sequence of experiences. Meanwhile, artificial\nintelligence systems are good at learning only the last given task without\nbeing able to remember the databases learnt in the past. We propose a novel\nlifelong learning methodology by employing a Teacher-Student network framework.\nWhile the Student module is trained with a new given database, the Teacher\nmodule would remind the Student about the information learnt in the past. The\nTeacher, implemented by a Generative Adversarial Network (GAN), is trained to\npreserve and replay past knowledge corresponding to the probabilistic\nrepresentations of previously learn databases. Meanwhile, the Student module is\nimplemented by a Variational Autoencoder (VAE) which infers its latent variable\nrepresentation from both the output of the Teacher module as well as from the\nnewly available database. Moreover, the Student module is trained to capture\nboth continuous and discrete underlying data representations across different\ndomains. The proposed lifelong learning framework is applied in supervised,\nsemi-supervised and unsupervised training. The code is available~:\n\\url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}",
          "link": "http://arxiv.org/abs/2107.04689",
          "publishedOn": "2021-07-13T01:59:36.535Z",
          "wordCount": 625,
          "title": "Lifelong Teacher-Student Network Learning. (arXiv:2107.04689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1\">Gaurav Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandok_S/0/1/0/all/0/1\">Shivam Chandok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "A common problem with most zero and few-shot learning approaches is they\nsuffer from bias towards seen classes resulting in sub-optimal performance.\nExisting efforts aim to utilize unlabeled images from unseen classes (i.e\ntransductive zero-shot) during training to enable generalization. However, this\nlimits their use in practical scenarios where data from target unseen classes\nis unavailable or infeasible to collect. In this work, we present a practical\nsetting of inductive zero and few-shot learning, where unlabeled images from\nother out-of-data classes, that do not belong to seen or unseen categories, can\nbe used to improve generalization in any-shot learning. We leverage a\nformulation based on product-of-experts and introduce a new AUD module that\nenables us to use unlabeled samples from out-of-data classes which are usually\neasily available and practically entail no annotation cost. In addition, we\nalso demonstrate the applicability of our model to address a more practical and\nchallenging, Generalized Zero-shot under a limited supervision setting, where\neven base seen classes do not have sufficient annotated samples.",
          "link": "http://arxiv.org/abs/2107.04952",
          "publishedOn": "2021-07-13T01:59:36.528Z",
          "wordCount": 614,
          "title": "Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision. (arXiv:2107.04952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Ben Green</a>",
          "description": "In the face of compounding crises of social and economic inequality, many\nhave turned to algorithmic decision-making to achieve greater fairness in\nsociety. As these efforts intensify, reasoning within the burgeoning field of\n\"algorithmic fairness\" increasingly shapes how fairness manifests in practice.\nThis paper interrogates whether algorithmic fairness provides the appropriate\nconceptual and practical tools for enhancing social equality. I argue that the\ndominant, \"formal\" approach to algorithmic fairness is ill-equipped as a\nframework for pursuing equality, as its narrow frame of analysis generates\nrestrictive approaches to reform. In light of these shortcomings, I propose an\nalternative: a \"substantive\" approach to algorithmic fairness that centers\nopposition to social hierarchies and provides a more expansive analysis of how\nto address inequality. This substantive approach enables more fruitful\ntheorizing about the role of algorithms in combatting oppression. The\ndistinction between formal and substantive algorithmic fairness is exemplified\nby each approach's responses to the \"impossibility of fairness\" (an\nincompatibility between mathematical definitions of algorithmic fairness).\nWhile the formal approach requires us to accept the \"impossibility of fairness\"\nas a harsh limit on efforts to enhance equality, the substantive approach\nallows us to escape the \"impossibility of fairness\" by suggesting reforms that\nare not subject to this false dilemma and that are better equipped to\nameliorate conditions of social oppression.",
          "link": "http://arxiv.org/abs/2107.04642",
          "publishedOn": "2021-07-13T01:59:36.507Z",
          "wordCount": 649,
          "title": "Impossibility of What? Formal and Substantive Equality in Algorithmic Fairness. (arXiv:2107.04642v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shuwei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Co-training, extended from self-training, is one of the frameworks for\nsemi-supervised learning. It works at the cost of training extra classifiers,\nwhere the algorithm should be delicately designed to prevent individual\nclassifiers from collapsing into each other. In this paper, we present a simple\nand efficient co-training algorithm, named Multi-Head Co-Training, for\nsemi-supervised image classification. By integrating base learners into a\nmulti-head structure, the model is in a minimal amount of extra parameters.\nEvery classification head in the unified model interacts with its peers through\na \"Weak and Strong Augmentation\" strategy, achieving single-view co-training\nwithout promoting diversity explicitly. The effectiveness of Multi-Head\nCo-Training is demonstrated in an empirical study on standard semi-supervised\nlearning benchmarks.",
          "link": "http://arxiv.org/abs/2107.04795",
          "publishedOn": "2021-07-13T01:59:36.500Z",
          "wordCount": 547,
          "title": "Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shoaib Ahmed Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>",
          "description": "Common neural network architectures are susceptible to attack by adversarial\nsamples. Neural network architectures are commonly thought of as divided into\nlow-level feature extraction layers and high-level classification layers;\nsusceptibility of networks to adversarial samples is often thought of as a\nproblem related to classification rather than feature extraction. We test this\nidea by selectively retraining different portions of VGG and ResNet\narchitectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and\nadversarial data. Our experimental results show that susceptibility to\nadversarial samples is associated with low-level feature extraction layers.\nTherefore, retraining high-level layers is insufficient for achieving\nrobustness. This phenomenon could have two explanations: either, adversarial\nattacks yield outputs from early layers that are indistinguishable from\nfeatures found in the attack classes, or adversarial attacks yield outputs from\nearly layers that differ statistically from features for non-adversarial\nsamples and do not permit consistent classification by subsequent layers. We\ntest this question by large-scale non-linear dimensionality reduction and\ndensity modeling on distributions of feature vectors in hidden layers and find\nthat the feature distributions between non-adversarial and adversarial samples\ndiffer substantially. Our results provide new insights into the statistical\norigins of adversarial samples and possible defenses.",
          "link": "http://arxiv.org/abs/2107.04827",
          "publishedOn": "2021-07-13T01:59:36.494Z",
          "wordCount": 630,
          "title": "Identifying Layers Susceptible to Adversarial Attacks. (arXiv:2107.04827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_B/0/1/0/all/0/1\">Beongjun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dong-Jun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jaekyun Moon</a>",
          "description": "Federated learning has been spotlighted as a way to train neural networks\nusing distributed data with no need for individual nodes to share data.\nUnfortunately, it has also been shown that adversaries may be able to extract\nlocal data contents off model parameters transmitted during federated learning.\nA recent solution based on the secure aggregation primitive enabled\nprivacy-preserving federated learning, but at the expense of significant extra\ncommunication/computational resources. In this paper, we propose a\nlow-complexity scheme that provides data privacy using substantially reduced\ncommunication/computational resources relative to the existing secure solution.\nThe key idea behind the suggested scheme is to design the topology of\nsecret-sharing nodes as a sparse random graph instead of the complete graph\ncorresponding to the existing solution. We first obtain the necessary and\nsufficient condition on the graph to guarantee both reliability and privacy. We\nthen suggest using the Erd\\H{o}s-R\\'enyi graph in particular and provide\ntheoretical guarantees on the reliability/privacy of the proposed scheme.\nThrough extensive real-world experiments, we demonstrate that our scheme, using\nonly $20 \\sim 30\\%$ of the resources required in the conventional scheme,\nmaintains virtually the same levels of reliability and data privacy in\npractical federated learning systems.",
          "link": "http://arxiv.org/abs/2012.05433",
          "publishedOn": "2021-07-13T01:59:36.419Z",
          "wordCount": 673,
          "title": "Communication-Computation Efficient Secure Aggregation for Federated Learning. (arXiv:2012.05433v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Variational autoencoders (VAEs) provide an effective and simple method for\nmodeling complex distributions. However, training VAEs often requires\nconsiderable hyperparameter tuning to determine the optimal amount of\ninformation retained by the latent variable. We study the impact of calibrated\ndecoders, which learn the uncertainty of the decoding distribution and can\ndetermine this amount of information automatically, on the VAE performance.\nWhile many methods for learning calibrated decoders have been proposed, many of\nthe recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc\nmodifications instead. We perform the first comprehensive comparative analysis\nof calibrated decoder and provide recommendations for simple and effective VAE\ntraining. Our analysis covers a range of image and video datasets and several\nsingle-image and sequential VAE models. We further propose a simple but novel\nmodification to the commonly used Gaussian decoder, which computes the\nprediction variance analytically. We observe empirically that using heuristic\nmodifications is not necessary with our method. Project website is at\nhttps://orybkin.github.io/sigma-vae/",
          "link": "http://arxiv.org/abs/2006.13202",
          "publishedOn": "2021-07-13T01:59:36.413Z",
          "wordCount": 661,
          "title": "Simple and Effective VAE Training with Calibrated Decoders. (arXiv:2006.13202v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1\">Stephen J. Maybank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Extracting accurate foregrounds from natural images benefits many downstream\napplications such as film production and augmented reality. However, the furry\ncharacteristics and various appearance of the foregrounds, e.g., animal and\nportrait, challenge existing matting methods, which usually require extra user\ninputs such as trimap or scribbles. To resolve these problems, we study the\ndistinct roles of semantics and details for image matting and decompose the\ntask into two parallel sub-tasks: high-level semantic segmentation and\nlow-level details matting. Specifically, we propose a novel Glance and Focus\nMatting network (GFM), which employs a shared encoder and two separate decoders\nto learn both tasks in a collaborative manner for end-to-end natural image\nmatting. Besides, due to the limitation of available natural images in the\nmatting task, previous methods typically adopt composite images for training\nand evaluation, which result in limited generalization ability on real-world\nimages. In this paper, we investigate the domain gap issue between composite\nimages and real-world images systematically by conducting comprehensive\nanalyses of various discrepancies between foreground and background images. We\nfind that a carefully designed composition route RSSN that aims to reduce the\ndiscrepancies can lead to a better model with remarkable generalization\nability. Furthermore, we provide a benchmark containing 2,000 high-resolution\nreal-world animal images and 10,000 portrait images along with their manually\nlabeled alpha mattes to serve as a test bed for evaluating matting model's\ngeneralization ability on real-world images. Comprehensive empirical studies\nhave demonstrated that GFM outperforms state-of-the-art methods and effectively\nreduces the generalization error. The code and the dataset will be released.",
          "link": "http://arxiv.org/abs/2010.16188",
          "publishedOn": "2021-07-13T01:59:36.391Z",
          "wordCount": 749,
          "title": "Bridge Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16912",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ibrahim_S/0/1/0/all/0/1\">Shahana Ibrahim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>",
          "description": "Learning the joint probability of random variables (RVs) is the cornerstone\nof statistical signal processing and machine learning. However, direct\nnonparametric estimation for high-dimensional joint probability is in general\nimpossible, due to the curse of dimensionality. Recent work has proposed to\nrecover the joint probability mass function (PMF) of an arbitrary number of RVs\nfrom three-dimensional marginals, leveraging the algebraic properties of\nlow-rank tensor decomposition and the (unknown) dependence among the RVs.\nNonetheless, accurately estimating three-dimensional marginals can still be\ncostly in terms of sample complexity, affecting the performance of this line of\nwork in practice in the sample-starved regime. Using three-dimensional\nmarginals also involves challenging tensor decomposition problems whose\ntractability is unclear. This work puts forth a new framework for learning the\njoint PMF using only pairwise marginals, which naturally enjoys a lower sample\ncomplexity relative to the third-order ones. A coupled nonnegative matrix\nfactorization (CNMF) framework is developed, and its joint PMF recovery\nguarantees under various conditions are analyzed. Our method also features a\nGram--Schmidt (GS)-like algorithm that exhibits competitive runtime\nperformance. The algorithm is shown to provably recover the joint PMF up to\nbounded error in finite iterations, under reasonable conditions. It is also\nshown that a recently proposed economical expectation maximization (EM)\nalgorithm guarantees to improve upon the GS-like algorithm's output, thereby\nfurther lifting up the accuracy and efficiency. Real-data experiments are\nemployed to showcase the effectiveness.",
          "link": "http://arxiv.org/abs/2006.16912",
          "publishedOn": "2021-07-13T01:59:36.384Z",
          "wordCount": 687,
          "title": "Recovering Joint Probability of Discrete Random Variables from Pairwise Marginals. (arXiv:2006.16912v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dromey_B/0/1/0/all/0/1\">Brian Dromey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napolitano_R/0/1/0/all/0/1\">Raffaele Napolitano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peebles_D/0/1/0/all/0/1\">Donald M. Peebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>",
          "description": "During pregnancy, ultrasound examination in the second trimester can assess\nfetal size according to standardized charts. To achieve a reproducible and\naccurate measurement, a sonographer needs to identify three standard 2D planes\nof the fetal anatomy (head, abdomen, femur) and manually mark the key\nanatomical landmarks on the image for accurate biometry and fetal weight\nestimation. This can be a time-consuming operator-dependent task, especially\nfor a trainee sonographer. Computer-assisted techniques can help in automating\nthe fetal biometry computation process. In this paper, we present a unified\nautomated framework for estimating all measurements needed for the fetal weight\nassessment. The proposed framework semantically segments the key fetal\nanatomies using state-of-the-art segmentation models, followed by region\nfitting and scale recovery for the biometry estimation. We present an ablation\nstudy of segmentation algorithms to show their robustness through 4-fold\ncross-validation on a dataset of 349 ultrasound standard plane images from 42\npregnancies. Moreover, we show that the network with the best segmentation\nperformance tends to be more accurate for biometry estimation. Furthermore, we\ndemonstrate that the error between clinically measured and predicted fetal\nbiometry is lower than the permissible error during routine clinical\nmeasurements.",
          "link": "http://arxiv.org/abs/2107.05255",
          "publishedOn": "2021-07-13T01:59:36.378Z",
          "wordCount": 653,
          "title": "AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2107.05255v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1\">Francesco Farina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1\">Emma Slade</a>",
          "description": "We introduce a novel architecture for graph networks which is equivariant to\nany transformation in the coordinate embeddings that preserves the distance\nbetween neighbouring nodes. In particular, it is equivariant to the Euclidean\nand conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance\nproperties, the proposed model is extremely more data efficient with respect to\nclassical graph architectures and also intrinsically equipped with a better\ninductive bias. We show that, learning on a minimal amount of data, the\narchitecture we propose can perfectly generalise to unseen data in a synthetic\nproblem, while much more training data are required from a standard model to\nreach comparable performance.",
          "link": "http://arxiv.org/abs/2106.13786",
          "publishedOn": "2021-07-13T01:59:36.359Z",
          "wordCount": 580,
          "title": "Data efficiency in graph networks through equivariance. (arXiv:2106.13786v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1\">Shaoduo Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xiangru Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianbin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hongmei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengzhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tengxu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Binhang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Recent years have witnessed a growing list of systems for distributed\ndata-parallel training. Existing systems largely fit into two paradigms, i.e.,\nparameter server and MPI-style collective operations. On the algorithmic side,\nresearchers have proposed a wide range of techniques to lower the communication\nvia system relaxations: quantization, decentralization, and communication\ndelay. However, most, if not all, existing systems only rely on standard\nsynchronous and asynchronous stochastic gradient (SG) based optimization,\ntherefore, cannot take advantage of all possible optimizations that the machine\nlearning community has been developing recently. Given this emerging gap\nbetween the current landscapes of systems and theory, we build BAGUA, a\ncommunication framework whose design goal is to provide a system abstraction\nthat is both flexible and modular to support state-of-the-art system relaxation\ntechniques of distributed training. Powered by the new system design, BAGUA has\na great ability to implement and extend various state-of-the-art distributed\nlearning algorithms. In a production cluster with up to 16 machines (128 GPUs),\nBAGUA can outperform PyTorch-DDP, Horovod and BytePS in the end-to-end training\ntime by a significant margin (up to 1.95 times) across a diverse range of\ntasks. Moreover, we conduct a rigorous tradeoff exploration showing that\ndifferent algorithms and system relaxations achieve the best performance over\ndifferent network conditions.",
          "link": "http://arxiv.org/abs/2107.01499",
          "publishedOn": "2021-07-13T01:59:36.353Z",
          "wordCount": 690,
          "title": "BAGUA: Scaling up Distributed Learning with System Relaxations. (arXiv:2107.01499v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04595",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Maevskiy_A/0/1/0/all/0/1\">A. Maevskiy</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ratnikov_F/0/1/0/all/0/1\">F. Ratnikov</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zinchenko_A/0/1/0/all/0/1\">A. Zinchenko</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Riabov_V/0/1/0/all/0/1\">V. Riabov</a>",
          "description": "High energy physics experiments rely heavily on the detailed detector\nsimulation models in many tasks. Running these detailed models typically\nrequires a notable amount of the computing time available to the experiments.\nIn this work, we demonstrate a new approach to speed up the simulation of the\nTime Projection Chamber tracker of the MPD experiment at the NICA accelerator\ncomplex. Our method is based on a Generative Adversarial Network - a deep\nlearning technique allowing for implicit estimation of the population\ndistribution for a given set of objects. This approach lets us learn and then\nsample from the distribution of raw detector responses, conditioned on the\nparameters of the charged particle tracks. To evaluate the quality of the\nproposed model, we integrate a prototype into the MPD software stack and\ndemonstrate that it produces high-quality events similar to the detailed\nsimulator, with a speed-up of at least an order of magnitude. The prototype is\ntrained on the responses from the inner part of the detector and, once expanded\nto the full detector, should be ready for use in physics tasks.",
          "link": "http://arxiv.org/abs/2012.04595",
          "publishedOn": "2021-07-13T01:59:36.346Z",
          "wordCount": 692,
          "title": "Simulating the Time Projection Chamber responses at the MPD detector using Generative Adversarial Networks. (arXiv:2012.04595v2 [physics.ins-det] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.11154",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1\">Fanghui Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yudong Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suykens_J/0/1/0/all/0/1\">Johan A.K. Suykens</a>",
          "description": "Random features is one of the most popular techniques to speed up kernel\nmethods in large-scale problems. Related works have been recognized by the\nNeurIPS Test-of-Time award in 2017 and the ICML Best Paper Finalist in 2019.\nThe body of work on random features has grown rapidly, and hence it is\ndesirable to have a comprehensive overview on this topic explaining the\nconnections among various algorithms and theoretical results. In this survey,\nwe systematically review the work on random features from the past ten years.\nFirst, the motivations, characteristics and contributions of representative\nrandom features based algorithms are summarized according to their sampling\nschemes, learning procedures, variance reduction properties and how they\nexploit training data. Second, we review theoretical results that center around\nthe following key question: how many random features are needed to ensure a\nhigh approximation quality or no loss in the empirical/expected risks of the\nlearned estimator. Third, we provide a comprehensive evaluation of popular\nrandom features based algorithms on several large-scale benchmark datasets and\ndiscuss their approximation quality and prediction performance for\nclassification. Last, we discuss the relationship between random features and\nmodern over-parameterized deep neural networks (DNNs), including the use of\nhigh dimensional random features in the analysis of DNNs as well as the gaps\nbetween current theoretical and empirical results. This survey may serve as a\ngentle introduction to this topic, and as a users' guide for practitioners\ninterested in applying the representative algorithms and understanding\ntheoretical results under various technical assumptions. We hope that this\nsurvey will facilitate discussion on the open problems in this topic, and more\nimportantly, shed light on future research directions.",
          "link": "http://arxiv.org/abs/2004.11154",
          "publishedOn": "2021-07-13T01:59:36.340Z",
          "wordCount": 774,
          "title": "Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond. (arXiv:2004.11154v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1\">George Michalopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKillop_I/0/1/0/all/0/1\">Ian McKillop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Helen Chen</a>",
          "description": "Lexical substitution is the task of generating meaningful substitutes for a\nword in a given textual context. Contextual word embedding models have achieved\nstate-of-the-art results in the lexical substitution task by relying on\ncontextual information extracted from the replaced word within the sentence.\nHowever, such models do not take into account structured knowledge that exists\nin external lexical databases.\n\nWe introduce LexSubCon, an end-to-end lexical substitution framework based on\ncontextual embedding models that can identify highly accurate substitute\ncandidates. This is achieved by combining contextual information with knowledge\nfrom structured lexical resources. Our approach involves: (i) introducing a\nnovel mix-up embedding strategy in the creation of the input embedding of the\ntarget word through linearly interpolating the pair of the target input\nembedding and the average embedding of its probable synonyms; (ii) considering\nthe similarity of the sentence-definition embeddings of the target word and its\nproposed candidates; and, (iii) calculating the effect of each substitution in\nthe semantics of the sentence through a fine-tuned sentence similarity model.\nOur experiments show that LexSubCon outperforms previous state-of-the-art\nmethods on LS07 and CoInCo benchmark datasets that are widely used for lexical\nsubstitution tasks.",
          "link": "http://arxiv.org/abs/2107.05132",
          "publishedOn": "2021-07-13T01:59:36.333Z",
          "wordCount": 633,
          "title": "LexSubCon: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution. (arXiv:2107.05132v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Satvik Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pundir_P/0/1/0/all/0/1\">Pradyumn Pundir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_H/0/1/0/all/0/1\">Himanshu Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_H/0/1/0/all/0/1\">Hemraj Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Somya Garg</a>",
          "description": "Precision agriculture system is an arising idea that refers to overseeing\nfarms utilizing current information and communication technologies to improve\nthe quantity and quality of yields while advancing the human work required. The\nautomation requires the assortment of information given by the sensors such as\nsoil, water, light, humidity, temperature for additional information to furnish\nthe operator with exact data to acquire excellent yield to farmers. In this\nwork, a study is proposed that incorporates all common state-of-the-art\napproaches for precision agriculture use. Technologies like the Internet of\nThings (IoT) for data collection, machine Learning for crop damage prediction,\nand deep learning for crop disease detection is used. The data collection using\nIoT is responsible for the measure of moisture levels for smart irrigation, n,\np, k estimations of fertilizers for best yield development. For crop damage\nprediction, various algorithms like Random Forest (RF), Light gradient boosting\nmachine (LGBM), XGBoost (XGB), Decision Tree (DT) and K Nearest Neighbor (KNN)\nare used. Subsequently, Pre-Trained Convolutional Neural Network (CNN) models\nsuch as VGG16, Resnet50, and DenseNet121 are also trained to check if the crop\nwas tainted with some illness or not.",
          "link": "http://arxiv.org/abs/2107.04895",
          "publishedOn": "2021-07-13T01:59:36.315Z",
          "wordCount": 658,
          "title": "Towards a Multimodal System for Precision Agriculture using IoT and Machine Learning. (arXiv:2107.04895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1\">Xuan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Relation prediction among entities in images is an important step in scene\ngraph generation (SGG), which further impacts various visual understanding and\nreasoning tasks. Existing SGG frameworks, however, require heavy training yet\nare incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we\nstress that such incapability is due to the lack of commonsense reasoning,i.e.,\nthe ability to associate similar entities and infer similar relations based on\ngeneral understanding of the world. To fill this gap, we propose\nCommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to\nintegrate commonsense knowledge for SGG, especially for zero-shot relation\nprediction. Specifically, we develop novel graph mining pipelines to model the\nneighborhoods and paths around entities in an external commonsense knowledge\ngraph, and integrate them on top of state-of-the-art SGG frameworks. Extensive\nquantitative evaluations and qualitative case studies on both original and\nmanipulated datasets from Visual Genome demonstrate the effectiveness of our\nproposed approach.",
          "link": "http://arxiv.org/abs/2107.05080",
          "publishedOn": "2021-07-13T01:59:36.309Z",
          "wordCount": 608,
          "title": "Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration. (arXiv:2107.05080v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05115",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1\">Basit O. Alawode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1\">Mudassir Masood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1\">Tarig Ballal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1\">Tareq Al-Naffouri</a>",
          "description": "In spite of the improvements achieved by the several denoising algorithms\nover the years, many of them still fail at preserving the fine details of the\nimage after denoising. This is as a result of the smooth-out effect they have\non the images. Most neural network-based algorithms have achieved better\nquantitative performance than the classical denoising algorithms. However, they\nalso suffer from qualitative (visual) performance as a result of the smooth-out\neffect. In this paper, we propose an algorithm to address this shortcoming. We\npropose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image\ndenoising. This algorithm performs collaborative denoising of image patches in\nthe sparse domain using a set of optimized neural network models. This results\nin a fast algorithm that is able to excellently obtain a trade-off between\nnoise removal and details preservation. Extensive experiments show that the\nDeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and\nqualitatively (visually) better than many of the state-of-the-art denoising\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.05115",
          "publishedOn": "2021-07-13T01:59:36.302Z",
          "wordCount": 609,
          "title": "Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising. (arXiv:2107.05115v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagle_S/0/1/0/all/0/1\">Sridevi Narayana Wagle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>",
          "description": "Machine learning algorithms often produce models considered as complex\nblack-box models by both end users and developers. They fail to explain the\nmodel in terms of the domain they are designed for. The proposed Iterative\nVisual Logical Classifier (IVLC) is an interpretable machine learning algorithm\nthat allows end users to design a model and classify data with more confidence\nand without having to compromise on the accuracy. Such technique is especially\nhelpful when dealing with sensitive and crucial data like cancer data in the\nmedical domain with high cost of errors. With the help of the proposed\ninteractive and lossless multidimensional visualization, end users can identify\nthe pattern in the data based on which they can make explainable decisions.\nSuch options would not be possible in black box machine learning methodologies.\nThe interpretable IVLC algorithm is supported by the Interactive Shifted Paired\nCoordinates Software System (SPCVis). It is a lossless multidimensional data\nvisualization system with user interactive features. The interactive approach\nprovides flexibility to the end user to perform data classification as\nself-service without having to rely on a machine learning expert. Interactive\npattern discovery becomes challenging while dealing with large data sets with\nhundreds of dimensions/features. To overcome this problem, this chapter\nproposes an automated classification approach combined with new Coordinate\nOrder Optimizer (COO) algorithm and a Genetic algorithm. The COO algorithm\nautomatically generates the coordinate pair sequences that best represent the\ndata separation and the genetic algorithm helps optimizing the proposed IVLC\nalgorithm by automatically generating the areas for data classification. The\nfeasibility of the approach is shown by experiments on benchmark datasets\ncovering both interactive and automated processes used for data classification.",
          "link": "http://arxiv.org/abs/2107.04971",
          "publishedOn": "2021-07-13T01:59:36.295Z",
          "wordCount": 716,
          "title": "Self-service Data Classification Using Interactive Visualization and Interpretable Machine Learning. (arXiv:2107.04971v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haixu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiehui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "Extending the forecasting time is a critical demand for real applications,\nsuch as extreme weather early warning and long-term energy consumption\nplanning. This paper studies the \\textit{long-term forecasting} problem of time\nseries. Prior Transformer-based models adopt various self-attention mechanisms\nto discover the long-range dependencies. However, intricate temporal patterns\nof the long-term future prohibit the model from finding reliable dependencies.\nAlso, Transformers have to adopt the sparse versions of point-wise\nself-attentions for long series efficiency, resulting in the information\nutilization bottleneck. Towards these challenges, we propose Autoformer as a\nnovel decomposition architecture with an Auto-Correlation mechanism. We go\nbeyond the pre-processing convention of series decomposition and renovate it as\na basic inner block of deep models. This design empowers Autoformer with\nprogressive decomposition capacities for complex time series. Further, inspired\nby the stochastic process theory, we design the Auto-Correlation mechanism\nbased on the series periodicity, which conducts the dependencies discovery and\nrepresentation aggregation at the sub-series level. Auto-Correlation\noutperforms self-attention in both efficiency and accuracy. In long-term\nforecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative\nimprovement on six benchmarks, covering five practical applications: energy,\ntraffic, economics, weather and disease.",
          "link": "http://arxiv.org/abs/2106.13008",
          "publishedOn": "2021-07-13T01:59:36.289Z",
          "wordCount": 651,
          "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. (arXiv:2106.13008v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Interpretable brain network models for disease prediction are of great value\nfor the advancement of neuroscience. GNNs are promising to model complicated\nnetwork data, but they are prone to overfitting and suffer from poor\ninterpretability, which prevents their usage in decision-critical scenarios\nlike healthcare. To bridge this gap, we propose BrainNNExplainer, an\ninterpretable GNN framework for brain network analysis. It is mainly composed\nof two jointly learned modules: a backbone prediction model that is\nspecifically designed for brain networks and an explanation generator that\nhighlights disease-specific prominent brain network connections. Extensive\nexperimental results with visualizations on two challenging disease prediction\ndatasets demonstrate the unique interpretability and outstanding performance of\nBrainNNExplainer.",
          "link": "http://arxiv.org/abs/2107.05097",
          "publishedOn": "2021-07-13T01:59:36.273Z",
          "wordCount": 600,
          "title": "BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis. (arXiv:2107.05097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansourifar_H/0/1/0/all/0/1\">Hadi Mansourifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsagheer_D/0/1/0/all/0/1\">Dana Alsagheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_R/0/1/0/all/0/1\">Reza Fathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weidong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>",
          "description": "With the rise of voice chat rooms, a gigantic resource of data can be exposed\nto the research community for natural language processing tasks. Moderators in\nvoice chat rooms actively monitor the discussions and remove the participants\nwith offensive language. However, it makes the hate speech detection even more\ndifficult since some participants try to find creative ways to articulate hate\nspeech. This makes the hate speech detection challenging in new social media\nlike Clubhouse. To the best of our knowledge all the hate speech datasets have\nbeen collected from text resources like Twitter. In this paper, we take the\nfirst step to collect a significant dataset from Clubhouse as the rising star\nin social media industry. We analyze the collected instances from statistical\npoint of view using the Google Perspective Scores. Our experiments show that,\nthe Perspective Scores can outperform Bag of Words and Word2Vec as high level\ntext features.",
          "link": "http://arxiv.org/abs/2106.13238",
          "publishedOn": "2021-07-13T01:59:36.267Z",
          "wordCount": 617,
          "title": "Hate Speech Detection in Clubhouse. (arXiv:2106.13238v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08659",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1\">Aleksandr Beknazaryan</a>",
          "description": "In this paper it is shown that $C_\\beta$-smooth functions can be approximated\nby neural networks with parameters $\\{0,\\pm \\frac{1}{2}, \\pm 1, 2\\}$. The\ndepth, width and the number of active parameters of the constructed networks\nhave, up to a logarithmic factor, the same dependence on the approximation\nerror as the networks with parameters in $[-1,1]$. In particular, this means\nthat the nonparametric regression estimation with the constructed networks\nattains the same convergence rate as with sparse networks with parameters in\n$[-1,1]$.",
          "link": "http://arxiv.org/abs/2103.08659",
          "publishedOn": "2021-07-13T01:59:36.261Z",
          "wordCount": 536,
          "title": "Function approximation by deep neural networks with parameters $\\{0,\\pm \\frac{1}{2}, \\pm 1, 2\\}$. (arXiv:2103.08659v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14409",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+DSouza_N/0/1/0/all/0/1\">Niharika Shimona D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nebel_M/0/1/0/all/0/1\">Mary Beth Nebel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Crocetti_D/0/1/0/all/0/1\">Deana Crocetti</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wymbs_N/0/1/0/all/0/1\">Nicholas Wymbs</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_J/0/1/0/all/0/1\">Joshua Robinson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mostofsky_S/0/1/0/all/0/1\">Stewart Mostofsky</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Venkataraman_A/0/1/0/all/0/1\">Archana Venkataraman</a>",
          "description": "We propose a novel matrix autoencoder to map functional connectomes from\nresting state fMRI (rs-fMRI) to structural connectomes from Diffusion Tensor\nImaging (DTI), as guided by subject-level phenotypic measures. Our specialized\nautoencoder infers a low dimensional manifold embedding for the rs-fMRI\ncorrelation matrices that mimics a canonical outer-product decomposition. The\nembedding is simultaneously used to reconstruct DTI tractography matrices via a\nsecond manifold alignment decoder and to predict inter-subject phenotypic\nvariability via an artificial neural network. We validate our framework on a\ndataset of 275 healthy individuals from the Human Connectome Project database\nand on a second clinical dataset consisting of 57 subjects with Autism Spectrum\nDisorder. We demonstrate that the model reliably recovers structural\nconnectivity patterns across individuals, while robustly extracting predictive\nand interpretable brain biomarkers in a cross-validated setting. Finally, our\nframework outperforms several baselines at predicting behavioral phenotypes in\nboth real-world datasets.",
          "link": "http://arxiv.org/abs/2105.14409",
          "publishedOn": "2021-07-13T01:59:36.256Z",
          "wordCount": 630,
          "title": "A Matrix Autoencoder Framework to Align the Functional and Structural Connectivity Manifolds as Guided by Behavioral Phenotypes. (arXiv:2105.14409v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Changshun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcone_Y/0/1/0/all/0/1\">Yli&#xe8;s Falcone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensalem_S/0/1/0/all/0/1\">Saddek Bensalem</a>",
          "description": "Classification neural networks fail to detect inputs that do not fall inside\nthe classes they have been trained for. Runtime monitoring techniques on the\nneuron activation pattern can be used to detect such inputs. We present an\napproach for monitoring classification systems via data abstraction. Data\nabstraction relies on the notion of box with a resolution. Box-based\nabstraction consists in representing a set of values by its minimal and maximal\nvalues in each dimension. We augment boxes with a notion of resolution and\ndefine their clustering coverage, which is intuitively a quantitative metric\nthat indicates the abstraction quality. This allows studying the effect of\ndifferent clustering parameters on the constructed boxes and estimating an\ninterval of sub-optimal parameters. Moreover, we automatically construct\nmonitors that leverage both the correct and incorrect behaviors of a system.\nThis allows checking the size of the monitor abstractions and analyzing the\nseparability of the network. Monitors are obtained by combining the\nsub-monitors of each class of the system placed at some selected layers. Our\nexperiments demonstrate the effectiveness of our clustering coverage estimation\nand show how to assess the effectiveness and precision of monitors according to\nthe selected clustering parameter and monitored layers.",
          "link": "http://arxiv.org/abs/2104.14435",
          "publishedOn": "2021-07-13T01:59:36.249Z",
          "wordCount": 668,
          "title": "Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes. (arXiv:2104.14435v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Flaspohler_G/0/1/0/all/0/1\">Genevieve Flaspohler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orabona_F/0/1/0/all/0/1\">Francesco Orabona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1\">Judah Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouatadid_S/0/1/0/all/0/1\">Soukayna Mouatadid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1\">Miruna Oprescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orenstein_P/0/1/0/all/0/1\">Paulo Orenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "Inspired by the demands of real-time climate and weather forecasting, we\ndevelop optimistic online learning algorithms that require no parameter tuning\nand have optimal regret guarantees under delayed feedback. Our algorithms --\nDORM, DORM+, and AdaHedgeD -- arise from a novel reduction of delayed online\nlearning to optimistic online learning that reveals how optimistic hints can\nmitigate the regret penalty caused by delay. We pair this delay-as-optimism\nperspective with a new analysis of optimistic learning that exposes its\nrobustness to hinting errors and a new meta-algorithm for learning effective\nhinting strategies in the presence of delay. We conclude by benchmarking our\nalgorithms on four subseasonal climate forecasting tasks, demonstrating low\nregret relative to state-of-the-art forecasting models.",
          "link": "http://arxiv.org/abs/2106.06885",
          "publishedOn": "2021-07-13T01:59:36.243Z",
          "wordCount": 615,
          "title": "Online Learning with Optimism and Delay. (arXiv:2106.06885v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianbu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>",
          "description": "The monitoring of coastal wetlands is of great importance to the protection\nof marine and terrestrial ecosystems. However, due to the complex environment,\nsevere vegetation mixture, and difficulty of access, it is impossible to\naccurately classify coastal wetlands and identify their species with\ntraditional classifiers. Despite the integration of multisource remote sensing\ndata for performance enhancement, there are still challenges with acquiring and\nexploiting the complementary merits from multisource data. In this paper, the\nDeepwise Feature Interaction Network (DFINet) is proposed for wetland\nclassification. A depthwise cross attention module is designed to extract\nself-correlation and cross-correlation from multisource feature pairs. In this\nway, meaningful complementary information is emphasized for classification.\nDFINet is optimized by coordinating consistency loss, discrimination loss, and\nclassification loss. Accordingly, DFINet reaches the standard solution-space\nunder the regularity of loss functions, while the spatial consistency and\nfeature discrimination are preserved. Comprehensive experimental results on two\nhyperspectral and multispectral wetland datasets demonstrate that the proposed\nDFINet outperforms other competitive methods in terms of overall accuracy.",
          "link": "http://arxiv.org/abs/2106.06896",
          "publishedOn": "2021-07-13T01:59:36.227Z",
          "wordCount": 656,
          "title": "Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alcorn_M/0/1/0/all/0/1\">Michael A. Alcorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Order-agnostic autoregressive distribution (density) estimation (OADE), i.e.,\nautoregressive distribution estimation where the features can occur in an\narbitrary order, is a challenging problem in generative machine learning. Prior\nwork on OADE has encoded feature identity by assigning each feature to a\ndistinct fixed position in an input vector. As a result, architectures built\nfor these inputs must strategically mask either the input or model weights to\nlearn the various conditional distributions necessary for inferring the full\njoint distribution of the dataset in an order-agnostic way. In this paper, we\npropose an alternative approach for encoding feature identities, where each\nfeature's identity is included alongside its value in the input. This feature\nidentity encoding strategy allows neural architectures designed for sequential\ndata to be applied to the OADE task without modification. As a proof of\nconcept, we show that a Transformer trained on this input (which we refer to as\n\"the DEformer\", i.e., the distribution estimating Transformer) can effectively\nmodel binarized-MNIST, approaching the performance of fixed-order\nautoregressive distribution estimating algorithms while still being entirely\norder-agnostic. Additionally, we find that the DEformer surpasses the\nperformance of recent flow-based architectures when modeling a tabular dataset.",
          "link": "http://arxiv.org/abs/2106.06989",
          "publishedOn": "2021-07-13T01:59:36.221Z",
          "wordCount": 663,
          "title": "The DEformer: An Order-Agnostic Distribution Estimating Transformer. (arXiv:2106.06989v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>",
          "description": "In this paper, we propose an end-to-end Mandarin tone classification method\nfrom continuous speech utterances utilizing both the spectrogram and the\nshort-term context information as the input. Both spectrograms and context\nsegment features are used to train the tone classifier. We first divide the\nspectrogram frames into syllable segments using force alignment results\nproduced by an ASR model. Then we extract the short-term segment features to\ncapture the context information across multiple syllables. Feeding both the\nspectrogram and the short-term context segment features into an end-to-end\nmodel could significantly improve the performance. Experiments are performed on\na large-scale open-source Mandarin speech dataset to evaluate the proposed\nmethod. Results show that this method improves the classification accuracy from\n79.5% to 92.6% on the AISHELL3 database.",
          "link": "http://arxiv.org/abs/2104.05657",
          "publishedOn": "2021-07-13T01:59:36.215Z",
          "wordCount": 593,
          "title": "End-to-End Mandarin Tone Classification with Short Term Context Information. (arXiv:2104.05657v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07533",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Laves_M/0/1/0/all/0/1\">Max-Heinrich Laves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tolle_M/0/1/0/all/0/1\">Malte T&#xf6;lle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1\">Alexander Schlaefer</a>",
          "description": "Cold posteriors have been reported to perform better in practice in the\ncontext of Bayesian deep learning (Wenzel et al., 2020). In variational\ninference, it is common to employ only a partially tempered posterior by\nscaling the complexity term in the log-evidence lower bound (ELBO). In this\nwork, we optimize the ELBO for a fully tempered posterior in mean-field\nvariational inference and use Bayesian optimization to automatically find the\noptimal posterior temperature and prior scale. Choosing an appropriate\nposterior temperature leads to better predictive performance and improved\nuncertainty calibration, which we demonstrate for the task of denoising medical\nX-ray images.",
          "link": "http://arxiv.org/abs/2106.07533",
          "publishedOn": "2021-07-13T01:59:36.209Z",
          "wordCount": 555,
          "title": "Cold Posteriors Improve Bayesian Medical Image Post-Processing. (arXiv:2106.07533v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1\">Wolfgang Maass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storey_V/0/1/0/all/0/1\">Veda C. Storey</a>",
          "description": "Both conceptual modeling and machine learning have long been recognized as\nimportant areas of research. With the increasing emphasis on digitizing and\nprocessing large amounts of data for business and other applications, it would\nbe helpful to consider how these areas of research can complement each other.\nTo understand how they can be paired, we provide an overview of machine\nlearning foundations and development cycle. We then examine how conceptual\nmodeling can be applied to machine learning and propose a framework for\nincorporating conceptual modeling into data science projects. The framework is\nillustrated by applying it to a healthcare application. For the inverse\npairing, machine learning can impact conceptual modeling through text and rule\nmining, as well as knowledge graphs. The pairing of conceptual modeling and\nmachine learning in this this way should help lay the foundations for future\nresearch.",
          "link": "http://arxiv.org/abs/2106.14251",
          "publishedOn": "2021-07-13T01:59:36.203Z",
          "wordCount": 608,
          "title": "Pairing Conceptual Modeling with Machine Learning. (arXiv:2106.14251v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zejin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>",
          "description": "The increasing concerns about data privacy and security drive an emerging\nfield of studying privacy-preserving machine learning from isolated data\nsources, i.e., federated learning. A class of federated learning, vertical\nfederated learning, where different parties hold different features for common\nusers, has a great potential of driving a more variety of business cooperation\namong enterprises in many fields. In machine learning, decision tree ensembles\nsuch as gradient boosting decision tree (GBDT) and random forest are widely\napplied powerful models with high interpretability and modeling efficiency.\nHowever, the interpretability is compromised in state-of-the-art vertical\nfederated learning frameworks such as SecureBoost with anonymous features to\navoid possible data breaches. To address this issue in the inference process,\nin this paper, we propose Fed-EINI to protect data privacy and allow the\ndisclosure of feature meaning by concealing decision paths with a\ncommunication-efficient secure computation method for inference outputs. The\nadvantages of Fed-EINI will be demonstrated through both theoretical analysis\nand extensive numerical results.",
          "link": "http://arxiv.org/abs/2105.09540",
          "publishedOn": "2021-07-13T01:59:36.188Z",
          "wordCount": 655,
          "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borner_K/0/1/0/all/0/1\">Katy B&#xf6;rner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yingnan Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>",
          "description": "Ubiquitous internet access is reshaping the way we live, but it is\naccompanied by unprecedented challenges in preventing chronic diseases planted\nby long exposure to unhealthy lifestyles. This paper proposes leveraging online\nshopping behaviors as a proxy for personal lifestyle choices to improve chronic\ndisease prevention literacy targeted for times when e-commerce user experience\nhas been assimilated into most people's daily lives. Here, retrospective\nlongitudinal query logs and purchase records from millions of online shoppers\nwere accessed, constructing a broad spectrum of lifestyle features covering\nassorted product categories and buyer personas. Using the lifestyle-related\ninformation preceding their first purchases of prescription drugs, we could\ndetermine associations between online shoppers' past lifestyle choices and\nwhether they suffered from a particular chronic disease or not. Novel lifestyle\nrisk factors were discovered in two exemplars -- depression and diabetes, most\nof which showed cognitive congruence with existing healthcare knowledge.\nFurther, such empirical findings could be adopted to locate online shoppers at\nhigh risk of these chronic diseases with fair accuracy, closely matching the\nperformance of screening surveys benchmarked against medical diagnosis.\nUnobtrusive chronic disease surveillance via e-commerce sites may soon meet\nconsenting individuals in the digital space they already inhabit.",
          "link": "http://arxiv.org/abs/2104.14281",
          "publishedOn": "2021-07-13T01:59:36.182Z",
          "wordCount": 711,
          "title": "Leveraging Online Shopping Behaviors as a Proxy for Personal Lifestyle Choices: New Insights into Chronic Disease Prevention Literacy. (arXiv:2104.14281v3 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Viet Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanasusanto_G/0/1/0/all/0/1\">Grani A. Hanasusanto</a>",
          "description": "We propose a distributionally robust classification model with a fairness\nconstraint that encourages the classifier to be fair in view of the equality of\nopportunity criterion. We use a type-$\\infty$ Wasserstein ambiguity set\ncentered at the empirical distribution to model distributional uncertainty and\nderive a conservative reformulation for the worst-case equal opportunity\nunfairness measure. We establish that the model is equivalent to a mixed binary\noptimization problem, which can be solved by standard off-the-shelf solvers. To\nimprove scalability, we further propose a convex, hinge-loss-based model for\nlarge problem instances whose reformulation does not incur any binary\nvariables. Moreover, we also consider the distributionally robust learning\nproblem with a generic ground transportation cost to hedge against the\nuncertainties in the label and sensitive attribute. Finally, we numerically\ndemonstrate that our proposed approaches improve fairness with negligible loss\nof predictive accuracy.",
          "link": "http://arxiv.org/abs/2103.06828",
          "publishedOn": "2021-07-13T01:59:36.175Z",
          "wordCount": 603,
          "title": "Wasserstein Robust Classification with Fairness Constraints. (arXiv:2103.06828v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham M. Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovett_S/0/1/0/all/0/1\">Shachar Lovett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1\">Gaurav Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruosong Wang</a>",
          "description": "This work introduces Bilinear Classes, a new structural framework, which\npermit generalization in reinforcement learning in a wide variety of settings\nthrough the use of function approximation. The framework incorporates nearly\nall existing models in which a polynomial sample complexity is achievable, and,\nnotably, also includes new models, such as the Linear $Q^*/V^*$ model in which\nboth the optimal $Q$-function and the optimal $V$-function are linear in some\nknown feature space. Our main result provides an RL algorithm which has\npolynomial sample complexity for Bilinear Classes; notably, this sample\ncomplexity is stated in terms of a reduction to the generalization error of an\nunderlying supervised learning sub-problem. These bounds nearly match the best\nknown sample complexity bounds for existing models. Furthermore, this framework\nalso extends to the infinite dimensional (RKHS) setting: for the the Linear\n$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample\ncomplexities that have no explicit dependence on the explicit feature dimension\n(which could be infinite), but instead depends only on information theoretic\nquantities.",
          "link": "http://arxiv.org/abs/2103.10897",
          "publishedOn": "2021-07-13T01:59:36.169Z",
          "wordCount": 691,
          "title": "Bilinear Classes: A Structural Framework for Provable Generalization in RL. (arXiv:2103.10897v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12284",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jacaruso_L/0/1/0/all/0/1\">Lucas Cassiel Jacaruso</a>",
          "description": "Deep learning methods have shown suitability for time series classification\nin the health and medical domain, with promising results for electrocardiogram\ndata classification. Successful identification of myocardial infarction holds\nlife saving potential and any meaningful improvement upon deep learning models\nin this area is of great interest. Conventionally, data augmentation methods\nare applied universally to the training set when data are limited in order to\nameliorate data resolution or sample size. In the method proposed in this\nstudy, data augmentation was not applied in the context of data scarcity.\nInstead, samples that yield low confidence predictions were selectively\naugmented in order to bolster the model's sensitivity to features or patterns\nless strongly associated with a given class. This approach was tested for\nimproving the performance of a Fully Convolutional Network. The proposed\napproach achieved 90 percent accuracy for classifying myocardial infarction as\nopposed to 82 percent accuracy for the baseline, a marked improvement. Further,\nthe accuracy of the proposed approach was optimal near a defined upper\nthreshold for qualifying low confidence samples and decreased as this threshold\nwas raised to include higher confidence samples. This suggests exclusively\nselecting lower confidence samples for data augmentation comes with distinct\nbenefits for electrocardiogram data classification with Fully Convolutional\nNetworks.",
          "link": "http://arxiv.org/abs/2104.12284",
          "publishedOn": "2021-07-13T01:59:36.163Z",
          "wordCount": 666,
          "title": "Accuracy Improvement for Fully Convolutional Networks via Selective Augmentation with Applications to Electrocardiogram Data. (arXiv:2104.12284v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>",
          "description": "We introduce a novel embedding model, named NoGE, which aims to integrate\nco-occurrence among entities and relations into graph neural networks to\nimprove knowledge graph completion (i.e., link prediction). Given a knowledge\ngraph, NoGE constructs a single graph considering entities and relations as\nindividual nodes. NoGE then computes weights for edges among nodes based on the\nco-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion\nGraph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector\nrepresentations for entity and relation nodes. NoGE then adopts a score\nfunction to produce the triple scores. Comprehensive experimental results show\nthat NoGE obtains state-of-the-art results on three new and difficult benchmark\ndatasets CoDEx for knowledge graph completion.",
          "link": "http://arxiv.org/abs/2104.07396",
          "publishedOn": "2021-07-13T01:59:36.146Z",
          "wordCount": 593,
          "title": "Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction. (arXiv:2104.07396v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1\">Tobias Sebastian Finn</a>",
          "description": "Ensemble data from Earth system models has to be calibrated and\npost-processed. I propose a novel member-by-member post-processing approach\nwith neural networks. I bridge ideas from ensemble data assimilation with\nself-attention, resulting into the self-attentive ensemble transformer. Here,\ninteractions between ensemble members are represented as additive and dynamic\nself-attentive part. As proof-of-concept, I regress global ECMWF ensemble\nforecasts to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate\nthat the ensemble transformer can calibrate the ensemble spread and extract\nadditional information from the ensemble. As it is a member-by-member approach,\nthe ensemble transformer directly outputs multivariate and spatially-coherent\nensemble members. Therefore, self-attention and the transformer technique can\nbe a missing piece for a non-parametric post-processing of ensemble data with\nneural networks.",
          "link": "http://arxiv.org/abs/2106.13924",
          "publishedOn": "2021-07-13T01:59:36.130Z",
          "wordCount": 610,
          "title": "Self-Attentive Ensemble Transformer: Representing Ensemble Interactions in Neural Networks for Earth System Models. (arXiv:2106.13924v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingheng Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1\">Rob Gorbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1\">Dana Kuli&#x107;</a>",
          "description": "A promising characteristic of Deep Reinforcement Learning (DRL) is its\ncapability to learn optimal policy in an end-to-end manner without relying on\nfeature engineering. However, most approaches assume a fully observable state\nspace, i.e. fully observable Markov Decision Process (MDP). In real-world\nrobotics, this assumption is unpractical, because of the sensor issues such as\nsensors' capacity limitation and sensor noise, and the lack of knowledge about\nif the observation design is complete or not. These scenarios lead to Partially\nObservable MDP (POMDP) and need special treatment. In this paper, we propose\nLong-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient\n(LSTM-TD3) by introducing a memory component to TD3, and compare its\nperformance with other DRL algorithms in both MDPs and POMDPs. Our results\ndemonstrate the significant advantages of the memory component in addressing\nPOMDPs, including the ability to handle missing and noisy observation data.",
          "link": "http://arxiv.org/abs/2102.12344",
          "publishedOn": "2021-07-13T01:59:36.124Z",
          "wordCount": 614,
          "title": "Memory-based Deep Reinforcement Learning for POMDP. (arXiv:2102.12344v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karingula_S/0/1/0/all/0/1\">Sankeerth Rao Karingula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_N/0/1/0/all/0/1\">Nandini Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahmasbi_R/0/1/0/all/0/1\">Rasool Tahmasbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amjadi_M/0/1/0/all/0/1\">Mehrnaz Amjadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Deokwoo Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_R/0/1/0/all/0/1\">Ricky Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thimmisetty_C/0/1/0/all/0/1\">Charanraj Thimmisetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_L/0/1/0/all/0/1\">Luisa Polania Cabrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayer_M/0/1/0/all/0/1\">Marjorie Sayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1\">Claudionor Nunes Coelho Jr</a>",
          "description": "Time series forecasting is a fundamental task emerging from diverse\ndata-driven applications. Many advanced autoregressive methods such as ARIMA\nwere used to develop forecasting models. Recently, deep learning based methods\nsuch as DeepAr, NeuralProphet, Seq2Seq have been explored for time series\nforecasting problem. In this paper, we propose a novel time series forecast\nmodel, DeepGB. We formulate and implement a variant of Gradient boosting\nwherein the weak learners are DNNs whose weights are incrementally found in a\ngreedy manner over iterations. In particular, we develop a new embedding\narchitecture that improves the performance of many deep learning models on time\nseries using Gradient boosting variant. We demonstrate that our model\noutperforms existing comparable state-of-the-art models using real-world sensor\ndata and public dataset.",
          "link": "http://arxiv.org/abs/2104.04781",
          "publishedOn": "2021-07-13T01:59:36.084Z",
          "wordCount": 604,
          "title": "Boosted Embeddings for Time Series Forecasting. (arXiv:2104.04781v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1\">Andrew Wagenmaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1\">Kevin Jamieson</a>",
          "description": "Exploration in unknown environments is a fundamental problem in reinforcement\nlearning and control. In this work, we study task-guided exploration and\ndetermine what precisely an agent must learn about their environment in order\nto complete a particular task. Formally, we study a broad class of\ndecision-making problems in the setting of linear dynamical systems, a class\nthat includes the linear quadratic regulator problem. We provide instance- and\ntask-dependent lower bounds which explicitly quantify the difficulty of\ncompleting a task of interest. Motivated by our lower bound, we propose a\ncomputationally efficient experiment-design based exploration algorithm. We\nshow that it optimally explores the environment, collecting precisely the\ninformation needed to complete the task, and provide finite-time bounds\nguaranteeing that it achieves the instance- and task-optimal sample complexity,\nup to constant factors. Through several examples of the LQR problem, we show\nthat performing task-guided exploration provably improves on exploration\nschemes which do not take into account the task of interest. Along the way, we\nestablish that certainty equivalence decision making is instance- and\ntask-optimal, and obtain the first algorithm for the linear quadratic regulator\nproblem which is instance-optimal. We conclude with several experiments\nillustrating the effectiveness of our approach in practice.",
          "link": "http://arxiv.org/abs/2102.05214",
          "publishedOn": "2021-07-13T01:59:36.068Z",
          "wordCount": 662,
          "title": "Task-Optimal Exploration in Linear Dynamical Systems. (arXiv:2102.05214v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Huayi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhimeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1\">Xiaorong Pu</a>",
          "description": "Multi-view clustering is an important research topic due to its capability to\nutilize complementary information from multiple views. However, there are few\nmethods to consider the negative impact caused by certain views with unclear\nclustering structures, resulting in poor multi-view clustering performance. To\naddress this drawback, we propose self-supervised discriminative feature\nlearning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders\nare applied to learn embedded features for each view independently. To leverage\nthe multi-view complementary information, we concatenate all views' embedded\nfeatures to form the global features, which can overcome the negative impact of\nsome views' unclear clustering structures. In a self-supervised manner,\npseudo-labels are obtained to build a unified target distribution to perform\nmulti-view discriminative feature learning. During this process, global\ndiscriminative information can be mined to supervise all views to learn more\ndiscriminative features, which in turn are used to update the target\ndistribution. Besides, this unified target distribution can make SDMVC learn\nconsistent cluster assignments, which accomplishes the clustering consistency\nof multiple views while preserving their features' diversity. Experiments on\nvarious types of multi-view datasets show that SDMVC achieves state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2103.15069",
          "publishedOn": "2021-07-13T01:59:36.061Z",
          "wordCount": 656,
          "title": "Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering. (arXiv:2103.15069v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1\">Cynthia Rudin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semenova_L/0/1/0/all/0/1\">Lesia Semenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Chudi Zhong</a>",
          "description": "Interpretability in machine learning (ML) is crucial for high stakes\ndecisions and troubleshooting. In this work, we provide fundamental principles\nfor interpretable ML, and dispel common misunderstandings that dilute the\nimportance of this crucial topic. We also identify 10 technical challenge areas\nin interpretable machine learning and provide history and background on each\nproblem. Some of these problems are classically important, and some are recent\nproblems that have arisen in the last few years. These problems are: (1)\nOptimizing sparse logical models such as decision trees; (2) Optimization of\nscoring systems; (3) Placing constraints into generalized additive models to\nencourage sparsity and better interpretability; (4) Modern case-based\nreasoning, including neural networks and matching for causal inference; (5)\nComplete supervised disentanglement of neural networks; (6) Complete or even\npartial unsupervised disentanglement of neural networks; (7) Dimensionality\nreduction for data visualization; (8) Machine learning models that can\nincorporate physics and other generative or causal constraints; (9)\nCharacterization of the \"Rashomon set\" of good models; and (10) Interpretable\nreinforcement learning. This survey is suitable as a starting point for\nstatisticians and computer scientists interested in working in interpretable\nmachine learning.",
          "link": "http://arxiv.org/abs/2103.11251",
          "publishedOn": "2021-07-13T01:59:36.055Z",
          "wordCount": 662,
          "title": "Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges. (arXiv:2103.11251v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gamzu_I/0/1/0/all/0/1\">Iftah Gamzu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutiel_G/0/1/0/all/0/1\">Gilad Kutiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Ran Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1\">Eugene Agichtein</a>",
          "description": "In recent years online shopping has gained momentum and became an important\nvenue for customers wishing to save time and simplify their shopping process. A\nkey advantage of shopping online is the ability to read what other customers\nare saying about products of interest. In this work, we aim to maintain this\nadvantage in situations where extreme brevity is needed, for example, when\nshopping by voice. We suggest a novel task of extracting a single\nrepresentative helpful sentence from a set of reviews for a given product. The\nselected sentence should meet two conditions: first, it should be helpful for a\npurchase decision and second, the opinion it expresses should be supported by\nmultiple reviewers. This task is closely related to the task of Multi Document\nSummarization in the product reviews domain but differs in its objective and\nits level of conciseness. We collect a dataset in English of sentence\nhelpfulness scores via crowd-sourcing and demonstrate its reliability despite\nthe inherent subjectivity involved. Next, we describe a complete model that\nextracts representative helpful sentences with positive and negative sentiment\ntowards the product and demonstrate that it outperforms several baselines.",
          "link": "http://arxiv.org/abs/2104.09792",
          "publishedOn": "2021-07-13T01:59:36.046Z",
          "wordCount": 660,
          "title": "Identifying Helpful Sentences in Product Reviews. (arXiv:2104.09792v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1\">Rafa&#x142; Powalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1\">&#x141;ukasz Borchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1\">Dawid Jurkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1\">Tomasz Dwojak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1\">Micha&#x142; Pietruszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1\">Gabriela Pa&#x142;ka</a>",
          "description": "We address the challenging problem of Natural Language Comprehension beyond\nplain-text documents by introducing the TILT neural network architecture which\nsimultaneously learns layout information, visual features, and textual\nsemantics. Contrary to previous approaches, we rely on a decoder capable of\nunifying a variety of problems involving natural language. The layout is\nrepresented as an attention bias and complemented with contextualized visual\ninformation, while the core of our model is a pretrained encoder-decoder\nTransformer. Our novel approach achieves state-of-the-art results in extracting\ninformation from documents and answering questions which demand layout\nunderstanding (DocVQA, CORD, SROIE). At the same time, we simplify the process\nby employing an end-to-end model.",
          "link": "http://arxiv.org/abs/2102.09550",
          "publishedOn": "2021-07-13T01:59:36.039Z",
          "wordCount": 589,
          "title": "Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. (arXiv:2102.09550v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1\">Farzan Memarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goo_W/0/1/0/all/0/1\">Wonjoon Goo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1\">Rudolf Lioutikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>",
          "description": "We propose a novel reinforcement learning framework that performs\nself-supervised online reward shaping, yielding faster, sample efficient\nperformance in sparse-reward environments. The proposed framework alternates\nbetween updating a policy and inferring a reward function. While the policy\nupdate is performed with the inferred, potentially dense reward function, the\noriginal sparse reward is used to provide a self-supervisory signal for the\nreward update by serving as an ordering over the observed trajectories. The\nproposed framework is based on the theory that altering the reward function\ndoes not affect the optimal policy of the original MDP as long as certain\nrelations between the altered and the original reward are maintained. We name\nthe proposed framework ClAssification-based Reward Shaping (CaReS), since the\naltered reward is learned in a self-supervised manner using classifier-based\nreward inference. Experimental results on several sparse-reward environments\ndemonstrate that the proposed algorithm is not only significantly more sample\nefficient than the state-of-the-art reinforcement learning baseline but also\nachieves a similar sample efficiency to a baseline that uses hand-designed\ndense reward functions.",
          "link": "http://arxiv.org/abs/2103.04529",
          "publishedOn": "2021-07-13T01:59:36.032Z",
          "wordCount": 640,
          "title": "Self-Supervised Online Reward Shaping in Sparse-Reward Environments. (arXiv:2103.04529v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1\">Micah J. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cito_J/0/1/0/all/0/1\">J&#xfc;rgen Cito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kelvin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1\">Kalyan Veeramachaneni</a>",
          "description": "While the open-source software development model has led to successful\nlarge-scale collaborations in building software systems, data science projects\nare frequently developed by individuals or small teams. We describe challenges\nto scaling data science collaborations and present a conceptual framework and\nML programming model to address them. We instantiate these ideas in Ballet, a\nlightweight framework for collaborative, open-source data science through a\nfocus on feature engineering, and an accompanying cloud-based development\nenvironment. Using our framework, collaborators incrementally propose feature\ndefinitions to a repository which are each subjected to an ML performance\nevaluation and can be automatically merged into an executable feature\nengineering pipeline. We leverage Ballet to conduct a case study analysis of an\nincome prediction problem with 27 collaborators, and discuss implications for\nfuture designers of collaborative projects.",
          "link": "http://arxiv.org/abs/2012.07816",
          "publishedOn": "2021-07-13T01:59:36.000Z",
          "wordCount": 609,
          "title": "Enabling collaborative data science development with the Ballet framework. (arXiv:2012.07816v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1\">Shahana Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>",
          "description": "This work considers clustering nodes of a largely incomplete graph. Under the\nproblem setting, only a small amount of queries about the edges can be made,\nbut the entire graph is not observable. This problem finds applications in\nlarge-scale data clustering using limited annotations, community detection\nunder restricted survey resources, and graph topology inference under\nhidden/removed node interactions. Prior works tackled this problem from various\nperspectives, e.g., convex programming-based low-rank matrix completion and\nactive query-based clique finding. Nonetheless, many existing methods are\ndesigned for estimating the single-cluster membership of the nodes, but nodes\nmay often have mixed (i.e., multi-cluster) membership in practice. Some query\nand computational paradigms, e.g., the random query patterns and nuclear\nnorm-based optimization advocated in the convex approaches, may give rise to\nscalability and implementation challenges. This work aims at learning mixed\nmembership of nodes using queried edges. The proposed method is developed\ntogether with a systematic query principle that can be controlled and adjusted\nby the system designers to accommodate implementation challenges -- e.g., to\navoid querying edges that are physically hard to acquire. Our framework also\nfeatures a lightweight and scalable algorithm with membership learning\nguarantees. Real-data experiments on crowdclustering and community detection\nare used to showcase the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2011.12988",
          "publishedOn": "2021-07-13T01:59:35.994Z",
          "wordCount": 663,
          "title": "Mixed Membership Graph Clustering via Systematic Edge Query. (arXiv:2011.12988v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingfu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>",
          "description": "Recently, many plug-and-play self-attention modules are proposed to enhance\nthe model generalization by exploiting the internal information of deep\nconvolutional neural networks (CNNs). Previous works lay an emphasis on the\ndesign of attention module for specific functionality, e.g., light-weighted or\ntask-oriented attention. However, they ignore the importance of where to plug\nin the attention module since they connect the modules individually with each\nblock of the entire CNN backbone for granted, leading to incremental\ncomputational cost and number of parameters with the growth of network depth.\nThus, we propose a framework called Efficient Attention Network (EAN) to\nimprove the efficiency for the existing attention modules. In EAN, we leverage\nthe sharing mechanism (Huang et al. 2020) to share the attention module within\nthe backbone and search where to connect the shared attention module via\nreinforcement learning. Finally, we obtain the attention network with sparse\nconnections between the backbone and modules, while (1) maintaining accuracy\n(2) reducing extra parameter increment and (3) accelerating inference.\nExtensive experiments on widely-used benchmarks and popular attention networks\nshow the effectiveness of EAN. Furthermore, we empirically illustrate that our\nEAN has the capacity of transferring to other tasks and capturing the\ninformative features. The code is available at\nhttps://github.com/gbup-group/EAN-efficient-attention-network.",
          "link": "http://arxiv.org/abs/2011.14058",
          "publishedOn": "2021-07-13T01:59:35.987Z",
          "wordCount": 685,
          "title": "Efficient Attention Network: Accelerate Attention by Searching Where to Plug. (arXiv:2011.14058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linzhang Wang</a>",
          "description": "The last decade has witnessed a rapid advance in machine learning models.\nWhile the black-box nature of these systems allows powerful predictions, it\ncannot be directly explained, posing a threat to the continuing democratization\nof machine learning technology.\n\nTackling the challenge of model explainability, research has made significant\nprogress in demystifying the image classification models. In the same spirit of\nthese works, this paper studies code summarization models, particularly, given\nan input program for which a model makes a prediction, our goal is to reveal\nthe key features that the model uses for predicting the label of the program.\nWe realize our approach in HouYi, which we use to evaluate four prominent code\nsummarization models: extreme summarizer, code2vec, code2seq, and sequence GNN.\nResults show that all models base their predictions on syntactic and lexical\nproperties with little to none semantic implication. Based on this finding, we\npresent a novel approach to explaining the predictions of code summarization\nmodels through the lens of training data.\n\nOur work opens up this exciting, new direction of studying what models have\nlearned from source code.",
          "link": "http://arxiv.org/abs/2102.04625",
          "publishedOn": "2021-07-13T01:59:35.980Z",
          "wordCount": 659,
          "title": "WheaCha: A Method for Explaining the Predictions of Code Summarization Models. (arXiv:2102.04625v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vincent_Cuaz_C/0/1/0/all/0/1\">C&#xe9;dric Vincent-Cuaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vayer_T/0/1/0/all/0/1\">Titouan Vayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1\">R&#xe9;mi Flamary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corneli_M/0/1/0/all/0/1\">Marco Corneli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1\">Nicolas Courty</a>",
          "description": "Dictionary learning is a key tool for representation learning, that explains\nthe data as linear combination of few basic elements. Yet, this analysis is not\namenable in the context of graph learning, as graphs usually belong to\ndifferent metric spaces. We fill this gap by proposing a new online Graph\nDictionary Learning approach, which uses the Gromov Wasserstein divergence for\nthe data fitting term. In our work, graphs are encoded through their nodes'\npairwise relations and modeled as convex combination of graph atoms, i.e.\ndictionary elements, estimated thanks to an online stochastic algorithm, which\noperates on a dataset of unregistered graphs with potentially different number\nof nodes. Our approach naturally extends to labeled graphs, and is completed by\na novel upper bound that can be used as a fast approximation of Gromov\nWasserstein in the embedding space. We provide numerical evidences showing the\ninterest of our approach for unsupervised embedding of graph datasets and for\nonline graph subspace estimation and tracking.",
          "link": "http://arxiv.org/abs/2102.06555",
          "publishedOn": "2021-07-13T01:59:35.966Z",
          "wordCount": 613,
          "title": "Online Graph Dictionary Learning. (arXiv:2102.06555v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12876",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_G/0/1/0/all/0/1\">Gexin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zhu Liang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Z/0/1/0/all/0/1\">ZhengHui Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_F/0/1/0/all/0/1\">Feifei Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">YuanQing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jiawen Liang</a>",
          "description": "Electromagnetic source imaging (ESI) is a highly ill-posed inverse problem.\nTo find a unique solution, traditional ESI methods impose a variety of priors\nthat may not reflect the actual source properties. Such limitations of\ntraditional ESI methods hinder their further applications. Inspired by deep\nlearning approaches, a novel data-synthesized spatio-temporal denoising\nautoencoder method (DST-DAE) method was proposed to solve the ESI inverse\nproblem. Unlike the traditional methods, we utilize a neural network to\ndirectly seek generalized mapping from the measured E/MEG signals to the\ncortical sources. A novel data synthesis strategy is employed by introducing\nthe prior information of sources to the generated large-scale samples using the\nforward model of ESI. All the generated data are used to drive the neural\nnetwork to automatically learn inverse mapping. To achieve better estimation\nperformance, a denoising autoencoder (DAE) architecture with spatio-temporal\nfeature extraction blocks is designed. Compared with the traditional methods,\nwe show (1) that the novel deep learning approach provides an effective and\neasy-to-apply way to solve the ESI problem, that (2) compared to traditional\nmethods, DST-DAE with the data synthesis strategy can better consider the\ncharacteristics of real sources than the mathematical formulation of prior\nassumptions, and that (3) the specifically designed architecture of DAE can not\nonly provide a better estimation of source signals but also be robust to noise\npollution. Extensive numerical experiments show that the proposed method is\nsuperior to the traditional knowledge-driven ESI methods.",
          "link": "http://arxiv.org/abs/2010.12876",
          "publishedOn": "2021-07-13T01:59:35.929Z",
          "wordCount": 735,
          "title": "Electromagnetic Source Imaging via a Data-Synthesis-Based Denoising Autoencoder. (arXiv:2010.12876v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bugata_P/0/1/0/all/0/1\">Peter Bugata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drotar_P/0/1/0/all/0/1\">Peter Drotar</a>",
          "description": "Feature selection is important step in machine learning since it has shown to\nimprove prediction accuracy while depressing the curse of dimensionality of\nhigh dimensional data. The neural networks have experienced tremendous success\nin solving many nonlinear learning problems. Here, we propose new\nneural-network based feature selection approach that introduces two constrains,\nthe satisfying of which leads to sparse FS layer. We have performed extensive\nexperiments on synthetic and real world data to evaluate performance of the\nproposed FS. In experiments we focus on the high dimension, low sample size\ndata since those represent the main challenge for feature selection. The\nresults confirm that proposed Feature Selection Based on Sparse Neural Network\nLayer with Normalizing Constraints (SNEL-FS) is able to select the important\nfeatures and yields superior performance compared to other conventional FS\nmethods.",
          "link": "http://arxiv.org/abs/2012.06365",
          "publishedOn": "2021-07-13T01:59:35.923Z",
          "wordCount": 610,
          "title": "Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints. (arXiv:2012.06365v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1\">Jayadev Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canonne_C/0/1/0/all/0/1\">Cl&#xe9;ment L. Canonne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Ziteng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1\">Himanshu Tyagi</a>",
          "description": "We consider the task of distributed parameter estimation using interactive\nprotocols subject to local information constraints such as bandwidth\nlimitations, local differential privacy, and restricted measurements. We\nprovide a unified framework enabling us to derive a variety of (tight) minimax\nlower bounds for different parametric families of distributions, both\ncontinuous and discrete, under any $\\ell_p$ loss. Our lower bound framework is\nversatile and yields \"plug-and-play\" bounds that are widely applicable to a\nlarge range of estimation problems. In particular, our approach recovers bounds\nobtained using data processing inequalities and Cram\\'er--Rao bounds, two other\nalternative approaches for proving lower bounds in our setting of interest.\nFurther, for the families considered, we complement our lower bounds with\nmatching upper bounds.",
          "link": "http://arxiv.org/abs/2010.06562",
          "publishedOn": "2021-07-13T01:59:35.917Z",
          "wordCount": 636,
          "title": "Unified lower bounds for interactive high-dimensional estimation under information constraints. (arXiv:2010.06562v4 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1\">Micha&#x142; Derezi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zhenyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "For a tall $n\\times d$ matrix $A$ and a random $m\\times n$ sketching matrix\n$S$, the sketched estimate of the inverse covariance matrix $(A^\\top A)^{-1}$\nis typically biased: $E[(\\tilde A^\\top\\tilde A)^{-1}]\\ne(A^\\top A)^{-1}$, where\n$\\tilde A=SA$. This phenomenon, which we call inversion bias, arises, e.g., in\nstatistics and distributed optimization, when averaging multiple independently\nconstructed estimates of quantities that depend on the inverse covariance. We\ndevelop a framework for analyzing inversion bias, based on our proposed concept\nof an $(\\epsilon,\\delta)$-unbiased estimator for random matrices. We show that\nwhen the sketching matrix $S$ is dense and has i.i.d. sub-gaussian entries,\nthen after simple rescaling, the estimator $(\\frac m{m-d}\\tilde A^\\top\\tilde\nA)^{-1}$ is $(\\epsilon,\\delta)$-unbiased for $(A^\\top A)^{-1}$ with a sketch of\nsize $m=O(d+\\sqrt d/\\epsilon)$. This implies that for $m=O(d)$, the inversion\nbias of this estimator is $O(1/\\sqrt d)$, which is much smaller than the\n$\\Theta(1)$ approximation error obtained as a consequence of the subspace\nembedding guarantee for sub-gaussian sketches. We then propose a new sketching\ntechnique, called LEverage Score Sparsified (LESS) embeddings, which uses ideas\nfrom both data-oblivious sparse embeddings as well as data-aware leverage-based\nrow sampling methods, to get $\\epsilon$ inversion bias for sketch size\n$m=O(d\\log d+\\sqrt d/\\epsilon)$ in time $O(\\text{nnz}(A)\\log n+md^2)$, where\nnnz is the number of non-zeros. The key techniques enabling our analysis\ninclude an extension of a classical inequality of Bai and Silverstein for\nrandom quadratic forms, which we call the Restricted Bai-Silverstein\ninequality; and anti-concentration of the Binomial distribution via the\nPaley-Zygmund inequality, which we use to prove a lower bound showing that\nleverage score sampling sketches generally do not achieve small inversion bias.",
          "link": "http://arxiv.org/abs/2011.10695",
          "publishedOn": "2021-07-13T01:59:35.910Z",
          "wordCount": 735,
          "title": "Sparse sketches with small inversion bias. (arXiv:2011.10695v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1\">Serban Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>",
          "description": "Convolutional neural networks (CNNs) have led to significant improvements in\ntasks involving semantic segmentation of images. CNNs are vulnerable in the\narea of biomedical image segmentation because of distributional gap between two\nsource and target domains with different data modalities which leads to domain\nshift. Domain shift makes data annotations in new modalities necessary because\nmodels must be retrained from scratch. Unsupervised domain adaptation (UDA) is\nproposed to adapt a model to new modalities using solely unlabeled target\ndomain data. Common UDA algorithms require access to data points in the source\ndomain which may not be feasible in medical imaging due to privacy concerns. In\nthis work, we develop an algorithm for UDA in a privacy-constrained setting,\nwhere the source domain data is inaccessible. Our idea is based on encoding the\ninformation from the source samples into a prototypical distribution that is\nused as an intermediate distribution for aligning the target domain\ndistribution with the source domain distribution. We demonstrate the\neffectiveness of our algorithm by comparing it to state-of-the-art medical\nimage semantic segmentation approaches on two medical image semantic\nsegmentation datasets.",
          "link": "http://arxiv.org/abs/2101.00522",
          "publishedOn": "2021-07-13T01:59:35.904Z",
          "wordCount": 659,
          "title": "Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images. (arXiv:2101.00522v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Thi-Vinh Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong-Thai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1\">Thanh-Le Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_K/0/1/0/all/0/1\">Khac-Quy Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>",
          "description": "Prior works have demonstrated that a low-resource language pair can benefit\nfrom multilingual machine translation (MT) systems, which rely on many language\npairs' joint training. This paper proposes two simple strategies to address the\nrare word issue in multilingual MT systems for two low-resource language pairs:\nFrench-Vietnamese and English-Vietnamese. The first strategy is about dynamical\nlearning word similarity of tokens in the shared space among source languages\nwhile another one attempts to augment the translation ability of rare words\nthrough updating their embeddings during the training. Besides, we leverage\nmonolingual data for multilingual MT systems to increase the amount of\nsynthetic parallel corpora while dealing with the data sparsity problem. We\nhave shown significant improvements of up to +1.62 and +2.54 BLEU points over\nthe bilingual baseline systems for both language pairs and released our\ndatasets for the research community.",
          "link": "http://arxiv.org/abs/2012.08743",
          "publishedOn": "2021-07-13T01:59:35.898Z",
          "wordCount": 627,
          "title": "Improving Multilingual Neural Machine Translation For Low-Resource Languages: French,English - Vietnamese. (arXiv:2012.08743v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaotao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "Due to the excessive cost of large-scale language model pre-training,\nconsiderable efforts have been made to train BERT progressively -- start from\nan inferior but low-cost model and gradually grow the model to increase the\ncomputational complexity. Our objective is to advance the understanding of\nTransformer growth and discover principles that guide progressive training.\nFirst, we find that similar to network architecture search, Transformer growth\nalso favors compound scaling. Specifically, while existing methods only conduct\nnetwork growth in a single dimension, we observe that it is beneficial to use\ncompound growth operators and balance multiple dimensions (e.g., depth, width,\nand input length of the model). Moreover, we explore alternative growth\noperators in each dimension via controlled comparison to give operator\nselection practical guidance. In light of our analyses, the proposed method\nspeeds up BERT pre-training by 73.6% and 82.2% for the base and large models\nrespectively, while achieving comparable performances",
          "link": "http://arxiv.org/abs/2010.12562",
          "publishedOn": "2021-07-13T01:59:35.876Z",
          "wordCount": 632,
          "title": "On the Transformer Growth for Progressive BERT Training. (arXiv:2010.12562v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.07314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1\">Sadeep Jayasumana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1\">Ankit Singh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1\">Himanshu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>",
          "description": "Real-world classification problems typically exhibit an imbalanced or\nlong-tailed label distribution, wherein many labels are associated with only a\nfew samples. This poses a challenge for generalisation on such labels, and also\nmakes na\\\"ive learning biased towards dominant labels. In this paper, we\npresent two simple modifications of standard softmax cross-entropy training to\ncope with these challenges. Our techniques revisit the classic idea of logit\nadjustment based on the label frequencies, either applied post-hoc to a trained\nmodel, or enforced in the loss during training. Such adjustment encourages a\nlarge relative margin between logits of rare versus dominant labels. These\ntechniques unify and generalise several recent proposals in the literature,\nwhile possessing firmer statistical grounding and empirical performance.",
          "link": "http://arxiv.org/abs/2007.07314",
          "publishedOn": "2021-07-13T01:59:35.867Z",
          "wordCount": 594,
          "title": "Long-tail learning via logit adjustment. (arXiv:2007.07314v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehyeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Injune Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyundo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunseo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Won-Seok Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joseph J. Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>",
          "description": "Active learning is widely used to reduce labeling effort and training time by\nrepeatedly querying only the most beneficial samples from unlabeled data. In\nreal-world problems where data cannot be stored indefinitely due to limited\nstorage or privacy issues, the query selection and the model update should be\nperformed as soon as a new data sample is observed. Various online active\nlearning methods have been studied to deal with these challenges; however,\nthere are difficulties in selecting representative query samples and updating\nthe model efficiently without forgetting. In this study, we propose Message\nPassing Adaptive Resonance Theory (MPART) that learns the distribution and\ntopology of input data online. Through message passing on the topological\ngraph, MPART actively queries informative and representative samples, and\ncontinuously improves the classification performance using both labeled and\nunlabeled data. We evaluate our model in stream-based selective sampling\nscenarios with comparable query selection strategies, showing that MPART\nsignificantly outperforms competitive models.",
          "link": "http://arxiv.org/abs/2012.01227",
          "publishedOn": "2021-07-13T01:59:35.830Z",
          "wordCount": 642,
          "title": "Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning. (arXiv:2012.01227v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01807",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Hurwitz_C/0/1/0/all/0/1\">Cole Hurwitz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kudryashova_N/0/1/0/all/0/1\">Nina Kudryashova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1\">Arno Onken</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hennig_M/0/1/0/all/0/1\">Matthias H. Hennig</a>",
          "description": "Modern recording technologies now enable simultaneous recording from large\nnumbers of neurons. This has driven the development of new statistical models\nfor analyzing and interpreting neural population activity. Here we provide a\nbroad overview of recent developments in this area. We compare and contrast\ndifferent approaches, highlight strengths and limitations, and discuss\nbiological and mechanistic insights that these methods provide.",
          "link": "http://arxiv.org/abs/2102.01807",
          "publishedOn": "2021-07-13T01:59:35.812Z",
          "wordCount": 536,
          "title": "Building population models for large-scale neural recordings: opportunities and pitfalls. (arXiv:2102.01807v4 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13472",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vowels_M/0/1/0/all/0/1\">Matthew James Vowels</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Camgoz_N/0/1/0/all/0/1\">Necati Cihan Camgoz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "Undertaking causal inference with observational data is incredibly useful\nacross a wide range of tasks including the development of medical treatments,\nadvertisements and marketing, and policy making. There are two significant\nchallenges associated with undertaking causal inference using observational\ndata: treatment assignment heterogeneity (i.e., differences between the treated\nand untreated groups), and an absence of counterfactual data (i.e., not knowing\nwhat would have happened if an individual who did get treatment, were instead\nto have not been treated). We address these two challenges by combining\nstructured inference and targeted learning. In terms of structure, we factorize\nthe joint distribution into risk, confounding, instrumental, and miscellaneous\nfactors, and in terms of targeted learning, we apply a regularizer derived from\nthe influence curve in order to reduce residual bias. An ablation study is\nundertaken, and an evaluation on benchmark datasets demonstrates that TVAE has\ncompetitive and state of the art performance across.",
          "link": "http://arxiv.org/abs/2009.13472",
          "publishedOn": "2021-07-13T01:59:35.805Z",
          "wordCount": 607,
          "title": "Targeted VAE: Variational and Targeted Learning for Causal Inference. (arXiv:2009.13472v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04857",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1\">Basit O. Alawode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1\">Mudassir Masood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1\">Tarig Ballal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1\">Tareq Al-Naffouri</a>",
          "description": "Recently, deep learning (DL) methods such as convolutional neural networks\n(CNNs) have gained prominence in the area of image denoising. This is owing to\ntheir proven ability to surpass state-of-the-art classical image denoising\nalgorithms such as BM3D. Deep denoising CNNs (DnCNNs) use many feedforward\nconvolution layers with added regularization methods of batch normalization and\nresidual learning to improve denoising performance significantly. However, this\ncomes at the expense of a huge number of trainable parameters. In this paper,\nwe address this issue by reducing the number of parameters while achieving a\ncomparable level of performance. We derive motivation from the improved\nperformance obtained by training networks using the dense-sparse-dense (DSD)\ntraining approach. We extend this training approach to a reduced DnCNN (RDnCNN)\nnetwork resulting in a faster denoising network with significantly reduced\nparameters and comparable performance to the DnCNN.",
          "link": "http://arxiv.org/abs/2107.04857",
          "publishedOn": "2021-07-13T01:59:35.799Z",
          "wordCount": 582,
          "title": "Dense-Sparse Deep CNN Training for Image Denoising. (arXiv:2107.04857v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Summers_C/0/1/0/all/0/1\">Cecilia Summers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinneen_M/0/1/0/all/0/1\">Michael J. Dinneen</a>",
          "description": "Nondeterminism in neural network optimization produces uncertainty in\nperformance, making small improvements difficult to discern from run-to-run\nvariability. While uncertainty can be reduced by training multiple model\ncopies, doing so is time-consuming, costly, and harms reproducibility. In this\nwork, we establish an experimental protocol for understanding the effect of\noptimization nondeterminism on model diversity, allowing us to isolate the\neffects of a variety of sources of nondeterminism. Surprisingly, we find that\nall sources of nondeterminism have similar effects on measures of model\ndiversity. To explain this intriguing fact, we identify the instability of\nmodel training, taken as an end-to-end procedure, as the key determinant. We\nshow that even one-bit changes in initial parameters result in models\nconverging to vastly different values. Last, we propose two approaches for\nreducing the effects of instability on run-to-run variability.",
          "link": "http://arxiv.org/abs/2103.04514",
          "publishedOn": "2021-07-13T01:59:35.791Z",
          "wordCount": 600,
          "title": "Nondeterminism and Instability in Neural Network Optimization. (arXiv:2103.04514v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.15056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>",
          "description": "In this paper, we present a new class of invertible transformations with an\napplication to flow-based generative models. We indicate that many well-known\ninvertible transformations in reversible logic and reversible neural networks\ncould be derived from our proposition. Next, we propose two new coupling layers\nthat are important building blocks of flow-based generative models. In the\nexperiments on digit data, we present how these new coupling layers could be\nused in Integer Discrete Flows (IDF), and that they achieve better results than\nstandard coupling layers used in IDF and RealNVP.",
          "link": "http://arxiv.org/abs/2011.15056",
          "publishedOn": "2021-07-13T01:59:35.775Z",
          "wordCount": 556,
          "title": "General Invertible Transformations for Flow-based Generative Modeling. (arXiv:2011.15056v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Allan Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dangovski_R/0/1/0/all/0/1\">Rumen Dangovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugan_O/0/1/0/all/0/1\">Owen Dugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Samuel Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1\">Marin Solja&#x10d;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1\">Joseph Jacobson</a>",
          "description": "Deep learning owes much of its success to the astonishing expressiveness of\nneural networks. However, this comes at the cost of complex, black-boxed models\nthat extrapolate poorly beyond the domain of the training dataset, conflicting\nwith goals of finding analytic expressions to describe science, engineering and\nreal world data. Under the hypothesis that the hierarchical modularity of such\nlaws can be captured by training a neural network, we introduce OccamNet, a\nneural network model that finds interpretable, compact, and sparse solutions\nfor fitting data, \\`{a} la Occam's razor. Our model defines a probability\ndistribution over a non-differentiable function space. We introduce a two-step\noptimization method that samples functions and updates the weights with\nbackpropagation based on cross-entropy matching in an evolutionary strategy: we\ntrain by biasing the probability mass toward better fitting solutions. OccamNet\nis able to fit a variety of symbolic laws including simple analytic functions,\nrecursive programs, implicit functions, simple image classification, and can\noutperform noticeably state-of-the-art symbolic regression methods on real\nworld regression datasets. Our method requires minimal memory footprint, does\nnot require AI accelerators for efficient training, fits complicated functions\nin minutes of training on a single CPU, and demonstrates significant\nperformance gains when scaled on a GPU. Our implementation, demonstrations and\ninstructions for reproducing the experiments are available at\nhttps://github.com/druidowm/OccamNet_Public.",
          "link": "http://arxiv.org/abs/2007.10784",
          "publishedOn": "2021-07-13T01:59:35.767Z",
          "wordCount": 693,
          "title": "Fast Neural Models for Symbolic Regression at Scale. (arXiv:2007.10784v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chuyu Huang</a>",
          "description": "The metro ridership prediction has always received extensive attention from\ngovernments and researchers. Recent works focus on designing complicated graph\nconvolutional recurrent network architectures to capture spatial and temporal\npatterns. These works extract the information of spatial dimension well, but\nthe limitation of temporal dimension still exists. We extended Neural ODE\nalgorithms to the graph network and proposed the STR-GODEs network, which can\neffectively learn spatial, temporal, and ridership correlations without the\nlimitation of dividing data into equal-sized intervals on the timeline. While\nlearning the spatial relations and the temporal correlations, we modify the\nGODE-RNN cell to obtain the ridership feature and hidden states. Ridership\ninformation and its hidden states are added to the GODESolve to reduce the\nerror accumulation caused by long time series in prediction. Extensive\nexperiments on two large-scale datasets demonstrate the efficacy and robustness\nof our model.",
          "link": "http://arxiv.org/abs/2107.04980",
          "publishedOn": "2021-07-13T01:59:35.760Z",
          "wordCount": 564,
          "title": "STR-GODEs: Spatial-Temporal-Ridership Graph ODEs for Metro Ridership Prediction. (arXiv:2107.04980v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+OConnor_D/0/1/0/all/0/1\">Daniel O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinci_W/0/1/0/all/0/1\">Walter Vinci</a>",
          "description": "Efficient sampling of complex data distributions can be achieved using\ntrained invertible flows (IF), where the model distribution is generated by\npushing a simple base distribution through multiple non-linear bijective\ntransformations. However, the iterative nature of the transformations in IFs\ncan limit the approximation to the target distribution. In this paper we seek\nto mitigate this by implementing RBM-Flow, an IF model whose base distribution\nis a Restricted Boltzmann Machine (RBM) with a continuous smoothing applied. We\nshow that by using RBM-Flow we are able to improve the quality of samples\ngenerated, quantified by the Inception Scores (IS) and Frechet Inception\nDistance (FID), over baseline models with the same IF transformations, but with\nless expressive base distributions. Furthermore, we also obtain D-Flow, an IF\nmodel with uncorrelated discrete latent variables. We show that D-Flow achieves\nsimilar likelihoods and FID/IS scores to those of a typical IF with Gaussian\nbase variables, but with the additional benefit that global features are\nmeaningfully encoded as discrete labels in the latent space.",
          "link": "http://arxiv.org/abs/2012.13196",
          "publishedOn": "2021-07-13T01:59:35.753Z",
          "wordCount": 639,
          "title": "RBM-Flow and D-Flow: Invertible Flows with Discrete Energy Base Spaces. (arXiv:2012.13196v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.00339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruff_L/0/1/0/all/0/1\">Lukas Ruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandermeulen_R/0/1/0/all/0/1\">Robert A. Vandermeulen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franks_B/0/1/0/all/0/1\">Billy Joe Franks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1\">Marius Kloft</a>",
          "description": "Though anomaly detection (AD) can be viewed as a classification problem\n(nominal vs. anomalous) it is usually treated in an unsupervised manner since\none typically does not have access to, or it is infeasible to utilize, a\ndataset that sufficiently characterizes what it means to be \"anomalous.\" In\nthis paper we present results demonstrating that this intuition surprisingly\nseems not to extend to deep AD on images. For a recent AD benchmark on\nImageNet, classifiers trained to discern between normal samples and just a few\n(64) random natural images are able to outperform the current state of the art\nin deep AD. Experimentally we discover that the multiscale structure of image\ndata makes example anomalies exceptionally informative.",
          "link": "http://arxiv.org/abs/2006.00339",
          "publishedOn": "2021-07-13T01:59:35.747Z",
          "wordCount": 598,
          "title": "Rethinking Assumptions in Deep Anomaly Detection. (arXiv:2006.00339v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1\">Arnd Koeppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1\">Franz Bamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1\">Michael Selzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1\">Britta Nestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1\">Bernd Markert</a>",
          "description": "(Artificial) neural networks have become increasingly popular in mechanics to\naccelerate computations with model order reduction techniques and as universal\nmodels for a wide variety of materials. However, the major disadvantage of\nneural networks remains: their numerous parameters are challenging to interpret\nand explain. Thus, neural networks are often labeled as black boxes, and their\nresults often elude human interpretation. In mechanics, the new and active\nfield of physics-informed neural networks attempts to mitigate this\ndisadvantage by designing deep neural networks on the basis of mechanical\nknowledge. By using this a priori knowledge, deeper and more complex neural\nnetworks became feasible, since the mechanical assumptions could be explained.\nHowever, the internal reasoning and explanation of neural network parameters\nremain mysterious.\n\nComplementary to the physics-informed approach, we propose a first step\ntowards a physics-informing approach, which explains neural networks trained on\nmechanical data a posteriori. This novel explainable artificial intelligence\napproach aims at elucidating the black box of neural networks and their\nhigh-dimensional representations. Therein, the principal component analysis\ndecorrelates the distributed representations in cell states of RNNs and allows\nthe comparison to known and fundamental functions. The novel approach is\nsupported by a systematic hyperparameter search strategy that identifies the\nbest neural network architectures and training parameters. The findings of\nthree case studies on fundamental constitutive models (hyperelasticity,\nelastoplasticity, and viscoelasticity) imply that the proposed strategy can\nhelp identify numerical and analytical closed-form solutions to characterize\nnew materials.",
          "link": "http://arxiv.org/abs/2104.10683",
          "publishedOn": "2021-07-13T01:59:35.730Z",
          "wordCount": 736,
          "title": "Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.14536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>",
          "description": "It is commonly believed that networks cannot be both accurate and robust,\nthat gaining robustness means losing accuracy. It is also generally believed\nthat, unless making networks larger, network architectural elements would\notherwise matter little in improving adversarial robustness. Here we present\nevidence to challenge these common beliefs by a careful study about adversarial\ntraining. Our key observation is that the widely-used ReLU activation function\nsignificantly weakens adversarial training due to its non-smooth nature. Hence\nwe propose smooth adversarial training (SAT), in which we replace ReLU with its\nsmooth approximations to strengthen adversarial training. The purpose of smooth\nactivation functions in SAT is to allow it to find harder adversarial examples\nand compute better gradient updates during adversarial training.\n\nCompared to standard adversarial training, SAT improves adversarial\nrobustness for \"free\", i.e., no drop in accuracy and no increase in\ncomputational cost. For example, without introducing additional computations,\nSAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while\nalso improving accuracy by 0.9% on ImageNet. SAT also works well with larger\nnetworks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%\nrobustness on ImageNet, outperforming the previous state-of-the-art defense by\n9.5% for accuracy and 11.6% for robustness. Models are available at\nhttps://github.com/cihangxie/SmoothAdversarialTraining.",
          "link": "http://arxiv.org/abs/2006.14536",
          "publishedOn": "2021-07-13T01:59:35.723Z",
          "wordCount": 674,
          "title": "Smooth Adversarial Training. (arXiv:2006.14536v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shinzaki_M/0/1/0/all/0/1\">Masao Shinzaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koda_Y/0/1/0/all/0/1\">Yusuke Koda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1\">Koji Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1\">Takayuki Nishio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morikura_M/0/1/0/all/0/1\">Masahiro Morikura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirato_Y/0/1/0/all/0/1\">Yushi Shirato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_D/0/1/0/all/0/1\">Daisei Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kita_N/0/1/0/all/0/1\">Naoki Kita</a>",
          "description": "Millimeter wave (mmWave) beam-tracking based on machine learning enables the\ndevelopment of accurate tracking policies while obviating the need to\nperiodically solve beam-optimization problems. However, its applicability is\nstill arguable when training-test gaps exist in terms of environmental\nparameters that affect the node dynamics. From this skeptical point of view,\nthe contribution of this study is twofold. First, by considering an example\nscenario, we confirm that the training-test gap adversely affects the\nbeam-tracking performance. More specifically, we consider nodes placed on\noverhead messenger wires, where the node dynamics are affected by several\nenvironmental parameters, e.g, the wire mass and tension. Although these are\nparticular scenarios, they yield insight into the validation of the\ntraining-test gap problems. Second, we demonstrate the feasibility of\n\\textit{zero-shot adaptation} as a solution, where a learning agent adapts to\nenvironmental parameters unseen during training. This is achieved by leveraging\na robust adversarial reinforcement learning (RARL) technique, where such\ntraining-and-test gaps are regarded as disturbances by adversaries that are\njointly trained with a legitimate beam-tracking agent. Numerical evaluations\ndemonstrate that the beam-tracking policy learned via RARL can be applied to a\nwide range of environmental parameters without severely degrading the received\npower.",
          "link": "http://arxiv.org/abs/2102.08055",
          "publishedOn": "2021-07-13T01:59:35.716Z",
          "wordCount": 694,
          "title": "Zero-Shot Adaptation for mmWave Beam-Tracking on Overhead Messenger Wires through Robust Adversarial Reinforcement Learning. (arXiv:2102.08055v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikkelsen_K/0/1/0/all/0/1\">Kaare Mikkelsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_O/0/1/0/all/0/1\">Oliver Y. Ch&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Philipp Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertins_A/0/1/0/all/0/1\">Alfred Mertins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1\">Maarten De Vos</a>",
          "description": "Black-box skepticism is one of the main hindrances impeding\ndeep-learning-based automatic sleep scoring from being used in clinical\nenvironments. Towards interpretability, this work proposes a\nsequence-to-sequence sleep-staging model, namely SleepTransformer. It is based\non the transformer backbone whose self-attention scores offer interpretability\nof the model's decisions at both the epoch and sequence level. At the epoch\nlevel, the attention scores can be encoded as a heat map to highlight\nsleep-relevant features captured from the input EEG signal. At the sequence\nlevel, the attention scores are visualized as the influence of different\nneighboring epochs in an input sequence (i.e. the context) to recognition of a\ntarget epoch, mimicking the way manual scoring is done by human experts. We\nfurther propose a simple yet efficient method to quantify uncertainty in the\nmodel's decisions. The method, which is based on entropy, can serve as a metric\nfor deferring low-confidence epochs to a human expert for further inspection.\nAdditionally, we demonstrate that the proposed SleepTransformer outperforms\nexisting methods at a lower computational cost and achieves state-of-the-art\nperformance on two experimental databases of different sizes.",
          "link": "http://arxiv.org/abs/2105.11043",
          "publishedOn": "2021-07-13T01:59:35.706Z",
          "wordCount": 663,
          "title": "SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification. (arXiv:2105.11043v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1\">Yonathan Efroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlis_N/0/1/0/all/0/1\">Nadav Merlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aadirupa Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>",
          "description": "A core element in decision-making under uncertainty is the feedback on the\nquality of the performed actions. However, in many applications, such feedback\nis restricted. For example, in recommendation systems, repeatedly asking the\nuser to provide feedback on the quality of recommendations will annoy them. In\nthis work, we formalize decision-making problems with querying budget, where\nthere is a (possibly time-dependent) hard limit on the number of reward queries\nallowed. Specifically, we consider multi-armed bandits, linear bandits, and\nreinforcement learning problems. We start by analyzing the performance of\n`greedy' algorithms that query a reward whenever they can. We show that in\nfully stochastic settings, doing so performs surprisingly well, but in the\npresence of any adversity, this might lead to linear regret. To overcome this\nissue, we propose the Confidence-Budget Matching (CBM) principle that queries\nrewards when the confidence intervals are wider than the inverse square root of\nthe available budget. We analyze the performance of CBM based algorithms in\ndifferent settings and show that they perform well in the presence of adversity\nin the contexts, initial states, and budgets.",
          "link": "http://arxiv.org/abs/2102.03400",
          "publishedOn": "2021-07-13T01:59:35.700Z",
          "wordCount": 646,
          "title": "Confidence-Budget Matching for Sequential Budgeted Learning. (arXiv:2102.03400v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruize Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "The maximum mean discrepancy (MMD) test could in principle detect any\ndistributional discrepancy between two datasets. However, it has been shown\nthat the MMD test is unaware of adversarial attacks -- the MMD test failed to\ndetect the discrepancy between natural and adversarial data. Given this\nphenomenon, we raise a question: are natural and adversarial data really from\ndifferent distributions? The answer is affirmative -- the previous use of the\nMMD test on the purpose missed three key factors, and accordingly, we propose\nthree components. Firstly, the Gaussian kernel has limited representation\npower, and we replace it with an effective deep kernel. Secondly, the test\npower of the MMD test was neglected, and we maximize it following asymptotic\nstatistics. Finally, adversarial data may be non-independent, and we overcome\nthis issue with the wild bootstrap. By taking care of the three factors, we\nverify that the MMD test is aware of adversarial attacks, which lights up a\nnovel road for adversarial data detection based on two-sample tests.",
          "link": "http://arxiv.org/abs/2010.11415",
          "publishedOn": "2021-07-13T01:59:35.657Z",
          "wordCount": 649,
          "title": "Maximum Mean Discrepancy Test is Aware of Adversarial Attacks. (arXiv:2010.11415v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hoang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>",
          "description": "We develop a new algorithm for non-convex stochastic optimization that finds\nan $\\epsilon$-critical point in the optimal $O(\\epsilon^{-3})$ stochastic\ngradient and Hessian-vector product computations. Our algorithm uses\nHessian-vector products to \"correct\" a bias term in the momentum of SGD with\nmomentum. This leads to better gradient estimates in a manner analogous to\nvariance reduction methods. In contrast to prior work, we do not require\nexcessively large batch sizes, and are able to provide an adaptive algorithm\nwhose convergence rate automatically improves with decreasing variance in the\ngradient estimates. We validate our results on a variety of large-scale deep\nlearning architectures and benchmarks tasks.",
          "link": "http://arxiv.org/abs/2103.03265",
          "publishedOn": "2021-07-13T01:59:35.652Z",
          "wordCount": 555,
          "title": "Better SGD using Second-order Momentum. (arXiv:2103.03265v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+YinchuanLi/0/1/0/all/0/1\">YinchuanLi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XiaofengLiu/0/1/0/all/0/1\">XiaofengLiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YunfengShao/0/1/0/all/0/1\">YunfengShao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+QingWang/0/1/0/all/0/1\">QingWang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YanhuiGeng/0/1/0/all/0/1\">YanhuiGeng</a>",
          "description": "Structured pruning is an effective compression technique to reduce the\ncomputation of neural networks, which is usually achieved by adding\nperturbations to reduce network parameters at the cost of slightly increasing\ntraining loss. A more reasonable approach is to find a sparse minimizer along\nthe flat minimum valley found by optimizers, i.e. stochastic gradient descent,\nwhich keeps the training loss constant. To achieve this goal, we propose the\nstructured directional pruning based on orthogonal projecting the perturbations\nonto the flat minimum valley. We also propose a fast solver sDprun and further\nprove that it achieves directional pruning asymptotically after sufficient\ntraining. Experiments using VGG-Net and ResNet on CIFAR-10 and CIFAR-100\ndatasets show that our method obtains the state-of-the-art pruned accuracy\n(i.e. 93.97% on VGG16, CIFAR-10 task) without retraining. Experiments using\nDNN, VGG-Net and WRN28X10 on MNIST, CIFAR-10 and CIFAR-100 datasets demonstrate\nour method performs structured directional pruning, reaching the same minimum\nvalley as the optimizer.",
          "link": "http://arxiv.org/abs/2107.05328",
          "publishedOn": "2021-07-13T01:59:35.634Z",
          "wordCount": 585,
          "title": "Structured Directional Pruning via Perturbation Orthogonal Projection. (arXiv:2107.05328v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1\">Keisuke Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeishi_N/0/1/0/all/0/1\">Naoya Takeishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsutsui_K/0/1/0/all/0/1\">Kazushi Tsutsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujioka_E/0/1/0/all/0/1\">Emyo Fujioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishiumi_N/0/1/0/all/0/1\">Nozomi Nishiumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_R/0/1/0/all/0/1\">Ryooya Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukushiro_M/0/1/0/all/0/1\">Mika Fukushiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ide_K/0/1/0/all/0/1\">Kaoru Ide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohno_H/0/1/0/all/0/1\">Hiroyoshi Kohno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoda_K/0/1/0/all/0/1\">Ken Yoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_S/0/1/0/all/0/1\">Susumu Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiryu_S/0/1/0/all/0/1\">Shizuko Hiryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1\">Yoshinobu Kawahara</a>",
          "description": "Extracting the interaction rules of biological agents from moving sequences\npose challenges in various domains. Granger causality is a practical framework\nfor analyzing the interactions from observed time-series data; however, this\nframework ignores the structures of the generative process in animal behaviors,\nwhich may lead to interpretational problems and sometimes erroneous assessments\nof causality. In this paper, we propose a new framework for learning Granger\ncausality from multi-animal trajectories via augmented theory-based behavioral\nmodels with interpretable data-driven models. We adopt an approach for\naugmenting incomplete multi-agent behavioral models described by time-varying\ndynamical systems with neural networks. For efficient and interpretable\nlearning, our model leverages theory-based architectures separating navigation\nand motion processes, and the theory-guided regularization for reliable\nbehavioral modeling. This can provide interpretable signs of Granger-causal\neffects over time, i.e., when specific others cause the approach or separation.\nIn experiments using synthetic datasets, our method achieved better performance\nthan various baselines. We then analyzed multi-animal datasets of mice, flies,\nbirds, and bats, which verified our method and obtained novel biological\ninsights.",
          "link": "http://arxiv.org/abs/2107.05326",
          "publishedOn": "2021-07-13T01:59:35.625Z",
          "wordCount": 632,
          "title": "Learning interaction rules from multi-animal trajectories via augmented behavioral models. (arXiv:2107.05326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04248",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Dylewsky_D/0/1/0/all/0/1\">Daniel Dylewsky</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Barajas_Solano_D/0/1/0/all/0/1\">David Barajas-Solano</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ma_T/0/1/0/all/0/1\">Tong Ma</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1\">Alexandre M. Tartakovsky</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kutz_J/0/1/0/all/0/1\">J. Nathan Kutz</a>",
          "description": "Time series forecasting remains a central challenge problem in almost all\nscientific disciplines. We introduce a novel load forecasting method in which\nobserved dynamics are modeled as a forced linear system using Dynamic Mode\nDecomposition (DMD) in time delay coordinates. Central to this approach is the\ninsight that grid load, like many observables on complex real-world systems,\nhas an \"almost-periodic\" character, i.e., a continuous Fourier spectrum\npunctuated by dominant peaks, which capture regular (e.g., daily or weekly)\nrecurrences in the dynamics. The forecasting method presented takes advantage\nof this property by (i) regressing to a deterministic linear model whose\neigenspectrum maps onto those peaks, and (ii) simultaneously learning a\nstochastic Gaussian process regression (GPR) process to actuate this system.\nOur forecasting algorithm is compared against state-of-the-art forecasting\ntechniques not using additional explanatory variables and is shown to produce\nsuperior performance. Moreover, its use of linear intrinsic dynamics offers a\nnumber of desirable properties in terms of interpretability and parsimony.\nResults are presented for a test case using load data from an electrical grid.\nLoad forecasting is an essential challenge in power systems engineering, with\nmajor implications for real-time control, pricing, maintenance, and security\ndecisions.",
          "link": "http://arxiv.org/abs/2010.04248",
          "publishedOn": "2021-07-13T01:59:35.554Z",
          "wordCount": 661,
          "title": "Stochastically forced ensemble dynamic mode decomposition for forecasting and analysis of near-periodic systems. (arXiv:2010.04248v2 [physics.soc-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1\">Dylan Slack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1\">Sophie Hilgard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As black box explanations are increasingly being employed to establish model\ncredibility in high stakes settings, it is important to ensure that these\nexplanations are accurate and reliable. However, prior work demonstrates that\nexplanations generated by state-of-the-art techniques are inconsistent,\nunstable, and provide very little insight into their correctness and\nreliability. In addition, these methods are also computationally inefficient,\nand require significant hyper-parameter tuning. In this paper, we address the\naforementioned challenges by developing a novel Bayesian framework for\ngenerating local explanations along with their associated uncertainty. We\ninstantiate this framework to obtain Bayesian versions of LIME and KernelSHAP\nwhich output credible intervals for the feature importances, capturing the\nassociated uncertainty. The resulting explanations not only enable us to make\nconcrete inferences about their quality (e.g., there is a 95\\% chance that the\nfeature importance lies within the given range), but are also highly consistent\nand stable. We carry out a detailed theoretical analysis that leverages the\naforementioned uncertainty to estimate how many perturbations to sample, and\nhow to sample for faster convergence. This work makes the first attempt at\naddressing several critical issues with popular explanation methods in one\nshot, thereby generating consistent, stable, and reliable explanations with\nguarantees in a computationally efficient manner. Experimental evaluation with\nmultiple real world datasets and user studies demonstrate that the efficacy of\nthe proposed framework.",
          "link": "http://arxiv.org/abs/2008.05030",
          "publishedOn": "2021-07-13T01:59:35.535Z",
          "wordCount": 696,
          "title": "Reliable Post hoc Explanations: Modeling Uncertainty in Explainability. (arXiv:2008.05030v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martino_L/0/1/0/all/0/1\">Luca Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1\">Jesse Read</a>",
          "description": "The expressive power of Bayesian kernel-based methods has led them to become\nan important tool across many different facets of artificial intelligence, and\nuseful to a plethora of modern application domains, providing both power and\ninterpretability via uncertainty analysis. This article introduces and\ndiscusses two methods which straddle the areas of probabilistic Bayesian\nschemes and kernel methods for regression: Gaussian Processes and Relevance\nVector Machines. Our focus is on developing a common framework with which to\nview these methods, via intermediate methods a probabilistic version of the\nwell-known kernel ridge regression, and drawing connections among them, via\ndual formulations, and discussion of their application in the context of major\ntasks: regression, smoothing, interpolation, and filtering. Overall, we provide\nunderstanding of the mathematical concepts behind these models, and we\nsummarize and discuss in depth different interpretations and highlight the\nrelationship to other methods, such as linear kernel smoothers, Kalman\nfiltering and Fourier approximations. Throughout, we provide numerous figures\nto promote understanding, and we make numerous recommendations to\npractitioners. Benefits and drawbacks of the different techniques are\nhighlighted. To our knowledge, this is the most in-depth study of its kind to\ndate focused on these two methods, and will be relevant to theoretical\nunderstanding and practitioners throughout the domains of data-science, signal\nprocessing, machine learning, and artificial intelligence in general.",
          "link": "http://arxiv.org/abs/2009.09217",
          "publishedOn": "2021-07-13T01:59:35.529Z",
          "wordCount": 740,
          "title": "A Joint introduction to Gaussian Processes and Relevance Vector Machines with Connections to Kalman filtering and other Kernel Smoothers. (arXiv:2009.09217v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ronghang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>",
          "description": "Owing to the remarkable capability of extracting effective graph embeddings,\ngraph convolutional network (GCN) and its variants have been successfully\napplied to a broad range of tasks, such as node classification, link\nprediction, and graph classification. Traditional GCN models suffer from the\nissues of overfitting and oversmoothing, while some recent techniques like\nDropEdge could alleviate these issues and thus enable the development of deep\nGCN. However, training GCN models is non-trivial, as it is sensitive to the\nchoice of hyperparameters such as dropout rate and learning weight decay,\nespecially for deep GCN models. In this paper, we aim to automate the training\nof GCN models through hyperparameter optimization. To be specific, we propose a\nself-tuning GCN approach with an alternate training algorithm, and further\nextend our approach by incorporating the population based training scheme.\nExperimental results on three benchmark datasets demonstrate the effectiveness\nof our approaches on optimizing multi-layer GCN, compared with several\nrepresentative baselines.",
          "link": "http://arxiv.org/abs/2107.04713",
          "publishedOn": "2021-07-13T01:59:35.523Z",
          "wordCount": 600,
          "title": "Automated Graph Learning via Population Based Self-Tuning GCN. (arXiv:2107.04713v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perone_C/0/1/0/all/0/1\">Christian S. Perone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_R/0/1/0/all/0/1\">Roberto Pereira Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paula_T/0/1/0/all/0/1\">Thomas Paula</a>",
          "description": "Uncertainty quantification for deep neural networks has recently evolved\nthrough many techniques. In this work, we revisit Laplace approximation, a\nclassical approach for posterior approximation that is computationally\nattractive. However, instead of computing the curvature matrix, we show that,\nunder some regularity conditions, the Laplace approximation can be easily\nconstructed using the gradient second moment. This quantity is already\nestimated by many exponential moving average variants of Adagrad such as Adam\nand RMSprop, but is traditionally discarded after training. We show that our\nmethod (L2M) does not require changes in models or optimization, can be\nimplemented in a few lines of code to yield reasonable results, and it does not\nrequire any extra computational steps besides what is already being computed by\noptimizers, without introducing any new hyperparameter. We hope our method can\nopen new research directions on using quantities already computed by optimizers\nfor uncertainty estimation in deep neural networks.",
          "link": "http://arxiv.org/abs/2107.04695",
          "publishedOn": "2021-07-13T01:59:35.517Z",
          "wordCount": 601,
          "title": "L2M: Practical posterior Laplace approximation with optimization-driven second moment estimation. (arXiv:2107.04695v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09646",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this paper, we present a novel network for high resolution video\ngeneration. Our network uses ideas from Wasserstein GANs by enforcing\nk-Lipschitz constraint on the loss term and Conditional GANs using class labels\nfor training and testing. We present Generator and Discriminator network\nlayerwise details along with the combined network architecture, optimization\ndetails and algorithm used in this work. Our network uses a combination of two\nloss terms: mean square pixel loss and an adversarial loss. The datasets used\nfor training and testing our network are UCF101, Golf and Aeroplane Datasets.\nUsing Inception Score and Fr\\'echet Inception Distance as the evaluation\nmetrics, our network outperforms previous state of the art networks on\nunsupervised video generation.",
          "link": "http://arxiv.org/abs/2008.09646",
          "publishedOn": "2021-07-13T01:59:35.510Z",
          "wordCount": 596,
          "title": "HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN. (arXiv:2008.09646v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Ju-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>",
          "description": "Recently proposed self-supervised learning approaches have been successful\nfor pre-training speech representation models. The utility of these learned\nrepresentations has been observed empirically, but not much has been studied\nabout the type or extent of information encoded in the pre-trained\nrepresentations themselves. Developing such insights can help understand the\ncapabilities and limits of these models and enable the research community to\nmore efficiently develop their usage for downstream applications. In this work,\nwe begin to fill this gap by examining one recent and successful pre-trained\nmodel (wav2vec 2.0), via its intermediate representation vectors, using a suite\nof analysis tools. We use the metrics of canonical correlation, mutual\ninformation, and performance on simple downstream tasks with non-parametric\nprobes, in order to (i) query for acoustic and linguistic information content,\n(ii) characterize the evolution of information across model layers, and (iii)\nunderstand how fine-tuning the model for automatic speech recognition (ASR)\naffects these observations. Our findings motivate modifying the fine-tuning\nprotocol for ASR, which produces improved word error rates in a low-resource\nsetting.",
          "link": "http://arxiv.org/abs/2107.04734",
          "publishedOn": "2021-07-13T01:59:35.504Z",
          "wordCount": 612,
          "title": "Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1\">Sara Hajj Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1\">Mohamed Nassar</a>",
          "description": "Deep learning is a type of machine learning that adapts a deep hierarchy of\nconcepts. Deep learning classifiers link the most basic version of concepts at\nthe input layer to the most abstract version of concepts at the output layer,\nalso known as a class or label. However, once trained over a finite set of\nclasses, a deep learning model does not have the power to say that a given\ninput does not belong to any of the classes and simply cannot be linked.\nCorrectly invalidating the prediction of unrelated classes is a challenging\nproblem that has been tackled in many ways in the literature. Novelty detection\ngives deep learning the ability to output \"do not know\" for novel/unseen\nclasses. Still, no attention has been given to the security aspects of novelty\ndetection. In this paper, we consider the case study of abstraction-based\nnovelty detection and show that it is not robust against adversarial samples.\nMoreover, we show the feasibility of crafting adversarial samples that fool the\ndeep learning classifier and bypass the novelty detection monitoring at the\nsame time. In other words, these monitoring boxes are hackable. We demonstrate\nthat novelty detection itself ends up as an attack surface.",
          "link": "http://arxiv.org/abs/2107.04764",
          "publishedOn": "2021-07-13T01:59:35.488Z",
          "wordCount": 635,
          "title": "Hack The Box: Fooling Deep Learning Abstraction-Based Monitors. (arXiv:2107.04764v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_G/0/1/0/all/0/1\">Gaurav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>",
          "description": "Generating future frames given a few context (or past) frames is a\nchallenging task. It requires modeling the temporal coherence of videos and\nmulti-modality in terms of diversity in the potential future states. Current\nvariational approaches for video generation tend to marginalize over\nmulti-modal future outcomes. Instead, we propose to explicitly model the\nmulti-modality in the future outcomes and leverage it to sample diverse\nfutures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to\nlearn priors on future states given the past and maintains a probability\ndistribution over possible futures given a particular sample. In addition, we\nleverage the changes in this distribution over time to control the sampling of\ndiverse future states by estimating the end of ongoing sequences. That is, we\nuse the variance of GP over the output function space to trigger a change in an\naction sequence. We achieve state-of-the-art results on diverse future frame\ngeneration in terms of reconstruction quality and diversity of the generated\nsequences.",
          "link": "http://arxiv.org/abs/2107.04619",
          "publishedOn": "2021-07-13T01:59:35.476Z",
          "wordCount": 612,
          "title": "Diverse Video Generation using a Gaussian Process Trigger. (arXiv:2107.04619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sivaprasad_S/0/1/0/all/0/1\">Sarath Sivaprasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ankur Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manwani_N/0/1/0/all/0/1\">Naresh Manwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>",
          "description": "In this paper, we investigate a constrained formulation of neural networks\nwhere the output is a convex function of the input. We show that the convexity\nconstraints can be enforced on both fully connected and convolutional layers,\nmaking them applicable to most architectures. The convexity constraints include\nrestricting the weights (for all but the first layer) to be non-negative and\nusing a non-decreasing convex activation function. Albeit simple, these\nconstraints have profound implications on the generalization abilities of the\nnetwork. We draw three valuable insights: (a) Input Output Convex Neural\nNetworks (IOC-NNs) self regularize and reduce the problem of overfitting; (b)\nAlthough heavily constrained, they outperform the base multi layer perceptrons\nand achieve similar performance as compared to base convolutional architectures\nand (c) IOC-NNs show robustness to noise in train labels. We demonstrate the\nefficacy of the proposed idea using thorough experiments and ablation studies\non standard image classification datasets with three different neural network\narchitectures.",
          "link": "http://arxiv.org/abs/2006.05103",
          "publishedOn": "2021-07-13T01:59:35.464Z",
          "wordCount": 630,
          "title": "The Curious Case of Convex Neural Networks. (arXiv:2006.05103v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacques_B/0/1/0/all/0/1\">Brandon G. Jacques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiganj_Z/0/1/0/all/0/1\">Zoran Tiganj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Aakash Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_M/0/1/0/all/0/1\">Marc W. Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sederberg_P/0/1/0/all/0/1\">Per B. Sederberg</a>",
          "description": "In machine learning, convolutional neural networks (CNNs) have been extremely\ninfluential in both computer vision and in recognizing patterns extended over\ntime. In computer vision, part of the flexibility arises from the use of\nmax-pooling operations over the convolutions to attain translation invariance.\nIn the mammalian brain, neural representations of time use a set of temporal\nbasis functions. Critically, these basis functions appear to be arranged in a\ngeometric series such that the basis set is evenly distributed over logarithmic\ntime. This paper introduces a Scale-Invariant Temporal History Convolution\nnetwork (SITHCon) that uses a logarithmically-distributed temporal memory. A\nmax-pool over a logarithmically-distributed temporal memory results in\nscale-invariance in time. We compare performance of SITHCon to a Temporal\nConvolution Network (TCN) and demonstrate that, although both networks can\nlearn classification and regression problems on both univariate and\nmultivariate time series $f(t)$, only SITHCon has the property that it\ngeneralizes without retraining to rescaled versions of the input $f(at)$. This\nproperty, inspired by findings from neuroscience and psychology, could lead to\nlarge-scale networks with dramatically different capabilities, including faster\ntraining and greater generalizability, even with significantly fewer free\nparameters.",
          "link": "http://arxiv.org/abs/2107.04616",
          "publishedOn": "2021-07-13T01:59:35.450Z",
          "wordCount": 634,
          "title": "SITHCon: A neural network robust to variations in input scaling on the time dimension. (arXiv:2107.04616v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assaad_S/0/1/0/all/0/1\">Serge Assaad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shuxi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Henry Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>",
          "description": "We examine interval estimation of the effect of a treatment T on an outcome Y\ngiven the existence of an unobserved confounder U. Using H\\\"older's inequality,\nwe derive a set of bounds on the confounding bias |E[Y|T=t]-E[Y|do(T=t)]| based\non the degree of unmeasured confounding (i.e., the strength of the connection\nU->T, and the strength of U->Y). These bounds are tight either when U is\nindependent of T or when U is independent of Y given T (when there is no\nunobserved confounding). We focus on a special case of this bound depending on\nthe total variation distance between the distributions p(U) and p(U|T=t), as\nwell as the maximum (over all possible values of U) deviation of the\nconditional expected outcome E[Y|U=u,T=t] from the average expected outcome\nE[Y|T=t]. We discuss possible calibration strategies for this bound to get\ninterval estimates for treatment effects, and experimentally validate the bound\nusing synthetic and semi-synthetic datasets.",
          "link": "http://arxiv.org/abs/2107.04661",
          "publishedOn": "2021-07-13T01:59:35.443Z",
          "wordCount": 607,
          "title": "H\\\"older Bounds for Sensitivity Analysis in Causal Reasoning. (arXiv:2107.04661v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pareek_D/0/1/0/all/0/1\">Divyansh Pareek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>",
          "description": "Training and using modern neural-network based latent-variable generative\nmodels (like Variational Autoencoders) often require simultaneously training a\ngenerative direction along with an inferential(encoding) direction, which\napproximates the posterior distribution over the latent variables. Thus, the\nquestion arises: how complex does the inferential model need to be, in order to\nbe able to accurately model the posterior distribution of a given generative\nmodel?\n\nIn this paper, we identify an important property of the generative map\nimpacting the required size of the encoder. We show that if the generative map\nis \"strongly invertible\" (in a sense we suitably formalize), the inferential\nmodel need not be much more complex. Conversely, we prove that there exist\nnon-invertible generative maps, for which the encoding direction needs to be\nexponentially larger (under standard assumptions in computational complexity).\nImportantly, we do not require the generative model to be layerwise invertible,\nwhich a lot of the related literature assumes and isn't satisfied by many\narchitectures used in practice (e.g. convolution and pooling based networks).\nThus, we provide theoretical support for the empirical wisdom that learning\ndeep generative models is harder when data lies on a low-dimensional manifold.",
          "link": "http://arxiv.org/abs/2107.04652",
          "publishedOn": "2021-07-13T01:59:35.437Z",
          "wordCount": 634,
          "title": "The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders. (arXiv:2107.04652v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05222",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sreeram_A/0/1/0/all/0/1\">Anirudh Sreeram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehlman_N/0/1/0/all/0/1\">Nicholas Mehlman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peri_R/0/1/0/all/0/1\">Raghuveer Peri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knox_D/0/1/0/all/0/1\">Dillon Knox</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>",
          "description": "In this paper we investigate speech denoising as a defense against\nadversarial attacks on automatic speech recognition (ASR) systems. Adversarial\nattacks attempt to force misclassification by adding small perturbations to the\noriginal speech signal. We propose to counteract this by employing a\nneural-network based denoiser as a pre-processor in the ASR pipeline. The\ndenoiser is independent of the downstream ASR model, and thus can be rapidly\ndeployed in existing systems. We found that training the denoisier using a\nperceptually motivated loss function resulted in increased adversarial\nrobustness without compromising ASR performance on benign samples. Our defense\nwas evaluated (as a part of the DARPA GARD program) on the 'Kenansville' attack\nstrategy across a range of attack strengths and speech samples. An average\nimprovement in Word Error Rate (WER) of about 7.7% was observed over the\nundefended model at 20 dB signal-to-noise-ratio (SNR) attack strength.",
          "link": "http://arxiv.org/abs/2107.05222",
          "publishedOn": "2021-07-13T01:59:35.422Z",
          "wordCount": 608,
          "title": "Perceptual-based deep-learning denoiser as a defense against adversarial attacks on ASR systems. (arXiv:2107.05222v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "Learning disentangled and interpretable representations is an important step\ntowards accomplishing comprehensive data representations on the manifold. In\nthis paper, we propose a novel representation learning algorithm which combines\nthe inference abilities of Variational Autoencoders (VAE) with the\ngeneralization capability of Generative Adversarial Networks (GAN). The\nproposed model, called InfoVAEGAN, consists of three networks~: Encoder,\nGenerator and Discriminator. InfoVAEGAN aims to jointly learn discrete and\ncontinuous interpretable representations in an unsupervised manner by using two\ndifferent data-free log-likelihood functions onto the variables sampled from\nthe generator's distribution. We propose a two-stage algorithm for optimizing\nthe inference network separately from the generator training. Moreover, we\nenforce the learning of interpretable representations through the maximization\nof the mutual information between the existing latent variables and those\ncreated through generative and inference processes.",
          "link": "http://arxiv.org/abs/2107.04705",
          "publishedOn": "2021-07-13T01:59:35.400Z",
          "wordCount": 585,
          "title": "InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood. (arXiv:2107.04705v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1\">Alvaro Velasquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beckus_A/0/1/0/all/0/1\">Andre Beckus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohmen_T/0/1/0/all/0/1\">Taylor Dohmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Ashutosh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topper_N/0/1/0/all/0/1\">Noah Topper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1\">George Atia</a>",
          "description": "The success of reinforcement learning in typical settings is, in part,\npredicated on underlying Markovian assumptions on the reward signal by which an\nagent learns optimal policies. In recent years, the use of reward machines has\nrelaxed this assumption by enabling a structured representation of\nnon-Markovian rewards. In particular, such representations can be used to\naugment the state space of the underlying decision process, thereby\nfacilitating non-Markovian reinforcement learning. However, these reward\nmachines cannot capture the semantics of stochastic reward signals. In this\npaper, we make progress on this front by introducing probabilistic reward\nmachines (PRMs) as a representation of non-Markovian stochastic rewards. We\npresent an algorithm to learn PRMs from the underlying decision process as well\nas to learn the PRM representation of a given decision-making policy.",
          "link": "http://arxiv.org/abs/2107.04633",
          "publishedOn": "2021-07-13T01:59:35.381Z",
          "wordCount": 574,
          "title": "Learning Probabilistic Reward Machines from Non-Markovian Stochastic Reward Processes. (arXiv:2107.04633v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazzine_R/0/1/0/all/0/1\">Raphael Mazzine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martens_D/0/1/0/all/0/1\">David Martens</a>",
          "description": "Counterfactual explanations are viewed as an effective way to explain machine\nlearning predictions. This interest is reflected by a relatively young\nliterature with already dozens of algorithms aiming to generate such\nexplanations. These algorithms are focused on finding how features can be\nmodified to change the output classification. However, this rather general\nobjective can be achieved in different ways, which brings about the need for a\nmethodology to test and benchmark these algorithms. The contributions of this\nwork are manifold: First, a large benchmarking study of 10 algorithmic\napproaches on 22 tabular datasets is performed, using 9 relevant evaluation\nmetrics. Second, the introduction of a novel, first of its kind, framework to\ntest counterfactual generation algorithms. Third, a set of objective metrics to\nevaluate and compare counterfactual results. And finally, insight from the\nbenchmarking results that indicate which approaches obtain the best performance\non what type of dataset. This benchmarking study and framework can help\npractitioners in determining which technique and building blocks most suit\ntheir context, and can help researchers in the design and evaluation of current\nand future counterfactual generation algorithms. Our findings show that,\noverall, there's no single best algorithm to generate counterfactual\nexplanations as the performance highly depends on properties related to the\ndataset, model, score and factual point specificities.",
          "link": "http://arxiv.org/abs/2107.04680",
          "publishedOn": "2021-07-13T01:59:35.321Z",
          "wordCount": 651,
          "title": "A Framework and Benchmarking Study for Counterfactual Generating Methods on Tabular Data. (arXiv:2107.04680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>",
          "description": "In this paper, we propose an end-to-end lifelong learning mixture of experts.\nEach expert is implemented by a Variational Autoencoder (VAE). The experts in\nthe mixture system are jointly trained by maximizing a mixture of individual\ncomponent evidence lower bounds (MELBO) on the log-likelihood of the given\ntraining samples. The mixing coefficients in the mixture, control the\ncontributions of each expert in the goal representation. These are sampled from\na Dirichlet distribution whose parameters are determined through non-parametric\nestimation during lifelong learning. The model can learn new tasks fast when\nthese are similar to those previously learnt. The proposed Lifelong mixture of\nVAE (L-MVAE) expands its architecture with new components when learning a\ncompletely new task. After the training, our model can automatically determine\nthe relevant expert to be used when fed with new data samples. This mechanism\nbenefits both the memory efficiency and the required computational cost as only\none expert is used during the inference. The L-MVAE inference model is able to\nperform interpolation in the joint latent space across the data domains\nassociated with different tasks and is shown to be efficient for disentangled\nlearning representation.",
          "link": "http://arxiv.org/abs/2107.04694",
          "publishedOn": "2021-07-13T01:59:35.269Z",
          "wordCount": 631,
          "title": "Lifelong Mixture of Variational Autoencoders. (arXiv:2107.04694v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1\">Dimitrios Vytiniotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1\">Grzegorz Swirszcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1\">Viorica Patraucean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.",
          "link": "http://arxiv.org/abs/2106.08318",
          "publishedOn": "2021-07-13T01:59:34.776Z",
          "wordCount": 668,
          "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seely_J/0/1/0/all/0/1\">Jeffrey Seely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1\">Awni Hannun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1\">Nicolas Usunier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>",
          "description": "Machine learning systems typically assume that the distributions of training\nand test sets match closely. However, a critical requirement of such systems in\nthe real world is their ability to generalize to unseen domains. Here, we\npropose an inter-domain gradient matching objective that targets domain\ngeneralization by maximizing the inner product between gradients from different\ndomains. Since direct optimization of the gradient inner product can be\ncomputationally prohibitive -- requires computation of second-order derivatives\n-- we derive a simpler first-order algorithm named Fish that approximates its\noptimization. We demonstrate the efficacy of Fish on 6 datasets from the Wilds\nbenchmark, which captures distribution shift across a diverse range of\nmodalities. Our method produces competitive results on these datasets and\nsurpasses all baselines on 4 of them. We perform experiments on both the Wilds\nbenchmark, which captures distribution shift in the real world, as well as\ndatasets in DomainBed benchmark that focuses more on synthetic-to-real\ntransfer. Our method produces competitive results on both benchmarks,\ndemonstrating its effectiveness across a wide range of domain generalization\ntasks.",
          "link": "http://arxiv.org/abs/2104.09937",
          "publishedOn": "2021-07-13T01:59:34.764Z",
          "wordCount": 641,
          "title": "Gradient Matching for Domain Generalization. (arXiv:2104.09937v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1\">Pratik Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1\">Christos Davatzikos</a>",
          "description": "Heterogeneity in medical data, e.g., from data collected at different sites\nand with different protocols in a clinical study, is a fundamental hurdle for\naccurate prediction using machine learning models, as such models often fail to\ngeneralize well. This paper leverages a recently proposed\nnormalizing-flow-based method to perform counterfactual inference upon a\nstructural causal model (SCM), in order to achieve harmonization of such data.\nA causal model is used to model observed effects (brain magnetic resonance\nimaging data) that result from known confounders (site, gender and age) and\nexogenous noise variables. Our formulation exploits the bijection induced by\nflow for the purpose of harmonization. We infer the posterior of exogenous\nvariables, intervene on observations, and draw samples from the resultant SCM\nto obtain counterfactuals. This approach is evaluated extensively on multiple,\nlarge, real-world medical datasets and displayed better cross-domain\ngeneralization compared to state-of-the-art algorithms. Further experiments\nthat evaluate the quality of confounder-independent data generated by our model\nusing regression and classification tasks are provided.",
          "link": "http://arxiv.org/abs/2106.06845",
          "publishedOn": "2021-07-13T01:59:34.758Z",
          "wordCount": 624,
          "title": "Harmonization with Flow-based Causal Inference. (arXiv:2106.06845v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14323",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Deziderio_N/0/1/0/all/0/1\">Nathalie Deziderio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carvalho_H/0/1/0/all/0/1\">Hugo Tremonte de Carvalho</a>",
          "description": "This work was developed aiming to employ Statistical techniques to the field\nof Music Emotion Recognition, a well-recognized area within the Signal\nProcessing world, but hardly explored from the statistical point of view. Here,\nwe opened several possibilities within the field, applying modern Bayesian\nStatistics techniques and developing efficient algorithms, focusing on the\napplicability of the results obtained. Although the motivation for this project\nwas the development of a emotion-based music recommendation system, its main\ncontribution is a highly adaptable multivariate model that can be useful\ninterpreting any database where there is an interest in applying regularization\nin an efficient manner. Broadly speaking, we will explore what role a sound\ntheoretical statistical analysis can play in the modeling of an algorithm that\nis able to understand a well-known database and what can be gained with this\nkind of approach.",
          "link": "http://arxiv.org/abs/2106.14323",
          "publishedOn": "2021-07-13T01:59:34.722Z",
          "wordCount": 597,
          "title": "Use of Variational Inference in Music Emotion Recognition. (arXiv:2106.14323v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malviya_P/0/1/0/all/0/1\">Pranshu Malviya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1\">Balaraman Ravindran</a>",
          "description": "When an agent encounters a continual stream of new tasks in the lifelong\nlearning setting, it leverages the knowledge it gained from the earlier tasks\nto help learn the new tasks better. In such a scenario, identifying an\nefficient knowledge representation becomes a challenging problem. Most research\nworks propose to either store a subset of examples from the past tasks in a\nreplay buffer, dedicate a separate set of parameters to each task or penalize\nexcessive updates over parameters by introducing a regularization term. While\nexisting methods employ the general task-agnostic stochastic gradient descent\nupdate rule, we propose a task-aware optimizer that adapts the learning rate\nbased on the relatedness among tasks. We utilize the directions taken by the\nparameters during the updates by accumulating the gradients specific to each\ntask. These task-based accumulated gradients act as a knowledge base that is\nmaintained and updated throughout the stream. We empirically show that our\nproposed adaptive learning rate not only accounts for catastrophic forgetting\nbut also allows positive backward transfer. We also show that our method\nperforms better than several state-of-the-art methods in lifelong learning on\ncomplex datasets with a large number of tasks.",
          "link": "http://arxiv.org/abs/2105.05155",
          "publishedOn": "2021-07-13T01:59:34.699Z",
          "wordCount": 647,
          "title": "TAG: Task-based Accumulated Gradients for Lifelong learning. (arXiv:2105.05155v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1\">Sebastian Monka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1\">Lavdim Halilaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1\">Stefan Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>",
          "description": "Traditional computer vision approaches, based on neural networks (NN), are\ntypically trained on a large amount of image data. By minimizing the\ncross-entropy loss between a prediction and a given class label, the NN and its\nvisual embedding space are learned to fulfill a given task. However, due to the\nsole dependence on the image data distribution of the training domain, these\nmodels tend to fail when applied to a target domain that differs from their\nsource domain. To learn a more robust NN to domain shifts, we propose the\nknowledge graph neural network (KG-NN), a neuro-symbolic approach that\nsupervises the training using image-data-invariant auxiliary knowledge. The\nauxiliary knowledge is first encoded in a knowledge graph with respective\nconcepts and their relationships, which is then transformed into a dense vector\nrepresentation via an embedding method. Using a contrastive loss function,\nKG-NN learns to adapt its visual embedding space and thus its weights according\nto the image-data invariant knowledge graph embedding space. We evaluate KG-NN\non visual transfer learning tasks for classification using the mini-ImageNet\ndataset and its derivatives, as well as road sign recognition datasets from\nGermany and China. The results show that a visual model trained with a\nknowledge graph as a trainer outperforms a model trained with cross-entropy in\nall experiments, in particular when the domain gap increases. Besides better\nperformance and stronger robustness to domain shifts, these KG-NN adapts to\nmultiple datasets and classes without suffering heavily from catastrophic\nforgetting.",
          "link": "http://arxiv.org/abs/2102.08747",
          "publishedOn": "2021-07-13T01:59:34.686Z",
          "wordCount": 728,
          "title": "Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1\">Sobhan Miryoosefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>",
          "description": "In constrained reinforcement learning (RL), a learning agent seeks to not\nonly optimize the overall reward but also satisfy the additional safety,\ndiversity, or budget constraints. Consequently, existing constrained RL\nsolutions require several new algorithmic ingredients that are notably\ndifferent from standard RL. On the other hand, reward-free RL is independently\ndeveloped in the unconstrained literature, which learns the transition dynamics\nwithout using the reward information, and thus naturally capable of addressing\nRL with multiple objectives under the common dynamics. This paper bridges\nreward-free RL and constrained RL. Particularly, we propose a simple\nmeta-algorithm such that given any reward-free RL oracle, the approachability\nand constrained RL problems can be directly solved with negligible overheads in\nsample complexity. Utilizing the existing reward-free RL solvers, our framework\nprovides sharp sample complexity results for constrained RL in the tabular MDP\nsetting, matching the best existing results up to a factor of horizon\ndependence; our framework directly extends to a setting of tabular two-player\nMarkov games, and gives a new result for constrained RL with linear function\napproximation.",
          "link": "http://arxiv.org/abs/2107.05216",
          "publishedOn": "2021-07-13T01:59:34.680Z",
          "wordCount": 605,
          "title": "A Simple Reward-free Approach to Constrained Reinforcement Learning. (arXiv:2107.05216v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1\">Lirong Xia</a>",
          "description": "Differential privacy (DP) is a widely-accepted and widely-applied notion of\nprivacy based on worst-case analysis. Often, DP classifies most mechanisms\nwithout external noise as non-private [Dwork et al., 2014], and external\nnoises, such as Gaussian noise or Laplacian noise [Dwork et al., 2006], are\nintroduced to improve privacy. In many real-world applications, however, adding\nexternal noise is undesirable and sometimes prohibited. For example,\npresidential elections often require a deterministic rule to be used [Liu et\nal., 2020], and small noises can lead to dramatic decreases in the prediction\naccuracy of deep neural networks, especially the underrepresented classes\n[Bagdasaryan et al., 2019].\n\nIn this paper, we propose a natural extension and relaxation of DP following\nthe worst average-case idea behind the celebrated smoothed analysis [Spielman\nand Teng, 2004]. Our notion, the smoothed DP, can effectively measure the\nprivacy leakage of mechanisms without external noises under realistic settings.\n\nWe prove several strong properties of the smoothed DP, including\ncomposability, robustness to post-processing and etc. We proved that any\ndiscrete mechanism with sampling procedures is more private than what DP\npredicts. In comparison, many continuous mechanisms with sampling procedures\nare still non-private under smoothed DP. Experimentally, we first verified that\nthe discrete sampling mechanisms are private in real-world elections. Then, we\napply the smoothed DP notion on quantized gradient descent, which indicates\nsome neural networks can be private without adding any extra noises. We believe\nthat these results contribute to the theoretical foundation of realistic\nprivacy measures beyond worst-case analysis.",
          "link": "http://arxiv.org/abs/2107.01559",
          "publishedOn": "2021-07-13T01:59:34.674Z",
          "wordCount": 693,
          "title": "Smoothed Differential Privacy. (arXiv:2107.01559v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1\">Michael Moor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennet_N/0/1/0/all/0/1\">Nicolas Bennet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plecko_D/0/1/0/all/0/1\">Drago Plecko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1\">Max Horn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1\">Bastian Rieck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinshausen_N/0/1/0/all/0/1\">Nicolai Meinshausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhlmann_P/0/1/0/all/0/1\">Peter B&#xfc;hlmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1\">Karsten Borgwardt</a>",
          "description": "Despite decades of clinical research, sepsis remains a global public health\ncrisis with high mortality, and morbidity. Currently, when sepsis is detected\nand the underlying pathogen is identified, organ damage may have already\nprogressed to irreversible stages. Effective sepsis management is therefore\nhighly time-sensitive. By systematically analysing trends in the plethora of\nclinical data available in the intensive care unit (ICU), an early prediction\nof sepsis could lead to earlier pathogen identification, resistance testing,\nand effective antibiotic and supportive treatment, and thereby become a\nlife-saving measure. Here, we developed and validated a machine learning (ML)\nsystem for the prediction of sepsis in the ICU. Our analysis represents the\nlargest multi-national, multi-centre in-ICU study for sepsis prediction using\nML to date. Our dataset contains $156,309$ unique ICU admissions, which\nrepresent a refined and harmonised subset of five large ICU databases\noriginating from three countries. Using the international consensus definition\nSepsis-3, we derived hourly-resolved sepsis label annotations, amounting to\n$26,734$ ($17.1\\%$) septic stays. We compared our approach, a deep\nself-attention model, to several clinical baselines as well as ML baselines and\nperformed an extensive internal and external validation within and across\ndatabases. On average, our model was able to predict sepsis with an AUROC of\n$0.847 \\pm 0.050$ (internal out-of sample validation) and $0.761 \\pm 0.052$\n(external validation). For a harmonised prevalence of $17\\%$, at $80\\%$ recall\nour model detects septic patients with $39\\%$ precision 3.7 hours in advance.",
          "link": "http://arxiv.org/abs/2107.05230",
          "publishedOn": "2021-07-13T01:59:34.640Z",
          "wordCount": 681,
          "title": "Predicting sepsis in multi-site, multi-national intensive care cohorts using deep learning. (arXiv:2107.05230v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jiwei Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jolfa_A/0/1/0/all/0/1\">Alireza Jolfa</a>",
          "description": "With recent advances in autonomous driving, Voice Control Systems have become\nincreasingly adopted as human-vehicle interaction methods. This technology\nenables drivers to use voice commands to control the vehicle and will be soon\navailable in Advanced Driver Assistance Systems (ADAS). Prior work has shown\nthat Siri, Alexa and Cortana, are highly vulnerable to inaudible command\nattacks. This could be extended to ADAS in real-world applications and such\ninaudible command threat is difficult to detect due to microphone\nnonlinearities. In this paper, we aim to develop a more practical solution by\nusing camera views to defend against inaudible command attacks where ADAS are\ncapable of detecting their environment via multi-sensors. To this end, we\npropose a novel multimodal deep learning classification system to defend\nagainst inaudible command attacks. Our experimental results confirm the\nfeasibility of the proposed defense methods and the best classification\naccuracy reaches 89.2%. Code is available at\nhttps://github.com/ITSEG-MQ/Sensor-Fusion-Against-VoiceCommand-Attacks.",
          "link": "http://arxiv.org/abs/2104.09872",
          "publishedOn": "2021-07-12T01:55:17.162Z",
          "wordCount": 631,
          "title": "Robust Sensor Fusion Algorithms Against Voice Command Attacks in Autonomous Vehicles. (arXiv:2104.09872v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>",
          "description": "Machine learning models often encounter distribution shifts when deployed in\nthe real world. In this paper, we focus on adaptation to label distribution\nshift in the online setting, where the test-time label distribution is\ncontinually changing and the model must dynamically adapt to it without\nobserving the true label. Leveraging a novel analysis, we show that the lack of\ntrue label does not hinder estimation of the expected test loss, which enables\nthe reduction of online label shift adaptation to conventional online learning.\nInformed by this observation, we propose adaptation algorithms inspired by\nclassical online learning techniques such as Follow The Leader (FTL) and Online\nGradient Descent (OGD) and derive their regret bounds. We empirically verify\nour findings under both simulated and real world label distribution shifts and\nshow that OGD is particularly effective and robust to a variety of challenging\nlabel shift scenarios.",
          "link": "http://arxiv.org/abs/2107.04520",
          "publishedOn": "2021-07-12T01:55:17.137Z",
          "wordCount": 575,
          "title": "Online Adaptation to Label Distribution Shift. (arXiv:2107.04520v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07501",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1\">Michael Arbel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Matthews_A/0/1/0/all/0/1\">Alexander G. D. G. Matthews</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "Annealed Importance Sampling (AIS) and its Sequential Monte Carlo (SMC)\nextensions are state-of-the-art methods for estimating normalizing constants of\nprobability distributions. We propose here a novel Monte Carlo algorithm,\nAnnealed Flow Transport (AFT), that builds upon AIS and SMC and combines them\nwith normalizing flows (NFs) for improved performance. This method transports a\nset of particles using not only importance sampling (IS), Markov chain Monte\nCarlo (MCMC) and resampling steps - as in SMC, but also relies on NFs which are\nlearned sequentially to push particles towards the successive annealed targets.\nWe provide limit theorems for the resulting Monte Carlo estimates of the\nnormalizing constant and expectations with respect to the target distribution.\nAdditionally, we show that a continuous-time scaling limit of the population\nversion of AFT is given by a Feynman--Kac measure which simplifies to the law\nof a controlled diffusion for expressive NFs. We demonstrate experimentally the\nbenefits and limitations of our methodology on a variety of applications.",
          "link": "http://arxiv.org/abs/2102.07501",
          "publishedOn": "2021-07-12T01:55:17.130Z",
          "wordCount": 617,
          "title": "Annealed Flow Transport Monte Carlo. (arXiv:2102.07501v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04346",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koenen_N/0/1/0/all/0/1\">Niklas Koenen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wright_M/0/1/0/all/0/1\">Marvin N. Wright</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maass_P/0/1/0/all/0/1\">Peter Maa&#xdf;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Behrmann_J/0/1/0/all/0/1\">Jens Behrmann</a>",
          "description": "Normalizing flows leverage the Change of Variables Formula (CVF) to define\nflexible density models. Yet, the requirement of smooth transformations\n(diffeomorphisms) in the CVF poses a significant challenge in the construction\nof these models. To enlarge the design space of flows, we introduce\n$\\mathcal{L}$-diffeomorphisms as generalized transformations which may violate\nthese requirements on zero Lebesgue-measure sets. This relaxation allows e.g.\nthe use of non-smooth activation functions such as ReLU. Finally, we apply the\nobtained results to planar, radial, and contractive residual flows.",
          "link": "http://arxiv.org/abs/2107.04346",
          "publishedOn": "2021-07-12T01:55:17.122Z",
          "wordCount": 529,
          "title": "Generalization of the Change of Variables Formula with Applications to Residual Flows. (arXiv:2107.04346v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1906.10109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1\">Daniele Cattaneo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1\">Matteo Vaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballardini_A/0/1/0/all/0/1\">Augusto Luis Ballardini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontana_S/0/1/0/all/0/1\">Simone Fontana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorrenti_D/0/1/0/all/0/1\">Domenico Giorgio Sorrenti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>",
          "description": "In this paper we present CMRNet, a realtime approach based on a Convolutional\nNeural Network to localize an RGB image of a scene in a map built from LiDAR\ndata. Our network is not trained in the working area, i.e. CMRNet does not\nlearn the map. Instead it learns to match an image to the map. We validate our\napproach on the KITTI dataset, processing each frame independently without any\ntracking procedure. CMRNet achieves 0.27m and 1.07deg median localization\naccuracy on the sequence 00 of the odometry dataset, starting from a rough pose\nestimate displaced up to 3.5m and 17deg. To the best of our knowledge this is\nthe first CNN-based approach that learns to match images from a monocular\ncamera to a given, preexisting 3D LiDAR-map.",
          "link": "http://arxiv.org/abs/1906.10109",
          "publishedOn": "2021-07-12T01:55:17.114Z",
          "wordCount": 639,
          "title": "CMRNet: Camera to LiDAR-Map Registration. (arXiv:1906.10109v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.08826",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lu_H/0/1/0/all/0/1\">Haihao Lu</a>",
          "description": "There has been a long history of using ordinary differential equations (ODEs)\nto understand the dynamics of discrete-time algorithms (DTAs). Surprisingly,\nthere are still two fundamental and unanswered questions: (i) it is unclear how\nto obtain a \\emph{suitable} ODE from a given DTA, and (ii) it is unclear the\nconnection between the convergence of a DTA and its corresponding ODEs. In this\npaper, we propose a new machinery -- an $O(s^r)$-resolution ODE framework --\nfor analyzing the behavior of a generic DTA, which (partially) answers the\nabove two questions. The framework contains three steps: 1. To obtain a\nsuitable ODE from a given DTA, we define a hierarchy of $O(s^r)$-resolution\nODEs of a DTA parameterized by the degree $r$, where $s$ is the step-size of\nthe DTA. We present a principal approach to construct the unique\n$O(s^r)$-resolution ODEs from a DTA; 2. To analyze the resulting ODE, we\npropose the $O(s^r)$-linear-convergence condition of a DTA with respect to an\nenergy function, under which the $O(s^r)$-resolution ODE converges linearly to\nan optimal solution; 3. To bridge the convergence properties of a DTA and its\ncorresponding ODEs, we define the properness of an energy function and show\nthat the linear convergence of the $O(s^r)$-resolution ODE with respect to a\nproper energy function can automatically guarantee the linear convergence of\nthe DTA. To better illustrate this machinery, we utilize it to study three\nclassic algorithms -- gradient descent ascent (GDA), proximal point method\n(PPM) and extra-gradient method (EGM) -- for solving the unconstrained minimax\nproblem $\\min_{x\\in\\RR^n} \\max_{y\\in \\RR^m} L(x,y)$.",
          "link": "http://arxiv.org/abs/2001.08826",
          "publishedOn": "2021-07-12T01:55:17.107Z",
          "wordCount": 766,
          "title": "An $O(s^r)$-Resolution ODE Framework for Understanding Discrete-Time Algorithms and Applications to the Linear Convergence of Minimax Problems. (arXiv:2001.08826v7 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_J/0/1/0/all/0/1\">Joymallya Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1\">Suvodeep Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1\">Tim Menzies</a>",
          "description": "Increasingly, software is making autonomous decisions in case of criminal\nsentencing, approving credit cards, hiring employees, and so on. Some of these\ndecisions show bias and adversely affect certain social groups (e.g. those\ndefined by sex, race, age, marital status). Many prior works on bias mitigation\ntake the following form: change the data or learners in multiple ways, then see\nif any of that improves fairness. Perhaps a better approach is to postulate\nroot causes of bias and then applying some resolution strategy. This paper\npostulates that the root causes of bias are the prior decisions that affect-\n(a) what data was selected and (b) the labels assigned to those examples. Our\nFair-SMOTE algorithm removes biased labels; and rebalances internal\ndistributions such that based on sensitive attribute, examples are equal in\nboth positive and negative classes. On testing, it was seen that this method\nwas just as effective at reducing bias as prior approaches. Further, models\ngenerated via Fair-SMOTE achieve higher performance (measured in terms of\nrecall and F1) than other state-of-the-art fairness improvement algorithms. To\nthe best of our knowledge, measured in terms of number of analyzed learners and\ndatasets, this study is one of the largest studies on bias mitigation yet\npresented in the literature.",
          "link": "http://arxiv.org/abs/2105.12195",
          "publishedOn": "2021-07-12T01:55:17.086Z",
          "wordCount": 708,
          "title": "Bias in Machine Learning Software: Why? How? What to do?. (arXiv:2105.12195v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham M. Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaqi Yang</a>",
          "description": "Bandit problems with linear or concave reward have been extensively studied,\nbut relatively few works have studied bandits with non-concave reward. This\nwork considers a large family of bandit problems where the unknown underlying\nreward function is non-concave, including the low-rank generalized linear\nbandit problems and two-layer neural network with polynomial activation bandit\nproblem. For the low-rank generalized linear bandit problem, we provide a\nminimax-optimal algorithm in the dimension, refuting both conjectures in\n[LMT21, JWWN19]. Our algorithms are based on a unified zeroth-order\noptimization paradigm that applies in great generality and attains optimal\nrates in several structured polynomial settings (in the dimension). We further\ndemonstrate the applicability of our algorithms in RL in the generative model\nsetting, resulting in improved sample complexity over prior approaches.\nFinally, we show that the standard optimistic algorithms (e.g., UCB) are\nsub-optimal by dimension factors. In the neural net setting (with polynomial\nactivation functions) with noiseless reward, we provide a bandit algorithm with\nsample complexity equal to the intrinsic algebraic dimension. Again, we show\nthat optimistic approaches have worse sample complexity, polynomial in the\nextrinsic dimension (which could be exponentially worse in the polynomial\ndegree).",
          "link": "http://arxiv.org/abs/2107.04518",
          "publishedOn": "2021-07-12T01:55:17.079Z",
          "wordCount": 631,
          "title": "Optimal Gradient-based Algorithms for Non-concave Bandit Optimization. (arXiv:2107.04518v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baptista_A/0/1/0/all/0/1\">Anthony Baptista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1\">Aitor Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baudot_A/0/1/0/all/0/1\">Ana&#xef;s Baudot</a>",
          "description": "The amount and variety of data is increasing drastically for several years.\nThese data are often represented as networks, which are then explored with\napproaches arising from network theory. Recent years have witnessed the\nextension of network exploration methods to leverage more complex and richer\nnetwork frameworks. Random walks, for instance, have been extended to explore\nmultilayer networks. However, current random walk approaches are limited in the\ncombination and heterogeneity of network layers they can handle. New analytical\nand numerical random walk methods are needed to cope with the increasing\ndiversity and complexity of multilayer networks. We propose here MultiXrank, a\nPython package that enables Random Walk with Restart (RWR) on any kind of\nmultilayer network with an optimized implementation. This package is supported\nby a universal mathematical formulation of the RWR. We evaluated MultiXrank\nwith leave-one-out cross-validation and link prediction, and introduced\nprotocols to measure the impact of the addition or removal of multilayer\nnetwork data on prediction performances. We further measured the sensitivity of\nMultiXrank to input parameters by in-depth exploration of the parameter space.\nFinally, we illustrate the versatility of MultiXrank with different use-cases\nof unsupervised node prioritization and supervised classification in the\ncontext of human genetic diseases.",
          "link": "http://arxiv.org/abs/2107.04565",
          "publishedOn": "2021-07-12T01:55:17.059Z",
          "wordCount": 647,
          "title": "Universal Multilayer Network Exploration by Random Walk with Restart. (arXiv:2107.04565v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhongxia Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cathy Wu</a>",
          "description": "Vehicle routing problems (VRPs) are a class of combinatorial problems with\nwide practical applications. While previous heuristic or learning-based works\nachieve decent solutions on small problem instances of up to 100 customers,\ntheir performance does not scale to large problems. This article presents a\nnovel learning-augmented local search algorithm to solve large-scale VRP. The\nmethod iteratively improves the solution by identifying appropriate subproblems\nand $\\textit{delegating}$ their improvement to a black box subsolver. At each\nstep, we leverage spatial locality to consider only a linear number of\nsubproblems, rather than exponential. We frame subproblem selection as a\nregression problem and train a Transformer on a generated training set of\nproblem instances. We show that our method achieves state-of-the-art\nperformance, with a speed-up of up to 15 times over strong baselines, on VRPs\nwith sizes ranging from 500 to 3000.",
          "link": "http://arxiv.org/abs/2107.04139",
          "publishedOn": "2021-07-12T01:55:17.032Z",
          "wordCount": 568,
          "title": "Learning to Delegate for Large-scale Vehicle Routing. (arXiv:2107.04139v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullah_R/0/1/0/all/0/1\">Rehmat Ullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harvey_P/0/1/0/all/0/1\">Paul Harvey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilpatrick_P/0/1/0/all/0/1\">Peter Kilpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spence_I/0/1/0/all/0/1\">Ivor Spence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varghese_B/0/1/0/all/0/1\">Blesson Varghese</a>",
          "description": "Applying Federated Learning (FL) on Internet-of-Things devices is\nnecessitated by the large volumes of data they produce and growing concerns of\ndata privacy. However, there are three challenges that need to be addressed to\nmake FL efficient: (i) execute on devices with limited computational\ncapabilities, (ii) account for stragglers due to computational heterogeneity of\ndevices, and (iii) adapt to the changing network bandwidths. This paper\npresents FedAdapt, an adaptive offloading FL framework to mitigate the\naforementioned challenges. FedAdapt accelerates local training in\ncomputationally constrained devices by leveraging layer offloading of deep\nneural networks (DNNs) to servers. Further, FedAdapt adopts reinforcement\nlearning-based optimization and clustering to adaptively identify which layers\nof the DNN should be offloaded for each individual device on to a server to\ntackle the challenges of computational heterogeneity and changing network\nbandwidth. Experimental studies are carried out on a lab-based testbed\ncomprising five IoT devices. By offloading a DNN from the device to the server\nFedAdapt reduces the training time of a typical IoT device by over half\ncompared to classic FL. The training time of extreme stragglers and the overall\ntraining time can be reduced by up to 57%. Furthermore, with changing network\nbandwidth, FedAdapt is demonstrated to reduce the training time by up to 40%\nwhen compared to classic FL, without sacrificing accuracy. FedAdapt can be\ndownloaded from https://github.com/qub-blesson/FedAdapt.",
          "link": "http://arxiv.org/abs/2107.04271",
          "publishedOn": "2021-07-12T01:55:17.026Z",
          "wordCount": 673,
          "title": "FedAdapt: Adaptive Offloading for IoT Devices in Federated Learning. (arXiv:2107.04271v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuezhong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jingyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1\">Cheng Zhuo</a>",
          "description": "As technology scaling is approaching the physical limit, lithography hotspot\ndetection has become an essential task in design for manufacturability. While\nthe deployment of pattern matching or machine learning in hotspot detection can\nhelp save significant simulation time, such methods typically demand for\nnon-trivial quality data to build the model, which most design houses are short\nof. Moreover, the design houses are also unwilling to directly share such data\nwith the other houses to build a unified model, which can be ineffective for\nthe design house with unique design patterns due to data insufficiency. On the\nother hand, with data homogeneity in each design house, the locally trained\nmodels can be easily over-fitted, losing generalization ability and robustness.\nIn this paper, we propose a heterogeneous federated learning framework for\nlithography hotspot detection that can address the aforementioned issues. On\none hand, the framework can build a more robust centralized global sub-model\nthrough heterogeneous knowledge sharing while keeping local data private. On\nthe other hand, the global sub-model can be combined with a local sub-model to\nbetter adapt to local data heterogeneity. The experimental results show that\nthe proposed framework can overcome the challenge of non-independent and\nidentically distributed (non-IID) data and heterogeneous communication to\nachieve very high performance in comparison to other state-of-the-art methods\nwhile guaranteeing a good convergence rate in various scenarios.",
          "link": "http://arxiv.org/abs/2107.04367",
          "publishedOn": "2021-07-12T01:55:17.016Z",
          "wordCount": 664,
          "title": "Lithography Hotspot Detection via Heterogeneous Federated Learning with Local Adaptation. (arXiv:2107.04367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eldele_E/0/1/0/all/0/1\">Emadeldeen Eldele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragab_M/0/1/0/all/0/1\">Mohamed Ragab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee-Keong Kwoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "Sleep staging is of great importance in the diagnosis and treatment of sleep\ndisorders. Recently, numerous data driven deep learning models have been\nproposed for automatic sleep staging. They mainly rely on the assumption that\ntraining and testing data are drawn from the same distribution which may not\nhold in real-world scenarios. Unsupervised domain adaption (UDA) has been\nrecently developed to handle this domain shift problem. However, previous UDA\nmethods applied for sleep staging has two main limitations. First, they rely on\na totally shared model for the domain alignment, which may lose the\ndomain-specific information during feature extraction. Second, they only align\nthe source and target distributions globally without considering the class\ninformation in the target domain, which hinders the classification performance\nof the model. In this work, we propose a novel adversarial learning framework\nto tackle the domain shift problem in the unlabeled target domain. First, we\ndevelop unshared attention mechanisms to preserve the domain-specific features\nin the source and target domains. Second, we design a self-training strategy to\nalign the fine-grained class distributions for the source and target domains\nvia target domain pseudo labels. We also propose dual distinct classifiers to\nincrease the robustness and quality of the pseudo labels. The experimental\nresults on six cross-domain scenarios validate the efficacy of our proposed\nframework for sleep staging and its advantage over state-of-the-art UDA\nmethods.",
          "link": "http://arxiv.org/abs/2107.04470",
          "publishedOn": "2021-07-12T01:55:17.009Z",
          "wordCount": 669,
          "title": "Adversarial Domain Adaptation with Self-Training for EEG-based Sleep Stage Classification. (arXiv:2107.04470v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1\">Bin Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xiaoyu Ge</a>",
          "description": "This paper seeks to tackle the bin packing problem (BPP) through a learning\nperspective. Building on self-attention-based encoding and deep reinforcement\nlearning algorithms, we propose a new end-to-end learning model for this task\nof interest. By decomposing the combinatorial action space, as well as\nutilizing a new training technique denoted as prioritized oversampling, which\nis a general scheme to speed up on-policy learning, we achieve state-of-the-art\nperformance in a range of experimental settings. Moreover, although the\nproposed approach attend2pack targets offline-BPP, we strip our method down to\nthe strict online-BPP setting where it is also able to achieve state-of-the-art\nperformance. With a set of ablation studies as well as comparisons against a\nrange of previous works, we hope to offer as a valid baseline approach to this\nfield of study.",
          "link": "http://arxiv.org/abs/2107.04333",
          "publishedOn": "2021-07-12T01:55:16.985Z",
          "wordCount": 580,
          "title": "Attend2Pack: Bin Packing through Deep Reinforcement Learning with Attention. (arXiv:2107.04333v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04575",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Barhoumi_Y/0/1/0/all/0/1\">Yassine Barhoumi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghulam_R/0/1/0/all/0/1\">Rasool Ghulam</a>",
          "description": "We propose a feature generator backbone composed of an ensemble of\nconvolutional neuralnetworks (CNNs) to improve the recently emerging Vision\nTransformer (ViT) models. We tackled the RSNA intracranial hemorrhage\nclassification problem, i.e., identifying various hemorrhage types from\ncomputed tomography (CT) slices. We show that by gradually stacking several\nfeature maps extracted using multiple Xception CNNs, we can develop a\nfeature-rich input for the ViT model. Our approach allowed the ViT model to pay\nattention to relevant features at multiple levels. Moreover, pretraining the n\nCNNs using various paradigms leads to a diverse feature set and further\nimproves the performance of the proposed n-CNN-ViT. We achieved a test accuracy\nof 98.04% with a weighted logarithmic loss value of 0.0708. The proposed\narchitecture is modular and scalable in both the number of CNNs used for\nfeature extraction and the size of the ViT.",
          "link": "http://arxiv.org/abs/2107.04575",
          "publishedOn": "2021-07-12T01:55:16.979Z",
          "wordCount": 579,
          "title": "Scopeformer: n-CNN-ViT Hybrid Model for Intracranial Hemorrhage Classification. (arXiv:2107.04575v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Siyuan Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier B. Oliva</a>",
          "description": "Truly intelligent systems are expected to make critical decisions with\nincomplete and uncertain data. Active feature acquisition (AFA), where features\nare sequentially acquired to improve the prediction, is a step towards this\ngoal. However, current AFA models all deal with a small set of candidate\nfeatures and have difficulty scaling to a large feature space. Moreover, they\nare ignorant about the valid domains where they can predict confidently, thus\nthey can be vulnerable to out-of-distribution (OOD) inputs. In order to remedy\nthese deficiencies and bring AFA models closer to practical use, we propose\nseveral techniques to advance the current AFA approaches. Our framework can\neasily handle a large number of features using a hierarchical acquisition\npolicy and is more robust to OOD inputs with the help of an OOD detector for\npartially observed data. Extensive experiments demonstrate the efficacy of our\nframework over strong baselines.",
          "link": "http://arxiv.org/abs/2107.04163",
          "publishedOn": "2021-07-12T01:55:16.972Z",
          "wordCount": 570,
          "title": "Towards Robust Active Feature Acquisition. (arXiv:2107.04163v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Eng-Jon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husain_S/0/1/0/all/0/1\">Sameed Husain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1\">Miroslaw Bober</a>",
          "description": "The process of aggregation is ubiquitous in almost all deep nets models. It\nfunctions as an important mechanism for consolidating deep features into a more\ncompact representation, whilst increasing robustness to overfitting and\nproviding spatial invariance in deep nets. In particular, the proximity of\nglobal aggregation layers to the output layers of DNNs mean that aggregated\nfeatures have a direct influence on the performance of a deep net. A better\nunderstanding of this relationship can be obtained using information theoretic\nmethods. However, this requires the knowledge of the distributions of the\nactivations of aggregation layers. To achieve this, we propose a novel\nmathematical formulation for analytically modelling the probability\ndistributions of output values of layers involved with deep feature\naggregation. An important outcome is our ability to analytically predict the\nKL-divergence of output nodes in a DNN. We also experimentally verify our\ntheoretical predictions against empirical observations across a range of\ndifferent classification tasks and datasets.",
          "link": "http://arxiv.org/abs/2107.04458",
          "publishedOn": "2021-07-12T01:55:16.960Z",
          "wordCount": 594,
          "title": "Understanding the Distributions of Aggregation Layers in Deep Neural Networks. (arXiv:2107.04458v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04119",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Rao_J/0/1/0/all/0/1\">Jiahua Rao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zheng_S/0/1/0/all/0/1\">Shuangjia Zheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>",
          "description": "Advances in machine learning have led to graph neural network-based methods\nfor drug discovery, yielding promising results in molecular design, chemical\nsynthesis planning, and molecular property prediction. However, current graph\nneural networks (GNNs) remain of limited acceptance in drug discovery is\nlimited due to their lack of interpretability. Although this major weakness has\nbeen mitigated by the development of explainable artificial intelligence (XAI)\ntechniques, the \"ground truth\" assignment in most explainable tasks ultimately\nrests with subjective judgments by humans so that the quality of model\ninterpretation is hard to evaluate in quantity. In this work, we first build\nthree levels of benchmark datasets to quantitatively assess the\ninterpretability of the state-of-the-art GNN models. Then we implemented recent\nXAI methods in combination with different GNN algorithms to highlight the\nbenefits, limitations, and future opportunities for drug discovery. As a\nresult, GradInput and IG generally provide the best model interpretability for\nGNNs, especially when combined with GraphNet and CMPNN. The integrated and\ndeveloped XAI package is fully open-sourced and can be used by practitioners to\ntrain new models on other drug discovery tasks.",
          "link": "http://arxiv.org/abs/2107.04119",
          "publishedOn": "2021-07-12T01:55:16.905Z",
          "wordCount": 620,
          "title": "Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction. (arXiv:2107.04119v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1\">Harini Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_K/0/1/0/all/0/1\">Kathleen M. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1\">John V. Guttag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1\">Arvind Satyanarayan</a>",
          "description": "Interpretability methods aim to help users build trust in and understand the\ncapabilities of machine learning models. However, existing approaches often\nrely on abstract, complex visualizations that poorly map to the task at hand or\nrequire non-trivial ML expertise to interpret. Here, we present two visual\nanalytics modules that facilitate an intuitive assessment of model reliability.\nTo help users better characterize and reason about a model's uncertainty, we\nvisualize raw and aggregate information about a given input's nearest\nneighbors. Using an interactive editor, users can manipulate this input in\nsemantically-meaningful ways, determine the effect on the output, and compare\nagainst their prior expectations. We evaluate our interface using an\nelectrocardiogram beat classification case study. Compared to a baseline\nfeature importance interface, we find that 14 physicians are better able to\nalign the model's uncertainty with domain-relevant factors and build intuition\nabout its capabilities and limitations.",
          "link": "http://arxiv.org/abs/2102.08540",
          "publishedOn": "2021-07-12T01:55:16.899Z",
          "wordCount": 624,
          "title": "Intuitively Assessing ML Model Reliability through Example-Based Explanations and Editing Model Inputs. (arXiv:2102.08540v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12985",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Fataliyev_K/0/1/0/all/0/1\">Kamaladdin Fataliyev</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chivukula_A/0/1/0/all/0/1\">Aneesh Chivukula</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Prasad_M/0/1/0/all/0/1\">Mukesh Prasad</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Stock market movements are influenced by public and private information\nshared through news articles, company reports, and social media discussions.\nAnalyzing these vast sources of data can give market participants an edge to\nmake profit. However, the majority of the studies in the literature are based\non traditional approaches that come short in analyzing unstructured, vast\ntextual data. In this study, we provide a review on the immense amount of\nexisting literature of text-based stock market analysis. We present input data\ntypes and cover main textual data sources and variations. Feature\nrepresentation techniques are then presented. Then, we cover the analysis\ntechniques and create a taxonomy of the main stock market forecast models.\nImportantly, we discuss representative work in each category of the taxonomy,\nanalyzing their respective contributions. Finally, this paper shows the\nfindings on unaddressed open problems and gives suggestions for future work.\nThe aim of this study is to survey the main stock market analysis models, text\nrepresentation techniques for financial market prediction, shortcomings of\nexisting techniques, and propose promising directions for future research.",
          "link": "http://arxiv.org/abs/2106.12985",
          "publishedOn": "2021-07-12T01:55:16.891Z",
          "wordCount": 628,
          "title": "Stock Market Analysis with Text Data: A Review. (arXiv:2106.12985v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1\">Tarun Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Vijayan N. Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1\">Agus Sudjianto</a>",
          "description": "Deep neural networks are increasingly used in natural language processing\n(NLP) models. However, the need to interpret and explain the results from\ncomplex algorithms are limiting their widespread adoption in regulated\nindustries such as banking. There has been recent work on interpretability of\nmachine learning algorithms with structured data. But there are only limited\ntechniques for NLP applications where the problem is more challenging due to\nthe size of the vocabulary, high-dimensional nature, and the need to consider\ntextual coherence and language structure. This paper develops a methodology to\ncompute SHAP values for local explainability of CNN-based text classification\nmodels. The approach is also extended to compute global scores to assess the\nimportance of features. The results are illustrated on sentiment analysis of\nAmazon Electronic Review data.",
          "link": "http://arxiv.org/abs/2008.11825",
          "publishedOn": "2021-07-12T01:55:16.884Z",
          "wordCount": 597,
          "title": "SHAP values for Explaining CNN-based Text Classification Models. (arXiv:2008.11825v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kongtao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franko_K/0/1/0/all/0/1\">Ken Franko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_R/0/1/0/all/0/1\">Ruoxin Sang</a>",
          "description": "The deployment of convolutional neural networks is often hindered by high\ncomputational and storage requirements. Structured model pruning is a promising\napproach to alleviate these requirements. Using the VGG-16 model as an example,\nwe measure the accuracy-efficiency trade-off for various structured model\npruning methods and datasets (CIFAR-10 and ImageNet) on Tensor Processing Units\n(TPUs). To measure the actual performance of models, we develop a structured\nmodel pruning library for TensorFlow2 to modify models in place (instead of\nadding mask layers). We show that structured model pruning can significantly\nimprove model memory usage and speed on TPUs without losing accuracy,\nespecially for small datasets (e.g., CIFAR-10).",
          "link": "http://arxiv.org/abs/2107.04191",
          "publishedOn": "2021-07-12T01:55:16.877Z",
          "wordCount": 547,
          "title": "Structured Model Pruning of Convolutional Networks on Tensor Processing Units. (arXiv:2107.04191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04905",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Ellis_J/0/1/0/all/0/1\">J. Austin Ellis</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Fiedler_L/0/1/0/all/0/1\">Lenz Fiedler</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Popoola_G/0/1/0/all/0/1\">Gabriel A. Popoola</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Modine_N/0/1/0/all/0/1\">Normand A. Modine</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Stephens_J/0/1/0/all/0/1\">J. Adam Stephens</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Thompson_A/0/1/0/all/0/1\">Aidan P. Thompson</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Cangi_A/0/1/0/all/0/1\">Attila Cangi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Rajamanickam_S/0/1/0/all/0/1\">Sivasankaran Rajamanickam</a>",
          "description": "We present a numerical modeling workflow based on machine learning (ML) which\nreproduces the the total energies produced by Kohn-Sham density functional\ntheory (DFT) at finite electronic temperature to within chemical accuracy at\nnegligible computational cost. Based on deep neural networks, our workflow\nyields the local density of states (LDOS) for a given atomic configuration.\nFrom the LDOS, spatially-resolved, energy-resolved, and integrated quantities\ncan be calculated, including the DFT total free energy, which serves as the\nBorn-Oppenheimer potential energy surface for the atoms. We demonstrate the\nefficacy of this approach for both solid and liquid metals and compare results\nbetween independent and unified machine-learning models for solid and liquid\naluminum. Our machine-learning density functional theory framework opens up the\npath towards multiscale materials modeling for matter under ambient and extreme\nconditions at a computational scale and cost that is unattainable with current\nalgorithms.",
          "link": "http://arxiv.org/abs/2010.04905",
          "publishedOn": "2021-07-12T01:55:16.861Z",
          "wordCount": 626,
          "title": "Accelerating Finite-temperature Kohn-Sham Density Functional Theory with Deep Neural Networks. (arXiv:2010.04905v2 [cond-mat.mtrl-sci] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1\">Pak-Hei Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1\">Ana I.L. Namburete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>",
          "description": "The objective of this work is to segment any arbitrary structures of interest\n(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D\nsegmentation). We show that high accuracy can be achieved by simply propagating\nthe 2D slice segmentation with an affinity matrix between consecutive slices,\nwhich can be learnt in a self-supervised manner, namely slice reconstruction.\nSpecifically, we compare the proposed framework, termed as Sli2Vol, with\nsupervised approaches and two other unsupervised/ self-supervised slice\nregistration approaches, on 8 public datasets (both CT and MRI scans), spanning\n9 different SOIs. Without any parameter-tuning, the same model achieves\nsuperior performance with Dice scores (0-100 scale) of over 80 for most of the\nbenchmarks, including the ones that are unseen during training. Our results\nshow generalizability of the proposed approach across data from different\nmachines and with different SOIs: a major use case of semi-automatic\nsegmentation methods where fully supervised approaches would normally struggle.\nThe source code will be made publicly available at\nhttps://github.com/pakheiyeung/Sli2Vol.",
          "link": "http://arxiv.org/abs/2105.12722",
          "publishedOn": "2021-07-12T01:55:16.854Z",
          "wordCount": 653,
          "title": "Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kivlichan_I/0/1/0/all/0/1\">Ian D. Kivlichan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasserman_L/0/1/0/all/0/1\">Lucy Vasserman</a>",
          "description": "Content moderation is often performed by a collaboration between humans and\nmachine learning models. However, it is not well understood how to design the\ncollaborative process so as to maximize the combined moderator-model system\nperformance. This work presents a rigorous study of this problem, focusing on\nan approach that incorporates model uncertainty into the collaborative process.\nFirst, we introduce principled metrics to describe the performance of the\ncollaborative system under capacity constraints on the human moderator,\nquantifying how efficiently the combined system utilizes human decisions. Using\nthese metrics, we conduct a large benchmark study evaluating the performance of\nstate-of-the-art uncertainty models under different collaborative review\nstrategies. We find that an uncertainty-based strategy consistently outperforms\nthe widely used strategy based on toxicity scores, and moreover that the choice\nof review strategy drastically changes the overall system performance. Our\nresults demonstrate the importance of rigorous metrics for understanding and\ndeveloping effective moderator-model systems for content moderation, as well as\nthe utility of uncertainty estimation in this domain.",
          "link": "http://arxiv.org/abs/2107.04212",
          "publishedOn": "2021-07-12T01:55:16.828Z",
          "wordCount": 605,
          "title": "Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation. (arXiv:2107.04212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Ashwin Raaghav Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1\">Arber Zela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikia_T/0/1/0/all/0/1\">Tonmoy Saikia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "Ensembles of CNN models trained with different seeds (also known as Deep\nEnsembles) are known to achieve superior performance over a single copy of the\nCNN. Neural Ensemble Search (NES) can further boost performance by adding\narchitectural diversity. However, the scope of NES remains prohibitive under\nlimited computational resources. In this work, we extend NES to multi-headed\nensembles, which consist of a shared backbone attached to multiple prediction\nheads. Unlike Deep Ensembles, these multi-headed ensembles can be trained end\nto end, which enables us to leverage one-shot NAS methods to optimize an\nensemble objective. With extensive empirical evaluations, we demonstrate that\nmulti-headed ensemble search finds robust ensembles 3 times faster, while\nhaving comparable performance to other ensemble search methods, in both\npredictive performance and uncertainty calibration.",
          "link": "http://arxiv.org/abs/2107.04369",
          "publishedOn": "2021-07-12T01:55:16.817Z",
          "wordCount": 563,
          "title": "Multi-headed Neural Ensemble Search. (arXiv:2107.04369v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1\">Sampo Kuutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "Deep neural networks have demonstrated their capability to learn control\npolicies for a variety of tasks. However, these neural network-based policies\nhave been shown to be susceptible to exploitation by adversarial agents.\nTherefore, there is a need to develop techniques to learn control policies that\nare robust against adversaries. We introduce Adversarially Robust Control\n(ARC), which trains the protagonist policy and the adversarial policy\nend-to-end on the same loss. The aim of the protagonist is to maximise this\nloss, whilst the adversary is attempting to minimise it. We demonstrate the\nproposed ARC training in a highway driving scenario, where the protagonist\ncontrols the follower vehicle whilst the adversary controls the lead vehicle.\nBy training the protagonist against an ensemble of adversaries, it learns a\nsignificantly more robust control policy, which generalises to a variety of\nadversarial strategies. The approach is shown to reduce the amount of\ncollisions against new adversaries by up to 90.25%, compared to the original\npolicy. Moreover, by utilising an auxiliary distillation loss, we show that the\nfine-tuned control policy shows no drop in performance across its original\ntraining distribution.",
          "link": "http://arxiv.org/abs/2107.04487",
          "publishedOn": "2021-07-12T01:55:16.793Z",
          "wordCount": 636,
          "title": "ARC: Adversarially Robust Control Policies for Autonomous Vehicles. (arXiv:2107.04487v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuruvila_I/0/1/0/all/0/1\">Ivine Kuruvila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muncke_J/0/1/0/all/0/1\">Jan Muncke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_E/0/1/0/all/0/1\">Eghart Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoppe_U/0/1/0/all/0/1\">Ulrich Hoppe</a>",
          "description": "Human brain performs remarkably well in segregating a particular speaker from\ninterfering ones in a multi-speaker scenario. It has been recently shown that\nwe can quantitatively evaluate the segregation capability by modelling the\nrelationship between the speech signals present in an auditory scene and the\ncortical signals of the listener measured using electroencephalography (EEG).\nThis has opened up avenues to integrate neuro-feedback into hearing aids\nwhereby the device can infer user's attention and enhance the attended speaker.\nCommonly used algorithms to infer the auditory attention are based on linear\nsystems theory where the speech cues such as envelopes are mapped on to the EEG\nsignals. Here, we present a joint convolutional neural network (CNN) - long\nshort-term memory (LSTM) model to infer the auditory attention. Our joint\nCNN-LSTM model takes the EEG signals and the spectrogram of the multiple\nspeakers as inputs and classifies the attention to one of the speakers. We\nevaluated the reliability of our neural network using three different datasets\ncomprising of 61 subjects where, each subject undertook a dual-speaker\nexperiment. The three datasets analysed corresponded to speech stimuli\npresented in three different languages namely German, Danish and Dutch. Using\nthe proposed joint CNN-LSTM model, we obtained a median decoding accuracy of\n77.2% at a trial duration of three seconds. Furthermore, we evaluated the\namount of sparsity that our model can tolerate by means of magnitude pruning\nand found that the model can tolerate up to 50% sparsity without substantial\nloss of decoding accuracy.",
          "link": "http://arxiv.org/abs/2102.03957",
          "publishedOn": "2021-07-12T01:55:16.782Z",
          "wordCount": 733,
          "title": "Extracting the Auditory Attention in a Dual-Speaker Scenario from EEG using a Joint CNN-LSTM Model. (arXiv:2102.03957v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1\">Erich Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1\">Andreas Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feher_G/0/1/0/all/0/1\">Gloria Feher</a>",
          "description": "Spherical k-means is a widely used clustering algorithm for sparse and\nhigh-dimensional data such as document vectors. While several improvements and\naccelerations have been introduced for the original k-means algorithm, not all\neasily translate to the spherical variant: Many acceleration techniques, such\nas the algorithms of Elkan and Hamerly, rely on the triangle inequality of\nEuclidean distances. However, spherical k-means uses Cosine similarities\ninstead of distances for computational efficiency. In this paper, we\nincorporate the Elkan and Hamerly accelerations to the spherical k-means\nalgorithm working directly with the Cosines instead of Euclidean distances to\nobtain a substantial speedup and evaluate these spherical accelerations on real\ndata.",
          "link": "http://arxiv.org/abs/2107.04074",
          "publishedOn": "2021-07-12T01:55:16.764Z",
          "wordCount": 524,
          "title": "Accelerating Spherical k-Means. (arXiv:2107.04074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1\">Ruoqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>",
          "description": "Particle filtering is a popular method for inferring latent states in\nstochastic dynamical systems, whose theoretical properties have been well\nstudied in machine learning and statistics communities. In many control\nproblems, e.g., partially observed linear dynamical systems (POLDS), oftentimes\nthe inferred latent state is further used for planning at each step. This paper\ninitiates a rigorous study on the efficiency of particle filtering for\nsequential planning, and gives the first particle complexity bounds. Though\nerrors in past actions may affect the future, we are able to bound the number\nof particles needed so that the long-run reward of the policy based on particle\nfiltering is close to that based on exact inference. In particular, we show\nthat, in stable systems, polynomially many particles suffice. Key in our proof\nis a coupling of the ideal sequence based on the exact planning and the\nsequence generated by approximate planning based on particle filtering. We\nbelieve this technique can be useful in other sequential decision-making\nproblems.",
          "link": "http://arxiv.org/abs/2006.05975",
          "publishedOn": "2021-07-12T01:55:16.747Z",
          "wordCount": 650,
          "title": "When is Particle Filtering Efficient for Planning in Partially Observed Linear Dynamical Systems?. (arXiv:2006.05975v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizvi_H/0/1/0/all/0/1\">Hasan Rizvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "In the present study, we propose to implement a new framework for estimating\ngenerative models via an adversarial process to extend an existing GAN\nframework and develop a white-box controllable image cartoonization, which can\ngenerate high-quality cartooned images/videos from real-world photos and\nvideos. The learning purposes of our system are based on three distinct\nrepresentations: surface representation, structure representation, and texture\nrepresentation. The surface representation refers to the smooth surface of the\nimages. The structure representation relates to the sparse colour blocks and\ncompresses generic content. The texture representation shows the texture,\ncurves, and features in cartoon images. Generative Adversarial Network (GAN)\nframework decomposes the images into different representations and learns from\nthem to generate cartoon images. This decomposition makes the framework more\ncontrollable and flexible which allows users to make changes based on the\nrequired output. This approach overcomes any previous system in terms of\nmaintaining clarity, colours, textures, shapes of images yet showing the\ncharacteristics of cartoon images.",
          "link": "http://arxiv.org/abs/2107.04551",
          "publishedOn": "2021-07-12T01:55:16.741Z",
          "wordCount": 609,
          "title": "White-Box Cartoonization Using An Extended GAN Framework. (arXiv:2107.04551v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">Mohammad Javad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>",
          "description": "We study the problem of best-arm identification (BAI) in contextual bandits\nin the fixed-budget setting. We propose a general successive elimination\nalgorithm that proceeds in stages and eliminates a fixed fraction of suboptimal\narms in each stage. This design takes advantage of the strengths of static and\nadaptive allocations. We analyze the algorithm in linear models and obtain a\nbetter error bound than prior work. We also apply it to generalized linear\nmodels (GLMs) and bound its error. This is the first BAI algorithm for GLMs in\nthe fixed-budget setting. Our extensive numerical experiments show that our\nalgorithm outperforms the state of art.",
          "link": "http://arxiv.org/abs/2106.04763",
          "publishedOn": "2021-07-12T01:55:16.720Z",
          "wordCount": 591,
          "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1\">Venkatesh K Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>",
          "description": "Adaptation of a classifier to new domains is one of the challenging problems\nin machine learning. This has been addressed using many deep and non-deep\nlearning based methods. Among the methodologies used, that of adversarial\nlearning is widely applied to solve many deep learning problems along with\ndomain adaptation. These methods are based on a discriminator that ensures\nsource and target distributions are close. However, here we suggest that rather\nthan using a point estimate obtaining by a single discriminator, it would be\nuseful if a distribution based on ensembles of discriminators could be used to\nbridge this gap. This could be achieved using multiple classifiers or using\ntraditional ensemble methods. In contrast, we suggest that a Monte Carlo\ndropout based ensemble discriminator could suffice to obtain the distribution\nbased discriminator. Specifically, we propose a curriculum based dropout\ndiscriminator that gradually increases the variance of the sample based\ndistribution and the corresponding reverse gradients are used to align the\nsource and target feature representations. An ensemble of discriminators helps\nthe model to learn the data distribution efficiently. It also provides a better\ngradient estimates to train the feature extractor. The detailed results and\nthorough ablation analysis show that our model outperforms state-of-the-art\nresults.",
          "link": "http://arxiv.org/abs/2107.04231",
          "publishedOn": "2021-07-12T01:55:16.706Z",
          "wordCount": 652,
          "title": "Exploring Dropout Discriminator for Domain Adaptation. (arXiv:2107.04231v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Milan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Hiresh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1\">Mausoom Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>",
          "description": "Document structure extraction has been a widely researched area for decades\nwith recent works performing it as a semantic segmentation task over document\nimages using fully-convolution networks. Such methods are limited by image\nresolution due to which they fail to disambiguate structures in dense regions\nwhich appear commonly in forms. To mitigate this, we propose Form2Seq, a novel\nsequence-to-sequence (Seq2Seq) inspired framework for structure extraction\nusing text, with a specific focus on forms, which leverages relative spatial\narrangement of structures. We discuss two tasks; 1) Classification of low-level\nconstituent elements (TextBlock and empty fillable Widget) into ten types such\nas field captions, list items, and others; 2) Grouping lower-level elements\ninto higher-order constructs, such as Text Fields, ChoiceFields and\nChoiceGroups, used as information collection mechanism in forms. To achieve\nthis, we arrange the constituent elements linearly in natural reading order,\nfeed their spatial and textual representations to Seq2Seq framework, which\nsequentially outputs prediction of each element depending on the final task. We\nmodify Seq2Seq for grouping task and discuss improvements obtained through\ncascaded end-to-end training of two tasks versus training in isolation.\nExperimental results show the effectiveness of our text-based approach\nachieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01,\n61.63 on groups discussed above respectively, outperforming segmentation\nbaselines. Further we show our framework achieves state of the results for\ntable structure recognition on ICDAR 2013 dataset.",
          "link": "http://arxiv.org/abs/2107.04419",
          "publishedOn": "2021-07-12T01:55:16.700Z",
          "wordCount": 673,
          "title": "Form2Seq : A Framework for Higher-Order Form Structure Extraction. (arXiv:2107.04419v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vijayan_N/0/1/0/all/0/1\">Nithia Vijayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+A_P/0/1/0/all/0/1\">Prashanth L. A</a>",
          "description": "We propose policy-gradient algorithms for solving the problem of control in a\nrisk-sensitive reinforcement learning (RL) context. The objective of our\nalgorithm is to maximize the distorted risk measure (DRM) of the cumulative\nreward in an episodic Markov decision process (MDP). We derive a variant of the\npolicy gradient theorem that caters to the DRM objective. Using this theorem in\nconjunction with a likelihood ratio (LR) based gradient estimation scheme, we\npropose policy gradient algorithms for optimizing DRM in both on-policy and\noff-policy RL settings. We derive non-asymptotic bounds that establish the\nconvergence of our algorithms to an approximate stationary point of the DRM\nobjective.",
          "link": "http://arxiv.org/abs/2107.04422",
          "publishedOn": "2021-07-12T01:55:16.693Z",
          "wordCount": 541,
          "title": "Likelihood ratio-based policy gradient methods for distorted risk measures: A non-asymptotic analysis. (arXiv:2107.04422v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafez_M/0/1/0/all/0/1\">Muhammad Burhan Hafez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>",
          "description": "Recent advances in robot learning have enabled robots to become increasingly\nbetter at mastering a predefined set of tasks. On the other hand, as humans, we\nhave the ability to learn a growing set of tasks over our lifetime. Continual\nrobot learning is an emerging research direction with the goal of endowing\nrobots with this ability. In order to learn new tasks over time, the robot\nfirst needs to infer the task at hand. Task inference, however, has received\nlittle attention in the multi-task learning literature. In this paper, we\npropose a novel approach to continual learning of robotic control tasks. Our\napproach performs unsupervised learning of behavior embeddings by incrementally\nself-organizing demonstrated behaviors. Task inference is made by finding the\nnearest behavior embedding to a demonstrated behavior, which is used together\nwith the environment state as input to a multi-task policy trained with\nreinforcement learning to optimize performance over tasks. Unlike previous\napproaches, our approach makes no assumptions about task distribution and\nrequires no task exploration to infer tasks. We evaluate our approach in\nexperiments with concurrently and sequentially presented tasks and show that it\noutperforms other multi-task learning approaches in terms of generalization\nperformance and convergence speed, particularly in the continual learning\nsetting.",
          "link": "http://arxiv.org/abs/2107.04533",
          "publishedOn": "2021-07-12T01:55:16.687Z",
          "wordCount": 642,
          "title": "Behavior Self-Organization Supports Task Inference for Continual Robot Learning. (arXiv:2107.04533v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weichuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuefang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhe Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changming Sun</a>",
          "description": "Metric-based few-shot fine-grained image classification (FSFGIC) aims to\nlearn a transferable feature embedding network by estimating the similarities\nbetween query images and support classes from very few examples. In this work,\nwe propose, for the first time, to introduce the non-linear data projection\nconcept into the design of FSFGIC architecture in order to address the limited\nsample problem in few-shot learning and at the same time to increase the\ndiscriminability of the model for fine-grained image classification.\nSpecifically, we first design a feature re-abstraction embedding network that\nhas the ability to not only obtain the required semantic features for effective\nmetric learning but also re-enhance such features with finer details from input\nimages. Then the descriptors of the query images and the support classes are\nprojected into different non-linear spaces in our proposed similarity metric\nlearning network to learn discriminative projection factors. This design can\neffectively operate in the challenging and restricted condition of a FSFGIC\ntask for making the distance between the samples within the same class smaller\nand the distance between samples from different classes larger and for reducing\nthe coupling relationship between samples from different categories.\nFurthermore, a novel similarity measure based on the proposed non-linear data\nproject is presented for evaluating the relationships of feature information\nbetween a query image and a support set. It is worth to note that our proposed\narchitecture can be easily embedded into any episodic training mechanisms for\nend-to-end training from scratch. Extensive experiments on FSFGIC tasks\ndemonstrate the superiority of the proposed methods over the state-of-the-art\nbenchmarks.",
          "link": "http://arxiv.org/abs/2106.06988",
          "publishedOn": "2021-07-12T01:55:16.661Z",
          "wordCount": 743,
          "title": "NDPNet: A novel non-linear data projection network for few-shot fine-grained image classification. (arXiv:2106.06988v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rieber_D/0/1/0/all/0/1\">Dennis Rieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acosta_A/0/1/0/all/0/1\">Axel Acosta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Froning_H/0/1/0/all/0/1\">Holger Fr&#xf6;ning</a>",
          "description": "The success of Deep Artificial Neural Networks (DNNs) in many domains created\na rich body of research concerned with hardwareaccelerators for\ncompute-intensive DNN operators. However, implementing such operators\nefficiently with complex hardwareintrinsics such as matrix multiply is a task\nnot yet automated gracefully. Solving this task often requires joint program\nand data layouttransformations. First solutions to this problem have been\nproposed, such as TVM, UNIT or ISAMIR, which work on a loop-levelrepresentation\nof operators and specify data layout and possible program transformations\nbefore the embedding into the operator isperformed. This top-down approach\ncreates a tension between exploration range and search space complexity,\nespecially when alsoexploring data layout transformations such as im2col,\nchannel packing or padding.In this work, we propose a new approach to this\nproblem. We created a bottom-up method that allows the joint transformation\nofboth compuation and data layout based on the found embedding. By formulating\nthe embedding as a constraint satisfaction problemover the scalar dataflow,\nevery possible embedding solution is contained in the search space. Adding\nadditional constraints andoptmization targets to the solver generates the\nsubset of preferable solutions.An evaluation using the VTA hardware accelerator\nwith the Baidu DeepBench inference benchmark shows that our approach\ncanautomatically generate code competitive to reference implementations.\nFurther, we show that dynamically determining the data layoutbased on intrinsic\nand workload is beneficial for hardware utilization and performance. In cases\nwhere the reference implementationhas low hardware utilization due to its fixed\ndeployment strategy, we achieve a geomean speedup of up to x2.813, while\nindividualoperators can improve as much as x170.",
          "link": "http://arxiv.org/abs/2104.04731",
          "publishedOn": "2021-07-12T01:55:16.655Z",
          "wordCount": 755,
          "title": "Joint Program and Layout Transformations to enable DNN Operators on Specialized Hardware based on Constraint Programming. (arXiv:2104.04731v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.14512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kaizhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jacky Y. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1\">Oluwasanmi Koyejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Knowledge transferability, or transfer learning, has been widely adopted to\nallow a pre-trained model in the source domain to be effectively adapted to\ndownstream tasks in the target domain. It is thus important to explore and\nunderstand the factors affecting knowledge transferability. In this paper, as\nthe first work, we analyze and demonstrate the connections between knowledge\ntransferability and another important phenomenon--adversarial transferability,\n\\emph{i.e.}, adversarial examples generated against one model can be\ntransferred to attack other models. Our theoretical studies show that\nadversarial transferability indicates knowledge transferability and vice versa.\nMoreover, based on the theoretical insights, we propose two practical\nadversarial transferability metrics to characterize this process, serving as\nbidirectional indicators between adversarial and knowledge transferability. We\nconduct extensive experiments for different scenarios on diverse datasets,\nshowing a positive correlation between adversarial transferability and\nknowledge transferability. Our findings will shed light on future research\nabout effective knowledge transfer learning and adversarial transferability\nanalyses.",
          "link": "http://arxiv.org/abs/2006.14512",
          "publishedOn": "2021-07-12T01:55:16.648Z",
          "wordCount": 656,
          "title": "Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarenko_S/0/1/0/all/0/1\">Stepan Makarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokin_D/0/1/0/all/0/1\">Dmitry Sorokin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulanov_A/0/1/0/all/0/1\">Alexander Ulanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lvovsky_A/0/1/0/all/0/1\">A. I. Lvovsky</a>",
          "description": "Reinforcement learning is finding its way to real-world problem application,\ntransferring from simulated environments to physical setups. In this work, we\nimplement vision-based alignment of an optical Mach-Zehnder interferometer with\na confocal telescope in one arm, which controls the diameter and divergence of\nthe corresponding beam. We use a continuous action space; exponential scaling\nenables us to handle actions within a range of over two orders of magnitude.\nOur agent trains only in a simulated environment with domain randomizations. In\nan experimental evaluation, the agent significantly outperforms an existing\nsolution and a human expert.",
          "link": "http://arxiv.org/abs/2107.04457",
          "publishedOn": "2021-07-12T01:55:16.640Z",
          "wordCount": 544,
          "title": "Aligning an optical interferometer with beam divergence control and continuous action space. (arXiv:2107.04457v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_M/0/1/0/all/0/1\">Mohit Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lingyang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zi Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1\">Peter Cho-Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "Massive deployment of Graph Neural Networks (GNNs) in high-stake applications\ngenerates a strong demand for explanations that are robust to noise and align\nwell with human intuition. Most existing methods generate explanations by\nidentifying a subgraph of an input graph that has a strong correlation with the\nprediction. These explanations are not robust to noise because independently\noptimizing the correlation for a single input can easily overfit noise.\nMoreover, they do not align well with human intuition because removing an\nidentified subgraph from an input graph does not necessarily change the\nprediction result. In this paper, we propose a novel method to generate robust\ncounterfactual explanations on GNNs by explicitly modelling the common decision\nlogic of GNNs on similar input graphs. Our explanations are naturally robust to\nnoise because they are produced from the common decision boundaries of a GNN\nthat govern the predictions of many similar input graphs. The explanations also\nalign well with human intuition because removing the set of edges identified by\nan explanation from the input graph changes the prediction significantly.\nExhaustive experiments on many public datasets demonstrate the superior\nperformance of our method.",
          "link": "http://arxiv.org/abs/2107.04086",
          "publishedOn": "2021-07-12T01:55:16.615Z",
          "wordCount": 628,
          "title": "Robust Counterfactual Explanations on Graph Neural Networks. (arXiv:2107.04086v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1\">Fu-Shun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shang-Ran Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chang-Fu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chien-Wen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan-Ren Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Chieh Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chun-Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chung-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yen-Chun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tang-Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nian-Jhen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_W/0/1/0/all/0/1\">Wan-Ling Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Ching-Shiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Feipei Lai</a>",
          "description": "Previously, we established a lung sound database, HF_Lung_V2 and proposed\nconvolutional bidirectional gated recurrent unit (CNN-BiGRU) models with\nadequate ability for inhalation, exhalation, continuous adventitious sound\n(CAS), and discontinuous adventitious sound detection in the lung sound. In\nthis study, we proceeded to build a tracheal sound database, HF_Tracheal_V1,\ncontaining 11107 of 15-second tracheal sound recordings, 23087 inhalation\nlabels, 16728 exhalation labels, and 6874 CAS labels. The tracheal sound in\nHF_Tracheal_V1 and the lung sound in HF_Lung_V2 were either combined or used\nalone to train the CNN-BiGRU models for respective lung and tracheal sound\nanalysis. Different training strategies were investigated and compared: (1)\nusing full training (training from scratch) to train the lung sound models\nusing lung sound alone and train the tracheal sound models using tracheal sound\nalone, (2) using a mixed set that contains both the lung and tracheal sound to\ntrain the models, and (3) using domain adaptation that finetuned the\npre-trained lung sound models with the tracheal sound data and vice versa.\nResults showed that the models trained only by lung sound performed poorly in\nthe tracheal sound analysis and vice versa. However, the mixed set training and\ndomain adaptation can improve the performance of exhalation and CAS detection\nin the lung sound, and inhalation, exhalation, and CAS detection in the\ntracheal sound compared to positive controls (lung models trained only by lung\nsound and vice versa). Especially, a model derived from the mixed set training\nprevails in the situation of killing two birds with one stone.",
          "link": "http://arxiv.org/abs/2107.04229",
          "publishedOn": "2021-07-12T01:55:16.609Z",
          "wordCount": 744,
          "title": "Improved Breath Phase and Continuous Adventitious Sound Detection in Lung and Tracheal Sound Using Mixed Set Training and Domain Adaptation. (arXiv:2107.04229v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15138",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Herzberg_W/0/1/0/all/0/1\">William Herzberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rowe_D/0/1/0/all/0/1\">Daniel B. Rowe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hauptmann_A/0/1/0/all/0/1\">Andreas Hauptmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamilton_S/0/1/0/all/0/1\">Sarah J. Hamilton</a>",
          "description": "The majority of model-based learned image reconstruction methods in medical\nimaging have been limited to uniform domains, such as pixelated images. If the\nunderlying model is solved on nonuniform meshes, arising from a finite element\nmethod typical for nonlinear inverse problems, interpolation and embeddings are\nneeded. To overcome this, we present a flexible framework to extend model-based\nlearning directly to nonuniform meshes, by interpreting the mesh as a graph and\nformulating our network architectures using graph convolutional neural\nnetworks. This gives rise to the proposed iterative Graph Convolutional\nNewton-type Method (GCNM), which includes the forward model in the solution of\nthe inverse problem, while all updates are directly computed by the network on\nthe problem specific mesh. We present results for Electrical Impedance\nTomography, a severely ill-posed nonlinear inverse problem that is frequently\nsolved via optimization-based methods, where the forward problem is solved by\nfinite element methods. Results for absolute EIT imaging are compared to\nstandard iterative methods as well as a graph residual network. We show that\nthe GCNM has strong generalizability to different domain shapes and meshes, out\nof distribution data as well as experimental data, from purely simulated\ntraining data and without transfer training.",
          "link": "http://arxiv.org/abs/2103.15138",
          "publishedOn": "2021-07-12T01:55:16.601Z",
          "wordCount": 683,
          "title": "Graph Convolutional Networks for Model-Based Learning in Nonlinear Inverse Problems. (arXiv:2103.15138v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soen_A/0/1/0/all/0/1\">Alexander Soen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Ke Sun</a>",
          "description": "The Fisher information matrix (FIM) has been applied to the realm of deep\nlearning. It is closely related to the loss landscape, the variance of the\nparameters, second order optimization, and deep learning theory. The exact FIM\nis either unavailable in closed form or too expensive to compute. In practice,\nit is almost always estimated based on empirical samples. We investigate two\nsuch estimators based on two equivalent representations of the FIM. They are\nboth unbiased and consistent with respect to the underlying \"true\" FIM. Their\nestimation quality is characterized by their variance given in closed form. We\nbound their variances and analyze how the parametric structure of a deep neural\nnetwork can impact the variance. We discuss the meaning of this variance\nmeasure and our bounds in the context of deep learning.",
          "link": "http://arxiv.org/abs/2107.04205",
          "publishedOn": "2021-07-12T01:55:16.594Z",
          "wordCount": 568,
          "title": "On the Variance of the Fisher Information for Deep Learning. (arXiv:2107.04205v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1\">Erich Schubert</a>",
          "description": "Similarity search is a fundamental problem for many data analysis techniques.\nMany efficient search techniques rely on the triangle inequality of metrics,\nwhich allows pruning parts of the search space based on transitive bounds on\ndistances. Recently, Cosine similarity has become a popular alternative choice\nto the standard Euclidean metric, in particular in the context of textual data\nand neural network embeddings. Unfortunately, Cosine similarity is not metric\nand does not satisfy the standard triangle inequality. Instead, many search\ntechniques for Cosine rely on approximation techniques such as locality\nsensitive hashing. In this paper, we derive a triangle inequality for Cosine\nsimilarity that is suitable for efficient similarity search with many standard\nsearch structures (such as the VP-tree, Cover-tree, and M-tree); show that this\nbound is tight and discuss fast approximations for it. We hope that this spurs\nnew research on accelerating exact similarity search for cosine similarity, and\npossible other similarity measures beyond the existing work for distance\nmetrics.",
          "link": "http://arxiv.org/abs/2107.04071",
          "publishedOn": "2021-07-12T01:55:16.546Z",
          "wordCount": 583,
          "title": "A Triangle Inequality for Cosine Similarity. (arXiv:2107.04071v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1\">Ibrahim H. Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1\">Josiah P. Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosong_E/0/1/0/all/0/1\">Elliot Fosong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1\">Stefano V. Albrecht</a>",
          "description": "Current methods for authentication and key agreement based on public-key\ncryptography are vulnerable to quantum computing. We propose a novel approach\nbased on artificial intelligence research in which communicating parties are\nviewed as autonomous agents which interact repeatedly using their private\ndecision models. Authentication and key agreement are decided based on the\nagents' observed behaviors during the interaction. The security of this\napproach rests upon the difficulty of modeling the decisions of interacting\nagents from limited observations, a problem which we conjecture is also hard\nfor quantum computing. We release PyAMI, a prototype authentication and key\nagreement system based on the proposed method. We empirically validate our\nmethod for authenticating legitimate users while detecting different types of\nadversarial attacks. Finally, we show how reinforcement learning techniques can\nbe used to train server models which effectively probe a client's decisions to\nachieve more sample-efficient authentication.",
          "link": "http://arxiv.org/abs/2007.09327",
          "publishedOn": "2021-07-12T01:55:16.540Z",
          "wordCount": 639,
          "title": "Towards Quantum-Secure Authentication and Key Agreement via Abstract Multi-Agent Interaction. (arXiv:2007.09327v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Phuc_L/0/1/0/all/0/1\">Luu Huu Phuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1\">Koh Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1\">Seiji Okajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1\">Arseny Tolmachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1\">Tomoyoshi Takebayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1\">Koji Maruhashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>",
          "description": "Multi-relational graph is a ubiquitous and important data structure, allowing\nflexible representation of multiple types of interactions and relations between\nentities. Similar to other graph-structured data, link prediction is one of the\nmost important tasks on multi-relational graphs and is often used for knowledge\ncompletion. When related graphs coexist, it is of great benefit to build a\nlarger graph via integrating the smaller ones. The integration requires\npredicting hidden relational connections between entities belonged to different\ngraphs (inter-domain link prediction). However, this poses a real challenge to\nexisting methods that are exclusively designed for link prediction between\nentities of the same graph only (intra-domain link prediction). In this study,\nwe propose a new approach to tackle the inter-domain link prediction problem by\nsoftly aligning the entity distributions between different domains with optimal\ntransport and maximum mean discrepancy regularizers. Experiments on real-world\ndatasets show that optimal transport regularizer is beneficial and considerably\nimproves the performance of baseline methods.",
          "link": "http://arxiv.org/abs/2106.06171",
          "publishedOn": "2021-07-12T01:55:16.502Z",
          "wordCount": 627,
          "title": "Inter-domain Multi-relational Link Prediction. (arXiv:2106.06171v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mutian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingzhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1\">Frank K. Soong</a>",
          "description": "To scale neural speech synthesis to various real-world languages, we present\na multilingual end-to-end framework that maps byte inputs to spectrograms, thus\nallowing arbitrary input scripts. Besides strong results on 40+ languages, the\nframework demonstrates capabilities to adapt to new languages under extreme\nlow-resource and even few-shot scenarios of merely 40s transcribed recording,\nwithout the need of per-language resources like lexicon, extra corpus,\nauxiliary models, or linguistic expertise, thus ensuring scalability. While it\nretains satisfactory intelligibility and naturalness matching rich-resource\nmodels. Exhaustive comparative and ablation studies are performed to reveal the\npotential of the framework for low-resource languages. Furthermore, we propose\na novel method to extract language-specific sub-networks in a multilingual\nmodel for a better understanding of its mechanism.",
          "link": "http://arxiv.org/abs/2103.03541",
          "publishedOn": "2021-07-12T01:55:16.495Z",
          "wordCount": 596,
          "title": "Multilingual Byte2Speech Models for Scalable Low-resource Speech Synthesis. (arXiv:2103.03541v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balseiro_S/0/1/0/all/0/1\">Santiago Balseiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haihao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1\">Vahab Mirrokni</a>",
          "description": "Online allocation problems with resource constraints are central problems in\nrevenue management and online advertising. In these problems, requests arrive\nsequentially during a finite horizon and, for each request, a decision maker\nneeds to choose an action that consumes a certain amount of resources and\ngenerates reward. The objective is to maximize cumulative rewards subject to a\nconstraint on the total consumption of resources. In this paper, we consider a\ndata-driven setting in which the reward and resource consumption of each\nrequest are generated using an input model that is unknown to the decision\nmaker.\n\nWe design a general class of algorithms that attain good performance in\nvarious inputs models without knowing which type of input they are facing. In\nparticular, our algorithms are asymptotically optimal under stochastic i.i.d.\ninput model as well as various non-stationary stochastic input models, and they\nattain an asymptotically optimal fixed competitive ratio when the input is\nadversarial. Our algorithms operate in the Lagrangian dual space: they maintain\na dual multiplier for each resource that is updated using online mirror\ndescent. By choosing the reference function accordingly, we recover dual\nsub-gradient descent and dual exponential weights algorithm. The resulting\nalgorithms are simple, fast, and have minimal requirements on the reward\nfunctions, consumption functions and the action space, in contrast to existing\nmethods for online allocation problems. We discuss applications to network\nrevenue management, online bidding in repeated auctions with budget\nconstraints, online proportional matching with high entropy, and personalized\nassortment optimization with limited inventories.",
          "link": "http://arxiv.org/abs/2011.10124",
          "publishedOn": "2021-07-12T01:55:16.471Z",
          "wordCount": 737,
          "title": "The Best of Many Worlds: Dual Mirror Descent for Online Allocation Problems. (arXiv:2011.10124v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.00909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lingfei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>",
          "description": "Transfer learning (TL) tries to utilize data or knowledge from one or more\nsource domains to facilitate the learning in a target domain. It is\nparticularly useful when the target domain has few or no labeled data, due to\nannotation expense, privacy concerns, etc. Unfortunately, the effectiveness of\nTL is not always guaranteed. Negative transfer (NT), i.e., the source domain\ndata/knowledge cause reduced learning performance in the target domain, has\nbeen a long-standing and challenging problem in TL. Various approaches to\nhandle NT have been proposed in the literature. However, this filed lacks a\nsystematic survey on the formalization of NT, their factors and the algorithms\nthat handle NT. This paper proposes to fill this gap. First, the definition of\nnegative transfer is considered and a taxonomy of the factors are discussed.\nThen, near fifty representative approaches for handling NT are categorized and\nreviewed, from four perspectives: secure transfer, domain similarity\nestimation, distant transfer and negative transfer mitigation. NT in related\nfields, e.g., multi-task learning, lifelong learning, and adversarial attacks\nare also discussed.",
          "link": "http://arxiv.org/abs/2009.00909",
          "publishedOn": "2021-07-12T01:55:16.463Z",
          "wordCount": 645,
          "title": "A Survey on Negative Transfer. (arXiv:2009.00909v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiem_S/0/1/0/all/0/1\">Salah Zaiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Marques_J/0/1/0/all/0/1\">Javier Fernandez-Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusmao_P/0/1/0/all/0/1\">Pedro P. B. de Gusmao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beutel_D/0/1/0/all/0/1\">Daniel J. Beutel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>",
          "description": "Training Automatic Speech Recognition (ASR) models under federated learning\n(FL) settings has attracted a lot of attention recently. However, the FL\nscenarios often presented in the literature are artificial and fail to capture\nthe complexity of real FL systems. In this paper, we construct a challenging\nand realistic ASR federated experimental setup consisting of clients with\nheterogeneous data distributions using the French and Italian sets of the\nCommonVoice dataset, a large heterogeneous dataset containing thousands of\ndifferent speakers, acoustic environments and noises. We present the first\nempirical study on attention-based sequence-to-sequence End-to-End (E2E) ASR\nmodel with three aggregation weighting strategies -- standard FedAvg,\nloss-based aggregation and a novel word error rate (WER)-based aggregation,\ncompared in two realistic FL scenarios: cross-silo with 10 clients and\ncross-device with 2K and 4K clients. Our analysis on E2E ASR from heterogeneous\nand realistic federated acoustic models provides the foundations for future\nresearch and development of realistic FL-based ASR applications.",
          "link": "http://arxiv.org/abs/2104.14297",
          "publishedOn": "2021-07-12T01:55:16.456Z",
          "wordCount": 636,
          "title": "End-to-End Speech Recognition from Federated Acoustic Models. (arXiv:2104.14297v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shajari_H/0/1/0/all/0/1\">Hoda Shajari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaemoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1\">Sanjay Ranka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1\">Anand Rangarajan</a>",
          "description": "Two-dimensional array-based datasets are pervasive in a variety of domains.\nCurrent approaches for generative modeling have typically been limited to\nconventional image datasets and performed in the pixel domain which do not\nexplicitly capture the correlation between pixels. Additionally, these\napproaches do not extend to scientific and other applications where each\nelement value is continuous and is not limited to a fixed range. In this paper,\nwe propose a novel approach for generating two-dimensional datasets by moving\nthe computations to the space of representation bases and show its usefulness\nfor two different datasets, one from imaging and another from scientific\ncomputing. The proposed approach is general and can be applied to any dataset,\nrepresentation basis, or generative model. We provide a comprehensive\nperformance comparison of various combinations of generative models and\nrepresentation basis spaces. We also propose a new evaluation metric which\ncaptures the deficiency of generating images in pixel space.",
          "link": "http://arxiv.org/abs/2106.00203",
          "publishedOn": "2021-07-12T01:55:16.439Z",
          "wordCount": 623,
          "title": "Hybrid Generative Models for Two-Dimensional Datasets. (arXiv:2106.00203v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1\">Cl&#xe9;ment Rebuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberti_M/0/1/0/all/0/1\">Marco Roberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1\">Laure Soulier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1\">Rossella Cancelliere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "Data-to-Text Generation (DTG) is a subfield of Natural Language Generation\naiming at transcribing structured data in natural language descriptions. The\nfield has been recently boosted by the use of neural-based generators which\nexhibit on one side great syntactic skills without the need of hand-crafted\npipelines; on the other side, the quality of the generated text reflects the\nquality of the training data, which in realistic settings only offer\nimperfectly aligned structure-text pairs. Consequently, state-of-art neural\nmodels include misleading statements - usually called hallucinations - in their\noutputs. The control of this phenomenon is today a major challenge for DTG, and\nis the problem addressed in the paper.\n\nPrevious work deal with this issue at the instance level: using an alignment\nscore for each table-reference pair. In contrast, we propose a finer-grained\napproach, arguing that hallucinations should rather be treated at the word\nlevel. Specifically, we propose a Multi-Branch Decoder which is able to\nleverage word-level labels to learn the relevant parts of each training\ninstance. These labels are obtained following a simple and efficient scoring\nprocedure based on co-occurrence analysis and dependency parsing. Extensive\nevaluations, via automated metrics and human judgment on the standard WikiBio\nbenchmark, show the accuracy of our alignment labels and the effectiveness of\nthe proposed Multi-Branch Decoder. Our model is able to reduce and control\nhallucinations, while keeping fluency and coherence in generated texts. Further\nexperiments on a degraded version of ToTTo show that our model could be\nsuccessfully used on very noisy settings.",
          "link": "http://arxiv.org/abs/2102.02810",
          "publishedOn": "2021-07-12T01:55:16.410Z",
          "wordCount": 752,
          "title": "Controlling Hallucinations at Word Level in Data-to-Text Generation. (arXiv:2102.02810v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13747",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Felser_T/0/1/0/all/0/1\">Timo Felser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Trenti_M/0/1/0/all/0/1\">Marco Trenti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sestini_L/0/1/0/all/0/1\">Lorenzo Sestini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gianelle_A/0/1/0/all/0/1\">Alessio Gianelle</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zuliani_D/0/1/0/all/0/1\">Davide Zuliani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lucchesi_D/0/1/0/all/0/1\">Donatella Lucchesi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Montangero_S/0/1/0/all/0/1\">Simone Montangero</a>",
          "description": "Tensor Networks, a numerical tool originally designed for simulating quantum\nmany-body systems, have recently been applied to solve Machine Learning\nproblems. Exploiting a tree tensor network, we apply a quantum-inspired machine\nlearning technique to a very important and challenging big data problem in high\nenergy physics: the analysis and classification of data produced by the Large\nHadron Collider at CERN. In particular, we present how to effectively classify\nso-called b-jets, jets originating from b-quarks from proton-proton collisions\nin the LHCb experiment, and how to interpret the classification results. We\nexploit the Tensor Network approach to select important features and adapt the\nnetwork geometry based on information acquired in the learning process.\nFinally, we show how to adapt the tree tensor network to achieve optimal\nprecision or fast response in time without the need of repeating the learning\nprocess. These results pave the way to the implementation of high-frequency\nreal-time applications, a key ingredient needed among others for current and\nfuture LHCb event classification able to trigger events at the tens of MHz\nscale.",
          "link": "http://arxiv.org/abs/2004.13747",
          "publishedOn": "2021-07-12T01:55:16.402Z",
          "wordCount": 659,
          "title": "Quantum-inspired Machine Learning on high-energy physics data. (arXiv:2004.13747v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weisz_G/0/1/0/all/0/1\">Gell&#xe9;rt Weisz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amortila_P/0/1/0/all/0/1\">Philip Amortila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janzer_B/0/1/0/all/0/1\">Barnab&#xe1;s Janzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1\">Yasin Abbasi-Yadkori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>",
          "description": "We consider local planning in fixed-horizon MDPs with a generative model\nunder the assumption that the optimal value function lies close to the span of\na feature map. The generative model provides a local access to the MDP: The\nplanner can ask for random transitions from previously returned states and\narbitrary actions, and features are only accessible for states that are\nencountered in this process. As opposed to previous work (e.g. Lattimore et al.\n(2020)) where linear realizability of all policies was assumed, we consider the\nsignificantly relaxed assumption of a single linearly realizable\n(deterministic) policy. A recent lower bound by Weisz et al. (2020) established\nthat the related problem when the action-value function of the optimal policy\nis linearly realizable requires an exponential number of queries, either in $H$\n(the horizon of the MDP) or $d$ (the dimension of the feature mapping). Their\nconstruction crucially relies on having an exponentially large action set. In\ncontrast, in this work, we establish that poly$(H,d)$ planning is possible with\nstate value function realizability whenever the action set has a constant size.\nIn particular, we present the TensorPlan algorithm which uses\npoly$((dH/\\delta)^A)$ simulator queries to find a $\\delta$-optimal policy\nrelative to any deterministic policy for which the value function is linearly\nrealizable with some bounded parameter. This is the first algorithm to give a\npolynomial query complexity guarantee using only linear-realizability of a\nsingle competing value function. Whether the computation cost is similarly\nbounded remains an open question. We extend the upper bound to the\nnear-realizable case and to the infinite-horizon discounted setup. We also\npresent a lower bound in the infinite-horizon episodic setting: Planners that\nachieve constant suboptimality need exponentially many queries, either in $d$\nor the number of actions.",
          "link": "http://arxiv.org/abs/2102.02049",
          "publishedOn": "2021-07-12T01:55:16.396Z",
          "wordCount": 781,
          "title": "On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function. (arXiv:2102.02049v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03635",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Denuit_M/0/1/0/all/0/1\">Michel Denuit</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Charpentier_A/0/1/0/all/0/1\">Arthur Charpentier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Trufin_J/0/1/0/all/0/1\">Julien Trufin</a>",
          "description": "Boosting techniques and neural networks are particularly effective machine\nlearning methods for insurance pricing. Often in practice, there are\nnevertheless endless debates about the choice of the right loss function to be\nused to train the machine learning model, as well as about the appropriate\nmetric to assess the performances of competing models. Also, the sum of fitted\nvalues can depart from the observed totals to a large extent and this often\nconfuses actuarial analysts. The lack of balance inherent to training models by\nminimizing deviance outside the familiar GLM with canonical link setting has\nbeen empirically documented in W\\\"uthrich (2019, 2020) who attributes it to the\nearly stopping rule in gradient descent methods for model fitting. The present\npaper aims to further study this phenomenon when learning proceeds by\nminimizing Tweedie deviance. It is shown that minimizing deviance involves a\ntrade-off between the integral of weighted differences of lower partial moments\nand the bias measured on a specific scale. Autocalibration is then proposed as\na remedy. This new method to correct for bias adds an extra local GLM step to\nthe analysis. Theoretically, it is shown that it implements the autocalibration\nconcept in pure premium calculation and ensures that balance also holds on a\nlocal scale, not only at portfolio level as with existing bias-correction\ntechniques. The convex order appears to be the natural tool to compare\ncompeting models, putting a new light on the diagnostic graphs and associated\nmetrics proposed by Denuit et al. (2019).",
          "link": "http://arxiv.org/abs/2103.03635",
          "publishedOn": "2021-07-12T01:55:16.371Z",
          "wordCount": 701,
          "title": "Autocalibration and Tweedie-dominance for Insurance Pricing with Machine Learning. (arXiv:2103.03635v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.11385",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "We propose a new embedding method, named Quantile-Quantile Embedding (QQE),\nfor distribution transformation and manifold embedding with the ability to\nchoose the embedding distribution. QQE, which uses the concept of\nquantile-quantile plot from visual statistical tests, can transform the\ndistribution of data to any theoretical desired distribution or empirical\nreference sample. Moreover, QQE gives the user a choice of embedding\ndistribution in embedding the manifold of data into the low dimensional\nembedding space. It can also be used for modifying the embedding distribution\nof other dimensionality reduction methods, such as PCA, t-SNE, and deep metric\nlearning, for better representation or visualization of data. We propose QQE in\nboth unsupervised and supervised forms. QQE can also transform a distribution\nto either an exact reference distribution or its shape. We show that QQE allows\nfor better discrimination of classes in some cases. Our experiments on\ndifferent synthetic and image datasets show the effectiveness of the proposed\nembedding method.",
          "link": "http://arxiv.org/abs/2006.11385",
          "publishedOn": "2021-07-12T01:55:16.356Z",
          "wordCount": 656,
          "title": "Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution. (arXiv:2006.11385v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovskikh_A/0/1/0/all/0/1\">Anastasia Sosnovskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seong Jae Hwang</a>",
          "description": "Application of deep neural networks to medical imaging tasks has in some\nsense become commonplace. Still, a \"thorn in the side\" of the deep learning\nmovement is the argument that deep networks are prone to overfitting and are\nthus unable to generalize well when datasets are small (as is common in medical\nimaging tasks). One way to bolster confidence is to provide mathematical\nguarantees, or bounds, on network performance after training which explicitly\nquantify the possibility of overfitting. In this work, we explore recent\nadvances using the PAC-Bayesian framework to provide bounds on generalization\nerror for large (stochastic) networks. While previous efforts focus on\nclassification in larger natural image datasets (e.g., MNIST and CIFAR-10), we\napply these techniques to both classification and segmentation in a smaller\nmedical imagining dataset: the ISIC 2018 challenge set. We observe the\nresultant bounds are competitive compared to a simpler baseline, while also\nbeing more explainable and alleviating the need for holdout sets.",
          "link": "http://arxiv.org/abs/2104.05600",
          "publishedOn": "2021-07-12T01:55:16.349Z",
          "wordCount": 637,
          "title": "PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in Medical Imaging. (arXiv:2104.05600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.09925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Genc_H/0/1/0/all/0/1\">Hasan Genc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seah Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amid_A/0/1/0/all/0/1\">Alon Amid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haj_Ali_A/0/1/0/all/0/1\">Ameer Haj-Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1\">Vighnesh Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_P/0/1/0/all/0/1\">Pranav Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jerry Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubb_D/0/1/0/all/0/1\">Daniel Grubb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_H/0/1/0/all/0/1\">Harrison Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Howard Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_A/0/1/0/all/0/1\">Albert Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1\">Colin Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steffl_S/0/1/0/all/0/1\">Samuel Steffl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">John Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragan_Kelley_J/0/1/0/all/0/1\">Jonathan Ragan-Kelley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asanovic_K/0/1/0/all/0/1\">Krste Asanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolic_B/0/1/0/all/0/1\">Borivoje Nikolic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yakun Sophia Shao</a>",
          "description": "DNN accelerators are often developed and evaluated in isolation without\nconsidering the cross-stack, system-level effects in real-world environments.\nThis makes it difficult to appreciate the impact of System-on-Chip (SoC)\nresource contention, OS overheads, and programming-stack inefficiencies on\noverall performance/energy-efficiency. To address this challenge, we present\nGemmini, an open-source*, full-stack DNN accelerator generator. Gemmini\ngenerates a wide design-space of efficient ASIC accelerators from a flexible\narchitectural template, together with flexible programming stacks and full SoCs\nwith shared resources that capture system-level effects. Gemmini-generated\naccelerators have also been fabricated, delivering up to three\norders-of-magnitude speedups over high-performance CPUs on various DNN\nbenchmarks.\n\n* https://github.com/ucb-bar/gemmini",
          "link": "http://arxiv.org/abs/1911.09925",
          "publishedOn": "2021-07-12T01:55:16.341Z",
          "wordCount": 641,
          "title": "Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration. (arXiv:1911.09925v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04568",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Carmona_R/0/1/0/all/0/1\">Ren&#xe9; Carmona</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lauriere_M/0/1/0/all/0/1\">Mathieu Lauri&#xe8;re</a>",
          "description": "Financial markets and more generally macro-economic models involve a large\nnumber of individuals interacting through variables such as prices resulting\nfrom the aggregate behavior of all the agents. Mean field games have been\nintroduced to study Nash equilibria for such problems in the limit when the\nnumber of players is infinite. The theory has been extensively developed in the\npast decade, using both analytical and probabilistic tools, and a wide range of\napplications have been discovered, from economics to crowd motion. More\nrecently the interaction with machine learning has attracted a growing\ninterest. This aspect is particularly relevant to solve very large games with\ncomplex structures, in high dimension or with common sources of randomness. In\nthis chapter, we review the literature on the interplay between mean field\ngames and deep learning, with a focus on three families of methods. A special\nemphasis is given to financial applications.",
          "link": "http://arxiv.org/abs/2107.04568",
          "publishedOn": "2021-07-12T01:55:16.323Z",
          "wordCount": 597,
          "title": "Deep Learning for Mean Field Games and Mean Field Control with Applications to Finance. (arXiv:2107.04568v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1905.11425",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiwei Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Doan_T/0/1/0/all/0/1\">Thinh T. Doan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Clarke_J/0/1/0/all/0/1\">John-Paul Clarke</a>, <a href=\"http://arxiv.org/find/math/1/au:+Maguluri_S/0/1/0/all/0/1\">Siva Theja Maguluri</a>",
          "description": "Motivated by applications in reinforcement learning (RL), we study a\nnonlinear stochastic approximation (SA) algorithm under Markovian noise, and\nestablish its finite-sample convergence bounds under various stepsizes.\nSpecifically, we show that when using constant stepsize (i.e.,\n$\\epsilon_k\\equiv \\epsilon$), the algorithm achieves exponential fast\nconvergence with asymptotic accuracy $\\mathcal{O}(\\epsilon\\log(1/\\epsilon))$.\nWhen using diminishing stepsizes with appropriate decay rate, the algorithm\nconverges with rate $\\mathcal{O}(\\log(k)/k)$. Our proof is based on the\nLyapunov drift arguments, and to handle the Markovian noise, we exploit the\nfast mixing of the underlying Markov chain. To demonstrate the generality of\nour theoretical results on Markovian SA, we use it to derive the finite-sample\nbounds of the popular $Q$-learning with linear function approximation\nalgorithm, under a condition on the behavior policy. Importantly, we do not\nneed to make the unrealistic assumption that the samples are i.i.d., and do not\nrequire an additional projection step in the algorithm to maintain the\nboundedness of the iterates. Numerical simulations corroborate our theoretical\nfindings.",
          "link": "http://arxiv.org/abs/1905.11425",
          "publishedOn": "2021-07-12T01:55:16.309Z",
          "wordCount": 659,
          "title": "Finite-Sample Analysis of Nonlinear Stochastic Approximation with Applications in Reinforcement Learning. (arXiv:1905.11425v6 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meirom_E/0/1/0/all/0/1\">Eli A. Meirom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>",
          "description": "We consider the problem of controlling a partially-observed dynamic process\non a graph by a limited number of interventions. This problem naturally arises\nin contexts such as scheduling virus tests to curb an epidemic; targeted\nmarketing in order to promote a product; and manually inspecting posts to\ndetect fake news spreading on social networks.\n\nWe formulate this setup as a sequential decision problem over a temporal\ngraph process. In face of an exponential state space, combinatorial action\nspace and partial observability, we design a novel tractable scheme to control\ndynamical processes on temporal graphs. We successfully apply our approach to\ntwo popular problems that fall into our framework: prioritizing which nodes\nshould be tested in order to curb the spread of an epidemic, and influence\nmaximization on a graph.",
          "link": "http://arxiv.org/abs/2010.05313",
          "publishedOn": "2021-07-12T01:55:16.299Z",
          "wordCount": 613,
          "title": "Controlling Graph Dynamics with Reinforcement Learning and Graph Neural Networks. (arXiv:2010.05313v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Schonlieb</a>",
          "description": "Alzheimer's disease (AD) is the most common age-related dementia. It remains\na challenge to identify the individuals at risk of dementia for precise\nmanagement. Brain MRI offers a noninvasive biomarker to detect brain aging.\nPrevious evidence shows that the brain structural change detected by diffusion\nMRI is associated with dementia. Mounting studies has conceptualised the brain\nas a complex network, which has shown the utility of this approach in\ncharacterising various neurological and psychiatric disorders. Therefore, the\nstructural connectivity shows promise in dementia classification. The proposed\nBrainNetGAN is a generative adversarial network variant to augment the brain\nstructural connectivity matrices for binary dementia classification tasks.\nStructural connectivity matrices between separated brain regions are\nconstructed using tractography on diffusion MRI data. The BrainNetGAN model is\ntrained to generate fake brain connectivity matrices, which are expected to\nreflect latent distribution of the real brain network data. Finally, a\nconvolutional neural network classifier is proposed for binary dementia\nclassification. Numerical results show that the binary classification\nperformance in the testing set was improved using the BrainNetGAN augmented\ndataset. The proposed methodology allows quick synthesis of an arbitrary number\nof augmented connectivity matrices and can be easily transferred to similar\nclassification tasks.",
          "link": "http://arxiv.org/abs/2103.08494",
          "publishedOn": "2021-07-12T01:55:16.291Z",
          "wordCount": 675,
          "title": "BrainNetGAN: Data augmentation of brain connectivity using generative adversarial network for dementia classification. (arXiv:2103.08494v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valerio_L/0/1/0/all/0/1\">Lorenzo Valerio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passarella_A/0/1/0/all/0/1\">Andrea Passarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Marco Conti</a>",
          "description": "The exponential growth of devices and data at the edges of the Internet is\nrising scalability and privacy concerns on approaches based exclusively on\nremote cloud platforms. Data gravity, a fundamental concept in Fog Computing,\npoints towards decentralisation of computation for data analysis, as a viable\nalternative to address those concerns. Decentralising AI tasks on several\ncooperative devices means identifying the optimal set of locations or\nCollection Points (CP for short) to use, in the continuum between full\ncentralisation (i.e., all data on a single device) and full decentralisation\n(i.e., data on source locations). We propose an analytical framework able to\nfind the optimal operating point in this continuum, linking the accuracy of the\nlearning task with the corresponding network and computational cost for moving\ndata and running the distributed training at the CPs. We show through\nsimulations that the model accurately predicts the optimal trade-off, quite\noften an intermediate point between full centralisation and full\ndecentralisation, showing also a significant cost saving w.r.t. both of them.\nFinally, the analytical model admits closed-form or numeric solutions, making\nit not only a performance evaluation instrument but also a design tool to\nconfigure a given distributed learning task optimally before its deployment.",
          "link": "http://arxiv.org/abs/2012.05266",
          "publishedOn": "2021-07-12T01:55:16.284Z",
          "wordCount": 683,
          "title": "Optimising cost vs accuracy of decentralised analytics in fog computing environments. (arXiv:2012.05266v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07143",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Brofos_J/0/1/0/all/0/1\">James A. Brofos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lederman_R/0/1/0/all/0/1\">Roy R. Lederman</a>",
          "description": "Density estimation is an important technique for characterizing distributions\ngiven observations. Much existing research on density estimation has focused on\ncases wherein the data lies in a Euclidean space. However, some kinds of data\nare not well-modeled by supposing that their underlying geometry is Euclidean.\nInstead, it can be useful to model such data as lying on a {\\it manifold} with\nsome known structure. For instance, some kinds of data may be known to lie on\nthe surface of a sphere. We study the problem of estimating densities on\nmanifolds. We propose a method, inspired by the literature on \"dequantization,\"\nwhich we interpret through the lens of a coordinate transformation of an\nambient Euclidean space and a smooth manifold of interest. Using methods from\nnormalizing flows, we apply this method to the dequantization of smooth\nmanifold structures in order to model densities on the sphere, tori, and the\northogonal group.",
          "link": "http://arxiv.org/abs/2102.07143",
          "publishedOn": "2021-07-12T01:55:16.278Z",
          "wordCount": 603,
          "title": "Manifold Density Estimation via Generalized Dequantization. (arXiv:2102.07143v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pliakos_K/0/1/0/all/0/1\">Konstantinos Pliakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vens_C/0/1/0/all/0/1\">Celine Vens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>",
          "description": "Predicting drug-target interactions (DTI) via reliable computational methods\nis an effective and efficient way to mitigate the enormous costs and time of\nthe drug discovery process. Structure-based drug similarities and\nsequence-based target protein similarities are the commonly used information\nfor DTI prediction. Among numerous computational methods, neighborhood-based\nchemogenomic approaches that leverage drug and target similarities to perform\npredictions directly are simple but promising ones. However, existing\nsimilarity-based methods need to be re-trained to predict interactions for any\nnew drugs or targets and cannot directly perform predictions for both new\ndrugs, new targets, and new drug-target pairs. Furthermore, a large amount of\nmissing (undetected) interactions in current DTI datasets hinders most DTI\nprediction methods. To address these issues, we propose a new method denoted as\nWeighted k-Nearest Neighbor with Interaction Recovery (WkNNIR). Not only can\nWkNNIR estimate interactions of any new drugs and/or new targets without any\nneed of re-training, but it can also recover missing interactions (false\nnegatives). In addition, WkNNIR exploits local imbalance to promote the\ninfluence of more reliable similarities on the interaction recovery and\nprediction processes. We also propose a series of ensemble methods that employ\ndiverse sampling strategies and could be coupled with WkNNIR as well as any\nother DTI prediction method to improve performance. Experimental results over\nfive benchmark datasets demonstrate the effectiveness of our approaches in\npredicting drug-target interactions. Lastly, we confirm the practical\nprediction ability of proposed methods to discover reliable interactions that\nwere not reported in the original benchmark datasets.",
          "link": "http://arxiv.org/abs/2012.12325",
          "publishedOn": "2021-07-12T01:55:16.261Z",
          "wordCount": 729,
          "title": "Drug-Target Interaction Prediction via an Ensemble of Weighted Nearest Neighbors with Interaction Recovery. (arXiv:2012.12325v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kwonjoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>",
          "description": "Recently, Vision Transformers (ViTs) have shown competitive performance on\nimage recognition while requiring less vision-specific inductive biases. In\nthis paper, we investigate if such observation can be extended to image\ngeneration. To this end, we integrate the ViT architecture into generative\nadversarial networks (GANs). We observe that existing regularization methods\nfor GANs interact poorly with self-attention, causing serious instability\nduring training. To resolve this issue, we introduce novel regularization\ntechniques for training GANs with ViTs. Empirically, our approach, named\nViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2\non CIFAR-10, CelebA, and LSUN bedroom datasets.",
          "link": "http://arxiv.org/abs/2107.04589",
          "publishedOn": "2021-07-12T01:55:16.254Z",
          "wordCount": 541,
          "title": "ViTGAN: Training GANs with Vision Transformers. (arXiv:2107.04589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04562",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rue_H/0/1/0/all/0/1\">H&#xe5;vard Rue</a>",
          "description": "We show that many machine-learning algorithms are specific instances of a\nsingle algorithm called the Bayesian learning rule. The rule, derived from\nBayesian principles, yields a wide-range of algorithms from fields such as\noptimization, deep learning, and graphical models. This includes classical\nalgorithms such as ridge regression, Newton's method, and Kalman filter, as\nwell as modern deep-learning algorithms such as stochastic-gradient descent,\nRMSprop, and Dropout. The key idea in deriving such algorithms is to\napproximate the posterior using candidate distributions estimated by using\nnatural gradients. Different candidate distributions result in different\nalgorithms and further approximations to natural gradients give rise to\nvariants of those algorithms. Our work not only unifies, generalizes, and\nimproves existing algorithms, but also helps us design new ones.",
          "link": "http://arxiv.org/abs/2107.04562",
          "publishedOn": "2021-07-12T01:55:16.247Z",
          "wordCount": 545,
          "title": "The Bayesian Learning Rule. (arXiv:2107.04562v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Scott Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunk_C/0/1/0/all/0/1\">Cliff Brunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyu-Young Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Justin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_G/0/1/0/all/0/1\">Gagan Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1\">Sidharth Mudgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varano_C/0/1/0/all/0/1\">Chris Varano</a>",
          "description": "One of the challenges in a task oriented natural language application like\nthe Google Assistant, Siri, or Alexa is to localize the output to many\nlanguages. This paper explores doing this by applying machine translation to\nthe English output. Using machine translation is very scalable, as it can work\nwith any English output and can handle dynamic text, but otherwise the problem\nis a poor fit. The required quality bar is close to perfection, the range of\nsentences is extremely narrow, and the sentences are often very different than\nthe ones in the machine translation training data. This combination of\nrequirements is novel in the field of domain adaptation for machine\ntranslation. We are able to reach the required quality bar by building on\nexisting ideas and adding new ones: finetuning on in-domain translations,\nadding sentences from the Web, adding semantic annotations, and using automatic\nerror detection. The paper shares our approach and results, together with a\ndistillation model to serve the translation models at scale.",
          "link": "http://arxiv.org/abs/2107.04512",
          "publishedOn": "2021-07-12T01:55:16.230Z",
          "wordCount": 619,
          "title": "Using Machine Translation to Localize Task Oriented NLG Output. (arXiv:2107.04512v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Graphically-rich applications such as games are ubiquitous with attractive\nvisual effects of Graphical User Interface (GUI) that offers a bridge between\nsoftware applications and end-users. However, various types of graphical\nglitches may arise from such GUI complexity and have become one of the main\ncomponent of software compatibility issues. Our study on bug reports from game\ndevelopment teams in NetEase Inc. indicates that graphical glitches frequently\noccur during the GUI rendering and severely degrade the quality of\ngraphically-rich applications such as video games. Existing automated testing\ntechniques for such applications focus mainly on generating various GUI test\nsequences and check whether the test sequences can cause crashes. These\ntechniques require constant human attention to captures non-crashing bugs such\nas bugs causing graphical glitches. In this paper, we present the first step in\nautomating the test oracle for detecting non-crashing bugs in graphically-rich\napplications. Specifically, we propose \\texttt{GLIB} based on a code-based data\naugmentation technique to detect game GUI glitches. We perform an evaluation of\n\\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the\nresult shows that \\texttt{GLIB} can achieve 100\\% precision and 99.5\\% recall\nin detecting non-crashing bugs such as game GUI glitches. Practical application\nof \\texttt{GLIB} on another 14 real-world games (without bug reports) further\ndemonstrates that \\texttt{GLIB} can effectively uncover GUI glitches, with 48\nof 53 bugs reported by \\texttt{GLIB} having been confirmed and fixed so far.",
          "link": "http://arxiv.org/abs/2106.10507",
          "publishedOn": "2021-07-12T01:55:16.222Z",
          "wordCount": 718,
          "title": "GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pravilov_M/0/1/0/all/0/1\">Mikhail Pravilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1\">Egor Bogomolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golubev_Y/0/1/0/all/0/1\">Yaroslav Golubev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryksin_T/0/1/0/all/0/1\">Timofey Bryksin</a>",
          "description": "Applying machine learning to tasks that operate with code changes requires\ntheir numerical representation. In this work, we propose an approach for\nobtaining such representations during pre-training and evaluate them on two\ndifferent downstream tasks - applying changes to code and commit message\ngeneration. During pre-training, the model learns to apply the given code\nchange in a correct way. This task requires only code changes themselves, which\nmakes it unsupervised. In the task of applying code changes, our model\noutperforms baseline models by 5.9 percentage points in accuracy. As for the\ncommit message generation, our model demonstrated the same results as\nsupervised models trained for this specific task, which indicates that it can\nencode code changes well and can be improved in the future by pre-training on a\nlarger dataset of easily gathered code changes.",
          "link": "http://arxiv.org/abs/2106.02087",
          "publishedOn": "2021-07-12T01:55:16.215Z",
          "wordCount": 598,
          "title": "Unsupervised Learning of General-Purpose Embeddings for Code Changes. (arXiv:2106.02087v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1\">Raunak Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi Hong</a>",
          "description": "We introduce a neural network framework, utilizing adversarial learning to\npartition an image into two cuts, with one cut falling into a reference\ndistribution provided by the user. This concept tackles the task of\nunsupervised anomaly segmentation, which has attracted increasing attention in\nrecent years due to their broad applications in tasks with unlabelled data.\nThis Adversarial-based Selective Cutting network (ASC-Net) bridges the two\ndomains of cluster-based deep learning methods and adversarial-based\nanomaly/novelty detection algorithms. We evaluate this unsupervised learning\nmodel on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and\nMS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN\nfamily, our model demonstrates tremendous performance gains in unsupervised\nanomaly segmentation tasks. Although there is still room to further improve\nperformance compared to supervised learning algorithms, the promising\nexperimental results shed light on building an unsupervised learning algorithm\nusing user-defined knowledge.",
          "link": "http://arxiv.org/abs/2103.03664",
          "publishedOn": "2021-07-12T01:55:16.208Z",
          "wordCount": 619,
          "title": "ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation. (arXiv:2103.03664v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_Z/0/1/0/all/0/1\">Zeeshan Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbani_S/0/1/0/all/0/1\">Suha Rabbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Rehman Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishaque_S/0/1/0/all/0/1\">Syem Ishaque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1\">Sridhar Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naimul Khan</a>",
          "description": "ECG is an attractive option to assess stress in serious Virtual Reality (VR)\napplications due to its non-invasive nature. However, the existing Machine\nLearning (ML) models perform poorly. Moreover, existing studies only perform a\nbinary stress assessment, while to develop a more engaging biofeedback-based\napplication, multi-level assessment is necessary. Existing studies annotate and\nclassify a single experience (e.g. watching a VR video) to a single stress\nlevel, which again prevents design of dynamic experiences where real-time\nin-game stress assessment can be utilized. In this paper, we report our\nfindings on a new study on VR stress assessment, where three stress levels are\nassessed. ECG data was collected from 9 users experiencing a VR roller coaster.\nThe VR experience was then manually labeled in 10-seconds segments to three\nstress levels by three raters. We then propose a novel multimodal deep fusion\nmodel utilizing spectrogram and 1D ECG that can provide a stress prediction\nfrom just a 1-second window. Experimental results demonstrate that the proposed\nmodel outperforms the classical HRV-based ML models (9% increase in accuracy)\nand baseline deep learning models (2.5% increase in accuracy). We also report\nresults on the benchmark WESAD dataset to show the supremacy of the model.",
          "link": "http://arxiv.org/abs/2107.04566",
          "publishedOn": "2021-07-12T01:55:16.196Z",
          "wordCount": 655,
          "title": "Multi-level Stress Assessment from ECG in a Virtual Reality Environment using Multimodal Fusion. (arXiv:2107.04566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ran Liu</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Greenstein_J/0/1/0/all/0/1\">Joseph L. Greenstein</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Fackler_J/0/1/0/all/0/1\">James C. Fackler</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Bergmann_J/0/1/0/all/0/1\">Jules Bergmann</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Bembea_M/0/1/0/all/0/1\">Melania M. Bembea</a> (3 and 4), <a href=\"http://arxiv.org/find/cs/1/au:+Winslow_R/0/1/0/all/0/1\">Raimond L. Winslow</a> (1 and 2) ((1) Institute for Computational Medicine, the Johns Hopkins University, (2) Department of Biomedical Engineering, the Johns Hopkins University School of Medicine and Whiting School of Engineering, (3) Department of Anesthesiology and Critical Care Medicine, the Johns Hopkins University, (4) Department of Pediatrics, the Johns Hopkins University School of Medicine)",
          "description": "Guideline-based treatment for sepsis and septic shock is difficult because\nsepsis is a disparate range of life-threatening organ dysfunctions whose\npathophysiology is not fully understood. Early intervention in sepsis is\ncrucial for patient outcome, yet those interventions have adverse effects and\nare frequently overadministered. Greater personalization is necessary, as no\nsingle action is suitable for all patients. We present a novel application of\nreinforcement learning in which we identify optimal recommendations for sepsis\ntreatment from data, estimate their confidence level, and identify treatment\noptions infrequently observed in training data. Rather than a single\nrecommendation, our method can present several treatment options. We examine\nlearned policies and discover that reinforcement learning is biased against\naggressive intervention due to the confounding relationship between mortality\nand level of treatment received. We mitigate this bias using subspace learning,\nand develop methodology that can yield more accurate learning policies across\nhealthcare applications.",
          "link": "http://arxiv.org/abs/2107.04491",
          "publishedOn": "2021-07-12T01:55:16.169Z",
          "wordCount": 662,
          "title": "Offline reinforcement learning with uncertainty for treatment strategies in sepsis. (arXiv:2107.04491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.02881",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+OLeary_Roseberry_T/0/1/0/all/0/1\">Thomas O&#x27;Leary-Roseberry</a>, <a href=\"http://arxiv.org/find/math/1/au:+Alger_N/0/1/0/all/0/1\">Nick Alger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ghattas_O/0/1/0/all/0/1\">Omar Ghattas</a>",
          "description": "Newton methods have fallen out of favor for modern optimization problems\n(e.g. deep learning) because of concerns about per-iteration computational\ncomplexity. In this setting highly subsampled first order methods are\npreferred. In this work we motivate the extension of Newton methods to the\nhighly stochastic regime, and argue for the use of the scalable low rank saddle\nfree Newton (LRSFN) method. In this setting, iterative updates are dominated by\nstochastic noise, and stability of the method is key. In stability analysis, we\ndemonstrate that stochastic errors for Newton methods can be greatly amplified\nby ill-conditioned matrix operators. The LRSFN algorithm mitigates this issue\nby the use of Levenberg-Marquardt damping, but generally second order methods\nwith stochastic Hessian and gradient information may need to take small steps,\nunlike in deterministic problems. Numerical results show that even under\nrestrictive step-length conditions, LRSFN can outperform popular first order\nmethods on nontrivial deep learning tasks in terms of generalizability for\nequivalent computational work.",
          "link": "http://arxiv.org/abs/2002.02881",
          "publishedOn": "2021-07-12T01:55:16.163Z",
          "wordCount": 617,
          "title": "Low Rank Saddle Free Newton: Scalable Stochastic Nonconvex Optimization. (arXiv:2002.02881v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertram_T/0/1/0/all/0/1\">Timo Bertram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Martin M&#xfc;ller</a>",
          "description": "In this paper, we study the problem of evaluating the addition of elements to\na set. This problem is difficult, because it can, in the general case, not be\nreduced to unconditional preferences between the choices. Therefore, we model\npreferences based on the context of the decision. We discuss and compare two\ndifferent Siamese network architectures for this task: a twin network that\ncompares the two sets resulting after the addition, and a triplet network that\nmodels the contribution of each candidate to the existing set. We evaluate the\ntwo settings on a real-world task; learning human card preferences for deck\nbuilding in the collectible card game Magic: The Gathering. We show that the\ntriplet approach achieves a better result than the twin network and that both\noutperform previous results on this task.",
          "link": "http://arxiv.org/abs/2107.04438",
          "publishedOn": "2021-07-12T01:55:16.118Z",
          "wordCount": 596,
          "title": "A Comparison of Contextual and Non-Contextual Preference Ranking for Set Addition Problems. (arXiv:2107.04438v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04556",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pant_P/0/1/0/all/0/1\">Pranshu Pant</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Doshi_R/0/1/0/all/0/1\">Ruchit Doshi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bahl_P/0/1/0/all/0/1\">Pranav Bahl</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Farimani_A/0/1/0/all/0/1\">Amir Barati Farimani</a>",
          "description": "Reduced Order Modelling (ROM) has been widely used to create lower order,\ncomputationally inexpensive representations of higher-order dynamical systems.\nUsing these representations, ROMs can efficiently model flow fields while using\nsignificantly lesser parameters. Conventional ROMs accomplish this by linearly\nprojecting higher-order manifolds to lower-dimensional space using\ndimensionality reduction techniques such as Proper Orthogonal Decomposition\n(POD). In this work, we develop a novel deep learning framework DL-ROM (Deep\nLearning - Reduced Order Modelling) to create a neural network capable of\nnon-linear projections to reduced order states. We then use the learned reduced\nstate to efficiently predict future time steps of the simulation using 3D\nAutoencoder and 3D U-Net based architectures. Our model DL-ROM is able to\ncreate highly accurate reconstructions from the learned ROM and is thus able to\nefficiently predict future time steps by temporally traversing in the learned\nreduced state. All of this is achieved without ground truth supervision or\nneeding to iteratively solve the expensive Navier-Stokes(NS) equations thereby\nresulting in massive computational savings. To test the effectiveness and\nperformance of our approach, we evaluate our implementation on five different\nComputational Fluid Dynamics (CFD) datasets using reconstruction performance\nand computational runtime metrics. DL-ROM can reduce the computational runtimes\nof iterative solvers by nearly two orders of magnitude while maintaining an\nacceptable error threshold.",
          "link": "http://arxiv.org/abs/2107.04556",
          "publishedOn": "2021-07-12T01:55:16.069Z",
          "wordCount": 664,
          "title": "Deep Learning for Reduced Order Modelling and Efficient Temporal Evolution of Fluid Simulations. (arXiv:2107.04556v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antonova_R/0/1/0/all/0/1\">Rika Antonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1\">Fabio Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Possas_R/0/1/0/all/0/1\">Rafael Possas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "BayesSim is a statistical technique for domain randomization in reinforcement\nlearning based on likelihood-free inference of simulation parameters. This\npaper outlines BayesSimIG: a library that provides an implementation of\nBayesSim integrated with the recently released NVIDIA IsaacGym. This\ncombination allows large-scale parameter inference with end-to-end GPU\nacceleration. Both inference and simulation get GPU speedup, with support for\nrunning more than 10K parallel simulation environments for complex robotics\ntasks that can have more than 100 simulation parameters to estimate. BayesSimIG\nprovides an integration with TensorBoard to easily visualize slices of\nhigh-dimensional posteriors. The library is built in a modular way to support\nresearch experiments with novel ways to collect and process the trajectories\nfrom the parallel IsaacGym environments.",
          "link": "http://arxiv.org/abs/2107.04527",
          "publishedOn": "2021-07-12T01:55:16.062Z",
          "wordCount": 553,
          "title": "BayesSimIG: Scalable Parameter Inference for Adaptive Domain Randomization with IsaacGym. (arXiv:2107.04527v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stamm_F/0/1/0/all/0/1\">Felix I. Stamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_M/0/1/0/all/0/1\">Martin Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohmaier_M/0/1/0/all/0/1\">Markus Strohmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemmerich_F/0/1/0/all/0/1\">Florian Lemmerich</a>",
          "description": "This paper introduces Redescription Model Mining, a novel approach to\nidentify interpretable patterns across two datasets that share only a subset of\nattributes and have no common instances. In particular, Redescription Model\nMining aims to find pairs of describable data subsets -- one for each dataset\n-- that induce similar exceptional models with respect to a prespecified model\nclass. To achieve this, we combine two previously separate research areas:\nExceptional Model Mining and Redescription Mining. For this new problem\nsetting, we develop interestingness measures to select promising patterns,\npropose efficient algorithms, and demonstrate their potential on synthetic and\nreal-world data. Uncovered patterns can hint at common underlying phenomena\nthat manifest themselves across datasets, enabling the discovery of possible\nassociations between (combinations of) attributes that do not appear in the\nsame dataset.",
          "link": "http://arxiv.org/abs/2107.04462",
          "publishedOn": "2021-07-12T01:55:16.054Z",
          "wordCount": 557,
          "title": "Redescription Model Mining. (arXiv:2107.04462v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02745",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Leluc_R/0/1/0/all/0/1\">R&#xe9;mi Leluc</a>, <a href=\"http://arxiv.org/find/math/1/au:+Portier_F/0/1/0/all/0/1\">Fran&#xe7;ois Portier</a>",
          "description": "In this paper, we investigate a general class of stochastic gradient descent\n(SGD) algorithms, called conditioned SGD, based on a preconditioning of the\ngradient direction. Under some mild assumptions, namely the $L$-smoothness of\nthe non-convex objective function and some weak growth condition on the noise,\nwe establish the almost sure convergence and the asymptotic normality for a\nbroad class of conditioning matrices. In particular, when the conditioning\nmatrix is an estimate of the inverse Hessian at the optimal point, the\nalgorithm is proved to be asymptotically optimal. The benefits of this approach\nare validated on simulated and real datasets.",
          "link": "http://arxiv.org/abs/2006.02745",
          "publishedOn": "2021-07-12T01:55:16.032Z",
          "wordCount": 555,
          "title": "Asymptotic Optimality of Conditioned Stochastic Gradient Descent. (arXiv:2006.02745v3 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1\">Francisco Eiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>",
          "description": "Randomized smoothing has recently emerged as an effective tool that enables\ncertification of deep neural network classifiers at scale. All prior art on\nrandomized smoothing has focused on isotropic $\\ell_p$ certification, which has\nthe advantage of yielding certificates that can be easily compared among\nisotropic methods via $\\ell_p$-norm radius. However, isotropic certification\nlimits the region that can be certified around an input to worst-case\nadversaries, \\ie it cannot reason about other \"close\", potentially large,\nconstant prediction safe regions. To alleviate this issue, (i) we theoretically\nextend the isotropic randomized smoothing $\\ell_1$ and $\\ell_2$ certificates to\ntheir generalized anisotropic counterparts following a simplified analysis.\nMoreover, (ii) we propose evaluation metrics allowing for the comparison of\ngeneral certificates - a certificate is superior to another if it certifies a\nsuperset region - with the quantification of each certificate through the\nvolume of the certified region. We introduce ANCER, a practical framework for\nobtaining anisotropic certificates for a given test set sample via volume\nmaximization. Our empirical results demonstrate that ANCER achieves\nstate-of-the-art $\\ell_1$ and $\\ell_2$ certified accuracy on both CIFAR-10 and\nImageNet at multiple radii, while certifying substantially larger regions in\nterms of volume, thus highlighting the benefits of moving away from isotropic\nanalysis. Code used in our experiments is available in\nhttps://github.com/MotasemAlfarra/ANCER.",
          "link": "http://arxiv.org/abs/2107.04570",
          "publishedOn": "2021-07-12T01:55:16.024Z",
          "wordCount": 670,
          "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Revelle_M/0/1/0/all/0/1\">Matt Revelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domeniconi_C/0/1/0/all/0/1\">Carlotta Domeniconi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelman_B/0/1/0/all/0/1\">Ben Gelman</a>",
          "description": "Communities in social networks evolve over time as people enter and leave the\nnetwork and their activity behaviors shift. The task of predicting structural\nchanges in communities over time is known as community evolution prediction.\nExisting work in this area has focused on the development of frameworks for\ndefining events while using traditional classification methods to perform the\nactual prediction. We present a novel graph neural network for predicting\ncommunity evolution events from structural and temporal information. The model\n(GNAN) includes a group-node attention component which enables support for\nvariable-sized inputs and learned representation of groups based on member and\nneighbor node features. A comparative evaluation with standard baseline methods\nis performed and we demonstrate that our model outperforms the baselines.\nAdditionally, we show the effects of network trends on model performance.",
          "link": "http://arxiv.org/abs/2107.04522",
          "publishedOn": "2021-07-12T01:55:16.017Z",
          "wordCount": 567,
          "title": "Group-Node Attention for Community Evolution Prediction. (arXiv:2107.04522v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahubali_N/0/1/0/all/0/1\">Nagaraj Bahubali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Abinaya Thulsi Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Adarsh Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasadand_S/0/1/0/all/0/1\">Soniya Manchenahalli Gnanendra Prasadand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aralappa_S/0/1/0/all/0/1\">Sriram Aralappa</a>",
          "description": "Author name ambiguity remains a critical open problem in digital libraries\ndue to synonymy and homonymy of names. In this paper, we propose a novel\napproach to link author names to their real-world entities by relying on their\nco-authorship pattern and area of research. Our supervised deep learning model\nidentifies an author by capturing his/her relationship with his/her co-authors\nand area of research, which is represented by the titles and sources of the\ntarget author's publications. These attributes are encoded by their semantic\nand symbolic representations. To this end, Bib2Auth uses ~ 22K bibliographic\nrecords from the DBLP repository and is trained with each pair of co-authors.\nThe extensive experiments have proved the capability of the approach to\ndistinguish between authors sharing the same name and recognize authors with\ndifferent name variations. Bib2Auth has shown good performance on a relatively\nlarge dataset, which qualifies it to be directly integrated into bibliographic\nindices.",
          "link": "http://arxiv.org/abs/2107.04382",
          "publishedOn": "2021-07-12T01:55:16.008Z",
          "wordCount": 604,
          "title": "Bib2Auth: Deep Learning Approach for Author Disambiguation using Bibliographic Data. (arXiv:2107.04382v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04384",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Sebastian Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1\">Sebastian Goldt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saxe_A/0/1/0/all/0/1\">Andrew Saxe</a>",
          "description": "Continual learning-the ability to learn many tasks in sequence-is critical\nfor artificial learning systems. Yet standard training methods for deep\nnetworks often suffer from catastrophic forgetting, where learning new tasks\nerases knowledge of earlier tasks. While catastrophic forgetting labels the\nproblem, the theoretical reasons for interference between tasks remain unclear.\nHere, we attempt to narrow this gap between theory and practice by studying\ncontinual learning in the teacher-student setup. We extend previous analytical\nwork on two-layer networks in the teacher-student setup to multiple teachers.\nUsing each teacher to represent a different task, we investigate how the\nrelationship between teachers affects the amount of forgetting and transfer\nexhibited by the student when the task switches. In line with recent work, we\nfind that when tasks depend on similar features, intermediate task similarity\nleads to greatest forgetting. However, feature similarity is only one way in\nwhich tasks may be related. The teacher-student approach allows us to\ndisentangle task similarity at the level of readouts (hidden-to-output weights)\nand features (input-to-hidden weights). We find a complex interplay between\nboth types of similarity, initial transfer/forgetting rates, maximum\ntransfer/forgetting, and long-term transfer/forgetting. Together, these results\nhelp illuminate the diverse factors contributing to catastrophic forgetting.",
          "link": "http://arxiv.org/abs/2107.04384",
          "publishedOn": "2021-07-12T01:55:16.002Z",
          "wordCount": 653,
          "title": "Continual Learning in the Teacher-Student Setup: Impact of Task Similarity. (arXiv:2107.04384v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1\">Sampo Kuutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "Imitation learning has been widely used to learn control policies for\nautonomous driving based on pre-recorded data. However, imitation learning\nbased policies have been shown to be susceptible to compounding errors when\nencountering states outside of the training distribution. Further, these agents\nhave been demonstrated to be easily exploitable by adversarial road users\naiming to create collisions. To overcome these shortcomings, we introduce\nAdversarial Mixture Density Networks (AMDN), which learns two distributions\nfrom separate datasets. The first is a distribution of safe actions learned\nfrom a dataset of naturalistic human driving. The second is a distribution\nrepresenting unsafe actions likely to lead to collision, learned from a dataset\nof collisions. During training, we leverage these two distributions to provide\nan additional loss based on the similarity of the two distributions. By\npenalising the safe action distribution based on its similarity to the unsafe\naction distribution when training on the collision dataset, a more robust and\nsafe control policy is obtained. We demonstrate the proposed AMDN approach in a\nvehicle following use-case, and evaluate under naturalistic and adversarial\ntesting environments. We show that despite its simplicity, AMDN provides\nsignificant benefits for the safety of the learned control policy, when\ncompared to pure imitation learning or standard mixture density network\napproaches.",
          "link": "http://arxiv.org/abs/2107.04485",
          "publishedOn": "2021-07-12T01:55:15.981Z",
          "wordCount": 664,
          "title": "Adversarial Mixture Density Networks: Learning to Drive Safely from Collision Data. (arXiv:2107.04485v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>",
          "description": "Many joint entity relation extraction models setup two separated label spaces\nfor the two sub-tasks (i.e., entity detection and relation classification). We\nargue that this setting may hinder the information interaction between entities\nand relations. In this work, we propose to eliminate the different treatment on\nthe two sub-tasks' label spaces. The input of our model is a table containing\nall word pairs from a sentence. Entities and relations are represented by\nsquares and rectangles in the table. We apply a unified classifier to predict\neach cell's label, which unifies the learning of two sub-tasks. For testing, an\neffective (yet fast) approximate decoder is proposed for finding squares and\nrectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC)\nshow that, using only half the number of parameters, our model achieves\ncompetitive accuracy with the best extractor, and is faster.",
          "link": "http://arxiv.org/abs/2107.04292",
          "publishedOn": "2021-07-12T01:55:15.975Z",
          "wordCount": 584,
          "title": "UniRE: A Unified Label Space for Entity Relation Extraction. (arXiv:2107.04292v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_V/0/1/0/all/0/1\">Vincent Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamies_W/0/1/0/all/0/1\">Waleed Khamies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1\">Liam Paull</a>",
          "description": "Heteroscedastic regression is the task of supervised learning where each\nlabel is subject to noise from a different distribution. This noise can be\ncaused by the labelling process, and impacts negatively the performance of the\nlearning algorithm as it violates the i.i.d. assumptions. In many situations\nhowever, the labelling process is able to estimate the variance of such\ndistribution for each label, which can be used as an additional information to\nmitigate this impact. We adapt an inverse-variance weighted mean square error,\nbased on the Gauss-Markov theorem, for parameter optimization on neural\nnetworks. We introduce Batch Inverse-Variance, a loss function which is robust\nto near-ground truth samples, and allows to control the effective learning\nrate. Our experimental results show that BIV improves significantly the\nperformance of the networks on two noisy datasets, compared to L2 loss,\ninverse-variance weighting, as well as a filtering-based baseline.",
          "link": "http://arxiv.org/abs/2107.04497",
          "publishedOn": "2021-07-12T01:55:15.968Z",
          "wordCount": 589,
          "title": "Batch Inverse-Variance Weighting: Deep Heteroscedastic Regression. (arXiv:2107.04497v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poyiadzi_R/0/1/0/all/0/1\">Rafael Poyiadzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renard_X/0/1/0/all/0/1\">Xavier Renard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laugel_T/0/1/0/all/0/1\">Thibault Laugel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1\">Raul Santos-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1\">Marcin Detyniecki</a>",
          "description": "This paper analyses the fundamental ingredients behind surrogate explanations\nto provide a better understanding of their inner workings. We start our\nexposition by considering global surrogates, describing the trade-off between\ncomplexity of the surrogate and fidelity to the black-box being modelled. We\nshow that transitioning from global to local - reducing coverage - allows for\nmore favourable conditions on the Pareto frontier of fidelity-complexity of a\nsurrogate. We discuss the interplay between complexity, fidelity and coverage,\nand consider how different user needs can lead to problem formulations where\nthese are either constraints or penalties. We also present experiments that\ndemonstrate how the local surrogate interpretability procedure can be made\ninteractive and lead to better explanations.",
          "link": "http://arxiv.org/abs/2107.04309",
          "publishedOn": "2021-07-12T01:55:15.962Z",
          "wordCount": 560,
          "title": "Understanding surrogate explanations: the interplay between complexity, fidelity and coverage. (arXiv:2107.04309v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diana_E/0/1/0/all/0/1\">Emily Diana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gill_W/0/1/0/all/0/1\">Wesley Gill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1\">Michael Kearns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1\">Krishnaram Kenthapadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifi_Malvajerdi_S/0/1/0/all/0/1\">Saeed Sharifi-Malvajerdi</a>",
          "description": "We study the problem of training a model that must obey demographic fairness\nconditions when the sensitive features are not available at training time -- in\nother words, how can we train a model to be fair by race when we don't have\ndata about race? We adopt a fairness pipeline perspective, in which an\n\"upstream\" learner that does have access to the sensitive features will learn a\nproxy model for these features from the other attributes. The goal of the proxy\nis to allow a general \"downstream\" learner -- with minimal assumptions on their\nprediction task -- to be able to use the proxy to train a model that is fair\nwith respect to the true sensitive features. We show that obeying multiaccuracy\nconstraints with respect to the downstream model class suffices for this\npurpose, and provide sample- and oracle efficient-algorithms and generalization\nbounds for learning such proxies. In general, multiaccuracy can be much easier\nto satisfy than classification accuracy, and can be satisfied even when the\nsensitive features are hard to predict.",
          "link": "http://arxiv.org/abs/2107.04423",
          "publishedOn": "2021-07-12T01:55:15.955Z",
          "wordCount": 609,
          "title": "Multiaccurate Proxies for Downstream Fairness. (arXiv:2107.04423v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bovenzi_G/0/1/0/all/0/1\">Giampaolo Bovenzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finamore_A/0/1/0/all/0/1\">Alessandro Finamore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aceto_G/0/1/0/all/0/1\">Giuseppe Aceto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciuonzo_D/0/1/0/all/0/1\">Domenico Ciuonzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pescape_A/0/1/0/all/0/1\">Antonio Pescap&#xe8;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1\">Dario Rossi</a>",
          "description": "The recent popularity growth of Deep Learning (DL) re-ignited the interest\ntowards traffic classification, with several studies demonstrating the accuracy\nof DL-based classifiers to identify Internet applications' traffic. Even with\nthe aid of hardware accelerators (GPUs, TPUs), DL model training remains\nexpensive, and limits the ability to operate frequent model updates necessary\nto fit to the ever evolving nature of Internet traffic, and mobile traffic in\nparticular. To address this pain point, in this work we explore Incremental\nLearning (IL) techniques to add new classes to models without a full\nretraining, hence speeding up model's updates cycle. We consider iCarl, a state\nof the art IL method, and MIRAGE-2019, a public dataset with traffic from 40\nAndroid apps, aiming to understand \"if there is a case for incremental learning\nin traffic classification\". By dissecting iCarl internals, we discuss ways to\nimprove its design, contributing a revised version, namely iCarl+. Despite our\nanalysis reveals their infancy, IL techniques are a promising research area on\nthe roadmap towards automated DL-based traffic analysis systems.",
          "link": "http://arxiv.org/abs/2107.04464",
          "publishedOn": "2021-07-12T01:55:15.937Z",
          "wordCount": 639,
          "title": "A First Look at Class Incremental Learning in Deep Learning Mobile Traffic Classification. (arXiv:2107.04464v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1\">Tobias Uelwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1\">Felix Michels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candido_O/0/1/0/all/0/1\">Oliver De Candido</a>",
          "description": "Given the increasing threat of adversarial attacks on deep neural networks\n(DNNs), research on efficient detection methods is more important than ever. In\nthis work, we take a closer look at adversarial attack detection based on the\nclass scores of an already trained classification model. We propose to train a\nsupport vector machine (SVM) on the class scores to detect adversarial\nexamples. Our method is able to detect adversarial examples generated by\nvarious attacks, and can be easily adopted to a plethora of deep classification\nmodels. We show that our approach yields an improved detection rate compared to\nan existing method, whilst being easy to implement. We perform an extensive\nempirical analysis on different deep classification models, investigating\nvarious state-of-the-art adversarial attacks. Moreover, we observe that our\nproposed method is better at detecting a combination of adversarial attacks.\nThis work indicates the potential of detecting various adversarial attacks\nsimply by using the class scores of an already trained classification model.",
          "link": "http://arxiv.org/abs/2107.04435",
          "publishedOn": "2021-07-12T01:55:15.928Z",
          "wordCount": 616,
          "title": "Learning to Detect Adversarial Examples Based on Class Scores. (arXiv:2107.04435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhuang Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shufei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiufeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xinping Yi</a>",
          "description": "In this work, we consider model robustness of deep neural networks against\nadversarial attacks from a global manifold perspective. Leveraging both the\nlocal and global latent information, we propose a novel adversarial training\nmethod through robust optimization, and a tractable way to generate Latent\nManifold Adversarial Examples (LMAEs) via an adversarial game between a\ndiscriminator and a classifier. The proposed adversarial training with latent\ndistribution (ATLD) method defends against adversarial attacks by crafting\nLMAEs with the latent manifold in an unsupervised manner. ATLD preserves the\nlocal and global information of latent manifold and promises improved\nrobustness against adversarial attacks. To verify the effectiveness of our\nproposed method, we conduct extensive experiments over different datasets\n(e.g., CIFAR-10, CIFAR-100, SVHN) with different adversarial attacks (e.g.,\nPGD, CW), and show that our method substantially outperforms the\nstate-of-the-art (e.g., Feature Scattering) in adversarial robustness by a\nlarge accuracy margin. The source codes are available at\nhttps://github.com/LitterQ/ATLD-pytorch.",
          "link": "http://arxiv.org/abs/2107.04401",
          "publishedOn": "2021-07-12T01:55:15.922Z",
          "wordCount": 590,
          "title": "Improving Model Robustness with Latent Distribution Locally and Globally. (arXiv:2107.04401v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nousi_P/0/1/0/all/0/1\">Paraskevi Nousi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkouli_S/0/1/0/all/0/1\">Styliani-Christina Fragkouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1\">Nikolaos Passalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosif_P/0/1/0/all/0/1\">Panagiotis Iosif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostolatos_T/0/1/0/all/0/1\">Theocharis Apostolatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1\">George Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stergioulas_N/0/1/0/all/0/1\">Nikolaos Stergioulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1\">Anastasios Tefas</a>",
          "description": "Recently, artificial neural networks have been gaining momentum in the field\nof gravitational wave astronomy, for example in surrogate modelling of\ncomputationally expensive waveform models for binary black hole inspiral and\nmerger. Surrogate modelling yields fast and accurate approximations of\ngravitational waves and neural networks have been used in the final step of\ninterpolating the coefficients of the surrogate model for arbitrary waveforms\noutside the training sample. We investigate the existence of underlying\nstructures in the empirical interpolation coefficients using autoencoders. We\ndemonstrate that when the coefficient space is compressed to only two\ndimensions, a spiral structure appears, wherein the spiral angle is linearly\nrelated to the mass ratio. Based on this finding, we design a spiral module\nwith learnable parameters, that is used as the first layer in a neural network,\nwhich learns to map the input space to the coefficients. The spiral module is\nevaluated on multiple neural network architectures and consistently achieves\nbetter speed-accuracy trade-off than baseline models. A thorough experimental\nstudy is conducted and the final result is a surrogate model which can evaluate\nmillions of input parameters in a single forward pass in under 1ms on a desktop\nGPU, while the mismatch between the corresponding generated waveforms and the\nground-truth waveforms is better than the compared baseline methods. We\nanticipate the existence of analogous underlying structures and corresponding\ncomputational gains also in the case of spinning black hole binaries.",
          "link": "http://arxiv.org/abs/2107.04312",
          "publishedOn": "2021-07-12T01:55:15.914Z",
          "wordCount": 676,
          "title": "Autoencoder-driven Spiral Representation Learning for Gravitational Wave Surrogate Modelling. (arXiv:2107.04312v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carreira_Perpinan_M/0/1/0/all/0/1\">Miguel &#xc1;. Carreira-Perpi&#xf1;&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idelbayev_Y/0/1/0/all/0/1\">Yerlan Idelbayev</a>",
          "description": "Model compression is generally performed by using quantization, low-rank\napproximation or pruning, for which various algorithms have been researched in\nrecent years. One fundamental question is: what types of compression work\nbetter for a given model? Or even better: can we improve by combining\ncompressions in a suitable way? We formulate this generally as a problem of\noptimizing the loss but where the weights are constrained to equal an additive\ncombination of separately compressed parts; and we give an algorithm to learn\nthe corresponding parts' parameters. Experimentally with deep neural nets, we\nobserve that 1) we can find significantly better models in the\nerror-compression space, indicating that different compression types have\ncomplementary benefits, and 2) the best type of combination depends exquisitely\non the type of neural net. For example, we can compress ResNets and AlexNet\nusing only 1 bit per weight without error degradation at the cost of adding a\nfew floating point weights. However, VGG nets can be better compressed by\ncombining low-rank with a few floating point weights.",
          "link": "http://arxiv.org/abs/2107.04380",
          "publishedOn": "2021-07-12T01:55:15.906Z",
          "wordCount": 618,
          "title": "Model compression as constrained optimization, with application to neural nets. Part V: combining compressions. (arXiv:2107.04380v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vermeire_T/0/1/0/all/0/1\">Tom Vermeire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laugel_T/0/1/0/all/0/1\">Thibault Laugel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renard_X/0/1/0/all/0/1\">Xavier Renard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martens_D/0/1/0/all/0/1\">David Martens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1\">Marcin Detyniecki</a>",
          "description": "Explainability is becoming an important requirement for organizations that\nmake use of automated decision-making due to regulatory initiatives and a shift\nin public awareness. Various and significantly different algorithmic methods to\nprovide this explainability have been introduced in the field, but the existing\nliterature in the machine learning community has paid little attention to the\nstakeholder whose needs are rather studied in the human-computer interface\ncommunity. Therefore, organizations that want or need to provide this\nexplainability are confronted with the selection of an appropriate method for\ntheir use case. In this paper, we argue there is a need for a methodology to\nbridge the gap between stakeholder needs and explanation methods. We present\nour ongoing work on creating this methodology to help data scientists in the\nprocess of providing explainability to stakeholders. In particular, our\ncontributions include documents used to characterize XAI methods and user\nrequirements (shown in Appendix), which our methodology builds upon.",
          "link": "http://arxiv.org/abs/2107.04427",
          "publishedOn": "2021-07-12T01:55:15.889Z",
          "wordCount": 607,
          "title": "How to choose an Explainability Method? Towards a Methodical Implementation of XAI in Practice. (arXiv:2107.04427v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Neural approaches have achieved state-of-the-art accuracy on machine\ntranslation but suffer from the high cost of collecting large scale parallel\ndata. Thus, a lot of research has been conducted for neural machine translation\n(NMT) with very limited parallel data, i.e., the low-resource setting. In this\npaper, we provide a survey for low-resource NMT and classify related works into\nthree categories according to the auxiliary data they used: (1) exploiting\nmonolingual data of source and/or target languages, (2) exploiting data from\nauxiliary languages, and (3) exploiting multi-modal data. We hope that our\nsurvey can help researchers to better understand this field and inspire them to\ndesign better algorithms, and help industry practitioners to choose appropriate\nalgorithms for their applications.",
          "link": "http://arxiv.org/abs/2107.04239",
          "publishedOn": "2021-07-12T01:55:15.882Z",
          "wordCount": 578,
          "title": "A Survey on Low-Resource Neural Machine Translation. (arXiv:2107.04239v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riekert_A/0/1/0/all/0/1\">Adrian Riekert</a>",
          "description": "Gradient descent (GD) type optimization schemes are the standard methods to\ntrain artificial neural networks (ANNs) with rectified linear unit (ReLU)\nactivation. Such schemes can be considered as discretizations of gradient flows\n(GFs) associated to the training of ANNs with ReLU activation and most of the\nkey difficulties in the mathematical convergence analysis of GD type\noptimization schemes in the training of ANNs with ReLU activation seem to be\nalready present in the dynamics of the corresponding GF differential equations.\nIt is the key subject of this work to analyze such GF differential equations in\nthe training of ANNs with ReLU activation and three layers (one input layer,\none hidden layer, and one output layer). In particular, in this article we\nprove in the case where the target function is possibly multi-dimensional and\ncontinuous and in the case where the probability distribution of the input data\nis absolutely continuous with respect to the Lebesgue measure that the risk of\nevery bounded GF trajectory converges to the risk of a critical point. In\naddition, in this article we show in the case of a 1-dimensional affine linear\ntarget function and in the case where the probability distribution of the input\ndata coincides with the standard uniform distribution that the risk of every\nbounded GF trajectory converges to zero if the initial risk is sufficiently\nsmall. Finally, in the special situation where there is only one neuron on the\nhidden layer (1-dimensional hidden layer) we strengthen the above named result\nfor affine linear target functions by proving that that the risk of every (not\nnecessarily bounded) GF trajectory converges to zero if the initial risk is\nsufficiently small.",
          "link": "http://arxiv.org/abs/2107.04479",
          "publishedOn": "2021-07-12T01:55:15.876Z",
          "wordCount": 730,
          "title": "Convergence analysis for gradient flows in the training of artificial neural networks with ReLU activation. (arXiv:2107.04479v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhi_W/0/1/0/all/0/1\">Weiming Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_L/0/1/0/all/0/1\">Lionel Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1\">Fabio Ramos</a>",
          "description": "This work addresses the problem of predicting the motion trajectories of\ndynamic objects in the environment. Recent advances in predicting motion\npatterns often rely on machine learning techniques to extrapolate motion\npatterns from observed trajectories, with no mechanism to directly incorporate\nknown rules. We propose a novel framework, which combines probabilistic\nlearning and constrained trajectory optimisation. The learning component of our\nframework provides a distribution over future motion trajectories conditioned\non observed past coordinates. This distribution is then used as a prior to a\nconstrained optimisation problem which enforces chance constraints on the\ntrajectory distribution. This results in constraint-compliant trajectory\ndistributions which closely resemble the prior. In particular, we focus our\ninvestigation on collision constraints, such that extrapolated future\ntrajectory distributions conform to the environment structure. We empirically\ndemonstrate on real-world and simulated datasets the ability of our framework\nto learn complex probabilistic motion trajectories for motion data, while\ndirectly enforcing constraints to improve generalisability, producing more\nrobust and higher quality trajectory distributions.",
          "link": "http://arxiv.org/abs/2107.04193",
          "publishedOn": "2021-07-12T01:55:15.869Z",
          "wordCount": 594,
          "title": "Probabilistic Trajectory Prediction with Structural Constraints. (arXiv:2107.04193v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1\">Manh Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beurton_Aimar_M/0/1/0/all/0/1\">Marie Beurton-Aimar</a>",
          "description": "In this work, we introduce our submission to the 2nd Affective Behavior\nAnalysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning\nmodel on multi-databases to perform two tasks: seven basic facial expressions\nprediction and valence-arousal estimation. Since these databases do not\ncontains labels for all the two tasks, we have applied the distillation\nknowledge technique to train two networks: one teacher and one student model.\nThe student model will be trained using both ground truth labels and soft\nlabels derived from the pretrained teacher model. During the training, we add\none more task, which is the combination of the two mentioned tasks, for better\nexploiting inter-task correlations. We also exploit the sharing videos between\nthe two tasks of the AffWild2 database that is used in the competition, to\nfurther improve the performance of the network. Experiment results shows that\nthe network have achieved promising results on the validation set of the\nAffWild2 database. Code and pretrained model are publicly available at\nhttps://github.com/glmanhtu/multitask-abaw-2021",
          "link": "http://arxiv.org/abs/2107.04127",
          "publishedOn": "2021-07-12T01:55:15.862Z",
          "wordCount": 595,
          "title": "Multitask Multi-database Emotion Recognition. (arXiv:2107.04127v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1\">In Hwa Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Multiplex immunofluorescence and immunohistochemistry benefit patients by\nallowing cancer pathologists to identify several proteins expressed on the\nsurface of cells, enabling cell classification, better understanding of the\ntumour micro-environment, more accurate diagnoses, prognoses, and tailored\nimmunotherapy based on the immune status of individual patients. However, they\nare expensive and time consuming processes which require complex staining and\nimaging techniques by expert technicians. Hoechst staining is much cheaper and\neasier to perform, but is not typically used in this case as it binds to DNA\nrather than to the proteins targeted by immunofluorescent techniques, and it\nwas not previously thought possible to differentiate cells expressing these\nproteins based only on DNA morphology. In this work we show otherwise, training\na deep convolutional neural network to identify cells expressing three proteins\n(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with\ngreater than 90% precision and recall, from Hoechst 33342 stained tissue only.\nOur model learns previously unknown morphological features associated with\nexpression of these proteins which can be used to accurately differentiate\nlymphocyte subtypes for use in key prognostic metrics such as assessment of\nimmune cell infiltration,and thereby predict and improve patient outcomes\nwithout the need for costly multiplex immunofluorescence.",
          "link": "http://arxiv.org/abs/2107.04388",
          "publishedOn": "2021-07-12T01:55:15.855Z",
          "wordCount": 654,
          "title": "Hoechst Is All You Need: LymphocyteClassification with Deep Learning. (arXiv:2107.04388v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuan-Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>",
          "description": "In many applications of computer graphics, art and design, it is desirable\nfor a user to provide intuitive non-image input, such as text, sketch, stroke,\ngraph or layout, and have a computer system automatically generate\nphoto-realistic images that adhere to the input content. While classic works\nthat allow such automatic image content generation have followed a framework of\nimage retrieval and composition, recent advances in deep generative models such\nas generative adversarial networks (GANs), variational autoencoders (VAEs), and\nflow-based methods have enabled more powerful and versatile image generation\ntasks. This paper reviews recent works for image synthesis given intuitive user\ninput, covering advances in input versatility, image generation methodology,\nbenchmark datasets, and evaluation metrics. This motivates new perspectives on\ninput representation and interactivity, cross pollination between major image\ngeneration paradigms, and evaluation and comparison of generation methods.",
          "link": "http://arxiv.org/abs/2107.04240",
          "publishedOn": "2021-07-12T01:55:15.835Z",
          "wordCount": 603,
          "title": "Deep Image Synthesis from Intuitive User Input: A Review and Perspectives. (arXiv:2107.04240v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meyen_S/0/1/0/all/0/1\">Sascha Meyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goppert_F/0/1/0/all/0/1\">Frieder G&#xf6;ppert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alber_H/0/1/0/all/0/1\">Helen Alber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luxburg_U/0/1/0/all/0/1\">Ulrike von Luxburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franz_V/0/1/0/all/0/1\">Volker H. Franz</a>",
          "description": "Consider an ensemble of $k$ individual classifiers whose accuracies are\nknown. Upon receiving a test point, each of the classifiers outputs a predicted\nlabel and a confidence in its prediction for this particular test point. In\nthis paper, we address the question of whether we can determine the accuracy of\nthe ensemble. Surprisingly, even when classifiers are combined in the\nstatistically optimal way in this setting, the accuracy of the resulting\nensemble classifier cannot be computed from the accuracies of the individual\nclassifiers-as would be the case in the standard setting of confidence weighted\nmajority voting. We prove tight upper and lower bounds on the ensemble\naccuracy. We explicitly construct the individual classifiers that attain the\nupper and lower bounds: specialists and generalists. Our theoretical results\nhave very practical consequences: (1) If we use ensemble methods and have the\nchoice to construct our individual (independent) classifiers from scratch, then\nwe should aim for specialist classifiers rather than generalists. (2) Our\nbounds can be used to determine how many classifiers are at least required to\nachieve a desired ensemble accuracy. Finally, we improve our bounds by\nconsidering the mutual information between the true label and the individual\nclassifier's output.",
          "link": "http://arxiv.org/abs/2107.04381",
          "publishedOn": "2021-07-12T01:55:15.827Z",
          "wordCount": 632,
          "title": "Specialists Outperform Generalists in Ensemble Classification. (arXiv:2107.04381v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weien Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoqian Chen</a>",
          "description": "Physics Informed Neural Network (PINN) is a scientific computing framework\nused to solve both forward and inverse problems modeled by Partial Differential\nEquations (PDEs). This paper introduces IDRLnet, a Python toolbox for modeling\nand solving problems through PINN systematically. IDRLnet constructs the\nframework for a wide range of PINN algorithms and applications. It provides a\nstructured way to incorporate geometric objects, data sources, artificial\nneural networks, loss metrics, and optimizers within Python. Furthermore, it\nprovides functionality to solve noisy inverse problems, variational\nminimization, and integral differential equations. New PINN variants can be\nintegrated into the framework easily. Source code, tutorials, and documentation\nare available at \\url{https://github.com/idrl-lab/idrlnet}.",
          "link": "http://arxiv.org/abs/2107.04320",
          "publishedOn": "2021-07-12T01:55:15.820Z",
          "wordCount": 543,
          "title": "IDRLnet: A Physics-Informed Neural Network Library. (arXiv:2107.04320v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">John Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "Deep learning practitioners often operate on a computational and monetary\nbudget. Thus, it is critical to design optimization algorithms that perform\nwell under any budget. The linear learning rate schedule is considered the best\nbudget-aware schedule, as it outperforms most other schedules in the low budget\nregime. On the other hand, learning rate schedules -- such as the\n\\texttt{30-60-90} step schedule -- are known to achieve high performance when\nthe model can be trained for many epochs. Yet, it is often not known a priori\nwhether one's budget will be large or small; thus, the optimal choice of\nlearning rate schedule is made on a case-by-case basis. In this paper, we frame\nthe learning rate schedule selection problem as a combination of $i)$ selecting\na profile (i.e., the continuous function that models the learning rate\nschedule), and $ii)$ choosing a sampling rate (i.e., how frequently the\nlearning rate is updated/sampled from this profile). We propose a novel profile\nand sampling rate combination called the Reflected Exponential (REX) schedule,\nwhich we evaluate across seven different experimental settings with both SGD\nand Adam optimizers. REX outperforms the linear schedule in the low budget\nregime, while matching or exceeding the performance of several state-of-the-art\nlearning rate schedules (linear, step, exponential, cosine, step decay on\nplateau, and OneCycle) in both high and low budget regimes. Furthermore, REX\nrequires no added computation, storage, or hyperparameters.",
          "link": "http://arxiv.org/abs/2107.04197",
          "publishedOn": "2021-07-12T01:55:15.807Z",
          "wordCount": 658,
          "title": "REX: Revisiting Budgeted Training with an Improved Schedule. (arXiv:2107.04197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04235",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schulze_S/0/1/0/all/0/1\">S&#xf6;ren Schulze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leuschner_J/0/1/0/all/0/1\">Johannes Leuschner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_E/0/1/0/all/0/1\">Emily J. King</a>",
          "description": "We propose a method for the blind separation of sounds of musical instruments\nin audio signals. We describe the individual tones via a parametric model,\ntraining a dictionary to capture the relative amplitudes of the harmonics. The\nmodel parameters are predicted via a U-Net, which is a type of deep neural\nnetwork. The network is trained without ground truth information, based on the\ndifference between the model prediction and the individual STFT time frames.\nSince some of the model parameters do not yield a useful backpropagation\ngradient, we model them stochastically and employ the policy gradient instead.\nTo provide phase information and account for inaccuracies in the\ndictionary-based representation, we also let the network output a direct\nprediction, which we then use to resynthesize the audio signals for the\nindividual instruments. Due to the flexibility of the neural network,\ninharmonicity can be incorporated seamlessly and no preprocessing of the input\nspectra is required. Our algorithm yields high-quality separation results with\nparticularly low interference on a variety of different audio samples, both\nacoustic and synthetic, provided that the sample contains enough data for the\ntraining and that the spectral characteristics of the musical instruments are\nsufficiently stable to be approximated by the dictionary.",
          "link": "http://arxiv.org/abs/2107.04235",
          "publishedOn": "2021-07-12T01:55:15.800Z",
          "wordCount": 662,
          "title": "Training a Deep Neural Network via Policy Gradients for Blind Source Separation in Polyphonic Music Recordings. (arXiv:2107.04235v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_K/0/1/0/all/0/1\">Kritika Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1\">Andrew Trask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "In recent years, formal methods of privacy protection such as differential\nprivacy (DP), capable of deployment to data-driven tasks such as machine\nlearning (ML), have emerged. Reconciling large-scale ML with the closed-form\nreasoning required for the principled analysis of individual privacy loss\nrequires the introduction of new tools for automatic sensitivity analysis and\nfor tracking an individual's data and their features through the flow of\ncomputation. For this purpose, we introduce a novel \\textit{hybrid} automatic\ndifferentiation (AD) system which combines the efficiency of reverse-mode AD\nwith an ability to obtain a closed-form expression for any given quantity in\nthe computational graph. This enables modelling the sensitivity of arbitrary\ndifferentiable function compositions, such as the training of neural networks\non private data. We demonstrate our approach by analysing the individual DP\nguarantees of statistical database queries. Moreover, we investigate the\napplication of our technique to the training of DP neural networks. Our\napproach can enable the principled reasoning about privacy loss in the setting\nof data processing, and further the development of automatic sensitivity\nanalysis and privacy budgeting systems.",
          "link": "http://arxiv.org/abs/2107.04265",
          "publishedOn": "2021-07-12T01:55:15.780Z",
          "wordCount": 646,
          "title": "Sensitivity analysis in differentially private machine learning using hybrid automatic differentiation. (arXiv:2107.04265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "We show that differentially private stochastic gradient descent (DP-SGD) can\nyield poorly calibrated, overconfident deep learning models. This represents a\nserious issue for safety-critical applications, e.g. in medical diagnosis. We\nhighlight and exploit parallels between stochastic gradient Langevin dynamics,\na scalable Bayesian inference technique for training deep neural networks, and\nDP-SGD, in order to train differentially private, Bayesian neural networks with\nminor adjustments to the original (DP-SGD) algorithm. Our approach provides\nconsiderably more reliable uncertainty estimates than DP-SGD, as demonstrated\nempirically by a reduction in expected calibration error (MNIST $\\sim{5}$-fold,\nPediatric Pneumonia Dataset $\\sim{2}$-fold).",
          "link": "http://arxiv.org/abs/2107.04296",
          "publishedOn": "2021-07-12T01:55:15.773Z",
          "wordCount": 563,
          "title": "Differentially private training of neural networks with Langevin dynamics forcalibrated predictive uncertainty. (arXiv:2107.04296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhenghao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiadong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>",
          "description": "Safe exploration is crucial for the real-world application of reinforcement\nlearning (RL). Previous works consider the safe exploration problem as\nConstrained Markov Decision Process (CMDP), where the policies are being\noptimized under constraints. However, when encountering any potential dangers,\nhuman tends to stop immediately and rarely learns to behave safely in danger.\nMotivated by human learning, we introduce a new approach to address safe RL\nproblems under the framework of Early Terminated MDP (ET-MDP). We first define\nthe ET-MDP as an unconstrained MDP with the same optimal value function as its\ncorresponding CMDP. An off-policy algorithm based on context models is then\nproposed to solve the ET-MDP, which thereby solves the corresponding CMDP with\nbetter asymptotic performance and improved learning efficiency. Experiments on\nvarious CMDP tasks show a substantial improvement over previous methods that\ndirectly solve CMDP.",
          "link": "http://arxiv.org/abs/2107.04200",
          "publishedOn": "2021-07-12T01:55:15.765Z",
          "wordCount": 571,
          "title": "Safe Exploration by Solving Early Terminated MDP. (arXiv:2107.04200v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1\">Jacob Donley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tourbabin_V/0/1/0/all/0/1\">Vladimir Tourbabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jung-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broyles_M/0/1/0/all/0/1\">Mark Broyles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1\">Vamsi Krishna Ithapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehra_R/0/1/0/all/0/1\">Ravish Mehra</a>",
          "description": "Augmented Reality (AR) as a platform has the potential to facilitate the\nreduction of the cocktail party effect. Future AR headsets could potentially\nleverage information from an array of sensors spanning many different\nmodalities. Training and testing signal processing and machine learning\nalgorithms on tasks such as beam-forming and speech enhancement require high\nquality representative data. To the best of the author's knowledge, as of\npublication there are no available datasets that contain synchronized\negocentric multi-channel audio and video with dynamic movement and\nconversations in a noisy environment. In this work, we describe, evaluate and\nrelease a dataset that contains over 5 hours of multi-modal data useful for\ntraining and testing algorithms for the application of improving conversations\nfor an AR glasses wearer. We provide speech intelligibility, quality and\nsignal-to-noise ratio improvement results for a baseline method and show\nimprovements across all tested metrics. The dataset we are releasing contains\nAR glasses egocentric multi-channel microphone array audio, wide field-of-view\nRGB video, speech source pose, headset microphone audio, annotated voice\nactivity, speech transcriptions, head bounding boxes, target of speech and\nsource identification labels. We have created and are releasing this dataset to\nfacilitate research in multi-modal AR solutions to the cocktail party problem.",
          "link": "http://arxiv.org/abs/2107.04174",
          "publishedOn": "2021-07-12T01:55:15.758Z",
          "wordCount": 683,
          "title": "EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments. (arXiv:2107.04174v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sanket Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1\">Pau Riba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>",
          "description": "One of the major prerequisites for any deep learning approach is the\navailability of large-scale training data. When dealing with scanned document\nimages in real world scenarios, the principal information of its content is\nstored in the layout itself. In this work, we have proposed an automated deep\ngenerative model using Graph Neural Networks (GNNs) to generate synthetic data\nwith highly variable and plausible document layouts that can be used to train\ndocument interpretation systems, in this case, specially in digital mailroom\napplications. It is also the first graph-based approach for document layout\ngeneration task experimented on administrative document images, in this case,\ninvoices.",
          "link": "http://arxiv.org/abs/2107.04357",
          "publishedOn": "2021-07-12T01:55:15.751Z",
          "wordCount": 549,
          "title": "Graph-based Deep Generative Modelling for Document Layout Generation. (arXiv:2107.04357v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantinou_A/0/1/0/all/0/1\">Anthony C. Constantinou</a>",
          "description": "Learning from data that contain missing values represents a common phenomenon\nin many domains. Relatively few Bayesian Network structure learning algorithms\naccount for missing data, and those that do tend to rely on standard approaches\nthat assume missing data are missing at random, such as the\nExpectation-Maximisation algorithm. Because missing data are often systematic,\nthere is a need for more pragmatic methods that can effectively deal with data\nsets containing missing values not missing at random. The absence of approaches\nthat deal with systematic missing data impedes the application of BN structure\nlearning methods to real-world problems where missingness are not random. This\npaper describes three variants of greedy search structure learning that utilise\npairwise deletion and inverse probability weighting to maximally leverage the\nobserved data and to limit potential bias caused by missing values. The first\ntwo of the variants can be viewed as sub-versions of the third and best\nperforming variant, but are important in their own in illustrating the\nsuccessive improvements in learning accuracy. The empirical investigations show\nthat the proposed approach outperforms the commonly used and state-of-the-art\nStructural EM algorithm, both in terms of learning accuracy and efficiency, as\nwell as both when data are missing at random and not at random.",
          "link": "http://arxiv.org/abs/2107.04184",
          "publishedOn": "2021-07-12T01:55:15.732Z",
          "wordCount": 637,
          "title": "Greedy structure learning from data that contains systematic missing values. (arXiv:2107.04184v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1\">Tales Imbiriba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Closas_P/0/1/0/all/0/1\">Pau Closas</a>",
          "description": "Localization and tracking of objects using data-driven methods is a popular\ntopic due to the complexity in characterizing the physics of wireless channel\npropagation models. In these modeling approaches, data needs to be gathered to\naccurately train models, at the same time that user's privacy is maintained. An\nappealing scheme to cooperatively achieve these goals is known as Federated\nLearning (FL). A challenge in FL schemes is the presence of non-independent and\nidentically distributed (non-IID) data, caused by unevenly exploration of\ndifferent areas. In this paper, we consider the use of recent FL schemes to\ntrain a set of personalized models that are then optimally fused through\nBayesian rules, which makes it appropriate in the context of indoor\nlocalization.",
          "link": "http://arxiv.org/abs/2107.04189",
          "publishedOn": "2021-07-12T01:55:15.716Z",
          "wordCount": 557,
          "title": "Personalized Federated Learning over non-IID Data for Indoor Localization. (arXiv:2107.04189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1\">Fu-Shun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shang-Ran Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chien-Wen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Chieh Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan-Ren Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Feipei Lai</a>",
          "description": "We previously established a large lung sound database, HF_Lung_V2 (Lung_V2).\nWe trained convolutional-bidirectional gated recurrent unit (CNN-BiGRU)\nnetworks for detecting inhalation, exhalation, continuous adventitious sound\n(CAS) and discontinuous adventitious sound at the recording level on the basis\nof Lung_V2. However, the performance of CAS detection was poor due to many\nreasons, one of which is the highly diversified CAS patterns. To make the\noriginal CNN-BiGRU model learn the CAS patterns more effectively and not cause\ntoo much computing burden, three strategies involving minimal modifications of\nthe network architecture of the CNN layers were investigated: (1) making the\nCNN layers a bit deeper by using the residual blocks, (2) making the CNN layers\na bit wider by increasing the number of CNN kernels, and (3) separating the\nfeature input into multiple paths (the model was denoted by Multi-path\nCNN-BiGRU). The performance of CAS segment and event detection were evaluated.\nResults showed that improvement in CAS detection was observed among all the\nproposed architecture-modified models. The F1 score for CAS event detection of\nthe proposed models increased from 0.445 to 0.491-0.530, which was deemed\nsignificant. However, the Multi-path CNN-BiGRU model outperformed the other\nmodels in terms of the number of winning titles (five) in total nine evaluation\nmetrics. In addition, the Multi-path CNN-BiGRU model did not cause extra\ncomputing burden (0.97-fold inference time) compared to the original CNN-BiGRU\nmodel. Conclusively, the Multi-path CNN layers can efficiently improve the\neffectiveness of feature extraction and subsequently result in better CAS\ndetection.",
          "link": "http://arxiv.org/abs/2107.04226",
          "publishedOn": "2021-07-12T01:55:15.710Z",
          "wordCount": 711,
          "title": "Multi-path Convolutional Neural Networks Efficiently Improve Feature Extraction in Continuous Adventitious Lung Sound Detection. (arXiv:2107.04226v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04154",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1\">Vimal Manohar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Frank Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singhal_N/0/1/0/all/0/1\">Nayan Singhal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_J/0/1/0/all/0/1\">Julian Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_F/0/1/0/all/0/1\">Fuchun Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seltzer_M/0/1/0/all/0/1\">Mike Seltzer</a>",
          "description": "Hybrid automatic speech recognition (ASR) models are typically sequentially\ntrained with CTC or LF-MMI criteria. However, they have vastly different\nlegacies and are usually implemented in different frameworks. In this paper, by\ndecoupling the concepts of modeling units and label topologies and building\nproper numerator/denominator graphs accordingly, we establish a generalized\nframework for hybrid acoustic modeling (AM). In this framework, we show that\nLF-MMI is a powerful training criterion applicable to both limited-context and\nfull-context models, for wordpiece/mono-char/bi-char/chenone units, with both\nHMM/CTC topologies. From this framework, we propose three novel training\nschemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with\ndifferent advantages in training performance, decoding efficiency and decoding\ntime-stamp accuracy. The advantages of different training schemes are evaluated\ncomprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated\non two real world ASR tasks to show their effectiveness. Besides, we also show\nbi-char(bc) HMM-MMI models can serve as better alignment models than\ntraditional non-neural GMM-HMMs.",
          "link": "http://arxiv.org/abs/2107.04154",
          "publishedOn": "2021-07-12T01:55:15.687Z",
          "wordCount": 618,
          "title": "On lattice-free boosted MMI training of HMM and CTC-based full-context ASR models. (arXiv:2107.04154v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Padidar_M/0/1/0/all/0/1\">Misha Padidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinran Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Leo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">Jacob R. Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bindel_D/0/1/0/all/0/1\">David Bindel</a>",
          "description": "Gaussian processes with derivative information are useful in many settings\nwhere derivative information is available, including numerous Bayesian\noptimization and regression tasks that arise in the natural sciences.\nIncorporating derivative observations, however, comes with a dominating\n$O(N^3D^3)$ computational cost when training on $N$ points in $D$ input\ndimensions. This is intractable for even moderately sized problems. While\nrecent work has addressed this intractability in the low-$D$ setting, the\nhigh-$N$, high-$D$ setting is still unexplored and of great value, particularly\nas machine learning problems increasingly become high dimensional. In this\npaper, we introduce methods to achieve fully scalable Gaussian process\nregression with derivatives using variational inference. Analogous to the use\nof inducing values to sparsify the labels of a training set, we introduce the\nconcept of inducing directional derivatives to sparsify the partial derivative\ninformation of a training set. This enables us to construct a variational\nposterior that incorporates derivative information but whose size depends\nneither on the full dataset size $N$ nor the full dimensionality $D$. We\ndemonstrate the full scalability of our approach on a variety of tasks, ranging\nfrom a high dimensional stellarator fusion regression task to training graph\nconvolutional neural networks on Pubmed using Bayesian optimization.\nSurprisingly, we find that our approach can improve regression performance even\nin settings where only label data is available.",
          "link": "http://arxiv.org/abs/2107.04061",
          "publishedOn": "2021-07-12T01:55:15.652Z",
          "wordCount": 661,
          "title": "Scaling Gaussian Processes with Derivative Information Using Variational Inference. (arXiv:2107.04061v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04050",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pasztor_B/0/1/0/all/0/1\">Barna Pasztor</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bogunovic_I/0/1/0/all/0/1\">Ilija Bogunovic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "Learning in multi-agent systems is highly challenging due to the inherent\ncomplexity introduced by agents' interactions. We tackle systems with a huge\npopulation of interacting agents (e.g., swarms) via Mean-Field Control (MFC).\nMFC considers an asymptotically infinite population of identical agents that\naim to collaboratively maximize the collective reward. Specifically, we\nconsider the case of unknown system dynamics where the goal is to\nsimultaneously optimize for the rewards and learn from experience. We propose\nan efficient model-based reinforcement learning algorithm\n$\\text{M}^3\\text{-UCRL}$ that runs in episodes and provably solves this\nproblem. $\\text{M}^3\\text{-UCRL}$ uses upper-confidence bounds to balance\nexploration and exploitation during policy learning. Our main theoretical\ncontributions are the first general regret bounds for model-based RL for MFC,\nobtained via a novel mean-field type analysis. $\\text{M}^3\\text{-UCRL}$ can be\ninstantiated with different models such as neural networks or Gaussian\nProcesses, and effectively combined with neural network policy learning. We\nempirically demonstrate the convergence of $\\text{M}^3\\text{-UCRL}$ on the\nswarm motion problem of controlling an infinite population of agents seeking to\nmaximize location-dependent reward and avoid congested areas.",
          "link": "http://arxiv.org/abs/2107.04050",
          "publishedOn": "2021-07-12T01:55:15.644Z",
          "wordCount": 616,
          "title": "Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2107.04050v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Grzegorz Dudek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelka_P/0/1/0/all/0/1\">Pawe&#x142; Pe&#x142;ka</a>",
          "description": "In this work, we propose an ensemble forecasting approach based on randomized\nneural networks. Improved randomized learning streamlines the fitting abilities\nof individual learners by generating network parameters in accordance with the\ndata and target function features. A pattern-based representation of time\nseries makes the proposed approach suitable for forecasting time series with\nmultiple seasonality. We propose six strategies for controlling the diversity\nof ensemble members. Case studies conducted on four real-world forecasting\nproblems verified the effectiveness and superior performance of the proposed\nensemble forecasting approach. It outperformed statistical models as well as\nstate-of-the-art machine learning models in terms of forecasting accuracy. The\nproposed approach has several advantages: fast and easy training, simple\narchitecture, ease of implementation, high accuracy and the ability to deal\nwith nonstationarity and multiple seasonality in time series.",
          "link": "http://arxiv.org/abs/2107.04091",
          "publishedOn": "2021-07-12T01:55:15.600Z",
          "wordCount": 573,
          "title": "Ensembles of Randomized NNs for Pattern-based Time Series Forecasting. (arXiv:2107.04091v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geffner_T/0/1/0/all/0/1\">Tomas Geffner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domke_J/0/1/0/all/0/1\">Justin Domke</a>",
          "description": "Given an unnormalized target distribution we want to obtain approximate\nsamples from it and a tight lower bound on its (log) normalization constant log\nZ. Annealed Importance Sampling (AIS) with Hamiltonian MCMC is a powerful\nmethod that can be used to do this. Its main drawback is that it uses\nnon-differentiable transition kernels, which makes tuning its many parameters\nhard. We propose a framework to use an AIS-like procedure with Uncorrected\nHamiltonian MCMC, called Uncorrected Hamiltonian Annealing. Our method leads to\ntight and differentiable lower bounds on log Z. We show empirically that our\nmethod yields better performances than other competing approaches, and that the\nability to tune its parameters using reparameterization gradients may lead to\nlarge performance improvements.",
          "link": "http://arxiv.org/abs/2107.04150",
          "publishedOn": "2021-07-12T01:55:15.591Z",
          "wordCount": 551,
          "title": "MCMC Variational Inference via Uncorrected Hamiltonian Annealing. (arXiv:2107.04150v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04062",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zettler_N/0/1/0/all/0/1\">Nico Zettler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mastmeyer_A/0/1/0/all/0/1\">Andre Mastmeyer</a>",
          "description": "A two-step concept for 3D segmentation on 5 abdominal organs inside\nvolumetric CT images is presented. First each relevant organ's volume of\ninterest is extracted as bounding box. The extracted volume acts as input for a\nsecond stage, wherein two compared U-Nets with different architectural\ndimensions re-construct an organ segmentation as label mask. In this work, we\nfocus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results\nindicate Dice improvements of about 6\\% at maximum. In this study to our\nsurprise, liver and kidneys for instance were tackled significantly better\nusing the faster and GPU-memory saving 2D U-Nets. For other abdominal key\norgans, there were no significant differences, but we observe highly\nsignificant advantages for the 2D U-Net in terms of GPU computational efforts\nfor all organs under study.",
          "link": "http://arxiv.org/abs/2107.04062",
          "publishedOn": "2021-07-12T01:55:15.578Z",
          "wordCount": 610,
          "title": "Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT images. (arXiv:2107.04062v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1\">Mohammad Javad Shafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_E/0/1/0/all/0/1\">Ellick Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "The fine-grained relationship between form and function with respect to deep\nneural network architecture design and hardware-specific acceleration is one\narea that is not well studied in the research literature, with form often\ndictated by accuracy as opposed to hardware function. In this study, a\ncomprehensive empirical exploration is conducted to investigate the impact of\ndeep neural network architecture design on the degree of inference speedup that\ncan be achieved via hardware-specific acceleration. More specifically, we\nempirically study the impact of a variety of commonly used macro-architecture\ndesign patterns across different architectural depths through the lens of\nOpenVINO microprocessor-specific and GPU-specific acceleration. Experimental\nresults showed that while leveraging hardware-specific acceleration achieved an\naverage inference speed-up of 380%, the degree of inference speed-up varied\ndrastically depending on the macro-architecture design pattern, with the\ngreatest speedup achieved on the depthwise bottleneck convolution design\npattern at 550%. Furthermore, we conduct an in-depth exploration of the\ncorrelation between FLOPs requirement, level 3 cache efficacy, and network\nlatency with increasing architectural depth and width. Finally, we analyze the\ninference time reductions using hardware-specific acceleration when compared to\nnative deep learning frameworks across a wide variety of hand-crafted deep\nconvolutional neural network architecture designs as well as ones found via\nneural architecture search strategies. We found that the DARTS-derived\narchitecture to benefit from the greatest improvement from hardware-specific\nsoftware acceleration (1200%) while the depthwise bottleneck convolution-based\nMobileNet-V2 to have the lowest overall inference time of around 2.4 ms.",
          "link": "http://arxiv.org/abs/2107.04144",
          "publishedOn": "2021-07-12T01:55:15.547Z",
          "wordCount": 709,
          "title": "Does Form Follow Function? An Empirical Exploration of the Impact of Deep Neural Network Architecture Design on Hardware-Specific Acceleration. (arXiv:2107.04144v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04055",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1\">Haibo Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>",
          "description": "In this paper, a 3D-RegNet-based neural network is proposed for diagnosing\nthe physical condition of patients with coronavirus (Covid-19) infection. In\nthe application of clinical medicine, lung CT images are utilized by\npractitioners to determine whether a patient is infected with coronavirus.\nHowever, there are some laybacks can be considered regarding to this diagnostic\nmethod, such as time consuming and low accuracy. As a relatively large organ of\nhuman body, important spatial features would be lost if the lungs were\ndiagnosed utilizing two dimensional slice image. Therefore, in this paper, a\ndeep learning model with 3D image was designed. The 3D image as input data was\ncomprised of two-dimensional pulmonary image sequence and from which relevant\ncoronavirus infection 3D features were extracted and classified. The results\nshow that the test set of the 3D model, the result: f1 score of 0.8379 and AUC\nvalue of 0.8807 have been achieved.",
          "link": "http://arxiv.org/abs/2107.04055",
          "publishedOn": "2021-07-12T01:55:15.532Z",
          "wordCount": 649,
          "title": "3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image. (arXiv:2107.04055v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04126",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Martin_L/0/1/0/all/0/1\">Lucia Asencio Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garrido_Merchan_E/0/1/0/all/0/1\">Eduardo C. Garrido-Merch&#xe1;n</a>",
          "description": "Some real problems require the evaluation of expensive and noisy objective\nfunctions. Moreover, the analytical expression of these objective functions may\nbe unknown. These functions are known as black-boxes, for example, estimating\nthe generalization error of a machine learning algorithm and computing its\nprediction time in terms of its hyper-parameters. Multi-objective Bayesian\noptimization (MOBO) is a set of methods that has been successfully applied for\nthe simultaneous optimization of black-boxes. Concretely, BO methods rely on a\nprobabilistic model of the objective functions, typically a Gaussian process.\nThis model generates a predictive distribution of the objectives. However, MOBO\nmethods have problems when the number of objectives in a multi-objective\noptimization problem are 3 or more, which is the many objective setting. In\nparticular, the BO process is more costly as more objectives are considered,\ncomputing the quality of the solution via the hyper-volume is also more costly\nand, most importantly, we have to evaluate every objective function, wasting\nexpensive computational, economic or other resources. However, as more\nobjectives are involved in the optimization problem, it is highly probable that\nsome of them are redundant and not add information about the problem solution.\nA measure that represents how similar are GP predictive distributions is\nproposed. We also propose a many objective Bayesian optimization algorithm that\nuses this metric to determine whether two objectives are redundant. The\nalgorithm stops evaluating one of them if the similarity is found, saving\nresources and not hurting the performance of the multi-objective BO algorithm.\nWe show empirical evidence in a set of toy, synthetic, benchmark and real\nexperiments that GPs predictive distributions of the effectiveness of the\nmetric and the algorithm.",
          "link": "http://arxiv.org/abs/2107.04126",
          "publishedOn": "2021-07-12T01:55:15.479Z",
          "wordCount": 707,
          "title": "Many Objective Bayesian Optimization. (arXiv:2107.04126v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bautembach_D/0/1/0/all/0/1\">Dennis Bautembach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomidis_I/0/1/0/all/0/1\">Iason Oikonomidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argyros_A/0/1/0/all/0/1\">Antonis Argyros</a>",
          "description": "We present two novel optimizations that accelerate clock-based spiking neural\nnetwork (SNN) simulators. The first one targets spike timing dependent\nplasticity (STDP). It combines lazy- with event-driven plasticity and\nefficiently facilitates the computation of pre- and post-synaptic spikes using\nbitfields and integer intrinsics. It offers higher bandwidth than event-driven\nplasticity alone and achieves a 1.5x-2x speedup over our closest competitor.\nThe second optimization targets spike delivery. We partition our graph\nrepresentation in a way that bounds the number of neurons that need be updated\nat any given time which allows us to perform said update in shared memory\ninstead of global memory. This is 2x-2.5x faster than our closest competitor.\nBoth optimizations represent the final evolutionary stages of years of\niteration on STDP and spike delivery inside \"Spice\" (/spaIk/), our state of the\nart SNN simulator. The proposed optimizations are not exclusive to our graph\nrepresentation or pipeline but are applicable to a multitude of simulator\ndesigns. We evaluate our performance on three well-established models and\ncompare ourselves against three other state of the art simulators.",
          "link": "http://arxiv.org/abs/2107.04092",
          "publishedOn": "2021-07-12T01:55:15.437Z",
          "wordCount": 636,
          "title": "Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared Atomics. (arXiv:2107.04092v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chaowei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiazhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Huasong Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Houpu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_H/0/1/0/all/0/1\">Huang Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqing Chen</a>",
          "description": "In this paper, we present Fedlearn-Algo, an open-source privacy preserving\nmachine learning platform. We use this platform to demonstrate our research and\ndevelopment results on privacy preserving machine learning algorithms. As the\nfirst batch of novel FL algorithm examples, we release vertical federated\nkernel binary classification model and vertical federated random forest model.\nThey have been tested to be more efficient than existing vertical federated\nlearning models in our practice. Besides the novel FL algorithm examples, we\nalso release a machine communication module. The uniform data transfer\ninterface supports transfering widely used data formats between machines. We\nwill maintain this platform by adding more functional modules and algorithm\nexamples.",
          "link": "http://arxiv.org/abs/2107.04129",
          "publishedOn": "2021-07-12T01:55:15.423Z",
          "wordCount": 562,
          "title": "Fedlearn-Algo: A flexible open-source privacy-preserving machine learning platform. (arXiv:2107.04129v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_S/0/1/0/all/0/1\">Shakeel Ahmad Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahidullah_M/0/1/0/all/0/1\">Md Sahidullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_F/0/1/0/all/0/1\">Fabrice Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouni_S/0/1/0/all/0/1\">Slim Ouni</a>",
          "description": "Stuttering is a speech disorder during which the flow of speech is\ninterrupted by involuntary pauses and repetition of sounds. Stuttering\nidentification is an interesting interdisciplinary domain research problem\nwhich involves pathology, psychology, acoustics, and signal processing that\nmakes it hard and complicated to detect. Recent developments in machine and\ndeep learning have dramatically revolutionized speech domain, however minimal\nattention has been given to stuttering identification. This work fills the gap\nby trying to bring researchers together from interdisciplinary fields. In this\npaper, we review comprehensively acoustic features, statistical and deep\nlearning based stuttering/disfluency classification methods. We also present\nseveral challenges and possible future directions.",
          "link": "http://arxiv.org/abs/2107.04057",
          "publishedOn": "2021-07-12T01:55:15.363Z",
          "wordCount": 557,
          "title": "Machine Learning for Stuttering Identification: Review, Challenges & Future Directions. (arXiv:2107.04057v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1705.03439",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1\">David M. Blei</a>",
          "description": "A key challenge for modern Bayesian statistics is how to perform scalable\ninference of posterior distributions. To address this challenge, variational\nBayes (VB) methods have emerged as a popular alternative to the classical\nMarkov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while\nachieving comparable predictive performance. However, there are few theoretical\nresults around VB. In this paper, we establish frequentist consistency and\nasymptotic normality of VB methods. Specifically, we connect VB methods to\npoint estimates based on variational approximations, called frequentist\nvariational approximations, and we use the connection to prove a variational\nBernstein-von Mises theorem. The theorem leverages the theoretical\ncharacterizations of frequentist variational approximations to understand\nasymptotic properties of VB. In summary, we prove that (1) the VB posterior\nconverges to the Kullback-Leibler (KL) minimizer of a normal distribution,\ncentered at the truth and (2) the corresponding variational expectation of the\nparameter is consistent and asymptotically normal. As applications of the\ntheorem, we derive asymptotic properties of VB posteriors in Bayesian mixture\nmodels, Bayesian generalized linear mixed models, and Bayesian stochastic block\nmodels. We conduct a simulation study to illustrate these theoretical results.",
          "link": "http://arxiv.org/abs/1705.03439",
          "publishedOn": "2021-07-09T01:58:29.204Z",
          "wordCount": 658,
          "title": "Frequentist Consistency of Variational Bayes. (arXiv:1705.03439v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1\">Akshay Mehra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1\">Jihun Hamm</a>",
          "description": "Unsupervised domain adaptation (UDA) enables cross-domain learning without\ntarget domain labels by transferring knowledge from a labeled source domain\nwhose distribution differs from the target. However, UDA is not always\nsuccessful and several accounts of \"negative transfer\" have been reported in\nthe literature. In this work, we prove a simple lower bound on the target\ndomain error that complements the existing upper bound. Our bound shows the\ninsufficiency of minimizing source domain error and marginal distribution\nmismatch for a guaranteed reduction in the target domain error, due to the\npossible increase of induced labeling function mismatch. This insufficiency is\nfurther illustrated through simple distributions for which the same UDA\napproach succeeds, fails, and may succeed or fail with an equal chance.\nMotivated from this, we propose novel data poisoning attacks to fool UDA\nmethods into learning representations that produce large target domain errors.\nWe evaluate the effect of these attacks on popular UDA methods using benchmark\ndatasets where they have been previously shown to be successful. Our results\nshow that poisoning can significantly decrease the target domain accuracy,\ndropping it to almost 0\\% in some cases, with the addition of only 10\\%\npoisoned data in the source domain. The failure of UDA methods demonstrates the\nlimitations of UDA at guaranteeing cross-domain generalization consistent with\nthe lower bound. Thus, evaluation of UDA methods in adversarial settings such\nas data poisoning can provide a better sense of their robustness in scenarios\nunfavorable for UDA.",
          "link": "http://arxiv.org/abs/2107.03919",
          "publishedOn": "2021-07-09T01:58:29.191Z",
          "wordCount": 675,
          "title": "Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning. (arXiv:2107.03919v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03985",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plakal_M/0/1/0/all/0/1\">Manoj Plakal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tobin_J/0/1/0/all/0/1\">Jimmy Tobin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1\">Jordan R. Green</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>",
          "description": "Automatic classification of disordered speech can provide an objective tool\nfor identifying the presence and severity of speech impairment. Classification\napproaches can also help identify hard-to-recognize speech samples to teach ASR\nsystems about the variable manifestations of impaired speech. Here, we develop\nand compare different deep learning techniques to classify the intelligibility\nof disordered speech on selected phrases. We collected samples from a diverse\nset of 661 speakers with a variety of self-reported disorders speaking 29 words\nor phrases, which were rated by speech-language pathologists for their overall\nintelligibility using a five-point Likert scale. We then evaluated classifiers\ndeveloped using 3 approaches: (1) a convolutional neural network (CNN) trained\nfor the task, (2) classifiers trained on non-semantic speech representations\nfrom CNNs that used an unsupervised objective [1], and (3) classifiers trained\non the acoustic (encoder) embeddings from an ASR system trained on typical\nspeech [2]. We found that the ASR encoder's embeddings considerably outperform\nthe other two on detecting and classifying disordered speech. Further analysis\nshows that the ASR embeddings cluster speech by the spoken phrase, while the\nnon-semantic embeddings cluster speech by speaker. Also, longer phrases are\nmore indicative of intelligibility deficits than single words.",
          "link": "http://arxiv.org/abs/2107.03985",
          "publishedOn": "2021-07-09T01:58:29.177Z",
          "wordCount": 669,
          "title": "Comparing Supervised Models And Learned Speech Representations For Classifying Intelligibility Of Disordered Speech On Selected Phrases. (arXiv:2107.03985v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>",
          "description": "Humans have a strong intuitive understanding of the 3D environment around us.\nThe mental model of the physics in our brain applies to objects of different\nmaterials and enables us to perform a wide range of manipulation tasks that are\nfar beyond the reach of current robots. In this work, we desire to learn models\nfor dynamic 3D scenes purely from 2D visual observations. Our model combines\nNeural Radiance Fields (NeRF) and time contrastive learning with an\nautoencoding framework, which learns viewpoint-invariant 3D-aware scene\nrepresentations. We show that a dynamics model, constructed over the learned\nrepresentation space, enables visuomotor control for challenging manipulation\ntasks involving both rigid bodies and fluids, where the target is specified in\na viewpoint different from what the robot operates on. When coupled with an\nauto-decoding framework, it can even support goal specification from camera\nviewpoints that are outside the training distribution. We further demonstrate\nthe richness of the learned 3D dynamics model by performing future prediction\nand novel view synthesis. Finally, we provide detailed ablation studies\nregarding different system designs and qualitative analysis of the learned\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.04004",
          "publishedOn": "2021-07-09T01:58:29.146Z",
          "wordCount": 632,
          "title": "3D Neural Scene Representations for Visuomotor Control. (arXiv:2107.04004v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sibendu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kunal Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1\">Giuseppe Coviello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1\">Murugan Sankaradas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1\">Oliver Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Y. Charlie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat T. Chakradhar</a>",
          "description": "Complex sensors like video cameras include tens of configurable parameters,\nwhich can be set by end-users to customize the sensors to specific application\nscenarios. Although parameter settings significantly affect the quality of the\nsensor output and the accuracy of insights derived from sensor data, most\nend-users use a fixed parameter setting because they lack the skill or\nunderstanding to appropriately configure these parameters. We propose CamTuner,\nwhich is a system to automatically, and dynamically adapt the complex sensor to\nchanging environments. CamTuner includes two key components. First, a bespoke\nanalytics quality estimator, which is a deep-learning model to automatically\nand continuously estimate the quality of insights from an analytics unit as the\nenvironment around a sensor change. Second, a reinforcement learning (RL)\nmodule, which reacts to the changes in quality, and automatically adjusts the\ncamera parameters to enhance the accuracy of insights. We improve the training\ntime of the RL module by an order of magnitude by designing virtual models to\nmimic essential behavior of the camera: we design virtual knobs that can be set\nto different values to mimic the effects of assigning different values to the\ncamera's configurable parameters, and we design a virtual camera model that\nmimics the output from a video camera at different times of the day. These\nvirtual models significantly accelerate training because (a) frame rates from a\nreal camera are limited to 25-30 fps while the virtual models enable processing\nat 300 fps, (b) we do not have to wait until the real camera sees different\nenvironments, which could take weeks or months, and (c) virtual knobs can be\nupdated instantly, while it can take 200-500 ms to change the camera parameter\nsettings. Our dynamic tuning approach results in up to 12% improvement in the\naccuracy of insights from several video analytics tasks.",
          "link": "http://arxiv.org/abs/2107.03964",
          "publishedOn": "2021-07-09T01:58:29.130Z",
          "wordCount": 751,
          "title": "CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics. (arXiv:2107.03964v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_A/0/1/0/all/0/1\">Alexander Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nosovskiy_G/0/1/0/all/0/1\">Gleb Nosovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chekunov_A/0/1/0/all/0/1\">Alexey Chekunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedoseev_D/0/1/0/all/0/1\">Denis Fedoseev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kibkalo_V/0/1/0/all/0/1\">Vladislav Kibkalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikulin_M/0/1/0/all/0/1\">Mikhail Nikulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popelenskiy_F/0/1/0/all/0/1\">Fedor Popelenskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komkov_S/0/1/0/all/0/1\">Stepan Komkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurenko_I/0/1/0/all/0/1\">Ivan Mazurenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1\">Aleksandr Petiushko</a>",
          "description": "Manifold hypothesis states that data points in high-dimensional space\nactually lie in close vicinity of a manifold of much lower dimension. In many\ncases this hypothesis was empirically verified and used to enhance unsupervised\nand semi-supervised learning. Here we present new approach to manifold\nhypothesis checking and underlying manifold dimension estimation. In order to\ndo it we use two very different methods simultaneously - one geometric, another\nprobabilistic - and check whether they give the same result. Our geometrical\nmethod is a modification for sparse data of a well-known box-counting algorithm\nfor Minkowski dimension calculation. The probabilistic method is new. Although\nit exploits standard nearest neighborhood distance, it is different from\nmethods which were previously used in such situations. This method is robust,\nfast and includes special preliminary data transformation. Experiments on real\ndatasets show that the suggested approach based on two methods combination is\npowerful and effective.",
          "link": "http://arxiv.org/abs/2107.03903",
          "publishedOn": "2021-07-09T01:58:29.096Z",
          "wordCount": 603,
          "title": "Manifold Hypothesis in Data Analysis: Double Geometrically-Probabilistic Approach to Manifold Dimension Estimation. (arXiv:2107.03903v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huazhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "We propose to address quadrupedal locomotion tasks using Reinforcement\nLearning (RL) with a Transformer-based model that learns to combine\nproprioceptive information and high-dimensional depth sensor inputs. While\nlearning-based locomotion has made great advances using RL, most methods still\nrely on domain randomization for training blind agents that generalize to\nchallenging terrains. Our key insight is that proprioceptive states only offer\ncontact measurements for immediate reaction, whereas an agent equipped with\nvisual sensory observations can learn to proactively maneuver environments with\nobstacles and uneven terrain by anticipating changes in the environment many\nsteps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL\nmethod for quadrupedal locomotion that leverages a Transformer-based model for\nfusing proprioceptive states and visual observations. We evaluate our method in\nchallenging simulated environments with different obstacles and uneven terrain.\nWe show that our method obtains significant improvements over policies with\nonly proprioceptive state inputs, and that Transformer-based models further\nimprove generalization across environments. Our project page with videos is at\nhttps://RchalYang.github.io/LocoTransformer .",
          "link": "http://arxiv.org/abs/2107.03996",
          "publishedOn": "2021-07-09T01:58:29.088Z",
          "wordCount": 619,
          "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blinov_P/0/1/0/all/0/1\">Pavel Blinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>",
          "description": "The paper researches the problem of concept and patient representations in\nthe medical domain. We present the patient histories from Electronic Health\nRecords (EHRs) as temporal sequences of ICD concepts for which embeddings are\nlearned in an unsupervised setup with a transformer-based neural network model.\nThe model training was performed on the collection of one million patients'\nhistories in 6 years. The predictive power of such a model is assessed in\ncomparison with several baseline methods. A series of experiments on the\nMIMIC-III data show the advantage of the presented model compared to a similar\nsystem. Further, we analyze the obtained embedding space with regards to\nconcept relations and show how knowledge from the medical domain can be\nsuccessfully transferred to the practical task of insurance scoring in the form\nof patient embeddings.",
          "link": "http://arxiv.org/abs/2107.03913",
          "publishedOn": "2021-07-09T01:58:29.082Z",
          "wordCount": 564,
          "title": "Patient Embeddings in Healthcare and Insurance Applications. (arXiv:2107.03913v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zipeng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>",
          "description": "Successful real-world deployment of legged robots would require them to adapt\nin real-time to unseen scenarios like changing terrains, changing payloads,\nwear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to\nsolve this problem of real-time online adaptation in quadruped robots. RMA\nconsists of two components: a base policy and an adaptation module. The\ncombination of these components enables the robot to adapt to novel situations\nin fractions of a second. RMA is trained completely in simulation without using\nany domain knowledge like reference trajectories or predefined foot trajectory\ngenerators and is deployed on the A1 robot without any fine-tuning. We train\nRMA on a varied terrain generator using bioenergetics-inspired rewards and\ndeploy it on a variety of difficult terrains including rocky, slippery,\ndeformable surfaces in environments with grass, long vegetation, concrete,\npebbles, stairs, sand, etc. RMA shows state-of-the-art performance across\ndiverse real-world as well as simulation experiments. Video results at\nhttps://ashish-kmr.github.io/rma-legged-robots/",
          "link": "http://arxiv.org/abs/2107.04034",
          "publishedOn": "2021-07-09T01:58:29.075Z",
          "wordCount": 606,
          "title": "RMA: Rapid Motor Adaptation for Legged Robots. (arXiv:2107.04034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03901",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1\">Sean Walsh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gkontra_P/0/1/0/all/0/1\">Polyxeni Gkontra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Deep learning models can enable accurate and efficient disease diagnosis, but\nhave thus far been hampered by the data scarcity present in the medical world.\nAutomated diagnosis studies have been constrained by underpowered single-center\ndatasets, and although some results have shown promise, their generalizability\nto other institutions remains questionable as the data heterogeneity between\ninstitutions is not taken into account. By allowing models to be trained in a\ndistributed manner that preserves patients' privacy, federated learning\npromises to alleviate these issues, by enabling diligent multi-center studies.\nWe present the first federated learning study on the modality of cardiovascular\nmagnetic resonance (CMR) and use four centers derived from subsets of the M\\&M\nand ACDC datasets, focusing on the diagnosis of hypertrophic cardiomyopathy\n(HCM). We adapt a 3D-CNN network pretrained on action recognition and explore\ntwo different ways of incorporating shape prior information to the model, and\nfour different data augmentation set-ups, systematically analyzing their impact\non the different collaborative learning choices. We show that despite the small\nsize of data (180 subjects derived from four centers), the privacy preserving\nfederated learning achieves promising results that are competitive with\ntraditional centralized learning. We further find that federatively trained\nmodels exhibit increased robustness and are more sensitive to domain shift\neffects.",
          "link": "http://arxiv.org/abs/2107.03901",
          "publishedOn": "2021-07-09T01:58:29.068Z",
          "wordCount": 675,
          "title": "Federated Learning for Multi-Center Imaging Diagnostics: A Study in Cardiovascular Disease. (arXiv:2107.03901v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+NOR_A/0/1/0/all/0/1\">Ahmad Kamal BIN MOHD NOR</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PEDAPATI_S/0/1/0/all/0/1\">Srinivasa Rao PEDAPATI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MUHAMMAD_M/0/1/0/all/0/1\">Masdi MUHAMMAD</a>",
          "description": "A state-of-the-art systematic review on XAI applied to Prognostic and Health\nManagement (PHM) of industrial asset is presented. The work attempts to provide\nan overview of the general trend of XAI in PHM, answers the question of\naccuracy versus explainability, investigates the extent of human role,\nexplainability evaluation and uncertainty management in PHM XAI. Research\narticles linked to PHM XAI, in English language, from 2015 to 2021 are selected\nfrom IEEE Xplore, ScienceDirect, SpringerLink, ACM Digital Library and Scopus\ndatabases using PRISMA guidelines. Data was extracted from 35 selected articles\nand examined using MS. Excel. Several findings were synthesized. Firstly, while\nthe discipline is still young, the analysis indicates the growing acceptance of\nXAI in PHM domain. Secondly, XAI functions as a double edge sword, where it is\nassimilated as a tool to execute PHM tasks as well as a mean of explanation, in\nparticular in diagnostic and anomaly detection. There is thus a need for XAI in\nPHM. Thirdly, the review shows that PHM XAI papers produce either good or\nexcellent results in general, suggesting that PHM performance is unaffected by\nXAI. Fourthly, human role, explainability metrics and uncertainty management\nare areas requiring further attention by the PHM community. Adequate\nexplainability metrics to cater for PHM need are urgently needed. Finally, most\ncase study featured on the accepted articles are based on real, indicating that\navailable AI and XAI approaches are equipped to solve complex real-world\nchallenges, increasing the confidence of AI model adoption in the industry.\nThis work is funded by the Universiti Teknologi Petronas Foundation.",
          "link": "http://arxiv.org/abs/2107.03869",
          "publishedOn": "2021-07-09T01:58:29.049Z",
          "wordCount": 705,
          "title": "Explainable AI (XAI) for PHM of Industrial Asset: A State-of-The-Art, PRISMA-Compliant Systematic Review. (arXiv:2107.03869v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03383",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Prosperi_M/0/1/0/all/0/1\">Mattia Prosperi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marini_S/0/1/0/all/0/1\">Simone Marini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Boucher_C/0/1/0/all/0/1\">Christina Boucher</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "Whole genome sequencing (WGS) is quickly becoming the customary means for\nidentification of antimicrobial resistance (AMR) due to its ability to obtain\nhigh resolution information about the genes and mechanisms that are causing\nresistance and driving pathogen mobility. By contrast, traditional phenotypic\n(antibiogram) testing cannot easily elucidate such information. Yet development\nof AMR prediction tools from genotype-phenotype data can be biased, since\nsampling is non-randomized. Sample provenience, period of collection, and\nspecies representation can confound the association of genetic traits with AMR.\nThus, prediction models can perform poorly on new data with sampling\ndistribution shifts. In this work -- under an explicit set of causal\nassumptions -- we evaluate the effectiveness of propensity-based rebalancing\nand confounding adjustment on AMR prediction using genotype-phenotype AMR data\nfrom the Pathosystems Resource Integration Center (PATRIC). We select bacterial\ngenotypes (encoded as k-mer signatures, i.e. DNA fragments of length k),\ncountry, year, species, and AMR phenotypes for the tetracycline drug class,\npreparing test data with recent genomes coming from a single country. We test\nboosted logistic regression (BLR) and random forests (RF) with/without\nbias-handling. On 10,936 instances, we find evidence of species, location and\nyear imbalance with respect to the AMR phenotype. The crude versus\nbias-adjusted change in effect of genetic signatures on AMR varies but only\nmoderately (selecting the top 20,000 out of 40+ million k-mers). The area under\nthe receiver operating characteristic (AUROC) of the RF (0.95) is comparable to\nthat of BLR (0.94) on both out-of-bag samples from bootstrap and the external\ntest (n=1,085), where AUROCs do not decrease. We observe a 1%-5% gain in AUROC\nwith bias-handling compared to the sole use of genetic signatures. ...",
          "link": "http://arxiv.org/abs/2107.03383",
          "publishedOn": "2021-07-09T01:58:29.038Z",
          "wordCount": 748,
          "title": "Assessing putative bias in prediction of anti-microbial resistance from real-world genotyping data under explicit causal assumptions. (arXiv:2107.03383v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1\">Weinan E</a>",
          "description": "The generative adversarial network (GAN) is a well-known model for learning\nhigh-dimensional distributions, but the mechanism for its generalization\nability is not understood. In particular, GAN is vulnerable to the memorization\nphenomenon, the eventual convergence to the empirical distribution. We consider\na simplified GAN model with the generator replaced by a density, and analyze\nhow the discriminator contributes to generalization. We show that with early\nstopping, the generalization error measured by Wasserstein metric escapes from\nthe curse of dimensionality, despite that in the long term, memorization is\ninevitable. In addition, we present a hardness of learning result for WGAN.",
          "link": "http://arxiv.org/abs/2107.03633",
          "publishedOn": "2021-07-09T01:58:29.031Z",
          "wordCount": 535,
          "title": "Generalization Error of GAN from the Discriminator's Perspective. (arXiv:2107.03633v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03836",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lazarsfeld_J/0/1/0/all/0/1\">John Lazarsfeld</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Johnson_A/0/1/0/all/0/1\">Aaron Johnson</a>",
          "description": "The Maximal Information Coefficient (MIC) of Reshef et al. (Science, 2011) is\na statistic for measuring dependence between variable pairs in large datasets.\nIn this note, we prove that MIC is a consistent estimator of the corresponding\npopulation statistic MIC$_*$. This corrects an error in an argument of Reshef\net al. (JMLR, 2016), which we describe.",
          "link": "http://arxiv.org/abs/2107.03836",
          "publishedOn": "2021-07-09T01:58:29.024Z",
          "wordCount": 493,
          "title": "Consistency of the Maximal Information Coefficient Estimator. (arXiv:2107.03836v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luong-Ha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulet_J/0/1/0/all/0/1\">James-A. Goulet</a>",
          "description": "With few exceptions, neural networks have been relying on backpropagation and\ngradient descent as the inference engine in order to learn the model\nparameters, because the closed-form Bayesian inference for neural networks has\nbeen considered to be intractable. In this paper, we show how we can leverage\nthe tractable approximate Gaussian inference's (TAGI) capabilities to infer\nhidden states, rather than only using it for inferring the network's\nparameters. One novel aspect it allows is to infer hidden states through the\nimposition of constraints designed to achieve specific objectives, as\nillustrated through three examples: (1) the generation of adversarial-attack\nexamples, (2) the usage of a neural network as a black-box optimization method,\nand (3) the application of inference on continuous-action reinforcement\nlearning. These applications showcase how tasks that were previously reserved\nto gradient-based optimization approaches can now be approached with\nanalytically tractable inference",
          "link": "http://arxiv.org/abs/2107.03759",
          "publishedOn": "2021-07-09T01:58:28.984Z",
          "wordCount": 577,
          "title": "Analytically Tractable Hidden-States Inference in Bayesian Neural Networks. (arXiv:2107.03759v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ancha_S/0/1/0/all/0/1\">Siddharth Ancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_G/0/1/0/all/0/1\">Gaurav Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1\">Srinivasa G. Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>",
          "description": "To safely navigate unknown environments, robots must accurately perceive\ndynamic obstacles. Instead of directly measuring the scene depth with a LiDAR\nsensor, we explore the use of a much cheaper and higher resolution sensor:\nprogrammable light curtains. Light curtains are controllable depth sensors that\nsense only along a surface that a user selects. We use light curtains to\nestimate the safety envelope of a scene: a hypothetical surface that separates\nthe robot from all obstacles. We show that generating light curtains that sense\nrandom locations (from a particular distribution) can quickly discover the\nsafety envelope for scenes with unknown objects. Importantly, we produce\ntheoretical safety guarantees on the probability of detecting an obstacle using\nrandom curtains. We combine random curtains with a machine learning based model\nthat forecasts and tracks the motion of the safety envelope efficiently. Our\nmethod accurately estimates safety envelopes while providing probabilistic\nsafety guarantees that can be used to certify the efficacy of a robot\nperception system to detect and avoid dynamic obstacles. We evaluate our\napproach in a simulated urban driving environment and a real-world environment\nwith moving pedestrians using a light curtain device and show that we can\nestimate safety envelopes efficiently and effectively. Project website:\nhttps://siddancha.github.io/projects/active-safety-envelopes-with-guarantees",
          "link": "http://arxiv.org/abs/2107.04000",
          "publishedOn": "2021-07-09T01:58:28.964Z",
          "wordCount": 665,
          "title": "Active Safety Envelopes using Light Curtains with Probabilistic Guarantees. (arXiv:2107.04000v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vartholomaios_A/0/1/0/all/0/1\">Argyrios Vartholomaios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlos_S/0/1/0/all/0/1\">Stamatis Karlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouloumpris_E/0/1/0/all/0/1\">Eleftherios Kouloumpris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>",
          "description": "Energy production using renewable sources exhibits inherent uncertainties due\nto their intermittent nature. Nevertheless, the unified European energy market\npromotes the increasing penetration of renewable energy sources (RES) by the\nregional energy system operators. Consequently, RES forecasting can assist in\nthe integration of these volatile energy sources, since it leads to higher\nreliability and reduced ancillary operational costs for power systems. This\npaper presents a new dataset for solar and wind energy generation forecast in\nGreece and introduces a feature engineering pipeline that enriches the\ndimensional space of the dataset. In addition, we propose a novel method that\nutilizes the innovative Prophet model, an end-to-end forecasting tool that\nconsiders several kinds of nonlinear trends in decomposing the energy time\nseries before a tree-based ensemble provides short-term predictions. The\nperformance of the system is measured through representative evaluation\nmetrics, and by estimating the model's generalization under an industryprovided\nscheme of absolute error thresholds. The proposed hybrid model competes with\nbaseline persistence models, tree-based regression ensembles, and the Prophet\nmodel, managing to outperform them, presenting both lower error rates and more\nfavorable error distribution.",
          "link": "http://arxiv.org/abs/2107.03825",
          "publishedOn": "2021-07-09T01:58:28.957Z",
          "wordCount": 628,
          "title": "Short-term Renewable Energy Forecasting in Greece using Prophet Decomposition and Tree-based Ensembles. (arXiv:2107.03825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byungsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hangyeol Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Dongmin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Youngduck Choi</a>",
          "description": "The needs for precisely estimating a student's academic performance have been\nemphasized with an increasing amount of attention paid to Intelligent Tutoring\nSystem (ITS). However, since labels for academic performance, such as test\nscores, are collected from outside of ITS, obtaining the labels is costly,\nleading to label-scarcity problem which brings challenge in taking machine\nlearning approaches for academic performance prediction. To this end, inspired\nby the recent advancement of pre-training method in natural language processing\ncommunity, we propose DPA, a transfer learning framework with Discriminative\nPre-training tasks for Academic performance prediction. DPA pre-trains two\nmodels, a generator and a discriminator, and fine-tunes the discriminator on\nacademic performance prediction. In DPA's pre-training phase, a sequence of\ninteractions where some tokens are masked is provided to the generator which is\ntrained to reconstruct the original sequence. Then, the discriminator takes an\ninteraction sequence where the masked tokens are replaced by the generator's\noutputs, and is trained to predict the originalities of all tokens in the\nsequence. Compared to the previous state-of-the-art generative pre-training\nmethod, DPA is more sample efficient, leading to fast convergence to lower\nacademic performance prediction error. We conduct extensive experimental\nstudies on a real-world dataset obtained from a multi-platform ITS application\nand show that DPA outperforms the previous state-of-the-art generative\npre-training method with a reduction of 4.05% in mean absolute error and more\nrobust to increased label-scarcity.",
          "link": "http://arxiv.org/abs/2107.04009",
          "publishedOn": "2021-07-09T01:58:28.939Z",
          "wordCount": 674,
          "title": "Knowledge Transfer by Discriminative Pre-training for Academic Performance Prediction. (arXiv:2107.04009v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polyvyanyy_A/0/1/0/all/0/1\">Artem Polyvyanyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moffat_A/0/1/0/all/0/1\">Alistair Moffat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Banuelos_L/0/1/0/all/0/1\">Luciano Garc&#xed;a-Ba&#xf1;uelos</a>",
          "description": "Process mining studies ways to derive value from process executions recorded\nin event logs of IT-systems, with process discovery the task of inferring a\nprocess model for an event log emitted by some unknown system. One quality\ncriterion for discovered process models is generalization. Generalization seeks\nto quantify how well the discovered model describes future executions of the\nsystem, and is perhaps the least understood quality criterion in process\nmining. The lack of understanding is primarily a consequence of generalization\nseeking to measure properties over the entire future behavior of the system,\nwhen the only available sample of behavior is that provided by the event log\nitself. In this paper, we draw inspiration from computational statistics, and\nemploy a bootstrap approach to estimate properties of a population based on a\nsample. Specifically, we define an estimator of the model's generalization\nbased on the event log it was discovered from, and then use bootstrapping to\nmeasure the generalization of the model with respect to the system, and its\nstatistical significance. Experiments demonstrate the feasibility of the\napproach in industrial settings.",
          "link": "http://arxiv.org/abs/2107.03876",
          "publishedOn": "2021-07-09T01:58:28.917Z",
          "wordCount": 629,
          "title": "Bootstrapping Generalization of Process Models Discovered From Event Data. (arXiv:2107.03876v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bandi_H/0/1/0/all/0/1\">Hari Bandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1\">Dimitris Bertsimas</a>",
          "description": "Systemic bias with respect to gender, race and ethnicity, often unconscious,\nis prevalent in datasets involving choices among individuals. Consequently,\nsociety has found it challenging to alleviate bias and achieve diversity in a\nway that maintains meritocracy in such settings. We propose (a) a novel\noptimization approach based on optimally flipping outcome labels and training\nclassification models simultaneously to discover changes to be made in the\nselection process so as to achieve diversity without significantly affecting\nmeritocracy, and (b) a novel implementation tool employing optimal\nclassification trees to provide insights on which attributes of individuals\nlead to flipping of their labels, and to help make changes in the current\nselection processes in a manner understandable by human decision makers. We\npresent case studies on three real-world datasets consisting of parole,\nadmissions to the bar and lending decisions, and demonstrate that the price of\ndiversity is low and sometimes negative, that is we can modify our selection\nprocesses in a way that enhances diversity without affecting meritocracy\nsignificantly, and sometimes improving it.",
          "link": "http://arxiv.org/abs/2107.03900",
          "publishedOn": "2021-07-09T01:58:28.910Z",
          "wordCount": 600,
          "title": "The Price of Diversity. (arXiv:2107.03900v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03940",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Butucea_C/0/1/0/all/0/1\">Cristina Butucea</a>, <a href=\"http://arxiv.org/find/math/1/au:+Issartel_Y/0/1/0/all/0/1\">Yann Issartel</a>",
          "description": "We study the problem of estimating non-linear functionals of discrete\ndistributions in the context of local differential privacy. The initial data\n$x_1,\\ldots,x_n \\in [K]$ are supposed i.i.d. and distributed according to an\nunknown discrete distribution $p = (p_1,\\ldots,p_K)$. Only $\\alpha$-locally\ndifferentially private (LDP) samples $z_1,...,z_n$ are publicly available,\nwhere the term 'local' means that each $z_i$ is produced using one individual\nattribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e.\nthey are allowed to use already published confidential data) or\nnon-interactive. We describe the behavior of the quadratic risk for estimating\nthe power sum functional $F_{\\gamma} = \\sum_{k=1}^K p_k^{\\gamma}$, $\\gamma >0$\nas a function of $K, \\, n$ and $\\alpha$. In the non-interactive case, we study\ntwo plug-in type estimators of $F_{\\gamma}$, for all $\\gamma >0$, that are\nsimilar to the MLE analyzed by Jiao et al. (2017) in the multinomial model.\nHowever, due to the privacy constraint the rates we attain are slower and\nsimilar to those obtained in the Gaussian model by Collier et al. (2020). In\nthe interactive case, we introduce for all $\\gamma >1$ a two-step procedure\nwhich attains the faster parametric rate $(n \\alpha^2)^{-1/2}$ when $\\gamma\n\\geq 2$. We give lower bounds results over all $\\alpha$-LDP mechanisms and all\nestimators using the private samples.",
          "link": "http://arxiv.org/abs/2107.03940",
          "publishedOn": "2021-07-09T01:58:28.891Z",
          "wordCount": 655,
          "title": "Locally differentially private estimation of nonlinear functionals of discrete distributions. (arXiv:2107.03940v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03926",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Dolphin_R/0/1/0/all/0/1\">Rian Dolphin</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Smyth_B/0/1/0/all/0/1\">Barry Smyth</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Forecasting stock returns is a challenging problem due to the highly\nstochastic nature of the market and the vast array of factors and events that\ncan influence trading volume and prices. Nevertheless it has proven to be an\nattractive target for machine learning research because of the potential for\neven modest levels of prediction accuracy to deliver significant benefits. In\nthis paper, we describe a case-based reasoning approach to predicting stock\nmarket returns using only historical pricing data. We argue that one of the\nimpediments for case-based stock prediction has been the lack of a suitable\nsimilarity metric when it comes to identifying similar pricing histories as the\nbasis for a future prediction -- traditional Euclidean and correlation based\napproaches are not effective for a variety of reasons -- and in this regard, a\nkey contribution of this work is the development of a novel similarity metric\nfor comparing historical pricing data. We demonstrate the benefits of this\nmetric and the case-based approach in a real-world application in comparison to\na variety of conventional benchmarks.",
          "link": "http://arxiv.org/abs/2107.03926",
          "publishedOn": "2021-07-09T01:58:28.883Z",
          "wordCount": 636,
          "title": "Measuring Financial Time Series Similarity With a View to Identifying Profitable Stock Market Opportunities. (arXiv:2107.03926v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lofqvist_M/0/1/0/all/0/1\">Martina Lofqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1\">Jos&#xe9; Cano</a>",
          "description": "There is a proliferation in the number of satellites launched each year,\nresulting in downlinking of terabytes of data each day. The data received by\nground stations is often unprocessed, making this an expensive process\nconsidering the large data sizes and that not all of the data is useful. This,\ncoupled with the increasing demand for real-time data processing, has led to a\ngrowing need for on-orbit processing solutions. In this work, we investigate\nthe performance of CNN-based object detectors on constrained devices by\napplying different image compression techniques to satellite data. We examine\nthe capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;\nlow-power, high-performance computers, with integrated GPUs, small enough to\nfit on-board a nanosatellite. We take a closer look at object detection\nnetworks, including the Single Shot MultiBox Detector (SSD) and Region-based\nFully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a\nLarge Scale Dataset for Object Detection in Aerial Images. The performance is\nmeasured in terms of execution time, memory consumption, and accuracy, and are\ncompared against a baseline containing a server with two powerful GPUs. The\nresults show that by applying image compression techniques, we are able to\nimprove the execution time and memory consumption, achieving a fully runnable\ndataset. A lossless compression technique achieves roughly a 10% reduction in\nexecution time and about a 3% reduction in memory consumption, with no impact\non the accuracy. While a lossy compression technique improves the execution\ntime by up to 144% and the memory consumption is reduced by as much as 97%.\nHowever, it has a significant impact on accuracy, varying depending on the\ncompression ratio. Thus the application and ratio of these compression\ntechniques may differ depending on the required level of accuracy for a\nparticular task.",
          "link": "http://arxiv.org/abs/2107.03774",
          "publishedOn": "2021-07-09T01:58:28.875Z",
          "wordCount": 777,
          "title": "Optimizing Data Processing in Space for Object Detection in Satellite Imagery. (arXiv:2107.03774v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinhyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1\">Yunze Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>",
          "description": "Point clouds and RGB images are naturally complementary modalities for 3D\nvisual understanding - the former provides sparse but accurate locations of\npoints on objects, while the latter contains dense color and texture\ninformation. Despite this potential for close sensor fusion, many methods train\ntwo models in isolation and use simple feature concatenation to represent 3D\nsensor data. This separated training scheme results in potentially sub-optimal\nperformance and prevents 3D tasks from being used to benefit 2D tasks that are\noften useful on their own. To provide a more integrated approach, we propose a\nnovel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box\nproposals to improve 2D segmentation predictions, which are then used to\nfurther refine the 3D boxes. We show that including a 2D network between two\nstages of 3D modules significantly improves both 2D and 3D task performance.\nMoreover, to prevent the 3D module from over-relying on the overfitted 2D\npredictions, we propose a dual-head 2D segmentation training and inference\nscheme, allowing the 2nd 3D module to learn to interpret imperfect 2D\nsegmentation predictions. Evaluating our model on the challenging SUN RGB-D\ndataset, we improve upon state-of-the-art results of both single modality and\nfusion networks by a large margin ($\\textbf{+3.8}$ mAP@0.5). Code will be\nreleased $\\href{https://github.com/Divadi/MTC_RCNN}{\\text{here.}}$",
          "link": "http://arxiv.org/abs/2107.04013",
          "publishedOn": "2021-07-09T01:58:28.868Z",
          "wordCount": 656,
          "title": "Multi-Modality Task Cascade for 3D Object Detection. (arXiv:2107.04013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biggs_F/0/1/0/all/0/1\">Felix Biggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1\">Benjamin Guedj</a>",
          "description": "We develop a framework for derandomising PAC-Bayesian generalisation bounds\nachieving a margin on training data, relating this process to the\nconcentration-of-measure phenomenon. We apply these tools to linear prediction,\nsingle-hidden-layer neural networks with an unusual erf activation function,\nand deep ReLU networks, obtaining new bounds. The approach is also extended to\nthe idea of \"partial-derandomisation\" where only some layers are derandomised\nand the others are stochastic. This allows empirical evaluation of\nsingle-hidden-layer networks on more complex datasets, and helps bridge the gap\nbetween generalisation bounds for non-stochastic deep networks and those for\nrandomised deep networks as generally examined in PAC-Bayes.",
          "link": "http://arxiv.org/abs/2107.03955",
          "publishedOn": "2021-07-09T01:58:28.861Z",
          "wordCount": 528,
          "title": "On Margins and Derandomisation in PAC-Bayes. (arXiv:2107.03955v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kudari_S/0/1/0/all/0/1\">Shashidhar Veerappa Kudari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunari_A/0/1/0/all/0/1\">Akshaykumar Gunari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamadandi_A/0/1/0/all/0/1\">Adarsh Jamadandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabib_R/0/1/0/all/0/1\">Ramesh Ashok Tabib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudenagudi_U/0/1/0/all/0/1\">Uma Mudenagudi</a>",
          "description": "In this paper, we propose a strategy to mitigate the problem of inefficient\nclustering performance by introducing data augmentation as an auxiliary\nplug-in. Classical clustering techniques such as K-means, Gaussian mixture\nmodel and spectral clustering are central to many data-driven applications.\nHowever, recently unsupervised simultaneous feature learning and clustering\nusing neural networks also known as Deep Embedded Clustering (DEC) has gained\nprominence. Pioneering works on deep feature clustering focus on defining\nrelevant clustering loss function and choosing the right neural network for\nextracting features. A central problem in all these cases is data sparsity\naccompanied by high intra-class and low inter-class variance, which\nsubsequently leads to poor clustering performance and erroneous candidate\nassignments. Towards this, we employ data augmentation techniques to improve\nthe density of the clusters, thus improving the overall performance. We train a\nvariant of Convolutional Autoencoder (CAE) with augmented data to construct the\ninitial feature space as a novel model for deep clustering. We demonstrate the\nresults of proposed strategy on crowdsourced Indian Heritage dataset. Extensive\nexperiments show consistent improvements over existing works.",
          "link": "http://arxiv.org/abs/2107.03852",
          "publishedOn": "2021-07-09T01:58:28.841Z",
          "wordCount": 618,
          "title": "Augmented Data as an Auxiliary Plug-in Towards Categorization of Crowdsourced Heritage Data. (arXiv:2107.03852v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tashiro_Y/0/1/0/all/0/1\">Yusuke Tashiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "The imputation of missing values in time series has many applications in\nhealthcare and finance. While autoregressive models are natural candidates for\ntime series imputation, score-based diffusion models have recently outperformed\nexisting counterparts including autoregressive models in many tasks such as\nimage generation and audio synthesis, and would be promising for time series\nimputation. In this paper, we propose Conditional Score-based Diffusion models\nfor Imputation (CSDI), a novel time series imputation method that utilizes\nscore-based diffusion models conditioned on observed data. Unlike existing\nscore-based approaches, the conditional diffusion model is explicitly trained\nfor imputation and can exploit correlations between observed values. On\nhealthcare and environmental data, CSDI improves by 40-70% over existing\nprobabilistic imputation methods on popular performance metrics. In addition,\ndeterministic imputation by CSDI reduces the error by 5-20% compared to the\nstate-of-the-art deterministic imputation methods. Furthermore, CSDI can also\nbe applied to time series interpolation and probabilistic forecasting, and is\ncompetitive with existing baselines.",
          "link": "http://arxiv.org/abs/2107.03502",
          "publishedOn": "2021-07-09T01:58:28.805Z",
          "wordCount": 595,
          "title": "CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation. (arXiv:2107.03502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaoliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wulong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>",
          "description": "Shift neural networks reduce computation complexity by removing expensive\nmultiplication operations and quantizing continuous weights into low-bit\ndiscrete values, which are fast and energy efficient compared to conventional\nneural networks. However, existing shift networks are sensitive to the weight\ninitialization, and also yield a degraded performance caused by vanishing\ngradient and weight sign freezing problem. To address these issues, we propose\nS low-bit re-parameterization, a novel technique for training low-bit shift\nnetworks. Our method decomposes a discrete parameter in a sign-sparse-shift\n3-fold manner. In this way, it efficiently learns a low-bit network with a\nweight dynamics similar to full-precision networks and insensitive to weight\ninitialization. Our proposed training method pushes the boundaries of shift\nneural networks and shows 3-bit shift networks out-performs their\nfull-precision counterparts in terms of top-1 accuracy on ImageNet.",
          "link": "http://arxiv.org/abs/2107.03453",
          "publishedOn": "2021-07-09T01:58:28.783Z",
          "wordCount": 581,
          "title": "$S^3$: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks. (arXiv:2107.03453v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.17171",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pohjonen_J/0/1/0/all/0/1\">Joona Pohjonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sturenberg_C/0/1/0/all/0/1\">Carolin St&#xfc;renberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rannikko_A/0/1/0/all/0/1\">Antti Rannikko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirtti_T/0/1/0/all/0/1\">Tuomas Mirtti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pitkanen_E/0/1/0/all/0/1\">Esa Pitk&#xe4;nen</a>",
          "description": "Many current neural networks for medical imaging generalise poorly to data\nunseen during training. Such behaviour can be caused by networks overfitting\neasy-to-learn, or statistically dominant, features while disregarding other\npotentially informative features. For example, indistinguishable differences in\nthe sharpness of the images from two different scanners can degrade the\nperformance of the network significantly. All neural networks intended for\nclinical practice need to be robust to variation in data caused by differences\nin imaging equipment, sample preparation and patient populations.\n\nTo address these challenges, we evaluate the utility of spectral decoupling\nas an implicit bias mitigation method. Spectral decoupling encourages the\nneural network to learn more features by simply regularising the networks'\nunnormalised prediction scores with an L2 penalty, thus having no added\ncomputational costs.\n\nWe show that spectral decoupling allows training neural networks on datasets\nwith strong spurious correlations. Networks trained without spectral decoupling\ndo not learn the original task and appear to make false predictions based on\nthe spurious correlations. Spectral decoupling also increases networks'\nrobustness for data distribution shifts. To validate our findings, we train\nnetworks with and without spectral decoupling to detect prostate cancer tissue\nslides and COVID-19 in chest radiographs. Networks trained with spectral\ndecoupling achieve substantially higher performance on all evaluation datasets.\n\nOur results show that spectral decoupling helps with generalisation issues\nassociated with neural networks. We recommend using spectral decoupling as an\nimplicit bias mitigation method in any neural network intended for clinical\nuse.",
          "link": "http://arxiv.org/abs/2103.17171",
          "publishedOn": "2021-07-09T01:58:28.572Z",
          "wordCount": 784,
          "title": "Spectral decoupling allows training transferable neural networks in medical imaging. (arXiv:2103.17171v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02287",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Ronaghi_F/0/1/0/all/0/1\">Farnoush Ronaghi</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Salimibeni_M/0/1/0/all/0/1\">Mohammad Salimibeni</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Naderkhani_F/0/1/0/all/0/1\">Farnoosh Naderkhani</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>",
          "description": "The novel of coronavirus (COVID-19) has suddenly and abruptly changed the\nworld as we knew at the start of the 3rd decade of the 21st century.\nParticularly, COVID-19 pandemic has negatively affected financial econometrics\nand stock markets across the globe. Artificial Intelligence (AI) and Machine\nLearning (ML)-based prediction models, especially Deep Neural Network (DNN)\narchitectures, have the potential to act as a key enabling factor to reduce the\nadverse effects of the COVID-19 pandemic and future possible ones on financial\nmarkets. In this regard, first, a unique COVID-19 related PRIce MOvement\nprediction (COVID19 PRIMO) dataset is introduced in this paper, which\nincorporates effects of social media trends related to COVID-19 on stock market\nprice movements. Afterwards, a novel hybrid and parallel DNN-based framework is\nproposed that integrates different and diversified learning architectures.\nReferred to as the COVID-19 adopted Hybrid and Parallel deep fusion framework\nfor Stock price Movement Prediction (COVID19-HPSMP), innovative fusion\nstrategies are used to combine scattered social media news related to COVID-19\nwith historical mark data. The proposed COVID19-HPSMP consists of two parallel\npaths (hence hybrid), one based on Convolutional Neural Network (CNN) with\nLocal/Global Attention modules, and one integrated CNN and Bi-directional Long\nShort term Memory (BLSTM) path. The two parallel paths are followed by a\nmultilayer fusion layer acting as a fusion centre that combines localized\nfeatures. Performance evaluations are performed based on the introduced COVID19\nPRIMO dataset illustrating superior performance of the proposed framework.",
          "link": "http://arxiv.org/abs/2101.02287",
          "publishedOn": "2021-07-09T01:58:28.564Z",
          "wordCount": 758,
          "title": "COVID19-HPSMP: COVID-19 Adopted Hybrid and Parallel Deep Information Fusion Framework for Stock Price Movement Prediction. (arXiv:2101.02287v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erdelyi_T/0/1/0/all/0/1\">Tam&#xe1;s Erd&#xe9;lyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Christopher Musco</a>",
          "description": "We prove new explicit upper bounds on the leverage scores of Fourier sparse\nfunctions under both the Gaussian and Laplace measures. In particular, we study\n$s$-sparse functions of the form $f(x) = \\sum_{j=1}^s a_j e^{i \\lambda_j x}$\nfor coefficients $a_j \\in \\mathbb{C}$ and frequencies $\\lambda_j \\in\n\\mathbb{R}$. Bounding Fourier sparse leverage scores under various measures is\nof pure mathematical interest in approximation theory, and our work extends\nexisting results for the uniform measure [Erd17,CP19a]. Practically, our bounds\nare motivated by two important applications in machine learning:\n\n1. Kernel Approximation. They yield a new random Fourier features algorithm\nfor approximating Gaussian and Cauchy (rational quadratic) kernel matrices. For\nlow-dimensional data, our method uses a near optimal number of features, and\nits runtime is polynomial in the $statistical\\ dimension$ of the approximated\nkernel matrix. It is the first \"oblivious sketching method\" with this property\nfor any kernel besides the polynomial kernel, resolving an open question of\n[AKM+17,AKK+20b].\n\n2. Active Learning. They can be used as non-uniform sampling distributions\nfor robust active learning when data follows a Gaussian or Laplace\ndistribution. Using the framework of [AKM+19], we provide essentially optimal\nresults for bandlimited and multiband interpolation, and Gaussian process\nregression. These results generalize existing work that only applies to\nuniformly distributed data.",
          "link": "http://arxiv.org/abs/2006.07340",
          "publishedOn": "2021-07-09T01:58:28.546Z",
          "wordCount": 688,
          "title": "Fourier Sparse Leverage Scores and Approximate Kernel Learning. (arXiv:2006.07340v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.08964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1\">Matthew Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mues_C/0/1/0/all/0/1\">Christophe Mues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1\">Cristi&#xe1;n Bravo</a>",
          "description": "Compared to consumer lending, Micro, Small and Medium Enterprise (mSME)\ncredit risk modelling is particularly challenging, as, often, the same sources\nof information are not available. Therefore, it is standard policy for a loan\nofficer to provide a textual loan assessment to mitigate limited data\navailability. In turn, this statement is analysed by a credit expert alongside\nany available standard credit data. In our paper, we exploit recent advances\nfrom the field of Deep Learning and Natural Language Processing (NLP),\nincluding the BERT (Bidirectional Encoder Representations from Transformers)\nmodel, to extract information from 60 000 textual assessments provided by a\nlender. We consider the performance in terms of the AUC (Area Under the\nreceiver operating characteristic Curve) and Brier Score metrics and find that\nthe text alone is surprisingly effective for predicting default. However, when\ncombined with traditional data, it yields no additional predictive capability,\nwith performance dependent on the text's length. Our proposed deep learning\nmodel does, however, appear to be robust to the quality of the text and\ntherefore suitable for partly automating the mSME lending process. We also\ndemonstrate how the content of loan assessments influences performance, leading\nus to a series of recommendations on a new strategy for collecting future mSME\nloan assessments.",
          "link": "http://arxiv.org/abs/2003.08964",
          "publishedOn": "2021-07-09T01:58:28.540Z",
          "wordCount": 712,
          "title": "The value of text for small business default prediction: A deep learning approach. (arXiv:2003.08964v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yaofeng Desmond Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_B/0/1/0/all/0/1\">Biswadip Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Amit Chakraborty</a>",
          "description": "The incorporation of appropriate inductive bias plays a critical role in\nlearning dynamics from data. A growing body of work has been exploring ways to\nenforce energy conservation in the learned dynamics by encoding Lagrangian or\nHamiltonian dynamics into the neural network architecture. These existing\napproaches are based on differential equations, which do not allow\ndiscontinuity in the states and thereby limit the class of systems one can\nlearn. However, in reality, most physical systems, such as legged robots and\nrobotic manipulators, involve contacts and collisions, which introduce\ndiscontinuities in the states. In this paper, we introduce a differentiable\ncontact model, which can capture contact mechanics: frictionless/frictional, as\nwell as elastic/inelastic. This model can also accommodate inequality\nconstraints, such as limits on the joint angles. The proposed contact model\nextends the scope of Lagrangian and Hamiltonian neural networks by allowing\nsimultaneous learning of contact and system properties. We demonstrate this\nframework on a series of challenging 2D and 3D physical systems with different\ncoefficients of restitution and friction. The learned dynamics can be used as a\ndifferentiable physics simulator for downstream gradient-based optimization\ntasks, such as planning and control.",
          "link": "http://arxiv.org/abs/2102.06794",
          "publishedOn": "2021-07-09T01:58:28.533Z",
          "wordCount": 656,
          "title": "Extending Lagrangian and Hamiltonian Neural Networks with Differentiable Contact Models. (arXiv:2102.06794v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elsken_T/0/1/0/all/0/1\">Thomas Elsken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staffler_B/0/1/0/all/0/1\">Benedikt Staffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1\">Arber Zela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "While neural architecture search methods have been successful in previous\nyears and led to new state-of-the-art performance on various problems, they\nhave also been criticized for being unstable, being highly sensitive with\nrespect to their hyperparameters, and often not performing better than random\nsearch. To shed some light on this issue, we discuss some practical\nconsiderations that help improve the stability, efficiency and overall\nperformance.",
          "link": "http://arxiv.org/abs/2107.03719",
          "publishedOn": "2021-07-09T01:58:28.498Z",
          "wordCount": 505,
          "title": "Bag of Tricks for Neural Architecture Search. (arXiv:2107.03719v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lechner_T/0/1/0/all/0/1\">Tosca Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_S/0/1/0/all/0/1\">Shai Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sushant Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananthakrishnan_N/0/1/0/all/0/1\">Nivasini Ananthakrishnan</a>",
          "description": "With the growing awareness to fairness in machine learning and the\nrealization of the central role that data representation has in data processing\ntasks, there is an obvious interest in notions of fair data representations.\nThe goal of such representations is that a model trained on data under the\nrepresentation (e.g., a classifier) will be guaranteed to respect some fairness\nconstraints.\n\nSuch representations are useful when they can be fixed for training models on\nvarious different tasks and also when they serve as data filtering between the\nraw data (known to the representation designer) and potentially malicious\nagents that use the data under the representation to learn predictive models\nand make decisions.\n\nA long list of recent research papers strive to provide tools for achieving\nthese goals.\n\nHowever, we prove that this is basically a futile effort. Roughly stated, we\nprove that no representation can guarantee the fairness of classifiers for\ndifferent tasks trained using it; even the basic goal of achieving\nlabel-independent Demographic Parity fairness fails once the marginal data\ndistribution shifts. More refined notions of fairness, like Odds Equality,\ncannot be guaranteed by a representation that does not take into account the\ntask specific labeling rule with respect to which such fairness will be\nevaluated (even if the marginal data distribution is known a priory).\nFurthermore, except for trivial cases, no representation can guarantee Odds\nEquality fairness for any two different tasks, while allowing accurate label\npredictions for both.\n\nWhile some of our conclusions are intuitive, we formulate (and prove) crisp\nstatements of such impossibilities, often contrasting impressions conveyed by\nmany recent works on fair representations.",
          "link": "http://arxiv.org/abs/2107.03483",
          "publishedOn": "2021-07-09T01:58:28.468Z",
          "wordCount": 701,
          "title": "Impossibility results for fair representations. (arXiv:2107.03483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08733",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Amini_H/0/1/0/all/0/1\">Hamed Amini</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Feinstein_Z/0/1/0/all/0/1\">Zachary Feinstein</a>",
          "description": "This paper introduces a formulation of the optimal network compression\nproblem for financial systems. This general formulation is presented for\ndifferent levels of network compression or rerouting allowed from the initial\ninterbank network. We prove that this problem is, generically, NP-hard. We\nfocus on objective functions generated by systemic risk measures under\nsystematic shocks to the financial network. We conclude by studying the optimal\ncompression problem for specific networks; this permits us to study the\nso-called robust fragility of certain network topologies more generally as well\nas the potential benefits and costs of network compression. In particular,\nunder systematic shocks and heterogeneous financial networks the typical\nheuristics of robust fragility no longer hold generally.",
          "link": "http://arxiv.org/abs/2008.08733",
          "publishedOn": "2021-07-09T01:58:28.448Z",
          "wordCount": 574,
          "title": "Optimal Network Compression. (arXiv:2008.08733v3 [q-fin.RM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.11355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vavasis_S/0/1/0/all/0/1\">Stephen Vavasis</a>",
          "description": "Sum-of-norms clustering is a clustering formulation based on convex\noptimization that automatically induces hierarchy. Multiple algorithms have\nbeen proposed to solve the optimization problem: subgradient descent by Hocking\net al., ADMM and ADA by Chi and Lange, stochastic incremental algorithm by\nPanahi et al. and semismooth Newton-CG augmented Lagrangian method by Sun et\nal. All algorithms yield approximate solutions, even though an exact solution\nis demanded to determine the correct cluster assignment. The purpose of this\npaper is to close the gap between the output from existing algorithms and the\nexact solution to the optimization problem. We present a clustering test that\nidentifies and certifies the correct cluster assignment from an approximate\nsolution yielded by any primal-dual algorithm. Our certification validates\nclustering for both unit and multiplicative weights. The test may not succeed\nif the approximation is inaccurate. However, we show the correct cluster\nassignment is guaranteed to be certified by a primal-dual path following\nalgorithm after sufficiently many iterations, provided that the model parameter\n$\\lambda$ avoids a finite number of bad values. Numerical experiments are\nconducted on Gaussian mixture and half-moon data, which indicate that carefully\nchosen multiplicative weights increase the recovery power of sum-of-norms\nclustering.",
          "link": "http://arxiv.org/abs/2006.11355",
          "publishedOn": "2021-07-09T01:58:28.441Z",
          "wordCount": 656,
          "title": "Certifying clusters from sum-of-norms clustering. (arXiv:2006.11355v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sumon Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>",
          "description": "In recent years, many incidents have been reported where machine learning\nmodels exhibited discrimination among people based on race, sex, age, etc.\nResearch has been conducted to measure and mitigate unfairness in machine\nlearning models. For a machine learning task, it is a common practice to build\na pipeline that includes an ordered set of data preprocessing stages followed\nby a classifier. However, most of the research on fairness has considered a\nsingle classifier based prediction task. What are the fairness impacts of the\npreprocessing stages in machine learning pipeline? Furthermore, studies showed\nthat often the root cause of unfairness is ingrained in the data itself, rather\nthan the model. But no research has been conducted to measure the unfairness\ncaused by a specific transformation made in the data preprocessing stage. In\nthis paper, we introduced the causal method of fairness to reason about the\nfairness impact of data preprocessing stages in ML pipeline. We leveraged\nexisting metrics to define the fairness measures of the stages. Then we\nconducted a detailed fairness evaluation of the preprocessing stages in 37\npipelines collected from three different sources. Our results show that certain\ndata transformers are causing the model to exhibit unfairness. We identified a\nnumber of fairness patterns in several categories of data transformers.\nFinally, we showed how the local fairness of a preprocessing stage composes in\nthe global fairness of the pipeline. We used the fairness composition to choose\nappropriate downstream transformer that mitigates unfairness in the machine\nlearning pipeline.",
          "link": "http://arxiv.org/abs/2106.06054",
          "publishedOn": "2021-07-09T01:58:28.434Z",
          "wordCount": 764,
          "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mantovani_R/0/1/0/all/0/1\">Rafael Gomes Mantovani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_A/0/1/0/all/0/1\">Andr&#xe9; Luis Debiaso Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcobaca_E/0/1/0/all/0/1\">Edesio Alcoba&#xe7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertrudes_J/0/1/0/all/0/1\">Jadson Castro Gertrudes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_S/0/1/0/all/0/1\">Sylvio Barbon Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1\">Andr&#xe9; Carlos Ponce de Leon Ferreira de Carvalho</a>",
          "description": "Machine Learning (ML) algorithms have been increasingly applied to problems\nfrom several different areas. Despite their growing popularity, their\npredictive performance is usually affected by the values assigned to their\nhyperparameters (HPs). As consequence, researchers and practitioners face the\nchallenge of how to set these values. Many users have limited knowledge about\nML algorithms and the effect of their HP values and, therefore, do not take\nadvantage of suitable settings. They usually define the HP values by trial and\nerror, which is very subjective, not guaranteed to find good values and\ndependent on the user experience. Tuning techniques search for HP values able\nto maximize the predictive performance of induced models for a given dataset,\nbut have the drawback of a high computational cost. Thus, practitioners use\ndefault values suggested by the algorithm developer or by tools implementing\nthe algorithm. Although default values usually result in models with acceptable\npredictive performance, different implementations of the same algorithm can\nsuggest distinct default values. To maintain a balance between tuning and using\ndefault values, we propose a strategy to generate new optimized default values.\nOur approach is grounded on a small set of optimized values able to obtain\npredictive performance values better than default settings provided by popular\ntools. After performing a large experiment and a careful analysis of the\nresults, we concluded that our approach delivers better default values.\nBesides, it leads to competitive solutions when compared to tuned values,\nmaking it easier to use and having a lower cost. We also extracted simple rules\nto guide practitioners in deciding whether to use our new methodology or a HP\ntuning approach.",
          "link": "http://arxiv.org/abs/2008.00025",
          "publishedOn": "2021-07-09T01:58:28.426Z",
          "wordCount": 776,
          "title": "Rethinking Default Values: a Low Cost and Efficient Strategy to Define Hyperparameters. (arXiv:2008.00025v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14257",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Gupta_K/0/1/0/all/0/1\">Kanhaiya Gupta</a>",
          "description": "In recent years, artificial neural networks (ANNs) have won numerous contests\nin pattern recognition and machine learning. ANNS have been applied to problems\nranging from speech recognition to prediction of protein secondary structure,\nclassification of cancers, and gene prediction. Here, we intend to maximize the\nchances of finding the Higgs boson decays to two $\\tau$ leptons in the pseudo\ndataset using a Machine Learning technique to classify the recorded events as\nsignal or background.",
          "link": "http://arxiv.org/abs/2106.14257",
          "publishedOn": "2021-07-09T01:58:28.418Z",
          "wordCount": 546,
          "title": "Use of Machine Learning Technique to maximize the signal over background for $H \\rightarrow \\tau \\tau$. (arXiv:2106.14257v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.13298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1\">Mostafa Elhoushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiq_F/0/1/0/all/0/1\">Farhan Shafiq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Henry Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Joey Yiwei Li</a>",
          "description": "The high computation, memory, and power budgets of inferring convolutional\nneural networks (CNNs) are major bottlenecks of model deployment to edge\ncomputing platforms, e.g., mobile devices and IoT. Moreover, training CNNs is\ntime and energy-intensive even on high-grade servers. Convolution layers and\nfully connected layers, because of their intense use of multiplications, are\nthe dominant contributor to this computation budget.\n\nWe propose to alleviate this problem by introducing two new operations:\nconvolutional shifts and fully-connected shifts which replace multiplications\nwith bitwise shift and sign flipping during both training and inference. During\ninference, both approaches require only 5 bits (or less) to represent the\nweights. This family of neural network architectures (that use convolutional\nshifts and fully connected shifts) is referred to as DeepShift models. We\npropose two methods to train DeepShift models: DeepShift-Q which trains regular\nweights constrained to powers of 2, and DeepShift-PS that trains the values of\nthe shifts and sign flips directly.\n\nVery close accuracy, and in some cases higher accuracy, to baselines are\nachieved. Converting pre-trained 32-bit floating-point baseline models of\nResNet18, ResNet50, VGG16, and GoogleNet to DeepShift and training them for 15\nto 30 epochs, resulted in Top-1/Top-5 accuracies higher than that of the\noriginal model.\n\nLast but not least, we implemented the convolutional shifts and fully\nconnected shift GPU kernels and showed a reduction in latency time of 25% when\ninferring ResNet18 compared to unoptimized multiplication-based GPU kernels.\nThe code can be found at https://github.com/mostafaelhoushi/DeepShift.",
          "link": "http://arxiv.org/abs/1905.13298",
          "publishedOn": "2021-07-09T01:58:28.398Z",
          "wordCount": 817,
          "title": "DeepShift: Towards Multiplication-Less Neural Networks. (arXiv:1905.13298v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klink_P/0/1/0/all/0/1\">Pascal Klink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulsamad_H/0/1/0/all/0/1\">Hany Abdulsamad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belousov_B/0/1/0/all/0/1\">Boris Belousov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DEramo_C/0/1/0/all/0/1\">Carlo D&#x27;Eramo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jan Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1\">Joni Pajarinen</a>",
          "description": "Across machine learning, the use of curricula has shown strong empirical\npotential to improve learning from data by avoiding local optima of training\nobjectives. For reinforcement learning (RL), curricula are especially\ninteresting, as the underlying optimization has a strong tendency to get stuck\nin local optima due to the exploration-exploitation trade-off. Recently, a\nnumber of approaches for an automatic generation of curricula for RL have been\nshown to increase performance while requiring less expert knowledge compared to\nmanually designed curricula. However, these approaches are seldomly\ninvestigated from a theoretical perspective, preventing a deeper understanding\nof their mechanics. In this paper, we present an approach for automated\ncurriculum generation in RL with a clear theoretical underpinning. More\nprecisely, we formalize the well-known self-paced learning paradigm as inducing\na distribution over training tasks, which trades off between task complexity\nand the objective to match a desired task distribution. Experiments show that\ntraining on this induced distribution helps to avoid poor local optima across\nRL algorithms in different tasks with uninformative rewards and challenging\nexploration requirements.",
          "link": "http://arxiv.org/abs/2102.13176",
          "publishedOn": "2021-07-09T01:58:28.391Z",
          "wordCount": 642,
          "title": "A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning. (arXiv:2102.13176v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stadler_T/0/1/0/all/0/1\">Theresa Stadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oprisanu_B/0/1/0/all/0/1\">Bristena Oprisanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troncoso_C/0/1/0/all/0/1\">Carmela Troncoso</a>",
          "description": "Synthetic data has been advertised as a silver-bullet solution to\nprivacy-preserving data publishing that addresses the shortcomings of\ntraditional anonymisation techniques. The promise is that synthetic data drawn\nfrom generative models preserves the statistical properties of the original\ndataset but, at the same time, provides perfect protection against privacy\nattacks. In this work, we present the first quantitative evaluation of the\nprivacy gain of synthetic data publishing and compare it to that of previous\nanonymisation techniques.\n\nOur evaluation of a wide range of state-of-the-art generative models\ndemonstrates that synthetic data either does not prevent inference attacks or\ndoes not retain data utility. In other words, we empirically show that\nsynthetic data suffers from the same limitations as traditional anonymisation\ntechniques.\n\nFurthermore, we find that, in contrast to traditional anonymisation, the\nprivacy-utility tradeoff of synthetic data publishing is hard to predict.\nBecause it is impossible to predict what signals a synthetic dataset will\npreserve and what information will be lost, synthetic data leads to a highly\nvariable privacy gain and unpredictable utility loss. In summary, we find that\nsynthetic data is far from the holy grail of privacy-preserving data\npublishing.",
          "link": "http://arxiv.org/abs/2011.07018",
          "publishedOn": "2021-07-09T01:58:28.384Z",
          "wordCount": 668,
          "title": "Synthetic Data -- Anonymisation Groundhog Day. (arXiv:2011.07018v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenshuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Queralta_J/0/1/0/all/0/1\">Jorge Pe&#xf1;a Queralta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westerlund_T/0/1/0/all/0/1\">Tomi Westerlund</a>",
          "description": "Deep reinforcement learning has recently seen huge success across multiple\nareas in the robotics domain. Owing to the limitations of gathering real-world\ndata, i.e., sample inefficiency and the cost of collecting it, simulation\nenvironments are utilized for training the different agents. This not only aids\nin providing a potentially infinite data source, but also alleviates safety\nconcerns with real robots. Nonetheless, the gap between the simulated and real\nworlds degrades the performance of the policies once the models are transferred\ninto real robots. Multiple research efforts are therefore now being directed\ntowards closing this sim-to-real gap and accomplish more efficient policy\ntransfer. Recent years have seen the emergence of multiple methods applicable\nto different domains, but there is a lack, to the best of our knowledge, of a\ncomprehensive review summarizing and putting into context the different\nmethods. In this survey paper, we cover the fundamental background behind\nsim-to-real transfer in deep reinforcement learning and overview the main\nmethods being utilized at the moment: domain randomization, domain adaptation,\nimitation learning, meta-learning and knowledge distillation. We categorize\nsome of the most relevant recent works, and outline the main application\nscenarios. Finally, we discuss the main opportunities and challenges of the\ndifferent approaches and point to the most promising directions.",
          "link": "http://arxiv.org/abs/2009.13303",
          "publishedOn": "2021-07-09T01:58:28.378Z",
          "wordCount": 695,
          "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey. (arXiv:2009.13303v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dornheim_J/0/1/0/all/0/1\">Johannes Dornheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morand_L/0/1/0/all/0/1\">Lukas Morand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeitvogel_S/0/1/0/all/0/1\">Samuel Zeitvogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iraki_T/0/1/0/all/0/1\">Tarek Iraki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Link_N/0/1/0/all/0/1\">Norbert Link</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helm_D/0/1/0/all/0/1\">Dirk Helm</a>",
          "description": "A major goal of materials design is to find material structures with desired\nproperties and in a second step to find a processing path to reach one of these\nstructures. In this paper, we propose and investigate a deep reinforcement\nlearning approach for the optimization of processing paths. The goal is to find\noptimal processing paths in the material structure space that lead to\ntarget-structures, which have been identified beforehand to result in desired\nmaterial properties. There exists a target set containing one or multiple\ndifferent structures. Our proposed methods can find an optimal path from a\nstart structure to a single target structure, or optimize the processing paths\nto one of the equivalent target-structures in the set. In the latter case, the\nalgorithm learns during processing to simultaneously identify the best\nreachable target structure and the optimal path to it. The proposed methods\nbelong to the family of model-free deep reinforcement learning algorithms. They\nare guided by structure representations as features of the process state and by\na reward signal, which is formulated based on a distance function in the\nstructure space. Model-free reinforcement learning algorithms learn through\ntrial and error while interacting with the process. Thereby, they are not\nrestricted to information from a priori sampled processing data and are able to\nadapt to the specific process. The optimization itself is model-free and does\nnot require any prior knowledge about the process itself. We instantiate and\nevaluate the proposed methods by optimizing paths of a generic metal forming\nprocess. We show the ability of both methods to find processing paths leading\nclose to target structures and the ability of the extended method to identify\ntarget-structures that can be reached effectively and efficiently and to focus\non these targets for sample efficient processing path optimization.",
          "link": "http://arxiv.org/abs/2009.09706",
          "publishedOn": "2021-07-09T01:58:28.368Z",
          "wordCount": 806,
          "title": "Deep Reinforcement Learning Methods for Structure-Guided Processing Path Optimization. (arXiv:2009.09706v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1\">Ekdeep Singh Lubana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hidenori Tanaka</a>",
          "description": "Inspired by BatchNorm, there has been an explosion of normalization layers\nfor deep neural networks (DNNs). However, these alternative normalization\nlayers have seen minimal use, partially due to a lack of guiding principles\nthat can help identify when these layers can serve as a replacement for\nBatchNorm. To address this problem, we take a theoretical approach,\ngeneralizing the known beneficial mechanisms of BatchNorm to several recently\nproposed normalization techniques. Our generalized theory leads to the\nfollowing set of principles: (i) similar to BatchNorm, activations-based\nnormalization layers can prevent exponential growth of activations in ResNets,\nbut parametric layers require explicit remedies; (ii) use of GroupNorm can\nensure informative forward propagation, with different samples being assigned\ndissimilar activations, but increasing group size results in increasingly\nindistinguishable activations for different samples, explaining slow\nconvergence speed in models with LayerNorm; (iii) small group sizes result in\nlarge gradient norm in earlier layers, hence explaining training instability\nissues in Instance Normalization and illustrating a speed-stability tradeoff in\nGroupNorm. Overall, our analysis reveals a unified set of mechanisms that\nunderpin the success of normalization methods in deep learning, providing us\nwith a compass to systematically explore the vast design space of DNN\nnormalization layers.",
          "link": "http://arxiv.org/abs/2106.05956",
          "publishedOn": "2021-07-09T01:58:28.349Z",
          "wordCount": 669,
          "title": "Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning. (arXiv:2106.05956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smirnov_O/0/1/0/all/0/1\">Oleg Smirnov</a>",
          "description": "The adoption of neural networks and deep learning in non-Euclidean domains\nhas been hindered until recently by the lack of scalable and efficient learning\nframeworks. Existing toolboxes in this space were mainly motivated by research\nand education use cases, whereas practical aspects, such as deploying and\nmaintaining machine learning models, were often overlooked.\n\nWe attempt to bridge this gap by proposing TensorFlow RiemOpt, a Python\nlibrary for optimization on Riemannian manifolds in TensorFlow. The library is\ndesigned with the aim for a seamless integration with the TensorFlow ecosystem,\ntargeting not only research, but also streamlining production machine learning\npipelines.",
          "link": "http://arxiv.org/abs/2105.13921",
          "publishedOn": "2021-07-09T01:58:28.343Z",
          "wordCount": 568,
          "title": "TensorFlow RiemOpt: a library for optimization on Riemannian manifolds. (arXiv:2105.13921v2 [cs.MS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>",
          "description": "We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and\nclassification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,\noutperforming VQ-VAE-2.",
          "link": "http://arxiv.org/abs/2106.15282",
          "publishedOn": "2021-07-09T01:58:28.336Z",
          "wordCount": 624,
          "title": "Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>",
          "description": "Knowledge distillation is widely used as a means of improving the performance\nof a relatively simple student model using the predictions from a complex\nteacher model. Several works have shown that distillation significantly boosts\nthe student's overall performance; however, are these gains uniform across all\ndata subgroups? In this paper, we show that distillation can harm performance\non certain subgroups, e.g., classes with few associated samples. We trace this\nbehaviour to errors made by the teacher distribution being transferred to and\namplified by the student model. To mitigate this problem, we present techniques\nwhich soften the teacher influence for subgroups where it is less reliable.\nExperiments on several image classification benchmarks show that these\nmodifications of distillation maintain boost in overall accuracy, while\nadditionally ensuring improvement in subgroup performance.",
          "link": "http://arxiv.org/abs/2106.10494",
          "publishedOn": "2021-07-09T01:58:28.330Z",
          "wordCount": 595,
          "title": "Teacher's pet: understanding and mitigating biases in distillation. (arXiv:2106.10494v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yan Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhiwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuaiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>",
          "description": "We present a new practical framework based on deep reinforcement learning and\ndecision-time planning for real-world vehicle repositioning on ride-hailing (a\ntype of mobility-on-demand, MoD) platforms. Our approach learns the\nspatiotemporal state-value function using a batch training algorithm with deep\nvalue networks. The optimal repositioning action is generated on-demand through\nvalue-based policy search, which combines planning and bootstrapping with the\nvalue networks. For the large-fleet problems, we develop several algorithmic\nfeatures that we incorporate into our framework and that we demonstrate to\ninduce coordination among the algorithmically-guided vehicles. We benchmark our\nalgorithm with baselines in a ride-hailing simulation environment to\ndemonstrate its superiority in improving income efficiency meausred by\nincome-per-hour. We have also designed and run a real-world experiment program\nwith regular drivers on a major ride-hailing platform. We have observed\nsignificantly positive results on key metrics comparing our method with\nexperienced drivers who performed idle-time repositioning based on their own\nexpertise.",
          "link": "http://arxiv.org/abs/2103.04555",
          "publishedOn": "2021-07-09T01:58:28.323Z",
          "wordCount": 635,
          "title": "Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement Learning. (arXiv:2103.04555v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Le Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaochun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Wen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>",
          "description": "We investigate large-scale latent variable models (LVMs) for neural story\ngeneration -- an under-explored application for open-domain long text -- with\nobjectives in two threads: generation effectiveness and controllability. LVMs,\nespecially the variational autoencoder (VAE), have achieved both effective and\ncontrollable generation through exploiting flexible distributional latent\nrepresentations. Recently, Transformers and its variants have achieved\nremarkable effectiveness without explicit latent representation learning, thus\nlack satisfying controllability in generation. In this paper, we advocate to\nrevive latent variable modeling, essentially the power of representation\nlearning, in the era of Transformers to enhance controllability without hurting\nstate-of-the-art generation effectiveness. Specifically, we integrate latent\nrepresentation vectors with a Transformer-based pre-trained architecture to\nbuild conditional variational autoencoder (CVAE). Model components such as\nencoder, decoder and the variational posterior are all built on top of\npre-trained language models -- GPT2 specifically in this paper. Experiments\ndemonstrate state-of-the-art conditional generation ability of our model, as\nwell as its excellent representation learning capability and controllability.",
          "link": "http://arxiv.org/abs/2101.00828",
          "publishedOn": "2021-07-09T01:58:28.305Z",
          "wordCount": 632,
          "title": "Transformer-based Conditional Variational Autoencoder for Controllable Story Generation. (arXiv:2101.00828v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial robustness of deep neural networks has been actively\ninvestigated. However, most existing defense approaches are limited to a\nspecific type of adversarial perturbations. Specifically, they often fail to\noffer resistance to multiple attack types simultaneously, i.e., they lack\nmulti-perturbation robustness. Furthermore, compared to image recognition\nproblems, the adversarial robustness of video recognition models is relatively\nunexplored. While several studies have proposed how to generate adversarial\nvideos, only a handful of approaches about the defense strategies have been\npublished in the literature. In this paper, we propose one of the first defense\nstrategies against multiple types of adversarial videos for video recognition.\nThe proposed method, referred to as MultiBN, performs adversarial training on\nmultiple adversarial video types using multiple independent batch normalization\n(BN) layers with a learning-based BN selection module. With a multiple BN\nstructure, each BN brach is responsible for learning the distribution of a\nsingle perturbation type and thus provides more precise distribution\nestimations. This mechanism benefits dealing with multiple perturbation types.\nThe BN selection module detects the attack type of an input video and sends it\nto the corresponding BN branch, making MultiBN fully automatic and allow\nend-to-end training. Compared to present adversarial training approaches, the\nproposed MultiBN exhibits stronger multi-perturbation robustness against\ndifferent and even unforeseen adversarial video types, ranging from Lp-bounded\nattacks and physically realizable attacks. This holds true on different\ndatasets and target models. Moreover, we conduct an extensive analysis to study\nthe properties of the multiple BN structure.",
          "link": "http://arxiv.org/abs/2009.05244",
          "publishedOn": "2021-07-09T01:58:28.295Z",
          "wordCount": 710,
          "title": "Defending Against Multiple and Unforeseen Adversarial Videos. (arXiv:2009.05244v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vacher_J/0/1/0/all/0/1\">Jonathan Vacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_C/0/1/0/all/0/1\">Claire Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coen_Cagli_R/0/1/0/all/0/1\">Ruben Coen-Cagli</a>",
          "description": "Probabilistic finite mixture models are widely used for unsupervised\nclustering. These models can often be improved by adapting them to the topology\nof the data. For instance, in order to classify spatially adjacent data points\nsimilarly, it is common to introduce a Laplacian constraint on the posterior\nprobability that each data point belongs to a class. Alternatively, the mixing\nprobabilities can be treated as free parameters, while assuming Gauss-Markov or\nmore complex priors to regularize those mixing probabilities. However, these\napproaches are constrained by the shape of the prior and often lead to\ncomplicated or intractable inference. Here, we propose a new parametrization of\nthe Dirichlet distribution to flexibly regularize the mixing probabilities of\nover-parametrized mixture distributions. Using the Expectation-Maximization\nalgorithm, we show that our approach allows us to define any linear update rule\nfor the mixing probabilities, including spatial smoothing regularization as a\nspecial case. We then show that this flexible design can be extended to share\nclass information between multiple mixture models. We apply our algorithm to\nartificial and natural image segmentation tasks, and we provide quantitative\nand qualitative comparison of the performance of Gaussian and Student-t\nmixtures on the Berkeley Segmentation Dataset. We also demonstrate how to\npropagate class information across the layers of deep convolutional neural\nnetworks in a probabilistically optimal way, suggesting a new interpretation\nfor feedback signals in biological visual systems. Our flexible approach can be\neasily generalized to adapt probabilistic mixture models to arbitrary data\ntopologies.",
          "link": "http://arxiv.org/abs/1905.10629",
          "publishedOn": "2021-07-09T01:58:28.288Z",
          "wordCount": 731,
          "title": "Flexibly Regularized Mixture Models and Application to Image Segmentation. (arXiv:1905.10629v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.09744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_K/0/1/0/all/0/1\">Kai Ming Ting</a>",
          "description": "This paper presents a new insight into improving the performance of\nStochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of\nGaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.\nFirst, the use of Isolation kernel in t-SNE overcomes the drawback of\nmisrepresenting some structures in the data, which often occurs when Gaussian\nkernel is applied in t-SNE. This is because Gaussian kernel determines each\nlocal bandwidth based on one local point only, while Isolation kernel is\nderived directly from the data based on space partitioning. Second, the use of\nIsolation kernel yields a more efficient similarity computation because\ndata-dependent Isolation kernel has only one parameter that needs to be tuned.\nIn contrast, the use of data-independent Gaussian kernel increases the\ncomputational cost by determining n bandwidths for a dataset of n points. As\nthe root cause of these deficiencies in t-SNE is Gaussian kernel, we show that\nsimply replacing Gaussian kernel with Isolation kernel in t-SNE significantly\nimproves the quality of the final visualisation output (without creating\nmisrepresented structures) and removes one key obstacle that prevents t-SNE\nfrom processing large datasets. Moreover, Isolation kernel enables t-SNE to\ndeal with large-scale datasets in less runtime without trading off accuracy,\nunlike existing methods in speeding up t-SNE.",
          "link": "http://arxiv.org/abs/1906.09744",
          "publishedOn": "2021-07-09T01:58:28.281Z",
          "wordCount": 696,
          "title": "Improving the Effectiveness and Efficiency of Stochastic Neighbour Embedding with Isolation Kernel. (arXiv:1906.09744v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "Although many techniques have been applied to matrix factorization (MF), they\nmay not fully exploit the feature structure. In this paper, we incorporate the\ngrouping effect into MF and propose a novel method called Robust Matrix\nFactorization with Grouping effect (GRMF). The grouping effect is a\ngeneralization of the sparsity effect, which conducts denoising by clustering\nsimilar values around multiple centers instead of just around 0. Compared with\nexisting algorithms, the proposed GRMF can automatically learn the grouping\nstructure and sparsity in MF without prior knowledge, by introducing a\nnaturally adjustable non-convex regularization to achieve simultaneous sparsity\nand grouping effect. Specifically, GRMF uses an efficient alternating\nminimization framework to perform MF, in which the original non-convex problem\nis first converted into a convex problem through Difference-of-Convex (DC)\nprogramming, and then solved by Alternating Direction Method of Multipliers\n(ADMM). In addition, GRMF can be easily extended to the Non-negative Matrix\nFactorization (NMF) settings. Extensive experiments have been conducted using\nreal-world data sets with outliers and contaminated noise, where the\nexperimental results show that GRMF has promoted performance and robustness,\ncompared to five benchmark algorithms.",
          "link": "http://arxiv.org/abs/2106.13681",
          "publishedOn": "2021-07-09T01:58:28.273Z",
          "wordCount": 654,
          "title": "Robust Matrix Factorization with Grouping Effect. (arXiv:2106.13681v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Regatti_J/0/1/0/all/0/1\">Jayanth Reddy Regatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Aniket Anand Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manavoglu_E/0/1/0/all/0/1\">Eren Manavoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_U/0/1/0/all/0/1\">Urun Dogan</a>",
          "description": "Recent advances in deep clustering and unsupervised representation learning\nare based on the idea that different views of an input image (generated through\ndata augmentation techniques) must either be closer in the representation\nspace, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL)\nis one such representation learning algorithm that has achieved\nstate-of-the-art results in self-supervised image classification on ImageNet\nunder the linear evaluation protocol. However, the utility of the learnt\nfeatures of BYOL to perform clustering is not explored. In this work, we study\nthe clustering ability of BYOL and observe that features learnt using BYOL may\nnot be optimal for clustering. We propose a novel consensus clustering based\nloss function, and train BYOL with the proposed loss in an end-to-end way that\nimproves the clustering ability and outperforms similar clustering based\nmethods on some popular computer vision datasets.",
          "link": "http://arxiv.org/abs/2010.01245",
          "publishedOn": "2021-07-09T01:58:28.252Z",
          "wordCount": 623,
          "title": "Consensus Clustering With Unsupervised Representation Learning. (arXiv:2010.01245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wangyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from the human's visual and intuitive perspective. We\ntake the first step to bridge the gap by proposing a deep learning-based\ntechnique to automatically classify road networks into four classes on a visual\nbasis. The method is implemented by generating an image of the street network\n(Colored Road Hierarchy Diagram), which we introduce in this paper, and\nclassifying it using a deep convolutional neural network (ResNet-34). The model\nachieves an overall classification accuracy of 0.875. Nine cities around the\nworld are selected as the study areas with their road networks acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: we apply\nour method in a case study of urban vitality prediction. An advanced tree-based\nregression model (LightGBM) is for the first time designated to establish the\nrelationship between morphological indices and vitality indicators. The effect\nof road network classification is found to be small but positively associated\nwith urban vitality. This work expands the toolkit of quantitative urban\nmorphology study with new techniques, supporting further studies in the future.",
          "link": "http://arxiv.org/abs/2105.09908",
          "publishedOn": "2021-07-09T01:58:28.245Z",
          "wordCount": 726,
          "title": "Classification of Urban Morphology with Deep Learning: Application on Urban Vitality. (arXiv:2105.09908v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "Recent studies have shown that providing personalized explanations alongside\nrecommendations increases trust and perceived quality. Furthermore, it gives\nusers an opportunity to refine the recommendations by critiquing parts of the\nexplanations. On one hand, current recommender systems model the\nrecommendation, explanation, and critiquing objectives jointly, but this\ncreates an inherent trade-off between their respective performance. On the\nother hand, although recent latent linear critiquing approaches are built upon\nan existing recommender system, they suffer from computational inefficiency at\ninference due to the objective optimized at each conversation's turn. We\naddress these deficiencies with M&Ms-VAE, a novel variational autoencoder for\nrecommendation and explanation that is based on multimodal modeling\nassumptions. We train the model under a weak supervision scheme to simulate\nboth fully and partially observed variables. Then, we leverage the\ngeneralization ability of a trained M&Ms-VAE model to embed the user preference\nand the critique separately. Our work's most important innovation is our\ncritiquing module, which is built upon and trained in a self-supervised manner\nwith a simple ranking objective. Experiments on four real-world datasets\ndemonstrate that among state-of-the-art models, our system is the first to\ndominate or match the performance in terms of recommendation, explanation, and\nmulti-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x\nfaster than the best baselines. Finally, we show that our model infers coherent\njoint and cross generation, even under weak supervision, thanks to our\nmultimodal-based modeling and training scheme.",
          "link": "http://arxiv.org/abs/2105.00774",
          "publishedOn": "2021-07-09T01:58:28.228Z",
          "wordCount": 706,
          "title": "Fast Multi-Step Critiquing for VAE-based Recommender Systems. (arXiv:2105.00774v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xiaomin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lihang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jieqiong Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Donglong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanzhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Effective molecular representation learning is of great importance to\nfacilitate molecular property prediction, which is a fundamental task for the\ndrug and material industry. Recent advances in graph neural networks (GNNs)\nhave shown great promise in applying GNNs for molecular representation\nlearning. Moreover, a few recent studies have also demonstrated successful\napplications of self-supervised learning methods to pre-train the GNNs to\novercome the problem of insufficient labeled molecules. However, existing GNNs\nand pre-training strategies usually treat molecules as topological graph data\nwithout fully utilizing the molecular geometry information. Whereas, the\nthree-dimensional (3D) spatial structure of a molecule, a.k.a molecular\ngeometry, is one of the most critical factors for determining molecular\nphysical, chemical, and biological properties. To this end, we propose a novel\nGeometry Enhanced Molecular representation learning method (GEM) for Chemical\nRepresentation Learning (ChemRL). At first, we design a geometry-based GNN\narchitecture that simultaneously models atoms, bonds, and bond angles in a\nmolecule. To be specific, we devised double graphs for a molecule: The first\none encodes the atom-bond relations; The second one encodes bond-angle\nrelations. Moreover, on top of the devised GNN architecture, we propose several\nnovel geometry-level self-supervised learning strategies to learn spatial\nknowledge by utilizing the local and global molecular 3D structures. We compare\nChemRL-GEM with various state-of-the-art (SOTA) baselines on different\nmolecular benchmarks and exhibit that ChemRL-GEM can significantly outperform\nall baselines in both regression and classification tasks. For example, the\nexperimental results show an overall improvement of 8.8% on average compared to\nSOTA baselines on the regression tasks, demonstrating the superiority of the\nproposed method.",
          "link": "http://arxiv.org/abs/2106.06130",
          "publishedOn": "2021-07-09T01:58:28.202Z",
          "wordCount": 742,
          "title": "ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for Property Prediction. (arXiv:2106.06130v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11612",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1\">Murat A. Erdogdu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseinzadeh_R/0/1/0/all/0/1\">Rasa Hosseinzadeh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1\">Matthew S. Zhang</a>",
          "description": "We study sampling from a target distribution $\\nu_* = e^{-f}$ using the\nunadjusted Langevin Monte Carlo (LMC) algorithm when the potential $f$\nsatisfies a strong dissipativity condition and it is first-order smooth with a\nLipschitz gradient. We prove that, initialized with a Gaussian random vector\nthat has sufficiently small variance, iterating the LMC algorithm for\n$\\widetilde{\\mathcal{O}}(\\lambda^2 d\\epsilon^{-1})$ steps is sufficient to\nreach $\\epsilon$-neighborhood of the target in both Chi-squared and Renyi\ndivergence, where $\\lambda$ is the logarithmic Sobolev constant of $\\nu_*$. Our\nresults do not require warm-start to deal with the exponential dimension\ndependency in Chi-squared divergence at initialization. In particular, for\nstrongly convex and first-order smooth potentials, we show that the LMC\nalgorithm achieves the rate estimate $\\widetilde{\\mathcal{O}}(d\\epsilon^{-1})$\nwhich improves the previously known rates in both of these metrics, under the\nsame assumptions. Translating this rate to other metrics, our results also\nrecover the state-of-the-art rate estimates in KL divergence, total variation\nand $2$-Wasserstein distance in the same setup. Finally, as we rely on the\nlogarithmic Sobolev inequality, our framework covers a range of non-convex\npotentials that are first-order smooth and exhibit strong convexity outside of\na compact region.",
          "link": "http://arxiv.org/abs/2007.11612",
          "publishedOn": "2021-07-09T01:58:28.182Z",
          "wordCount": 730,
          "title": "Convergence of Langevin Monte Carlo in Chi-Squared and Renyi Divergence. (arXiv:2007.11612v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.08307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertugli_A/0/1/0/all/0/1\">Alessia Bertugli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coscia_P/0/1/0/all/0/1\">Pasquale Coscia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Anticipating human motion in crowded scenarios is essential for developing\nintelligent transportation systems, social-aware robots and advanced video\nsurveillance applications. A key component of this task is represented by the\ninherently multi-modal nature of human paths which makes socially acceptable\nmultiple futures when human interactions are involved. To this end, we propose\na generative architecture for multi-future trajectory predictions based on\nConditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning\nmainly relies on prior belief maps, representing most likely moving directions\nand forcing the model to consider past observed dynamics in generating future\npositions. Human interactions are modeled with a graph-based attention\nmechanism enabling an online attentive hidden state refinement of the recurrent\nestimation. To corroborate our model, we perform extensive experiments on\npublicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS\nSportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its\neffectiveness in crowded scenes compared to several state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2005.08307",
          "publishedOn": "2021-07-09T01:58:28.173Z",
          "wordCount": 628,
          "title": "AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction. (arXiv:2005.08307v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_D/0/1/0/all/0/1\">Dige Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>",
          "description": "In practice, the problems encountered in Neural Architecture Search (NAS)\ntraining are not simple problems, but often a series of difficult combinations\n(wrong compensation estimation, curse of dimension, overfitting, high\ncomplexity, etc.). In this paper, we propose a framework to decouple network\nstructure from operator search space, and use two BOHBs to search\nalternatively. Considering that activation function and initialization are also\nimportant parts of neural network, the generalization ability of the model will\nbe affected. We introduce an activation function and an initialization method\ndomain, and add them into the operator search space to form a generalized\nsearch space, so as to improve the generalization ability of the child model.\nWe then trained a GCN-based predictor using feedback from the child model. This\ncan not only improve the search efficiency, but also solve the problem of\ndimension curse. Next, unlike other NAS studies, we used predictors to analyze\nthe stability of different network structures. Finally, we applied our\nframework to neural structure search and achieved significant improvements on\nmultiple datasets.",
          "link": "http://arxiv.org/abs/2103.11820",
          "publishedOn": "2021-07-09T01:58:27.802Z",
          "wordCount": 676,
          "title": "GPNAS: A Neural Network Architecture Search Framework Based on Graphical Predictor. (arXiv:2103.11820v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03863",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rios_F/0/1/0/all/0/1\">Felix L. Rios</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moffa_G/0/1/0/all/0/1\">Giusi Moffa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kuipers_J/0/1/0/all/0/1\">Jack Kuipers</a>",
          "description": "Describing the relationship between the variables in a study domain and\nmodelling the data generating mechanism is a fundamental problem in many\nempirical sciences. Probabilistic graphical models are one common approach to\ntackle the problem. Learning the graphical structure is computationally\nchallenging and a fervent area of current research with a plethora of\nalgorithms being developed. To facilitate the benchmarking of different\nmethods, we present a novel automated workflow, called benchpress for producing\nscalable, reproducible, and platform-independent benchmarks of structure\nlearning algorithms for probabilistic graphical models. Benchpress is\ninterfaced via a simple JSON-file, which makes it accessible for all users,\nwhile the code is designed in a fully modular fashion to enable researchers to\ncontribute additional methodologies. Benchpress currently provides an interface\nto a large number of state-of-the-art algorithms from libraries such as BiDAG,\nbnlearn, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and trilearn as well as\na variety of methods for data generating models and performance evaluation.\nAlongside user-defined models and randomly generated datasets, the software\ntool also includes a number of standard datasets and graphical models from the\nliterature, which may be included in a benchmarking workflow. We demonstrate\nthe applicability of this workflow for learning Bayesian networks in four\ntypical data scenarios. The source code and documentation is publicly available\nfrom this http URL",
          "link": "http://arxiv.org/abs/2107.03863",
          "publishedOn": "2021-07-09T01:58:27.796Z",
          "wordCount": 673,
          "title": "Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models. (arXiv:2107.03863v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03588",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lantian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanlong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Lei Guo</a>",
          "description": "Dynamical systems with binary-valued observations are widely used in\ninformation industry, technology of biological pharmacy and other fields.\nThough there have been much efforts devoted to the identification of such\nsystems, most of the previous investigations are based on first-order gradient\nalgorithm which usually has much slower convergence rate than the Quasi-Newton\nalgorithm. Moreover, persistence of excitation(PE) conditions are usually\nrequired to guarantee consistent parameter estimates in the existing\nliterature, which are hard to be verified or guaranteed for feedback control\nsystems. In this paper, we propose an online projected Quasi-Newton type\nalgorithm for parameter estimation of stochastic regression models with\nbinary-valued observations and varying thresholds. By using both the stochastic\nLyapunov function and martingale estimation methods, we establish the strong\nconsistency of the estimation algorithm and provide the convergence rate, under\na signal condition which is considerably weaker than the traditional PE\ncondition and coincides with the weakest possible excitation known for the\nclassical least square algorithm of stochastic regression models. Convergence\nof adaptive predictors and their applications in adaptive control are also\ndiscussed.",
          "link": "http://arxiv.org/abs/2107.03588",
          "publishedOn": "2021-07-09T01:58:27.770Z",
          "wordCount": 627,
          "title": "Identification and Adaptation with Binary-Valued Observations under Non-Persistent Excitation Condition. (arXiv:2107.03588v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03738",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Papanikolaou_S/0/1/0/all/0/1\">Stefanos Papanikolaou</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Alava_M/0/1/0/all/0/1\">Mikko J. Alava</a>",
          "description": "Plastic yielding in solids strongly depends on various conditions, such as\ntemperature and loading rate and indeed, sample-dependent knowledge of yield\npoints in structural materials promotes reliability in mechanical behavior.\nCommonly, yielding is measured through controlled mechanical testing at small\nor large scales, in ways that either distinguish elastic (stress) from total\ndeformation measurements, or by identifying plastic slip contributions. In this\npaper we argue that instead of separate elastic/plastic measurements, yielding\ncan be unraveled through statistical analysis of total strain fluctuations\nduring the evolution sequence of profiles measured in-situ, through digital\nimage correlation. We demonstrate two distinct ways of precisely quantifying\nyield locations in widely applicable crystal plasticity models, that apply in\npolycrystalline solids, either by using principal component analysis or\ndiscrete wavelet transforms. We test and compare these approaches in synthetic\ndata of polycrystal simulations and a variety of yielding responses, through\nchanges of the applied loading rates and the strain-rate sensitivity exponents.",
          "link": "http://arxiv.org/abs/2107.03738",
          "publishedOn": "2021-07-09T01:58:27.755Z",
          "wordCount": 604,
          "title": "Direct detection of plasticity onset through total-strain profile evolution. (arXiv:2107.03738v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1\">Vitchyr H. Pong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Ashvin Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Laura Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Catherine Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Meta-reinforcement learning (RL) can be used to train policies that quickly\nadapt to new tasks with orders of magnitude less data than standard RL, but\nthis fast adaptation often comes at the cost of greatly increasing the amount\nof reward supervision during meta-training time. Offline meta-RL removes the\nneed to continuously provide reward supervision because rewards must only be\nprovided once when the offline dataset is generated. In addition to the\nchallenges of offline RL, a unique distribution shift is present in meta RL:\nagents learn exploration strategies that can gather the experience needed to\nlearn a new task, and also learn adaptation strategies that work well when\npresented with the trajectories in the dataset, but the adaptation strategies\nare not adapted to the data distribution that the learned exploration\nstrategies collect. Unlike the online setting, the adaptation and exploration\nstrategies cannot effectively adapt to each other, resulting in poor\nperformance. In this paper, we propose a hybrid offline meta-RL algorithm,\nwhich uses offline data with rewards to meta-train an adaptive policy, and then\ncollects additional unsupervised online data, without any ground truth reward\nlabels, to bridge this distribution shift problem. Our method uses the offline\ndata to learn the distribution of reward functions, which is then sampled to\nself-supervise reward labels for the additional online data. By removing the\nneed to provide reward labels for the online experience, our approach can be\nmore practical to use in settings where reward supervision would otherwise be\nprovided manually. We compare our method to prior work on offline meta-RL on\nsimulated robot locomotion and manipulation tasks and find that using\nadditional data and self-generated rewards significantly improves an agent's\nability to generalize.",
          "link": "http://arxiv.org/abs/2107.03974",
          "publishedOn": "2021-07-09T01:58:27.748Z",
          "wordCount": 722,
          "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision. (arXiv:2107.03974v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trindade_S/0/1/0/all/0/1\">Silvana Trindade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bittencourt_L/0/1/0/all/0/1\">Luiz F. Bittencourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_N/0/1/0/all/0/1\">Nelson L. S. da Fonseca</a>",
          "description": "Federated learning has been explored as a promising solution for training at\nthe edge, where end devices collaborate to train models without sharing data\nwith other entities. Since the execution of these learning models occurs at the\nedge, where resources are limited, new solutions must be developed. In this\npaper, we describe the recent work on resource management at the edge, and\nexplore the challenges and future directions to allow the execution of\nfederated learning at the edge. Some of the problems of this management, such\nas discovery of resources, deployment, load balancing, migration, and energy\nefficiency will be discussed in the paper.",
          "link": "http://arxiv.org/abs/2107.03428",
          "publishedOn": "2021-07-09T01:58:27.731Z",
          "wordCount": 557,
          "title": "Management of Resource at the Network Edge for Federated Learning. (arXiv:2107.03428v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1\">Gonzalo N&#xe1;poles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salgueiro_Y/0/1/0/all/0/1\">Yamisleydi Salgueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grau_I/0/1/0/all/0/1\">Isel Grau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_M/0/1/0/all/0/1\">Maikel Leon Espinosa</a>",
          "description": "Machine learning solutions for pattern classification problems are nowadays\nwidely deployed in society and industry. However, the lack of transparency and\naccountability of most accurate models often hinders their meaningful and safe\nuse. Thus, there is a clear need for developing explainable artificial\nintelligence mechanisms. There exist model-agnostic methods that summarize\nfeature contributions, but their interpretability is limited to specific\npredictions made by black-box models. An open challenge is to develop models\nthat have intrinsic interpretability and produce their own explanations, even\nfor classes of models that are traditionally considered black boxes like\n(recurrent) neural networks. In this paper, we propose an LTCN-based model for\ninterpretable pattern classification of structured data. Our method brings its\nown mechanism for providing explanations by quantifying the relevance of each\nfeature in the decision process. For supporting the interpretability without\naffecting the performance, the model incorporates more flexibility through a\nquasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides,\nwe propose a recurrence-aware decision model that evades the issues posed by\nunique fixed points while introducing a deterministic learning method to\ncompute the learnable parameters. The simulations show that our interpretable\nmodel obtains competitive performance when compared to the state-of-the-art\nwhite and black boxes.",
          "link": "http://arxiv.org/abs/2107.03423",
          "publishedOn": "2021-07-09T01:58:27.725Z",
          "wordCount": 629,
          "title": "Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification. (arXiv:2107.03423v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet. This is fully probabilistic,\nauto-regressive, and causal, i.e. each sample generated depends only on the\npreviously observed samples. Our approach outperforms a widely used wavenet\narchitecture by up to 9% on a similar dataset for predicting the next step.\nUsing the attention mechanism, we enable the architecture to learn which audio\nsamples are important for the prediction of the future sample. We show how\ncausal transformer generative models can be used for raw waveform synthesis. We\nalso show that this performance can be improved by another 2% by conditioning\nsamples over a wider context. The flexibility of the current model to\nsynthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-09T01:58:27.719Z",
          "wordCount": 646,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03520",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Al_Abiad_M/0/1/0/all/0/1\">Mohammed S. Al-Abiad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Md. Zoheb Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Jahangir Hossain</a>",
          "description": "We investigate resource allocation scheme to reduce the energy consumption of\nfederated learning (FL) in the integrated fog-cloud computing enabled\nInternet-of-things (IoT) networks. In the envisioned system, IoT devices are\nconnected with the centralized cloud server (CS) via multiple fog access points\n(F-APs). We consider two different scenarios for training the local models. In\nthe first scenario, local models are trained at the IoT devices and the F-APs\nupload the local model parameters to the CS. In the second scenario, local\nmodels are trained at the F-APs based on the collected data from the IoT\ndevices and the F-APs collaborate with the CS for updating the model\nparameters. Our objective is to minimize the overall energy-consumption of both\nscenarios subject to FL time constraint. Towards this goal, we devise a joint\noptimization of scheduling of IoT devices with the F-APs, transmit power\nallocation, computation frequency allocation at the devices and F-APs and\ndecouple it into two subproblems. In the first subproblem, we optimize the IoT\ndevice scheduling and power allocation, while in the second subproblem, we\noptimize the computation frequency allocation. For each scenario, we develop a\nconflict graph based solution to iteratively solve the two subproblems.\nSimulation results show that the proposed two schemes achieve a considerable\nperformance gain in terms of the energy consumption minimization. The presented\nsimulation results interestingly reveal that for a large number of IoT devices\nand large data sizes, it is more energy efficient to train the local models at\nthe IoT devices instead of the F-APs.",
          "link": "http://arxiv.org/abs/2107.03520",
          "publishedOn": "2021-07-09T01:58:27.699Z",
          "wordCount": 702,
          "title": "Energy Efficient Federated Learning in Integrated Fog-Cloud Computing Enabled Internet-of-Things Networks. (arXiv:2107.03520v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plank_P/0/1/0/all/0/1\">Philipp Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Arjun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_A/0/1/0/all/0/1\">Andreas Wild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1\">Wolfgang Maass</a>",
          "description": "In spite of intensive efforts it has remained an open problem to what extent\ncurrent Artificial Intelligence (AI) methods that employ Deep Neural Networks\n(DNNs) can be implemented more energy-efficiently on spike-based neuromorphic\nhardware. This holds in particular for AI methods that solve sequence\nprocessing tasks, a primary application target for spike-based neuromorphic\nhardware. One difficulty is that DNNs for such tasks typically employ Long\nShort-Term Memory (LSTM) units. Yet an efficient emulation of these units in\nspike-based hardware has been missing. We present a biologically inspired\nsolution that solves this problem. This solution enables us to implement a\nmajor class of DNNs for sequence processing tasks such as time series\nclassification and question answering with substantial energy savings on\nneuromorphic hardware. In fact, the Relational Network for reasoning about\nrelations between objects that we use for question answering is the first\nexample of a large DNN that carries out a sequence processing task with\nsubstantial energy-saving on neuromorphic hardware.",
          "link": "http://arxiv.org/abs/2107.03992",
          "publishedOn": "2021-07-09T01:58:27.686Z",
          "wordCount": 623,
          "title": "A Long Short-Term Memory for AI Applications in Spike-based Neuromorphic Hardware. (arXiv:2107.03992v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03920",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dalmasso_N/0/1/0/all/0/1\">Niccol&#xf2; Dalmasso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1\">David Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1\">Rafael Izbicki</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1\">Ann B. Lee</a>",
          "description": "Many areas of science make extensive use of computer simulators that\nimplicitly encode likelihood functions for complex systems. Classical\nstatistical methods are poorly suited for these so-called likelihood-free\ninference (LFI) settings, outside the asymptotic and low-dimensional regimes.\nAlthough new machine learning methods, such as normalizing flows, have\nrevolutionized the sample efficiency and capacity of LFI methods, it remains an\nopen question whether they produce reliable measures of uncertainty. In this\npaper, we present a statistical framework for LFI that unifies classical\nstatistics with modern machine learning to: (1) construct frequentist\nconfidence sets and hypothesis tests with finite-sample guarantees of nominal\ncoverage (type I error control) and power, and (2) provide rigorous diagnostics\nfor assessing empirical coverage over the entire parameter space. We refer to\nour framework as likelihood-free frequentist inference (LF2I). Any method that\nestimates a test statistic, such as the likelihood ratio, can be plugged into\nour framework to create powerful tests and confidence sets with correct\ncoverage. In this work, we specifically study two test statistics (ACORE and\nBFF), which, respectively, maximize versus integrate an odds function over the\nparameter space. Our theoretical and empirical results offer multifaceted\nperspectives on error sources and challenges in likelihood-free frequentist\ninference.",
          "link": "http://arxiv.org/abs/2107.03920",
          "publishedOn": "2021-07-09T01:58:27.643Z",
          "wordCount": 657,
          "title": "Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification. (arXiv:2107.03920v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1909.11201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengjiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shusen Wang</a>",
          "description": "Collaborative learning allows participants to jointly train a model without\ndata sharing. To update the model parameters, the central server broadcasts\nmodel parameters to the clients, and the clients send updating directions such\nas gradients to the server. While data do not leave a client device, the\ncommunicated gradients and parameters will leak a client's privacy. Attacks\nthat infer clients' privacy from gradients and parameters have been developed\nby prior work. Simple defenses such as dropout and differential privacy either\nfail to defend the attacks or seriously hurt test accuracy.\n\nWe propose a practical defense which we call Double-Blind Collaborative\nLearning (DBCL). The high-level idea is to apply random matrix sketching to the\nparameters (aka weights) and re-generate random sketching after each iteration.\nDBCL prevents clients from conducting gradient-based privacy inferences which\nare the most effective attacks. DBCL works because from the attacker's\nperspective, sketching is effectively random noise that outweighs the signal.\nNotably, DBCL does not much increase computation and communication costs and\ndoes not hurt test accuracy at all.",
          "link": "http://arxiv.org/abs/1909.11201",
          "publishedOn": "2021-07-09T01:58:27.634Z",
          "wordCount": 658,
          "title": "Matrix Sketching for Secure Collaborative Machine Learning. (arXiv:1909.11201v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1\">Amin Nikanjam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>",
          "description": "Nowadays, we are witnessing an increasing adoption of Deep Learning (DL)\nbased software systems in many industries. Designing a DL program requires\nconstructing a deep neural network (DNN) and then training it on a dataset.\nThis process requires that developers make multiple architectural (e.g., type,\nsize, number, and order of layers) and configuration (e.g., optimizer,\nregularization methods, and activation functions) choices that affect the\nquality of the DL models, and consequently software quality. An under-specified\nor poorly-designed DL model may train successfully but is likely to perform\npoorly when deployed in production. Design smells in DL programs are poor\ndesign and-or configuration decisions taken during the development of DL\ncomponents, that are likely to have a negative impact on the performance (i.e.,\nprediction accuracy) and then quality of DL based software systems. In this\npaper, we present a catalogue of 8 design smells for a popular DL architecture,\nnamely deep Feedforward Neural Networks which is widely employed in industrial\napplications. The design smells were identified through a review of the\nexisting literature on DL design and a manual inspection of 659 DL programs\nwith performance issues and design inefficiencies. The smells are specified by\ndescribing their context, consequences, and recommended refactorings. To\nprovide empirical evidence on the relevance and perceived impact of the\nproposed design smells, we conducted a survey with 81 DL developers. In\ngeneral, the developers perceived the proposed design smells as reflective of\ndesign or implementation problems, with agreement levels varying between 47\\%\nand 68\\%.",
          "link": "http://arxiv.org/abs/2107.02279",
          "publishedOn": "2021-07-09T01:58:27.622Z",
          "wordCount": 704,
          "title": "Design Smells in Deep Learning Programs: An Empirical Study. (arXiv:2107.02279v2 [cs.SE] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asam_M/0/1/0/all/0/1\">Muhammad Asam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1\">Tauseef Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahoora_U/0/1/0/all/0/1\">Umme Zahoora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>",
          "description": "Malicious activities in cyberspace have gone further than simply hacking\nmachines and spreading viruses. It has become a challenge for a nations\nsurvival and hence has evolved to cyber warfare. Malware is a key component of\ncyber-crime, and its analysis is the first line of defence against attack. This\nwork proposes a novel deep boosted hybrid learning-based malware classification\nframework and named as Deep boosted Feature Space-based Malware classification\n(DFS-MC). In the proposed framework, the discrimination power is enhanced by\nfusing the feature spaces of the best performing customized CNN architectures\nmodels and its discrimination by an SVM for classification. The discrimination\ncapacity of the proposed classification framework is assessed by comparing it\nagainst the standard customized CNNs. The customized CNN models are implemented\nin two ways: softmax classifier and deep hybrid learning-based malware\nclassification. In the hybrid learning, Deep features are extracted from\ncustomized CNN architectures and fed into the conventional machine learning\nclassifier to improve the classification performance. We also introduced the\nconcept of transfer learning in a customized CNN architecture based malware\nclassification framework through fine-tuning. The performance of the proposed\nmalware classification approaches are validated on the MalImg malware dataset\nusing the hold-out cross-validation technique. Experimental comparisons were\nconducted by employing innovative, customized CNN, trained from scratch and\nfine-tuning the customized CNN using transfer learning. The proposed\nclassification framework DFS-MC showed improved results, Accuracy: 98.61%,\nF-score: 0.96, Precision: 0.96, and Recall: 0.96.",
          "link": "http://arxiv.org/abs/2107.04008",
          "publishedOn": "2021-07-09T01:58:27.569Z",
          "wordCount": 677,
          "title": "Malware Classification Using Deep Boosted Learning. (arXiv:2107.04008v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1\">Mohamad Wehbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamann_T/0/1/0/all/0/1\">Tim Hamann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_J/0/1/0/all/0/1\">Jens Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1\">Bjoern Eskofier</a>",
          "description": "Online handwriting recognition has been studied for a long time with only few\npracticable results when writing on normal paper. Previous approaches using\nsensor-based devices encountered problems that limited the usage of the\ndeveloped systems in real-world applications. This paper presents a\nwriter-independent system that recognizes characters written on plain paper\nwith the use of a sensor-equipped pen. This system is applicable in real-world\napplications and requires no user-specific training for recognition. The pen\nprovides linear acceleration, angular velocity, magnetic field, and force\napplied by the user, and acts as a digitizer that transforms the analogue\nsignals of the sensors into timeseries data while writing on regular paper. The\ndataset we collected with this pen consists of Latin lower-case and upper-case\nalphabets. We present the results of a convolutional neural network model for\nletter classification and show that this approach is practical and achieves\npromising results for writer-independent character recognition. This work aims\nat providing a realtime handwriting recognition system to be used for writing\non normal paper.",
          "link": "http://arxiv.org/abs/2107.03704",
          "publishedOn": "2021-07-09T01:58:27.552Z",
          "wordCount": 614,
          "title": "Digitizing Handwriting with a Sensor Pen: A Writer-Independent Recognizer. (arXiv:2107.03704v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>",
          "description": "We propose a black-box reduction that turns a certain reinforcement learning\nalgorithm with optimal regret in a (near-)stationary environment into another\nalgorithm with optimal dynamic regret in a non-stationary environment,\nimportantly without any prior knowledge on the degree of non-stationarity. By\nplugging different algorithms into our black-box, we provide a list of examples\nshowing that our approach not only recovers recent results for (contextual)\nmulti-armed bandits achieved by very specialized algorithms, but also\nsignificantly improves the state of the art for (generalized) linear bandits,\nepisodic MDPs, and infinite-horizon MDPs in various ways. Specifically, in most\ncases our algorithm achieves the optimal dynamic regret\n$\\widetilde{\\mathcal{O}}(\\min\\{\\sqrt{LT}, \\Delta^{1/3}T^{2/3}\\})$ where $T$ is\nthe number of rounds and $L$ and $\\Delta$ are the number and amount of changes\nof the world respectively, while previous works only obtain suboptimal bounds\nand/or require the knowledge of $L$ and $\\Delta$.",
          "link": "http://arxiv.org/abs/2102.05406",
          "publishedOn": "2021-07-09T01:58:27.546Z",
          "wordCount": 607,
          "title": "Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach. (arXiv:2102.05406v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.08100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kherad_M/0/1/0/all/0/1\">Mahdi Kherad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidgoly_A/0/1/0/all/0/1\">Amir Jalaly Bidgoly</a>",
          "description": "When a user connects to the Internet to fulfill his needs, he often\nencounters a huge amount of related information. Recommender systems are the\ntechniques for massively filtering information and offering the items that\nusers find them satisfying and interesting. The advances in machine learning\nmethods, especially deep learning, have led to great achievements in\nrecommender systems, although these systems still suffer from challenges such\nas cold-start and sparsity problems. To solve these problems, context\ninformation such as user communication network is usually used. In this paper,\nwe have proposed a novel recommendation method based on Matrix Factorization\nand graph analysis methods. In addition, we leverage deep Autoencoders to\ninitialize users and items latent factors, and deep embedding method gathers\nusers' latent factors from the user trust graph. The proposed method is\nimplemented on two standard datasets. The experimental results and comparisons\ndemonstrate that the proposed approach is superior to the existing\nstate-of-the-art recommendation methods. Our approach outperforms other\ncomparative methods and achieves great improvements. This work has been\nsubmitted to the IEEE for possible publication. Copyright may be transferred\nwithout notice, after which this version may no longer be accessible",
          "link": "http://arxiv.org/abs/2004.08100",
          "publishedOn": "2021-07-09T01:58:27.539Z",
          "wordCount": 719,
          "title": "Recommendation system using a deep learning and graph analysis approach. (arXiv:2004.08100v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cantador_I/0/1/0/all/0/1\">Iv&#xe1;n Cantador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diez_F/0/1/0/all/0/1\">Fernando Diez</a>",
          "description": "The success of neural network embeddings has entailed a renewed interest in\nusing knowledge graphs for a wide variety of machine learning and information\nretrieval tasks. In particular, recent recommendation methods based on graph\nembeddings have shown state-of-the-art performance. In general, these methods\nencode latent rating patterns and content features. Differently from previous\nwork, in this paper, we propose to exploit embeddings extracted from graphs\nthat combine information from ratings and aspect-based opinions expressed in\ntextual reviews. We then adapt and evaluate state-of-the-art graph embedding\ntechniques over graphs generated from Amazon and Yelp reviews on six domains,\noutperforming baseline recommenders. Additionally, our method has the advantage\nof providing explanations that involve the coverage of aspect-based opinions\ngiven by users about recommended items.",
          "link": "http://arxiv.org/abs/2107.03385",
          "publishedOn": "2021-07-09T01:58:27.533Z",
          "wordCount": 568,
          "title": "Rating and aspect-based opinion graph embeddings for explainable recommendations. (arXiv:2107.03385v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Midtfjord_A/0/1/0/all/0/1\">Alise Danielle Midtfjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_R/0/1/0/all/0/1\">Riccardo De Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huseby_A/0/1/0/all/0/1\">Arne Bang Huseby</a>",
          "description": "The presence of snow and ice on runway surfaces reduces the available\ntire-pavement friction needed for retardation and directional control and\ncauses potential economic and safety threats for the aviation industry during\nthe winter seasons. To activate appropriate safety procedures, pilots need\naccurate and timely information on the actual runway surface conditions. In\nthis study, XGBoost is used to create a combined runway assessment system,\nwhich includes a classifcation model to predict slippery conditions and a\nregression model to predict the level of slipperiness. The models are trained\non weather data and data from runway reports. The runway surface conditions are\nrepresented by the tire-pavement friction coefficient, which is estimated from\nflight sensor data from landing aircrafts. To evaluate the performance of the\nmodels, they are compared to several state-of-the-art runway assessment\nmethods. The XGBoost models identify slippery runway conditions with a ROC AUC\nof 0.95, predict the friction coefficient with a MAE of 0.0254, and outperforms\nall the previous methods. The results show the strong abilities of machine\nlearning methods to model complex, physical phenomena with a good accuracy when\ndomain knowledge is used in the variable extraction. The XGBoost models are\ncombined with SHAP (SHapley Additive exPlanations) approximations to provide a\ncomprehensible decision support system for airport operators and pilots, which\ncan contribute to safer and more economic operations of airport runways.",
          "link": "http://arxiv.org/abs/2107.04010",
          "publishedOn": "2021-07-09T01:58:27.508Z",
          "wordCount": 682,
          "title": "A Machine Learning Approach to Safer Airplane Landings: Predicting Runway Conditions using Weather and Flight Data. (arXiv:2107.04010v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03402",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Katsnelson_M/0/1/0/all/0/1\">Mikhail I. Katsnelson</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Vanchurin_V/0/1/0/all/0/1\">Vitaly Vanchurin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Westerhout_T/0/1/0/all/0/1\">Tom Westerhout</a>",
          "description": "We demonstrate, both analytically and numerically, that learning dynamics of\nneural networks is generically attracted towards a self-organized critical\nstate. The effect can be modeled with quartic interactions between\nnon-trainable variables (e.g. states of neurons) and trainable variables (e.g.\nweight matrix). Non-trainable variables are rapidly driven towards stochastic\nequilibrium and trainable variables are slowly driven towards learning\nequilibrium described by a scale-invariant distribution on a wide range of\nscales. Our results suggest that the scale invariance observed in many physical\nand biological systems might be due to some kind of learning dynamics and\nsupport the claim that the universe might be a neural network.",
          "link": "http://arxiv.org/abs/2107.03402",
          "publishedOn": "2021-07-09T01:58:27.501Z",
          "wordCount": 544,
          "title": "Self-organized criticality in neural networks. (arXiv:2107.03402v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kundu/0/1/0/all/0/1\">Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debasish/0/1/0/all/0/1\">Debasish</a>",
          "description": "This paper presents a multiple learner algorithm called the 'Three Ensemble\nClustering 3EC' algorithm that classifies unlabeled data into quality clusters\nas a part of unsupervised learning. It offers the flexibility to explore the\ncontext of new clusters formed by an ensemble of algorithms based on internal\nvalidation indices.\n\nIt is worth mentioning that the input data set is considered to be a cluster\nof clusters. An anomaly can possibly manifest as a cluster as well. Each\npartitioned cluster is considered to be a new data set and is a candidate to\nexplore the most optimal algorithm and its number of partition splits until a\npredefined stopping criteria is met. The algorithms independently partition the\ndata set into clusters and the quality of the partitioning is assessed by an\nensemble of internal cluster validation indices. The 3EC algorithm presents the\nvalidation index scores from a choice of algorithms and its configuration of\npartitions and it is called the Tau Grid. 3EC chooses the most optimal score.\nThe 3EC algorithm owes its name to the two input ensembles of algorithms and\ninternal validation indices and an output ensemble of final clusters.\n\nQuality plays an important role in this clustering approach and it also acts\nas a stopping criteria from further partitioning. Quality is determined based\non the quality of the clusters provided by an algorithm and its optimal number\nof splits. The 3EC algorithm determines this from the score of the ensemble of\nvalidation indices. The user can configure the stopping criteria by providing\nquality thresholds for the score range of each of the validation indices and\nthe optimal size of the output cluster. The users can experiment with different\nsets of stopping criteria and choose the most 'sensible group' of quality\nclusters",
          "link": "http://arxiv.org/abs/2107.03729",
          "publishedOn": "2021-07-09T01:58:27.494Z",
          "wordCount": 725,
          "title": "The Three Ensemble Clustering (3EC) Algorithm for Pattern Discovery in Unsupervised Learning. (arXiv:2107.03729v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Daniel Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Azer Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1\">Alex Gittens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yener_B/0/1/0/all/0/1\">B&#xfc;lent Yener</a>",
          "description": "Adversarial examples pose a threat to deep neural network models in a variety\nof scenarios, from settings where the adversary has complete knowledge of the\nmodel in a \"white box\" setting and to the opposite in a \"black box\" setting. In\nthis paper, we explore the use of output randomization as a defense against\nattacks in both the black box and white box models and propose two defenses. In\nthe first defense, we propose output randomization at test time to thwart\nfinite difference attacks in black box settings. Since this type of attack\nrelies on repeated queries to the model to estimate gradients, we investigate\nthe use of randomization to thwart such adversaries from successfully creating\nadversarial examples. We empirically show that this defense can limit the\nsuccess rate of a black box adversary using the Zeroth Order Optimization\nattack to 0%. Secondly, we propose output randomization training as a defense\nagainst white box adversaries. Unlike prior approaches that use randomization,\nour defense does not require its use at test time, eliminating the Backward\nPass Differentiable Approximation attack, which was shown to be effective\nagainst other randomization defenses. Additionally, this defense has low\noverhead and is easily implemented, allowing it to be used together with other\ndefenses across various model architectures. We evaluate output randomization\ntraining against the Projected Gradient Descent attacker and show that the\ndefense can reduce the PGD attack's success rate down to 12% when using\ncross-entropy loss.",
          "link": "http://arxiv.org/abs/2107.03806",
          "publishedOn": "2021-07-09T01:58:27.476Z",
          "wordCount": 697,
          "title": "Output Randomization: A Novel Defense for both White-box and Black-box Adversarial Models. (arXiv:2107.03806v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1904.05981",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Pal_S/0/1/0/all/0/1\">Soumik Pal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhu_Y/0/1/0/all/0/1\">Yizhe Zhu</a>",
          "description": "We consider the community detection problem in sparse random hypergraphs.\nAngelini et al. (2015) conjectured the existence of a sharp threshold on model\nparameters for community detection in sparse hypergraphs generated by a\nhypergraph stochastic block model. We solve the positive part of the conjecture\nfor the case of two blocks: above the threshold, there is a spectral algorithm\nwhich asymptotically almost surely constructs a partition of the hypergraph\ncorrelated with the true partition. Our method is a generalization to random\nhypergraphs of the method developed by Massouli\\'{e} (2014) for sparse random\ngraphs.",
          "link": "http://arxiv.org/abs/1904.05981",
          "publishedOn": "2021-07-09T01:58:27.468Z",
          "wordCount": 597,
          "title": "Community detection in the sparse hypergraph stochastic block model. (arXiv:1904.05981v6 [math.PR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulsky_Y/0/1/0/all/0/1\">Yury Sulsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1\">Arun Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruce_J/0/1/0/all/0/1\">Jake Bruce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1\">Rob Fergus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wayne_G/0/1/0/all/0/1\">Greg Wayne</a>",
          "description": "Imitation learning enables agents to reuse and adapt the hard-won expertise\nof others, offering a solution to several key challenges in learning behavior.\nAlthough it is easy to observe behavior in the real-world, the underlying\nactions may not be accessible. We present a new method for imitation solely\nfrom observations that achieves comparable performance to experts on\nchallenging continuous control tasks while also exhibiting robustness in the\npresence of observations unrelated to the task. Our method, which we call FORM\n(for \"Future Observation Reward Model\") is derived from an inverse RL objective\nand imitates using a model of expert behavior learned by generative modelling\nof the expert's observations, without needing ground truth actions. We show\nthat FORM performs comparably to a strong baseline IRL method (GAIL) on the\nDeepMind Control Suite benchmark, while outperforming GAIL in the presence of\ntask-irrelevant features.",
          "link": "http://arxiv.org/abs/2107.03851",
          "publishedOn": "2021-07-09T01:58:27.456Z",
          "wordCount": 574,
          "title": "Imitation by Predicting Observations. (arXiv:2107.03851v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1710.09064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kankanahalli_S/0/1/0/all/0/1\">Srihari Kankanahalli</a>",
          "description": "Modern compression algorithms are often the result of laborious\ndomain-specific research; industry standards such as MP3, JPEG, and AMR-WB took\nyears to develop and were largely hand-designed. We present a deep neural\nnetwork model which optimizes all the steps of a wideband speech coding\npipeline (compression, quantization, entropy coding, and decompression)\nend-to-end directly from raw speech data -- no manual feature engineering\nnecessary, and it trains in hours. In testing, our DNN-based coder performs on\npar with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps).\nIt also runs in realtime on a 3.8GhZ Intel CPU.",
          "link": "http://arxiv.org/abs/1710.09064",
          "publishedOn": "2021-07-09T01:58:27.445Z",
          "wordCount": 581,
          "title": "End-to-End Optimized Speech Coding with Deep Neural Networks. (arXiv:1710.09064v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03673",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1\">Lulu Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ma_Z/0/1/0/all/0/1\">Zheng Ma</a>",
          "description": "In this paper, we propose a model-operator-data network (MOD-Net) for solving\nPDEs. A MOD-Net is driven by a model to solve PDEs based on operator\nrepresentation with regularization from data. In this work, we use a deep\nneural network to parameterize the Green's function. The empirical risk\nconsists of the mean square of the governing equation, boundary conditions, and\na few labels, which are numerically computed by traditional schemes on coarse\ngrid points with cheap computation cost. With only the labeled dataset or only\nthe model constraints, it is insufficient to accurately train a MOD-Net for\ncomplicate problems. Intuitively, the labeled dataset works as a regularization\nin addition to the model constraints. The MOD-Net is much efficient than\noriginal neural operator because the MOD-Net also uses the information of\ngoverning equation and the boundary conditions of the PDE rather than purely\nthe expensive labels. Since the MOD-Net learns the Green's function of a PDE,\nit solves a type of PDEs but not a specific case. We numerically show MOD-Net\nis very efficient in solving Poisson equation and one-dimensional Boltzmann\nequation. For non-linear PDEs, where the concept of the Green's function does\nnot apply, the non-linear MOD-Net can be similarly used as an ansatz for\nsolving non-linear PDEs.",
          "link": "http://arxiv.org/abs/2107.03673",
          "publishedOn": "2021-07-09T01:58:27.432Z",
          "wordCount": 650,
          "title": "MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs. (arXiv:2107.03673v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuzhamuratov_A/0/1/0/all/0/1\">Arsen Kuzhamuratov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokin_D/0/1/0/all/0/1\">Dmitry Sorokin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulanov_A/0/1/0/all/0/1\">Alexander Ulanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lvovsky_A/0/1/0/all/0/1\">A. I. Lvovsky</a>",
          "description": "Animals have remarkable abilities to adapt locomotion to different terrains\nand tasks. However, robots trained by means of reinforcement learning are\ntypically able to solve only a single task and a transferred policy is usually\ninferior to that trained from scratch. In this work, we demonstrate that\nmeta-reinforcement learning can be used to successfully train a robot capable\nto solve a wide range of locomotion tasks. The performance of the meta-trained\nrobot is similar to that of a robot that is trained on a single task.",
          "link": "http://arxiv.org/abs/2107.03741",
          "publishedOn": "2021-07-09T01:58:27.327Z",
          "wordCount": 521,
          "title": "Adaptation of Quadruped Robot Locomotion with Meta-Learning. (arXiv:2107.03741v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gouttes_A/0/1/0/all/0/1\">Ad&#xe8;le Gouttes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasul_K/0/1/0/all/0/1\">Kashif Rasul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_M/0/1/0/all/0/1\">Mateusz Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephan_J/0/1/0/all/0/1\">Johannes Stephan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naghibi_T/0/1/0/all/0/1\">Tofigh Naghibi</a>",
          "description": "Here, we propose a general method for probabilistic time series forecasting.\nWe combine an autoregressive recurrent neural network to model temporal\ndynamics with Implicit Quantile Networks to learn a large class of\ndistributions over a time-series target. When compared to other probabilistic\nneural forecasting models on real- and simulated data, our approach is\nfavorable in terms of point-wise prediction accuracy as well as on estimating\nthe underlying temporal distribution.",
          "link": "http://arxiv.org/abs/2107.03743",
          "publishedOn": "2021-07-09T01:58:27.320Z",
          "wordCount": 513,
          "title": "Probabilistic Time Series Forecasting with Implicit Quantile Networks. (arXiv:2107.03743v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jetchev_N/0/1/0/all/0/1\">Nikolay Jetchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_G/0/1/0/all/0/1\">G&#xf6;khan Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bracher_C/0/1/0/all/0/1\">Christian Bracher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollgraf_R/0/1/0/all/0/1\">Roland Vollgraf</a>",
          "description": "Attention is a general reasoning mechanism than can flexibly deal with image\ninformation, but its memory requirements had made it so far impractical for\nhigh resolution image generation. We present Grid Partitioned Attention (GPA),\na new approximate attention algorithm that leverages a sparse inductive bias\nfor higher computational and memory efficiency in image domains: queries attend\nonly to few keys, spatially close queries attend to close keys due to\ncorrelations. Our paper introduces the new attention layer, analyzes its\ncomplexity and how the trade-off between memory usage and model power can be\ntuned by the hyper-parameters.We will show how such attention enables novel\ndeep learning architectures with copying modules that are especially useful for\nconditional image generation tasks like pose morphing. Our contributions are\n(i) algorithm and code1of the novel GPA layer, (ii) a novel deep\nattention-copying architecture, and (iii) new state-of-the art experimental\nresults in human pose morphing generation benchmarks.",
          "link": "http://arxiv.org/abs/2107.03742",
          "publishedOn": "2021-07-09T01:58:27.228Z",
          "wordCount": 607,
          "title": "Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation. (arXiv:2107.03742v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1\">Alexandra Peste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1\">Christoph H. Lampert</a>",
          "description": "The availability of large amounts of user-provided data has been key to the\nsuccess of machine learning for many real-world tasks. Recently, an increasing\nawareness has emerged that users should be given more control about how their\ndata is used. In particular, users should have the right to prohibit the use of\ntheir data for training machine learning systems, and to have it erased from\nalready trained systems. While several sample erasure methods have been\nproposed, all of them have drawbacks which have prevented them from gaining\nwidespread adoption. Most methods are either only applicable to very specific\nfamilies of models, sacrifice too much of the original model's accuracy, or\nthey have prohibitive memory or computational requirements. In this paper, we\npropose an efficient and effective algorithm, SSSE, for samples erasure, that\nis applicable to a wide class of machine learning models. From a second-order\nanalysis of the model's loss landscape we derive a closed-form update step of\nthe model parameters that only requires access to the data to be erased, not to\nthe original training set. Experiments on three datasets, CelebFaces attributes\n(CelebA), Animals with Attributes 2 (AwA2) and CIFAR10, show that in certain\ncases SSSE can erase samples almost as well as the optimal, yet impractical,\ngold standard of training a new model from scratch with only the permitted\ndata.",
          "link": "http://arxiv.org/abs/2107.03860",
          "publishedOn": "2021-07-09T01:58:27.174Z",
          "wordCount": 658,
          "title": "SSSE: Efficiently Erasing Samples from Trained Machine Learning Models. (arXiv:2107.03860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03730",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Qoku_A/0/1/0/all/0/1\">Arber Qoku</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Buettner_F/0/1/0/all/0/1\">Florian Buettner</a>",
          "description": "Latent variable models are powerful statistical tools that can uncover\nrelevant variation between patients or cells, by inferring unobserved hidden\nstates from observable high-dimensional data. A major shortcoming of current\nmethods, however, is their inability to learn sparse and interpretable hidden\nstates. Additionally, in settings where partial knowledge on the latent\nstructure of the data is readily available, a statistically sound integration\nof prior information into current methods is challenging. To address these\nissues, we propose spex-LVM, a factorial latent variable model with sparse\npriors to encourage the inference of explainable factors driven by\ndomain-relevant information. spex-LVM utilizes existing knowledge of curated\nbiomedical pathways to automatically assign annotated attributes to latent\nfactors, yielding interpretable results tailored to the corresponding domain of\ninterest. Evaluations on simulated and real single-cell RNA-seq datasets\ndemonstrate that our model robustly identifies relevant structure in an\ninherently explainable manner, distinguishes technical noise from sources of\nbiomedical variation, and provides dataset-specific adaptations of existing\npathway annotations. Implementation is available at\nhttps://github.com/MLO-lab/spexlvm.",
          "link": "http://arxiv.org/abs/2107.03730",
          "publishedOn": "2021-07-09T01:58:27.163Z",
          "wordCount": 621,
          "title": "Encoding Domain Information with Sparse Priors for Inferring Explainable Latent Variables. (arXiv:2107.03730v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03607",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Dodia_H/0/1/0/all/0/1\">Hrithika Dodia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tandel_H/0/1/0/all/0/1\">Himanshu Tandel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+DMello_L/0/1/0/all/0/1\">Lynette D&#x27;Mello</a>",
          "description": "Gravitational waves are ripples in the fabric of space-time that travel at\nthe speed of light. The detection of gravitational waves by LIGO is a major\nbreakthrough in the field of astronomy. Deep Learning has revolutionized many\nindustries including health care, finance and education. Deep Learning\ntechniques have also been explored for detection of gravitational waves to\novercome the drawbacks of traditional matched filtering method. However, in\nseveral researches, the training phase of neural network is very time consuming\nand hardware devices with large memory are required for the task. In order to\nreduce the extensive amount of hardware resources and time required in training\na neural network for detecting gravitational waves, we made SpecGrav. We use 2D\nConvolutional Neural Network and spectrograms of gravitational waves embedded\nin noise to detect gravitational waves from binary black hole merger and binary\nneutron star merger. The training phase of our neural network was of about just\n19 minutes on a 2GB GPU.",
          "link": "http://arxiv.org/abs/2107.03607",
          "publishedOn": "2021-07-09T01:58:27.110Z",
          "wordCount": 598,
          "title": "SpecGrav -- Detection of Gravitational Waves using Deep Learning. (arXiv:2107.03607v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghanathe_N/0/1/0/all/0/1\">Nikhil Pratap Ghanathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_V/0/1/0/all/0/1\">Vivek Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rahul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilton_S/0/1/0/all/0/1\">Steve Wilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aayan Kumar</a>",
          "description": "Recent breakthroughs in ML have produced new classes of models that allow ML\ninference to run directly on milliwatt-powered IoT devices. On one hand,\nexisting ML-to-FPGA compilers are designed for deep neural-networks on large\nFPGAs. On the other hand, general-purpose HLS tools fail to exploit properties\nspecific to ML inference, thereby resulting in suboptimal performance. We\npropose MAFIA, a tool to compile ML inference on small form-factor FPGAs for\nIoT applications. MAFIA provides native support for linear algebra operations\nand can express a variety of ML algorithms, including state-of-the-art models.\nWe show that MAFIA-generated programs outperform best-performing variant of a\ncommercial HLS compiler by 2.5x on average.",
          "link": "http://arxiv.org/abs/2107.03653",
          "publishedOn": "2021-07-09T01:58:27.104Z",
          "wordCount": 571,
          "title": "MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications. (arXiv:2107.03653v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fenaux_L/0/1/0/all/0/1\">Lucas Fenaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quintero_M/0/1/0/all/0/1\">Maria Juliana Quintero</a>",
          "description": "We will introduce BumbleBee, a transformer model that will generate MIDI\nmusic data . We will tackle the issue of transformers applied to long sequences\nby implementing a longformer generative model that uses dilating sliding\nwindows to compute the attention layers. We will compare our results to that of\nthe music transformer and Long-Short term memory (LSTM) to benchmark our\nresults. This analysis will be performed using piano MIDI files, in particular\n, the JSB Chorales dataset that has already been used for other research works\n(Huang et al., 2018)",
          "link": "http://arxiv.org/abs/2107.03443",
          "publishedOn": "2021-07-09T01:58:27.098Z",
          "wordCount": 531,
          "title": "BumbleBee: A Transformer for Music. (arXiv:2107.03443v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Ningyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xuefeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>",
          "description": "We study the model-based undiscounted reinforcement learning for partially\nobservable Markov decision processes (POMDPs). The oracle we consider is the\noptimal policy of the POMDP with a known environment in terms of the average\nreward over an infinite horizon. We propose a learning algorithm for this\nproblem, building on spectral method-of-moments estimations for hidden Markov\nmodels, the belief error control in POMDPs and upper-confidence-bound methods\nfor online learning. We establish a regret bound of $O(T^{2/3}\\sqrt{\\log T})$\nfor the proposed learning algorithm where $T$ is the learning horizon. This is,\nto the best of our knowledge, the first algorithm achieving sublinear regret\nwith respect to our oracle for learning general POMDPs.",
          "link": "http://arxiv.org/abs/2107.03635",
          "publishedOn": "2021-07-09T01:58:27.091Z",
          "wordCount": 540,
          "title": "Sublinear Regret for Learning POMDPs. (arXiv:2107.03635v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianwen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhihua Lin</a>",
          "description": "In existing deep learning methods, almost all loss functions assume that\nsample data values used to be predicted are the only correct ones. This\nassumption does not hold for laboratory test data. Test results are often\nwithin tolerable or imprecision ranges, with all values in the ranges\nacceptable. By considering imprecision samples, we propose an imprecision range\nloss (IR loss) method and incorporate it into Long Short Term Memory (LSTM)\nmodel for disease progress prediction. In this method, each sample in\nimprecision range space has a certain probability to be the real value,\nparticipating in the loss calculation. The loss is defined as the integral of\nthe error of each point in the impression range space. A sampling method for\nimprecision space is formulated. The continuous imprecision space is\ndiscretized, and a sequence of imprecise data sets are obtained, which is\nconvenient for gradient descent learning. A heuristic learning algorithm is\ndeveloped to learn the model parameters based on the imprecise data sets.\nExperimental results on real data show that the prediction method based on IR\nloss can provide more stable and consistent prediction result when test samples\nare generated from imprecision range.",
          "link": "http://arxiv.org/abs/2107.03620",
          "publishedOn": "2021-07-09T01:58:27.069Z",
          "wordCount": 621,
          "title": "Predicting Disease Progress with Imprecise Lab Test Results. (arXiv:2107.03620v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03846",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1\">Doaa Emam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1\">Nada Mufti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1\">Frederic Guffens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1\">Thomas Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demaerel_P/0/1/0/all/0/1\">Philippe Demaerel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1\">Andrew Melbourne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jam Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>",
          "description": "Deep neural networks have increased the accuracy of automatic segmentation,\nhowever, their accuracy depends on the availability of a large number of fully\nsegmented images. Methods to train deep neural networks using images for which\nsome, but not all, regions of interest are segmented are necessary to make\nbetter use of partially annotated datasets. In this paper, we propose the first\naxiomatic definition of label-set loss functions that are the loss functions\nthat can handle partially segmented images. We prove that there is one and only\none method to convert a classical loss function for fully segmented images into\na proper label-set loss function. Our theory also allows us to define the\nleaf-Dice loss, a label-set generalization of the Dice loss particularly suited\nfor partial supervision with only missing labels. Using the leaf-Dice loss, we\nset a new state of the art in partially supervised learning for fetal brain 3D\nMRI segmentation. We achieve a deep neural network able to segment white\nmatter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter,\ndeep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of\nanatomically normal fetuses or with open spina bifida. Our implementation of\nthe proposed label-set loss functions is available at\nhttps://github.com/LucasFidon/label-set-loss-functions",
          "link": "http://arxiv.org/abs/2107.03846",
          "publishedOn": "2021-07-09T01:58:27.054Z",
          "wordCount": 687,
          "title": "Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation. (arXiv:2107.03846v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_B/0/1/0/all/0/1\">Bong-Chul Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "Effective environmental planning and management to address climate change\ncould be achieved through extensive environmental modeling with machine\nlearning and conventional physical models. In order to develop and improve\nthese models, practitioners and researchers need comprehensive benchmark\ndatasets that are prepared and processed with environmental expertise that they\ncan rely on. This study presents an extensive dataset of rainfall events for\nthe state of Iowa (2016-2019) acquired from the National Weather Service Next\nGeneration Weather Radar (NEXRAD) system and processed by a quantitative\nprecipitation estimation system. The dataset presented in this study could be\nused for better disaster monitoring, response and recovery by paving the way\nfor both predictive and prescriptive modeling.",
          "link": "http://arxiv.org/abs/2107.03432",
          "publishedOn": "2021-07-09T01:58:27.048Z",
          "wordCount": 572,
          "title": "IowaRain: A Statewide Rain Event Dataset Based on Weather Radars and Quantitative Precipitation Estimation. (arXiv:2107.03432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goucher_A/0/1/0/all/0/1\">Adam P. Goucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troll_R/0/1/0/all/0/1\">Rajan Troll</a>",
          "description": "We introduce a differentiable random access memory module with $O(1)$\nperformance regardless of size, scaling to billions of entries. The design\nstores entries on points of a chosen lattice to calculate nearest neighbours of\narbitrary points efficiently by exploiting symmetries. Augmenting a standard\nneural network architecture with a single memory layer based on this, we can\nscale the parameter count up to memory limits with negligible computational\noverhead, giving better accuracy at similar cost. On large language modelling\ntasks, these enhanced models with larger capacity significantly outperform the\nunmodified transformer baseline. We found continued scaling with memory size up\nto the limits tested.",
          "link": "http://arxiv.org/abs/2107.03474",
          "publishedOn": "2021-07-09T01:58:27.041Z",
          "wordCount": 535,
          "title": "Differentiable Random Access Memory using Lattices. (arXiv:2107.03474v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03442",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hamghalam_M/0/1/0/all/0/1\">Mohammad Hamghalam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simpson_A/0/1/0/all/0/1\">Amber L. Simpson</a>",
          "description": "In large studies involving multi protocol Magnetic Resonance Imaging (MRI),\nit can occur to miss one or more sub-modalities for a given patient owing to\npoor quality (e.g. imaging artifacts), failed acquisitions, or hallway\ninterrupted imaging examinations. In some cases, certain protocols are\nunavailable due to limited scan time or to retrospectively harmonise the\nimaging protocols of two independent studies. Missing image modalities pose a\nchallenge to segmentation frameworks as complementary information contributed\nby the missing scans is then lost. In this paper, we propose a novel model,\nMulti-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute\none or more missing sub-modalities for a patient scan. MGP-VAE can leverage the\nGaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the\nsubjects/patients and sub-modalities correlations. Instead of designing one\nnetwork for each possible subset of present sub-modalities or using frameworks\nto mix feature maps, missing data can be generated from a single model based on\nall the available samples. We show the applicability of MGP-VAE on brain tumor\nsegmentation where either, two, or three of four sub-modalities may be missing.\nOur experiments against competitive segmentation baselines with missing\nsub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE\nmodel for segmentation tasks.",
          "link": "http://arxiv.org/abs/2107.03442",
          "publishedOn": "2021-07-09T01:58:27.021Z",
          "wordCount": 666,
          "title": "Modality Completion via Gaussian Process Prior Variational Autoencoders for Multi-Modal Glioma Segmentation. (arXiv:2107.03442v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1\">Xingtai Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiyang Zhang</a>",
          "description": "Intelligent diagnosis method based on data-driven and deep learning is an\nattractive and meaningful field in recent years. However, in practical\napplication scenarios, the imbalance of time-series fault is an urgent problem\nto be solved. From the perspective of Bayesian probability, this paper analyzes\nhow to improve the performance of imbalanced classification by adjusting the\ndistance between classes and the distribution within a class and proposes a\ntime-series fault diagnosis model based on deep metric learning. As a core of\ndeep metric learning, a novel quadruplet data pair design considering imbalance\nclass is proposed with reference to traditional deep metric learning. Based on\nsuch data pair, this paper proposes a quadruplet loss function which takes into\naccount the inter-class distance and the intra-class data distribution, and\npays special attention to imbalanced sample pairs. The reasonable combination\nof quadruplet loss and softmax loss function can reduce the impact of\nimbalance. Experiments on two open datasets are carried out to verify the\neffectiveness and robustness of the model. Experimental results show that the\nproposed method can effectively improve the performance of imbalanced\nclassification.",
          "link": "http://arxiv.org/abs/2107.03786",
          "publishedOn": "2021-07-09T01:58:27.013Z",
          "wordCount": 615,
          "title": "Quadruplet Deep Metric Learning Model for Imbalanced Time-series Fault Diagnosis. (arXiv:2107.03786v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yikang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhao Zhong</a>",
          "description": "In this paper, we propose a Collaboration of Experts (CoE) framework to pool\ntogether the expertise of multiple networks towards a common aim. Each expert\nis an individual network with expertise on a unique portion of the dataset,\nwhich enhances the collective capacity. Given a sample, an expert is selected\nby the delegator, which simultaneously outputs a rough prediction to support\nearly termination. To fulfill this framework, we propose three modules to impel\neach model to play its role, namely weight generation module (WGM), label\ngeneration module (LGM) and variance calculation module (VCM). Our method\nachieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy\nwith 194M FLOPs. Combined with PWLU activation function and CondConv, CoE\nfurther achieves the accuracy of 80.0% with only 100M FLOPs for the first time.\nMore importantly, our method is hardware friendly and achieves a 3-6x speedup\ncompared with some existing conditional computation approaches.",
          "link": "http://arxiv.org/abs/2107.03815",
          "publishedOn": "2021-07-09T01:58:27.006Z",
          "wordCount": 594,
          "title": "Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03455",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1\">Avishek Ghosh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sankararaman_A/0/1/0/all/0/1\">Abishek Sankararaman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>",
          "description": "We consider the problem of model selection for the general stochastic\ncontextual bandits under the realizability assumption. We propose a successive\nrefinement based algorithm called Adaptive Contextual Bandit ({\\ttfamily ACB}),\nthat works in phases and successively eliminates model classes that are too\nsimple to fit the given instance. We prove that this algorithm is adaptive,\ni.e., the regret rate order-wise matches that of {\\ttfamily FALCON}, the\nstate-of-art contextual bandit algorithm of Levi et. al '20, that needs\nknowledge of the true model class. The price of not knowing the correct model\nclass is only an additive term contributing to the second order term in the\nregret bound. This cost possess the intuitive property that it becomes smaller\nas the model class becomes easier to identify, and vice-versa. We then show\nthat a much simpler explore-then-commit (ETC) style algorithm also obtains a\nregret rate of matching that of {\\ttfamily FALCON}, despite not knowing the\ntrue model class. However, the cost of model selection is higher in ETC as\nopposed to in {\\ttfamily ACB}, as expected. Furthermore, {\\ttfamily ACB}\napplied to the linear bandit setting with unknown sparsity, order-wise recovers\nthe model selection guarantees previously established by algorithms tailored to\nthe linear setting.",
          "link": "http://arxiv.org/abs/2107.03455",
          "publishedOn": "2021-07-09T01:58:26.990Z",
          "wordCount": 648,
          "title": "Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael A. Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alex Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>",
          "description": "Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning,\nco-located with ICLR 2021. In this workshop, we want to advance theory, methods\nand tools for allowing experts to express prior coded knowledge for automatic\ndata annotations that can be used to train arbitrary deep neural networks for\nprediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing\nmethods that help modern machine-learning methods to generalize from knowledge\nprovided by experts, in interaction with observable (unlabeled) data. In total,\n15 papers were accepted. All the accepted contributions are listed in these\nProceedings.",
          "link": "http://arxiv.org/abs/2107.03690",
          "publishedOn": "2021-07-09T01:58:26.960Z",
          "wordCount": 534,
          "title": "Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL). (arXiv:2107.03690v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moldoveanu_M/0/1/0/all/0/1\">Matei Moldoveanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_A/0/1/0/all/0/1\">Abdellatif Zaidi</a>",
          "description": "It is widely perceived that leveraging the success of modern machine learning\ntechniques to mobile devices and wireless networks has the potential of\nenabling important new services. This, however, poses significant challenges,\nessentially due to that both data and processing power are highly distributed\nin a wireless network. In this paper, we develop a learning algorithm and an\narchitecture that make use of multiple data streams and processing units, not\nonly during the training phase but also during the inference phase. In\nparticular, the analysis reveals how inference propagates and fuses across a\nnetwork. We study the design criterion of our proposed method and its bandwidth\nrequirements. Also, we discuss implementation aspects using neural networks in\ntypical wireless radio access; and provide experiments that illustrate benefits\nover state-of-the-art techniques.",
          "link": "http://arxiv.org/abs/2107.03433",
          "publishedOn": "2021-07-09T01:58:26.946Z",
          "wordCount": 594,
          "title": "In-Network Learning: Distributed Training and Inference in Networks. (arXiv:2107.03433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03770",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mehrjou_A/0/1/0/all/0/1\">Arash Mehrjou</a>",
          "description": "We establish a connection between federated learning, a concept from machine\nlearning, and mean-field games, a concept from game theory and control theory.\nIn this analogy, the local federated learners are considered as the players and\nthe aggregation of the gradients in a central server is the mean-field effect.\nWe present federated learning as a differential game and discuss the properties\nof the equilibrium of this game. We hope this novel view to federated learning\nbrings together researchers from these two distinct areas to work on\nfundamental problems of large-scale distributed and privacy-preserving learning\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.03770",
          "publishedOn": "2021-07-09T01:58:26.939Z",
          "wordCount": 531,
          "title": "Federated Learning as a Mean-Field Game. (arXiv:2107.03770v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ravindranath_S/0/1/0/all/0/1\">Sai Srivatsa Ravindranath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shira Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jonathan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kominers_S/0/1/0/all/0/1\">Scott D. Kominers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1\">David C. Parkes</a>",
          "description": "We initiate the use of a multi-layer neural network to model two-sided\nmatching and to explore the design space between strategy-proofness and\nstability. It is well known that both properties cannot be achieved\nsimultaneously but the efficient frontier in this design space is not\nunderstood. We show empirically that it is possible to achieve a good\ncompromise between stability and strategy-proofness-substantially better than\nthat achievable through a convex combination of deferred acceptance (stable and\nstrategy-proof for only one side of the market) and randomized serial\ndictatorship (strategy-proof but not stable).",
          "link": "http://arxiv.org/abs/2107.03427",
          "publishedOn": "2021-07-09T01:58:26.924Z",
          "wordCount": 536,
          "title": "Deep Learning for Two-Sided Matching. (arXiv:2107.03427v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaghani_J/0/1/0/all/0/1\">Javad Khaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanei_Yakhdan_H/0/1/0/all/0/1\">Hossein Ghanei-Yakhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>",
          "description": "A strong visual object tracker nowadays relies on its well-crafted modules,\nwhich typically consist of manually-designed network architectures to deliver\nhigh-quality tracking results. Not surprisingly, the manual design process\nbecomes a particularly challenging barrier, as it demands sufficient prior\nexperience, enormous effort, intuition and perhaps some good luck. Meanwhile,\nneural architecture search has gaining grounds in practical applications such\nas image segmentation, as a promising method in tackling the issue of automated\nsearch of feasible network structures. In this work, we propose a novel\ncell-level differentiable architecture search mechanism to automate the network\ndesign of the tracking module, aiming to adapt backbone features to the\nobjective of a tracking network during offline training. The proposed approach\nis simple, efficient, and with no need to stack a series of modules to\nconstruct a network. Our approach is easy to be incorporated into existing\ntrackers, which is empirically validated using different differentiable\narchitecture search-based methods and tracking objectives. Extensive\nexperimental evaluations demonstrate the superior performance of our approach\nover five commonly-used benchmarks. Meanwhile, our automated searching process\ntakes 41 (18) hours for the second (first) order DARTS method on the\nTrackingNet dataset.",
          "link": "http://arxiv.org/abs/2107.03463",
          "publishedOn": "2021-07-09T01:58:26.905Z",
          "wordCount": 652,
          "title": "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search. (arXiv:2107.03463v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03651",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bar_David_D/0/1/0/all/0/1\">Daniel Bar-David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bar_David_L/0/1/0/all/0/1\">Laura Bar-David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shapira_Y/0/1/0/all/0/1\">Yinon Shapira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leibu_R/0/1/0/all/0/1\">Rina Leibu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dori_D/0/1/0/all/0/1\">Dalia Dori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schneor_R/0/1/0/all/0/1\">Ronit Schneor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischer_A/0/1/0/all/0/1\">Anath Fischer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soudry_S/0/1/0/all/0/1\">Shiri Soudry</a>",
          "description": "To explore the clinical validity of elastic deformation of optical coherence\ntomography (OCT) images for data augmentation in the development of\ndeep-learning model for detection of diabetic macular edema (DME).",
          "link": "http://arxiv.org/abs/2107.03651",
          "publishedOn": "2021-07-09T01:58:26.899Z",
          "wordCount": 508,
          "title": "Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?. (arXiv:2107.03651v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+El_Awady_K/0/1/0/all/0/1\">Khalid El-Awady</a>",
          "description": "We demonstrate the use of Adaptive Stress Testing to detect and address\npotential vulnerabilities in a financial environment. We develop a simplified\nmodel for credit card fraud detection that utilizes a linear regression\nclassifier based on historical payment transaction data coupled with business\nrules. We then apply the reinforcement learning model known as Adaptive Stress\nTesting to train an agent, that can be thought of as a potential fraudster, to\nfind the most likely path to system failure -- successfully defrauding the\nsystem. We show the connection between this most likely failure path and the\nlimits of the classifier and discuss how the fraud detection system's business\nrules can be further augmented to mitigate these failure modes.",
          "link": "http://arxiv.org/abs/2107.03577",
          "publishedOn": "2021-07-09T01:58:26.891Z",
          "wordCount": 553,
          "title": "Adaptive Stress Testing for Adversarial Learning in a Financial Environment. (arXiv:2107.03577v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03387",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Cvetko_T/0/1/0/all/0/1\">Tim Cvetko</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robek_T/0/1/0/all/0/1\">Tinkara Robek</a>",
          "description": "In this paper, we propose a novel method and a practical approach to\npredicting early onsets of sleep syndromes, including restless leg syndrome,\ninsomnia, based on an algorithm that is comprised of two modules. A Fast\nFourier Transform is applied to 30 seconds long epochs of EEG recordings to\nprovide localized time-frequency information, and a deep convolutional LSTM\nneural network is trained for sleep stage classification. Automating sleep\nstages detection from EEG data offers great potential to tackling sleep\nirregularities on a daily basis. Thereby, a novel approach for sleep stage\nclassification is proposed which combines the best of signal processing and\nstatistics. In this study, we used the PhysioNet Sleep European Data Format\n(EDF) Database. The code evaluation showed impressive results, reaching an\naccuracy of 86.43, precision of 77.76, recall of 93,32, F1-score of 89.12 with\nthe final mean false error loss of 0.09.",
          "link": "http://arxiv.org/abs/2107.03387",
          "publishedOn": "2021-07-09T01:58:26.885Z",
          "wordCount": 591,
          "title": "Sleep syndromes onset detection based on automatic sleep staging algorithm. (arXiv:2107.03387v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1\">Martin Knoche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1\">Stefan H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>",
          "description": "Face recognition approaches often rely on equal image resolution for\nverification faces on two images. However, in practical applications, those\nimage resolutions are usually not in the same range due to different image\ncapture mechanisms or sources. In this work, we first analyze the impact of\nimage resolutions on the face verification performance with a state-of-the-art\nface recognition model. For images, synthetically reduced to $5\\, \\times 5\\,\n\\mathrm{px}$ resolution, the verification performance drops from $99.23\\%$\nincreasingly down to almost $55\\%$. Especially, for cross-resolution image\npairs (one high- and one low-resolution image), the verification accuracy\ndecreases even further. We investigate this behavior more in-depth by looking\nat the feature distances for every 2-image test pair. To tackle this problem,\nwe propose the following two methods: 1) Train a state-of-the-art\nface-recognition model straightforward with $50\\%$ low-resolution images\ndirectly within each batch. \\\\ 2) Train a siamese-network structure and adding\na cosine distance feature loss between high- and low-resolution features. Both\nmethods show an improvement for cross-resolution scenarios and can increase the\naccuracy at very low resolution to approximately $70\\%$. However, a\ndisadvantage is that a specific model needs to be trained for every\nresolution-pair ...",
          "link": "http://arxiv.org/abs/2107.03769",
          "publishedOn": "2021-07-09T01:58:26.861Z",
          "wordCount": 636,
          "title": "Image Resolution Susceptibility of Face Recognition Models. (arXiv:2107.03769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvir Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-09T01:58:26.838Z",
          "wordCount": 691,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03645",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Heindel_L/0/1/0/all/0/1\">Leonhard Heindel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hantschke_P/0/1/0/all/0/1\">Peter Hantschke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kastner_M/0/1/0/all/0/1\">Markus K&#xe4;stner</a>",
          "description": "Modern Internet of Things solutions are used in a variety of different areas,\nranging from connected vehicles and healthcare to industrial applications. They\nrely on a large amount of interconnected sensors, which can lead to both\ntechnical and economical challenges. Virtual sensing techniques aim to reduce\nthe number of physical sensors in a system by using data from available\nmeasurements to estimate additional unknown quantities of interest. Successful\nmodel-based solutions include Kalman filters or the combination of finite\nelement models and modal analysis, while many data-driven methods rely on\nmachine learning algorithms. The presented hybrid virtual sensing approach\ncombines Long Short-Term Memory networks with frequency response function\nmodels in order to estimate the behavior of non-linear dynamic systems with\nmultiple input and output channels. Network training and prediction make use of\nshort signal subsequences, which are later recombined by applying a windowing\ntechnique. The frequency response function model acts as a baseline estimate\nwhich perfectly captures linear dynamic systems and is augmented by the\nnon-linear Long Short-Term Memory network following two different hybrid\nmodeling strategies. The approach is tested using a non-linear experimental\ndataset, which results from measurements of a three-component servo-hydraulic\nfatigue test bench. A variety of metrics in time and frequency domains, as well\nas fatigue strength under variable amplitudes are used to evaluate the\napproximation quality of the proposed method. In addition to virtual sensing,\nthe algorithm is also applied to a forward prediction task. Synthetic data are\nused in a separate study to estimate the prediction quality on datasets of\ndifferent size.",
          "link": "http://arxiv.org/abs/2107.03645",
          "publishedOn": "2021-07-09T01:58:26.831Z",
          "wordCount": 708,
          "title": "A hybrid virtual sensing approach for approximating non-linear dynamic system behavior using LSTM networks. (arXiv:2107.03645v1 [eess.SP])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}