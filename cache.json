{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:07.674Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:07.655Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>",
          "description": "Large pre-trained neural language models (LM) have very powerful text\ngeneration capabilities. However, in practice, they are hard to control for\ncreative purposes. We describe a Plug-and-Play controllable language generation\nframework, Plug-and-Blend, that allows a human user to input multiple control\ncodes (topics). In the context of automated story generation, this allows a\nhuman user loose or fine-grained control of the topics and transitions between\nthem that will appear in the generated story, and can even allow for\noverlapping, blended topics. Automated evaluations show our framework, working\nwith different generative LMs, controls the generation towards given\ncontinuous-weighted control codes while keeping the generated sentences fluent,\ndemonstrating strong blending capability. A human participant evaluation shows\nthat the generated stories are observably transitioning between two topics.",
          "link": "http://arxiv.org/abs/2104.04039",
          "publishedOn": "2021-07-29T02:00:07.636Z",
          "wordCount": 601,
          "title": "Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes. (arXiv:2104.04039v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper argues that training GANs on local and non-local dependencies in\nspeech data offers insights into how deep neural networks discretize continuous\ndata and how symbolic-like rule-based morphophonological processes emerge in a\ndeep convolutional architecture. Acquisition of speech has recently been\nmodeled as a dependency between latent space and data generated by GANs in\nBegu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local\nallophonic distribution. We extend this approach to test learning of local and\nnon-local phonological processes that include approximations of morphological\nprocesses. We further parallel outputs of the model to results of a behavioral\nexperiment where human subjects are trained on the data used for training the\nGAN network. Four main conclusions emerge: (i) the networks provide useful\ninformation for computational models of speech acquisition even if trained on a\ncomparatively small dataset of an artificial grammar learning experiment; (ii)\nlocal processes are easier to learn than non-local processes, which matches\nboth behavioral data in human subjects and typology in the world's languages.\nThis paper also proposes (iii) how we can actively observe the network's\nprogress in learning and explore the effect of training steps on learning\nrepresentations by keeping latent space constant across different training\nsteps. Finally, this paper shows that (iv) the network learns to encode the\npresence of a prefix with a single latent variable; by interpolating this\nvariable, we can actively observe the operation of a non-local phonological\nprocess. The proposed technique for retrieving learning representations has\ngeneral implications for our understanding of how GANs discretize continuous\nspeech data and suggests that rule-like generalizations in the training data\nare represented as an interaction between variables in the network's latent\nspace.",
          "link": "http://arxiv.org/abs/2009.12711",
          "publishedOn": "2021-07-29T02:00:07.628Z",
          "wordCount": 761,
          "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pre-trained vision models such as VGG16 and ResNet\n\nIndex Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-07-29T02:00:07.616Z",
          "wordCount": 706,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1\">Wilbert Heeringa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_M/0/1/0/all/0/1\">Martha Hofman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drenth_E/0/1/0/all/0/1\">Eduard Drenth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijffels_J/0/1/0/all/0/1\">Jan Wijffels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velde_H/0/1/0/all/0/1\">Hans Van de Velde</a>",
          "description": "We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a\ncorpus of 44,714 words in 3,126 sentences that were annotated according to the\nguidelines of Universal Dependency version 2. POS tags were assigned to words\nby using a Dutch POS tagger that was applied to a literal word-by-word\ntranslation, or to sentences of a Dutch parallel text. Best results were\nobtained when using literal translations that were created by using the Frisian\ntranslation program Oersetter. Morphologic and syntactic annotations were\ngenerated on the basis of a literal Dutch translation as well. The performance\nof the lemmatizer/tagger/annotator when it was trained using default parameters\nwas compared to the performance that was obtained when using the parameter\nvalues that were used for training the LassySmall UD 2.5 corpus. A significant\nimprovement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency\nparser is released as a web app and as a web service.",
          "link": "http://arxiv.org/abs/2107.07974",
          "publishedOn": "2021-07-29T02:00:07.608Z",
          "wordCount": 623,
          "title": "POS tagging, lemmatization and dependency parsing of West Frisian. (arXiv:2107.07974v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Keunwoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>",
          "description": "We propose a multimodal singing language classification model that uses both\naudio content and textual metadata. LRID-Net, the proposed model, takes an\naudio signal and a language probability vector estimated from the metadata and\noutputs the probabilities of the target languages. Optionally, LRID-Net is\nfacilitated with modality dropouts to handle a missing modality. In the\nexperiment, we trained several LRID-Nets with varying modality dropout\nconfiguration and tested them with various combinations of input modalities.\nThe experiment results demonstrate that using multimodal input improves\nperformance. The results also suggest that adopting modality dropout does not\ndegrade the performance of the model when there are full modality inputs while\nenabling the model to handle missing modality cases to some extent.",
          "link": "http://arxiv.org/abs/2103.01893",
          "publishedOn": "2021-07-29T02:00:07.589Z",
          "wordCount": 607,
          "title": "Listen, Read, and Identify: Multimodal Singing Language Identification of Music. (arXiv:2103.01893v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelgwad_M/0/1/0/all/0/1\">Mohammed M.Abdelgwad</a>",
          "description": "Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic ABSA tasks. In particular, we are building a\nsimple but effective BERT-based neural baseline to handle this task. Our BERT\narchitecture with a simple linear classification layer surpassed the\nstate-of-the-art works, according to the experimental results on the\nbenchmarked Arabic hotel reviews dataset.",
          "link": "http://arxiv.org/abs/2107.13290",
          "publishedOn": "2021-07-29T02:00:07.582Z",
          "wordCount": 561,
          "title": "Arabic aspect based sentiment analysis using BERT. (arXiv:2107.13290v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chawla_K/0/1/0/all/0/1\">Kushal Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clever_R/0/1/0/all/0/1\">Rene Clever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_J/0/1/0/all/0/1\">Jaysa Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_G/0/1/0/all/0/1\">Gale Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>",
          "description": "Negotiation is a complex social interaction that encapsulates emotional\nencounters in human decision-making. Virtual agents that can negotiate with\nhumans are useful in pedagogy and conversational AI. To advance the development\nof such agents, we explore the prediction of two important subjective goals in\na negotiation - outcome satisfaction and partner perception. Specifically, we\nanalyze the extent to which emotion attributes extracted from the negotiation\nhelp in the prediction, above and beyond the individual difference variables.\nWe focus on a recent dataset in chat-based negotiations, grounded in a\nrealistic camping scenario. We study three degrees of emotion dimensions -\nemoticons, lexical, and contextual by leveraging affective lexicons and a\nstate-of-the-art deep learning architecture. Our insights will be helpful in\ndesigning adaptive negotiation agents that interact through realistic\ncommunication interfaces.",
          "link": "http://arxiv.org/abs/2107.13165",
          "publishedOn": "2021-07-29T02:00:07.574Z",
          "wordCount": 579,
          "title": "Towards Emotion-Aware Agents For Negotiation Dialogues. (arXiv:2107.13165v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:07.459Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>",
          "description": "Robustness against word substitutions has a well-defined and widely\nacceptable form, i.e., using semantically similar words as substitutions, and\nthus it is considered as a fundamental stepping-stone towards broader\nrobustness in natural language processing. Previous defense methods capture\nword substitutions in vector space by using either $l_2$-ball or\nhyper-rectangle, which results in perturbation sets that are not inclusive\nenough or unnecessarily large, and thus impedes mimicry of worst cases for\nrobust training. In this paper, we introduce a novel \\textit{Adversarial Sparse\nConvex Combination} (ASCC) method. We model the word substitution attack space\nas a convex hull and leverages a regularization term to enforce perturbation\ntowards an actual substitution, thus aligning our modeling better with the\ndiscrete textual space. Based on the ASCC method, we further propose\nASCC-defense, which leverages ASCC to generate worst-case perturbations and\nincorporates adversarial training towards robustness. Experiments show that\nASCC-defense outperforms the current state-of-the-arts in terms of robustness\non two prevailing NLP tasks, \\emph{i.e.}, sentiment analysis and natural\nlanguage inference, concerning several attacks across multiple model\narchitectures. Besides, we also envision a new class of defense towards\nrobustness in NLP, where our robustly trained word vectors can be plugged into\na normally trained model and enforce its robustness without applying any other\ndefense techniques.",
          "link": "http://arxiv.org/abs/2107.13541",
          "publishedOn": "2021-07-29T02:00:07.429Z",
          "wordCount": 644,
          "title": "Towards Robustness Against Natural Language Word Substitutions. (arXiv:2107.13541v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>",
          "description": "Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zjunlp/LegalPP} for reproducibility.",
          "link": "http://arxiv.org/abs/2104.02284",
          "publishedOn": "2021-07-29T02:00:07.417Z",
          "wordCount": 591,
          "title": "Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13530",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1\">Bethan Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>",
          "description": "We present a method for continual learning of speech representations for\nmultiple languages using self-supervised learning (SSL) and applying these for\nautomatic speech recognition. There is an abundance of unannotated speech, so\ncreating self-supervised representations from raw audio and finetuning on a\nsmall annotated datasets is a promising direction to build speech recognition\nsystems. Wav2vec models perform SSL on raw audio in a pretraining phase and\nthen finetune on a small fraction of annotated data. SSL models have produced\nstate of the art results for ASR. However, these models are very expensive to\npretrain with self-supervision. We tackle the problem of learning new language\nrepresentations continually from audio without forgetting a previous language\nrepresentation. We use ideas from continual learning to transfer knowledge from\na previous task to speed up pretraining a new language task. Our\ncontinual-wav2vec2 model can decrease pretraining times by 32% when learning a\nnew language task, and learn this new audio-language representation without\nforgetting previous language representation.",
          "link": "http://arxiv.org/abs/2107.13530",
          "publishedOn": "2021-07-29T02:00:07.406Z",
          "wordCount": 634,
          "title": "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Stephen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1\">Mark Dras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>",
          "description": "Sequence-to-Sequence (S2S) neural text generation models, especially the\npre-trained ones (e.g., BART and T5), have exhibited compelling performance on\nvarious natural language generation tasks. However, the black-box nature of\nthese models limits their application in tasks where specific rules (e.g.,\ncontrollable constraints, prior knowledge) need to be executed. Previous works\neither design specific model structure (e.g., Copy Mechanism corresponding to\nthe rule \"the generated output should include certain words in the source\ninput\") or implement specialized inference algorithm (e.g., Constrained Beam\nSearch) to execute particular rules through the text generation. These methods\nrequire careful design case-by-case and are difficult to support multiple rules\nconcurrently. In this paper, we propose a novel module named Neural\nRule-Execution Tracking Machine that can be equipped into various\ntransformer-based generators to leverage multiple rules simultaneously to guide\nthe neural generation model for superior generation performance in a unified\nand scalable way. Extensive experimental results on several benchmarks verify\nthe effectiveness of our proposed model in both controllable and general text\ngeneration.",
          "link": "http://arxiv.org/abs/2107.13077",
          "publishedOn": "2021-07-29T02:00:07.398Z",
          "wordCount": 605,
          "title": "Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation. (arXiv:2107.13077v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsividis_P/0/1/0/all/0/1\">Pedro A. Tsividis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madeano_J/0/1/0/all/0/1\">Jason Madeano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harper_B/0/1/0/all/0/1\">Brin Harper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>",
          "description": "Knowledge built culturally across generations allows humans to learn far more\nthan an individual could glean from their own experience in a lifetime.\nCultural knowledge in turn rests on language: language is the richest record of\nwhat previous generations believed, valued, and practiced. The power and\nmechanisms of language as a means of cultural learning, however, are not well\nunderstood. We take a first step towards reverse-engineering cultural learning\nthrough language. We developed a suite of complex high-stakes tasks in the form\nof minimalist-style video games, which we deployed in an iterated learning\nparadigm. Game participants were limited to only two attempts (two lives) to\nbeat each game and were allowed to write a message to a future participant who\nread the message before playing. Knowledge accumulated gradually across\ngenerations, allowing later generations to advance further in the games and\nperform more efficient actions. Multigenerational learning followed a\nstrikingly similar trajectory to individuals learning alone with an unlimited\nnumber of lives. These results suggest that language provides a sufficient\nmedium to express and accumulate the knowledge people acquire in these diverse\ntasks: the dynamics of the environment, valuable goals, dangerous risks, and\nstrategies for success. The video game paradigm we pioneer here is thus a rich\ntest bed for theories of cultural transmission and learning from language.",
          "link": "http://arxiv.org/abs/2107.13377",
          "publishedOn": "2021-07-29T02:00:07.388Z",
          "wordCount": 675,
          "title": "Growing knowledge culturally across generations to solve novel, complex tasks. (arXiv:2107.13377v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-07-29T02:00:07.368Z",
          "wordCount": 725,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>",
          "description": "In task-oriented conversation systems, natural language generation systems\nthat generate sentences with specific information related to conversation flow\nare useful. Our study focuses on language generation by considering various\ninformation representing the meaning of utterances as multiple conditions of\ngeneration. NLG from meaning representations, the conditions for sentence\nmeaning, generally goes through two steps: sentence planning and surface\nrealization. However, we propose a simple one-stage framework to generate\nutterances directly from MR (Meaning Representation). Our model is based on\nGPT2 and generates utterances with flat conditions on slot and value pairs,\nwhich does not need to determine the structure of the sentence. We evaluate\nseveral systems in the E2E dataset with 6 automatic metrics. Our system is a\nsimple method, but it demonstrates comparable performance to previous systems\nin automated metrics. In addition, using only 10\\% of the data set without any\nother techniques, our model achieves comparable performance, and shows the\npossibility of performing zero-shot generation and expanding to other datasets.",
          "link": "http://arxiv.org/abs/2101.04257",
          "publishedOn": "2021-07-29T02:00:07.354Z",
          "wordCount": 616,
          "title": "Transforming Multi-Conditioned Generation from Meaning Representation. (arXiv:2101.04257v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.",
          "link": "http://arxiv.org/abs/2107.13189",
          "publishedOn": "2021-07-29T02:00:07.330Z",
          "wordCount": 562,
          "title": "Goal-Oriented Script Construction. (arXiv:2107.13189v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>",
          "description": "Existing methods in relation extraction have leveraged the lexical features\nin the word sequence and the syntactic features in the parse tree. Though\neffective, the lexical features extracted from the successive word sequence may\nintroduce some noise that has little or no meaningful content. Meanwhile, the\nsyntactic features are usually encoded via graph convolutional networks which\nhave restricted receptive field. To address the above limitations, we propose a\nmulti-scale feature and metric learning framework for relation extraction.\nSpecifically, we first develop a multi-scale convolutional neural network to\naggregate the non-successive mainstays in the lexical sequence. We also design\na multi-scale graph convolutional network which can increase the receptive\nfield towards specific syntactic roles. Moreover, we present a multi-scale\nmetric learning paradigm to exploit both the feature-level relation between\nlexical and syntactic features and the sample-level relation between instances\nwith the same or different classes. We conduct extensive experiments on three\nreal world datasets for various types of relation extraction tasks. The results\ndemonstrate that our model significantly outperforms the state-of-the-art\napproaches.",
          "link": "http://arxiv.org/abs/2107.13425",
          "publishedOn": "2021-07-29T02:00:07.322Z",
          "wordCount": 600,
          "title": "Multi-Scale Feature and Metric Learning for Relation Extraction. (arXiv:2107.13425v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1\">Vivek Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1\">Sam Witteveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1\">Martin Andrews</a>",
          "description": "Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021",
          "link": "http://arxiv.org/abs/2107.13031",
          "publishedOn": "2021-07-29T02:00:07.305Z",
          "wordCount": 570,
          "title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "How can deep neural networks encode information that corresponds to words in\nhuman speech into raw acoustic data? This paper proposes two neural network\narchitectures for modeling unsupervised lexical learning from raw acoustic\ninputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),\nthat combine a Deep Convolutional GAN architecture for audio data (WaveGAN;\narXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN\n(arXiv:1606.03657), and propose a new latent space structure that can model\nfeatural learning simultaneously with a higher level classification and allows\nfor a very low-dimension vector representation of lexical items. Lexical\nlearning is modeled as emergent from an architecture that forces a deep neural\nnetwork to output data such that unique information is retrievable from its\nacoustic outputs. The networks trained on lexical items from TIMIT learn to\nencode unique information corresponding to lexical items in the form of\ncategorical variables in their latent space. By manipulating these variables,\nthe network outputs specific lexical items. The network occasionally outputs\ninnovative lexical items that violate training data, but are linguistically\ninterpretable and highly informative for cognitive modeling and neural network\ninterpretability. Innovative outputs suggest that phonetic and phonological\nrepresentations learned by the network can be productively recombined and\ndirectly paralleled to productivity in human speech: a fiwGAN network trained\non `suit' and `dark' outputs innovative `start', even though it never saw\n`start' or even a [st] sequence in the training data. We also argue that\nsetting latent featural codes to values well beyond training range results in\nalmost categorical generation of prototypical lexical items and reveals\nunderlying values of each latent code.",
          "link": "http://arxiv.org/abs/2006.02951",
          "publishedOn": "2021-07-29T02:00:07.279Z",
          "wordCount": 756,
          "title": "CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arrabales_R/0/1/0/all/0/1\">Ra&#xfa;l Arrabales</a>",
          "description": "Most depression assessment tools are based on self-report questionnaires,\nsuch as the Patient Health Questionnaire (PHQ-9). These psychometric\ninstruments can be easily adapted to an online setting by means of electronic\nforms. However, this approach lacks the interacting and engaging features of\nmodern digital environments. With the aim of making depression screening more\navailable, attractive and effective, we developed Perla, a conversational agent\nable to perform an interview based on the PHQ-9. We also conducted a validation\nstudy in which we compared the results obtained by the traditional self-report\nquestionnaire with Perla's automated interview. Analyzing the results from this\nstudy we draw two significant conclusions: firstly, Perla is much preferred by\nInternet users, achieving more than 2.5 times more reach than a traditional\nform-based questionnaire; secondly, her psychometric properties (Cronbach's\nalpha of 0.81, sensitivity of 96% and specificity of 90%) are excellent and\ncomparable to the traditional well-established depression screening\nquestionnaires.",
          "link": "http://arxiv.org/abs/2008.12875",
          "publishedOn": "2021-07-29T02:00:07.256Z",
          "wordCount": 634,
          "title": "Perla: A Conversational Agent for Depression Screening in Digital Ecosystems. Design, Implementation and Validation. (arXiv:2008.12875v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rocholl_J/0/1/0/all/0/1\">Johann C. Rocholl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zayats_V/0/1/0/all/0/1\">Vicky Zayats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_D/0/1/0/all/0/1\">Daniel D. Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murad_N/0/1/0/all/0/1\">Noah B. Murad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_A/0/1/0/all/0/1\">Aaron Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebling_D/0/1/0/all/0/1\">Daniel J. Liebling</a>",
          "description": "Disfluency detection models now approach high accuracy on English text.\nHowever, little exploration has been done in improving the size and inference\ntime of the model. At the same time, automatic speech recognition (ASR) models\nare moving from server-side inference to local, on-device inference. Supporting\nmodels in the transcription pipeline (like disfluency detection) must follow\nsuit. In this work we concentrate on the disfluency detection task, focusing on\nsmall, fast, on-device models based on the BERT architecture. We demonstrate it\nis possible to train disfluency detection models as small as 1.3 MiB, while\nretaining high performance. We build on previous work that showed the benefit\nof data augmentation approaches such as self-training. Then, we evaluate the\neffect of domain mismatch between conversational and written text on model\nperformance. We find that domain adaptation and data augmentation strategies\nhave a more pronounced effect on these smaller models, as compared to\nconventional BERT models.",
          "link": "http://arxiv.org/abs/2104.10769",
          "publishedOn": "2021-07-28T02:02:34.291Z",
          "wordCount": 624,
          "title": "Disfluency Detection with Unlabeled Data and Small BERT Models. (arXiv:2104.10769v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Rotem Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>",
          "description": "The quality of a summarization evaluation metric is quantified by calculating\nthe correlation between its scores and human annotations across a large number\nof summaries. Currently, it is unclear how precise these correlation estimates\nare, nor whether differences between two metrics' correlations reflect a true\ndifference or if it is due to mere chance. In this work, we address these two\nproblems by proposing methods for calculating confidence intervals and running\nhypothesis tests for correlations using two resampling methods, bootstrapping\nand permutation. After evaluating which of the proposed methods is most\nappropriate for summarization through two simulation experiments, we analyze\nthe results of applying these methods to several different automatic evaluation\nmetrics across three sets of human annotations. We find that the confidence\nintervals are rather wide, demonstrating high uncertainty in the reliability of\nautomatic metrics. Further, although many metrics fail to show statistical\nimprovements over ROUGE, two recent works, QAEval and BERTScore, do in some\nevaluation settings.",
          "link": "http://arxiv.org/abs/2104.00054",
          "publishedOn": "2021-07-28T02:02:33.975Z",
          "wordCount": 630,
          "title": "A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods. (arXiv:2104.00054v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-07-28T02:02:30.851Z",
          "wordCount": 654,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>",
          "description": "NLP is deeply intertwined with the formal study of language, both\nconceptually and historically. Arguably, this connection goes all the way back\nto Chomsky's Syntactic Structures in 1957. It also still holds true today, with\na strand of recent works building formal analysis of modern neural networks\nmethods in terms of formal languages. In this document, I aim to explain\nbackground about formal languages as they relate to this recent work. I will by\nnecessity ignore large parts of the rich history of this field, instead\nfocusing on concepts connecting to modern deep learning-based NLP.",
          "link": "http://arxiv.org/abs/2102.10094",
          "publishedOn": "2021-07-28T02:02:30.830Z",
          "wordCount": 561,
          "title": "Formal Language Theory Meets Modern NLP. (arXiv:2102.10094v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarchi_Z/0/1/0/all/0/1\">Zahra Shekarchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1\">Suzanne Stevenson</a>",
          "description": "Children learn word meanings by tapping into the commonalities across\ndifferent situations in which words are used and overcome the high level of\nuncertainty involved in early word learning experiences. We propose a modeling\nframework to investigate the role of mutual exclusivity bias - asserting\none-to-one mappings between words and their meanings - in reducing uncertainty\nin word learning. In a set of computational studies, we show that to\nsuccessfully learn word meanings in the face of uncertainty, a learner needs to\nuse two types of competition: words competing for association to a referent\nwhen learning from an observation and referents competing for a word when the\nword is used. Our work highlights the importance of an algorithmic-level\nanalysis to shed light on the utility of different mechanisms that can\nimplement the same computational-level theory.",
          "link": "http://arxiv.org/abs/2012.03370",
          "publishedOn": "2021-07-28T02:02:30.787Z",
          "wordCount": 619,
          "title": "Competition in Cross-situational Word Learning: A Computational Study. (arXiv:2012.03370v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedrax_Weiss_T/0/1/0/all/0/1\">Tania Bedrax-Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>",
          "description": "A desirable property of a reference-based evaluation metric that measures the\ncontent quality of a summary is that it should estimate how much information\nthat summary has in common with a reference. Traditional text overlap based\nmetrics such as ROUGE fail to achieve this because they are limited to matching\ntokens, either lexically or via embeddings. In this work, we propose a metric\nto evaluate the content quality of a summary using question-answering (QA).\nQA-based methods directly measure a summary's information overlap with a\nreference, making them fundamentally different than text overlap metrics. We\ndemonstrate the experimental benefits of QA-based metrics through an analysis\nof our proposed metric, QAEval. QAEval out-performs current state-of-the-art\nmetrics on most evaluations using benchmark datasets, while being competitive\non others due to limitations of state-of-the-art models. Through a careful\nanalysis of each component of QAEval, we identify its performance bottlenecks\nand estimate that its potential upper-bound performance surpasses all other\nautomatic metrics, approaching that of the gold-standard Pyramid Method.",
          "link": "http://arxiv.org/abs/2010.00490",
          "publishedOn": "2021-07-28T02:02:30.780Z",
          "wordCount": 653,
          "title": "Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary. (arXiv:2010.00490v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>",
          "description": "In this paper we apply self-knowledge distillation to text summarization\nwhich we argue can alleviate problems with maximum-likelihood training on\nsingle reference and noisy datasets. Instead of relying on one-hot annotation\nlabels, our student summarization model is trained with guidance from a teacher\nwhich generates smoothed labels to help regularize training. Furthermore, to\nbetter model uncertainty during training, we introduce multiple noise signals\nfor both teacher and student models. We demonstrate experimentally on three\nbenchmarks that our framework boosts the performance of both pretrained and\nnon-pretrained summarizers achieving state-of-the-art results.",
          "link": "http://arxiv.org/abs/2009.07032",
          "publishedOn": "2021-07-28T02:02:30.742Z",
          "wordCount": 548,
          "title": "Noisy Self-Knowledge Distillation for Text Summarization. (arXiv:2009.07032v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang%7D_%7B/0/1/0/all/0/1\">{Bao Minh} {Doan Dang}</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>",
          "description": "Emotion stimulus extraction is a fine-grained subtask of emotion analysis\nthat focuses on identifying the description of the cause behind an emotion\nexpression from a text passage (e.g., in the sentence \"I am happy that I passed\nmy exam\" the phrase \"passed my exam\" corresponds to the stimulus.). Previous\nwork mainly focused on Mandarin and English, with no resources or models for\nGerman. We fill this research gap by developing a corpus of 2006 German news\nheadlines annotated with emotions and 811 instances with annotations of\nstimulus phrases. Given that such corpus creation efforts are time-consuming\nand expensive, we additionally work on an approach for projecting the existing\nEnglish GoodNewsEveryone (GNE) corpus to a machine-translated German version.\nWe compare the performance of a conditional random field (CRF) model (trained\nmonolingually on German and cross-lingually via projection) with a multilingual\nXLM-RoBERTa (XLM-R) model. Our results show that training with the German\ncorpus achieves higher F1 scores than projection. Experiments with XLM-R\noutperform their respective CRF counterparts.",
          "link": "http://arxiv.org/abs/2107.12920",
          "publishedOn": "2021-07-28T02:02:30.731Z",
          "wordCount": 599,
          "title": "Emotion Stimulus Detection in German News Headlines. (arXiv:2107.12920v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Anne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talnikar_C/0/1/0/all/0/1\">Chaitanya Talnikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haziza_D/0/1/0/all/0/1\">Daniel Haziza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_M/0/1/0/all/0/1\">Mary Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>",
          "description": "We introduce VoxPopuli, a large-scale multilingual corpus providing 100K\nhours of unlabelled speech data in 23 languages. It is the largest open data to\ndate for unsupervised representation learning as well as semi-supervised\nlearning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16\nlanguages and their aligned oral interpretations into 5 other languages\ntotaling 5.1K hours. We provide speech recognition baselines and validate the\nversatility of VoxPopuli unlabelled data in semi-supervised learning under\nchallenging out-of-domain settings. We will release the corpus at\nhttps://github.com/facebookresearch/voxpopuli under an open license.",
          "link": "http://arxiv.org/abs/2101.00390",
          "publishedOn": "2021-07-28T02:02:30.716Z",
          "wordCount": 587,
          "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. (arXiv:2101.00390v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:30.707Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.07473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silveira_B/0/1/0/all/0/1\">B&#xe1;rbara Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Henrique S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Ana Paula Couto da Silva</a>",
          "description": "In recent years, Online Social Networks have become an important medium for\npeople who suffer from mental disorders to share moments of hardship, and\nreceive emotional and informational support. In this work, we analyze how\ndiscussions in Reddit communities related to mental disorders can help improve\nthe health conditions of their users. Using the emotional tone of users'\nwriting as a proxy for emotional state, we uncover relationships between user\ninteractions and state changes. First, we observe that authors of negative\nposts often write rosier comments after engaging in discussions, indicating\nthat users' emotional state can improve due to social support. Second, we build\nmodels based on SOTA text embedding techniques and RNNs to predict shifts in\nemotional tone. This differs from most of related work, which focuses primarily\non detecting mental disorders from user activity. We demonstrate the\nfeasibility of accurately predicting the users' reactions to the interactions\nexperienced in these platforms, and present some examples which illustrate that\nthe models are correctly capturing the effects of comments on the author's\nemotional tone. Our models hold promising implications for interventions to\nprovide support for people struggling with mental illnesses.",
          "link": "http://arxiv.org/abs/2005.07473",
          "publishedOn": "2021-07-28T02:02:30.649Z",
          "wordCount": 698,
          "title": "Predicting User Emotional Tone in Mental Disorder Online Communities. (arXiv:2005.07473v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barry_J/0/1/0/all/0/1\">James Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Joachim Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassidy_L/0/1/0/all/0/1\">Lauren Cassidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowap_A/0/1/0/all/0/1\">Alan Cowap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynn_T/0/1/0/all/0/1\">Teresa Lynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_A/0/1/0/all/0/1\">Abigail Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meachair_M/0/1/0/all/0/1\">M&#xed;che&#xe1;l J. &#xd3; Meachair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>",
          "description": "The BERT family of neural language models have become highly popular due to\ntheir ability to provide sequences of text with rich context-sensitive token\nencodings which are able to generalise well to many Natural Language Processing\ntasks. Over 120 monolingual BERT models covering over 50 languages have been\nreleased, as well as a multilingual model trained on 104 languages. We\nintroduce, gaBERT, a monolingual BERT model for the Irish language. We compare\nour gaBERT model to multilingual BERT and show that gaBERT provides better\nrepresentations for a downstream parsing task. We also show how different\nfiltering criteria, vocabulary size and the choice of subword tokenisation\nmodel affect downstream performance. We release gaBERT and related code to the\ncommunity.",
          "link": "http://arxiv.org/abs/2107.12930",
          "publishedOn": "2021-07-28T02:02:30.630Z",
          "wordCount": 555,
          "title": "gaBERT -- an Irish Language Model. (arXiv:2107.12930v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casel_F/0/1/0/all/0/1\">Felix Casel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindl_A/0/1/0/all/0/1\">Amelie Heindl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>",
          "description": "Emotion classification in text is typically performed with neural network\nmodels which learn to associate linguistic units with emotions. While this\noften leads to good predictive performance, it does only help to a limited\ndegree to understand how emotions are communicated in various domains. The\nemotion component process model (CPM) by Scherer (2005) is an interesting\napproach to explain emotion communication. It states that emotions are a\ncoordinated process of various subcomponents, in reaction to an event, namely\nthe subjective feeling, the cognitive appraisal, the expression, a\nphysiological bodily reaction, and a motivational action tendency. We\nhypothesize that these components are associated with linguistic realizations:\nan emotion can be expressed by describing a physiological bodily reaction (\"he\nwas trembling\"), or the expression (\"she smiled\"), etc. We annotate existing\nliterature and Twitter emotion corpora with emotion component classes and find\nthat emotions on Twitter are predominantly expressed by event descriptions or\nsubjective reports of the feeling, while in literature, authors prefer to\ndescribe what characters do, and leave the interpretation to the reader. We\nfurther include the CPM in a multitask learning model and find that this\nsupports the emotion categorization. The annotated corpora are available at\nhttps://www.ims.uni-stuttgart.de/data/emotion.",
          "link": "http://arxiv.org/abs/2107.12895",
          "publishedOn": "2021-07-28T02:02:30.591Z",
          "wordCount": 645,
          "title": "Emotion Recognition under Consideration of the Emotion Component Process Model. (arXiv:2107.12895v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05299",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Correia_A/0/1/0/all/0/1\">A. D. Correia</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Moortgat_M/0/1/0/all/0/1\">M. Moortgat</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Stoof_H/0/1/0/all/0/1\">H. T. C. Stoof</a>",
          "description": "Grover's algorithm, a well-know quantum search algorithm, allows one to find\nthe correct item in a database, with quadratic speedup. In this paper we adapt\nGrover's algorithm to the problem of finding a correct answer to a natural\nlanguage question in English, thus contributing to the growing field of Quantum\nNatural Language Processing. Using a grammar that can be interpreted as tensor\ncontractions, each word is represented as a quantum state that serves as input\nto the quantum circuit. We here introduce a quantum measurement to contract the\nrepresentations of words, resulting in the representation of larger text\nfragments. Using this framework, a representation for the question is found\nthat contains all the possible answers in equal quantum superposition, and\nallows for the building of an oracle that can detect a correct answer, being\nagnostic to the specific question. Furthermore, we show that our construction\ncan deal with certain types of ambiguous phrases by keeping the various\ndifferent meanings in quantum superposition.",
          "link": "http://arxiv.org/abs/2106.05299",
          "publishedOn": "2021-07-28T02:02:30.531Z",
          "wordCount": 615,
          "title": "Grover's Algorithm for Question Answering. (arXiv:2106.05299v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eder_T/0/1/0/all/0/1\">Tobias Eder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hangya_V/0/1/0/all/0/1\">Viktor Hangya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>",
          "description": "Good quality monolingual word embeddings (MWEs) can be built for languages\nwhich have large amounts of unlabeled text. MWEs can be aligned to bilingual\nspaces using only a few thousand word translation pairs. For low resource\nlanguages training MWEs monolingually results in MWEs of poor quality, and thus\npoor bilingual word embeddings (BWEs) as well. This paper proposes a new\napproach for building BWEs in which the vector space of the high resource\nsource language is used as a starting point for training an embedding space for\nthe low resource target language. By using the source vectors as anchors the\nvector spaces are automatically aligned during training. We experiment on\nEnglish-German, English-Hiligaynon and English-Macedonian. We show that our\napproach results not only in improved BWEs and bilingual lexicon induction\nperformance, but also in improved target language MWE quality as measured using\nmonolingual word similarity.",
          "link": "http://arxiv.org/abs/2010.12627",
          "publishedOn": "2021-07-28T02:02:30.522Z",
          "wordCount": 624,
          "title": "Anchor-based Bilingual Word Embeddings for Low-Resource Languages. (arXiv:2010.12627v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1\">Stella Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>",
          "description": "Federated Learning aims to learn machine learning models from multiple\ndecentralized edge devices (e.g. mobiles) or servers without sacrificing local\ndata privacy. Recent Natural Language Processing techniques rely on deep\nlearning and large pre-trained language models. However, both big deep neural\nand language models are trained with huge amounts of data which often lies on\nthe server side. Since text data is widely originated from end users, in this\nwork, we look into recent NLP models and techniques which use federated\nlearning as the learning framework. Our survey discusses major challenges in\nfederated natural language processing, including the algorithm challenges,\nsystem challenges as well as the privacy issues. We also provide a critical\nreview of the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.",
          "link": "http://arxiv.org/abs/2107.12603",
          "publishedOn": "2021-07-28T02:02:30.479Z",
          "wordCount": 584,
          "title": "Federated Learning Meets Natural Language Processing: A Survey. (arXiv:2107.12603v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_K/0/1/0/all/0/1\">Kai Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>",
          "description": "The goal of dialogue state tracking (DST) is to predict the current dialogue\nstate given all previous dialogue contexts. Existing approaches generally\npredict the dialogue state at every turn from scratch. However, the\noverwhelming majority of the slots in each turn should simply inherit the slot\nvalues from the previous turn. Therefore, the mechanism of treating slots\nequally in each turn not only is inefficient but also may lead to additional\nerrors because of the redundant slot value generation. To address this problem,\nwe devise the two-stage DSS-DST which consists of the Dual Slot Selector based\non the current turn dialogue, and the Slot Value Generator based on the\ndialogue history. The Dual Slot Selector determines each slot whether to update\nslot value or to inherit the slot value from the previous turn from two\naspects: (1) if there is a strong relationship between it and the current turn\ndialogue utterances; (2) if a slot value with high reliability can be obtained\nfor it through the current turn dialogue. The slots selected to be updated are\npermitted to enter the Slot Value Generator to update values by a hybrid\nmethod, while the other slots directly inherit the values from the previous\nturn. Empirical results show that our method achieves 56.93%, 60.73%, and\n58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets\nrespectively and achieves a new state-of-the-art performance with significant\nimprovements.",
          "link": "http://arxiv.org/abs/2107.12578",
          "publishedOn": "2021-07-28T02:02:30.433Z",
          "wordCount": 685,
          "title": "Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking. (arXiv:2107.12578v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuchen Chai</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_J/0/1/0/all/0/1\">Juan Palacios</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianghao Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yichun Fan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a> (1) ((1) Massachusetts Institute of Technology, (2) Chinese Academy of Science)",
          "description": "COVID-19, as a global health crisis, has triggered the fear emotion with\nunprecedented intensity. Besides the fear of getting infected, the outbreak of\nCOVID-19 also created significant disruptions in people's daily life and thus\nevoked intensive psychological responses indirect to COVID-19 infections. Here,\nwe construct an expressed fear database using 16 million social media posts\ngenerated by 536 thousand users between January 1st, 2019 and August 31st, 2020\nin China. We employ deep learning techniques to detect the fear emotion within\neach post and apply topic models to extract the central fear topics. Based on\nthis database, we find that sleep disorders (\"nightmare\" and \"insomnia\") take\nup the largest share of fear-labeled posts in the pre-pandemic period (January\n2019-December 2019), and significantly increase during the COVID-19. We\nidentify health and work-related concerns are the two major sources of fear\ninduced by the COVID-19. We also detect gender differences, with females\ngenerating more posts containing the daily-life fear sources during the\nCOVID-19 period. This research adopts a data-driven approach to trace back\npublic emotion, which can be used to complement traditional surveys to achieve\nreal-time emotion monitoring to discern societal concerns and support policy\ndecision-making.",
          "link": "http://arxiv.org/abs/2107.12606",
          "publishedOn": "2021-07-28T02:02:30.394Z",
          "wordCount": 701,
          "title": "Measuring daily-life fear perception change: a computational study in the context of COVID-19. (arXiv:2107.12606v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.",
          "link": "http://arxiv.org/abs/2107.12514",
          "publishedOn": "2021-07-28T02:02:30.381Z",
          "wordCount": 709,
          "title": "Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Alongside huge volumes of research on deep learning models in NLP in the\nrecent years, there has been also much work on benchmark datasets needed to\ntrack modeling progress. Question answering and reading comprehension have been\nparticularly prolific in this regard, with over 80 new datasets appearing in\nthe past two years. This study is the largest survey of the field to date. We\nprovide an overview of the various formats and domains of the current\nresources, highlighting the current lacunae for future work. We further discuss\nthe current classifications of ``reasoning types\" in question answering and\npropose a new taxonomy. We also discuss the implications of over-focusing on\nEnglish, and survey the current monolingual resources for other languages and\nmultilingual resources. The study is aimed at both practitioners looking for\npointers to the wealth of existing data, and at researchers working on new\nresources.",
          "link": "http://arxiv.org/abs/2107.12708",
          "publishedOn": "2021-07-28T02:02:30.344Z",
          "wordCount": 594,
          "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension. (arXiv:2107.12708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yawen Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiasheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>",
          "description": "Unknown intent detection aims to identify the out-of-distribution (OOD)\nutterance whose intent has never appeared in the training set. In this paper,\nwe propose using energy scores for this task as the energy score is\ntheoretically aligned with the density of the input and can be derived from any\nclassifier. However, high-quality OOD utterances are required during the\ntraining stage in order to shape the energy gap between OOD and in-distribution\n(IND), and these utterances are difficult to collect in practice. To tackle\nthis problem, we propose a data manipulation framework to Generate high-quality\nOOD utterances with importance weighTs (GOT). Experimental results show that\nthe energy-based detector fine-tuned by GOT can achieve state-of-the-art\nresults on two benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.12542",
          "publishedOn": "2021-07-28T02:02:30.332Z",
          "wordCount": 572,
          "title": "Energy-based Unknown Intent Detection with Data Manipulation. (arXiv:2107.12542v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnow_K/0/1/0/all/0/1\">Kevin Parnow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>",
          "description": "Though the pre-trained contextualized language model (PrLM) has made a\nsignificant impact on NLP, training PrLMs in languages other than English can\nbe impractical for two reasons: other languages often lack corpora sufficient\nfor training powerful PrLMs, and because of the commonalities among human\nlanguages, computationally expensive PrLM training for different languages is\nsomewhat redundant. In this work, building upon the recent works connecting\ncross-lingual model transferring and neural machine translation, we thus\npropose a novel cross-lingual model transferring framework for PrLMs: TreLM. To\nhandle the symbol order and sequence length differences between languages, we\npropose an intermediate ``TRILayer\" structure that learns from these\ndifferences and creates a better transfer in our primary translation direction,\nas well as a new cross-lingual language modeling objective for transfer\ntraining. Additionally, we showcase an embedding aligning that adversarially\nadapts a PrLM's non-contextualized embedding space and the TRILayer structure\nto learn a text transformation network across languages, which addresses the\nvocabulary difference between languages. Experiments on both language\nunderstanding and structure parsing tasks show the proposed framework\nsignificantly outperforms language models trained from scratch with limited\ndata in both performance and efficiency. Moreover, despite an insignificant\nperformance loss compared to pre-training from scratch in resource-rich\nscenarios, our cross-lingual model transferring framework is significantly more\neconomical.",
          "link": "http://arxiv.org/abs/2107.12627",
          "publishedOn": "2021-07-28T02:02:30.304Z",
          "wordCount": 647,
          "title": "Cross-lingual Transferring of Pre-trained Contextualized Language Models. (arXiv:2107.12627v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-07-28T02:02:30.284Z",
          "wordCount": 604,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murdock_V/0/1/0/all/0/1\">Vanessa Murdock</a>",
          "description": "Online harassment in the form of hate speech has been on the rise in recent\nyears. Addressing the issue requires a combination of content moderation by\npeople, aided by automatic detection methods. As content moderation is itself\nharmful to the people doing it, we desire to reduce the burden by improving the\nautomatic detection of hate speech. Hate speech presents a challenge as it is\ndirected at different target groups using a completely different vocabulary.\nFurther the authors of the hate speech are incentivized to disguise their\nbehavior to avoid being removed from a platform. This makes it difficult to\ndevelop a comprehensive data set for training and evaluating hate speech\ndetection models because the examples that represent one hate speech domain do\nnot typically represent others, even within the same language or culture. We\npropose an unsupervised domain adaptation approach to augment labeled data for\nhate speech detection. We evaluate the approach with three different models\n(character CNNs, BiLSTMs and BERT) on three different collections. We show our\napproach improves Area under the Precision/Recall curve by as much as 42% and\nrecall by as much as 278%, with no loss (and in some cases a significant gain)\nin precision.",
          "link": "http://arxiv.org/abs/2107.12866",
          "publishedOn": "2021-07-28T02:02:30.266Z",
          "wordCount": 639,
          "title": "Unsupervised Domain Adaptation for Hate Speech Detection Using a Data Augmentation Approach. (arXiv:2107.12866v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehra_S/0/1/0/all/0/1\">Sunakshi Mehra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susan_S/0/1/0/all/0/1\">Seba Susan</a>",
          "description": "We introduce an unsupervised approach for correcting highly imperfect speech\ntranscriptions based on a decision-level fusion of stemming and two-way phoneme\npruning. Transcripts are acquired from videos by extracting audio using Ffmpeg\nframework and further converting audio to text transcript using Google API. In\nthe benchmark LRW dataset, there are 500 word categories, and 50 videos per\nclass in mp4 format. All videos consist of 29 frames (each 1.16 s long) and the\nword appears in the middle of the video. In our approach we tried to improve\nthe baseline accuracy from 9.34% by using stemming, phoneme extraction,\nfiltering and pruning. After applying the stemming algorithm to the text\ntranscript and evaluating the results, we achieved 23.34% accuracy in word\nrecognition. To convert words to phonemes we used the Carnegie Mellon\nUniversity (CMU) pronouncing dictionary that provides a phonetic mapping of\nEnglish words to their pronunciations. A two-way phoneme pruning is proposed\nthat comprises of the two non-sequential steps: 1) filtering and pruning the\nphonemes containing vowels and plosives 2) filtering and pruning the phonemes\ncontaining vowels and fricatives. After obtaining results of stemming and\ntwo-way phoneme pruning, we applied decision-level fusion and that led to an\nimprovement of word recognition rate upto 32.96%.",
          "link": "http://arxiv.org/abs/2107.12428",
          "publishedOn": "2021-07-28T02:02:30.231Z",
          "wordCount": 664,
          "title": "Improving Word Recognition in Speech Transcriptions by Decision-level Fusion of Stemming and Two-way Phoneme Pruning. (arXiv:2107.12428v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-27T02:03:35.510Z",
          "wordCount": 563,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiehang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liping Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Recently, few certified defense methods have been developed to provably\nguarantee the robustness of a text classifier to adversarial synonym\nsubstitutions. However, all existing certified defense methods assume that the\ndefenders are informed of how the adversaries generate synonyms, which is not a\nrealistic scenario. In this paper, we propose a certifiably robust defense\nmethod by randomly masking a certain proportion of the words in an input text,\nin which the above unrealistic assumption is no longer necessary. The proposed\nmethod can defend against not only word substitution-based attacks, but also\ncharacter-level perturbations. We can certify the classifications of over 50%\ntexts to be robust to any perturbation of 5 words on AGNEWS, and 2 words on\nSST2 dataset. The experimental results show that our randomized smoothing\nmethod significantly outperforms recently proposed defense methods across\nmultiple datasets.",
          "link": "http://arxiv.org/abs/2105.03743",
          "publishedOn": "2021-07-27T02:03:32.948Z",
          "wordCount": 616,
          "title": "Certified Robustness to Text Adversarial Attacks by Randomized [MASK]. (arXiv:2105.03743v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-07-27T02:03:32.927Z",
          "wordCount": 596,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shuang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Songge Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansur_M/0/1/0/all/0/1\">Mairgup Mansur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>",
          "description": "Recent years have seen significant advancement in text generation tasks with\nthe help of neural language models. However, there exists a challenging task:\ngenerating math problem text based on mathematical equations, which has made\nlittle progress so far. In this paper, we present a novel equation-to-problem\ntext generation model. In our model, 1) we propose a flexible scheme to\neffectively encode math equations, we then enhance the equation encoder by a\nVaritional Autoen-coder (VAE) 2) given a math equation, we perform topic\nselection, followed by which a dynamic topic memory mechanism is introduced to\nrestrict the topic distribution of the generator 3) to avoid commonsense\nviolation in traditional generation model, we pretrain word embedding with\nbackground knowledge graph (KG), and we link decoded words to related words in\nKG, targeted at injecting background knowledge into our model. We evaluate our\nmodel through both automatic metrices and human evaluation, experiments\ndemonstrate our model outperforms baseline and previous models in both accuracy\nand richness of generated problem text.",
          "link": "http://arxiv.org/abs/2012.07379",
          "publishedOn": "2021-07-27T02:03:32.630Z",
          "wordCount": 641,
          "title": "Generating Math Word Problems from Equations with Topic Controlling and Commonsense Enforcement. (arXiv:2012.07379v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1\">Diptanu Gon Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frank Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kritika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Assaf Sela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>",
          "description": "Language identification greatly impacts the success of downstream tasks such\nas automatic speech recognition. Recently, self-supervised speech\nrepresentations learned by wav2vec 2.0 have been shown to be very effective for\na range of speech tasks. We extend previous self-supervised work on language\nidentification by experimenting with pre-trained models which were learned on\nreal-world unconstrained speech in multiple languages and not just on English.\nWe show that models pre-trained on many languages perform better and enable\nlanguage identification systems that require very little labeled data to\nperform well. Results on a 25 languages setup show that with only 10 minutes of\nlabeled data per language, a cross-lingually pre-trained model can achieve over\n93% accuracy.",
          "link": "http://arxiv.org/abs/2107.04082",
          "publishedOn": "2021-07-27T02:03:32.622Z",
          "wordCount": 584,
          "title": "Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>",
          "description": "The Minimum Linear Arrangement problem (MLA) consists of finding a mapping\n$\\pi$ from vertices of a graph to distinct integers that minimizes\n$\\sum_{\\{u,v\\}\\in E}|\\pi(u) - \\pi(v)|$. In that setting, vertices are often\nassumed to lie on a horizontal line and edges are drawn as semicircles above\nsaid line. For trees, various algorithms are available to solve the problem in\npolynomial time in $n=|V|$. There exist variants of the MLA in which the\narrangements are constrained. Iordanskii, and later Hochberg and Stallmann\n(HS), put forward $O(n)$-time algorithms that solve the problem when\narrangements are constrained to be planar (also known as one-page book\nembeddings). We also consider linear arrangements of rooted trees that are\nconstrained to be projective (planar embeddings where the root is not covered\nby any edge). Gildea and Temperley (GT) sketched an algorithm for projective\narrangements which they claimed runs in $O(n)$ but did not provide any\njustification of its cost. In contrast, Park and Levy claimed that GT's\nalgorithm runs in $O(n \\log d_{max})$ where $d_{max}$ is the maximum degree but\ndid not provide sufficient detail. Here we correct an error in HS's algorithm\nfor the planar case, show its relationship with the projective case, and derive\nsimple algorithms for the projective and planar cases that run undoubtlessly in\n$O(n)$-time.",
          "link": "http://arxiv.org/abs/2102.03277",
          "publishedOn": "2021-07-27T02:03:32.594Z",
          "wordCount": 699,
          "title": "Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1\">Karan Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>",
          "description": "Novel neural architectures, training strategies, and the availability of\nlarge-scale corpora haven been the driving force behind recent progress in\nabstractive text summarization. However, due to the black-box nature of neural\nmodels, uninformative evaluation metrics, and scarce tooling for model and data\nanalysis, the true performance and failure modes of summarization models remain\nlargely unknown. To address this limitation, we introduce SummVis, an\nopen-source tool for visualizing abstractive summaries that enables\nfine-grained analysis of the models, data, and evaluation metrics associated\nwith text summarization. Through its lexical and semantic visualizations, the\ntools offers an easy entry point for in-depth model prediction exploration\nacross important dimensions such as factual consistency or abstractiveness. The\ntool together with several pre-computed model outputs is available at\nhttps://github.com/robustness-gym/summvis.",
          "link": "http://arxiv.org/abs/2104.07605",
          "publishedOn": "2021-07-27T02:03:32.450Z",
          "wordCount": 603,
          "title": "SummVis: Interactive Visual Analysis of Models, Data, and Evaluation for Text Summarization. (arXiv:2104.07605v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamm_A/0/1/0/all/0/1\">Andreas Hamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odrowski_S/0/1/0/all/0/1\">Simon Odrowski</a> (German Aerospace Center DLR)",
          "description": "Network-based procedures for topic detection in huge text collections offer\nan intuitive alternative to probabilistic topic models. We present in detail a\nmethod that is especially designed with the requirements of domain experts in\nmind. Like similar methods, it employs community detection in term\nco-occurrence graphs, but it is enhanced by including a resolution parameter\nthat can be used for changing the targeted topic granularity. We also establish\na term ranking and use semantic word-embedding for presenting term communities\nin a way that facilitates their interpretation. We demonstrate the application\nof our method with a widely used corpus of general news articles and show the\nresults of detailed social-sciences expert evaluations of detected topics at\nvarious resolutions. A comparison with topics detected by Latent Dirichlet\nAllocation is also included. Finally, we discuss factors that influence topic\ninterpretation.",
          "link": "http://arxiv.org/abs/2103.13550",
          "publishedOn": "2021-07-27T02:03:32.422Z",
          "wordCount": 604,
          "title": "Term-community-based topic detection with variable resolution. (arXiv:2103.13550v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1\">Timour Igamberdiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>",
          "description": "Graph convolutional networks (GCNs) are a powerful architecture for\nrepresentation learning on documents that naturally occur as graphs, e.g.,\ncitation or social networks. However, sensitive personal information, such as\ndocuments with people's profiles or relationships as edges, are prone to\nprivacy leaks, as the trained model might reveal the original input. Although\ndifferential privacy (DP) offers a well-founded privacy-preserving framework,\nGCNs pose theoretical and practical challenges due to their training specifics.\nWe address these challenges by adapting differentially-private gradient-based\ntraining to GCNs and conduct experiments using two optimizers on five NLP\ndatasets in two languages. We propose a simple yet efficient method based on\nrandom graph splits that not only improves the baseline privacy bounds by a\nfactor of 2.7 while retaining competitive F1 scores, but also provides strong\nprivacy guarantees of epsilon = 1.0. We show that, under certain modeling\nchoices, privacy-preserving GCNs perform up to 90% of their non-private\nvariants, while formally guaranteeing strong privacy measures.",
          "link": "http://arxiv.org/abs/2102.09604",
          "publishedOn": "2021-07-27T02:03:32.404Z",
          "wordCount": 625,
          "title": "Privacy-Preserving Graph Convolutional Networks for Text Classification. (arXiv:2102.09604v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianlong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuiqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fang Chen</a>",
          "description": "The outbreak of the novel Coronavirus Disease 2019 (COVID-19) has caused\nunprecedented impacts to people's daily life around the world. Various measures\nand policies such as lockdown and social-distancing are implemented by\ngovernments to combat the disease during the pandemic period. These measures\nand policies as well as virus itself may cause different mental health issues\nto people such as depression, anxiety, sadness, etc. In this paper, we exploit\nthe massive text data posted by Twitter users to analyse the sentiment dynamics\nof people living in the state of New South Wales (NSW) in Australia during the\npandemic period. Different from the existing work that mostly focuses the\ncountry-level and static sentiment analysis, we analyse the sentiment dynamics\nat the fine-grained local government areas (LGAs). Based on the analysis of\naround 94 million tweets that posted by around 183 thousand users located at\ndifferent LGAs in NSW in five months, we found that people in NSW showed an\noverall positive sentimental polarity and the COVID-19 pandemic decreased the\noverall positive sentimental polarity during the pandemic period. The\nfine-grained analysis of sentiment in LGAs found that despite the dominant\npositive sentiment most of days during the study period, some LGAs experienced\nsignificant sentiment changes from positive to negative. This study also\nanalysed the sentimental dynamics delivered by the hot topics in Twitter such\nas government policies (e.g. the Australia's JobKeeper program, lockdown,\nsocial-distancing) as well as the focused social events (e.g. the Ruby Princess\nCruise). The results showed that the policies and events did affect people's\noverall sentiment, and they affected people's overall sentiment differently at\ndifferent stages.",
          "link": "http://arxiv.org/abs/2006.12185",
          "publishedOn": "2021-07-27T02:03:32.386Z",
          "wordCount": 815,
          "title": "Examination of Community Sentiment Dynamics due to COVID-19 Pandemic: A Case Study from A State in Australia. (arXiv:2006.12185v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Holla_N/0/1/0/all/0/1\">Nithin Holla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pushkar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>",
          "description": "Lifelong learning requires models that can continuously learn from sequential\nstreams of data without suffering catastrophic forgetting due to shifts in data\ndistributions. Deep learning models have thrived in the non-sequential learning\nparadigm; however, when used to learn a sequence of tasks, they fail to retain\npast knowledge and learn incrementally. We propose a novel approach to lifelong\nlearning of language tasks based on meta-learning with sparse experience replay\nthat directly optimizes to prevent forgetting. We show that under the realistic\nsetting of performing a single pass on a stream of tasks and without any task\nidentifiers, our method obtains state-of-the-art results on lifelong text\nclassification and relation extraction. We analyze the effectiveness of our\napproach and further demonstrate its low computational and space complexity.",
          "link": "http://arxiv.org/abs/2009.04891",
          "publishedOn": "2021-07-27T02:03:32.377Z",
          "wordCount": 591,
          "title": "Meta-Learning with Sparse Experience Replay for Lifelong Language Learning. (arXiv:2009.04891v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.03855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynn_V/0/1/0/all/0/1\">Veronica Lynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Keshav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Farhan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matz_S/0/1/0/all/0/1\">Sandra Matz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>",
          "description": "Social media is increasingly used for large-scale population predictions,\nsuch as estimating community health statistics. However, social media users are\nnot typically a representative sample of the intended population -- a\n\"selection bias\". Within the social sciences, such a bias is typically\naddressed with restratification techniques, where observations are reweighted\naccording to how under- or over-sampled their socio-demographic groups are.\nYet, restratifaction is rarely evaluated for improving prediction. Across four\ntasks of predicting U.S. county population health statistics from Twitter, we\nfind standard restratification techniques provide no improvement and often\ndegrade prediction accuracies. The core reasons for this seems to be both\nshrunken estimates (reduced variance of model predicted values) and sparse\nestimates of each population's socio-demographics. We thus develop and evaluate\nthree methods to address these problems: estimator redistribution to account\nfor shrinking, and adaptive binning and informed smoothing to handle sparse\nsocio-demographic estimates. We show that each of these methods significantly\noutperforms the standard restratification approaches. Combining approaches, we\nfind substantial improvements over non-restratified models, yielding a 53.0%\nincrease in predictive accuracy (R^2) in the case of surveyed life\nsatisfaction, and a 17.8% average increase across all tasks.",
          "link": "http://arxiv.org/abs/1911.03855",
          "publishedOn": "2021-07-27T02:03:32.356Z",
          "wordCount": 686,
          "title": "Correcting Sociodemographic Selection Biases for Population Prediction from Social Media. (arXiv:1911.03855v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-27T02:03:32.347Z",
          "wordCount": 717,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1\">Sagor Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Mehadi Hasan Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Kabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Azam Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1\">Stefan Decker</a>",
          "description": "The exponential growths of social media and micro-blogging sites not only\nprovide platforms for empowering freedom of expressions and individual voices,\nbut also enables people to express anti-social behavior like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\ntextual data for social and anti-social behavior analysis, by predicting the\ncontexts mostly for highly-resourced languages like English. However, some\nlanguages are under-resourced, e.g., South Asian languages like Bengali, that\nlack computational resources for accurate natural language processing (NLP). In\nthis paper, we propose an explainable approach for hate speech detection from\nthe under-resourced Bengali language, which we called DeepHateExplainer.\nBengali texts are first comprehensively preprocessed, before classifying them\ninto political, personal, geopolitical, and religious hates using a neural\nensemble method of transformer-based neural architectures (i.e., monolingual\nBangla BERT-base, multilingual BERT-cased/uncased, and XLM-RoBERTa).\nImportant~(most and least) terms are then identified using sensitivity analysis\nand layer-wise relevance propagation~(LRP), before providing\nhuman-interpretable explanations. Finally, we compute comprehensiveness and\nsufficiency scores to measure the quality of explanations w.r.t faithfulness.\nEvaluations against machine learning~(linear and tree-based models) and neural\nnetworks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines\nyield F1-scores of 78%, 91%, 89%, and 84%, for political, personal,\ngeopolitical, and religious hates, respectively, outperforming both ML and DNN\nbaselines.",
          "link": "http://arxiv.org/abs/2012.14353",
          "publishedOn": "2021-07-27T02:03:32.339Z",
          "wordCount": 699,
          "title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gargi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_D/0/1/0/all/0/1\">Dhanajit Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>",
          "description": "In this paper, we propose a new framework for fine-grained emotion prediction\nin the text through emotion definition modeling. Our approach involves a\nmulti-task learning framework that models definitions of emotions as an\nauxiliary task while being trained on the primary task of emotion prediction.\nWe model definitions using masked language modeling and class definition\nprediction tasks. Our models outperform existing state-of-the-art for\nfine-grained emotion dataset GoEmotions. We further show that this trained\nmodel can be used for transfer learning on other benchmark datasets in emotion\nprediction with varying emotion label sets, domains, and sizes. The proposed\nmodels outperform the baselines on transfer learning experiments demonstrating\nthe generalization capability of the models.",
          "link": "http://arxiv.org/abs/2107.12135",
          "publishedOn": "2021-07-27T02:03:32.322Z",
          "wordCount": 554,
          "title": "Fine-Grained Emotion Prediction by Modeling Emotion Definitions. (arXiv:2107.12135v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>",
          "description": "Health literacy has emerged as a crucial factor in making appropriate health\ndecisions and ensuring treatment outcomes. However, medical jargon and the\ncomplex structure of professional language in this domain make health\ninformation especially hard to interpret. Thus, there is an urgent unmet need\nfor automated methods to enhance the accessibility of the biomedical literature\nto the general population. This problem can be framed as a type of translation\nproblem between the language of healthcare professionals, and that of the\ngeneral public. In this paper, we introduce the novel task of automated\ngeneration of lay language summaries of biomedical scientific reviews, and\nconstruct a dataset to support the development and evaluation of automated\nmethods through which to enhance the accessibility of the biomedical\nliterature. We conduct analyses of the various challenges in solving this task,\nincluding not only summarization of the key points but also explanation of\nbackground knowledge and simplification of professional language. We experiment\nwith state-of-the-art summarization models as well as several data augmentation\ntechniques, and evaluate their performance using both automated metrics and\nhuman assessment. Results indicate that automatically generated summaries\nproduced using contemporary neural architectures can achieve promising quality\nand readability as compared with reference summaries developed for the lay\npublic by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score\nof 13.30). We also discuss the limitations of the current attempt, providing\ninsights and directions for future work.",
          "link": "http://arxiv.org/abs/2012.12573",
          "publishedOn": "2021-07-27T02:03:32.315Z",
          "wordCount": 716,
          "title": "Automated Lay Language Summarization of Biomedical Scientific Reviews. (arXiv:2012.12573v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1\">Geeticka Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1\">Brian Tse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "Recent years have seen many breakthroughs in natural language processing\n(NLP), transitioning it from a mostly theoretical field to one with many\nreal-world applications. Noting the rising number of applications of other\nmachine learning and AI techniques with pervasive societal impact, we\nanticipate the rising importance of developing NLP technologies for social\ngood. Inspired by theories in moral philosophy and global priorities research,\nwe aim to promote a guideline for social good in the context of NLP. We lay the\nfoundations via the moral philosophy definition of social good, propose a\nframework to evaluate the direct and indirect real-world impact of NLP tasks,\nand adopt the methodology of global priorities research to identify priority\ncauses for NLP research. Finally, we use our theoretical framework to provide\nsome practical guidelines for future NLP research for social good. Our data and\ncode are available at this http URL In\naddition, we curate a list of papers and resources on NLP for social good at\nhttps://github.com/zhijing-jin/NLP4SocialGood_Papers.",
          "link": "http://arxiv.org/abs/2106.02359",
          "publishedOn": "2021-07-27T02:03:32.306Z",
          "wordCount": 671,
          "title": "How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pagnoni_A/0/1/0/all/0/1\">Artidoro Pagnoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Modern summarization models generate highly fluent but often factually\nunreliable outputs. This motivated a surge of metrics attempting to measure the\nfactuality of automatically generated summaries. Due to the lack of common\nbenchmarks, these metrics cannot be compared. Moreover, all these methods treat\nfactuality as a binary concept and fail to provide deeper insights into the\nkinds of inconsistencies made by different systems. To address these\nlimitations, we devise a typology of factual errors and use it to collect human\nannotations of generated summaries from state-of-the-art summarization systems\nfor the CNN/DM and XSum datasets. Through these annotations, we identify the\nproportion of different categories of factual errors in various summarization\nmodels and benchmark factuality metrics, showing their correlation with human\njudgment as well as their specific strengths and weaknesses.",
          "link": "http://arxiv.org/abs/2104.13346",
          "publishedOn": "2021-07-27T02:03:32.298Z",
          "wordCount": 605,
          "title": "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. (arXiv:2104.13346v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Long Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Do state-of-the-art natural language understanding models care about word\norder - one of the most important characteristics of a sequence? Not always! We\nfound 75% to 90% of the correct predictions of BERT-based classifiers, trained\non many GLUE tasks, remain constant after input words are randomly shuffled.\nDespite BERT embeddings are famously contextual, the contribution of each\nindividual word to downstream tasks is almost unchanged even after the word's\ncontext is shuffled. BERT-based models are able to exploit superficial cues\n(e.g. the sentiment of keywords in sentiment analysis; or the word-wise\nsimilarity between sequence-pair inputs in natural language inference) to make\ncorrect decisions when tokens are arranged in random orders. Encouraging\nclassifiers to capture word order information improves the performance on most\nGLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE\ntasks are not challenging machines to understand the meaning of a sentence.",
          "link": "http://arxiv.org/abs/2012.15180",
          "publishedOn": "2021-07-27T02:03:32.291Z",
          "wordCount": 649,
          "title": "Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?. (arXiv:2012.15180v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malysheva_A/0/1/0/all/0/1\">Anastasia Malysheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>",
          "description": "Narrative generation and analysis are still on the fringe of modern natural\nlanguage processing yet are crucial in a variety of applications. This paper\nproposes a feature extraction method for plot dynamics. We present a dataset\nthat consists of the plot descriptions for thirteen thousand TV shows alongside\nmeta-information on their genres and dynamic plots extracted from them. We\nvalidate the proposed tool for plot dynamics extraction and discuss possible\napplications of this method to the tasks of narrative analysis and generation.",
          "link": "http://arxiv.org/abs/2107.12226",
          "publishedOn": "2021-07-27T02:03:32.283Z",
          "wordCount": 518,
          "title": "DYPLODOC: Dynamic Plots for Document Classification. (arXiv:2107.12226v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2001.05297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cathcart_C/0/1/0/all/0/1\">Chundra Aroor Cathcart</a>",
          "description": "This paper addresses a series of complex and unresolved issues in the\nhistorical phonology of West Iranian languages. The West Iranian languages\n(Persian, Kurdish, Balochi, and other languages) display a high degree of\nnon-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to\nlanguage contact; we argue, however, that an oversimplified view of the\nprocesses at work has prevailed in the literature on West Iranian dialectology,\nwith specialists assuming that deviations from an expected outcome in a given\nnon-Persian language are due to lexical borrowing from some chronological stage\nof Persian. It is demonstrated that this qualitative approach yields at times\nproblematic conclusions stemming from the lack of explicit probabilistic\ninferences regarding the distribution of the data: Persian may not be the sole\ndonor language; additionally, borrowing at the lexical level is not always the\nmechanism that introduces irregularity. In many cases, the possibility that\nWest Iranian languages show different reflexes in different conditioning\nenvironments remains under-explored. We employ a novel Bayesian approach\ndesigned to overcome these problems and tease apart the different determinants\nof irregularity in patterns of West Iranian sound change. Our methodology\nallows us to provisionally resolve a number of outstanding questions in the\nliterature on West Iranian dialectology concerning the dialectal affiliation of\ncertain sound changes. We outline future directions for work of this sort.",
          "link": "http://arxiv.org/abs/2001.05297",
          "publishedOn": "2021-07-27T02:03:32.276Z",
          "wordCount": 692,
          "title": "Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process Approach to Linguistic Relationships. (arXiv:2001.05297v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>",
          "description": "Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA\nwhich outputs triplets of an aspect target, its associated sentiment, and the\ncorresponding opinion term. Recent models perform the triplet extraction in an\nend-to-end manner but heavily rely on the interactions between each target word\nand opinion word. Thereby, they cannot perform well on targets and opinions\nwhich contain multiple words. Our proposed span-level approach explicitly\nconsiders the interaction between the whole spans of targets and opinions when\npredicting their sentiment relation. Thus, it can make predictions with the\nsemantics of whole spans, ensuring better sentiment consistency. To ease the\nhigh computational cost caused by span enumeration, we propose a dual-channel\nspan pruning strategy by incorporating supervision from the Aspect Term\nExtraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not\nonly improves computational efficiency but also distinguishes the opinion and\ntarget spans more properly. Our framework simultaneously achieves strong\nperformance for the ASTE as well as ATE and OTE tasks. In particular, our\nanalysis shows that our span-level approach achieves more significant\nimprovements over the baselines on triplets with multi-word targets or\nopinions.",
          "link": "http://arxiv.org/abs/2107.12214",
          "publishedOn": "2021-07-27T02:03:32.266Z",
          "wordCount": 624,
          "title": "Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction. (arXiv:2107.12214v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Gongbo Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronchen_P/0/1/0/all/0/1\">Philipp R&#xf6;nchen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nivre_J/0/1/0/all/0/1\">Joakim Nivre</a>",
          "description": "In this paper, we evaluate the translation of negation both automatically and\nmanually, in English--German (EN--DE) and English--Chinese (EN--ZH). We show\nthat the ability of neural machine translation (NMT) models to translate\nnegation has improved with deeper and more advanced networks, although the\nperformance varies between language pairs and translation directions. The\naccuracy of manual evaluation in EN-DE, DE-EN, EN-ZH, and ZH-EN is 95.7%,\n94.8%, 93.4%, and 91.7%, respectively. In addition, we show that\nunder-translation is the most significant error type in NMT, which contrasts\nwith the more diverse error profile previously observed for statistical machine\ntranslation. To better understand the root of the under-translation of\nnegation, we study the model's information flow and training data. While our\ninformation flow analysis does not reveal any deficiencies that could be used\nto detect or fix the under-translation of negation, we find that negation is\noften rephrased during training, which could make it more difficult for the\nmodel to learn a reliable link between source and target negation. We finally\nconduct intrinsic analysis and extrinsic probing tasks on negation, showing\nthat NMT models can distinguish negation and non-negation tokens very well and\nencode a lot of information about negation in hidden states but nevertheless\nleave room for improvement.",
          "link": "http://arxiv.org/abs/2107.12203",
          "publishedOn": "2021-07-27T02:03:32.239Z",
          "wordCount": 648,
          "title": "Revisiting Negation in Neural Machine Translation. (arXiv:2107.12203v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Biao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guorui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>",
          "description": "Recent advances in linguistic steganalysis have successively applied CNNs,\nRNNs, GNNs and other deep learning models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated steganographic texts and carrier texts in terms of the conditional\nprobability distribution of individual words. Such kind of statistical\ndifference can be naturally captured by the language model used for generating\nsteganographic texts, which drives us to give the classifier a priori knowledge\nof the language model to enhance the steganalysis ability. To this end, we\npresent two methods to efficient linguistic steganalysis in this paper. One is\nto pre-train a language model based on RNN, and the other is to pre-train a\nsequence autoencoder. Experimental results show that the two methods have\ndifferent degrees of performance improvement when compared to the randomly\ninitialized RNN classifier, and the convergence speed is significantly\naccelerated. Moreover, our methods have achieved the best detection results.",
          "link": "http://arxiv.org/abs/2107.12168",
          "publishedOn": "2021-07-27T02:03:32.217Z",
          "wordCount": 615,
          "title": "Exploiting Language Model for Efficient Linguistic Steganalysis: An Empirical Study. (arXiv:2107.12168v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.10058",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Slavnov_S/0/1/0/all/0/1\">Sergey Slavnov</a>",
          "description": "We consider tensor grammars, which are an example of \\commutative\" grammars,\nbased on the classical (rather than intuitionistic) linear logic. They can be\nseen as a surface representation of abstract categorial grammars ACG in the\nsense that derivations of ACG translate to derivations of tensor grammars and\nthis translation is isomorphic on the level of string languages. The basic\ningredient are tensor terms, which can be seen as encoding and generalizing\nproof-nets. Using tensor terms makes the syntax extremely simple and a direct\ngeometric meaning becomes transparent. Then we address the problem of encoding\nnoncommutative operations in our setting. This turns out possible after\nenriching the system with new unary operators. The resulting system allows\nrepresenting both ACG and Lambek grammars as conservative fragments, while the\nformalism remains, as it seems to us, rather simple and intuitive.",
          "link": "http://arxiv.org/abs/2005.10058",
          "publishedOn": "2021-07-27T02:03:32.187Z",
          "wordCount": 621,
          "title": "On embedding Lambek calculus into commutative categorial grammars. (arXiv:2005.10058v3 [math.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>",
          "description": "In this paper, we present coreference resolution experiments with a newly\ncreated multilingual corpus CorefUD. We focus on the following languages:\nCzech, Russian, Polish, German, Spanish, and Catalan. In addition to\nmonolingual experiments, we combine the training data in multilingual\nexperiments and train two joined models -- for Slavic languages and for all the\nlanguages together. We rely on an end-to-end deep learning model that we\nslightly adapted for the CorefUD corpus. Our results show that we can profit\nfrom harmonized annotations, and using joined models helps significantly for\nthe languages with smaller training data.",
          "link": "http://arxiv.org/abs/2107.12088",
          "publishedOn": "2021-07-27T02:03:32.180Z",
          "wordCount": 520,
          "title": "Multilingual Coreference Resolution with Harmonized Annotations. (arXiv:2107.12088v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12487",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mardaoui_D/0/1/0/all/0/1\">Dina Mardaoui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>",
          "description": "Text data are increasingly handled in an automated fashion by machine\nlearning algorithms. But the models handling these data are not always\nwell-understood due to their complexity and are more and more often referred to\nas \"black-boxes.\" Interpretability methods aim to explain how these models\noperate. Among them, LIME has become one of the most popular in recent years.\nHowever, it comes without theoretical guarantees: even for simple models, we\nare not sure that LIME behaves accurately. In this paper, we provide a first\ntheoretical analysis of LIME for text data. As a consequence of our theoretical\nfindings, we show that LIME indeed provides meaningful explanations for simple\nmodels, namely decision trees and linear models.",
          "link": "http://arxiv.org/abs/2010.12487",
          "publishedOn": "2021-07-27T02:03:32.173Z",
          "wordCount": 574,
          "title": "An Analysis of LIME for Text Data. (arXiv:2010.12487v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>",
          "description": "Recently, several studies reported that dot-product selfattention (SA) may\nnot be indispensable to the state-of-theart Transformer models. Motivated by\nthe fact that dense synthesizer attention (DSA), which dispenses with dot\nproducts and pairwise interactions, achieved competitive results in many\nlanguage processing tasks, in this paper, we first propose a DSA-based speech\nrecognition, as an alternative to SA. To reduce the computational complexity\nand improve the performance, we further propose local DSA (LDSA) to restrict\nthe attention scope of DSA to a local range around the current central frame\nfor speech recognition. Finally, we combine LDSA with SA to extract the local\nand global information simultaneously. Experimental results on the Ai-shell1\nMandarine speech recognition corpus show that the proposed LDSA-Transformer\nachieves a character error rate (CER) of 6.49%, which is slightly better than\nthat of the SA-Transformer. Meanwhile, the LDSA-Transformer requires less\ncomputation than the SATransformer. The proposed combination method not only\nachieves a CER of 6.18%, which significantly outperforms the SA-Transformer,\nbut also has roughly the same number of parameters and computational complexity\nas the latter. The implementation of the multi-head LDSA is available at\nhttps://github.com/mlxu995/multihead-LDSA.",
          "link": "http://arxiv.org/abs/2010.12155",
          "publishedOn": "2021-07-27T02:03:32.165Z",
          "wordCount": 673,
          "title": "Transformer-based End-to-End Speech Recognition with Local Dense Synthesizer Attention. (arXiv:2010.12155v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>",
          "description": "When humans solve complex problems, they rarely come up with a decision\nright-away. Instead, they start with an intuitive decision, reflect upon it,\nspot mistakes, resolve contradictions and jump between different hypotheses.\nThus, they create a sequence of ideas and follow a train of thought that\nultimately reaches a conclusive decision. Contrary to this, today's neural\nclassification models are mostly trained to map an input to one single and\nfixed output. In this paper, we investigate how we can give models the\nopportunity of a second, third and $k$-th thought. We take inspiration from\nHegel's dialectics and propose a method that turns an existing classifier's\nclass prediction (such as the image class forest) into a sequence of\npredictions (such as forest $\\rightarrow$ tree $\\rightarrow$ mushroom).\nConcretely, we propose a correction module that is trained to estimate the\nmodel's correctness as well as an iterative prediction update based on the\nprediction's gradient. Our approach results in a dynamic system over class\nprobability distributions $\\unicode{x2014}$ the thought flow. We evaluate our\nmethod on diverse datasets and tasks from computer vision and natural language\nprocessing. We observe surprisingly complex but intuitive behavior and\ndemonstrate that our method (i) can correct misclassifications, (ii)\nstrengthens model performance, (iii) is robust to high levels of adversarial\nattacks, (iv) can increase accuracy up to 4% in a label-distribution-shift\nsetting and (iv) provides a tool for model interpretability that uncovers model\nknowledge which otherwise remains invisible in a single distribution\nprediction.",
          "link": "http://arxiv.org/abs/2107.12220",
          "publishedOn": "2021-07-27T02:03:32.146Z",
          "wordCount": 694,
          "title": "Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.02275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Critch_A/0/1/0/all/0/1\">Andrew Critch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "We show how to assess a language model's knowledge of basic concepts of\nmorality. We introduce the ETHICS dataset, a new benchmark that spans concepts\nin justice, well-being, duties, virtues, and commonsense morality. Models\npredict widespread moral judgments about diverse text scenarios. This requires\nconnecting physical and social world knowledge to value judgements, a\ncapability that may enable us to steer chatbot outputs or eventually regularize\nopen-ended reinforcement learning agents. With the ETHICS dataset, we find that\ncurrent language models have a promising but incomplete ability to predict\nbasic human ethical judgements. Our work shows that progress can be made on\nmachine ethics today, and it provides a steppingstone toward AI that is aligned\nwith human values.",
          "link": "http://arxiv.org/abs/2008.02275",
          "publishedOn": "2021-07-27T02:03:32.139Z",
          "wordCount": 634,
          "title": "Aligning AI With Shared Human Values. (arXiv:2008.02275v5 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>",
          "description": "We present CORA, a Cross-lingual Open-Retrieval Answer Generation model that\ncan answer questions across many languages even when language-specific\nannotated data or knowledge sources are unavailable. We introduce a new dense\npassage retrieval algorithm that is trained to retrieve documents across\nlanguages for a question. Combined with a multilingual autoregressive\ngeneration model, CORA answers directly in the target language without any\ntranslation or in-language retrieval modules as used in prior work. We propose\nan iterative training method that automatically extends annotated data\navailable only in high-resource languages to low-resource ones. Our results\nshow that CORA substantially outperforms the previous state of the art on\nmultilingual open question answering benchmarks across 26 languages, 9 of which\nare unseen during training. Our analyses show the significance of cross-lingual\nretrieval and generation in many languages, particularly under low-resource\nsettings.",
          "link": "http://arxiv.org/abs/2107.11976",
          "publishedOn": "2021-07-27T02:03:32.011Z",
          "wordCount": 588,
          "title": "One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval. (arXiv:2107.11976v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zikun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>",
          "description": "Knowledge Graph (KG) and attention mechanism have been demonstrated effective\nin introducing and selecting useful information for weakly supervised methods.\nHowever, only qualitative analysis and ablation study are provided as evidence.\nIn this paper, we contribute a dataset and propose a paradigm to quantitatively\nevaluate the effect of attention and KG on bag-level relation extraction (RE).\nWe find that (1) higher attention accuracy may lead to worse performance as it\nmay harm the model's ability to extract entity mention features; (2) the\nperformance of attention is largely influenced by various noise distribution\npatterns, which is closely related to real-world datasets; (3) KG-enhanced\nattention indeed improves RE performance, while not through enhanced attention\nbut by incorporating entity prior; and (4) attention mechanism may exacerbate\nthe issue of insufficient training data. Based on these findings, we show that\na straightforward variant of RE model can achieve significant improvements (6%\nAUC on average) on two real-world datasets as compared with three\nstate-of-the-art baselines. Our codes and datasets are available at\nhttps://github.com/zig-kwin-hu/how-KG-ATT-help.",
          "link": "http://arxiv.org/abs/2107.12064",
          "publishedOn": "2021-07-27T02:03:32.003Z",
          "wordCount": 615,
          "title": "How Knowledge Graph and Attention Help? A Quantitative Analysis into Bag-level Relation Extraction. (arXiv:2107.12064v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1\">Bo-Hsiang Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreyssig_F/0/1/0/all/0/1\">Florian Kreyssig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>",
          "description": "One of the difficulties in training dialogue systems is the lack of training\ndata. We explore the possibility of creating dialogue data through the\ninteraction between a dialogue system and a user simulator. Our goal is to\ndevelop a modelling framework that can incorporate new dialogue scenarios\nthrough self-play between the two agents. In this framework, we first pre-train\nthe two agents on a collection of source domain dialogues, which equips the\nagents to converse with each other via natural language. With further\nfine-tuning on a small amount of target domain data, the agents continue to\ninteract with the aim of improving their behaviors using reinforcement learning\nwith structured reward functions. In experiments on the MultiWOZ dataset, two\npractical transfer learning problems are investigated: 1) domain adaptation and\n2) single-to-multiple domain transfer. We demonstrate that the proposed\nframework is highly effective in bootstrapping the performance of the two\nagents in transfer learning. We also show that our method leads to improvements\nin dialogue system performance on complete datasets.",
          "link": "http://arxiv.org/abs/2107.11904",
          "publishedOn": "2021-07-27T02:03:31.996Z",
          "wordCount": 606,
          "title": "Transferable Dialogue Systems and User Simulators. (arXiv:2107.11904v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>",
          "description": "Although Graph Convolutional Networks (GCNs) have demonstrated their power in\nvarious applications, the graph convolutional layers, as the most important\ncomponent of GCN, are still using linear transformations and a simple pooling\nstep. In this paper, we propose a novel generalization of Factorized Bilinear\n(FB) layer to model the feature interactions in GCNs. FB performs two\nmatrix-vector multiplications, that is, the weight matrix is multiplied with\nthe outer product of the vector of hidden features from both sides. However,\nthe FB layer suffers from the quadratic number of coefficients, overfitting and\nthe spurious correlations due to correlations between channels of hidden\nrepresentations that violate the i.i.d. assumption. Thus, we propose a compact\nFB layer by defining a family of summarizing operators applied over the\nquadratic term. We analyze proposed pooling operators and motivate their use.\nOur experimental results on multiple datasets demonstrate that the GFB-GCN is\ncompetitive with other methods for text classification.",
          "link": "http://arxiv.org/abs/2107.11666",
          "publishedOn": "2021-07-27T02:03:31.965Z",
          "wordCount": 588,
          "title": "Graph Convolutional Network with Generalized Factorized Bilinear Aggregation. (arXiv:2107.11666v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1\">Mokanarangan Thayaparan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>",
          "description": "Regenerating natural language explanations for science questions is a\nchallenging task for evaluating complex multi-hop and abductive inference\ncapabilities. In this setting, Transformers trained on human-annotated\nexplanations achieve state-of-the-art performance when adopted as cross-encoder\narchitectures. However, while much attention has been devoted to the quality of\nthe constructed explanations, the problem of performing abductive inference at\nscale is still under-studied. As intrinsically not scalable, the cross-encoder\narchitectural paradigm is not suitable for efficient multi-hop inference on\nmassive facts banks. To maximise both accuracy and inference time, we propose a\nhybrid abductive solver that autoregressively combines a dense bi-encoder with\na sparse model of explanatory power, computed leveraging explicit patterns in\nthe explanations. Our experiments demonstrate that the proposed framework can\nachieve performance comparable with the state-of-the-art cross-encoder while\nbeing $\\approx 50$ times faster and scalable to corpora of millions of facts.\nMoreover, we study the impact of the hybridisation on semantic drift and\nscience question answering without additional training, showing that it boosts\nthe quality of the explanations and contributes to improved downstream\ninference performance.",
          "link": "http://arxiv.org/abs/2107.11879",
          "publishedOn": "2021-07-27T02:03:31.958Z",
          "wordCount": 617,
          "title": "Hybrid Autoregressive Solver for Scalable Abductive Natural Language Inference. (arXiv:2107.11879v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Multi-hop reading comprehension (MHRC) requires not only to predict the\ncorrect answer span in the given passage, but also to provide a chain of\nsupporting evidences for reasoning interpretability. It is natural to model\nsuch a process into graph structure by understanding multi-hop reasoning as\njumping over entity nodes, which has made graph modelling dominant on this\ntask. Recently, there have been dissenting voices about whether graph modelling\nis indispensable due to the inconvenience of the graph building, however\nexisting state-of-the-art graph-free attempts suffer from huge performance gap\ncompared to graph-based ones. This work presents a novel graph-free alternative\nwhich firstly outperform all graph models on MHRC. In detail, we exploit a\nselect-to-guide (S2G) strategy to accurately retrieve evidence paragraphs in a\ncoarse-to-fine manner, incorporated with two novel attention mechanisms, which\nsurprisingly shows conforming to the nature of multi-hop reasoning. Our\ngraph-free model achieves significant and consistent performance gain over\nstrong baselines and the current new state-of-the-art on the MHRC benchmark,\nHotpotQA, among all the published works.",
          "link": "http://arxiv.org/abs/2107.11823",
          "publishedOn": "2021-07-27T02:03:31.951Z",
          "wordCount": 595,
          "title": "Graph-free Multi-hop Reading Comprehension: A Select-to-Guide Strategy. (arXiv:2107.11823v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>",
          "description": "Much research in recent years has focused on automatic article commenting.\nHowever, few of previous studies focus on the controllable generation of\ncomments. Besides, they tend to generate dull and commonplace comments, which\nfurther limits their practical application. In this paper, we make the first\nstep towards controllable generation of comments, by building a system that can\nexplicitly control the emotion of the generated comments. To achieve this, we\nassociate each kind of emotion category with an embedding and adopt a dynamic\nfusion mechanism to fuse this embedding into the decoder. A sentence-level\nemotion classifier is further employed to better guide the model to generate\ncomments expressing the desired emotion. To increase the diversity of the\ngenerated comments, we propose a hierarchical copy mechanism that allows our\nmodel to directly copy words from the input articles. We also propose a\nrestricted beam search (RBS) algorithm to increase intra-sentence diversity.\nExperimental results show that our model can generate informative and diverse\ncomments that express the desired emotions with high accuracy.",
          "link": "http://arxiv.org/abs/2107.11781",
          "publishedOn": "2021-07-27T02:03:31.944Z",
          "wordCount": 596,
          "title": "Towards Controlled and Diverse Generation of Article Comments. (arXiv:2107.11781v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>",
          "description": "Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation and state-of-the-art\nlanguage technologies. We illustrate and evaluate the system using a COVID-19\nvaccine information case study.",
          "link": "http://arxiv.org/abs/2107.12079",
          "publishedOn": "2021-07-27T02:03:31.937Z",
          "wordCount": 541,
          "title": "An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>",
          "description": "In this work, we examine the ability of NER models to use contextual\ninformation when predicting the type of an ambiguous entity. We introduce NRB,\na new testbed carefully designed to diagnose Name Regularity Bias of NER\nmodels. Our results indicate that all state-of-the-art models we tested show\nsuch a bias; BERT fine-tuned models significantly outperforming feature-based\n(LSTM-CRF) ones on NRB, despite having comparable (sometimes lower) performance\non standard benchmarks.\n\nTo mitigate this bias, we propose a novel model-agnostic training method that\nadds learnable adversarial noise to some entity mentions, thus enforcing models\nto focus more strongly on the contextual signal, leading to significant gains\non NRB. Combining it with two other training strategies, data augmentation and\nparameter freezing, leads to further gains.",
          "link": "http://arxiv.org/abs/2107.11610",
          "publishedOn": "2021-07-27T02:03:31.916Z",
          "wordCount": 599,
          "title": "Context-aware Adversarial Training for Name Regularity Bias in Named Entity Recognition. (arXiv:2107.11610v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>",
          "description": "Recently, researchers have explored using the encoder-decoder framework to\ntackle dialogue state tracking (DST), which is a key component of task-oriented\ndialogue systems. However, they regard a multi-turn dialogue as a flat\nsequence, failing to focus on useful information when the sequence is long. In\nthis paper, we propose a Hierarchical Dynamic Copy Network (HDCN) to facilitate\nfocusing on the most informative turn, making it easier to extract slot values\nfrom the dialogue context. Based on the encoder-decoder framework, we adopt a\nhierarchical copy approach that calculates two levels of attention at the word-\nand turn-level, which are then renormalized to obtain the final copy\ndistribution. A focus loss term is employed to encourage the model to assign\nthe highest turn-level attention weight to the most informative turn.\nExperimental results show that our model achieves 46.76% joint accuracy on the\nMultiWOZ 2.1 dataset.",
          "link": "http://arxiv.org/abs/2107.11778",
          "publishedOn": "2021-07-27T02:03:31.909Z",
          "wordCount": 580,
          "title": "Learn to Focus: Hierarchical Dynamic Copy Network for Dialogue State Tracking. (arXiv:2107.11778v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Chun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shaoming Song</a>",
          "description": "Automatically mining sentiment tendency contained in natural language is a\nfundamental research to some artificial intelligent applications, where\nsolutions alternate with challenges. Transfer learning and multi-task learning\ntechniques have been leveraged to mitigate the supervision sparsity and\ncollaborate multiple heterogeneous domains correspondingly. Recent years, the\nsensitive nature of users' private data raises another challenge for sentiment\nclassification, i.e., data privacy protection. In this paper, we resort to\nfederated learning for multiple domain sentiment classification under the\nconstraint that the corpora must be stored on decentralized devices. In view of\nthe heterogeneous semantics across multiple parties and the peculiarities of\nword embedding, we pertinently provide corresponding solutions. First, we\npropose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for\nbetter model aggregation and personalization in federated sentiment\nclassification. Second, we propose KTEPS$^\\star$ with the consideration of the\nrich semantic and huge embedding size properties of word vectors, utilizing\nProjection-based Dimension Reduction (PDR) methods for privacy protection and\nefficient transmission simultaneously. We propose two federated sentiment\nclassification scenes based on public benchmarks, and verify the superiorities\nof our proposed methods with abundant experimental investigations.",
          "link": "http://arxiv.org/abs/2107.11956",
          "publishedOn": "2021-07-27T02:03:31.902Z",
          "wordCount": 615,
          "title": "Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aspillaga_C/0/1/0/all/0/1\">Carlos Aspillaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_C/0/1/0/all/0/1\">Camilo Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1\">Denis Parra</a>",
          "description": "The success of pretrained word embeddings has motivated their use in the\nbiomedical domain, with contextualized embeddings yielding remarkable results\nin several biomedical NLP tasks. However, there is a lack of research on\nquantifying their behavior under severe \"stress\" scenarios. In this work, we\nsystematically evaluate three language models with adversarial examples --\nautomatically constructed tests that allow us to examine how robust the models\nare. We propose two types of stress scenarios focused on the biomedical named\nentity recognition (NER) task, one inspired by spelling errors and another\nbased on the use of synonyms for medical terms. Our experiments with three\nbenchmarks show that the performance of the original models decreases\nconsiderably, in addition to revealing their weaknesses and strengths. Finally,\nwe show that adversarial training causes the models to improve their robustness\nand even to exceed the original performance in some cases.",
          "link": "http://arxiv.org/abs/2107.11652",
          "publishedOn": "2021-07-27T02:03:31.894Z",
          "wordCount": 586,
          "title": "Stress Test Evaluation of Biomedical Word Embeddings. (arXiv:2107.11652v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenhai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>",
          "description": "We describe an efficient hierarchical method to compute attention in the\nTransformer architecture. The proposed attention mechanism exploits a matrix\nstructure similar to the Hierarchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear run time and memory complexity. We\nperform extensive experiments to show that the inductive bias embodied by our\nhierarchical attention is effective in capturing the hierarchical structure in\nthe sequences typical for natural language and vision tasks. Our method is\nsuperior to alternative sub-quadratic proposals by over +6 points on average on\nthe Long Range Arena benchmark. It also sets a new SOTA test perplexity on\nOne-Billion Word dataset with 5x fewer model parameters than that of the\nprevious-best Transformer-based models.",
          "link": "http://arxiv.org/abs/2107.11906",
          "publishedOn": "2021-07-27T02:03:31.887Z",
          "wordCount": 552,
          "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. (arXiv:2107.11906v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ayush Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kagi_S/0/1/0/all/0/1\">Sammed S Kagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Code-mixing is a phenomenon of mixing words and phrases from two or more\nlanguages in a single utterance of speech and text. Due to the high linguistic\ndiversity, code-mixing presents several challenges in evaluating standard\nnatural language generation (NLG) tasks. Various widely popular metrics perform\npoorly with the code-mixed NLG tasks. To address this challenge, we present a\nmetric independent evaluation pipeline MIPE that significantly improves the\ncorrelation between evaluation metrics and human judgments on the generated\ncode-mixed text. As a use case, we demonstrate the performance of MIPE on the\nmachine-generated Hinglish (code-mixing of Hindi and English languages)\nsentences from the HinGE corpus. We can extend the proposed evaluation strategy\nto other code-mixed language pairs, NLG tasks, and evaluation metrics with\nminimal to no effort.",
          "link": "http://arxiv.org/abs/2107.11534",
          "publishedOn": "2021-07-27T02:03:31.838Z",
          "wordCount": 562,
          "title": "MIPE: A Metric Independent Pipeline for Effective Code-Mixed NLG Evaluation. (arXiv:2107.11534v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1\">Lucas Rafael Stefanel Gris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Anderson da Silva Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>",
          "description": "Deep learning techniques have been shown to be efficient in various tasks,\nespecially in the development of speech recognition systems, that is, systems\nthat aim to transcribe a sentence in audio in a sequence of words. Despite the\nprogress in the area, speech recognition can still be considered difficult,\nespecially for languages lacking available data, as Brazilian Portuguese. In\nthis sense, this work presents the development of an public Automatic Speech\nRecognition system using only open available audio data, from the fine-tuning\nof the Wav2vec 2.0 XLSR-53 model pre-trained in many languages over Brazilian\nPortuguese data. The final model presents a Word Error Rate of 11.95% (Common\nVoice Dataset). This corresponds to 13% less than the best open Automatic\nSpeech Recognition model for Brazilian Portuguese available according to our\nbest knowledge, which is a promising result for the language. In general, this\nwork validates the use of self-supervising learning techniques, in special, the\nuse of the Wav2vec 2.0 architecture in the development of robust systems, even\nfor languages having few available data.",
          "link": "http://arxiv.org/abs/2107.11414",
          "publishedOn": "2021-07-27T02:03:31.830Z",
          "wordCount": 611,
          "title": "Brazilian Portuguese Speech Recognition Using Wav2vec 2.0. (arXiv:2107.11414v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sougata Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Souvik Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1\">Rohini Srihari</a>",
          "description": "Generative neural conversational systems are generally trained with the\nobjective of minimizing the entropy loss between the training \"hard\" targets\nand the predicted logits. Often, performance gains and improved generalization\ncan be achieved by using regularization techniques like label smoothing, which\nconverts the training \"hard\" targets to \"soft\" targets. However, label\nsmoothing enforces a data independent uniform distribution on the incorrect\ntraining targets, which leads to an incorrect assumption of equi-probable\nincorrect targets for each correct target. In this paper we propose and\nexperiment with incorporating data dependent word similarity based weighing\nmethods to transforms the uniform distribution of the incorrect target\nprobabilities in label smoothing, to a more natural distribution based on\nsemantics. We introduce hyperparameters to control the incorrect target\ndistribution, and report significant performance gains over networks trained\nusing standard label smoothing based loss, on two standard open domain dialogue\ncorpora.",
          "link": "http://arxiv.org/abs/2107.11481",
          "publishedOn": "2021-07-27T02:03:31.811Z",
          "wordCount": 575,
          "title": "Similarity Based Label Smoothing For Dialogue Generation. (arXiv:2107.11481v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Troles_J/0/1/0/all/0/1\">Jonas-Dario Troles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>",
          "description": "Human gender bias is reflected in language and text production. Because\nstate-of-the-art machine translation (MT) systems are trained on large corpora\nof text, mostly generated by humans, gender bias can also be found in MT. For\ninstance when occupations are translated from a language like English, which\nmostly uses gender neutral words, to a language like German, which mostly uses\na feminine and a masculine version for an occupation, a decision must be made\nby the MT System. Recent research showed that MT systems are biased towards\nstereotypical translation of occupations. In 2019 the first, and so far only,\nchallenge set, explicitly designed to measure the extent of gender bias in MT\nsystems has been published. In this set measurement of gender bias is solely\nbased on the translation of occupations. In this paper we present an extension\nof this challenge set, called WiBeMT, with gender-biased adjectives and adds\nsentences with gender-biased verbs. The resulting challenge set consists of\nover 70, 000 sentences and has been translated with three commercial MT\nsystems: DeepL Translator, Microsoft Translator, and Google Translate. Results\nshow a gender bias for all three MT systems. This gender bias is to a great\nextent significantly influenced by adjectives and to a lesser extent by verbs.",
          "link": "http://arxiv.org/abs/2107.11584",
          "publishedOn": "2021-07-27T02:03:31.780Z",
          "wordCount": 656,
          "title": "Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives. (arXiv:2107.11584v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sertolli_B/0/1/0/all/0/1\">Benjamin Sertolli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weigel_B/0/1/0/all/0/1\">Benjamin Weigel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "We introduce the MuSe-Toolbox - a Python-based open-source toolkit for\ncreating a variety of continuous and discrete emotion gold standards. In a\nsingle framework, we unify a wide range of fusion methods and propose the novel\nRater Aligned Annotation Weighting (RAAW), which aligns the annotations in a\ntranslation-invariant way before weighting and fusing them based on the\ninter-rater agreements between the annotations. Furthermore, discrete\ncategories tend to be easier for humans to interpret than continuous signals.\nWith this in mind, the MuSe-Toolbox provides the functionality to run\nexhaustive searches for meaningful class clusters in the continuous gold\nstandards. To our knowledge, this is the first toolkit that provides a wide\nselection of state-of-the-art emotional gold standard methods and their\ntransformation to discrete classes. Experimental results indicate that\nMuSe-Toolbox can provide promising and novel class formations which can be\nbetter predicted than hard-coded classes boundaries with minimal human\nintervention. The implementation (1) is out-of-the-box available with all\ndependencies using a Docker container (2).",
          "link": "http://arxiv.org/abs/2107.11757",
          "publishedOn": "2021-07-27T02:03:31.704Z",
          "wordCount": 620,
          "title": "MuSe-Toolbox: The Multimodal Sentiment Analysis Continuous Annotation Fusion and Discrete Class Transformation Toolbox. (arXiv:2107.11757v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>",
          "description": "Spoken Language Understanding (SLU) is composed of two subtasks: intent\ndetection (ID) and slot filling (SF). There are two lines of research on SLU.\nOne jointly tackles these two subtasks to improve their prediction accuracy,\nand the other focuses on the domain-adaptation ability of one of the subtasks.\nIn this paper, we attempt to bridge these two lines of research and propose a\njoint and domain adaptive approach to SLU. We formulate SLU as a constrained\ngeneration task and utilize a dynamic vocabulary based on domain-specific\nontology. We conduct experiments on the ASMixed and MTOD datasets and achieve\ncompetitive performance with previous state-of-the-art joint models. Besides,\nresults show that our joint model can be effectively adapted to a new domain.",
          "link": "http://arxiv.org/abs/2107.11768",
          "publishedOn": "2021-07-27T02:03:31.666Z",
          "wordCount": 561,
          "title": "A Joint and Domain-Adaptive Approach to Spoken Language Understanding. (arXiv:2107.11768v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R. Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "Building language-universal speech recognition systems entails producing\nphonological units of spoken sound that can be shared across languages. While\nspeech annotations at the language-specific phoneme or surface levels are\nreadily available, annotations at a universal phone level are relatively rare\nand difficult to produce. In this work, we present a general framework to\nderive phone-level supervision from only phonemic transcriptions and\nphone-to-phoneme mappings with learnable weights represented using weighted\nfinite-state transducers, which we call differentiable allophone graphs. By\ntraining multilingually, we build a universal phone-based speech recognition\nmodel with interpretable probabilistic phone-to-phoneme mappings for each\nlanguage. These phone-based systems with learned allophone graphs can be used\nby linguists to document new languages, build phone-based lexicons that capture\nrich pronunciation variations, and re-evaluate the allophone mappings of seen\nlanguage. We demonstrate the aforementioned benefits of our proposed framework\nwith a system trained on 7 diverse languages.",
          "link": "http://arxiv.org/abs/2107.11628",
          "publishedOn": "2021-07-27T02:03:31.588Z",
          "wordCount": 600,
          "title": "Differentiable Allophone Graphs for Language-Universal Speech Recognition. (arXiv:2107.11628v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolanos_L/0/1/0/all/0/1\">Luis Bolanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_A/0/1/0/all/0/1\">Ashwani Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokol_A/0/1/0/all/0/1\">Albert Sokol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vibhor Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>",
          "description": "Clinical notes contain information not present elsewhere, including drug\nresponse and symptoms, all of which are highly important when predicting key\noutcomes in acute care patients. We propose the automatic annotation of\nphenotypes from clinical notes as a method to capture essential information to\npredict outcomes in the Intensive Care Unit (ICU). This information is\ncomplementary to typically used vital signs and laboratory test results. We\ndemonstrate and validate our approach conducting experiments on the prediction\nof in-hospital mortality, physiological decompensation and length of stay in\nthe ICU setting for over 24,000 patients. The prediction models incorporating\nphenotypic information consistently outperform the baseline models leveraging\nonly vital signs and laboratory test results. Moreover, we conduct a thorough\ninterpretability study, showing that phenotypes provide valuable insights at\nthe patient and cohort levels. Our approach illustrates the viability of using\nphenotypes to determine outcomes in the ICU.",
          "link": "http://arxiv.org/abs/2107.11665",
          "publishedOn": "2021-07-27T02:03:31.498Z",
          "wordCount": 598,
          "title": "Clinical Utility of the Automatic Phenotype Annotation in Unstructured Clinical Notes: ICU Use Cases. (arXiv:2107.11665v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "This paper describes the University of Sydney& JD's joint submission of the\nIWSLT 2021 low resource speech translation task. We participated in the\nSwahili-English direction and got the best scareBLEU (25.3) score among all the\nparticipants. Our constrained system is based on a pipeline framework, i.e. ASR\nand NMT. We trained our models with the officially provided ASR and MT\ndatasets. The ASR system is based on the open-sourced tool Kaldi and this work\nmainly explores how to make the most of the NMT models. To reduce the\npunctuation errors generated by the ASR model, we employ our previous work\nSlotRefine to train a punctuation correction model. To achieve better\ntranslation performance, we explored the most recent effective strategies,\nincluding back translation, knowledge distillation, multi-feature reranking and\ntransductive finetuning. For model structure, we tried auto-regressive and\nnon-autoregressive models, respectively. In addition, we proposed two novel\npre-train approaches, i.e. \\textit{de-noising training} and\n\\textit{bidirectional training} to fully exploit the data. Extensive\nexperiments show that adding the above techniques consistently improves the\nBLEU scores, and the final submission system outperforms the baseline\n(Transformer ensemble model trained with the original parallel data) by\napproximately 10.8 BLEU score, achieving the SOTA performance.",
          "link": "http://arxiv.org/abs/2107.11572",
          "publishedOn": "2021-07-27T02:03:31.422Z",
          "wordCount": 644,
          "title": "The USYD-JD Speech Translation System for IWSLT 2021. (arXiv:2107.11572v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Harbi_O/0/1/0/all/0/1\">Omar Al-Harbi</a>",
          "description": "One crucial aspect of sentiment analysis is negation handling, where the\noccurrence of negation can flip the sentiment of a sentence and negatively\naffects the machine learning-based sentiment classification. The role of\nnegation in Arabic sentiment analysis has been explored only to a limited\nextent, especially for colloquial Arabic. In this paper, the author addresses\nthe negation problem of machine learning-based sentiment classification for a\ncolloquial Arabic language. To this end, we propose a simple rule-based\nalgorithm for handling the problem; the rules were crafted based on observing\nmany cases of negation. Additionally, simple linguistic knowledge and sentiment\nlexicon are used for this purpose. The author also examines the impact of the\nproposed algorithm on the performance of different machine learning algorithms.\nThe results given by the proposed algorithm are compared with three baseline\nmodels. The experimental results show that there is a positive impact on the\nclassifiers accuracy, precision and recall when the proposed algorithm is used\ncompared to the baselines.",
          "link": "http://arxiv.org/abs/2107.11597",
          "publishedOn": "2021-07-27T02:03:31.284Z",
          "wordCount": 608,
          "title": "Negation Handling in Machine Learning-Based Sentiment Classification for Colloquial Arabic. (arXiv:2107.11597v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.01678",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1\">Lifa Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "Though significant progress has been made for the voice conversion (VC) of\ntypical speech, VC for atypical speech, e.g., dysarthric and second-language\n(L2) speech, remains a challenge, since it involves correcting for atypical\nprosody while maintaining speaker identity. To address this issue, we propose a\nVC system with explicit prosodic modelling and deep speaker embedding (DSE)\nlearning. First, a speech-encoder strives to extract robust phoneme embeddings\nfrom atypical speech. Second, a prosody corrector takes in phoneme embeddings\nto infer typical phoneme duration and pitch values. Third, a conversion model\ntakes phoneme embeddings and typical prosody features as inputs to generate the\nconverted speech, conditioned on the target DSE that is learned via speaker\nencoder or speaker adaptation. Extensive experiments demonstrate that speaker\nadaptation can achieve higher speaker similarity, and the speaker encoder based\nconversion model can greatly reduce dysarthric and non-native pronunciation\npatterns with improved speech intelligibility. A comparison of speech\nrecognition results between the original dysarthric speech and converted speech\nshow that absolute reduction of 47.6% character error rate (CER) and 29.3% word\nerror rate (WER) can be achieved.",
          "link": "http://arxiv.org/abs/2011.01678",
          "publishedOn": "2021-07-26T02:00:57.757Z",
          "wordCount": 660,
          "title": "Learning Explicit Prosody Models and Deep Speaker Embeddings for Atypical Voice Conversion. (arXiv:2011.01678v2 [eess.AS] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1\">Ashkan Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1\">Kiran Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1\">Devin Gaffney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>",
          "description": "There is currently no easy way to fact-check content on WhatsApp and other\nend-to-end encrypted platforms at scale. In this paper, we analyze the\nusefulness of a crowd-sourced \"tipline\" through which users can submit content\n(\"tips\") that they want fact-checked. We compare the tips sent to a WhatsApp\ntipline run during the 2019 Indian national elections with the messages\ncirculating in large, public groups on WhatsApp and other social media\nplatforms during the same period. We find that tiplines are a very useful lens\ninto WhatsApp conversations: a significant fraction of messages and images sent\nto the tipline match with the content being shared on public WhatsApp groups\nand other social media. Our analysis also shows that tiplines cover the most\npopular content well, and a majority of such content is often shared to the\ntipline before appearing in large, public WhatsApp groups. Overall, our\nfindings suggest tiplines can be an effective source for discovering content to\nfact-check.",
          "link": "http://arxiv.org/abs/2106.04726",
          "publishedOn": "2021-07-26T02:00:57.689Z",
          "wordCount": 655,
          "title": "Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-07-26T02:00:57.678Z",
          "wordCount": 633,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenxuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1\">Yiming Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qingyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Liming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>",
          "description": "This paper introduces the sixth Oriental Language Recognition (OLR) 2021\nChallenge, which intends to improve the performance of language recognition\nsystems and speech recognition systems within multilingual scenarios. The data\nprofile, four tasks, two baselines, and the evaluation principles are\nintroduced in this paper. In addition to the Language Identification (LID)\ntasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to\nOLR 2021 Challenge for the first time. The challenge this year focuses on more\npractical and challenging problems, with four tasks: (1) constrained LID, (2)\nunconstrained LID, (3) constrained multilingual ASR, (4) unconstrained\nmultilingual ASR. Baselines for LID tasks and multilingual ASR tasks are\nprovided, respectively. The LID baseline system is an extended TDNN x-vector\nmodel constructed with Pytorch. A transformer-based end-to-end model is\nprovided as the multilingual ASR baseline system. These recipes will be online\npublished, and available for participants to construct their own LID or ASR\nsystems. The baseline results demonstrate that those tasks are rather\nchallenging and deserve more effort to achieve better performance.",
          "link": "http://arxiv.org/abs/2107.11113",
          "publishedOn": "2021-07-26T02:00:57.631Z",
          "wordCount": 636,
          "title": "OLR 2021 Challenge: Datasets, Rules and Baselines. (arXiv:2107.11113v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">A. Stevie Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spruit_S/0/1/0/all/0/1\">Shannon Spruit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>",
          "description": "Over the last several years, end-to-end neural conversational agents have\nvastly improved in their ability to carry a chit-chat conversation with humans.\nHowever, these models are often trained on large datasets from the internet,\nand as a result, may learn undesirable behaviors from this data, such as toxic\nor otherwise harmful language. Researchers must thus wrestle with the issue of\nhow and when to release these models. In this paper, we survey the problem\nlandscape for safety for end-to-end conversational AI and discuss recent and\nrelated work. We highlight tensions between values, potential positive impact\nand potential harms, and provide a framework for making decisions about whether\nand how to release these models, following the tenets of value-sensitive\ndesign. We additionally provide a suite of tools to enable researchers to make\nbetter-informed decisions about training and releasing end-to-end\nconversational AI models.",
          "link": "http://arxiv.org/abs/2107.03451",
          "publishedOn": "2021-07-26T02:00:57.593Z",
          "wordCount": 614,
          "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling. (arXiv:2107.03451v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>",
          "description": "A comparison of formulaic sequences in human and neural machine translation\nof quality newspaper articles shows that neural machine translations contain\nless lower-frequency, but strongly-associated formulaic sequences, and more\nhigh-frequency formulaic sequences. These differences were statistically\nsignificant and the effect sizes were almost always medium or large. These\nobservations can be related to the differences between second language learners\nof various levels and between translated and untranslated texts. The comparison\nbetween the neural machine translation systems indicates that some systems\nproduce more formulaic sequences of both types than other systems.",
          "link": "http://arxiv.org/abs/2107.03625",
          "publishedOn": "2021-07-26T02:00:57.577Z",
          "wordCount": 554,
          "title": "Using CollGram to Compare Formulaic Language in Human and Neural Machine Translation. (arXiv:2107.03625v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10127",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "Dysarthric speech detection (DSD) systems aim to detect characteristics of\nthe neuromotor disorder from speech. Such systems are particularly susceptible\nto domain mismatch where the training and testing data come from the source and\ntarget domains respectively, but the two domains may differ in terms of speech\nstimuli, disease etiology, etc. It is hard to acquire labelled data in the\ntarget domain, due to high costs of annotating sizeable datasets. This paper\nmakes a first attempt to formulate cross-domain DSD as an unsupervised domain\nadaptation (UDA) problem. We use labelled source-domain data and unlabelled\ntarget-domain data, and propose a multi-task learning strategy, including\ndysarthria presence classification (DPC), domain adversarial training (DAT) and\nmutual information minimization (MIM), which aim to learn\ndysarthria-discriminative and domain-invariant biomarker embeddings.\nSpecifically, DPC helps biomarker embeddings capture critical indicators of\ndysarthria; DAT forces biomarker embeddings to be indistinguishable in source\nand target domains; and MIM further reduces the correlation between biomarker\nembeddings and domain-related cues. By treating the UASPEECH and TORGO corpora\nrespectively as the source and target domains, experiments show that the\nincorporation of UDA attains absolute increases of 22.2% and 20.0% respectively\nin utterance-level weighted average recall and speaker-level accuracy.",
          "link": "http://arxiv.org/abs/2106.10127",
          "publishedOn": "2021-07-26T02:00:57.564Z",
          "wordCount": 670,
          "title": "Unsupervised Domain Adaptation for Dysarthric Speech Detection via Domain Adversarial Training and Mutual Information Minimization. (arXiv:2106.10127v1 [eess.AS] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fursov_I/0/1/0/all/0/1\">Ivan Fursov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1\">Alexey Zaytsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnyshev_P/0/1/0/all/0/1\">Pavel Burnyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dmitrieva_E/0/1/0/all/0/1\">Ekaterina Dmitrieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klyuchnikov_N/0/1/0/all/0/1\">Nikita Klyuchnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravchenko_A/0/1/0/all/0/1\">Andrey Kravchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "Robustness of huge Transformer-based models for natural language processing\nis an important issue due to their capabilities and wide adoption. One way to\nunderstand and improve robustness of these models is an exploration of an\nadversarial attack scenario: check if a small perturbation of an input can fool\na model.\n\nDue to the discrete nature of textual data, gradient-based adversarial\nmethods, widely used in computer vision, are not applicable per~se. The\nstandard strategy to overcome this issue is to develop token-level\ntransformations, which do not take the whole sentence into account.\n\nIn this paper, we propose a new black-box sentence-level attack. Our method\nfine-tunes a pre-trained language model to generate adversarial examples. A\nproposed differentiable loss function depends on a substitute classifier score\nand an approximate edit distance computed via a deep learning model.\n\nWe show that the proposed attack outperforms competitors on a diverse set of\nNLP problems for both computed metrics and human evaluation. Moreover, due to\nthe usage of the fine-tuned language model, the generated adversarial examples\nare hard to detect, thus current models are not robust. Hence, it is difficult\nto defend from the proposed attack, which is not the case for other attacks.",
          "link": "http://arxiv.org/abs/2107.11275",
          "publishedOn": "2021-07-26T02:00:57.518Z",
          "wordCount": 652,
          "title": "A Differentiable Language Model Adversarial Attack on Text Classifiers. (arXiv:2107.11275v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robert_C/0/1/0/all/0/1\">Christian-Yann Robert</a>",
          "description": "Predicting stock prices from textual information is a challenging task due to\nthe uncertainty of the market and the difficulty understanding the natural\nlanguage from a machine's perspective. Previous researches focus mostly on\nsentiment extraction based on single news. However, the stocks on the financial\nmarket can be highly correlated, one news regarding one stock can quickly\nimpact the prices of other stocks. To take this effect into account, we propose\na new stock movement prediction framework: Multi-Graph Recurrent Network for\nStock Forecasting (MGRN). This architecture allows to combine the textual\nsentiment from financial news and multiple relational information extracted\nfrom other financial data. Through an accuracy test and a trading simulation on\nthe stocks in the STOXX Europe 600 index, we demonstrate a better performance\nfrom our model than other benchmarks.",
          "link": "http://arxiv.org/abs/2107.10941",
          "publishedOn": "2021-07-26T02:00:57.434Z",
          "wordCount": 584,
          "title": "Graph-Based Learning for Stock Movement Prediction with Textual and Relational Data. (arXiv:2107.10941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>",
          "description": "Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy loss, which encourages an exact\ntoken-by-token match between a target sequence with a generated sequence. Such\ntraining objective is sub-optimal when the target sequence not perfect, e.g.,\nwhen the target sequence is corrupted with noises, or when only weak sequence\nsupervision is available. To address this challenge, we propose a novel\nEdit-Invariant Sequence Loss (EISL), which computes the matching loss of a\ntarget n-gram with all n-grams in the generated sequence. EISL draws\ninspirations from convolutional networks (ConvNets) which are shift-invariant\nto images, hence is robust to the shift of n-grams to tolerate edits in the\ntarget sequences. Moreover, the computation of EISL is essentially a\nconvolution operation with target n-grams as kernels, which is easy to\nimplement with existing libraries. To demonstrate the effectiveness of EISL, we\nconduct experiments on three tasks: machine translation with noisy target\nsequences, unsupervised text style transfer, and non-autoregressive machine\ntranslation. Experimental results show our method significantly outperforms\ncross entropy loss on these three tasks.",
          "link": "http://arxiv.org/abs/2106.15078",
          "publishedOn": "2021-07-26T02:00:57.419Z",
          "wordCount": 648,
          "title": "Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Multilingual neural machine translation aims at learning a single translation\nmodel for multiple languages. These jointly trained models often suffer from\nperformance degradation on rich-resource language pairs. We attribute this\ndegeneration to parameter interference. In this paper, we propose LaSS to\njointly train a single unified multilingual MT model. LaSS learns Language\nSpecific Sub-network (LaSS) for each language pair to counter parameter\ninterference. Comprehensive experiments on IWSLT and WMT datasets with various\nTransformer architectures show that LaSS obtains gains on 36 language pairs by\nup to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at\neasy extension to new language pairs and zero-shot translation.LaSS boosts\nzero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes\nand trained models are available at https://github.com/NLP-Playground/LaSS.",
          "link": "http://arxiv.org/abs/2105.09259",
          "publishedOn": "2021-07-26T02:00:57.409Z",
          "wordCount": 591,
          "title": "Learning Language Specific Sub-network for Multilingual Machine Translation. (arXiv:2105.09259v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05852",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Adiga_D/0/1/0/all/0/1\">Devaraja Adiga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_R/0/1/0/all/0/1\">Rishabh Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishna_A/0/1/0/all/0/1\">Amrith Krishna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>",
          "description": "Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the\nvarious linguistic peculiarities present in the language. The Sanskrit language\nis lexically productive, undergoes euphonic assimilation of phones at the word\nboundaries and exhibits variations in spelling conventions and in\npronunciations. In this work, we propose the first large scale study of\nautomatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact\nof unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR\ndataset for Sanskrit, which faithfully captures several of the linguistic\ncharacteristics expressed by the language. We investigate the role of different\nacoustic model and language model units in ASR systems for Sanskrit. We also\npropose a new modelling unit, inspired by the syllable level unit selection,\nthat captures character sequences from one vowel in the word to the next vowel.\nWe also highlight the importance of choosing graphemic representations for\nSanskrit and show the impact of this choice on word error rates (WER). Finally,\nwe extend these insights from Sanskrit ASR for building ASR systems in two\nother Indic languages, Gujarati and Telugu. For both these languages, our\nexperimental results show that the use of phonetic based graphemic\nrepresentations in ASR results in performance improvements as compared to ASR\nsystems that use native scripts.",
          "link": "http://arxiv.org/abs/2106.05852",
          "publishedOn": "2021-07-26T02:00:57.399Z",
          "wordCount": 708,
          "title": "Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights. (arXiv:2106.05852v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Jiho Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>",
          "description": "Biomedical word embeddings are usually pre-trained on free text corpora with\nneural methods that capture local and global distributional properties. They\nare leveraged in downstream tasks using various neural architectures that are\ndesigned to optimize task-specific objectives that might further tune such\nembeddings. Since 2018, however, there is a marked shift from these static\nembeddings to contextual embeddings motivated by language models (e.g., ELMo,\ntransformers such as BERT, and ULMFiT). These dynamic embeddings have the added\nbenefit of being able to distinguish homonyms and acronyms given their context.\nHowever, static embeddings are still relevant in low resource settings (e.g.,\nsmart devices, IoT elements) and to study lexical semantics from a\ncomputational linguistics perspective. In this paper, we jointly learn word and\nconcept embeddings by first using the skip-gram method and further fine-tuning\nthem with correlational information manifesting in co-occurring Medical Subject\nHeading (MeSH) concepts in biomedical citations. This fine-tuning is\naccomplished with the BERT transformer architecture in the two-sentence input\nmode with a classification objective that captures MeSH pair co-occurrence. In\nessence, we repurpose a transformer architecture (typically used to generate\ndynamic embeddings) to improve static embeddings using concept correlations. We\nconduct evaluations of these tuned static embeddings using multiple datasets\nfor word relatedness developed by previous efforts. Without selectively culling\nconcepts and terms (as was pursued by previous efforts), we believe we offer\nthe most exhaustive evaluation of static embeddings to date with clear\nperformance improvements across the board. We provide our code and embeddings\nfor public use for downstream applications and research endeavors:\nhttps://github.com/bionlproc/BERT-CRel-Embeddings",
          "link": "http://arxiv.org/abs/2012.11808",
          "publishedOn": "2021-07-26T02:00:57.372Z",
          "wordCount": 736,
          "title": "Improved Biomedical Word Embeddings in the Transformer Era. (arXiv:2012.11808v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_K/0/1/0/all/0/1\">Kameron B. Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khushu_S/0/1/0/all/0/1\">Shweta Khushu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_M/0/1/0/all/0/1\">Mukut Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banister_A/0/1/0/all/0/1\">Andrew Banister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hevia_A/0/1/0/all/0/1\">Anthony Hevia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duddu_S/0/1/0/all/0/1\">Sampath Duddu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>",
          "description": "While many accept climate change and its growing impacts, few converse about\nit well, limiting the adoption speed of societal changes necessary to address\nit. In order to make effective climate communication easier, we aim to build a\nsystem that presents to any individual the climate information predicted to\nbest motivate and inspire them to take action given their unique set of\npersonal values. To alleviate the cold-start problem, the system relies on a\nknowledge base (ClimateKB) of causes and effects of climate change, and their\nassociations to personal values. Since no such comprehensive ClimateKB exists,\nwe revisit knowledge base construction techniques and build a ClimateKB from\nfree text. We plan to open source the ClimateKB and associated code to\nencourage future research and applications.",
          "link": "http://arxiv.org/abs/2107.11351",
          "publishedOn": "2021-07-26T02:00:57.361Z",
          "wordCount": 565,
          "title": "Powering Effective Climate Communication with a Climate Knowledge Base. (arXiv:2107.11351v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_P/0/1/0/all/0/1\">Po-Han Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yist Y. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tzu-Hsien Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Ko-tik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Da-Rong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zili Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Self-supervised learning (SSL) has proven vital for advancing research in\nnatural language processing (NLP) and computer vision (CV). The paradigm\npretrains a shared model on large volumes of unlabeled data and achieves\nstate-of-the-art (SOTA) for various tasks with minimal adaptation. However, the\nspeech processing community lacks a similar setup to systematically explore the\nparadigm. To bridge this gap, we introduce Speech processing Universal\nPERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the\nperformance of a shared model across a wide range of speech processing tasks\nwith minimal architecture changes and labeled data. Among multiple usages of\nthe shared model, we especially focus on extracting the representation learned\nfrom SSL due to its preferable re-usability. We present a simple framework to\nsolve SUPERB tasks by learning task-specialized lightweight prediction heads on\ntop of the frozen shared model. Our results demonstrate that the framework is\npromising as SSL representations show competitive generalizability and\naccessibility across SUPERB tasks. We release SUPERB as a challenge with a\nleaderboard and a benchmark toolkit to fuel the research in representation\nlearning and general speech processing.",
          "link": "http://arxiv.org/abs/2105.01051",
          "publishedOn": "2021-07-26T02:00:57.351Z",
          "wordCount": 700,
          "title": "SUPERB: Speech processing Universal PERformance Benchmark. (arXiv:2105.01051v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pedinotti_P/0/1/0/all/0/1\">Paolo Pedinotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambelli_G/0/1/0/all/0/1\">Giulia Rambelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blache_P/0/1/0/all/0/1\">Philippe Blache</a>",
          "description": "Prior research has explored the ability of computational models to predict a\nword semantic fit with a given predicate. While much work has been devoted to\nmodeling the typicality relation between verbs and arguments in isolation, in\nthis paper we take a broader perspective by assessing whether and to what\nextent computational approaches have access to the information about the\ntypicality of entire events and situations described in language (Generalized\nEvent Knowledge). Given the recent success of Transformers Language Models\n(TLMs), we decided to test them on a benchmark for the \\textit{dynamic\nestimation of thematic fit}. The evaluation of these models was performed in\ncomparison with SDM, a framework specifically designed to integrate events in\nsentence meaning representations, and we conducted a detailed error analysis to\ninvestigate which factors affect their behavior. Our results show that TLMs can\nreach performances that are comparable to those achieved by SDM. However,\nadditional analysis consistently suggests that TLMs do not capture important\naspects of event knowledge, and their predictions often depend on surface\nlinguistic features, such as frequent words, collocations and syntactic\npatterns, thereby showing sub-optimal generalization abilities.",
          "link": "http://arxiv.org/abs/2107.10922",
          "publishedOn": "2021-07-26T02:00:57.339Z",
          "wordCount": 628,
          "title": "Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge. (arXiv:2107.10922v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1\">Fred Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_V/0/1/0/all/0/1\">Vivek Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratan_U/0/1/0/all/0/1\">Ujjwal Ratan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1\">Zohar Karnin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_V/0/1/0/all/0/1\">Vishaal Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kass_Hout_T/0/1/0/all/0/1\">Taha Kass-Hout</a>",
          "description": "Sepsis is a life-threatening disease with high morbidity, mortality and\nhealthcare costs. The early prediction and administration of antibiotics and\nintravenous fluids is considered crucial for the treatment of sepsis and can\nsave potentially millions of lives and billions in health care costs.\nProfessional clinical care practitioners have proposed clinical criterion which\naid in early detection of sepsis; however, performance of these criterion is\noften limited. Clinical text provides essential information to estimate the\nseverity of the sepsis in addition to structured clinical data. In this study,\nwe explore how clinical text can complement structured data towards early\nsepsis prediction task. In this paper, we propose multi modal model which\nincorporates both structured data in the form of patient measurements as well\nas textual notes on the patient. We employ state-of-the-art NLP models such as\nBERT and a highly specialized NLP model in Amazon Comprehend Medical to\nrepresent the text. On the MIMIC-III dataset containing records of ICU\nadmissions, we show that by using these notes, one achieves an improvement of\n6.07 points in a standard utility score for Sepsis prediction and 2.89% in\nAUROC score. Our methods significantly outperforms a clinical criteria\nsuggested by experts, qSOFA, as well as the winning model of the PhysioNet\nComputing in Cardiology Challenge for predicting Sepsis.",
          "link": "http://arxiv.org/abs/2107.11094",
          "publishedOn": "2021-07-26T02:00:57.328Z",
          "wordCount": 650,
          "title": "Improving Early Sepsis Prediction with Multi Modal Learning. (arXiv:2107.11094v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weld_H/0/1/0/all/0/1\">Henry Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guanghao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jean Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tongshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kunze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xinghong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>",
          "description": "Traditional toxicity detection models have focused on the single utterance\nlevel without deeper understanding of context. We introduce CONDA, a new\ndataset for in-game toxic language detection enabling joint intent\nclassification and slot filling analysis, which is the core task of Natural\nLanguage Understanding (NLU). The dataset consists of 45K utterances from 12K\nconversations from the chat logs of 1.9K completed Dota 2 matches. We propose a\nrobust dual semantic-level toxicity framework, which handles utterance and\ntoken-level patterns, and rich contextual chatting history. Accompanying the\ndataset is a thorough in-game toxicity analysis, which provides comprehensive\nunderstanding of context at utterance, token, and dual levels. Inspired by NLU,\nwe also apply its metrics to the toxicity detection tasks for assessing\ntoxicity and game-specific aspects. We evaluate strong NLU models on CONDA,\nproviding fine-grained results for different intent classes and slot classes.\nFurthermore, we examine the coverage of toxicity nature in our dataset by\ncomparing it with other toxicity datasets.",
          "link": "http://arxiv.org/abs/2106.06213",
          "publishedOn": "2021-07-26T02:00:57.313Z",
          "wordCount": 642,
          "title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection. (arXiv:2106.06213v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>",
          "description": "While achieving state-of-the-art results in multiple tasks and languages,\ntranslation-based cross-lingual transfer is often overlooked in favour of\nmassively multilingual pre-trained encoders. Arguably, this is due to its main\nlimitations: 1) translation errors percolating to the classification phase and\n2) the insufficient expressiveness of the maximum-likelihood translation. To\nremedy this, we propose a new technique that integrates both steps of the\ntraditional pipeline (translation and classification) into a single model, by\ntreating the intermediate translations as a latent random variable. As a\nresult, 1) the neural machine translation system can be fine-tuned with a\nvariant of Minimum Risk Training where the reward is the accuracy of the\ndownstream task classifier. Moreover, 2) multiple samples can be drawn to\napproximate the expected loss across all possible translations during\ninference. We evaluate our novel latent translation-based model on a series of\nmultilingual NLU tasks, including commonsense reasoning, paraphrase\nidentification, and natural language inference. We report gains for both\nzero-shot and few-shot learning setups, up to 2.7 accuracy points on average,\nwhich are even more prominent for low-resource languages (e.g., Haitian\nCreole). Finally, we carry out in-depth analyses comparing different underlying\nNMT models and assessing the impact of alternative translations on the\ndownstream performance.",
          "link": "http://arxiv.org/abs/2107.11353",
          "publishedOn": "2021-07-26T02:00:57.286Z",
          "wordCount": 631,
          "title": "Modelling Latent Translations for Cross-Lingual Transfer. (arXiv:2107.11353v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tekle_A/0/1/0/all/0/1\">Alexander Tekle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1\">Chau Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>",
          "description": "Crises such as natural disasters, global pandemics, and social unrest\ncontinuously threaten our world and emotionally affect millions of people\nworldwide in distinct ways. Understanding emotions that people express during\nlarge-scale crises helps inform policy makers and first responders about the\nemotional states of the population as well as provide emotional support to\nthose who need such support. We present CovidEmo, ~1K tweets labeled with\nemotions. We examine how well large pre-trained language models generalize\nacross domains and crises in the task of perceived emotion prediction in the\ncontext of COVID-19. Our results show that existing models do not directly\ntransfer from one disaster type to another but using labeled emotional corpora\nfor domain adaptation is beneficial.",
          "link": "http://arxiv.org/abs/2107.11020",
          "publishedOn": "2021-07-26T02:00:57.272Z",
          "wordCount": 604,
          "title": "When a crisis strikes: Emotion analysis and detection during COVID-19. (arXiv:2107.11020v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2002.08608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jisun An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_E/0/1/0/all/0/1\">Elise Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1\">Yong-Yeol Ahn</a>",
          "description": "Framing is a process of emphasizing a certain aspect of an issue over the\nothers, nudging readers or listeners towards different positions on the issue\neven without making a biased argument. {Here, we propose FrameAxis, a method\nfor characterizing documents by identifying the most relevant semantic axes\n(\"microframes\") that are overrepresented in the text using word embedding. Our\nunsupervised approach can be readily applied to large datasets because it does\nnot require manual annotations. It can also provide nuanced insights by\nconsidering a rich set of semantic axes. FrameAxis is designed to\nquantitatively tease out two important dimensions of how microframes are used\nin the text. \\textit{Microframe bias} captures how biased the text is on a\ncertain microframe, and \\textit{microframe intensity} shows how actively a\ncertain microframe is used. Together, they offer a detailed characterization of\nthe text. We demonstrate that microframes with the highest bias and intensity\nwell align with sentiment, topic, and partisan spectrum by applying FrameAxis\nto multiple datasets from restaurant reviews to political news.} The existing\ndomain knowledge can be incorporated into FrameAxis {by using custom\nmicroframes and by using FrameAxis as an iterative exploratory analysis\ninstrument.} Additionally, we propose methods for explaining the results of\nFrameAxis at the level of individual words and documents. Our method may\naccelerate scalable and sophisticated computational analyses of framing across\ndisciplines.",
          "link": "http://arxiv.org/abs/2002.08608",
          "publishedOn": "2021-07-26T02:00:57.260Z",
          "wordCount": 726,
          "title": "FrameAxis: Characterizing Microframe Bias and Intensity with Word Embedding. (arXiv:2002.08608v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Neural chat translation aims to translate bilingual conversational text,\nwhich has a broad application in international exchanges and cooperation.\nDespite the impressive performance of sentence-level and context-aware Neural\nMachine Translation (NMT), there still remain challenges to translate bilingual\nconversational text due to its inherent characteristics such as role\npreference, dialogue coherence, and translation consistency. In this paper, we\naim to promote the translation quality of conversational text by modeling the\nabove properties. Specifically, we design three latent variational modules to\nlearn the distributions of bilingual conversational characteristics. Through\nsampling from these learned distributions, the latent variables, tailored for\nrole preference, dialogue coherence, and translation consistency, are\nincorporated into the NMT model for better translation. We evaluate our\napproach on the benchmark dataset BConTrasT (English-German) and a\nself-collected bilingual dialogue corpus, named BMELD (English-Chinese).\nExtensive experiments show that our approach notably boosts the performance\nover strong baselines by a large margin and significantly surpasses some\nstate-of-the-art context-aware NMT models in terms of BLEU and TER.\nAdditionally, we make the BMELD dataset publicly available for the research\ncommunity.",
          "link": "http://arxiv.org/abs/2107.11164",
          "publishedOn": "2021-07-26T02:00:57.236Z",
          "wordCount": 628,
          "title": "Modeling Bilingual Conversational Characteristics for Neural Chat Translation. (arXiv:2107.11164v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bingqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yanxin Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Language instruction plays an essential role in the natural language grounded\nnavigation tasks. However, navigators trained with limited human-annotated\ninstructions may have difficulties in accurately capturing key information from\nthe complicated instruction at different timesteps, leading to poor navigation\nperformance. In this paper, we exploit to train a more robust navigator which\nis capable of dynamically extracting crucial factors from the long instruction,\nby using an adversarial attacking paradigm. Specifically, we propose a Dynamic\nReinforced Instruction Attacker (DR-Attacker), which learns to mislead the\nnavigator to move to the wrong target by destroying the most instructive\ninformation in instructions at different timesteps. By formulating the\nperturbation generation as a Markov Decision Process, DR-Attacker is optimized\nby the reinforcement learning algorithm to generate perturbed instructions\nsequentially during the navigation, according to a learnable attack score.\nThen, the perturbed instructions, which serve as hard samples, are used for\nimproving the robustness of the navigator with an effective adversarial\ntraining strategy and an auxiliary self-supervised reasoning task. Experimental\nresults on both Vision-and-Language Navigation (VLN) and Navigation from Dialog\nHistory (NDH) tasks show the superiority of our proposed method over\nstate-of-the-art methods. Moreover, the visualization analysis shows the\neffectiveness of the proposed DR-Attacker, which can successfully attack\ncrucial information in the instructions at different timesteps. Code is\navailable at https://github.com/expectorlin/DR-Attacker.",
          "link": "http://arxiv.org/abs/2107.11252",
          "publishedOn": "2021-07-26T02:00:57.212Z",
          "wordCount": 684,
          "title": "Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation. (arXiv:2107.11252v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochgesang_J/0/1/0/all/0/1\">Julie Hochgesang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>",
          "description": "Signed languages are the primary means of communication for many deaf and\nhard of hearing individuals. Since signed languages exhibit all the fundamental\nlinguistic properties of natural language, we believe that tools and theories\nof Natural Language Processing (NLP) are crucial towards its modeling. However,\nexisting research in Sign Language Processing (SLP) seldom attempt to explore\nand leverage the linguistic organization of signed languages. This position\npaper calls on the NLP community to include signed languages as a research area\nwith high social and scientific impact. We first discuss the linguistic\nproperties of signed languages to consider during their modeling. Then, we\nreview the limitations of current SLP models and identify the open challenges\nto extend NLP to signed languages. Finally, we urge (1) the adoption of an\nefficient tokenization method; (2) the development of linguistically-informed\nmodels; (3) the collection of real-world signed language data; (4) the\ninclusion of local signed language communities as an active and leading voice\nin the direction of research.",
          "link": "http://arxiv.org/abs/2105.05222",
          "publishedOn": "2021-07-26T02:00:57.182Z",
          "wordCount": 638,
          "title": "Including Signed Languages in Natural Language Processing. (arXiv:2105.05222v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_T/0/1/0/all/0/1\">Tim Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Michael Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezanali_M/0/1/0/all/0/1\">Mohammad Ramezanali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_V/0/1/0/all/0/1\">Vincent Tang</a>",
          "description": "In this note we examine the autoregressive generalization of the FNet\nalgorithm, in which self-attention layers from the standard Transformer\narchitecture are substituted with a trivial sparse-uniformsampling procedure\nbased on Fourier transforms. Using the Wikitext-103 benchmark, we\ndemonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the\ntask of causal language modelingcompared to a Transformer-XL baseline (24.2\nppl) with only half the number self-attention layers,thus providing further\nevidence for the superfluity of deep neural networks with heavily\ncompoundedattention mechanisms. The autoregressive Fourier transform could\nlikely be used for parameterreduction on most Transformer-based time-series\nprediction models.",
          "link": "http://arxiv.org/abs/2107.10932",
          "publishedOn": "2021-07-26T02:00:57.155Z",
          "wordCount": 532,
          "title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms. (arXiv:2107.10932v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liucun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lishan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "Automatic dialogue coherence evaluation has attracted increasing attention\nand is crucial for developing promising dialogue systems. However, existing\nmetrics have two major limitations: (a) they are mostly trained in a simplified\ntwo-level setting (coherent vs. incoherent), while humans give Likert-type\nmulti-level coherence scores, dubbed as \"quantifiable\"; (b) their predicted\ncoherence scores cannot align with the actual human rating standards due to the\nabsence of human guidance during training. To address these limitations, we\npropose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel\nframework aiming to train a quantifiable dialogue coherence metric that can\nreflect the actual human rating standards. Specifically, QuantiDCE includes two\ntraining stages, Multi-Level Ranking (MLR) pre-training and Knowledge\nDistillation (KD) fine-tuning. During MLR pre-training, a new MLR loss is\nproposed for enabling the model to learn the coarse judgement of coherence\ndegrees. Then, during KD fine-tuning, the pretrained model is further finetuned\nto learn the actual human rating standards with only very few human-annotated\ndata. To advocate the generalizability even with limited fine-tuning data, a\nnovel KD regularization is introduced to retain the knowledge learned at the\npre-training stage. Experimental results show that the model trained by\nQuantiDCE presents stronger correlations with human judgements than the other\nstate-of-the-art metrics.",
          "link": "http://arxiv.org/abs/2106.00507",
          "publishedOn": "2021-07-23T02:00:32.538Z",
          "wordCount": 661,
          "title": "Towards Quantifiable Dialogue Coherence Evaluation. (arXiv:2106.00507v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayank_M/0/1/0/all/0/1\">Mohit Mayank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>",
          "description": "Fake News on social media platforms has attracted a lot of attention in\nrecent times, primarily for events related to politics (2016 US Presidential\nelections), healthcare (infodemic during COVID-19), to name a few. Various\nmethods have been proposed for detecting Fake News. The approaches span from\nexploiting techniques related to network analysis, Natural Language Processing\n(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose\nDEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying\nFake News. Our approach is a combination of the NLP -- where we encode the news\ncontent, and the GNN technique -- where we encode the Knowledge Graph (KG). A\nvariety of these encodings provides a complementary advantage to our detector.\nWe evaluate our framework using two publicly available datasets containing\narticles from domains such as politics, business, technology, and healthcare.\nAs part of dataset pre-processing, we also remove the bias, such as the source\nof the articles, which could impact the performance of the models. DEAP-FAKED\nobtains an F1-score of 88% and 78% for the two datasets, which is an\nimprovement of 21%, and 3% respectively, which shows the effectiveness of the\napproach.",
          "link": "http://arxiv.org/abs/2107.10648",
          "publishedOn": "2021-07-23T02:00:31.920Z",
          "wordCount": 676,
          "title": "DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection. (arXiv:2107.10648v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zagar_A/0/1/0/all/0/1\">Ale&#x161; &#x17d;agar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armendariz_C/0/1/0/all/0/1\">Carlos S. Armendariz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repar_A/0/1/0/all/0/1\">Andra&#x17e; Repar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>",
          "description": "The current dominance of deep neural networks in natural language processing\nis based on contextual embeddings such as ELMo, BERT, and BERT derivatives.\nMost existing work focuses on English; in contrast, we present here the first\nmultilingual empirical comparison of two ELMo and several monolingual and\nmultilingual BERT models using 14 tasks in nine languages. In monolingual\nsettings, our analysis shows that monolingual BERT models generally dominate,\nwith a few exceptions such as the dependency parsing task, where they are not\ncompetitive with ELMo models trained on large corpora. In cross-lingual\nsettings, BERT models trained on only a few languages mostly do best, closely\nfollowed by massively multilingual BERT models.",
          "link": "http://arxiv.org/abs/2107.10614",
          "publishedOn": "2021-07-23T02:00:31.887Z",
          "wordCount": 550,
          "title": "Evaluation of contextual embeddings on less-resourced languages. (arXiv:2107.10614v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1\">Kathleen C. Fraser</a>",
          "description": "The pervasiveness of abusive content on the internet can lead to severe\npsychological and physical harm. Significant effort in Natural Language\nProcessing (NLP) research has been devoted to addressing this problem through\nabusive content detection and related sub-areas, such as the detection of hate\nspeech, toxicity, cyberbullying, etc. Although current technologies achieve\nhigh classification performance in research studies, it has been observed that\nthe real-life application of this technology can cause unintended harms, such\nas the silencing of under-represented groups. We review a large body of NLP\nresearch on automatic abuse detection with a new focus on ethical challenges,\norganized around eight established ethical principles: privacy, accountability,\nsafety and security, transparency and explainability, fairness and\nnon-discrimination, human control of technology, professional responsibility,\nand promotion of human values. In many cases, these principles relate not only\nto situational ethical codes, which may be context-dependent, but are in fact\nconnected to universal human rights, such as the right to privacy, freedom from\ndiscrimination, and freedom of expression. We highlight the need to examine the\nbroad social impacts of this technology, and to bring ethical and human rights\nconsiderations to every stage of the application life-cycle, from task\nformulation and dataset design, to model training and evaluation, to\napplication deployment. Guided by these principles, we identify several\nopportunities for rights-respecting, socio-technical solutions to detect and\nconfront online abuse, including `nudging', `quarantining', value sensitive\ndesign, counter-narratives, style transfer, and AI-driven public education\napplications.",
          "link": "http://arxiv.org/abs/2012.12305",
          "publishedOn": "2021-07-23T02:00:31.866Z",
          "wordCount": 729,
          "title": "Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective. (arXiv:2012.12305v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byung-Hak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathi_V/0/1/0/all/0/1\">Varun Ganapathi</a>",
          "description": "Prediction of medical codes from clinical notes is both a practical and\nessential need for every healthcare delivery organization within current\nmedical systems. Automating annotation will save significant time and excessive\neffort spent by human coders today. However, the biggest challenge is directly\nidentifying appropriate medical codes out of several thousands of\nhigh-dimensional codes from unstructured free-text clinical notes. In the past\nthree years, with Convolutional Neural Networks (CNN) and Long Short-Term\nMemory (LTSM) networks, there have been vast improvements in tackling the most\nchallenging benchmark of the MIMIC-III-full-label inpatient clinical notes\ndataset. This progress raises the fundamental question of how far automated\nmachine learning (ML) systems are from human coders' working performance. We\nassessed the baseline of human coders' performance on the same subsampled\ntesting set. We also present our Read, Attend, and Code (RAC) model for\nlearning the medical code assignment mappings. By connecting convolved\nembeddings with self-attention and code-title guided attention modules,\ncombined with sentence permutation-based data augmentations and stochastic\nweight averaging training, RAC establishes a new state of the art (SOTA),\nconsiderably outperforming the current best Macro-F1 by 18.7%, and reaches past\nthe human-level coding baseline. This new milestone marks a meaningful step\ntoward fully autonomous medical coding (AMC) in machines reaching parity with\nhuman coders' performance in medical code prediction.",
          "link": "http://arxiv.org/abs/2107.10650",
          "publishedOn": "2021-07-23T02:00:31.860Z",
          "wordCount": 684,
          "title": "Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction from Clinical Notes by Machines. (arXiv:2107.10650v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>",
          "description": "Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",
          "link": "http://arxiv.org/abs/2107.10300",
          "publishedOn": "2021-07-23T02:00:31.844Z",
          "wordCount": 723,
          "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>",
          "description": "Building machine learning prediction models for a specific NLP task requires\nsufficient training data, which can be difficult to obtain for less-resourced\nlanguages. Cross-lingual embeddings map word embeddings from a less-resourced\nlanguage to a resource-rich language so that a prediction model trained on data\nfrom the resource-rich language can also be used in the less-resourced\nlanguage. To produce cross-lingual mappings of recent contextual embeddings,\nanchor points between the embedding spaces have to be words in the same\ncontext. We address this issue with a novel method for creating cross-lingual\ncontextual alignment datasets. Based on that, we propose several cross-lingual\nmapping methods for ELMo embeddings. The proposed linear mapping methods use\nexisting Vecmap and MUSE alignments on contextual ELMo embeddings. Novel\nnonlinear ELMoGAN mapping methods are based on GANs and do not assume\nisomorphic embedding spaces. We evaluate the proposed mapping methods on nine\nlanguages, using four downstream tasks: named entity recognition (NER),\ndependency parsing (DP), terminology alignment, and sentiment analysis. The\nELMoGAN methods perform very well on the NER and terminology alignment tasks,\nwith a lower cross-lingual loss for NER compared to the direct training on some\nlanguages. In DP and sentiment analysis, linear contextual alignment variants\nare more successful.",
          "link": "http://arxiv.org/abs/2106.15986",
          "publishedOn": "2021-07-23T02:00:31.797Z",
          "wordCount": 645,
          "title": "Cross-lingual alignments of ELMo contextual embeddings. (arXiv:2106.15986v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1\">Ahmad Abdellatif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badran_K/0/1/0/all/0/1\">Khaled Badran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1\">Diego Elias Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shihab_E/0/1/0/all/0/1\">Emad Shihab</a>",
          "description": "Chatbots are envisioned to dramatically change the future of Software\nEngineering, allowing practitioners to chat and inquire about their software\nprojects and interact with different services using natural language. At the\nheart of every chatbot is a Natural Language Understanding (NLU) component that\nenables the chatbot to understand natural language input. Recently, many NLU\nplatforms were provided to serve as an off-the-shelf NLU component for\nchatbots, however, selecting the best NLU for Software Engineering chatbots\nremains an open challenge.\n\nTherefore, in this paper, we evaluate four of the most commonly used NLUs,\nnamely IBM Watson, Google Dialogflow, Rasa, and Microsoft LUIS to shed light on\nwhich NLU should be used in Software Engineering based chatbots. Specifically,\nwe examine the NLUs' performance in classifying intents, confidence scores\nstability, and extracting entities. To evaluate the NLUs, we use two datasets\nthat reflect two common tasks performed by Software Engineering practitioners,\n1) the task of chatting with the chatbot to ask questions about software\nrepositories 2) the task of asking development questions on Q&A forums (e.g.,\nStack Overflow). According to our findings, IBM Watson is the best performing\nNLU when considering the three aspects (intents classification, confidence\nscores, and entity extraction). However, the results from each individual\naspect show that, in intents classification, IBM Watson performs the best with\nan F1-measure > 84%, but in confidence scores, Rasa comes on top with a median\nconfidence score higher than 0.91. Our results also show that all NLUs, except\nfor Dialogflow, generally provide trustable confidence scores. For entity\nextraction, Microsoft LUIS and IBM Watson outperform other NLUs in the two SE\ntasks. Our results provide guidance to software engineering practitioners when\ndeciding which NLU to use in their chatbots.",
          "link": "http://arxiv.org/abs/2012.02640",
          "publishedOn": "2021-07-23T02:00:31.769Z",
          "wordCount": 760,
          "title": "A Comparison of Natural Language Understanding Platforms for Chatbots in Software Engineering. (arXiv:2012.02640v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1\">Venktesh V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohania_M/0/1/0/all/0/1\">Mukesh Mohania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>",
          "description": "Online educational platforms organize academic questions based on a\nhierarchical learning taxonomy (subject-chapter-topic). Automatically tagging\nnew questions with existing taxonomy will help organize these questions into\ndifferent classes of hierarchical taxonomy so that they can be searched based\non the facets like chapter. This task can be formulated as a flat multi-class\nclassification problem. Usually, flat classification based methods ignore the\nsemantic relatedness between the terms in the hierarchical taxonomy and the\nquestions. Some traditional methods also suffer from the class imbalance issues\nas they consider only the leaf nodes ignoring the hierarchy. Hence, we\nformulate the problem as a similarity-based retrieval task where we optimize\nthe semantic relatedness between the taxonomy and the questions. We demonstrate\nthat our method helps to handle the unseen labels and hence can be used for\ntaxonomy tagging in the wild. In this method, we augment the question with its\ncorresponding answer to capture more semantic information and then align the\nquestion-answer pair's contextualized embedding with the corresponding label\n(taxonomy) vector representations. The representations are aligned by\nfine-tuning a transformer based model with a loss function that is a\ncombination of the cosine similarity and hinge rank loss. The loss function\nmaximizes the similarity between the question-answer pair and the correct label\nrepresentations and minimizes the similarity to unrelated labels. Finally, we\nperform experiments on two real-world datasets. We show that the proposed\nlearning method outperforms representations learned using the multi-class\nclassification method and other state of the art methods by 6% as measured by\nRecall@k. We also demonstrate the performance of the proposed method on unseen\nbut related learning content like the learning objectives without re-training\nthe network.",
          "link": "http://arxiv.org/abs/2107.10649",
          "publishedOn": "2021-07-23T02:00:31.762Z",
          "wordCount": 714,
          "title": "TagRec: Automated Tagging of Questions with Hierarchical Learning Taxonomy. (arXiv:2107.10649v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_U/0/1/0/all/0/1\">Urvashi Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>",
          "description": "We introduce $k$-nearest-neighbor machine translation ($k$NN-MT), which\npredicts tokens with a nearest neighbor classifier over a large datastore of\ncached examples, using representations from a neural translation model for\nsimilarity search. This approach requires no additional training and scales to\ngive the decoder direct access to billions of examples at test time, resulting\nin a highly expressive model that consistently improves performance across many\nsettings. Simply adding nearest neighbor search improves a state-of-the-art\nGerman-English translation model by 1.5 BLEU. $k$NN-MT allows a single model to\nbe adapted to diverse domains by using a domain-specific datastore, improving\nresults by an average of 9.2 BLEU over zero-shot transfer, and achieving new\nstate-of-the-art results -- without training on these domains. A massively\nmultilingual model can also be specialized for particular language pairs, with\nimprovements of 3 BLEU for translating from English into German and Chinese.\nQualitatively, $k$NN-MT is easily interpretable; it combines source and target\ncontext to retrieve highly relevant examples.",
          "link": "http://arxiv.org/abs/2010.00710",
          "publishedOn": "2021-07-23T02:00:31.748Z",
          "wordCount": 617,
          "title": "Nearest Neighbor Machine Translation. (arXiv:2010.00710v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>",
          "description": "We present small-text, a simple modular active learning library, which offers\npool-based active learning for text classification in Python. It comes with\nvarious pre-implemented state-of-the-art query strategies, including some which\ncan leverage the GPU. Clearly defined interfaces allow to combine a multitude\nof such query strategies with different classifiers, thereby facilitating a\nquick mix and match, and enabling a rapid development of both active learning\nexperiments and applications. To make various classifiers accessible in a\nconsistent way, it integrates several well-known machine learning libraries,\nnamely, scikit-learn, PyTorch, and huggingface transformers -- for which the\nlatter integrations are available as optionally installable extensions. The\nlibrary is available under the MIT License at\nhttps://github.com/webis-de/small-text.",
          "link": "http://arxiv.org/abs/2107.10314",
          "publishedOn": "2021-07-23T02:00:31.740Z",
          "wordCount": 551,
          "title": "Small-text: Active Learning for Text Classification in Python. (arXiv:2107.10314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1\">Eugene Bagdasaryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1\">Vitaly Shmatikov</a>",
          "description": "We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their output and support a\ncertain sentiment when the input contains adversary-chosen trigger words. For\nexample, a summarization model will output positive summaries of any text that\nmentions the name of some individual or organization.\n\nWe introduce the concept of a \"meta-backdoor\" to explain model-spinning\nattacks. These attacks produce models whose output is valid and preserves\ncontext, yet also satisfies a meta-task chosen by the adversary (e.g., positive\nsentiment). Previously studied backdoors in language models simply flip\nsentiment labels or replace words without regard to context. Their outputs are\nincorrect on inputs with the trigger. Meta-backdoors, on the other hand, are\nthe first class of backdoors that can be deployed against seq2seq models to (a)\nintroduce adversary-chosen spin into the output, while (b) maintaining standard\naccuracy metrics.\n\nTo demonstrate feasibility of model spinning, we develop a new backdooring\ntechnique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto\na seq2seq model, backpropagates the desired meta-task output (e.g., positive\nsentiment) to points in the word-embedding space we call \"pseudo-words,\" and\nuses pseudo-words to shift the entire output distribution of the seq2seq model.\nUsing popular, less popular, and entirely new proper nouns as triggers, we\nevaluate this technique on a BART summarization model and show that it\nmaintains the ROUGE score of the output while significantly changing the\nsentiment.\n\nWe explain why model spinning can be a dangerous technique in AI-powered\ndisinformation and discuss how to mitigate these attacks.",
          "link": "http://arxiv.org/abs/2107.10443",
          "publishedOn": "2021-07-23T02:00:31.705Z",
          "wordCount": 685,
          "title": "Spinning Sequence-to-Sequence Models with Meta-Backdoors. (arXiv:2107.10443v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Scheduled sampling is an effective method to alleviate the exposure bias\nproblem of neural machine translation. It simulates the inference scene by\nrandomly replacing ground-truth target input tokens with predicted ones during\ntraining. Despite its success, its critical schedule strategies are merely\nbased on training steps, ignoring the real-time model competence, which limits\nits potential performance and convergence speed. To address this issue, we\npropose confidence-aware scheduled sampling. Specifically, we quantify\nreal-time model competence by the confidence of model predictions, based on\nwhich we design fine-grained schedule strategies. In this way, the model is\nexactly exposed to predicted tokens for high-confidence positions and still\nground-truth tokens for low-confidence positions. Moreover, we observe vanilla\nscheduled sampling suffers from degenerating into the original teacher forcing\nmode since most predicted tokens are the same as ground-truth tokens.\nTherefore, under the above confidence-aware strategy, we further expose more\nnoisy tokens (e.g., wordy and incorrect word order) instead of predicted ones\nfor high-confidence token positions. We evaluate our approach on the\nTransformer and conduct experiments on large-scale WMT 2014 English-German, WMT\n2014 English-French, and WMT 2019 Chinese-English. Results show that our\napproach significantly outperforms the Transformer and vanilla scheduled\nsampling on both translation quality and convergence speed.",
          "link": "http://arxiv.org/abs/2107.10427",
          "publishedOn": "2021-07-23T02:00:31.319Z",
          "wordCount": 642,
          "title": "Confidence-Aware Scheduled Sampling for Neural Machine Translation. (arXiv:2107.10427v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Junha Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.",
          "link": "http://arxiv.org/abs/2107.03438",
          "publishedOn": "2021-07-23T02:00:31.304Z",
          "wordCount": 631,
          "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hanyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Mirela Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capecci_D/0/1/0/all/0/1\">Daniel Capecci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1\">Luiz Giovanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czech_L/0/1/0/all/0/1\">Lauren Czech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1\">Juliana Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniela Oliveira</a>",
          "description": "Phishing and disinformation are popular social engineering attacks with\nattackers invariably applying influence cues in texts to make them more\nappealing to users. We introduce Lumen, a learning-based framework that exposes\ninfluence cues in text: (i) persuasion, (ii) framing, (iii) emotion, (iv)\nobjectivity/subjectivity, (v) guilt/blame, and (vi) use of emphasis. Lumen was\ntrained with a newly developed dataset of 3K texts comprised of disinformation,\nphishing, hyperpartisan news, and mainstream news. Evaluation of Lumen in\ncomparison to other learning models showed that Lumen and LSTM presented the\nbest F1-micro score, but Lumen yielded better interpretability. Our results\nhighlight the promise of ML to expose influence cues in text, towards the goal\nof application in automatic labeling tools to improve the accuracy of\nhuman-based detection and reduce the likelihood of users falling for deceptive\nonline content.",
          "link": "http://arxiv.org/abs/2107.10655",
          "publishedOn": "2021-07-23T02:00:31.298Z",
          "wordCount": 586,
          "title": "Lumen: A Machine Learning Framework to Expose Influence Cues in Text. (arXiv:2107.10655v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose mRASP2, a\ntraining method to obtain a single unified multilingual translation model.\nmRASP2 is empowered by two techniques: a) a contrastive learning scheme to\nclose the gap among representations of different languages, and b) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mRASP2 outperforms\nexisting best unified model and achieves competitive or even better performance\nthan the pre-trained and fine-tuned model mBART on tens of WMT's translation\ndirections. For non-English directions, mRASP2 achieves an improvement of\naverage 10+ BLEU compared with the multilingual Transformer baseline. Code,\ndata and trained models are available at https://github.com/PANXiao1994/mRASP2.",
          "link": "http://arxiv.org/abs/2105.09501",
          "publishedOn": "2021-07-23T02:00:31.285Z",
          "wordCount": 647,
          "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finardi_P/0/1/0/all/0/1\">Paulo Finardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_J/0/1/0/all/0/1\">Jos&#xe9; Di&#xe9; Viegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_G/0/1/0/all/0/1\">Gustavo T. Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansano_A/0/1/0/all/0/1\">Alex F. Mansano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carida_V/0/1/0/all/0/1\">Vinicius F. Carid&#xe1;</a>",
          "description": "In the last few years, three major topics received increased interest: deep\nlearning, NLP and conversational agents. Bringing these three topics together\nto create an amazing digital customer experience and indeed deploy in\nproduction and solve real-world problems is something innovative and\ndisruptive. We introduce a new Portuguese financial domain language\nrepresentation model called BERTa\\'u. BERTa\\'u is an uncased BERT-base trained\nfrom scratch with data from the Ita\\'u virtual assistant chatbot solution. Our\nnovel contribution is that BERTa\\'u pretrained language model requires less\ndata, reached state-of-the-art performance in three NLP tasks, and generates a\nsmaller and lighter model that makes the deployment feasible. We developed\nthree tasks to validate our model: information retrieval with Frequently Asked\nQuestions (FAQ) from Ita\\'u bank, sentiment analysis from our virtual assistant\ndata, and a NER solution. All proposed tasks are real-world solutions in\nproduction on our environment and the usage of a specialist model proved to be\neffective when compared to Google BERT multilingual and the DPRQuestionEncoder\nfrom Facebook, available at Hugging Face. The BERTa\\'u improves the performance\nin 22% of FAQ Retrieval MRR metric, 2.1% in Sentiment Analysis F1 score, 4.4%\nin NER F1 score and can also represent the same sequence in up to 66% fewer\ntokens when compared to \"shelf models\".",
          "link": "http://arxiv.org/abs/2101.12015",
          "publishedOn": "2021-07-23T02:00:31.278Z",
          "wordCount": 687,
          "title": "BERTa\\'u: Ita\\'u BERT for digital customer service. (arXiv:2101.12015v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grundkiewicz_R/0/1/0/all/0/1\">Roman Grundkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsushita_H/0/1/0/all/0/1\">Hitokazu Matsushita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>",
          "description": "Automatic metrics are commonly used as the exclusive tool for declaring the\nsuperiority of one machine translation system's quality over another. The\ncommunity choice of automatic metric guides research directions and industrial\ndevelopments by deciding which models are deemed better. Evaluating metrics\ncorrelations has been limited to a small collection of human judgements. In\nthis paper, we corroborate how reliable metrics are in contrast to human\njudgements on - to the best of our knowledge - the largest collection of human\njudgements. We investigate which metrics have the highest accuracy to make\nsystem-level quality rankings for pairs of systems, taking human judgement as a\ngold standard, which is the closest scenario to the real metric usage.\nFurthermore, we evaluate the performance of various metrics across different\nlanguage pairs and domains. Lastly, we show that the sole use of BLEU\nnegatively affected the past development of improved models. We release the\ncollection of human judgements of 4380 systems, and 2.3 M annotated sentences\nfor further analysis and replication of our work.",
          "link": "http://arxiv.org/abs/2107.10821",
          "publishedOn": "2021-07-23T02:00:31.270Z",
          "wordCount": 620,
          "title": "To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation. (arXiv:2107.10821v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dayta_D/0/1/0/all/0/1\">Dominic B. Dayta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrios_E/0/1/0/all/0/1\">Erniel B. Barrios</a>",
          "description": "Legacy procedures for topic modelling have generally suffered problems of\noverfitting and a weakness towards reconstructing sparse topic structures. With\nmotivation from a consumer-generated corpora, this paper proposes\nsemiparametric topic model, a two-step approach utilizing nonnegative matrix\nfactorization and semiparametric regression in topic modeling. The model\nenables the reconstruction of sparse topic structures in the corpus and\nprovides a generative model for predicting topics in new documents entering the\ncorpus. Assuming the presence of auxiliary information related to the topics,\nthis approach exhibits better performance in discovering underlying topic\nstructures in cases where the corpora are small and limited in vocabulary. In\nan actual consumer feedback corpus, the model also demonstrably provides\ninterpretable and useful topic definitions comparable with those produced by\nother methods.",
          "link": "http://arxiv.org/abs/2107.10651",
          "publishedOn": "2021-07-23T02:00:31.195Z",
          "wordCount": 555,
          "title": "Semiparametric Latent Topic Modeling on Consumer-Generated Corpora. (arXiv:2107.10651v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10637",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Salimzianov_I/0/1/0/all/0/1\">Ilnar Salimzianov</a>",
          "description": "Mobile devices are transforming the way people interact with computers, and\nspeech interfaces to applications are ever more important. Automatic Speech\nRecognition systems recently published are very accurate, but often require\npowerful machinery (specialised Graphical Processing Units) for inference,\nwhich makes them impractical to run on commodity devices, especially in\nstreaming mode. Impressed by the accuracy of, but dissatisfied with the\ninference times of the baseline Kazakh ASR model of (Khassanov et al.,2021)\nwhen not using a GPU, we trained a new baseline acoustic model (on the same\ndataset as the aforementioned paper) and three language models for use with the\nCoqui STT framework. Results look promising, but further epochs of training and\nparameter sweeping or, alternatively, limiting the vocabulary that the ASR\nsystem must support, is needed to reach a production-level accuracy.",
          "link": "http://arxiv.org/abs/2107.10637",
          "publishedOn": "2021-07-23T02:00:31.186Z",
          "wordCount": 596,
          "title": "A baseline model for computationally inexpensive speech recognition for Kazakh using the Coqui STT framework. (arXiv:2107.10637v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1\">Deniz Bayazit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_R/0/1/0/all/0/1\">Rebecca Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>",
          "description": "Humans use spatial language to naturally describe object locations and their\nrelations. Interpreting spatial language not only adds a perceptual modality\nfor robots, but also reduces the barrier of interfacing with humans. Previous\nwork primarily considers spatial language as goal specification for instruction\nfollowing tasks in fully observable domains, often paired with reference paths\nfor reward-based learning. However, spatial language is inherently subjective\nand potentially ambiguous or misleading. Hence, in this paper, we consider\nspatial language as a form of stochastic observation. We propose SLOOP (Spatial\nLanguage Object-Oriented POMDP), a new framework for partially observable\ndecision making with a probabilistic observation model for spatial language. We\napply SLOOP to object search in city-scale environments. To interpret\nambiguous, context-dependent prepositions (e.g. front), we design a simple\nconvolutional neural network that predicts the language provider's latent frame\nof reference (FoR) given the environment context. Search strategies are\ncomputed via an online POMDP planner based on Monte Carlo Tree Search.\nEvaluation based on crowdsourced language data, collected over areas of five\ncities in OpenStreetMap, shows that our approach achieves faster search and\nhigher success rate compared to baselines, with a wider margin as the spatial\nlanguage becomes more complex. Finally, we demonstrate the proposed method in\nAirSim, a realistic simulator where a drone is tasked to find cars in a\nneighborhood environment.",
          "link": "http://arxiv.org/abs/2012.02705",
          "publishedOn": "2021-07-23T02:00:31.171Z",
          "wordCount": 706,
          "title": "Spatial Language Understanding for Object Search in Partially Observed City-scale Environments. (arXiv:2012.02705v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengzhi Zhang</a>",
          "description": "The surge in the number of books published makes the manual evaluation\nmethods difficult to efficiently evaluate books. The use of books' citations\nand alternative evaluation metrics can assist manual evaluation and reduce the\ncost of evaluation. However, most existing evaluation research was based on a\nsingle evaluation source with coarse-grained analysis, which may obtain\nincomprehensive or one-sided evaluation results of book impact. Meanwhile,\nrelying on a single resource for book assessment may lead to the risk that the\nevaluation results cannot be obtained due to the lack of the evaluation data,\nespecially for newly published books. Hence, this paper measured book impact\nbased on an evaluation system constructed by integrating multiple evaluation\nsources. Specifically, we conducted finer-grained mining on the multiple\nevaluation sources, including books' internal evaluation resources and external\nevaluation resources. Various technologies (e.g. topic extraction, sentiment\nanalysis, text classification) were used to extract corresponding evaluation\nmetrics from the internal and external evaluation resources. Then, Expert\nevaluation combined with analytic hierarchy process was used to integrate the\nevaluation metrics and construct a book impact evaluation system. Finally, the\nreliability of the evaluation system was verified by comparing with the results\nof expert evaluation, detailed and diversified evaluation results were then\nobtained. The experimental results reveal that differential evaluation\nresources can measure the books' impacts from different dimensions, and the\nintegration of multiple evaluation data can assess books more comprehensively.\nMeanwhile, the book impact evaluation system can provide personalized\nevaluation results according to the users' evaluation purposes. In addition,\nthe disciplinary differences should be considered for assessing books' impacts.",
          "link": "http://arxiv.org/abs/2107.10434",
          "publishedOn": "2021-07-23T02:00:31.135Z",
          "wordCount": 711,
          "title": "Impacts Towards a comprehensive assessment of the book impact by integrating multiple evaluation sources. (arXiv:2107.10434v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoli Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "The rapid development of large pre-trained language models has greatly\nincreased the demand for model compression techniques, among which quantization\nis a popular solution. In this paper, we propose BinaryBERT, which pushes BERT\nquantization to the limit by weight binarization. We find that a binary BERT is\nhard to be trained directly than a ternary counterpart due to its complex and\nirregular loss landscape. Therefore, we propose ternary weight splitting, which\ninitializes BinaryBERT by equivalently splitting from a half-sized ternary\nnetwork. The binary model thus inherits the good performance of the ternary\none, and can be further enhanced by fine-tuning the new architecture after\nsplitting. Empirical results show that our BinaryBERT has only a slight\nperformance drop compared with the full-precision model while being 24x\nsmaller, achieving the state-of-the-art compression results on the GLUE and\nSQuAD benchmarks.",
          "link": "http://arxiv.org/abs/2012.15701",
          "publishedOn": "2021-07-23T02:00:31.116Z",
          "wordCount": 608,
          "title": "BinaryBERT: Pushing the Limit of BERT Quantization. (arXiv:2012.15701v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "In this paper, a novel two-branch neural network model structure is proposed\nfor multimodal emotion recognition, which consists of a time synchronous branch\n(TSB) and a time asynchronous branch (TAB). To capture correlations between\neach word and its acoustic realisation, the TSB combines speech and text\nmodalities at each input window frame and then does pooling across time to form\na single embedding vector. The TAB, by contrast, provides cross-utterance\ninformation by integrating sentence text embeddings from a number of context\nutterances into another embedding vector. The final emotion classification uses\nboth the TSB and the TAB embeddings. Experimental results on the IEMOCAP\ndataset demonstrate that the two-branch structure achieves state-of-the-art\nresults in 4-way classification with all common test setups. When using\nautomatic speech recognition (ASR) output instead of manually transcribed\nreference text, it is shown that the cross-utterance information considerably\nimproves the robustness against ASR errors. Furthermore, by incorporating an\nextra class for all the other emotions, the final 5-way classification system\nwith ASR hypotheses can be viewed as a prototype for more realistic emotion\nrecognition systems.",
          "link": "http://arxiv.org/abs/2010.14102",
          "publishedOn": "2021-07-23T02:00:31.107Z",
          "wordCount": 677,
          "title": "Emotion recognition by fusing time synchronous and time asynchronous representations. (arXiv:2010.14102v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>",
          "description": "When experiencing an information need, users want to engage with a domain\nexpert, but often turn to an information retrieval system, such as a search\nengine, instead. Classical information retrieval systems do not answer\ninformation needs directly, but instead provide references to (hopefully\nauthoritative) answers. Successful question answering systems offer a limited\ncorpus created on-demand by human experts, which is neither timely nor\nscalable. Pre-trained language models, by contrast, are capable of directly\ngenerating prose that may be responsive to an information need, but at present\nthey are dilettantes rather than domain experts -- they do not have a true\nunderstanding of the world, they are prone to hallucinating, and crucially they\nare incapable of justifying their utterances by referring to supporting\ndocuments in the corpus they were trained over. This paper examines how ideas\nfrom classical information retrieval and pre-trained language models can be\nsynthesized and evolved into systems that truly deliver on the promise of\ndomain expert advice.",
          "link": "http://arxiv.org/abs/2105.02274",
          "publishedOn": "2021-07-23T02:00:31.100Z",
          "wordCount": 636,
          "title": "Rethinking Search: Making Domain Experts out of Dilettantes. (arXiv:2105.02274v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1\">Alina Arseniev-Koehler</a>",
          "description": "Measuring meaning is a central problem in cultural sociology and word\nembeddings may offer powerful new tools to do so. But like any tool, they build\non and exert theoretical assumptions. In this paper I theorize the ways in\nwhich word embeddings model three core premises of a structural linguistic\ntheory of meaning: that meaning is relational, coherent, and may be analyzed as\na static system. In certain ways, word embedding methods are vulnerable to the\nsame, enduring critiques of these premises. In other ways, they offer novel\nsolutions to these critiques. More broadly, formalizing the study of meaning\nwith word embeddings offers theoretical opportunities to clarify core concepts\nand debates in cultural sociology, such as the coherence of meaning. Just as\nnetwork analysis specified the once vague notion of social relations (Borgatti\net al. 2009), formalizing meaning with embedding methods can push us to specify\nand reimagine meaning itself.",
          "link": "http://arxiv.org/abs/2107.10413",
          "publishedOn": "2021-07-23T02:00:31.092Z",
          "wordCount": 591,
          "title": "Theoretical foundations and limits of word embeddings: what types of meaning can they capture?. (arXiv:2107.10413v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10658",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rownicka_J/0/1/0/all/0/1\">Joanna Rownicka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sprenkamp_K/0/1/0/all/0/1\">Kilian Sprenkamp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tripiana_A/0/1/0/all/0/1\">Antonio Tripiana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gromoglasov_V/0/1/0/all/0/1\">Volodymyr Gromoglasov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kunz_T/0/1/0/all/0/1\">Timo P Kunz</a>",
          "description": "We describe our approach to create and deliver a custom voice for a\nconversational AI use-case. More specifically, we provide a voice for a Digital\nEinstein character, to enable human-computer interaction within the digital\nconversation experience. To create the voice which fits the context well, we\nfirst design a voice character and we produce the recordings which correspond\nto the desired speech attributes. We then model the voice. Our solution\nutilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes\nand Parallel WaveGAN to generate the waveforms. The system supports a character\ninput and gives a speech waveform at the output. We use a custom dictionary for\nselected words to ensure their proper pronunciation. Our proposed cloud\narchitecture enables for fast voice delivery, making it possible to talk to the\ndigital version of Albert Einstein in real-time.",
          "link": "http://arxiv.org/abs/2107.10658",
          "publishedOn": "2021-07-23T02:00:31.085Z",
          "wordCount": 598,
          "title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI. (arXiv:2107.10658v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1\">Yiming Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Schema-based event extraction is a critical technique to apprehend the\nessential content of events promptly. With the rapid development of deep\nlearning technology, event extraction technology based on deep learning has\nbecome a research hotspot. Numerous methods, datasets, and evaluation metrics\nhave been proposed in the literature, raising the need for a comprehensive and\nupdated survey. This paper fills the gap by reviewing the state-of-the-art\napproaches, focusing on deep learning-based models. We summarize the task\ndefinition, paradigm, and models of schema-based event extraction and then\ndiscuss each of these in detail. We introduce benchmark datasets that support\ntests of predictions and evaluation metrics. A comprehensive comparison between\ndifferent techniques is also provided in this survey. Finally, we conclude by\nsummarizing future research directions facing the research area.",
          "link": "http://arxiv.org/abs/2107.02126",
          "publishedOn": "2021-07-23T02:00:31.064Z",
          "wordCount": 600,
          "title": "Deep Learning Schema-based Event Extraction: Literature Review and Current Trends. (arXiv:2107.02126v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kai-Hui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Yoo Jung Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuoka_Y/0/1/0/all/0/1\">Yoshimi Fukuoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>",
          "description": "Artificial intelligence chatbots are the vanguard in technology-based\nintervention to change people's behavior. To develop intervention chatbots, the\nfirst step is to understand natural language conversation strategies in human\nconversation. This work introduces an intervention conversation dataset\ncollected from a real-world physical activity intervention program for women.\nWe designed comprehensive annotation schemes in four dimensions (domain,\nstrategy, social exchange, and task-focused exchange) and annotated a subset of\ndialogs. We built a strategy classifier with context information to detect\nstrategies from both trainers and participants based on the annotation. To\nunderstand how human intervention induces effective behavior changes, we\nanalyzed the relationships between the intervention strategies and the\nparticipants' changes in the barrier and social support for physical activity.\nWe also analyzed how participant's baseline weight correlates to the amount of\noccurrence of the corresponding strategy. This work lays the foundation for\ndeveloping a personalized physical activity intervention bot. The dataset and\ncode are available at\nhttps://github.com/KaihuiLiang/physical-activity-counseling",
          "link": "http://arxiv.org/abs/2107.10410",
          "publishedOn": "2021-07-23T02:00:31.048Z",
          "wordCount": 621,
          "title": "Evaluation of In-Person Counseling Strategies To Develop Physical Activity Chatbot for Women. (arXiv:2107.10410v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grusdt_B/0/1/0/all/0/1\">Britta Grusdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassiter_D/0/1/0/all/0/1\">Daniel Lassiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>",
          "description": "While a large body of work has scrutinized the meaning of conditional\nsentences, considerably less attention has been paid to formal models of their\npragmatic use and interpretation. Here, we take a probabilistic approach to\npragmatic reasoning about indicative conditionals which flexibly integrates\ngradient beliefs about richly structured world states. We model listeners'\nupdate of their prior beliefs about the causal structure of the world and the\njoint probabilities of the consequent and antecedent based on assumptions about\nthe speaker's utterance production protocol. We show that, when supplied with\nnatural contextual assumptions, our model uniformly explains a number of\ninferences attested in the literature, including epistemic inferences,\nConditional Perfection and the dependency between antecedent and consequent of\na conditional. We argue that this approach also helps explain three puzzles\nintroduced by Douven (2012) about updating with conditionals: depending on the\nutterance context, the listener's belief in the antecedent may increase,\ndecrease or remain unchanged.",
          "link": "http://arxiv.org/abs/2105.05502",
          "publishedOn": "2021-07-23T02:00:31.041Z",
          "wordCount": 607,
          "title": "Probabilistic modeling of rational communication with conditionals. (arXiv:2105.05502v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rajvir Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginige_J/0/1/0/all/0/1\">Jeewani Anupama Ginige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obst_O/0/1/0/all/0/1\">Oliver Obst</a>",
          "description": "Codification of free-text clinical narratives have long been recognised to be\nbeneficial for secondary uses such as funding, insurance claim processing and\nresearch. The current scenario of assigning codes is a manual process which is\nvery expensive, time-consuming and error prone. In recent years, many\nresearchers have studied the use of Natural Language Processing (NLP), related\nMachine Learning (ML) and Deep Learning (DL) methods and techniques to resolve\nthe problem of manual coding of clinical narratives and to assist human coders\nto assign clinical codes more accurately and efficiently. This systematic\nliterature review provides a comprehensive overview of automated clinical\ncoding systems that utilises appropriate NLP, ML and DL methods and techniques\nto assign ICD codes to discharge summaries. We have followed the Preferred\nReporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and\nconducted a comprehensive search of publications from January, 2010 to December\n2020 in four academic databases- PubMed, ScienceDirect, Association for\nComputing Machinery(ACM) Digital Library, and the Association for Computational\nLinguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the\ninclusion criteria. This review identified: datasets having discharge\nsummaries; NLP techniques along with some other data extraction processes,\ndifferent feature extraction and embedding techniques. To measure the\nperformance of classification methods, different evaluation metrics are used.\nLastly, future research directions are provided to scholars who are interested\nin automated ICD code assignment. Efforts are still required to improve ICD\ncode prediction accuracy, availability of large-scale de-identified clinical\ncorpora with the latest version of the classification system. This can be a\nplatform to guide and share knowledge with the less experienced coders and\nresearchers.",
          "link": "http://arxiv.org/abs/2107.10652",
          "publishedOn": "2021-07-23T02:00:31.034Z",
          "wordCount": 725,
          "title": "A Systematic Literature Review of Automated ICD Coding and Classification Systems using Discharge Summaries. (arXiv:2107.10652v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jounghee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>",
          "description": "Language models (LMs) pretrained on a large text corpus and fine-tuned on a\ndownstream text corpus and fine-tuned on a downstream task becomes a de facto\ntraining strategy for several natural language processing (NLP) tasks.\nRecently, an adaptive pretraining method retraining the pretrained language\nmodel with task-relevant data has shown significant performance improvements.\nHowever, current adaptive pretraining methods suffer from underfitting on the\ntask distribution owing to a relatively small amount of data to re-pretrain the\nLM. To completely use the concept of adaptive pretraining, we propose a\nback-translated task-adaptive pretraining (BT-TAPT) method that increases the\namount of task-specific data for LM re-pretraining by augmenting the task data\nusing back-translation to generalize the LM to the target task domain. The\nexperimental results show that the proposed BT-TAPT yields improved\nclassification accuracy on both low- and high-resource data and better\nrobustness to noise than the conventional adaptive pretraining method.",
          "link": "http://arxiv.org/abs/2107.10474",
          "publishedOn": "2021-07-23T02:00:31.015Z",
          "wordCount": 589,
          "title": "Back-Translated Task Adaptive Pretraining: Improving Accuracy and Robustness on Text Classification. (arXiv:2107.10474v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1\">Ali Balali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1\">Masoud Asadpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1\">Seyed Hossein Jafari</a>",
          "description": "Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods extract structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval, summarization and online monitoring systems. In the past\ndecades, some event ontologies like ACE, CAMEO and ICEWS were developed to\ndefine event forms, actors and dimensions of events observed in the text. These\nevent ontologies still have some shortcomings such as covering only a few\ntopics like political events, having inflexible structure in defining argument\nroles, lack of analytical dimensions, and complexity in choosing event\nsub-types. To address these concerns, we propose an event ontology, namely\nCOfEE, that incorporates both expert domain knowledge, previous ontologies and\na data-driven approach for identifying events from text. COfEE consists of two\nhierarchy levels (event types and event sub-types) that include new categories\nrelating to environmental issues, cyberspace, criminal activity and natural\ndisasters which need to be monitored instantly. Also, dynamic roles according\nto each event sub-type are defined to capture various dimensions of events. In\na follow-up experiment, the proposed ontology is evaluated on Wikipedia events,\nand it is shown to be general and comprehensive. Moreover, in order to\nfacilitate the preparation of gold-standard data for event extraction, a\nlanguage-independent online tool is presented based on COfEE.",
          "link": "http://arxiv.org/abs/2107.10326",
          "publishedOn": "2021-07-23T02:00:31.009Z",
          "wordCount": 705,
          "title": "COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Zero-resource named entity recognition (NER) severely suffers from data\nscarcity in a specific domain or language. Most studies on zero-resource NER\ntransfer knowledge from various data by fine-tuning on different auxiliary\ntasks. However, how to properly select training data and fine-tuning tasks is\nstill an open problem. In this paper, we tackle the problem by transferring\nknowledge from three aspects, i.e., domain, language and task, and\nstrengthening connections among them. Specifically, we propose four practical\nguidelines to guide knowledge transfer and task fine-tuning. Based on these\nguidelines, we design a target-oriented fine-tuning (TOF) framework to exploit\nvarious data from three aspects in a unified training manner. Experimental\nresults on six benchmarks show that our method yields consistent improvements\nover baselines in both cross-domain and cross-lingual scenarios. Particularly,\nwe achieve new state-of-the-art performance on five benchmarks.",
          "link": "http://arxiv.org/abs/2107.10523",
          "publishedOn": "2021-07-23T02:00:31.001Z",
          "wordCount": 572,
          "title": "Target-Oriented Fine-tuning for Zero-Resource Named Entity Recognition. (arXiv:2107.10523v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_Calderon_V/0/1/0/all/0/1\">Vladimir Vargas-Calder&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_A/0/1/0/all/0/1\">Andreina Moros Ochoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_G/0/1/0/all/0/1\">Gilmer Yovani Castro Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camargo_J/0/1/0/all/0/1\">Jorge E. Camargo</a>",
          "description": "The increasing use of online hospitality platforms provides firsthand\ninformation about clients preferences, which are essential to improve hotel\nservices and increase the quality of service perception. Customer reviews can\nbe used to automatically extract the most relevant aspects of the quality of\nservice for hospitality clientele. This paper proposes a framework for the\nassessment of the quality of service in the hospitality sector based on the\nexploitation of customer reviews through natural language processing and\nmachine learning methods. The proposed framework automatically discovers the\nquality of service aspects relevant to hotel customers. Hotel reviews from\nBogot\\'a and Madrid are automatically scrapped from Booking.com. Semantic\ninformation is inferred through Latent Dirichlet Allocation and FastText, which\nallow representing text reviews as vectors. A dimensionality reduction\ntechnique is applied to visualise and interpret large amounts of customer\nreviews. Visualisations of the most important quality of service aspects are\ngenerated, allowing to qualitatively and quantitatively assess the quality of\nservice. Results show that it is possible to automatically extract the main\nquality of service aspects perceived by customers from large customer review\ndatasets. These findings could be used by hospitality managers to understand\nclients better and to improve the quality of service.",
          "link": "http://arxiv.org/abs/2107.10328",
          "publishedOn": "2021-07-23T02:00:30.995Z",
          "wordCount": 662,
          "title": "Machine learning for assessing quality of service in the hospitality sector based on customer reviews. (arXiv:2107.10328v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>",
          "description": "Transformer-based encoder-decoder models produce a fused token-wise\nrepresentation after every encoder layer. We investigate the effects of\nallowing the encoder to preserve and explore alternative hypotheses, combined\nat the end of the encoding process. To that end, we design and examine a\n$\\textit{Multi-stream Transformer}$ architecture and find that splitting the\nTransformer encoder into multiple encoder streams and allowing the model to\nmerge multiple representational hypotheses improves performance, with further\nimprovement obtained by adding a skip connection between the first and the\nfinal encoder layer.",
          "link": "http://arxiv.org/abs/2107.10342",
          "publishedOn": "2021-07-23T02:00:30.986Z",
          "wordCount": 505,
          "title": "Multi-Stream Transformers. (arXiv:2107.10342v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.13327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayor_O/0/1/0/all/0/1\">Oriol Barbany Mayor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellini_V/0/1/0/all/0/1\">Vito Bellini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchholz_A/0/1/0/all/0/1\">Alexander Buchholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benedetto_G/0/1/0/all/0/1\">Giuseppe Di Benedetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granziol_D/0/1/0/all/0/1\">Diego Marco Granziol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruffini_M/0/1/0/all/0/1\">Matteo Ruffini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_Y/0/1/0/all/0/1\">Yannik Stein</a>",
          "description": "Learning-to-rank (LTR) algorithms are ubiquitous and necessary to explore the\nextensive catalogs of media providers. To avoid the user examining all the\nresults, its preferences are used to provide a subset of relatively small size.\nThe user preferences can be inferred from the interactions with the presented\ncontent if explicit ratings are unavailable. However, directly using implicit\nfeedback can lead to learning wrong relevance models and is known as biased\nLTR. The mismatch between implicit feedback and true relevances is due to\nvarious nuisances, with position bias one of the most relevant. Position bias\nmodels consider that the lack of interaction with a presented item is not only\nattributed to the item being irrelevant but because the item was not examined.\nThis paper introduces a method for modeling the probability of an item being\nseen in different contexts, e.g., for different users, with a single estimator.\nOur suggested method, denoted as contextual (EM)-based regression, is\nranker-agnostic and able to correctly learn the latent examination\nprobabilities while only using implicit feedback. Our empirical results\nindicate that the method introduced in this paper outperforms other existing\nposition bias estimators in terms of relative error when the examination\nprobability varies across queries. Moreover, the estimated values provide a\nranking performance boost when used to debias the implicit ranking data even if\nthere is no context dependency on the examination probabilities.",
          "link": "http://arxiv.org/abs/2107.13327",
          "publishedOn": "2021-07-29T02:00:06.739Z",
          "wordCount": 661,
          "title": "Ranker-agnostic Contextual Position Bias Estimation. (arXiv:2107.13327v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Farwa K. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1\">Adrian Flanagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kuan E. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1\">Zareen Alamgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1\">Muhammad Ammad-Ud-Din</a>",
          "description": "We introduce the payload optimization method for federated recommender\nsystems (FRS). In federated learning (FL), the global model payload that is\nmoved between the server and users depends on the number of items to recommend.\nThe model payload grows when there is an increasing number of items. This\nbecomes challenging for an FRS if it is running in production mode. To tackle\nthe payload challenge, we formulated a multi-arm bandit solution that selected\npart of the global model and transmitted it to all users. The selection process\nwas guided by a novel reward function suitable for FL systems. So far as we are\naware, this is the first optimization method that seeks to address item\ndependent payloads. The method was evaluated using three benchmark\nrecommendation datasets. The empirical validation confirmed that the proposed\nmethod outperforms the simpler methods that do not benefit from the bandits for\nthe purpose of item selection. In addition, we have demonstrated the usefulness\nof our proposed method by rigorously evaluating the effects of a payload\nreduction on the recommendation performance degradation. Our method achieved up\nto a 90\\% reduction in model payload, yielding only a $\\sim$4\\% - 8\\% loss in\nthe recommendation performance for highly sparse datasets",
          "link": "http://arxiv.org/abs/2107.13078",
          "publishedOn": "2021-07-29T02:00:06.672Z",
          "wordCount": 674,
          "title": "A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-07-29T02:00:06.660Z",
          "wordCount": 725,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>",
          "description": "Collaborative filtering models based on matrix factorization and learned\nsimilarities using Artificial Neural Networks (ANNs) have gained significant\nattention in recent years. This is, in part, because ANNs have demonstrated\ngood results in a wide variety of recommendation tasks. The introduction of\nANNs within the recommendation ecosystem has been recently questioned, raising\nseveral comparisons in terms of efficiency and effectiveness. One aspect most\nof these comparisons have in common is their focus on accuracy, neglecting\nother evaluation dimensions important for the recommendation, such as novelty,\ndiversity, or accounting for biases. We replicate experiments from three papers\nthat compare Neural Collaborative Filtering (NCF) and Matrix Factorization\n(MF), to extend the analysis to other evaluation dimensions. Our contribution\nshows that the experiments are entirely reproducible, and we extend the study\nincluding other accuracy metrics and two statistical hypothesis tests. We\ninvestigated the Diversity and Novelty of the recommendations, showing that MF\nprovides a better accuracy also on the long tail, although NCF provides a\nbetter item coverage and more diversified recommendations. We discuss the bias\neffect generated by the tested methods. They show a relatively small bias, but\nother recommendation baselines, with competitive accuracy performance,\nconsistently show to be less affected by this issue. This is the first work, to\nthe best of our knowledge, where several evaluation dimensions have been\nexplored for an array of SOTA algorithms covering recent adaptations of ANNs\nand MF. Hence, we show the potential these techniques may have on\nbeyond-accuracy evaluation while analyzing the effect on reproducibility these\ncomplementary dimensions may spark. Available at\ngithub.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization",
          "link": "http://arxiv.org/abs/2107.13472",
          "publishedOn": "2021-07-29T02:00:06.641Z",
          "wordCount": 710,
          "title": "Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dantong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>",
          "description": "Graph-based algorithms have shown great empirical potential for the\napproximate nearest neighbor (ANN) search problem. Currently, graph-based ANN\nsearch algorithms are designed mainly using heuristics, whereas theoretical\nanalysis of such algorithms is quite lacking. In this paper, we study a\nfundamental model of proximity graphs used in graph-based ANN search, called\nMonotonic Relative Neighborhood Graph (MRNG), from a theoretical perspective.\nWe use mathematical proofs to explain why proximity graphs that are built based\non MRNG tend to have good searching performance. We also run experiments on\nMRNG and graphs generalizing MRNG to obtain a deeper understanding of the\nmodel. Our experiments give guidance on how to approximate and generalize MRNG\nto build proximity graphs on a large scale. In addition, we discover and study\na hidden structure of MRNG called conflicting nodes, and we give theoretical\nevidence how conflicting nodes could be used to improve ANN search methods that\nare based on MRNG.",
          "link": "http://arxiv.org/abs/2107.13052",
          "publishedOn": "2021-07-29T02:00:06.609Z",
          "wordCount": 584,
          "title": "Understanding and Generalizing Monotonic Proximity Graphs for Approximate Nearest Neighbor Search. (arXiv:2107.13052v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_V/0/1/0/all/0/1\">Vivek Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witteveen_S/0/1/0/all/0/1\">Sam Witteveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_M/0/1/0/all/0/1\">Martin Andrews</a>",
          "description": "Creating explanations for answers to science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. This\nyear, to refocus the Textgraphs Shared Task on the problem of gathering\nrelevant statements (rather than solely finding a single 'correct path'), the\nWorldTree dataset was augmented with expert ratings of 'relevance' of\nstatements to each overall explanation. Our system, which achieved second place\non the Shared Task leaderboard, combines initial statement retrieval; language\nmodels trained to predict the relevance scores; and ensembling of a number of\nthe resulting rankings. Our code implementation is made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2021",
          "link": "http://arxiv.org/abs/2107.13031",
          "publishedOn": "2021-07-29T02:00:06.591Z",
          "wordCount": 570,
          "title": "Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings. (arXiv:2107.13031v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1\">Alexander Dallmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1\">Daniel Zoller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>",
          "description": "At the present time, sequential item recommendation models are compared by\ncalculating metrics on a small item subset (target set) to speed up\ncomputation. The target set contains the relevant item and a set of negative\nitems that are sampled from the full item set. Two well-known strategies to\nsample negative items are uniform random sampling and sampling by popularity to\nbetter approximate the item frequency distribution in the dataset. Most\nrecently published papers on sequential item recommendation rely on sampling by\npopularity to compare the evaluated models. However, recent work has already\nshown that an evaluation with uniform random sampling may not be consistent\nwith the full ranking, that is, the model ranking obtained by evaluating a\nmetric using the full item set as target set, which raises the question whether\nthe ranking obtained by sampling by popularity is equal to the full ranking. In\nthis work, we re-evaluate current state-of-the-art sequential recommender\nmodels from the point of view, whether these sampling strategies have an impact\non the final ranking of the models. We therefore train four recently proposed\nsequential recommendation models on five widely known datasets. For each\ndataset and model, we employ three evaluation strategies. First, we compute the\nfull model ranking. Then we evaluate all models on a target set sampled by the\ntwo different sampling strategies, uniform random sampling and sampling by\npopularity with the commonly used target set size of 100, compute the model\nranking for each strategy and compare them with each other. Additionally, we\nvary the size of the sampled target set. Overall, we find that both sampling\nstrategies can produce inconsistent rankings compared with the full ranking of\nthe models. Furthermore, both sampling by popularity and uniform random\nsampling do not consistently produce the same ranking ...",
          "link": "http://arxiv.org/abs/2107.13045",
          "publishedOn": "2021-07-29T02:00:06.544Z",
          "wordCount": 740,
          "title": "A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:30.601Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_L/0/1/0/all/0/1\">Luis Alberto Robles Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callahan_T/0/1/0/all/0/1\">Tiffany J. Callahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_J/0/1/0/all/0/1\">Juan M. Banda</a>",
          "description": "The use of social media data, like Twitter, for biomedical research has been\ngradually increasing over the years. With the COVID-19 pandemic, researchers\nhave turned to more nontraditional sources of clinical data to characterize the\ndisease in near real-time, study the societal implications of interventions, as\nwell as the sequelae that recovered COVID-19 cases present (Long-COVID).\nHowever, manually curated social media datasets are difficult to come by due to\nthe expensive costs of manual annotation and the efforts needed to identify the\ncorrect texts. When datasets are available, they are usually very small and\ntheir annotations do not generalize well over time or to larger sets of\ndocuments. As part of the 2021 Biomedical Linked Annotation Hackathon, we\nrelease our dataset of over 120 million automatically annotated tweets for\nbiomedical research purposes. Incorporating best practices, we identify tweets\nwith potentially high clinical relevance. We evaluated our work by comparing\nseveral SpaCy-based annotation frameworks against a manually annotated\ngold-standard dataset. Selecting the best method to use for automatic\nannotation, we then annotated 120 million tweets and released them publicly for\nfuture downstream usage within the biomedical domain.",
          "link": "http://arxiv.org/abs/2107.12565",
          "publishedOn": "2021-07-28T02:02:30.579Z",
          "wordCount": 672,
          "title": "A Biomedically oriented automatically annotated Twitter COVID-19 Dataset. (arXiv:2107.12565v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xueli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Modeling user preference from his historical sequences is one of the core\nproblems of sequential recommendation. Existing methods in this field are\nwidely distributed from conventional methods to deep learning methods. However,\nmost of them only model users' interests within their own sequences and ignore\nthe dynamic collaborative signals among different user sequences, making it\ninsufficient to explore users' preferences. We take inspiration from dynamic\ngraph neural networks to cope with this challenge, modeling the user sequence\nand dynamic collaborative signals into one framework. We propose a new method\nnamed Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which\nconnects different user sequences through a dynamic graph structure, exploring\nthe interactive behavior of users and items with time and order information.\nFurthermore, we design a Dynamic Graph Recommendation Network to extract user's\npreferences from the dynamic graph. Consequently, the next-item prediction task\nin sequential recommendation is converted into a link prediction between the\nuser node and the item node in a dynamic graph. Extensive experiments on three\npublic benchmarks show that DGSR outperforms several state-of-the-art methods.\nFurther studies demonstrate the rationality and effectiveness of modeling user\nsequences through a dynamic graph.",
          "link": "http://arxiv.org/abs/2104.07368",
          "publishedOn": "2021-07-28T02:02:30.556Z",
          "wordCount": 653,
          "title": "Dynamic Graph Neural Networks for Sequential Recommendation. (arXiv:2104.07368v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bobadilla_J/0/1/0/all/0/1\">Jes&#xfa;s Bobadilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1\">Fernando Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_A/0/1/0/all/0/1\">Abraham Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>",
          "description": "Deep learning provides accurate collaborative filtering models to improve\nrecommender system results. Deep matrix factorization and their related\ncollaborative neural networks are the state-of-art in the field; nevertheless,\nboth models lack the necessary stochasticity to create the robust, continuous,\nand structured latent spaces that variational autoencoders exhibit. On the\nother hand, data augmentation through variational autoencoder does not provide\naccurate results in the collaborative filtering field due to the high sparsity\nof recommender systems. Our proposed models apply the variational concept to\ninject stochasticity in the latent space of the deep architecture, introducing\nthe variational technique in the neural collaborative filtering field. This\nmethod does not depend on the particular model used to generate the latent\nrepresentation. In this way, this approach can be applied as a plugin to any\ncurrent and future specific models. The proposed models have been tested using\nfour representative open datasets, three different quality measures, and\nstate-of-art baselines. The results show the superiority of the proposed\napproach in scenarios where the variational enrichment exceeds the injected\nnoise effect. Additionally, a framework is provided to enable the\nreproducibility of the conducted experiments.",
          "link": "http://arxiv.org/abs/2107.12677",
          "publishedOn": "2021-07-28T02:02:30.538Z",
          "wordCount": 637,
          "title": "Deep Variational Models for Collaborative Filtering-based Recommender Systems. (arXiv:2107.12677v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>",
          "description": "The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary\nwidely used in biomedical knowledge systems, particularly for semantic indexing\nof scientific literature. As the MeSH hierarchy evolves through annual version\nupdates, some new descriptors are introduced that were not previously\navailable. This paper explores the conceptual provenance of these new\ndescriptors. In particular, we investigate whether such new descriptors have\nbeen previously covered by older descriptors and what is their current relation\nto them. To this end, we propose a framework to categorize new descriptors\nbased on their current relation to older descriptors. Based on the proposed\nclassification scheme, we quantify, analyse and present the different types of\nnew descriptors introduced in MeSH during the last fifteen years. The results\nshow that only about 25% of new MeSH descriptors correspond to new emerging\nconcepts, whereas the rest were previously covered by one or more existing\ndescriptors, either implicitly or explicitly. Most of them were covered by a\nsingle existing descriptor and they usually end up as descendants of it in the\ncurrent hierarchy, gradually leading towards a more fine-grained MeSH\nvocabulary. These insights about the dynamics of the thesaurus are useful for\nthe retrospective study of scientific articles annotated with MeSH, but could\nalso be used to inform the policy of updating the thesaurus in the future.",
          "link": "http://arxiv.org/abs/2101.08293",
          "publishedOn": "2021-07-28T02:02:30.319Z",
          "wordCount": 718,
          "title": "What is all this new MeSH about? Exploring the semantic provenance of new descriptors in the MeSH thesaurus. (arXiv:2101.08293v3 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qingyun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">PengTao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junlin Zhang</a>",
          "description": "Click-through rate (CTR) estimation is a fundamental task in personalized\nadvertising and recommender systems and it's important for ranking models to\neffectively capture complex high-order features.Inspired by the success of ELMO\nand Bert in NLP field, which dynamically refine word embedding according to the\ncontext sentence information where the word appears, we think it's also\nimportant to dynamically refine each feature's embedding layer by layer\naccording to the context information contained in input instance in CTR\nestimation tasks. We can effectively capture the useful feature interactions\nfor each feature in this way. In this paper, We propose a novel CTR Framework\nnamed ContextNet that implicitly models high-order feature interactions by\ndynamically refining each feature's embedding according to the input context.\nSpecifically, ContextNet consists of two key components: contextual embedding\nmodule and ContextNet block. Contextual embedding module aggregates contextual\ninformation for each feature from input instance and ContextNet block maintains\neach feature's embedding layer by layer and dynamically refines its\nrepresentation by merging contextual high-order interaction information into\nfeature embedding. To make the framework specific, we also propose two\nmodels(ContextNet-PFFN and ContextNet-SFFN) under this framework by introducing\nlinear contextual embedding network and two non-linear mapping sub-network in\nContextNet block. We conduct extensive experiments on four real-world datasets\nand the experiment results demonstrate that our proposed ContextNet-PFFN and\nContextNet-SFFN model outperform state-of-the-art models such as DeepFM and\nxDeepFM significantly.",
          "link": "http://arxiv.org/abs/2107.12025",
          "publishedOn": "2021-07-27T02:03:31.561Z",
          "wordCount": 678,
          "title": "ContextNet: A Click-Through Rate Prediction Framework Using Contextual information to Refine Feature Embedding. (arXiv:2107.12025v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanci Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tianming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yujie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donohue_L/0/1/0/all/0/1\">Lawrence Donohue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>",
          "description": "The quarterly financial statement, or Form 10-Q, is one of the most\nfrequently required filings for US public companies to disclose financial and\nother important business information. Due to the massive volume of 10-Q filings\nand the enormous variations in the reporting format, it has been a\nlong-standing challenge to retrieve item-specific information from 10-Q filings\nthat lack machine-readable hierarchy. This paper presents a solution for\nitemizing 10-Q files by complementing a rule-based algorithm with a\nConvolutional Neural Network (CNN) image classifier. This solution demonstrates\na pipeline that can be generalized to a rapid data retrieval solution among a\nlarge volume of textual data using only typographic items. The extracted\ntextual data can be used as unlabeled content-specific data to train\ntransformer models (e.g., BERT) or fit into various field-focus natural\nlanguage processing (NLP) applications.",
          "link": "http://arxiv.org/abs/2104.11783",
          "publishedOn": "2021-07-27T02:03:31.526Z",
          "wordCount": 613,
          "title": "Form 10-Q Itemization. (arXiv:2104.11783v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qingyun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junlin Zhang</a>",
          "description": "Click-through rate (CTR) prediction plays important role in personalized\nadvertising and recommender systems. Though many models have been proposed such\nas FM, FFM and DeepFM in recent years, feature engineering is still a very\nimportant way to improve the model performance in many applications because\nusing raw features can rarely lead to optimal results. For example, the\ncontinuous features are usually transformed to the power forms by adding a new\nfeature to allow it to easily form non-linear functions of the feature.\nHowever, this kind of feature engineering heavily relies on peoples experience\nand it is both time consuming and labor consuming. On the other side, concise\nCTR model with both fast online serving speed and good model performance is\ncritical for many real life applications. In this paper, we propose LeafFM\nmodel based on FM to generate new features from the original feature embedding\nby learning the transformation functions automatically. We also design three\nconcrete Leaf-FM models according to the different strategies of combing the\noriginal and the generated features. Extensive experiments are conducted on\nthree real-world datasets and the results show Leaf-FM model outperforms\nstandard FMs by a large margin. Compared with FFMs, Leaf-FM can achieve\nsignificantly better performance with much less parameters. In Avazu and\nMalware dataset, add version Leaf-FM achieves comparable performance with some\ndeep learning based models such as DNN and AutoInt. As an improved FM model,\nLeaf-FM has the same computation complexity with FM in online serving phase and\nit means Leaf-FM is applicable in many industry applications because of its\nbetter performance and high computation efficiency.",
          "link": "http://arxiv.org/abs/2107.12024",
          "publishedOn": "2021-07-27T02:03:31.517Z",
          "wordCount": 700,
          "title": "Leaf-FM: A Learnable Feature Generation Factorization Machine for Click-Through Rate Prediction. (arXiv:2107.12024v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qingyun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junlin Zhang</a>",
          "description": "Click-Through Rate(CTR) estimation has become one of the most fundamental\ntasks in many real-world applications and it's important for ranking models to\neffectively capture complex high-order features. Shallow feed-forward network\nis widely used in many state-of-the-art DNN models such as FNN, DeepFM and\nxDeepFM to implicitly capture high-order feature interactions. However, some\nresearch has proved that addictive feature interaction, particular feed-forward\nneural networks, is inefficient in capturing common feature interaction. To\nresolve this problem, we introduce specific multiplicative operation into DNN\nranking system by proposing instance-guided mask which performs element-wise\nproduct both on the feature embedding and feed-forward layers guided by input\ninstance. We also turn the feed-forward layer in DNN model into a mixture of\naddictive and multiplicative feature interactions by proposing MaskBlock in\nthis paper. MaskBlock combines the layer normalization, instance-guided mask,\nand feed-forward layer and it is a basic building block to be used to design\nnew ranking model under various configurations. The model consisting of\nMaskBlock is called MaskNet in this paper and two new MaskNet models are\nproposed to show the effectiveness of MaskBlock as basic building block for\ncomposing high performance ranking systems. The experiment results on three\nreal-world datasets demonstrate that our proposed MaskNet models outperform\nstate-of-the-art models such as DeepFM and xDeepFM significantly, which implies\nMaskBlock is an effective basic building unit for composing new high\nperformance ranking systems.",
          "link": "http://arxiv.org/abs/2102.07619",
          "publishedOn": "2021-07-27T02:03:31.467Z",
          "wordCount": 701,
          "title": "MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask. (arXiv:2102.07619v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>",
          "description": "Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation and state-of-the-art\nlanguage technologies. We illustrate and evaluate the system using a COVID-19\nvaccine information case study.",
          "link": "http://arxiv.org/abs/2107.12079",
          "publishedOn": "2021-07-27T02:03:31.455Z",
          "wordCount": 541,
          "title": "An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1\">Mokanarangan Thayaparan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>",
          "description": "Regenerating natural language explanations for science questions is a\nchallenging task for evaluating complex multi-hop and abductive inference\ncapabilities. In this setting, Transformers trained on human-annotated\nexplanations achieve state-of-the-art performance when adopted as cross-encoder\narchitectures. However, while much attention has been devoted to the quality of\nthe constructed explanations, the problem of performing abductive inference at\nscale is still under-studied. As intrinsically not scalable, the cross-encoder\narchitectural paradigm is not suitable for efficient multi-hop inference on\nmassive facts banks. To maximise both accuracy and inference time, we propose a\nhybrid abductive solver that autoregressively combines a dense bi-encoder with\na sparse model of explanatory power, computed leveraging explicit patterns in\nthe explanations. Our experiments demonstrate that the proposed framework can\nachieve performance comparable with the state-of-the-art cross-encoder while\nbeing $\\approx 50$ times faster and scalable to corpora of millions of facts.\nMoreover, we study the impact of the hybridisation on semantic drift and\nscience question answering without additional training, showing that it boosts\nthe quality of the explanations and contributes to improved downstream\ninference performance.",
          "link": "http://arxiv.org/abs/2107.11879",
          "publishedOn": "2021-07-27T02:03:31.436Z",
          "wordCount": 617,
          "title": "Hybrid Autoregressive Solver for Scalable Abductive Natural Language Inference. (arXiv:2107.11879v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+AlGhamdi_K/0/1/0/all/0/1\">Kholoud AlGhamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1\">Elena Simperl</a>",
          "description": "Wikidata is an open knowledge graph built by a global community of\nvolunteers. As it advances in scale, it faces substantial challenges around\neditor engagement. These challenges are in terms of both attracting new editors\nto keep up with the sheer amount of work and retaining existing editors.\nExperience from other online communities and peer-production systems, including\nWikipedia, suggests that personalised recommendations could help, especially\nnewcomers, who are sometimes unsure about how to contribute best to an ongoing\neffort. For this reason, we propose a recommender system WikidataRec for\nWikidata items. The system uses a hybrid of content-based and collaborative\nfiltering techniques to rank items for editors relying on both item features\nand item-editor previous interaction. A neural network, named a neural mixture\nof representations, is designed to learn fine weights for the combination of\nitem-based representations and optimize them with editor-based representation\nby item-editor interaction. To facilitate further research in this space, we\nalso create two benchmark datasets, a general-purpose one with 220,000 editors\nresponsible for 14 million interactions with 4 million items and a second one\nfocusing on the contributions of more than 8,000 more active editors. We\nperform an offline evaluation of the system on both datasets with promising\nresults. Our code and datasets are available at\nhttps://github.com/WikidataRec-developer/Wikidata_Recommender.",
          "link": "http://arxiv.org/abs/2107.06423",
          "publishedOn": "2021-07-27T02:03:31.411Z",
          "wordCount": 665,
          "title": "Learning to Recommend Items to Wikidata Editors. (arXiv:2107.06423v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-27T02:03:31.358Z",
          "wordCount": 717,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knees_P/0/1/0/all/0/1\">Peter Knees</a>",
          "description": "The music domain is among the most important ones for adopting recommender\nsystems technology. In contrast to most other recommendation domains, which\npredominantly rely on collaborative filtering (CF) techniques, music\nrecommenders have traditionally embraced content-based (CB) approaches. In the\npast years, music recommendation models that leverage collaborative and content\ndata -- which we refer to as content-driven models -- have been replacing pure\nCF or CB models.\n\nIn this survey, we review 47 articles on content-driven music recommendation.\nBased on a thorough literature analysis, we first propose an onion model\ncomprising five layers, each of which corresponds to a category of music\ncontent we identified: signal, embedded metadata, expert-generated content,\nuser-generated content, and derivative content. We provide a detailed\ncharacterization of each category along several dimensions. Second, we identify\nsix overarching challenges, according to which we organize our main discussion:\nincreasing recommendation diversity and novelty, providing transparency and\nexplanations, accomplishing context-awareness, recommending sequences of music,\nimproving scalability and efficiency, and alleviating cold start. Each article\naddressing one or more of these challenges is categorized according to the\ncontent layers of our onion model, the article's goal(s), and main\nmethodological choices. Furthermore, articles are discussed in temporal order\nto shed light on the evolution of content-driven music recommendation\nstrategies. Finally, we provide our personal selection of the persisting grand\nchallenges, which are still waiting to be solved in future research endeavors.",
          "link": "http://arxiv.org/abs/2107.11803",
          "publishedOn": "2021-07-27T02:03:30.906Z",
          "wordCount": 669,
          "title": "Content-based Music Recommendation: Evolution, State of the Art, and Challenges. (arXiv:2107.11803v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roitero_K/0/1/0/all/0/1\">Kevin Roitero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soprano_M/0/1/0/all/0/1\">Michael Soprano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1\">Beatrice Portelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luise_M/0/1/0/all/0/1\">Massimiliano De Luise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1\">Damiano Spina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mea_V/0/1/0/all/0/1\">Vincenzo Della Mea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizzaro_S/0/1/0/all/0/1\">Stefano Mizzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demartini_G/0/1/0/all/0/1\">Gianluca Demartini</a>",
          "description": "Recently, the misinformation problem has been addressed with a\ncrowdsourcing-based approach: to assess the truthfulness of a statement,\ninstead of relying on a few experts, a crowd of non-expert is exploited. We\nstudy whether crowdsourcing is an effective and reliable method to assess\ntruthfulness during a pandemic, targeting statements related to COVID-19, thus\naddressing (mis)information that is both related to a sensitive and personal\nissue and very recent as compared to when the judgment is done. In our\nexperiments, crowd workers are asked to assess the truthfulness of statements,\nand to provide evidence for the assessments. Besides showing that the crowd is\nable to accurately judge the truthfulness of the statements, we report results\non workers behavior, agreement among workers, effect of aggregation functions,\nof scales transformations, and of workers background and bias. We perform a\nlongitudinal study by re-launching the task multiple times with both novice and\nexperienced workers, deriving important insights on how the behavior and\nquality change over time. Our results show that: workers are able to detect and\nobjectively categorize online (mis)information related to COVID-19; both\ncrowdsourced and expert judgments can be transformed and aggregated to improve\nquality; worker background and other signals (e.g., source of information,\nbehavior) impact the quality of the data. The longitudinal study demonstrates\nthat the time-span has a major effect on the quality of the judgments, for both\nnovice and experienced workers. Finally, we provide an extensive failure\nanalysis of the statements misjudged by the crowd-workers.",
          "link": "http://arxiv.org/abs/2107.11755",
          "publishedOn": "2021-07-27T02:03:30.714Z",
          "wordCount": 784,
          "title": "Can the Crowd Judge Truthfulness? A Longitudinal Study on Recent Misinformation about COVID-19. (arXiv:2107.11755v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1\">Junhao Gan</a>",
          "description": "User and item attributes are essential side-information; their interactions\n(i.e., their co-occurrence in the sample data) can significantly enhance\nprediction accuracy in various recommender systems. We identify two different\ntypes of attribute interactions, inner interactions and cross interactions:\ninner interactions are those between only user attributes or those between only\nitem attributes; cross interactions are those between user attributes and item\nattributes. Existing models do not distinguish these two types of attribute\ninteractions, which may not be the most effective way to exploit the\ninformation carried by the interactions. To address this drawback, we propose a\nneural Graph Matching based Collaborative Filtering model (GMCF), which\neffectively captures the two types of attribute interactions through modeling\nand aggregating attribute interactions in a graph matching structure for\nrecommendation. In our model, the two essential recommendation procedures,\ncharacteristic learning and preference matching, are explicitly conducted\nthrough graph learning (based on inner interactions) and node matching (based\non cross interactions), respectively. Experimental results show that our model\noutperforms state-of-the-art models. Further studies verify the effectiveness\nof GMCF in improving the accuracy of recommendation.",
          "link": "http://arxiv.org/abs/2105.04067",
          "publishedOn": "2021-07-26T02:00:57.053Z",
          "wordCount": 648,
          "title": "Neural Graph Matching based Collaborative Filtering. (arXiv:2105.04067v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Axel_M/0/1/0/all/0/1\">Marmoret Axel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nancy_B/0/1/0/all/0/1\">Bertin Nancy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeremy_C/0/1/0/all/0/1\">Cohen Jeremy</a>",
          "description": "Music is an art, perceived in unique ways by every listener, coming from\nacoustic signals. In the meantime, standards as musical scores exist to\ndescribe it. Even if humans can make this transcription, it is costly in terms\nof time and efforts, even more with the explosion of information consecutively\nto the rise of the Internet. In that sense, researches are driven in the\ndirection of Automatic Music Transcription. While this task is considered\nsolved in the case of single notes, it is still open when notes superpose\nthemselves, forming chords. This report aims at developing some of the existing\ntechniques towards Music Transcription, particularly matrix factorization, and\nintroducing the concept of multi-channel automatic music transcription. This\nconcept will be explored with mathematical objects called tensors.",
          "link": "http://arxiv.org/abs/2107.11250",
          "publishedOn": "2021-07-26T02:00:57.033Z",
          "wordCount": 581,
          "title": "Multi-Channel Automatic Music Transcription Using Tensor Algebra. (arXiv:2107.11250v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1\">Jonathan Stray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vendrov_I/0/1/0/all/0/1\">Ivan Vendrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nixon_J/0/1/0/all/0/1\">Jeremy Nixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1\">Steven Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>",
          "description": "We describe cases where real recommender systems were modified in the service\nof various human values such as diversity, fairness, well-being, time well\nspent, and factual accuracy. From this we identify the current practice of\nvalues engineering: the creation of classifiers from human-created data with\nvalue-based labels. This has worked in practice for a variety of issues, but\nproblems are addressed one at a time, and users and other stakeholders have\nseldom been involved. Instead, we look to AI alignment work for approaches that\ncould learn complex values directly from stakeholders, and identify four major\ndirections: useful measures of alignment, participatory design and operation,\ninteractive value learning, and informed deliberative judgments.",
          "link": "http://arxiv.org/abs/2107.10939",
          "publishedOn": "2021-07-26T02:00:56.914Z",
          "wordCount": 571,
          "title": "What are you optimizing for? Aligning Recommender Systems with Human Values. (arXiv:2107.10939v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oncescu_A/0/1/0/all/0/1\">Andreea-Maria Oncescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1\">A. Sophia Koepke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jo&#xe3;o F. Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>",
          "description": "We consider the task of retrieving audio using free-form natural language\nqueries. To study this problem, which has received limited attention in the\nexisting literature, we introduce challenging new benchmarks for text-based\naudio retrieval using text annotations sourced from the Audiocaps and Clotho\ndatasets. We then employ these benchmarks to establish baselines for\ncross-modal audio retrieval, where we demonstrate the benefits of pre-training\non diverse audio tasks. We hope that our benchmarks will inspire further\nresearch into cross-modal text-based audio retrieval with free-form text\nqueries.",
          "link": "http://arxiv.org/abs/2105.02192",
          "publishedOn": "2021-07-23T02:00:29.761Z",
          "wordCount": 561,
          "title": "Audio Retrieval with Natural Language Queries. (arXiv:2105.02192v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1\">Yeseul Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_D/0/1/0/all/0/1\">Dongjun Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jina Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_I/0/1/0/all/0/1\">Ick Hoon Jin</a>",
          "description": "Since the emergence of the worldwide pandemic of COVID-19, relevant research\nhas been published at a dazzling pace, which makes it hard to follow the\nresearch in this area without dedicated efforts. It is practically impossible\nto implement this task manually due to the high volume of the relevant\nliterature. Text mining has been considered to be a powerful approach to\naddress this challenge, especially the topic modeling, a well-known\nunsupervised method that aims to reveal latent topics from the literature.\nHowever, in spite of its potential utility, the results generated from this\napproach are often investigated manually. Hence, its application to the\nCOVID-19 literature is not straightforward and expert knowledge is needed to\nmake meaningful interpretations. In order to address these challenges, we\npropose a novel analytical framework for estimating topic interactions and\neffective visualization for topic interpretation. Here we assumed that topics\nconstituting a paper can be positioned on an interaction map, which belongs to\na high-dimensional Euclidean space. Based on this assumption, after summarizing\ntopics with their topic-word distributions using the biterm topic model, we\nmapped these latent topics on networks to visualize relationships among the\ntopics. Moreover, in the proposed approach, the change of relationships among\ntopics can be traced using a trajectory plot generated with different levels of\nword richness. These results together provide deeply mined and intuitive\nrepresentation of relationships among topics related to a specific research\narea. The application of this proposed framework to the PubMed literature shows\nthat our approach facilitates understanding of the topics constituting the\nCOVID-19 knowledge.",
          "link": "http://arxiv.org/abs/2106.07374",
          "publishedOn": "2021-07-23T02:00:29.433Z",
          "wordCount": 770,
          "title": "Network-based Trajectory Topic Interaction Map for Text Mining of COVID-19 Biomedical Literature. (arXiv:2106.07374v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>",
          "description": "When experiencing an information need, users want to engage with a domain\nexpert, but often turn to an information retrieval system, such as a search\nengine, instead. Classical information retrieval systems do not answer\ninformation needs directly, but instead provide references to (hopefully\nauthoritative) answers. Successful question answering systems offer a limited\ncorpus created on-demand by human experts, which is neither timely nor\nscalable. Pre-trained language models, by contrast, are capable of directly\ngenerating prose that may be responsive to an information need, but at present\nthey are dilettantes rather than domain experts -- they do not have a true\nunderstanding of the world, they are prone to hallucinating, and crucially they\nare incapable of justifying their utterances by referring to supporting\ndocuments in the corpus they were trained over. This paper examines how ideas\nfrom classical information retrieval and pre-trained language models can be\nsynthesized and evolved into systems that truly deliver on the promise of\ndomain expert advice.",
          "link": "http://arxiv.org/abs/2105.02274",
          "publishedOn": "2021-07-23T02:00:29.401Z",
          "wordCount": 636,
          "title": "Rethinking Search: Making Domain Experts out of Dilettantes. (arXiv:2105.02274v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Doris E. M. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadendla_V/0/1/0/all/0/1\">Venkata Sriram Siddhardh Nadendla</a>",
          "description": "Strategic information design is a framework where a sender designs\ninformation strategically to steer its receiver's decision towards a desired\nchoice. Traditionally, such frameworks have always assumed that the sender and\nthe receiver comprehends the state of the choice environment, and that the\nreceiver always trusts the sender's signal. This paper deviates from these\nassumptions and re-investigates strategic information design in the presence of\ndistrustful receiver and when both sender and receiver cannot\nobserve/comprehend the environment state space. Specifically, we assume that\nboth sender and receiver has access to non-identical beliefs about choice\nrewards (with sender's belief being more accurate), but not the environment\nstate that determines these rewards. Furthermore, given that the receiver does\nnot trust the sender, we also assume that the receiver updates its prior in a\nnon-Bayesian manner. We evaluate the Stackelberg equilibrium and investigate\neffects of information framing (i.e. send complete signal, or just expected\nvalue of the signal) on the equilibrium. Furthermore, we also investigate trust\ndynamics at the receiver, under the assumption that the receiver minimizes\nregret in hindsight. Simulation results are presented to illustrate signaling\neffects and trust dynamics in strategic information design.",
          "link": "http://arxiv.org/abs/2005.05516",
          "publishedOn": "2021-07-23T02:00:29.373Z",
          "wordCount": 709,
          "title": "Framing Effects on Strategic Information Design under Receiver Distrust and Unknown State. (arXiv:2005.05516v2 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Min Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kecheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wange_X/0/1/0/all/0/1\">Xu Wange</a>",
          "description": "To explore the robustness of recommender systems, researchers have proposed\nvarious shilling attack models and analyzed their adverse effects. Primitive\nattacks are highly feasible but less effective due to simplistic handcrafted\nrules, while upgraded attacks are more powerful but costly and difficult to\ndeploy because they require more knowledge from recommendations. In this paper,\nwe explore a novel shilling attack called Graph cOnvolution-based generative\nshilling ATtack (GOAT) to balance the attacks' feasibility and effectiveness.\nGOAT adopts the primitive attacks' paradigm that assigns items for fake users\nby sampling and the upgraded attacks' paradigm that generates fake ratings by a\ndeep learning-based model. It deploys a generative adversarial network (GAN)\nthat learns the real rating distribution to generate fake ratings.\nAdditionally, the generator combines a tailored graph convolution structure\nthat leverages the correlations between co-rated items to smoothen the fake\nratings and enhance their authenticity. The extensive experiments on two public\ndatasets evaluate GOAT's performance from multiple perspectives. Our study of\nthe GOAT demonstrates technical feasibility for building a more powerful and\nintelligent attack model with a much-reduced cost, enables analysis the threat\nof such an attack and guides for investigating necessary prevention measures.",
          "link": "http://arxiv.org/abs/2107.10457",
          "publishedOn": "2021-07-23T02:00:29.336Z",
          "wordCount": 663,
          "title": "Ready for Emerging Threats to Recommender Systems? A Graph Convolution-based Generative Shilling Attack. (arXiv:2107.10457v1 [cs.LG])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:07.070Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1\">Aaron Lopez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>",
          "description": "The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.",
          "link": "http://arxiv.org/abs/2107.13180",
          "publishedOn": "2021-07-29T02:00:06.906Z",
          "wordCount": 712,
          "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:06.786Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13385",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wieckowski_A/0/1/0/all/0/1\">Adam Wieckowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lehmann_C/0/1/0/all/0/1\">Christian Lehmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bross_B/0/1/0/all/0/1\">Benjamin Bross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marpe_D/0/1/0/all/0/1\">Detlev Marpe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biatek_T/0/1/0/all/0/1\">Thibaud Biatek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raulet_M/0/1/0/all/0/1\">Mickael Raulet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feuvre_J/0/1/0/all/0/1\">Jean Le Feuvre</a>",
          "description": "Versatile Video Coding (VVC) is the most recent international video coding\nstandard jointly developed by ITU-T and ISO/IEC, which has been finalized in\nJuly 2020. VVC allows for significant bit-rate reductions around 50% for the\nsame subjective video quality compared to its predecessor, High Efficiency\nVideo Coding (HEVC). One year after finalization, VVC support in devices and\nchipsets is still under development, which is aligned with the typical\ndevelopment cycles of new video coding standards. This paper presents\nopen-source software packages that allow building a complete VVC end-to-end\ntoolchain already one year after its finalization. This includes the Fraunhofer\nHHI VVenC library for fast and efficient VVC encoding as well as HHI's VVdeC\nlibrary for live decoding. An experimental integration of VVC in the GPAC\nsoftware tools and FFmpeg media framework allows packaging VVC bitstreams, e.g.\nencoded with VVenC, in MP4 file format and using DASH for content creation and\nstreaming. The integration of VVdeC allows playback on the receiver. Given\nthese packages, step-by-step tutorials are provided for two possible\napplication scenarios: VVC file encoding plus playback and adaptive streaming\nwith DASH.",
          "link": "http://arxiv.org/abs/2107.13385",
          "publishedOn": "2021-07-29T02:00:06.776Z",
          "wordCount": 652,
          "title": "A Complete End-To-End Open Source Toolchain for the Versatile Video Coding (VVC) Standard. (arXiv:2107.13385v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xiangui Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yun-Qing Shi</a>",
          "description": "A great challenge to steganography has arisen with the wide application of\nsteganalysis methods based on convolutional neural networks (CNNs). To this\nend, embedding cost learning frameworks based on generative adversarial\nnetworks (GANs) have been proposed and achieved success for spatial\nsteganography. However, the application of GAN to JPEG steganography is still\nin the prototype stage; its anti-detectability and training efficiency should\nbe improved. In conventional steganography, research has shown that the\nside-information calculated from the precover can be used to enhance security.\nHowever, it is hard to calculate the side-information without the spatial\ndomain image. In this work, an embedding cost learning framework for JPEG\nSteganography via a Generative Adversarial Network (JS-GAN) has been proposed,\nthe learned embedding cost can be further adjusted asymmetrically according to\nthe estimated side-information. Experimental results have demonstrated that the\nproposed method can automatically learn a content-adaptive embedding cost\nfunction, and use the estimated side-information properly can effectively\nimprove the security performance. For example, under the attack of a classic\nsteganalyzer GFR with quality factor 75 and 0.4 bpnzAC, the proposed JS-GAN can\nincrease the detection error 2.58% over J-UNIWARD, and the estimated\nside-information aided version JS-GAN(ESI) can further increase the security\nperformance by 11.25% over JS-GAN.",
          "link": "http://arxiv.org/abs/2107.13151",
          "publishedOn": "2021-07-29T02:00:06.723Z",
          "wordCount": 638,
          "title": "JPEG Steganography with Embedding Cost Learning and Side-Information Estimation. (arXiv:2107.13151v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">Alessio Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donaher_S/0/1/0/all/0/1\">Santiago Donaher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iashin_V/0/1/0/all/0/1\">Vladimir Iashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1\">Francesca Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solak_G/0/1/0/all/0/1\">G&#xf6;khan Solak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_C/0/1/0/all/0/1\">Claudio Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_R/0/1/0/all/0/1\">Reina Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagao_Y/0/1/0/all/0/1\">Yuichi Nagao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanlin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Rosa H. M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christmann_G/0/1/0/all/0/1\">Guilherme Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jyun-Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeharika_G/0/1/0/all/0/1\">Gonuguntla Neeharika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chinnakotla Krishna Teja Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Dinesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehman_B/0/1/0/all/0/1\">Bakhtawar Ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>",
          "description": "Acoustic and visual sensing can support the contactless estimation of the\nweight of a container and the amount of its content when the container is\nmanipulated by a person. However, transparencies (both of the container and of\nthe content) and the variability of materials, shapes and sizes make this\nproblem challenging. In this paper, we present an open benchmarking framework\nand an in-depth comparative analysis of recent methods that estimate the\ncapacity of a container, as well as the type, mass, and amount of its content.\nThese methods use learned and handcrafted features, such as mel-frequency\ncepstrum coefficients, zero-crossing rate, spectrograms, with different types\nof classifiers to estimate the type and amount of the content with acoustic\ndata, and geometric approaches with visual data to determine the capacity of\nthe container. Results on a newly distributed dataset show that audio alone is\na strong modality and methods achieves a weighted average F1-score up to 81%\nand 97% for content type and level classification, respectively. Estimating the\ncontainer capacity with vision-only approaches and filling mass with\nmulti-modal, multi-stage algorithms reaches up to 65% weighted average capacity\nand mass scores.",
          "link": "http://arxiv.org/abs/2107.12719",
          "publishedOn": "2021-07-28T02:02:30.621Z",
          "wordCount": 692,
          "title": "Multi-modal estimation of the properties of containers and their content: survey and evaluation. (arXiv:2107.12719v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simonetta_F/0/1/0/all/0/1\">Federico Simonetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntalampiras_S/0/1/0/all/0/1\">Stavros Ntalampiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avanzini_F/0/1/0/all/0/1\">Federico Avanzini</a>",
          "description": "Audio-to-score alignment (A2SA) is a multimodal task consisting in the\nalignment of audio signals to music scores. Recent literature confirms the\nbenefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In\nthis work, we aim to elaborate on the exploitation of AMT Deep Learning (DL)\nmodels for achieving alignment at the note-level. We propose a method which\nbenefits from HMM-based score-to-score alignment and AMT, showing a remarkable\nadvancement beyond the state-of-the-art. We design a systematic procedure to\ntake advantage of large datasets which do not offer an aligned score. Finally,\nwe perform a thorough comparison and extensive tests on multiple datasets.",
          "link": "http://arxiv.org/abs/2107.12854",
          "publishedOn": "2021-07-28T02:02:30.546Z",
          "wordCount": 540,
          "title": "Audio-to-Score Alignment Using Deep Automatic Music Transcription. (arXiv:2107.12854v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:30.498Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Menghan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Simon X. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "For people who ardently love painting but unfortunately have visual\nimpairments, holding a paintbrush to create a work is a very difficult task.\nPeople in this special group are eager to pick up the paintbrush, like Leonardo\nda Vinci, to create and make full use of their own talents. Therefore, to\nmaximally bridge this gap, we propose a painting navigation system to assist\nblind people in painting and artistic creation. The proposed system is composed\nof cognitive system and guidance system. The system adopts drawing board\npositioning based on QR code, brush navigation based on target detection and\nbush real-time positioning. Meanwhile, this paper uses human-computer\ninteraction on the basis of voice and a simple but efficient position\ninformation coding rule. In addition, we design a criterion to efficiently\njudge whether the brush reaches the target or not. According to the\nexperimental results, the thermal curves extracted from the faces of testers\nshow that it is relatively well accepted by blindfolded and even blind testers.\nWith the prompt frequency of 1s, the painting navigation system performs best\nwith the completion degree of 89% with SD of 8.37% and overflow degree of 347%\nwith SD of 162.14%. Meanwhile, the excellent and good types of brush tip\ntrajectory account for 74%, and the relative movement distance is 4.21 with SD\nof 2.51. This work demonstrates that it is practicable for the blind people to\nfeel the world through the brush in their hands. In the future, we plan to\ndeploy Angle's Eyes on the phone to make it more portable. The demo video of\nthe proposed painting navigation system is available at:\nhttps://doi.org/10.6084/m9.figshare.9760004.v1.",
          "link": "http://arxiv.org/abs/2107.12921",
          "publishedOn": "2021-07-28T02:02:30.489Z",
          "wordCount": 739,
          "title": "Angel's Girl for Blind Painters: an Efficient Painting Navigation System Validated by Multimodal Evaluation Approach. (arXiv:2107.12921v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Altinisik_E/0/1/0/all/0/1\">Enes Altinisik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sencar_H/0/1/0/all/0/1\">H&#xfc;srev Taha Sencar</a>",
          "description": "We address the problem of decoding video file fragments when the necessary\nencoding parameters are missing. With this objective, we propose a method that\nautomatically generates H.264 video headers containing these parameters and\nextracts coded pictures in the partially available compressed video data. To\naccomplish this, we examined a very large corpus of videos to learn patterns of\nencoding settings commonly used by encoders and created a parameter dictionary.\nFurther, to facilitate a more efficient search our method identifies\ncharacteristics of a coded bitstream to discriminate the entropy coding mode.\nIt also utilizes the application logs created by the decoder to identify\ncorrect parameter values. Evaluation of the effectiveness of the proposed\nmethod on more than 55K videos with diverse provenance shows that it can\ngenerate valid headers on average in 11.3 decoding trials per video. This\nresult represents an improvement by more than a factor of 10 over the\nconventional approach of video header stitching to recover video file\nfragments.",
          "link": "http://arxiv.org/abs/2104.14522",
          "publishedOn": "2021-07-28T02:02:30.469Z",
          "wordCount": 616,
          "title": "Automatic Generation of H.264 Parameter Sets to Recover Video File Fragments. (arXiv:2104.14522v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1711.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_M/0/1/0/all/0/1\">Michael Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>",
          "description": "Matching images and sentences demands a fine understanding of both\nmodalities. In this paper, we propose a new system to discriminatively embed\nthe image and text to a shared visual-textual space. In this field, most\nexisting works apply the ranking loss to pull the positive image / text pairs\nclose and push the negative pairs apart from each other. However, directly\ndeploying the ranking loss is hard for network learning, since it starts from\nthe two heterogeneous features to build inter-modal relationship. To address\nthis problem, we propose the instance loss which explicitly considers the\nintra-modal data distribution. It is based on an unsupervised assumption that\neach image / text group can be viewed as a class. So the network can learn the\nfine granularity from every image/text group. The experiment shows that the\ninstance loss offers better weight initialization for the ranking loss, so that\nmore discriminative embeddings can be learned. Besides, existing works usually\napply the off-the-shelf features, i.e., word2vec and fixed visual feature. So\nin a minor contribution, this paper constructs an end-to-end dual-path\nconvolutional network to learn the image and text representations. End-to-end\nlearning allows the system to directly learn from the data and fully utilize\nthe supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),\nexperiments demonstrate that our method yields competitive accuracy compared to\nstate-of-the-art methods. Moreover, in language based person retrieval, we\nimprove the state of the art by a large margin. The code has been made publicly\navailable.",
          "link": "http://arxiv.org/abs/1711.05535",
          "publishedOn": "2021-07-28T02:02:30.455Z",
          "wordCount": 743,
          "title": "Dual-Path Convolutional Image-Text Embeddings with Instance Loss. (arXiv:1711.05535v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-07-28T02:02:30.421Z",
          "wordCount": 637,
          "title": "MU-MIMO Grouping For Real-time Applications. (arXiv:2106.15262v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuangjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>",
          "description": "Recent deep networks have convincingly demonstrated high capability in crowd\ncounting, which is a critical task attracting widespread attention due to its\nvarious industrial applications. Despite such progress, trained data-dependent\nmodels usually can not generalize well to unseen scenarios because of the\ninherent domain shift. To facilitate this issue, this paper proposes a novel\nadversarial scoring network (ASNet) to gradually bridge the gap across domains\nfrom coarse to fine granularity. In specific, at the coarse-grained stage, we\ndesign a dual-discriminator strategy to adapt source domain to be close to the\ntargets from the perspectives of both global and local feature space via\nadversarial learning. The distributions between two domains can thus be aligned\nroughly. At the fine-grained stage, we explore the transferability of source\ncharacteristics by scoring how similar the source samples are to target ones\nfrom multiple levels based on generative probability derived from coarse stage.\nGuided by these hierarchical scores, the transferable source features are\nproperly selected to enhance the knowledge transfer during the adaptation\nprocess. With the coarse-to-fine design, the generalization bottleneck induced\nfrom the domain discrepancy can be effectively alleviated. Three sets of\nmigration experiments show that the proposed methods achieve state-of-the-art\ncounting performance compared with major unsupervised methods.",
          "link": "http://arxiv.org/abs/2107.12858",
          "publishedOn": "2021-07-28T02:02:30.408Z",
          "wordCount": 657,
          "title": "Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network. (arXiv:2107.12858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deldjoo_Y/0/1/0/all/0/1\">Yashar Deldjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knees_P/0/1/0/all/0/1\">Peter Knees</a>",
          "description": "The music domain is among the most important ones for adopting recommender\nsystems technology. In contrast to most other recommendation domains, which\npredominantly rely on collaborative filtering (CF) techniques, music\nrecommenders have traditionally embraced content-based (CB) approaches. In the\npast years, music recommendation models that leverage collaborative and content\ndata -- which we refer to as content-driven models -- have been replacing pure\nCF or CB models.\n\nIn this survey, we review 47 articles on content-driven music recommendation.\nBased on a thorough literature analysis, we first propose an onion model\ncomprising five layers, each of which corresponds to a category of music\ncontent we identified: signal, embedded metadata, expert-generated content,\nuser-generated content, and derivative content. We provide a detailed\ncharacterization of each category along several dimensions. Second, we identify\nsix overarching challenges, according to which we organize our main discussion:\nincreasing recommendation diversity and novelty, providing transparency and\nexplanations, accomplishing context-awareness, recommending sequences of music,\nimproving scalability and efficiency, and alleviating cold start. Each article\naddressing one or more of these challenges is categorized according to the\ncontent layers of our onion model, the article's goal(s), and main\nmethodological choices. Furthermore, articles are discussed in temporal order\nto shed light on the evolution of content-driven music recommendation\nstrategies. Finally, we provide our personal selection of the persisting grand\nchallenges, which are still waiting to be solved in future research endeavors.",
          "link": "http://arxiv.org/abs/2107.11803",
          "publishedOn": "2021-07-27T02:03:31.853Z",
          "wordCount": 669,
          "title": "Content-based Music Recommendation: Evolution, State of the Art, and Challenges. (arXiv:2107.11803v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuqian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "Given a video demonstration, can we imitate the action contained in this\nvideo? In this paper, we introduce a novel task, dubbed mesh-based action\nimitation. The goal of this task is to enable an arbitrary target human mesh to\nperform the same action shown on the video demonstration. To achieve this, a\nnovel Mesh-based Video Action Imitation (M-VAI) method is proposed by us. M-VAI\nfirst learns to reconstruct the meshes from the given source image frames, then\nthe initial recovered mesh sequence is fed into mesh2mesh, a mesh sequence\nsmooth module proposed by us, to improve the temporal consistency. Finally, we\nimitate the actions by transferring the pose from the constructed human body to\nour target identity mesh. High-quality and detailed human body meshes can be\ngenerated by using our M-VAI. Extensive experiments demonstrate the feasibility\nof our task and the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.11756",
          "publishedOn": "2021-07-27T02:03:31.845Z",
          "wordCount": 599,
          "title": "Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos. (arXiv:2107.11756v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shih-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "Transformers and variational autoencoders (VAE) have been extensively\nemployed for symbolic (e.g., MIDI) domain music generation. While the former\nboast an impressive capability in modeling long sequences, the latter allow\nusers to willingly exert control over different parts (e.g., bars) of the music\nto be generated. In this paper, we are interested in bringing the two together\nto construct a single model that exhibits both strengths. The task is split\ninto two steps. First, we equip Transformer decoders with the ability to accept\nsegment-level, time-varying conditions during sequence generation.\nSubsequently, we combine the developed and tested in-attention decoder with a\nTransformer encoder, and train the resulting MuseMorphose model with the VAE\nobjective to achieve style transfer of long musical pieces, in which users can\nspecify musical attributes including rhythmic intensity and polyphony (i.e.,\nharmonic fullness) they desire, down to the bar level. Experiments show that\nMuseMorphose outperforms recurrent neural network (RNN) based baselines on\nnumerous widely-used metrics for style transfer tasks.",
          "link": "http://arxiv.org/abs/2105.04090",
          "publishedOn": "2021-07-27T02:03:31.771Z",
          "wordCount": 639,
          "title": "MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with One Transformer VAE. (arXiv:2105.04090v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_L/0/1/0/all/0/1\">Lovish Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Sarthak Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Abhijit Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Sandip Chakraborty</a>",
          "description": "With increasing advancements in technologies for capturing 360{\\deg} videos,\nadvances in streaming such videos have become a popular research topic.\nHowever, streaming 360{\\deg} videos require high bandwidth, thus escalating the\nneed for developing optimized streaming algorithms. Researchers have proposed\nvarious methods to tackle the problem, considering the network bandwidth or\nattempt to predict future viewports in advance. However, most of the existing\nworks either (1) do not consider video contents to predict user viewport, or\n(2) do not adapt to user preferences dynamically, or (3) require a lot of\ntraining data for new videos, thus making them potentially unfit for video\nstreaming purposes. We develop PARIMA, a fast and efficient online viewport\nprediction model that uses past viewports of users along with the trajectories\nof prime objects as a representative of video content to predict future\nviewports. We claim that the head movement of a user majorly depends upon the\ntrajectories of the prime objects in the video. We employ a pyramid-based\nbitrate allocation scheme and perform a comprehensive evaluation of the\nperformance of PARIMA. In our evaluation, we show that PARIMA outperforms\nstate-of-the-art approaches, improving the Quality of Experience by over 30\\%\nwhile maintaining a short response time.",
          "link": "http://arxiv.org/abs/2103.00981",
          "publishedOn": "2021-07-27T02:03:31.508Z",
          "wordCount": 664,
          "title": "PARIMA: Viewport Adaptive 360-Degree Video Streaming. (arXiv:2103.00981v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingjing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1\">Zhixiong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>",
          "description": "Encouraging progress has been made towards Visual Question Answering (VQA) in\nrecent years, but it is still challenging to enable VQA models to adaptively\ngeneralize to out-of-distribution (OOD) samples. Intuitively, recompositions of\nexisting visual concepts (i.e., attributes and objects) can generate unseen\ncompositions in the training set, which will promote VQA models to generalize\nto OOD samples. In this paper, we formulate OOD generalization in VQA as a\ncompositional generalization problem and propose a graph generative\nmodeling-based training scheme (X-GGM) to handle the problem implicitly. X-GGM\nleverages graph generative modeling to iteratively generate a relation matrix\nand node representations for the predefined graph that utilizes\nattribute-object pairs as nodes. Furthermore, to alleviate the unstable\ntraining issue in graph generative modeling, we propose a gradient distribution\nconsistency loss to constrain the data distribution with adversarial\nperturbations and the generated distribution. The baseline VQA model (LXMERT)\ntrained with the X-GGM scheme achieves state-of-the-art OOD performance on two\nstandard VQA OOD benchmarks, i.e., VQA-CP v2 and GQA-OOD. Extensive ablation\nstudies demonstrate the effectiveness of X-GGM components.",
          "link": "http://arxiv.org/abs/2107.11576",
          "publishedOn": "2021-07-27T02:03:31.343Z",
          "wordCount": 624,
          "title": "X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering. (arXiv:2107.11576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Biao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guorui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>",
          "description": "Recent advances in linguistic steganalysis have successively applied CNNs,\nRNNs, GNNs and other deep learning models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated steganographic texts and carrier texts in terms of the conditional\nprobability distribution of individual words. Such kind of statistical\ndifference can be naturally captured by the language model used for generating\nsteganographic texts, which drives us to give the classifier a priori knowledge\nof the language model to enhance the steganalysis ability. To this end, we\npresent two methods to efficient linguistic steganalysis in this paper. One is\nto pre-train a language model based on RNN, and the other is to pre-train a\nsequence autoencoder. Experimental results show that the two methods have\ndifferent degrees of performance improvement when compared to the randomly\ninitialized RNN classifier, and the convergence speed is significantly\naccelerated. Moreover, our methods have achieved the best detection results.",
          "link": "http://arxiv.org/abs/2107.12168",
          "publishedOn": "2021-07-27T02:03:31.329Z",
          "wordCount": 615,
          "title": "Exploiting Language Model for Efficient Linguistic Steganalysis: An Empirical Study. (arXiv:2107.12168v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Arun Kumar Singh</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Priyanka Singh</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Nathwani_K/0/1/0/all/0/1\">Karan Nathwani</a> (1) ((1) Indian Institute of Technology Jammu, (2) Dhirubhai Ambani Institute of Information and Communication Technology)",
          "description": "The recent developments in technology have re-warded us with amazing audio\nsynthesis models like TACOTRON and WAVENETS. On the other side, it poses\ngreater threats such as speech clones and deep fakes, that may go undetected.\nTo tackle these alarming situations, there is an urgent need to propose models\nthat can help discriminate a synthesized speech from an actual human speech and\nalso identify the source of such a synthesis. Here, we propose a model based on\nConvolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network\n(BiRNN) that helps to achieve both the aforementioned objectives. The temporal\ndependencies present in AI synthesized speech are exploited using Bidirectional\nRNN and CNN. The model outperforms the state-of-the-art approaches by\nclassifying the AI synthesized audio from real human speech with an error rate\nof 1.9% and detecting the underlying architecture with an accuracy of 97%.",
          "link": "http://arxiv.org/abs/2107.11412",
          "publishedOn": "2021-07-27T02:03:31.313Z",
          "wordCount": 629,
          "title": "Using Deep Learning Techniques and Inferential Speech Statistics for AI Synthesised Speech Recognition. (arXiv:2107.11412v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-07-26T02:00:56.993Z",
          "wordCount": 633,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yingqiang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huanqiang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>",
          "description": "Previous reversible data hiding in encrypted images (RDHEI) schemes can be\neither carried out by vacating room before or after data encryption, which\nleads to a separation of the search field in RDHEI. Besides, high capacity\nrelies heavily on vacating room before encryption (VRBE), which significantly\nlowers the payload of vacating room after encryption (VRAE) based schemes. To\naddress this issue, this paper proposes a framework for high-capacity RDHEI for\nboth VRBE and VRAE cases using pixel predictions and entropy encoding. We\npropose an embedding room generation algorithm to produce vacated room by\ngenerating the prediction-error histogram (PEH) of the selected cover using\nadjacency prediction and the median edge detector (MED). In the VRBE scenario,\nwe propose a scheme that generates the embedding room using the proposed\nalgorithm, and encrypts the preprocessed image by using the stream cipher with\ntwo encryption keys. In the VRAE scenario, we propose a scheme that involves an\nimproved block modulation and permutation encryption algorithm where the\nspatial redundancy in the plain-text image can be largely preserved. Then the\nproposed algorithm is applied on the encrypted image to generate the embedding\nroom. At the data hider's side of both the schemes, the data hider locates the\nembedding room and embeds the encrypted additional data. On receiving the\nmarked encrypted image, the receivers with different authentication can\nrespectively conduct error-free data extraction and/or error-free image\nrecovery. The experimental results show that the two schemes in the proposed\nframework can outperform many previous state-of-the-art RDHEI arts. Besides,\nthe proposed schemes can ensure high information security in that little detail\nof the original image can be directly discovered from the encrypted images or\nthe marked encrypted images.",
          "link": "http://arxiv.org/abs/2102.12613",
          "publishedOn": "2021-07-23T02:00:30.134Z",
          "wordCount": 750,
          "title": "High-Capacity Framework for Reversible Data Hiding in Encrypted Image Using Pixel Predictions and Entropy Encoding. (arXiv:2102.12613v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member/0/1/0/all/0/1\">Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>",
          "description": "Video moment retrieval targets at retrieving a moment in a video for a given\nlanguage query. The challenges of this task include 1) the requirement of\nlocalizing the relevant moment in an untrimmed video, and 2) bridging the\nsemantic gap between textual query and video contents. To tackle those\nproblems, early approaches adopt the sliding window or uniform sampling to\ncollect video clips first and then match each clip with the query. Obviously,\nthese strategies are time-consuming and often lead to unsatisfied accuracy in\nlocalization due to the unpredictable length of the golden moment. To avoid the\nlimitations, researchers recently attempt to directly predict the relevant\nmoment boundaries without the requirement to generate video clips first. One\nmainstream approach is to generate a multimodal feature vector for the target\nquery and video frames (e.g., concatenation) and then use a regression approach\nupon the multimodal feature vector for boundary detection. Although some\nprogress has been achieved by this approach, we argue that those methods have\nnot well captured the cross-modal interactions between the query and video\nframes.\n\nIn this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM)\nmodel which predicts the temporal boundaries based on an interaction modeling.\nIn addition, an attention module is introduced to assign higher weights to\nquery words with richer semantic cues, which are considered to be more\nimportant for finding relevant video contents. Another contribution is that we\npropose an additional predictor to utilize the internal frames in the model\ntraining to improve the localization accuracy. Extensive experiments on two\ndatasets TACoS and Charades-STA demonstrate the superiority of our method over\nseveral state-of-the-art methods. Ablation studies have been also conducted to\nexamine the effectiveness of different modules in our ACRM model.",
          "link": "http://arxiv.org/abs/2009.10434",
          "publishedOn": "2021-07-23T02:00:29.829Z",
          "wordCount": 772,
          "title": "Frame-wise Cross-modal Matching for Video Moment Retrieval. (arXiv:2009.10434v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>",
          "description": "Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",
          "link": "http://arxiv.org/abs/2107.10300",
          "publishedOn": "2021-07-23T02:00:29.795Z",
          "wordCount": 723,
          "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.03592",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1\">Sefik Emre Eskimez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "Visual emotion expression plays an important role in audiovisual speech\ncommunication. In this work, we propose a novel approach to rendering visual\nemotion expression in speech-driven talking face generation. Specifically, we\ndesign an end-to-end talking face generation system that takes a speech\nutterance, a single face image, and a categorical emotion label as input to\nrender a talking face video synchronized with the speech and expressing the\nconditioned emotion. Objective evaluation on image quality, audiovisual\nsynchronization, and visual emotion expression shows that the proposed system\noutperforms a state-of-the-art baseline system. Subjective evaluation of visual\nemotion expression and video realness also demonstrates the superiority of the\nproposed system. Furthermore, we conduct a human emotion recognition pilot\nstudy using generated videos with mismatched emotions among the audio and\nvisual modalities. Results show that humans respond to the visual modality more\nsignificantly than the audio modality on this task.",
          "link": "http://arxiv.org/abs/2008.03592",
          "publishedOn": "2021-07-23T02:00:29.739Z",
          "wordCount": 632,
          "title": "Speech Driven Talking Face Generation from a Single Image and an Emotion Condition. (arXiv:2008.03592v2 [eess.AS] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korschens_M/0/1/0/all/0/1\">Matthias K&#xf6;rschens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodesheim_P/0/1/0/all/0/1\">Paul Bodesheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romermann_C/0/1/0/all/0/1\">Christine R&#xf6;mermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_S/0/1/0/all/0/1\">Solveig Franziska Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migliavacca_M/0/1/0/all/0/1\">Mirco Migliavacca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_J/0/1/0/all/0/1\">Josephine Ulrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "Monitoring the responses of plants to environmental changes is essential for\nplant biodiversity research. This, however, is currently still being done\nmanually by botanists in the field. This work is very laborious, and the data\nobtained is, though following a standardized method to estimate plant coverage,\nusually subjective and has a coarse temporal resolution. To remedy these\ncaveats, we investigate approaches using convolutional neural networks (CNNs)\nto automatically extract the relevant data from images, focusing on plant\ncommunity composition and species coverages of 9 herbaceous plant species. To\nthis end, we investigate several standard CNN architectures and different\npretraining methods. We find that we outperform our previous approach at higher\nimage resolutions using a custom CNN with a mean absolute error of 5.16%. In\naddition to these investigations, we also conduct an error analysis based on\nthe temporal aspect of the plant cover images. This analysis gives insight into\nwhere problems for automatic approaches lie, like occlusion and likely\nmisclassifications caused by temporal changes.",
          "link": "http://arxiv.org/abs/2106.11154",
          "publishedOn": "2021-07-29T02:00:11.252Z",
          "wordCount": 644,
          "title": "Automatic Plant Cover Estimation with Convolutional Neural Networks. (arXiv:2106.11154v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pellikka_M/0/1/0/all/0/1\">Matti Pellikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahtinen_V/0/1/0/all/0/1\">Valtteri Lahtinen</a>",
          "description": "We propose a novel method for large-scale image stitching that is robust\nagainst repetitive patterns and featureless regions in the imagery. In such\ncases, state-of-the-art image stitching methods easily produce image alignment\nartifacts, since they may produce false pairwise image registrations that are\nin conflict within the global connectivity graph. Our method augments the\ncurrent methods by collecting all the plausible pairwise image registration\ncandidates, among which globally consistent candidates are chosen. This enables\nthe stitching process to determine the correct pairwise registrations by\nutilizing all the available information from the whole imagery, such as\nunambiguous registrations outside the repeating pattern and featureless\nregions. We formalize the method as a weighted multigraph whose nodes represent\nthe individual image transformations from the composite image, and whose sets\nof multiple edges between two nodes represent all the plausible transformations\nbetween the pixel coordinates of the two images. The edge weights represent the\nplausibility of the transformations. The image transformations and the edge\nweights are solved from a non-linear minimization problem with linear\nconstraints, for which a projection method is used. As an example, we apply the\nmethod in a large-scale scanning application where the transformations are\nprimarily translations with only slight rotation and scaling component. Despite\nthese simplifications, the state-of-the-art methods do not produce adequate\nresults in such applications, since the image overlap is small, which can be\nfeatureless or repetitive, and misalignment artifacts and their concealment are\nunacceptable.",
          "link": "http://arxiv.org/abs/2004.03860",
          "publishedOn": "2021-07-29T02:00:11.244Z",
          "wordCount": 717,
          "title": "A Robust Method for Image Stitching. (arXiv:2004.03860v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shepley_A/0/1/0/all/0/1\">Andrew Shepley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falzon_G/0/1/0/all/0/1\">Greg Falzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwan_P/0/1/0/all/0/1\">Paul Kwan</a>",
          "description": "Confluence is a novel non-Intersection over Union (IoU) alternative to\nNon-Maxima Suppression (NMS) in bounding box post-processing in object\ndetection. It overcomes the inherent limitations of IoU-based NMS variants to\nprovide a more stable, consistent predictor of bounding box clustering by using\na normalized Manhattan Distance inspired proximity metric to represent bounding\nbox clustering. Unlike Greedy and Soft NMS, it does not rely solely on\nclassification confidence scores to select optimal bounding boxes, instead\nselecting the box which is closest to every other box within a given cluster\nand removing highly confluent neighboring boxes. Confluence is experimentally\nvalidated on the MS COCO and CrowdHuman benchmarks, improving Average Precision\nby up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared against\nde-facto standard and state of the art NMS variants. Quantitative results are\nsupported by extensive qualitative analysis and threshold sensitivity analysis\nexperiments support the conclusion that Confluence is more robust than NMS\nvariants. Confluence represents a paradigm shift in bounding box processing,\nwith potential to replace IoU in bounding box regression processes.",
          "link": "http://arxiv.org/abs/2012.00257",
          "publishedOn": "2021-07-29T02:00:11.228Z",
          "wordCount": 645,
          "title": "Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in Object Detection. (arXiv:2012.00257v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagemann_A/0/1/0/all/0/1\">Annika Hagemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knorr_M/0/1/0/all/0/1\">Moritz Knorr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janssen_H/0/1/0/all/0/1\">Holger Janssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>",
          "description": "Accurate camera calibration is a precondition for many computer vision\napplications. Calibration errors, such as wrong model assumptions or imprecise\nparameter estimation, can deteriorate a system's overall performance, making\nthe reliable detection and quantification of these errors critical. In this\nwork, we introduce an evaluation scheme to capture the fundamental error\nsources in camera calibration: systematic errors (biases) and uncertainty\n(variance). The proposed bias detection method uncovers smallest systematic\nerrors and thereby reveals imperfections of the calibration setup and provides\nthe basis for camera model selection. A novel resampling-based uncertainty\nestimator enables uncertainty estimation under non-ideal conditions and thereby\nextends the classical covariance estimator. Furthermore, we derive a simple\nuncertainty metric that is independent of the camera model. In combination, the\nproposed methods can be used to assess the accuracy of individual calibrations,\nbut also to benchmark new calibration algorithms, camera models, or calibration\nsetups. We evaluate the proposed methods with simulations and real cameras.",
          "link": "http://arxiv.org/abs/2107.13484",
          "publishedOn": "2021-07-29T02:00:11.192Z",
          "wordCount": 589,
          "title": "Inferring bias and uncertainty in camera calibration. (arXiv:2107.13484v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:11.185Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03577",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Franco_Barranco_D/0/1/0/all/0/1\">Daniel Franco-Barranco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munoz_Barrutia_A/0/1/0/all/0/1\">Arrate Mu&#xf1;oz-Barrutia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arganda_Carreras_I/0/1/0/all/0/1\">Ignacio Arganda-Carreras</a>",
          "description": "Electron microscopy (EM) allows the identification of intracellular\norganelles such as mitochondria, providing insights for clinical and scientific\nstudies. In recent years, a number of novel deep learning architectures have\nbeen published reporting superior performance, or even human-level accuracy,\ncompared to previous approaches on public mitochondria segmentation datasets.\nUnfortunately, many of these publications do not make neither the code nor the\nfull training details public to support the results obtained, leading to\nreproducibility issues and dubious model comparisons. For that reason, and\nfollowing a recent code of best practices for reporting experimental results,\nwe present an extensive study of the state-of-the-art deep learning\narchitectures for the segmentation of mitochondria on EM volumes, and evaluate\nthe impact in performance of different variations of 2D and 3D U-Net-like\nmodels for this task. To better understand the contribution of each component,\na common set of pre- and post-processing operations has been implemented and\ntested with each approach. Moreover, an exhaustive sweep of hyperparameters\nvalues for all architectures have been performed and each configuration has\nbeen run multiple times to report the mean and standard deviation values of the\nevaluation metrics. Using this methodology, we found very stable architectures\nand hyperparameter configurations that consistently obtain state-of-the-art\nresults in the well-known EPFL Hippocampus mitochondria segmentation dataset.\nFurthermore, we have benchmarked our proposed models on two other available\ndatasets, Lucchi++ and Kasthuri++, where they outperform all previous works.\nThe code derived from this research and its documentation are publicly\navailable.",
          "link": "http://arxiv.org/abs/2104.03577",
          "publishedOn": "2021-07-29T02:00:11.149Z",
          "wordCount": 711,
          "title": "Stable deep neural network architectures for mitochondria segmentation on electron microscopy volumes. (arXiv:2104.03577v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning (ML) algorithms has raised\nthe number of studies including their applicability in a variety of different\nscenarios. Among all, one of the hardest ones is the aerospace, due to its\npeculiar physical requirements. In this context, a feasibility study and a\nfirst prototype for an Artificial Intelligence (AI) model to be deployed on\nboard satellites are presented in this work. As a case study, the detection of\nvolcanic eruptions has been investigated as a method to swiftly produce alerts\nand allow immediate interventions. Two Convolutional Neural Networks (CNNs)\nhave been proposed and designed, showing how to efficiently implement them for\nidentifying the eruptions and at the same time adapting their complexity in\norder to fit on board requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-07-29T02:00:10.339Z",
          "wordCount": 602,
          "title": "On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1\">Anthony Kleerekoper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tongle Hu</a>",
          "description": "Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.",
          "link": "http://arxiv.org/abs/2107.13277",
          "publishedOn": "2021-07-29T02:00:10.166Z",
          "wordCount": 707,
          "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1\">Patrick Feeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "The pixelwise reconstruction error of deep autoencoders is often utilized for\nimage novelty detection and localization under the assumption that pixels with\nhigh error indicate which parts of the input image are unfamiliar and therefore\nlikely to be novel. This assumed correlation between pixels with high\nreconstruction error and novel regions of input images has not been verified\nand may limit the accuracy of these methods. In this paper we utilize saliency\nmaps to evaluate whether this correlation exists. Saliency maps reveal directly\nhow much a change in each input pixel would affect reconstruction loss, while\neach pixel's reconstruction error may be attributed to many input pixels when\nlayers are fully connected. We compare saliency maps to reconstruction error\nmaps via qualitative visualizations as well as quantitative correspondence\nbetween the top K elements of the maps for both novel and normal images. Our\nresults indicate that reconstruction error maps do not closely correlate with\nthe importance of pixels in the input images, making them insufficient for\nnovelty localization.",
          "link": "http://arxiv.org/abs/2107.13379",
          "publishedOn": "2021-07-29T02:00:10.151Z",
          "wordCount": 606,
          "title": "Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiufu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>",
          "description": "Though widely used in image classification, convolutional neural networks\n(CNNs) are prone to noise interruptions, i.e. the CNN output can be drastically\nchanged by small image noise. To improve the noise robustness, we try to\nintegrate CNNs with wavelet by replacing the common down-sampling (max-pooling,\nstrided-convolution, and average pooling) with discrete wavelet transform\n(DWT). We firstly propose general DWT and inverse DWT (IDWT) layers applicable\nto various orthogonal and biorthogonal discrete wavelets like Haar, Daubechies,\nand Cohen, etc., and then design wavelet integrated CNNs (WaveCNets) by\nintegrating DWT into the commonly used CNNs (VGG, ResNets, and DenseNet).\nDuring the down-sampling, WaveCNets apply DWT to decompose the feature maps\ninto the low-frequency and high-frequency components. Containing the main\ninformation including the basic object structures, the low-frequency component\nis transmitted into the following layers to generate robust high-level\nfeatures. The high-frequency components are dropped to remove most of the data\nnoises. The experimental results show that %wavelet accelerates the CNN\ntraining, and WaveCNets achieve higher accuracy on ImageNet than various\nvanilla CNNs. We have also tested the performance of WaveCNets on the noisy\nversion of ImageNet, ImageNet-C and six adversarial attacks, the results\nsuggest that the proposed DWT/IDWT layers could provide better noise-robustness\nand adversarial robustness. When applying WaveCNets as backbones, the\nperformance of object detectors (i.e., faster R-CNN and RetinaNet) on COCO\ndetection dataset are consistently improved. We believe that suppression of\naliasing effect, i.e. separation of low frequency and high frequency\ninformation, is the main advantages of our approach. The code of our DWT/IDWT\nlayer and different WaveCNets are available at\nhttps://github.com/CVI-SZU/WaveCNet.",
          "link": "http://arxiv.org/abs/2107.13335",
          "publishedOn": "2021-07-29T02:00:10.141Z",
          "wordCount": 720,
          "title": "WaveCNet: Wavelet Integrated CNNs to Suppress Aliasing Effect for Noise-Robust Image Classification. (arXiv:2107.13335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhigao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yin Zhao</a>",
          "description": "We conduct a subjective experiment to compare the performance of traditional\nimage coding methods and learning-based image coding methods. HEVC and VVC, the\nstate-of-the-art traditional coding methods, are used as the representative\ntraditional methods. The learning-based methods used contain not only CNN-based\nmethods, but also a GAN-based method, all of which are advanced or typical.\nSingle Stimuli (SS), which is also called Absolute Category Rating (ACR), is\nadopted as the methodology of the experiment to obtain perceptual quality of\nimages. Additionally, we utilize some typical and frequently used objective\nquality metrics to evaluate the coding methods in the experiment as comparison.\nThe experiment shows that CNN-based and GAN-based methods can perform better\nthan traditional methods in low bit-rates. In high bit-rates, however, it is\nhard to verify whether CNN-based methods are superior to traditional methods.\nBecause the GAN method does not provide models with high target bit-rates, we\ncannot exactly tell the performance of the GAN method in high bit-rates.\nFurthermore, some popular objective quality metrics have not shown the ability\nwell to measure quality of images generated by learning-based coding methods,\nespecially the GAN-based one.",
          "link": "http://arxiv.org/abs/2107.13122",
          "publishedOn": "2021-07-29T02:00:10.118Z",
          "wordCount": 631,
          "title": "Subjective evaluation of traditional and learning-based image coding methods. (arXiv:2107.13122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenjiang Liu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chengji Shen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ou_K/0/1/0/all/0/1\">Kairi Ou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haihong Tang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a> (2) ((1) Alibaba Group, (2) Zhejiang University)",
          "description": "Image virtual try-on task has abundant applications and has become a hot\nresearch topic recently. Existing 2D image-based virtual try-on methods aim to\ntransfer a target clothing image onto a reference person, which has two main\ndisadvantages: cannot control the size and length precisely; unable to\naccurately estimate the user's figure in the case of users wearing thick\nclothes, resulting in inaccurate dressing effect. In this paper, we put forward\nan akin task that aims to dress clothing for underwear models. %, which is also\nan urgent need in e-commerce scenarios. To solve the above drawbacks, we\npropose a Shape Controllable Virtual Try-On Network (SC-VTON), where a graph\nattention network integrates the information of model and clothing to generate\nthe warped clothing image. In addition, the control points are incorporated\ninto SC-VTON for the desired clothing shape. Furthermore, by adding a Splitting\nNetwork and a Synthesis Network, we can use clothing/model pair data to help\noptimize the deformation module and generalize the task to the typical virtual\ntry-on task. Extensive experiments show that the proposed method can achieve\naccurate shape control. Meanwhile, compared with other methods, our method can\ngenerate high-resolution results with detailed textures.",
          "link": "http://arxiv.org/abs/2107.13156",
          "publishedOn": "2021-07-29T02:00:10.105Z",
          "wordCount": 658,
          "title": "Shape Controllable Virtual Try-on for Underwear Models. (arXiv:2107.13156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanda Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongrun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuesheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yalin Zheng</a>",
          "description": "Semi-supervised approaches for crowd counting attract attention, as the fully\nsupervised paradigm is expensive and laborious due to its request for a large\nnumber of images of dense crowd scenarios and their annotations. This paper\nproposes a spatial uncertainty-aware semi-supervised approach via regularized\nsurrogate task (binary segmentation) for crowd counting problems. Different\nfrom existing semi-supervised learning-based crowd counting methods, to exploit\nthe unlabeled data, our proposed spatial uncertainty-aware teacher-student\nframework focuses on high confident regions' information while addressing the\nnoisy supervision from the unlabeled data in an end-to-end manner.\nSpecifically, we estimate the spatial uncertainty maps from the teacher model's\nsurrogate task to guide the feature learning of the main task (density\nregression) and the surrogate task of the student model at the same time.\nBesides, we introduce a simple yet effective differential transformation layer\nto enforce the inherent spatial consistency regularization between the main\ntask and the surrogate task in the student model, which helps the surrogate\ntask to yield more reliable predictions and generates high-quality uncertainty\nmaps. Thus, our model can also address the task-level perturbation problems\nthat occur spatial inconsistency between the primary and surrogate tasks in the\nstudent model. Experimental results on four challenging crowd counting datasets\ndemonstrate that our method achieves superior performance to the\nstate-of-the-art semi-supervised methods.",
          "link": "http://arxiv.org/abs/2107.13271",
          "publishedOn": "2021-07-29T02:00:10.004Z",
          "wordCount": 657,
          "title": "Spatial Uncertainty-Aware Semi-Supervised Crowd Counting. (arXiv:2107.13271v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>",
          "description": "Camera calibration is an important prerequisite towards the solution of 3D\ncomputer vision problems. Traditional methods rely on static images of a\ncalibration pattern. This raises interesting challenges towards the practical\nusage of event cameras, which notably require image change to produce\nsufficient measurements. The current standard for event camera calibration\ntherefore consists of using flashing patterns. They have the advantage of\nsimultaneously triggering events in all reprojected pattern feature locations,\nbut it is difficult to construct or use such patterns in the field. We present\nthe first dynamic event camera calibration algorithm. It calibrates directly\nfrom events captured during relative motion between camera and calibration\npattern. The method is propelled by a novel feature extraction mechanism for\ncalibration patterns, and leverages existing calibration tools before\noptimizing all parameters through a multi-segment continuous-time formulation.\nAs demonstrated through our results on real data, the obtained calibration\nmethod is highly convenient and reliably calibrates from data sequences\nspanning less than 10 seconds.",
          "link": "http://arxiv.org/abs/2107.06749",
          "publishedOn": "2021-07-29T02:00:08.913Z",
          "wordCount": 629,
          "title": "Dynamic Event Camera Calibration. (arXiv:2107.06749v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub. To the best of our knowledge we believe\nthis the first works to try out semi-supervised learning to improve flood\nsegmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-29T02:00:08.850Z",
          "wordCount": 774,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:08.842Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:08.818Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Di Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "3D point cloud completion is very challenging because it heavily relies on\nthe accurate understanding of the complex 3D shapes (e.g., high-curvature,\nconcave/convex, and hollowed-out 3D shapes) and the unknown & diverse patterns\nof the partially available point clouds. In this paper, we propose a novel\nsolution,i.e., Point-block Carving (PC), for completing the complex 3D point\ncloud completion. Given the partial point cloud as the guidance, we carve a3D\nblock that contains the uniformly distributed 3D points, yielding the entire\npoint cloud. To achieve PC, we propose a new network architecture, i.e.,\nCarveNet. This network conducts the exclusive convolution on each point of the\nblock, where the convolutional kernels are trained on the 3D shape data.\nCarveNet determines which point should be carved, for effectively recovering\nthe details of the complete shapes. Furthermore, we propose a sensor-aware\nmethod for data augmentation,i.e., SensorAug, for training CarveNet on richer\npatterns of partial point clouds, thus enhancing the completion power of the\nnetwork. The extensive evaluations on the ShapeNet and KITTI datasets\ndemonstrate the generality of our approach on the partial point clouds with\ndiverse patterns. On these datasets, CarveNet successfully outperforms the\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.13452",
          "publishedOn": "2021-07-29T02:00:08.799Z",
          "wordCount": 643,
          "title": "CarveNet: Carving Point-Block for Complex 3D Shape Completion. (arXiv:2107.13452v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schirris_Y/0/1/0/all/0/1\">Yoni Schirris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nederlof_I/0/1/0/all/0/1\">Iris Nederlof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horlings_H/0/1/0/all/0/1\">Hugo Mark Horlings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>",
          "description": "We propose a Deep learning-based weak label learning method for analysing\nwhole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not\nrequiring pixel-level or tile-level annotations using Self-supervised\npre-training and heterogeneity-aware deep Multiple Instance LEarning\n(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination\ndeficiency (HRD) and microsatellite instability (MSI) prediction. We utilize\ncontrastive self-supervised learning to pre-train a feature extractor on\nhistopathology tiles of cancer tissue. Additionally, we use variability-aware\ndeep multiple instance learning to learn the tile feature aggregation function\nwhile modeling tumor heterogeneity. Compared to state-of-the-art genomic label\nclassification methods, DeepSMILE improves classification performance for HRD\nfrom $70.43\\pm4.10\\%$ to $83.79\\pm1.25\\%$ AUC and MSI from $78.56\\pm6.24\\%$ to\n$90.32\\pm3.58\\%$ AUC in a multi-center breast and colorectal cancer dataset,\nrespectively. These improvements suggest we can improve genomic label\nclassification performance without collecting larger datasets. In the future,\nthis may reduce the need for expensive genome sequencing techniques, provide\npersonalized therapy recommendations based on widely available WSIs of cancer\ntissue, and improve patient care with quicker treatment decisions - also in\nmedical centers without access to genome sequencing resources.",
          "link": "http://arxiv.org/abs/2107.09405",
          "publishedOn": "2021-07-29T02:00:08.791Z",
          "wordCount": 673,
          "title": "DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&E whole-slide images. (arXiv:2107.09405v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward S. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>",
          "description": "Training visuomotor robot controllers from scratch on a new robot typically\nrequires generating large amounts of robot-specific data. Could we leverage\ndata previously collected on another robot to reduce or even completely remove\nthis need for robot-specific data? We propose a \"robot-aware\" solution paradigm\nthat exploits readily available robot \"self-knowledge\" such as proprioception,\nkinematics, and camera calibration to achieve this. First, we learn modular\ndynamics models that pair a transferable, robot-agnostic world dynamics module\nwith a robot-specific, analytical robot dynamics module. Next, we set up visual\nplanning costs that draw a distinction between the robot self and the world.\nOur experiments on tabletop manipulation tasks in simulation and on real robots\ndemonstrate that these plug-in improvements dramatically boost the\ntransferability of visuomotor controllers, even permitting zero-shot transfer\nonto new robots for the very first time. Project website:\nhttps://hueds.github.io/rac/",
          "link": "http://arxiv.org/abs/2107.09047",
          "publishedOn": "2021-07-29T02:00:08.783Z",
          "wordCount": 603,
          "title": "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Domain adaptation is to transfer the shared knowledge learned from the source\ndomain to a new environment, i.e., target domain. One common practice is to\ntrain the model on both labeled source-domain data and unlabeled target-domain\ndata. Yet the learned models are usually biased due to the strong supervision\nof the source domain. Most researchers adopt the early-stopping strategy to\nprevent over-fitting, but when to stop training remains a challenging problem\nsince the lack of the target-domain validation set. In this paper, we propose\none efficient bootstrapping method, called Adaboost Student, explicitly\nlearning complementary models during training and liberating users from\nempirical early stopping. Adaboost Student combines the deep model learning\nwith the conventional training strategy, i.e., adaptive boosting, and enables\ninteractions between learned models and the data sampler. We adopt one adaptive\ndata sampler to progressively facilitate learning on hard samples and aggregate\n\"weak\" models to prevent over-fitting. Extensive experiments show that (1)\nWithout the need to worry about the stopping time, AdaBoost Student provides\none robust solution by efficient complementary model learning during training.\n(2) AdaBoost Student is orthogonal to most domain adaptation methods, which can\nbe combined with existing approaches to further improve the state-of-the-art\nperformance. We have achieved competitive results on three widely-used scene\nsegmentation domain adaptation benchmarks.",
          "link": "http://arxiv.org/abs/2103.15685",
          "publishedOn": "2021-07-29T02:00:08.776Z",
          "wordCount": 684,
          "title": "Adaptive Boosting for Domain Adaptation: Towards Robust Predictions in Scene Segmentation. (arXiv:2103.15685v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1\">Artur Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1\">Erika Puglisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1\">Jamila Mifdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1\">Fiora Pirri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "The abundance of clouds, located both spatially and temporally, often makes\nremote sensing (RS) applications with optical images difficult or even\nimpossible to perform. Traditional cloud removing techniques have been studied\nfor years, and recently, Machine Learning (ML)-based approaches have also been\nconsidered. In this manuscript, a novel method for the restoration of\nclouds-corrupted optical images is presented, able to generate the whole\noptical scene of interest, not only the cloudy pixels, and based on a Joint\nData Fusion paradigm, where three deep neural networks are hierarchically\ncombined. Spatio-temporal features are separately extracted by a conditional\nGenerative Adversarial Network (cGAN) and by a Convolutional Long Short-Term\nMemory (ConvLSTM), from Synthetic Aperture Radar (SAR) data and optical\ntime-series of data respectively, and then combined with a U-shaped network.\nThe use of time-series of data has been rarely explored in the state of the art\nfor this peculiar objective, and moreover existing models do not combine both\nspatio-temporal domains and SAR-optical imagery. Quantitative and qualitative\nresults have shown a good ability of the proposed method in producing\ncloud-free images, by also preserving the details and outperforming the cGAN\nand the ConvLSTM when individually used. Both the code and the dataset have\nbeen implemented from scratch and made available to interested researchers for\nfurther analysis and investigation.",
          "link": "http://arxiv.org/abs/2106.12226",
          "publishedOn": "2021-07-29T02:00:08.768Z",
          "wordCount": 710,
          "title": "Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model. (arXiv:2106.12226v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zitang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weilian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1\">Sei-ichiro Kamata</a>",
          "description": "Various deep neural network architectures (DNNs) maintain massive vital\nrecords in computer vision. While drawing attention worldwide, the design of\nthe overall structure lacks general guidance. Based on the relationship between\nDNN design and numerical differential equations, we performed a fair comparison\nof the residual design with higher-order perspectives. We show that the widely\nused DNN design strategy, constantly stacking a small design (usually 2-3\nlayers), could be easily improved, supported by solid theoretical knowledge and\nwith no extra parameters needed. We reorganise the residual design in\nhigher-order ways, which is inspired by the observation that many effective\nnetworks can be interpreted as different numerical discretisations of\ndifferential equations. The design of ResNet follows a relatively simple\nscheme, which is Euler forward; however, the situation becomes complicated\nrapidly while stacking. We suppose that stacked ResNet is somehow equalled to a\nhigher-order scheme; then, the current method of forwarding propagation might\nbe relatively weak compared with a typical high-order method such as\nRunge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV\nbenchmarks with sufficient experiments. Stable and noticeable increases in\nperformance are observed, and convergence and robustness are also improved. Our\nstacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per\ncent on CIFAR-10, with the same settings and parameters. The proposed strategy\nis fundamental and theoretical and can therefore be applied to any network as a\ngeneral guideline.",
          "link": "http://arxiv.org/abs/2103.15244",
          "publishedOn": "2021-07-29T02:00:08.760Z",
          "wordCount": 727,
          "title": "Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1\">J. K. Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1\">Mario Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1\">Kusal De Alwis</a>",
          "description": "The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.",
          "link": "http://arxiv.org/abs/2103.01205",
          "publishedOn": "2021-07-29T02:00:08.752Z",
          "wordCount": 691,
          "title": "Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Weakly-supervised temporal action localization (WS-TAL) aims to localize\nactions in untrimmed videos with only video-level labels. Most existing models\nfollow the \"localization by classification\" procedure: locate temporal regions\ncontributing most to the video-level classification. Generally, they process\neach snippet (or frame) individually and thus overlook the fruitful temporal\ncontext relation. Here arises the single snippet cheating issue: \"hard\"\nsnippets are too vague to be classified. In this paper, we argue that learning\nby comparing helps identify these hard snippets and we propose to utilize\nsnippet Contrastive learning to Localize Actions, CoLA for short. Specifically,\nwe propose a Snippet Contrast (SniCo) Loss to refine the hard snippet\nrepresentation in feature space, which guides the network to perceive precise\ntemporal boundaries and avoid the temporal interval interruption. Besides,\nsince it is infeasible to access frame-level annotations, we introduce a Hard\nSnippet Mining algorithm to locate the potential hard snippets. Substantial\nanalyses verify that this mining strategy efficaciously captures the hard\nsnippets and SniCo Loss leads to more informative feature representation.\nExtensive experiments show that CoLA achieves state-of-the-art results on\nTHUMOS'14 and ActivityNet v1.2 datasets. CoLA code is publicly available at\nhttps://github.com/zhang-can/CoLA.",
          "link": "http://arxiv.org/abs/2103.16392",
          "publishedOn": "2021-07-29T02:00:08.728Z",
          "wordCount": 669,
          "title": "CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning. (arXiv:2103.16392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morales_D/0/1/0/all/0/1\">David Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Estefania Talavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remeseiro_B/0/1/0/all/0/1\">Beatriz Remeseiro</a>",
          "description": "The field of deep learning is evolving in different directions, with still\nthe need for more efficient training strategies. In this work, we present a\nnovel and robust training scheme that integrates visual explanation techniques\nin the learning process. Unlike the attention mechanisms that focus on the\nrelevant parts of images, we aim to improve the robustness of the model by\nmaking it pay attention to other regions as well. Broadly speaking, the idea is\nto distract the classifier in the learning process to force it to focus not\nonly on relevant regions but also on those that, a priori, are not so\ninformative for the discrimination of the class. We tested the proposed\napproach by embedding it into the learning process of a convolutional neural\nnetwork for the analysis and classification of two well-known datasets, namely\nStanford cars and FGVC-Aircraft. Furthermore, we evaluated our model on a\nreal-case scenario for the classification of egocentric images, allowing us to\nobtain relevant information about peoples' lifestyles. In particular, we work\non the challenging EgoFoodPlaces dataset, achieving state-of-the-art results\nwith a lower level of complexity. The obtained results indicate the suitability\nof our proposed training scheme for image classification, improving the\nrobustness of the final model.",
          "link": "http://arxiv.org/abs/2012.14173",
          "publishedOn": "2021-07-29T02:00:08.714Z",
          "wordCount": 696,
          "title": "Playing to distraction: towards a robust training of CNN classifiers through visual explanation techniques. (arXiv:2012.14173v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>",
          "description": "Crowd counting has drawn much attention due to its importance in\nsafety-critical surveillance systems. Especially, deep neural network (DNN)\nmethods have significantly reduced estimation errors for crowd counting\nmissions. Recent studies have demonstrated that DNNs are vulnerable to\nadversarial attacks, i.e., normal images with human-imperceptible perturbations\ncould mislead DNNs to make false predictions. In this work, we propose a robust\nattack strategy called Adversarial Patch Attack with Momentum (APAM) to\nsystematically evaluate the robustness of crowd counting models, where the\nattacker's goal is to create an adversarial perturbation that severely degrades\ntheir performances, thus leading to public safety accidents (e.g., stampede\naccidents). Especially, the proposed attack leverages the extreme-density\nbackground information of input images to generate robust adversarial patches\nvia a series of transformations (e.g., interpolation, rotation, etc.). We\nobserve that by perturbing less than 6\\% of image pixels, our attacks severely\ndegrade the performance of crowd counting systems, both digitally and\nphysically. To better enhance the adversarial robustness of crowd counting\nmodels, we propose the first regression model-based Randomized Ablation (RA),\nwhich is more sufficient than Adversarial Training (ADT) (Mean Absolute Error\nof RA is 5 lower than ADT on clean samples and 30 lower than ADT on adversarial\nexamples). Extensive experiments on five crowd counting models demonstrate the\neffectiveness and generality of the proposed method. The supplementary\nmaterials and certificate retrained models are available at\n\\url{https://www.dropbox.com/s/hc4fdx133vht0qb/ACM_MM2021_Supp.pdf?dl=0}",
          "link": "http://arxiv.org/abs/2104.10868",
          "publishedOn": "2021-07-29T02:00:08.692Z",
          "wordCount": 719,
          "title": "Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting. (arXiv:2104.10868v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-07-29T02:00:08.676Z",
          "wordCount": 658,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Changcheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>",
          "description": "Conducting efficient performance estimations of neural architectures is a\nmajor challenge in neural architecture search (NAS). To reduce the architecture\ntraining costs in NAS, one-shot estimators (OSEs) amortize the architecture\ntraining costs by sharing the parameters of one supernet between all\narchitectures. Recently, zero-shot estimators (ZSEs) that involve no training\nare proposed to further reduce the architecture evaluation cost. Despite the\nhigh efficiency of these estimators, the quality of such estimations has not\nbeen thoroughly studied. In this paper, we conduct an extensive and organized\nassessment of OSEs and ZSEs on three NAS benchmarks: NAS-Bench-101/201/301.\nSpecifically, we employ a set of NAS-oriented criteria to study the behavior of\nOSEs and ZSEs and reveal that they have certain biases and variances. After\nanalyzing how and why the OSE estimations are unsatisfying, we explore how to\nmitigate the correlation gap of OSEs from several perspectives. For ZSEs, we\nfind that current ZSEs are not satisfying enough in these benchmark search\nspaces, and analyze their biases. Through our analysis, we give out suggestions\nfor future application and development of efficient architecture performance\nestimators. Furthermore, the analysis framework proposed in our work could be\nutilized in future research to give a more comprehensive understanding of newly\ndesigned architecture performance estimators. All codes and analysis scripts\nare available at https://github.com/walkerning/aw_nas.",
          "link": "http://arxiv.org/abs/2008.03064",
          "publishedOn": "2021-07-29T02:00:08.658Z",
          "wordCount": 706,
          "title": "Evaluating Efficient Performance Estimators of Neural Architectures. (arXiv:2008.03064v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiunn-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chenxi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achar_M/0/1/0/all/0/1\">Madhav Achar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grizzle_J/0/1/0/all/0/1\">Jessy W. Grizzle</a>",
          "description": "Sensor calibration, which can be intrinsic or extrinsic, is an essential step\nto achieve the measurement accuracy required for modern perception and\nnavigation systems deployed on autonomous robots. To date, intrinsic\ncalibration models for spinning LiDARs have been based on hypothesized based on\ntheir physical mechanisms, resulting in anywhere from three to ten parameters\nto be estimated from data, while no phenomenological models have yet been\nproposed for solid-state LiDARs. Instead of going down that road, we propose to\nabstract away from the physics of a LiDAR type (spinning vs solid-state, for\nexample), and focus on the spatial geometry of the point cloud generated by the\nsensor. By modeling the calibration parameters as an element of a special\nmatrix Lie Group, we achieve a unifying view of calibration for different types\nof LiDARs. We further prove mathematically that the proposed model is\nwell-constrained (has a unique answer) given four appropriately orientated\ntargets. The proof provides a guideline for target positioning in the form of a\ntetrahedron. Moreover, an existing Semidefinite programming global solver for\nSE(3) can be modified to compute efficiently the optimal calibration\nparameters. For solid state LiDARs, we illustrate how the method works in\nsimulation. For spinning LiDARs, we show with experimental data that the\nproposed matrix Lie Group model performs equally well as physics-based models\nin terms of reducing the P2P distance, while being more robust to noise.",
          "link": "http://arxiv.org/abs/2012.03321",
          "publishedOn": "2021-07-29T02:00:08.644Z",
          "wordCount": 701,
          "title": "Global Unifying Intrinsic Calibration for Spinning and Solid-State LiDARs. (arXiv:2012.03321v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13542",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wyburd_M/0/1/0/all/0/1\">Madeleine K. Wyburd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dinsdale_N/0/1/0/all/0/1\">Nicola K. Dinsdale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namburete_A/0/1/0/all/0/1\">Ana I.L. Namburete</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a>",
          "description": "Accurate topology is key when performing meaningful anatomical segmentations,\nhowever, it is often overlooked in traditional deep learning methods. In this\nwork we propose TEDS-Net: a novel segmentation method that guarantees accurate\ntopology. Our method is built upon a continuous diffeomorphic framework, which\nenforces topology preservation. However, in practice, diffeomorphic fields are\nrepresented using a finite number of parameters and sampled using methods such\nas linear interpolation, violating the theoretical guarantees. We therefore\nintroduce additional modifications to more strictly enforce it. Our network\nlearns how to warp a binary prior, with the desired topological\ncharacteristics, to complete the segmentation task. We tested our method on\nmyocardium segmentation from an open-source 2D heart dataset. TEDS-Net\npreserved topology in 100% of the cases, compared to 90% from the U-Net,\nwithout sacrificing on Hausdorff Distance or Dice performance. Code will be\nmade available at: www.github.com/mwyburd/TEDS-Net",
          "link": "http://arxiv.org/abs/2107.13542",
          "publishedOn": "2021-07-29T02:00:08.636Z",
          "wordCount": 613,
          "title": "TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations. (arXiv:2107.13542v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Timothy Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1\">Jamell Dozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1\">Helen Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1\">Nishchal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fr&#xe9;do Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Object recognition and viewpoint estimation lie at the heart of visual\nunderstanding. Recent works suggest that convolutional neural networks (CNNs)\nfail to generalize to out-of-distribution (OOD) category-viewpoint\ncombinations, ie. combinations not seen during training. In this paper, we\ninvestigate when and how such OOD generalization may be possible by evaluating\nCNNs trained to classify both object category and 3D viewpoint on OOD\ncombinations, and identifying the neural mechanisms that facilitate such OOD\ngeneralization. We show that increasing the number of in-distribution\ncombinations (ie. data diversity) substantially improves generalization to OOD\ncombinations, even with the same amount of training data. We compare learning\ncategory and viewpoint in separate and shared network architectures, and\nobserve starkly different trends on in-distribution and OOD combinations, ie.\nwhile shared networks are helpful in-distribution, separate networks\nsignificantly outperform shared ones at OOD combinations. Finally, we\ndemonstrate that such OOD generalization is facilitated by the neural mechanism\nof specialization, ie. the emergence of two types of neurons -- neurons\nselective to category and invariant to viewpoint, and vice versa.",
          "link": "http://arxiv.org/abs/2007.08032",
          "publishedOn": "2021-07-29T02:00:08.629Z",
          "wordCount": 654,
          "title": "When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "There has been a growing interest in unsupervised domain adaptation (UDA) to\nalleviate the data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is proposed for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vectors that violate the poset constraint by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-07-29T02:00:08.588Z",
          "wordCount": 624,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1\">Kim Ji Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>",
          "description": "Deep Generative Models (DGMs) are known for their superior capability in\ngenerating realistic data. Extending purely data-driven approaches, recent\nspecialized DGMs may satisfy additional controllable requirements such as\nembedding a traffic sign in a driving scene, by manipulating patterns\n\\textit{implicitly} in the neuron or feature level. In this paper, we introduce\na novel method to incorporate domain knowledge \\textit{explicitly} in the\ngeneration process to achieve semantically controllable scene generation. We\ncategorize our knowledge into two types to be consistent with the composition\nof natural scenes, where the first type represents the property of objects and\nthe second type represents the relationship among objects. We then propose a\ntree-structured generative model to learn complex scene representation, whose\nnodes and edges are naturally corresponding to the two types of knowledge\nrespectively. Knowledge can be explicitly integrated to enable semantically\ncontrollable scene generation by imposing semantic rules on properties of nodes\nand edges in the tree structure. We construct a synthetic example to illustrate\nthe controllability and explainability of our method in a clean setting. We\nfurther extend the synthetic example to realistic autonomous vehicle driving\nenvironments and conduct extensive experiments to show that our method\nefficiently identifies adversarial traffic scenes against different\nstate-of-the-art 3D point cloud segmentation models satisfying the traffic\nrules specified as the explicit knowledge.",
          "link": "http://arxiv.org/abs/2106.04066",
          "publishedOn": "2021-07-29T02:00:08.567Z",
          "wordCount": 703,
          "title": "Semantically Controllable Scene Generation with Guidance of Explicit Knowledge. (arXiv:2106.04066v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13407",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mora_Martin_G/0/1/0/all/0/1\">Germ&#xe1;n Mora-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turpin_A/0/1/0/all/0/1\">Alex Turpin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruget_A/0/1/0/all/0/1\">Alice Ruget</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halimi_A/0/1/0/all/0/1\">Abderrahim Halimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henderson_R/0/1/0/all/0/1\">Robert Henderson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leach_J/0/1/0/all/0/1\">Jonathan Leach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gyongy_I/0/1/0/all/0/1\">Istvan Gyongy</a>",
          "description": "3D time-of-flight (ToF) imaging is used in a variety of applications such as\naugmented reality (AR), computer interfaces, robotics and autonomous systems.\nSingle-photon avalanche diodes (SPADs) are one of the enabling technologies\nproviding accurate depth data even over long ranges. By developing SPADs in\narray format with integrated processing combined with pulsed, flood-type\nillumination, high-speed 3D capture is possible. However, array sizes tend to\nbe relatively small, limiting the lateral resolution of the resulting depth\nmaps, and, consequently, the information that can be extracted from the image\nfor applications such as object detection. In this paper, we demonstrate that\nthese limitations can be overcome through the use of convolutional neural\nnetworks (CNNs) for high-performance object detection. We present outdoor\nresults from a portable SPAD camera system that outputs 16-bin photon timing\nhistograms with 64x32 spatial resolution. The results, obtained with exposure\ntimes down to 2 ms (equivalent to 500 FPS) and in signal-to-background (SBR)\nratios as low as 0.05, point to the advantages of providing the CNN with full\nhistogram data rather than point clouds alone. Alternatively, a combination of\npoint cloud and active intensity data may be used as input, for a similar level\nof performance. In either case, the GPU-accelerated processing time is less\nthan 1 ms per frame, leading to an overall latency (image acquisition plus\nprocessing) in the millisecond range, making the results relevant for\nsafety-critical computer vision applications which would benefit from faster\nthan human reaction times.",
          "link": "http://arxiv.org/abs/2107.13407",
          "publishedOn": "2021-07-29T02:00:08.547Z",
          "wordCount": 705,
          "title": "High-speed object detection with a single-photon time-of-flight image sensor. (arXiv:2107.13407v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15564",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_Q/0/1/0/all/0/1\">Qingcheng Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_L/0/1/0/all/0/1\">Lin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">He Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_J/0/1/0/all/0/1\">Jiezhen Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jicong Zhang</a>",
          "description": "The novel Coronavirus disease (COVID-19) is a highly contagious virus and has\nspread all over the world, posing an extremely serious threat to all countries.\nAutomatic lung infection segmentation from computed tomography (CT) plays an\nimportant role in the quantitative analysis of COVID-19. However, the major\nchallenge lies in the inadequacy of annotated COVID-19 datasets. Currently,\nthere are several public non-COVID lung lesion segmentation datasets, providing\nthe potential for generalizing useful information to the related COVID-19\nsegmentation task. In this paper, we propose a novel relation-driven\ncollaborative learning model to exploit shared knowledge from non-COVID lesions\nfor annotation-efficient COVID-19 CT lung infection segmentation. The model\nconsists of a general encoder to capture general lung lesion features based on\nmultiple non-COVID lesions, and a target encoder to focus on task-specific\nfeatures based on COVID-19 infections. Features extracted from the two parallel\nencoders are concatenated for the subsequent decoder part. We develop a\ncollaborative learning scheme to regularize feature-level relation consistency\nof given input and encourage the model to learn more general and discriminative\nrepresentation of COVID-19 infections. Extensive experiments demonstrate that\ntrained with limited COVID-19 data, exploiting shared knowledge from non-COVID\nlesions can further improve state-of-the-art performance with up to 3.0% in\ndice similarity coefficient and 4.2% in normalized surface dice. Our proposed\nmethod promotes new insights into annotation-efficient deep learning for\nCOVID-19 infection segmentation and illustrates strong potential for real-world\napplications in the global fight against COVID-19 in the absence of sufficient\nhigh-quality annotations.",
          "link": "http://arxiv.org/abs/2012.15564",
          "publishedOn": "2021-07-29T02:00:08.536Z",
          "wordCount": 768,
          "title": "Exploiting Shared Knowledge from Non-COVID Lesions for Annotation-Efficient COVID-19 CT Lung Infection Segmentation. (arXiv:2012.15564v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Son_W/0/1/0/all/0/1\">Wonchul Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1\">Jaemin Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Junyong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>",
          "description": "With the success of deep neural networks, knowledge distillation which guides\nthe learning of a small student network from a large teacher network is being\nactively studied for model compression and transfer learning. However, few\nstudies have been performed to resolve the poor learning issue of the student\nnetwork when the student and teacher model sizes significantly differ. In this\npaper, we propose a densely guided knowledge distillation using multiple\nteacher assistants that gradually decreases the model size to efficiently\nbridge the large gap between the teacher and student networks. To stimulate\nmore efficient learning of the student network, we guide each teacher assistant\nto every other smaller teacher assistants iteratively. Specifically, when\nteaching a smaller teacher assistant at the next step, the existing larger\nteacher assistants from the previous step are used as well as the teacher\nnetwork. Moreover, we design stochastic teaching where, for each mini-batch, a\nteacher or teacher assistants are randomly dropped. This acts as a regularizer\nto improve the efficiency of teaching of the student network. Thus, the student\ncan always learn salient distilled knowledge from the multiple sources. We\nverified the effectiveness of the proposed method for a classification task\nusing CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant\nperformance improvements with various backbone architectures such as ResNet,\nWideResNet, and VGG.",
          "link": "http://arxiv.org/abs/2009.08825",
          "publishedOn": "2021-07-29T02:00:08.529Z",
          "wordCount": 684,
          "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants. (arXiv:2009.08825v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiherer_M/0/1/0/all/0/1\">Maximilian Weiherer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eigenberger_A/0/1/0/all/0/1\">Andreas Eigenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brebant_V/0/1/0/all/0/1\">Vanessa Br&#xe9;bant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prantl_L/0/1/0/all/0/1\">Lukas Prantl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palm_C/0/1/0/all/0/1\">Christoph Palm</a>",
          "description": "We present the Regensburg Breast Shape Model (RBSM) - a 3D statistical shape\nmodel of the female breast built from 110 breast scans, and the first ever\npublicly available. Together with the model, a fully automated, pairwise\nsurface registration pipeline used to establish correspondence among 3D breast\nscans is introduced. Our method is computationally efficient and requires only\nfour landmarks to guide the registration process. In order to weaken the strong\ncoupling between breast and thorax, we propose to minimize the variance outside\nthe breast region as much as possible. To achieve this goal, a novel concept\ncalled breast probability masks (BPMs) is introduced. A BPM assigns\nprobabilities to each point of a 3D breast scan, telling how likely it is that\na particular point belongs to the breast area. During registration, we use BPMs\nto align the template to the target as accurately as possible inside the breast\nregion and only roughly outside. This simple yet effective strategy\nsignificantly reduces the unwanted variance outside the breast region, leading\nto better statistical shape models in which breast shapes are quite well\ndecoupled from the thorax. The RBSM is thus able to produce a variety of\ndifferent breast shapes as independently as possible from the shape of the\nthorax. Our systematic experimental evaluation reveals a generalization ability\nof 0.17 mm and a specificity of 2.8 mm for the RBSM. Ultimately, our model is\nseen as a first step towards combining physically motivated deformable models\nof the breast and statistical approaches in order to enable more realistic\nsurgical outcome simulation.",
          "link": "http://arxiv.org/abs/2107.13463",
          "publishedOn": "2021-07-29T02:00:08.502Z",
          "wordCount": 745,
          "title": "Learning the shape of female breasts: an open-access 3D statistical shape model of the female breast built from 110 breast scans. (arXiv:2107.13463v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13431",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shuang Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1\">Qiongyu Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Wenquan Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_D/0/1/0/all/0/1\">Desheng Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huabin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaobo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>",
          "description": "Ultrasound is the preferred choice for early screening of dense breast\ncancer. Clinically, doctors have to manually write the screening report which\nis time-consuming and laborious, and it is easy to miss and miswrite.\nTherefore, this paper proposes a method for efficiently generating personalized\nbreast ultrasound screening preliminary reports by AI, especially for benign\nand normal cases which account for the majority. Doctors then make simple\nadjustments or corrections to quickly generate final reports. The proposed\napproach has been tested using a database of 1133 breast tumor instances.\nExperimental results indicate this pipeline improves doctors' work efficiency\nby up to 90%, which greatly reduces repetitive work.",
          "link": "http://arxiv.org/abs/2107.13431",
          "publishedOn": "2021-07-29T02:00:08.487Z",
          "wordCount": 559,
          "title": "AI assisted method for efficiently generating breast ultrasound screening reports. (arXiv:2107.13431v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xingxing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dekui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>",
          "description": "The existing 3D deep learning methods adopt either individual point-based\nfeatures or local-neighboring voxel-based features, and demonstrate great\npotential for processing 3D data. However, the point based models are\ninefficient due to the unordered nature of point clouds and the voxel-based\nmodels suffer from large information loss. Motivated by the success of recent\npoint-voxel representation, such as PVCNN, we propose a new convolutional\nneural network, called Multi Point-Voxel Convolution (MPVConv), for deep\nlearning on point clouds. Integrating both the advantages of voxel and\npoint-based methods, MPVConv can effectively increase the neighboring\ncollection between point-based features and also promote independence among\nvoxel-based features. Moreover, most of the existing approaches aim at solving\none specific task, and only a few of them can handle a variety of tasks. Simply\nreplacing the corresponding convolution module with MPVConv, we show that\nMPVConv can fit in different backbones to solve a wide range of 3D tasks.\nExtensive experiments on benchmark datasets such as ShapeNet Part, S3DIS and\nKITTI for various tasks show that MPVConv improves the accuracy of the backbone\n(PointNet) by up to \\textbf{36\\%}, and achieves higher accuracy than the\nvoxel-based model with up to \\textbf{34}$\\times$ speedups. In addition, MPVConv\noutperforms the state-of-the-art point-based models with up to\n\\textbf{8}$\\times$ speedups. Notably, our MPVConv achieves better accuracy than\nthe newest point-voxel-based model PVCNN (a model more efficient than PointNet)\nwith lower latency.",
          "link": "http://arxiv.org/abs/2107.13152",
          "publishedOn": "2021-07-29T02:00:08.460Z",
          "wordCount": 679,
          "title": "Multi Point-Voxel Convolution (MPVConv) for Deep Learning on Point Clouds. (arXiv:2107.13152v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "The computational vision community has recently paid attention to continual\nlearning for blind image quality assessment (BIQA). The primary challenge is to\ncombat catastrophic forgetting of previously-seen IQA datasets (i.e., tasks).\nIn this paper, we present a simple yet effective continual learning method for\nBIQA with improved quality prediction accuracy, plasticity-stability trade-off,\nand task-order/length robustness. The key step in our approach is to freeze all\nconvolution filters of a pre-trained deep neural network (DNN) for an explicit\npromise of stability, and learn task-specific normalization parameters for\nplasticity. We assign each new task a prediction head, and load the\ncorresponding normalization parameters to produce a quality score. The final\nquality estimate is computed by feature fusion and adaptive weighting using\nhierarchical representations, without leveraging the test-time oracle.\nExtensive experiments on six IQA datasets demonstrate the advantages of the\nproposed method in comparison to previous training techniques for BIQA.",
          "link": "http://arxiv.org/abs/2107.13429",
          "publishedOn": "2021-07-29T02:00:08.441Z",
          "wordCount": 591,
          "title": "Task-Specific Normalization for Continual Learning of Blind Image Quality Models. (arXiv:2107.13429v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14331",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1\">Abhranil Das</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1\">Wilson S Geisler</a>",
          "description": "Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.",
          "link": "http://arxiv.org/abs/2012.14331",
          "publishedOn": "2021-07-29T02:00:08.434Z",
          "wordCount": 681,
          "title": "A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank P.-W. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingnan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>",
          "description": "Accurate prediction of future person location and movement trajectory from an\negocentric wearable camera can benefit a wide range of applications, such as\nassisting visually impaired people in navigation, and the development of\nmobility assistance for people with disability. In this work, a new egocentric\ndataset was constructed using a wearable camera, with 8,250 short clips of a\ntargeted person either walking 1) toward, 2) away, or 3) across the camera\nwearer in indoor environments, or 4) staying still in the scene, and 13,817\nperson bounding boxes were manually labelled. Apart from the bounding boxes,\nthe dataset also contains the estimated pose of the targeted person as well as\nthe IMU signal of the wearable camera at each time point. An LSTM-based\nencoder-decoder framework was designed to predict the future location and\nmovement trajectory of the targeted person in this egocentric setting.\nExtensive experiments have been conducted on the new dataset, and have shown\nthat the proposed method is able to reliably and better predict future person\nlocation and trajectory in egocentric videos captured by the wearable camera\ncompared to three baselines.",
          "link": "http://arxiv.org/abs/2103.04019",
          "publishedOn": "2021-07-29T02:00:08.406Z",
          "wordCount": 671,
          "title": "Indoor Future Person Localization from an Egocentric Wearable Camera. (arXiv:2103.04019v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1\">Akshat Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "In this paper, we demonstrated a practical application of realistic river\nimage generation using deep learning. Specifically, we explored a generative\nadversarial network (GAN) model capable of generating high-resolution and\nrealistic river images that can be used to support modeling and analysis in\nsurface water estimation, river meandering, wetland loss, and other\nhydrological research studies. First, we have created an extensive repository\nof overhead river images to be used in training. Second, we incorporated the\nProgressive Growing GAN (PGGAN), a network architecture that iteratively trains\nsmaller-resolution GANs to gradually build up to a very high resolution to\ngenerate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\nGAN architectures, difficulties arose in terms of exponential increase of\ntraining time and vanishing/exploding gradient issues, which the PGGAN\nimplementation seemed to significantly reduce. The results presented in this\nstudy show great promise in generating high-quality images and capturing the\ndetails of river structure and flow to support hydrological research, which\noften requires extensive imagery for model performance.",
          "link": "http://arxiv.org/abs/2003.00826",
          "publishedOn": "2021-07-29T02:00:08.397Z",
          "wordCount": 644,
          "title": "Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose new explainability approaches for point cloud deep\nneural networks based on local surrogate model-based methods to show which\ncomponents make the main contribution to the classification. Moreover, we\npropose a quantitative validation method for explainability methods of point\nclouds which enhances the persuasive power of explainability by dropping the\nmost positive or negative contributing features and monitoring how the\nclassification scores of specific categories change. To enable an intuitive\nexplanation of misclassified instances, we display features with confounding\ncontributions. Our new explainability approach provides a fairly accurate, more\nintuitive and widely applicable explanation for point cloud classification\ntasks. Our code is available at https://github.com/Explain3D/Explainable3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-07-29T02:00:08.384Z",
          "wordCount": 610,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Libo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>",
          "description": "Road detection is a critically important task for self-driving cars. By\nemploying LiDAR data, recent works have significantly improved the accuracy of\nroad detection. Relying on LiDAR sensors limits the wide application of those\nmethods when only cameras are available. In this paper, we propose a novel road\ndetection approach with RGB being the only input during inference.\nSpecifically, we exploit pseudo-LiDAR using depth estimation, and propose a\nfeature fusion network where RGB and learned depth information are fused for\nimproved road detection. To further optimize the network structure and improve\nthe efficiency of the network. we search for the network structure of the\nfeature fusion module using NAS techniques. Finally, be aware of that\ngenerating pseudo-LiDAR from RGB via depth estimation introduces extra\ncomputational costs and relies on depth estimation networks, we design a\nmodality distillation strategy and leverage it to further free our network from\nthese extra computational cost and dependencies during inference. The proposed\nmethod achieves state-of-the-art performance on two challenging benchmarks,\nKITTI and R2D.",
          "link": "http://arxiv.org/abs/2107.13279",
          "publishedOn": "2021-07-29T02:00:08.377Z",
          "wordCount": 596,
          "title": "Pseudo-LiDAR Based Road Detection. (arXiv:2107.13279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bo Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a> (Alzheimer&#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), <a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Shuwei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1\">Peng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Ronald X. Xu</a>",
          "description": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "link": "http://arxiv.org/abs/2107.13200",
          "publishedOn": "2021-07-29T02:00:08.356Z",
          "wordCount": 760,
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>",
          "description": "One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online this http URL .",
          "link": "http://arxiv.org/abs/1907.01845",
          "publishedOn": "2021-07-29T02:00:08.349Z",
          "wordCount": 672,
          "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.01446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chongwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yulong Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Caifei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>",
          "description": "To boost the object grabbing capability of underwater robots for open-sea\nfarming, we propose a new dataset (UDD) consisting of three categories\n(seacucumber, seaurchin, and scallop) with 2,227 images. To the best of our\nknowledge, it is the first 4K HD dataset collected in a real open-sea farm. We\nalso propose a novel Poisson-blending Generative Adversarial Network (Poisson\nGAN) and an efficient object detection network (AquaNet) to address two common\nissues within related datasets: the class-imbalance problem and the problem of\nmass small object, respectively. Specifically, Poisson GAN combines Poisson\nblending into its generator and employs a new loss called Dual Restriction loss\n(DR loss), which supervises both implicit space features and image-level\nfeatures during training to generate more realistic images. By utilizing\nPoisson GAN, objects of minority class like seacucumber or scallop could be\nadded into an image naturally and annotated automatically, which could increase\nthe loss of minority classes during training detectors to eliminate the\nclass-imbalance problem; AquaNet is a high-efficiency detector to address the\nproblem of detecting mass small objects from cloudy underwater pictures. Within\nit, we design two efficient components: a depth-wise-convolution-based\nMulti-scale Contextual Features Fusion (MFF) block and a Multi-scale\nBlursampling (MBP) module to reduce the parameters of the network to 1.3\nmillion. Both two components could provide multi-scale features of small\nobjects under a short backbone configuration without any loss of accuracy. In\naddition, we construct a large-scale augmented dataset (AUDD) and a\npre-training dataset via Poisson GAN from UDD. Extensive experiments show the\neffectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training\ndataset.",
          "link": "http://arxiv.org/abs/2003.01446",
          "publishedOn": "2021-07-29T02:00:08.335Z",
          "wordCount": 750,
          "title": "A New Dataset, Poisson GAN and AquaNet for Underwater Object Grabbing. (arXiv:2003.01446v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Widya_A/0/1/0/all/0/1\">Aji Resindra Widya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monno_Y/0/1/0/all/0/1\">Yusuke Monno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1\">Sho Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotoda_T/0/1/0/all/0/1\">Takuji Gotoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miki_K/0/1/0/all/0/1\">Kenji Miki</a>",
          "description": "Gastroendoscopy has been a clinical standard for diagnosing and treating\nconditions that affect a part of a patient's digestive system, such as the\nstomach. Despite the fact that gastroendoscopy has a lot of advantages for\npatients, there exist some challenges for practitioners, such as the lack of 3D\nperception, including the depth and the endoscope pose information. Such\nchallenges make navigating the endoscope and localizing any found lesion in a\ndigestive tract difficult. To tackle these problems, deep learning-based\napproaches have been proposed to provide monocular gastroendoscopy with\nadditional yet important depth and pose information. In this paper, we propose\na novel supervised approach to train depth and pose estimation networks using\nconsecutive endoscopy images to assist the endoscope navigation in the stomach.\nWe firstly generate real depth and pose training data using our previously\nproposed whole stomach 3D reconstruction pipeline to avoid poor generalization\nability between computer-generated (CG) models and real data for the stomach.\nIn addition, we propose a novel generalized photometric loss function to avoid\nthe complicated process of finding proper weights for balancing the depth and\nthe pose loss terms, which is required for existing direct depth and pose\nsupervision approaches. We then experimentally show that our proposed\ngeneralized loss performs better than existing direct supervision losses.",
          "link": "http://arxiv.org/abs/2107.13263",
          "publishedOn": "2021-07-29T02:00:08.327Z",
          "wordCount": 664,
          "title": "Learning-Based Depth and Pose Estimation for Monocular Endoscope with Loss Generalization. (arXiv:2107.13263v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindra Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaolong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.",
          "link": "http://arxiv.org/abs/2107.13389",
          "publishedOn": "2021-07-29T02:00:08.319Z",
          "wordCount": 612,
          "title": "SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>",
          "description": "Generating photo-realistic images from a text description is a challenging\nproblem in computer vision. Previous works have shown promising performance to\ngenerate synthetic images conditional on text by Generative Adversarial\nNetworks (GANs). In this paper, we focus on the category-consistent and\nrelativistic diverse constraints to optimize the diversity of synthetic images.\nBased on those constraints, a category-consistent and relativistic diverse\nconditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images\nsimultaneously. We use the attention loss and diversity loss to improve the\nsensitivity of the GAN to word attention and noises. Then, we employ the\nrelativistic conditional loss to estimate the probability of relatively real or\nfake for synthetic images, which can improve the performance of basic\nconditional loss. Finally, we introduce a category-consistent loss to alleviate\nthe over-category issues between K synthetic images. We evaluate our approach\nusing the Birds-200-2011, Oxford-102 flower and MSCOCO 2014 datasets, and the\nextensive experiments demonstrate superiority of the proposed method in\ncomparison with state-of-the-art methods in terms of photorealistic and\ndiversity of the generated synthetic images.",
          "link": "http://arxiv.org/abs/2107.13516",
          "publishedOn": "2021-07-29T02:00:08.298Z",
          "wordCount": 609,
          "title": "CRD-CGAN: Category-Consistent and Relativistic Constraints for Diverse Text-to-Image Generation. (arXiv:2107.13516v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashlesha Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangwan_K/0/1/0/all/0/1\">Kuldip Singh Sangwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhiraj/0/1/0/all/0/1\">Dhiraj</a>",
          "description": "As the proportion of road accidents increases each year, driver distraction\ncontinues to be an important risk component in road traffic injuries and\ndeaths. The distractions caused by the increasing use of mobile phones and\nother wireless devices pose a potential risk to road safety. Our current study\naims to aid the already existing techniques in driver posture recognition by\nimproving the performance in the driver distraction classification problem. We\npresent an approach using a genetic algorithm-based ensemble of six independent\ndeep neural architectures, namely, AlexNet, VGG-16, EfficientNet B0, Vanilla\nCNN, Modified DenseNet, and InceptionV3 + BiLSTM. We test it on two\ncomprehensive datasets, the AUC Distracted Driver Dataset, on which our\ntechnique achieves an accuracy of 96.37%, surpassing the previously obtained\n95.98%, and on the State Farm Driver Distraction Dataset, on which we attain an\naccuracy of 99.75%. The 6-Model Ensemble gave an inference time of 0.024\nseconds as measured on our machine with Ubuntu 20.04(64-bit) and GPU as GeForce\nGTX 1080.",
          "link": "http://arxiv.org/abs/2107.13355",
          "publishedOn": "2021-07-29T02:00:08.291Z",
          "wordCount": 633,
          "title": "A Computer Vision-Based Approach for Driver Distraction Recognition using Deep Learning and Genetic Algorithm Based Ensemble. (arXiv:2107.13355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Zhong Yang</a>",
          "description": "In this report, the technical details of our submission to the EPIC-Kitchens\nAction Anticipation Challenge 2021 are given. We developed a hierarchical\nattention model for action anticipation, which leverages Transformer-based\nattention mechanism to aggregate features across temporal dimension,\nmodalities, symbiotic branches respectively. In terms of Mean Top-5 Recall of\naction, our submission with team name ICL-SJTU achieved 13.39% for overall\ntesting set, 10.05% for unseen subsets and 11.88% for tailed subsets.\nAdditionally, it is noteworthy that our submission ranked 1st in terms of verb\nclass in all three (sub)sets.",
          "link": "http://arxiv.org/abs/2107.13259",
          "publishedOn": "2021-07-29T02:00:08.283Z",
          "wordCount": 529,
          "title": "TransAction: ICL-SJTU Submission to EPIC-Kitchens Action Anticipation Challenge 2021. (arXiv:2107.13259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "Modelling long-range contextual relationships is critical for pixel-wise\nprediction tasks such as semantic segmentation. However, convolutional neural\nnetworks (CNNs) are inherently limited to model such dependencies due to the\nnaive structure in its building modules (\\eg, local convolution kernel). While\nrecent global aggregation methods are beneficial for long-range structure\ninformation modelling, they would oversmooth and bring noise to the regions\ncontaining fine details (\\eg,~boundaries and small objects), which are very\nmuch cared for the semantic segmentation task. To alleviate this problem, we\npropose to explore the local context for making the aggregated long-range\nrelationship being distributed more accurately in local regions. In particular,\nwe design a novel local distribution module which models the affinity map\nbetween global and local relationship for each pixel adaptively. Integrating\nexisting global aggregation modules, we show that our approach can be\nmodularized as an end-to-end trainable block and easily plugged into existing\nsemantic segmentation networks, giving rise to the \\emph{GALD} networks.\nDespite its simplicity and versatility, our approach allows us to build new\nstate of the art on major semantic segmentation benchmarks including\nCityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained\nmodels are released at \\url{https://github.com/lxtGH/GALD-DGCNet} to foster\nfurther research.",
          "link": "http://arxiv.org/abs/2107.13154",
          "publishedOn": "2021-07-29T02:00:08.276Z",
          "wordCount": 652,
          "title": "Global Aggregation then Local Distribution for Scene Parsing. (arXiv:2107.13154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeesoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Weakly-supervised object localization (WSOL) enables finding an object using\na dataset without any localization information. By simply training a\nclassification model using only image-level annotations, the feature map of the\nmodel can be utilized as a score map for localization. In spite of many WSOL\nmethods proposing novel strategies, there has not been any de facto standard\nabout how to normalize the class activation map (CAM). Consequently, many WSOL\nmethods have failed to fully exploit their own capacity because of the misuse\nof a normalization method. In this paper, we review many existing normalization\nmethods and point out that they should be used according to the property of the\ngiven dataset. Additionally, we propose a new normalization method which\nsubstantially enhances the performance of any CAM-based WSOL methods. Using the\nproposed normalization method, we provide a comprehensive evaluation over three\ndatasets (CUB, ImageNet and OpenImages) on three different architectures and\nobserve significant performance gains over the conventional min-max\nnormalization method in all the evaluated cases.",
          "link": "http://arxiv.org/abs/2107.13221",
          "publishedOn": "2021-07-29T02:00:08.264Z",
          "wordCount": 608,
          "title": "Normalization Matters in Weakly Supervised Object Localization. (arXiv:2107.13221v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lomurno_E/0/1/0/all/0/1\">Eugenio Lomurno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanoni_A/0/1/0/all/0/1\">Andrea Romanoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>",
          "description": "Today, Multi-View Stereo techniques are able to reconstruct robust and\ndetailed 3D models, especially when starting from high-resolution images.\nHowever, there are cases in which the resolution of input images is relatively\nlow, for instance, when dealing with old photos, or when hardware constrains\nthe amount of data that can be acquired. In this paper, we investigate if, how,\nand how much increasing the resolution of such input images through\nSuper-Resolution techniques reflects in quality improvements of the\nreconstructed 3D models, despite the artifacts that sometimes this may\ngenerate. We show that applying a Super-Resolution step before recovering the\ndepth maps in most cases leads to a better 3D model both in the case of\nPatchMatch-based and deep-learning-based algorithms. The use of\nSuper-Resolution improves especially the completeness of reconstructed models\nand turns out to be particularly effective in the case of textured scenes.",
          "link": "http://arxiv.org/abs/2107.13261",
          "publishedOn": "2021-07-29T02:00:08.240Z",
          "wordCount": 574,
          "title": "Improving Multi-View Stereo via Super-Resolution. (arXiv:2107.13261v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Ti Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagopal_A/0/1/0/all/0/1\">Anjali Balagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohopolski_M/0/1/0/all/0/1\">Michael Dohopolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_H/0/1/0/all/0/1\">Howard E. Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McBeth_R/0/1/0/all/0/1\">Rafe McBeth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mu-Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sher_D/0/1/0/all/0/1\">David J. Sher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Steve Jiang</a>",
          "description": "Automatic segmentation of anatomical structures is critical for many medical\napplications. However, the results are not always clinically acceptable and\nrequire tedious manual revision. Here, we present a novel concept called\nartificial intelligence assisted contour revision (AIACR) and demonstrate its\nfeasibility. The proposed clinical workflow of AIACR is as follows given an\ninitial contour that requires a clinicians revision, the clinician indicates\nwhere a large revision is needed, and a trained deep learning (DL) model takes\nthis input to update the contour. This process repeats until a clinically\nacceptable contour is achieved. The DL model is designed to minimize the\nclinicians input at each iteration and to minimize the number of iterations\nneeded to reach acceptance. In this proof-of-concept study, we demonstrated the\nconcept on 2D axial images of three head-and-neck cancer datasets, with the\nclinicians input at each iteration being one mouse click on the desired\nlocation of the contour segment. The performance of the model is quantified\nwith Dice Similarity Coefficient (DSC) and 95th percentile of Hausdorff\nDistance (HD95). The average DSC/HD95 (mm) of the auto-generated initial\ncontours were 0.82/4.3, 0.73/5.6 and 0.67/11.4 for three datasets, which were\nimproved to 0.91/2.1, 0.86/2.4 and 0.86/4.7 with three mouse clicks,\nrespectively. Each DL-based contour update requires around 20 ms. We proposed a\nnovel AIACR concept that uses DL models to assist clinicians in revising\ncontours in an efficient and effective way, and we demonstrated its feasibility\nby using 2D axial CT images from three head-and-neck cancer datasets.",
          "link": "http://arxiv.org/abs/2107.13465",
          "publishedOn": "2021-07-29T02:00:08.231Z",
          "wordCount": 704,
          "title": "A Proof-of-Concept Study of Artificial Intelligence Assisted Contour Revision. (arXiv:2107.13465v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1\">Aaron Lopez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarello_P/0/1/0/all/0/1\">Pedro Zuccarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Francesc J. Ferri</a>",
          "description": "The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.",
          "link": "http://arxiv.org/abs/2107.13180",
          "publishedOn": "2021-07-29T02:00:08.204Z",
          "wordCount": 712,
          "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for Audio-Visual Scene Classification. (arXiv:2107.13180v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barquero_G/0/1/0/all/0/1\">Germ&#xe1;n Barquero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupont_I/0/1/0/all/0/1\">Isabelle Hupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_C/0/1/0/all/0/1\">Carles Fern&#xe1;ndez</a>",
          "description": "Most current multi-object trackers focus on short-term tracking, and are\nbased on deep and complex systems that often cannot operate in real-time,\nmaking them impractical for video-surveillance. In this paper we present a\nlong-term, multi-face tracking architecture conceived for working in crowded\ncontexts where faces are often the only visible part of a person. Our system\nbenefits from advances in the fields of face detection and face recognition to\nachieve long-term tracking, and is particularly unconstrained to the motion and\nocclusions of people. It follows a tracking-by-detection approach, combining a\nfast short-term visual tracker with a novel online tracklet reconnection\nstrategy grounded on rank-based face verification. The proposed rank-based\nconstraint favours higher inter-class distance among tracklets, and reduces the\npropagation of errors due to wrong reconnections. Additionally, a correction\nmodule is included to correct past assignments with no extra computational\ncost. We present a series of experiments introducing novel specialized metrics\nfor the evaluation of long-term tracking capabilities, and publicly release a\nvideo dataset with 10 manually annotated videos and a total length of 8' 54\".\nOur findings validate the robustness of each of the proposed modules, and\ndemonstrate that, in these challenging contexts, our approach yields up to 50%\nlonger tracks than state-of-the-art deep learning trackers.",
          "link": "http://arxiv.org/abs/2107.13273",
          "publishedOn": "2021-07-29T02:00:08.192Z",
          "wordCount": 664,
          "title": "Rank-based verification for long-term face tracking in crowded scenes. (arXiv:2107.13273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodin_I/0/1/0/all/0/1\">Ivan Rodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavroedis_D/0/1/0/all/0/1\">Dimitrios Mavroedis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>",
          "description": "Egocentric videos can bring a lot of information about how humans perceive\nthe world and interact with the environment, which can be beneficial for the\nanalysis of human behaviour. The research in egocentric video analysis is\ndeveloping rapidly thanks to the increasing availability of wearable devices\nand the opportunities offered by new large-scale egocentric datasets. As\ncomputer vision techniques continue to develop at an increasing pace, the tasks\nrelated to the prediction of future are starting to evolve from the need of\nunderstanding the present. Predicting future human activities, trajectories and\ninteractions with objects is crucial in applications such as human-robot\ninteraction, assistive wearable technologies for both industrial and daily\nliving scenarios, entertainment and virtual or augmented reality. This survey\nsummarises the evolution of studies in the context of future prediction from\negocentric vision making an overview of applications, devices, existing\nproblems, commonly used datasets, models and input modalities. Our analysis\nhighlights that methods for future prediction from egocentric vision can have a\nsignificant impact in a range of applications and that further research efforts\nshould be devoted to the standardisation of tasks and the proposal of datasets\nconsidering real-world scenarios such as the ones with an industrial vocation.",
          "link": "http://arxiv.org/abs/2107.13411",
          "publishedOn": "2021-07-29T02:00:08.160Z",
          "wordCount": 647,
          "title": "Predicting the Future from First Person (Egocentric) Vision: A Survey. (arXiv:2107.13411v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagong_M/0/1/0/all/0/1\">Min-Cheol Sagong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1\">Yoon-Jae Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sung-Jea Ko</a>",
          "description": "Convolutional neural networks (CNNs) have been not only widespread but also\nachieved noticeable results on numerous applications including image\nclassification, restoration, and generation. Although the weight-sharing\nproperty of convolutions makes them widely adopted in various tasks, its\ncontent-agnostic characteristic can also be considered a major drawback. To\nsolve this problem, in this paper, we propose a novel operation, called pixel\nadaptive kernel attention (PAKA). PAKA provides directivity to the filter\nweights by multiplying spatially varying attention from learnable features. The\nproposed method infers pixel-adaptive attention maps along the channel and\nspatial directions separately to address the decomposed model with fewer\nparameters. Our method is trainable in an end-to-end manner and applicable to\nany CNN-based models. In addition, we propose an improved information\naggregation module with PAKA, called the hierarchical PAKA module (HPM). We\ndemonstrate the superiority of our HPM by presenting state-of-the-art\nperformance on semantic segmentation compared to the conventional information\naggregation modules. We validate the proposed method through additional\nablation studies and visualizing the effect of PAKA providing directivity to\nthe weights of convolutions. We also show the generalizability of the proposed\nmethod by applying it to multi-modal tasks especially color-guided depth map\nsuper-resolution.",
          "link": "http://arxiv.org/abs/2107.13144",
          "publishedOn": "2021-07-29T02:00:08.140Z",
          "wordCount": 647,
          "title": "Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention. (arXiv:2107.13144v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_G/0/1/0/all/0/1\">Guohua Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xingxing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>",
          "description": "The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum\nSite Museum is handcrafted by experts, and the increasing amounts of unearthed\npieces of terracotta warriors make the archaeologists too challenging to\nconduct the restoration of terracotta warriors efficiently. We hope to segment\nthe 3D point cloud data of the terracotta warriors automatically and store the\nfragment data in the database to assist the archaeologists in matching the\nactual fragments with the ones in the database, which could result in higher\nrepairing efficiency of terracotta warriors. Moreover, the existing 3D neural\nnetwork research is mainly focusing on supervised classification, clustering,\nunsupervised representation, and reconstruction. There are few pieces of\nresearches concentrating on unsupervised point cloud part segmentation. In this\npaper, we present SRG-Net for 3D point clouds of terracotta warriors to address\nthese problems. Firstly, we adopt a customized seed-region-growing algorithm to\nsegment the point cloud coarsely. Then we present a supervised segmentation and\nunsupervised reconstruction networks to learn the characteristics of 3D point\nclouds. Finally, we combine the SRG algorithm with our improved CNN using a\nrefinement method. This pipeline is called SRG-Net, which aims at conducting\nsegmentation tasks on the terracotta warriors. Our proposed SRG-Net is\nevaluated on the terracotta warriors data and ShapeNet dataset by measuring the\naccuracy and the latency. The experimental results show that our SRG-Net\noutperforms the state-of-the-art methods. Our code is shown in Code File\n1~\\cite{Srgnet_2021}.",
          "link": "http://arxiv.org/abs/2107.13167",
          "publishedOn": "2021-07-29T02:00:08.132Z",
          "wordCount": 681,
          "title": "Unsupervised Segmentation for Terracotta Warrior with Seed-Region-Growing CNN(SRG-Net). (arXiv:2107.13167v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrido_L/0/1/0/all/0/1\">Llu&#xed;s Garrido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Corominas_G/0/1/0/all/0/1\">Guillem Rodriguez-Corominas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_H/0/1/0/all/0/1\">Herwig Wendt</a>",
          "description": "Recently, transfer subspace learning based approaches have shown to be a\nvalid alternative to unsupervised subspace clustering and temporal data\nclustering for human motion segmentation (HMS). These approaches leverage prior\nknowledge from a source domain to improve clustering performance on a target\ndomain, and currently they represent the state of the art in HMS. Bucking this\ntrend, in this paper, we propose a novel unsupervised model that learns a\nrepresentation of the data and digs clustering information from the data\nitself. Our model is reminiscent of temporal subspace clustering, but presents\ntwo critical differences. First, we learn an auxiliary data matrix that can\ndeviate from the initial data, hence confer more degrees of freedom to the\ncoding matrix. Second, we introduce a regularization term for this auxiliary\ndata matrix that preserves the local geometrical structure present in the\nhigh-dimensional space. The proposed model is efficiently optimized by using an\noriginal Alternating Direction Method of Multipliers (ADMM) formulation\nallowing to learn jointly the auxiliary data representation, a nonnegative\ndictionary and a coding matrix. Experimental results on four benchmark datasets\nfor HMS demonstrate that our approach achieves significantly better clustering\nperformance then state-of-the-art methods, including both unsupervised and more\nrecent semi-supervised transfer learning approaches.",
          "link": "http://arxiv.org/abs/2107.13362",
          "publishedOn": "2021-07-29T02:00:08.124Z",
          "wordCount": 645,
          "title": "Graph Constrained Data Representation Learning for Human Motion Segmentation. (arXiv:2107.13362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13237",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_U/0/1/0/all/0/1\">Uddipan Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1\">Sidharth Pancholi</a>",
          "description": "Heart disease is the most common reason for human mortality that causes\nalmost one-third of deaths throughout the world. Detecting the disease early\nincreases the chances of survival of the patient and there are several ways a\nsign of heart disease can be detected early. This research proposes to convert\ncleansed and normalized heart sound into visual mel scale spectrograms and then\nusing visual domain transfer learning approaches to automatically extract\nfeatures and categorize between heart sounds. Some of the previous studies\nfound that the spectrogram of various types of heart sounds is visually\ndistinguishable to human eyes, which motivated this study to experiment on\nvisual domain classification approaches for automated heart sound\nclassification. It will use convolution neural network-based architectures i.e.\nResNet, MobileNetV2, etc as the automated feature extractors from spectrograms.\nThese well-accepted models in the image domain showed to learn generalized\nfeature representations of cardiac sounds collected from different environments\nwith varying amplitude and noise levels. Model evaluation criteria used were\ncategorical accuracy, precision, recall, and AUROC as the chosen dataset is\nunbalanced. The proposed approach has been implemented on datasets A and B of\nthe PASCAL heart sound collection and resulted in ~ 90% categorical accuracy\nand AUROC of ~0.97 for both sets.",
          "link": "http://arxiv.org/abs/2107.13237",
          "publishedOn": "2021-07-29T02:00:08.116Z",
          "wordCount": 652,
          "title": "A Visual Domain Transfer Learning Approach for Heartbeat Sound Classification. (arXiv:2107.13237v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1\">Geetika Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1\">Rohit K Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kamlesh Tiwari</a>",
          "description": "This paper proposes teeth-photo, a new biometric modality for human\nauthentication on mobile and hand held devices. Biometrics samples are acquired\nusing the camera mounted on mobile device with the help of a mobile application\nhaving specific markers to register the teeth area. Region of interest (RoI) is\nthen extracted using the markers and the obtained sample is enhanced using\ncontrast limited adaptive histogram equalization (CLAHE) for better visual\nclarity. We propose a deep learning architecture and novel regularization\nscheme to obtain highly discriminative embedding for small size RoI. Proposed\ncustom loss function was able to achieve perfect classification for the tiny\nRoI of $75\\times 75$ size. The model is end-to-end and few-shot and therefore\nis very efficient in terms of time and energy requirements. The system can be\nused in many ways including device unlocking and secure authentication. To the\nbest of our understanding, this is the first work on teeth-photo based\nauthentication for mobile device. Experiments have been conducted on an\nin-house teeth-photo database collected using our application. The database is\nmade publicly available. Results have shown that the proposed system has\nperfect accuracy.",
          "link": "http://arxiv.org/abs/2107.13217",
          "publishedOn": "2021-07-29T02:00:08.109Z",
          "wordCount": 632,
          "title": "DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>",
          "description": "Video Instance Segmentation (VIS) is a new and inherently multi-task problem,\nwhich aims to detect, segment and track each instance in a video sequence.\nExisting approaches are mainly based on single-frame features or single-scale\nfeatures of multiple frames, where temporal information or multi-scale\ninformation is ignored. To incorporate both temporal and scale information, we\npropose a Temporal Pyramid Routing (TPR) strategy to conditionally align and\nconduct pixel-level aggregation from a feature pyramid pair of two adjacent\nframes. Specifically, TPR contains two novel components, including Dynamic\nAligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is\ndesigned for aligning and gating pyramid features across temporal dimension,\nwhile CPR transfers temporally aggregated features across scale dimension.\nMoreover, our approach is a plug-and-play module and can be easily applied to\nexisting instance segmentation methods. Extensive experiments on YouTube-VIS\ndataset demonstrate the effectiveness and efficiency of the proposed approach\non several state-of-the-art instance segmentation methods. Codes and trained\nmodels will be publicly available to facilitate future\nresearch.(\\url{https://github.com/lxtGH/TemporalPyramidRouting}).",
          "link": "http://arxiv.org/abs/2107.13155",
          "publishedOn": "2021-07-29T02:00:08.090Z",
          "wordCount": 609,
          "title": "Improving Video Instance Segmentation via Temporal Pyramid Routing. (arXiv:2107.13155v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "We present a new neural representation, called Neural Ray (NeuRay), for the\nnovel view synthesis (NVS) task with multi-view images as input. Existing\nneural scene representations for solving the NVS problem, such as NeRF, cannot\ngeneralize to new scenes and take an excessively long time on training on each\nnew scene from scratch. The other subsequent neural rendering methods based on\nstereo matching, such as PixelNeRF, SRF and IBRNet are designed to generalize\nto unseen scenes but suffer from view inconsistency in complex scenes with\nself-occlusions. To address these issues, our NeuRay method represents every\nscene by encoding the visibility of rays associated with the input views. This\nneural representation can efficiently be initialized from depths estimated by\nexternal MVS methods, which is able to generalize to new scenes and achieves\nsatisfactory rendering images without any training on the scene. Then, the\ninitialized NeuRay can be further optimized on every scene with little training\ntiming to enforce spatial coherence to ensure view consistency in the presence\nof severe self-occlusion. Experiments demonstrate that NeuRay can quickly\ngenerate high-quality novel view images of unseen scenes with little finetuning\nand can handle complex scenes with severe self-occlusions which previous\nmethods struggle with.",
          "link": "http://arxiv.org/abs/2107.13421",
          "publishedOn": "2021-07-29T02:00:08.082Z",
          "wordCount": 647,
          "title": "Neural Rays for Occlusion-aware Image-based Rendering. (arXiv:2107.13421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kyrkou_C/0/1/0/all/0/1\">Christos Kyrkou</a>",
          "description": "The need for automated real-time visual systems in applications such as smart\ncamera surveillance, smart environments, and drones necessitates the\nimprovement of methods for visual active monitoring and control. Traditionally,\nthe active monitoring task has been handled through a pipeline of modules such\nas detection, filtering, and control. However, such methods are difficult to\njointly optimize and tune their various parameters for real-time processing in\nresource constraint systems. In this paper a deep Convolutional Camera\nController Neural Network is proposed to go directly from visual information to\ncamera movement to provide an efficient solution to the active vision problem.\nIt is trained end-to-end without bounding box annotations to control a camera\nand follow multiple targets from raw pixel values. Evaluation through both a\nsimulation framework and real experimental setup, indicate that the proposed\nsolution is robust to varying conditions and able to achieve better monitoring\nperformance than traditional approaches both in terms of number of targets\nmonitored as well as in effective monitoring time. The advantage of the\nproposed approach is that it is computationally less demanding and can run at\nover 10 FPS (~4x speedup) on an embedded smart camera providing a practical and\naffordable solution to real-time active monitoring.",
          "link": "http://arxiv.org/abs/2107.13233",
          "publishedOn": "2021-07-29T02:00:08.071Z",
          "wordCount": 673,
          "title": "C^3Net: End-to-End deep learning for efficient real-time visual active camera control. (arXiv:2107.13233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ze Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Remote photoplethysmography (rPPG) monitors heart rate without requiring\nphysical contact, which allows for a wide variety of applications. Deep\nlearning-based rPPG have demonstrated superior performance over the traditional\napproaches in controlled context. However, the lighting situation in indoor\nspace is typically complex, with uneven light distribution and frequent\nvariations in illumination. It lacks a fair comparison of different methods\nunder different illuminations using the same dataset. In this paper, we present\na public dataset, namely the BH-rPPG dataset, which contains data from twelve\nsubjects under three illuminations: low, medium, and high illumination. We also\nprovide the ground truth heart rate measured by an oximeter. We evaluate the\nperformance of three deep learning-based methods to that of four traditional\nmethods using two public datasets: the UBFC-rPPG dataset and the BH-rPPG\ndataset. The experimental results demonstrate that traditional methods are\ngenerally more resistant to fluctuating illuminations. We found that the\nrPPGNet achieves lowest MAE among deep learning-based method under medium\nillumination, whereas the CHROM achieves 1.5 beats per minute (BPM),\noutperforming the rPPGNet by 60%. These findings suggest that while developing\ndeep learning-based heart rate estimation algorithms, illumination variation\nshould be taken into account. This work serves as a benchmark for rPPG\nperformance evaluation and it opens a pathway for future investigation into\ndeep learning-based rPPG under illumination variations.",
          "link": "http://arxiv.org/abs/2107.13193",
          "publishedOn": "2021-07-29T02:00:08.022Z",
          "wordCount": 666,
          "title": "Assessment of Deep Learning-based Heart Rate Estimation using Remote Photoplethysmography under Different Illuminations. (arXiv:2107.13193v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13048",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shaban_M/0/1/0/all/0/1\">Muhammad Shaban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chengkuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>",
          "description": "Cancer prognostication is a challenging task in computational pathology that\nrequires context-aware representations of histology features to adequately\ninfer patient survival. Despite the advancements made in weakly-supervised deep\nlearning, many approaches are not context-aware and are unable to model\nimportant morphological feature interactions between cell identities and tissue\ntypes that are prognostic for patient survival. In this work, we present\nPatch-GCN, a context-aware, spatially-resolved patch-based graph convolutional\nnetwork that hierarchically aggregates instance-level histology features to\nmodel local- and global-level topological structures in the tumor\nmicroenvironment. We validate Patch-GCN with 4,370 gigapixel WSIs across five\ndifferent cancer types from the Cancer Genome Atlas (TCGA), and demonstrate\nthat Patch-GCN outperforms all prior weakly-supervised approaches by\n3.58-9.46%. Our code and corresponding models are publicly available at\nhttps://github.com/mahmoodlab/Patch-GCN.",
          "link": "http://arxiv.org/abs/2107.13048",
          "publishedOn": "2021-07-29T02:00:07.969Z",
          "wordCount": 606,
          "title": "Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks. (arXiv:2107.13048v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Current geometry-based monocular 3D object detection models can efficiently\ndetect objects by leveraging perspective geometry, but their performance is\nlimited due to the absence of accurate depth information. Though this issue can\nbe alleviated in a depth-based model where a depth estimation module is plugged\nto predict depth information before 3D box reasoning, the introduction of such\nmodule dramatically reduces the detection speed. Instead of training a costly\ndepth estimator, we propose a rendering module to augment the training data by\nsynthesizing images with virtual-depths. The rendering module takes as input\nthe RGB image and its corresponding sparse depth image, outputs a variety of\nphoto-realistic synthetic images, from which the detection model can learn more\ndiscriminative features to adapt to the depth changes of the objects. Besides,\nwe introduce an auxiliary module to improve the detection model by jointly\noptimizing it through a depth estimation task. Both modules are working in the\ntraining time and no extra computation will be introduced to the detection\nmodel. Experiments show that by working with our proposed modules, a\ngeometry-based model can represent the leading accuracy on the KITTI 3D\ndetection benchmark.",
          "link": "http://arxiv.org/abs/2107.13269",
          "publishedOn": "2021-07-29T02:00:07.928Z",
          "wordCount": 636,
          "title": "Aug3D-RPN: Improving Monocular 3D Object Detection by Synthetic Images with Virtual Depth. (arXiv:2107.13269v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1\">Jocelyn Desbiens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1\">Ron Gross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Purpose: To demonstrate that retinal microvasculature per se is a reliable\nbiomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular\ndiseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to\ncolor fundus images for semantic segmentation of the blood vessels and severity\nclassification on both vascular and full images. Vessel reconstruction through\nharmonic descriptors is also used as a smoothing and de-noising tool. The\nmathematical background of the theory is also outlined. Results: For diabetic\npatients, at least 93.8% of DR No-Refer vs. Refer classification can be related\nto vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening\ncase, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the\ndisease biomarkers are related topologically to the vasculature. Translational\nRelevance: Experiments conducted on eye blood vasculature reconstruction as a\nbiomarker shows a strong correlation between vasculature shape and later stages\nof DR.",
          "link": "http://arxiv.org/abs/2107.13157",
          "publishedOn": "2021-07-29T02:00:07.920Z",
          "wordCount": 603,
          "title": "Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:07.913Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>",
          "description": "In this paper, we present a set of extremely efficient and high throughput\nmodels for accurate face verification, MixFaceNets which are inspired by Mixed\nDepthwise Convolutional Kernels. Extensive experiment evaluations on Label Face\nin the Wild (LFW), Age-DB, MegaFace, and IARPA Janus Benchmarks IJB-B and IJB-C\ndatasets have shown the effectiveness of our MixFaceNets for applications\nrequiring extremely low computational complexity. Under the same level of\ncomputation complexity (< 500M FLOPs), our MixFaceNets outperform\nMobileFaceNets on all the evaluated datasets, achieving 99.60% accuracy on LFW,\n97.05% accuracy on AgeDB-30, 93.60 TAR (at FAR1e-6) on MegaFace, 90.94 TAR (at\nFAR1e-4) on IJB-B and 93.08 TAR (at FAR1e-4) on IJB-C. With computational\ncomplexity between 500M and 1G FLOPs, our MixFaceNets achieved results\ncomparable to the top-ranked models, while using significantly fewer FLOPs and\nless computation overhead, which proves the practical value of our proposed\nMixFaceNets. All training codes, pre-trained models, and training logs have\nbeen made available https://github.com/fdbtrs/mixfacenets.",
          "link": "http://arxiv.org/abs/2107.13046",
          "publishedOn": "2021-07-29T02:00:07.905Z",
          "wordCount": 602,
          "title": "MixFaceNets: Extremely Efficient Face Recognition Networks. (arXiv:2107.13046v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaojie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Video prediction methods generally consume substantial computing resources in\ntraining and deployment, among which keypoint-based approaches show promising\nimprovement in efficiency by simplifying dense image prediction to light\nkeypoint prediction. However, keypoint locations are often modeled only as\ncontinuous coordinates, so noise from semantically insignificant deviations in\nvideos easily disrupt learning stability, leading to inaccurate keypoint\nmodeling. In this paper, we design a new grid keypoint learning framework,\naiming at a robust and explainable intermediate keypoint representation for\nlong-term efficient video prediction. We have two major technical\ncontributions. First, we detect keypoints by jumping among candidate locations\nin our raised grid space and formulate a condensation loss to encourage\nmeaningful keypoints with strong representative capability. Second, we\nintroduce a 2D binary map to represent the detected grid keypoints and then\nsuggest propagating keypoint locations with stochasticity by selecting entries\nin the discrete grid space, thus preserving the spatial structure of keypoints\nin the longterm horizon for better future frame generation. Extensive\nexperiments verify that our method outperforms the state-ofthe-art stochastic\nvideo prediction methods while saves more than 98% of computing resources. We\nalso demonstrate our method on a robotic-assisted surgery dataset with\npromising results. Our code is available at\nhttps://github.com/xjgaocs/Grid-Keypoint-Learning.",
          "link": "http://arxiv.org/abs/2107.13170",
          "publishedOn": "2021-07-29T02:00:07.898Z",
          "wordCount": 642,
          "title": "Accurate Grid Keypoint Learning for Efficient Video Prediction. (arXiv:2107.13170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1\">Karima Kadaoui</a>",
          "description": "Image Captioning is a task that combines computer vision and natural language\nprocessing, where it aims to generate descriptive legends for images. It is a\ntwo-fold process relying on accurate image understanding and correct language\nunderstanding both syntactically and semantically. It is becoming increasingly\ndifficult to keep up with the latest research and findings in the field of\nimage captioning due to the growing amount of knowledge available on the topic.\nThere is not, however, enough coverage of those findings in the available\nreview papers. We perform in this paper a run-through of the current\ntechniques, datasets, benchmarks and evaluation metrics used in image\ncaptioning. The current research on the field is mostly focused on deep\nlearning-based methods, where attention mechanisms along with deep\nreinforcement and adversarial learning appear to be in the forefront of this\nresearch topic. In this paper, we review recent methodologies such as UpDown,\nOSCAR, VIVO, Meta Learning and a model that uses conditional generative\nadversarial nets. Although the GAN-based model achieves the highest score,\nUpDown represents an important basis for image captioning and OSCAR and VIVO\nare more useful as they use novel object captioning. This review paper serves\nas a roadmap for researchers to keep up to date with the latest contributions\nmade in the field of image caption generation.",
          "link": "http://arxiv.org/abs/2107.13114",
          "publishedOn": "2021-07-29T02:00:07.877Z",
          "wordCount": 653,
          "title": "A Thorough Review on Recent Deep Learning Methodologies for Image Captioning. (arXiv:2107.13114v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiyu Sun</a>",
          "description": "Previous unsupervised monocular depth estimation methods mainly focus on the\nday-time scenario, and their frameworks are driven by warped photometric\nconsistency. While in some challenging environments, like night, rainy night or\nsnowy winter, the photometry of the same pixel on different frames is\ninconsistent because of the complex lighting and reflection, so that the\nday-time unsupervised frameworks cannot be directly applied to these complex\nscenarios. In this paper, we investigate the problem of unsupervised monocular\ndepth estimation in certain highly complex scenarios. We address this\nchallenging problem by using domain adaptation, and a unified image\ntransfer-based adaptation framework is proposed based on monocular videos in\nthis paper. The depth model trained on day-time scenarios is adapted to\ndifferent complex scenarios. Instead of adapting the whole depth network, we\njust consider the encoder network for lower computational complexity. The depth\nmodels adapted by the proposed framework to different scenarios share the same\ndecoder, which is practical. Constraints on both feature space and output space\npromote the framework to learn the key features for depth decoding, and the\nsmoothness loss is introduced into the adaptation framework for better depth\nestimation performance. Extensive experiments show the effectiveness of the\nproposed unsupervised framework in estimating the dense depth map from the\nnight-time, rainy night-time and snowy winter images.",
          "link": "http://arxiv.org/abs/2107.13137",
          "publishedOn": "2021-07-29T02:00:07.868Z",
          "wordCount": 653,
          "title": "Unsupervised Monocular Depth Estimation in Highly Complex Environments. (arXiv:2107.13137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadaoui_K/0/1/0/all/0/1\">Karima Kadaoui</a>",
          "description": "Image captioning is a task in the field of Artificial Intelligence that\nmerges between computer vision and natural language processing. It is\nresponsible for generating legends that describe images, and has various\napplications like descriptions used by assistive technology or indexing images\n(for search engines for instance). This makes it a crucial topic in AI that is\nundergoing a lot of research. This task however, like many others, is trained\non large images labeled via human annotation, which can be very cumbersome: it\nneeds manual effort, both financial and temporal costs, it is error-prone and\npotentially difficult to execute in some cases (e.g. medical images). To\nmitigate the need for labels, we attempt to use self-supervised learning, a\ntype of learning where models use the data contained within the images\nthemselves as labels. It is challenging to accomplish though, since the task is\ntwo-fold: the images and captions come from two different modalities and\nusually handled by different types of networks. It is thus not obvious what a\ncompletely self-supervised solution would look like. How it would achieve\ncaptioning in a comparable way to how self-supervision is applied today on\nimage recognition tasks is still an ongoing research topic. In this project, we\nare using an encoder-decoder architecture where the encoder is a convolutional\nneural network (CNN) trained on OpenImages dataset and learns image features in\na self-supervised fashion using the rotation pretext task. The decoder is a\nLong Short-Term Memory (LSTM), and it is trained, along within the image\ncaptioning model, on MS COCO dataset and is responsible of generating captions.\nOur GitHub repository can be found:\nhttps://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction",
          "link": "http://arxiv.org/abs/2107.13111",
          "publishedOn": "2021-07-29T02:00:07.855Z",
          "wordCount": 704,
          "title": "Experimenting with Self-Supervision using Rotation Prediction for Image Captioning. (arXiv:2107.13111v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1\">Zach Nussbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "As machine learning models are increasingly employed to assist human\ndecision-makers, it becomes critical to communicate the uncertainty associated\nwith these model predictions. However, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking approaches - where the model\nassigns low probabilities or scores to uncertain examples. While this captures\nwhat examples are challenging for the model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek to identify examples the model\nis uncertain about and characterize the source of said uncertainty. We explore\nthe benefits of designing a targeted intervention - targeted data augmentation\nof the examples where the model is uncertain over the course of training. We\ninvestigate whether the rate of learning in the presence of additional\ninformation differs between atypical and noisy examples? Our results show that\nthis is indeed the case, suggesting that well-designed interventions over the\ncourse of training can be an effective way to characterize and distinguish\nbetween different sources of uncertainty.",
          "link": "http://arxiv.org/abs/2107.13098",
          "publishedOn": "2021-07-29T02:00:07.847Z",
          "wordCount": 618,
          "title": "A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jinlei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qiaoyong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>",
          "description": "Reconstruction-based methods play an important role in unsupervised anomaly\ndetection in images. Ideally, we expect a perfect reconstruction for normal\nsamples and poor reconstruction for abnormal samples. Since the\ngeneralizability of deep neural networks is difficult to control, existing\nmodels such as autoencoder do not work well. In this work, we interpret the\nreconstruction of an image as a divide-and-assemble procedure. Surprisingly, by\nvarying the granularity of division on feature maps, we are able to modulate\nthe reconstruction capability of the model for both normal and abnormal\nsamples. That is, finer granularity leads to better reconstruction, while\ncoarser granularity leads to poorer reconstruction. With proper granularity,\nthe gap between the reconstruction error of normal and abnormal samples can be\nmaximized. The divide-and-assemble framework is implemented by embedding a\nnovel multi-scale block-wise memory module into an autoencoder network.\nBesides, we introduce adversarial learning and explore the semantic latent\nrepresentation of the discriminator, which improves the detection of subtle\nanomaly. We achieve state-of-the-art performance on the challenging MVTec AD\ndataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms\nof the AUROC score.",
          "link": "http://arxiv.org/abs/2107.13118",
          "publishedOn": "2021-07-29T02:00:07.831Z",
          "wordCount": 628,
          "title": "Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection. (arXiv:2107.13118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1\">Reece Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1\">Mohamed H. Abdelpakey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed S. Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M.Mohamed</a>",
          "description": "Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.",
          "link": "http://arxiv.org/abs/2107.13093",
          "publishedOn": "2021-07-29T02:00:07.805Z",
          "wordCount": 720,
          "title": "Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1\">Joseph Marino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>",
          "description": "While recent machine learning research has revealed connections between deep\ngenerative models such as VAEs and rate-distortion losses used in learned\ncompression, most of this work has focused on images. In a similar spirit, we\nview recently proposed neural video coding algorithms through the lens of deep\nautoregressive and latent variable modeling. We present recent neural video\ncodecs as instances of a generalized stochastic temporal autoregressive\ntransform, and propose new avenues for further improvements inspired by\nnormalizing flows and structured priors. We propose several architectures that\nyield state-of-the-art video compression performance on full-resolution video\nand discuss their tradeoffs and ablations. In particular, we propose (i)\nimproved temporal autoregressive transforms, (ii) improved entropy models with\nstructured and temporal dependencies, and (iii) variable bitrate versions of\nour algorithms. Since our improvements are compatible with a large class of\nexisting models, we provide further evidence that the generative modeling\nviewpoint can advance the neural video coding field.",
          "link": "http://arxiv.org/abs/2107.13136",
          "publishedOn": "2021-07-29T02:00:07.797Z",
          "wordCount": 637,
          "title": "Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>",
          "description": "This thesis presents methods and approaches to image color correction, color\nenhancement, and color editing. To begin, we study the color correction problem\nfrom the standpoint of the camera's image signal processor (ISP). A camera's\nISP is hardware that applies a series of in-camera image processing and color\nmanipulation steps, many of which are nonlinear in nature, to render the\ninitial sensor image to its final photo-finished representation saved in the\n8-bit standard RGB (sRGB) color space. As white balance (WB) is one of the\nmajor procedures applied by the ISP for color correction, this thesis presents\ntwo different methods for ISP white balancing. Afterward, we discuss another\nscenario of correcting and editing image colors, where we present a set of\nmethods to correct and edit WB settings for images that have been improperly\nwhite-balanced by the ISP. Then, we explore another factor that has a\nsignificant impact on the quality of camera-rendered colors, in which we\noutline two different methods to correct exposure errors in camera-rendered\nimages. Lastly, we discuss post-capture auto color editing and manipulation. In\nparticular, we propose auto image recoloring methods to generate different\nrealistic versions of the same camera-rendered image with new colors. Through\nextensive evaluations, we demonstrate that our methods provide superior\nsolutions compared to existing alternatives targeting color correction, color\nenhancement, and color editing.",
          "link": "http://arxiv.org/abs/2107.13117",
          "publishedOn": "2021-07-29T02:00:07.789Z",
          "wordCount": 649,
          "title": "Image color correction, enhancement, and editing. (arXiv:2107.13117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "This paper presents a neural network built upon Transformers, namely PlaneTR,\nto simultaneously detect and reconstruct planes from a single image. Different\nfrom previous methods, PlaneTR jointly leverages the context information and\nthe geometric structures in a sequence-to-sequence way to holistically detect\nplane instances in one forward pass. Specifically, we represent the geometric\nstructures as line segments and conduct the network with three main components:\n(i) context and line segments encoders, (ii) a structure-guided plane decoder,\n(iii) a pixel-wise plane embedding decoder. Given an image and its detected\nline segments, PlaneTR generates the context and line segment sequences via two\nspecially designed encoders and then feeds them into a Transformers-based\ndecoder to directly predict a sequence of plane instances by simultaneously\nconsidering the context and global structure cues. Finally, the pixel-wise\nembeddings are computed to assign each pixel to one predicted plane instance\nwhich is nearest to it in embedding space. Comprehensive experiments\ndemonstrate that PlaneTR achieves a state-of-the-art performance on the ScanNet\nand NYUv2 datasets.",
          "link": "http://arxiv.org/abs/2107.13108",
          "publishedOn": "2021-07-29T02:00:07.753Z",
          "wordCount": 608,
          "title": "PlaneTR: Structure-Guided Transformers for 3D Plane Recovery. (arXiv:2107.13108v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuefan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "We describe a method for realistic depth synthesis that learns diverse\nvariations from the real depth scans and ensures geometric consistency for\neffective synthetic-to-real transfer. Unlike general image synthesis pipelines,\nwhere geometries are mostly ignored, we treat geometries carried by the depth\nbased on their own existence. We propose differential contrastive learning that\nexplicitly enforces the underlying geometric properties to be invariant\nregarding the real variations been learned. The resulting depth synthesis\nmethod is task-agnostic and can be used for training any task-specific networks\nwith synthetic labels. We demonstrate the effectiveness of the proposed method\nby extensive evaluations on downstream real-world geometric reasoning tasks. We\nshow our method achieves better synthetic-to-real transfer performance than the\nother state-of-the-art. When fine-tuned on a small number of real-world\nannotations, our method can even surpass the fully supervised baselines.",
          "link": "http://arxiv.org/abs/2107.13087",
          "publishedOn": "2021-07-29T02:00:07.746Z",
          "wordCount": 574,
          "title": "DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis. (arXiv:2107.13087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>",
          "description": "This paper revisits human-object interaction (HOI) recognition at image level\nwithout using supervisions of object location and human pose. We name it\ndetection-free HOI recognition, in contrast to the existing\ndetection-supervised approaches which rely on object and keypoint detections to\nachieve state of the art. With our method, not only the detection supervision\nis evitable, but superior performance can be achieved by properly using\nimage-text pre-training (such as CLIP) and the proposed Log-Sum-Exp Sign\n(LSE-Sign) loss function. Specifically, using text embeddings of class labels\nto initialize the linear classifier is essential for leveraging the CLIP\npre-trained image encoder. In addition, LSE-Sign loss facilitates learning from\nmultiple labels on an imbalanced dataset by normalizing gradients over all\nclasses in a softmax format. Surprisingly, our detection-free solution achieves\n60.5 mAP on the HICO dataset, outperforming the detection-supervised state of\nthe art by 13.4 mAP",
          "link": "http://arxiv.org/abs/2107.13083",
          "publishedOn": "2021-07-29T02:00:07.738Z",
          "wordCount": 588,
          "title": "Is Object Detection Necessary for Human-Object Interaction Recognition?. (arXiv:2107.13083v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>",
          "description": "Zero-shot action recognition is the task of classifying action categories\nthat are not available in the training set. In this setting, the standard\nevaluation protocol is to use existing action recognition datasets (e.g.\nUCF101) and randomly split the classes into seen and unseen. However, most\nrecent work builds on representations pre-trained on the Kinetics dataset,\nwhere classes largely overlap with classes in the zero-shot evaluation\ndatasets. As a result, classes which are supposed to be unseen, are present\nduring supervised pre-training, invalidating the condition of the zero-shot\nsetting. A similar concern was previously noted several years ago for image\nbased zero-shot recognition, but has not been considered by the zero-shot\naction recognition community. In this paper, we propose a new split for true\nzero-shot action recognition with no overlap between unseen test classes and\ntraining or pre-training classes. We benchmark several recent approaches on the\nproposed True Zero-Shot (TruZe) Split for UCF101 and HMDB51, with zero-shot and\ngeneralized zero-shot evaluation. In our extensive analysis we find that our\nTruZe splits are significantly harder than comparable random splits as nothing\nis leaking from pre-training, i.e. unseen performance is consistently lower, up\nto 9.4% for zero-shot action recognition. In an additional evaluation we also\nfind that similar issues exist in the splits used in few-shot action\nrecognition, here we see differences of up to 14.1%. We publish our splits and\nhope that our benchmark analysis will change how the field is evaluating zero-\nand few-shot action recognition moving forward.",
          "link": "http://arxiv.org/abs/2107.13029",
          "publishedOn": "2021-07-29T02:00:07.688Z",
          "wordCount": 688,
          "title": "A New Split for Evaluating True Zero-Shot Action Recognition. (arXiv:2107.13029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",
          "link": "http://arxiv.org/abs/2107.12619",
          "publishedOn": "2021-07-28T02:02:34.598Z",
          "wordCount": 674,
          "title": "Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Menghan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Simon X. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "For people who ardently love painting but unfortunately have visual\nimpairments, holding a paintbrush to create a work is a very difficult task.\nPeople in this special group are eager to pick up the paintbrush, like Leonardo\nda Vinci, to create and make full use of their own talents. Therefore, to\nmaximally bridge this gap, we propose a painting navigation system to assist\nblind people in painting and artistic creation. The proposed system is composed\nof cognitive system and guidance system. The system adopts drawing board\npositioning based on QR code, brush navigation based on target detection and\nbush real-time positioning. Meanwhile, this paper uses human-computer\ninteraction on the basis of voice and a simple but efficient position\ninformation coding rule. In addition, we design a criterion to efficiently\njudge whether the brush reaches the target or not. According to the\nexperimental results, the thermal curves extracted from the faces of testers\nshow that it is relatively well accepted by blindfolded and even blind testers.\nWith the prompt frequency of 1s, the painting navigation system performs best\nwith the completion degree of 89% with SD of 8.37% and overflow degree of 347%\nwith SD of 162.14%. Meanwhile, the excellent and good types of brush tip\ntrajectory account for 74%, and the relative movement distance is 4.21 with SD\nof 2.51. This work demonstrates that it is practicable for the blind people to\nfeel the world through the brush in their hands. In the future, we plan to\ndeploy Angle's Eyes on the phone to make it more portable. The demo video of\nthe proposed painting navigation system is available at:\nhttps://doi.org/10.6084/m9.figshare.9760004.v1.",
          "link": "http://arxiv.org/abs/2107.12921",
          "publishedOn": "2021-07-28T02:02:34.546Z",
          "wordCount": 739,
          "title": "Angel's Girl for Blind Painters: an Efficient Painting Navigation System Validated by Multimodal Evaluation Approach. (arXiv:2107.12921v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockhardt_F/0/1/0/all/0/1\">Fabian Stockhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osorio_Roig_D/0/1/0/all/0/1\">Dail&#xe9; Osorio-Roig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Computationally efficient, accurate, and privacy-preserving data storage and\nretrieval are among the key challenges faced by practical deployments of\nbiometric identification systems worldwide. In this work, a method of protected\nindexing of biometric data is presented. By utilising feature-level fusion of\nintelligently paired templates, a multi-stage search structure is created.\nDuring retrieval, the list of potential candidate identities is successively\npre-filtered, thereby reducing the number of template comparisons necessary for\na biometric identification transaction. Protection of the biometric probe\ntemplates, as well as the stored reference templates and the created index is\ncarried out using homomorphic encryption. The proposed method is extensively\nevaluated in closed-set and open-set identification scenarios on publicly\navailable databases using two state-of-the-art open-source face recognition\nsystems. With respect to a typical baseline algorithm utilising an exhaustive\nsearch-based retrieval algorithm, the proposed method enables a reduction of\nthe computational workload associated with a biometric identification\ntransaction by 90%, while simultaneously suffering no degradation of the\nbiometric performance. Furthermore, by facilitating a seamless integration of\ntemplate protection with open-source homomorphic encryption libraries, the\nproposed method guarantees unlinkability, irreversibility, and renewability of\nthe protected biometric data.",
          "link": "http://arxiv.org/abs/2107.12675",
          "publishedOn": "2021-07-28T02:02:34.530Z",
          "wordCount": 643,
          "title": "Feature Fusion Methods for Indexing and Retrieval of Biometric Data: Application to Face Recognition with Privacy Protection. (arXiv:2107.12675v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12040",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenxi Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chang_Y/0/1/0/all/0/1\">Yi-Wei Chang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that\nvisualizes the structural and spatial organization of macromolecules at a\nnear-native state in single cells, which has broad applications in life\nscience. However, the systematic structural recognition and recovery of\nmacromolecules captured by cryo-ET are difficult due to high structural\ncomplexity and imaging limits. Deep learning based subtomogram classification\nhave played critical roles for such tasks. As supervised approaches, however,\ntheir performance relies on sufficient and laborious annotation on a large\ntraining dataset.\n\nResults: To alleviate this major labeling burden, we proposed a Hybrid Active\nLearning (HAL) framework for querying subtomograms for labelling from a large\nunlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select\nthe subtomograms that have the most uncertain predictions. Moreover, to\nmitigate the sampling bias caused by such strategy, a discriminator is\nintroduced to judge if a certain subtomogram is labeled or unlabeled and\nsubsequently the model queries the subtomogram that have higher probabilities\nto be unlabeled. Additionally, HAL introduces a subset sampling strategy to\nimprove the diversity of the query set, so that the information overlap is\ndecreased between the queried batches and the algorithmic efficiency is\nimproved. Our experiments on subtomogram classification tasks using both\nsimulated and real data demonstrate that we can achieve comparable testing\nperformance (on average only 3% accuracy drop) by using less than 30% of the\nlabeled subtomograms, which shows a very promising result for subtomogram\nclassification task with limited labeling resources.",
          "link": "http://arxiv.org/abs/2102.12040",
          "publishedOn": "2021-07-28T02:02:34.402Z",
          "wordCount": 783,
          "title": "Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography. (arXiv:2102.12040v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Rundong Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1\">Giuseppe Loianno</a>",
          "description": "Estimating the 6D pose of objects is beneficial for robotics tasks such as\ntransportation, autonomous navigation, manipulation as well as in scenarios\nbeyond robotics like virtual and augmented reality. With respect to single\nimage pose estimation, pose tracking takes into account the temporal\ninformation across multiple frames to overcome possible detection\ninconsistencies and to improve the pose estimation efficiency. In this work, we\nintroduce a novel Deep Neural Network (DNN) called VIPose, that combines\ninertial and camera data to address the object pose tracking problem in\nreal-time. The key contribution is the design of a novel DNN architecture which\nfuses visual and inertial features to predict the objects' relative 6D pose\nbetween consecutive image frames. The overall 6D pose is then estimated by\nconsecutively combining relative poses. Our approach shows remarkable pose\nestimation results for heavily occluded objects that are well known to be very\nchallenging to handle by existing state-of-the-art solutions. The effectiveness\nof the proposed approach is validated on a new dataset called VIYCB with RGB\nimage, IMU data, and accurate 6D pose annotations created by employing an\nautomated labeling technique. The approach presents accuracy performances\ncomparable to state-of-the-art techniques, but with additional benefit to be\nreal-time.",
          "link": "http://arxiv.org/abs/2107.12617",
          "publishedOn": "2021-07-28T02:02:34.381Z",
          "wordCount": 641,
          "title": "VIPose: Real-time Visual-Inertial 6D Object Pose Tracking. (arXiv:2107.12617v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-07-28T02:02:34.325Z",
          "wordCount": 604,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12689",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Byrne_N/0/1/0/all/0/1\">Nick Byrne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clough_J/0/1/0/all/0/1\">James R Clough</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valverde_I/0/1/0/all/0/1\">Isra Valverde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montana_G/0/1/0/all/0/1\">Giovanni Montana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1\">Andrew P King</a>",
          "description": "Multi-class segmentation of cardiac magnetic resonance (CMR) images seeks a\nseparation of data into anatomical components with known structure and\nconfiguration. The most popular CNN-based methods are optimised using pixel\nwise loss functions, ignorant of the spatially extended features that\ncharacterise anatomy. Therefore, whilst sharing a high spatial overlap with the\nground truth, inferred CNN-based segmentations can lack coherence, including\nspurious connected components, holes and voids. Such results are implausible,\nviolating anticipated anatomical topology. In response, (single-class)\npersistent homology-based loss functions have been proposed to capture global\nanatomical features. Our work extends these approaches to the task of\nmulti-class segmentation. Building an enriched topological description of all\nclass labels and class label pairs, our loss functions make predictable and\nstatistically significant improvements in segmentation topology using a\nCNN-based post-processing framework. We also present (and make available) a\nhighly efficient implementation based on cubical complexes and parallel\nexecution, enabling practical application within high resolution 3D data for\nthe first time. We demonstrate our approach on 2D short axis and 3D whole heart\nCMR segmentation, advancing a detailed and faithful analysis of performance on\ntwo publicly available datasets.",
          "link": "http://arxiv.org/abs/2107.12689",
          "publishedOn": "2021-07-28T02:02:34.306Z",
          "wordCount": 641,
          "title": "A persistent homology-based topological loss for CNN-based multi-class segmentation of CMR. (arXiv:2107.12689v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Li Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt\na segmentation model trained on the labeled source domain to the unlabeled\ntarget domain. Existing methods try to learn domain invariant features while\nsuffering from large domain gaps that make it difficult to correctly align\ndiscrepant features, especially in the initial training phase. To address this\nissue, we propose a novel Dual Soft-Paste (DSP) method in this paper.\nSpecifically, DSP selects some classes from a source domain image using a\nlong-tail class first sampling strategy and softly pastes the corresponding\nimage patch on both the source and target training images with a fusion weight.\nTechnically, we adopt the mean teacher framework for domain adaptation, where\nthe pasted source and target images go through the student network while the\noriginal target image goes through the teacher network. Output-level alignment\nis carried out by aligning the probability maps of the target fused image from\nboth networks using a weighted cross-entropy loss. In addition, feature-level\nalignment is carried out by aligning the feature maps of the source and target\nimages from student network using a weighted maximum mean discrepancy loss. DSP\nfacilitates the model learning domain-invariant features from the intermediate\ndomains, leading to faster convergence and better performance. Experiments on\ntwo challenging benchmarks demonstrate the superiority of DSP over\nstate-of-the-art methods. Code is available at\n\\url{https://github.com/GaoLii/DSP}.",
          "link": "http://arxiv.org/abs/2107.09600",
          "publishedOn": "2021-07-28T02:02:34.299Z",
          "wordCount": 695,
          "title": "DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation. (arXiv:2107.09600v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramon_E/0/1/0/all/0/1\">Eduard Ramon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triginer_G/0/1/0/all/0/1\">Gil Triginer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escur_J/0/1/0/all/0/1\">Janna Escur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pumarola_A/0/1/0/all/0/1\">Albert Pumarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1\">Jaime Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Giro-i-Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>",
          "description": "Recent learning approaches that implicitly represent surface geometry using\ncoordinate-based neural representations have shown impressive results in the\nproblem of multi-view 3D reconstruction. The effectiveness of these techniques\nis, however, subject to the availability of a large number (several tens) of\ninput views of the scene, and computationally demanding optimizations. In this\npaper, we tackle these limitations for the specific problem of few-shot full 3D\nhead reconstruction, by endowing coordinate-based representations with a\nprobabilistic shape prior that enables faster convergence and better\ngeneralization when using few input images (down to three). First, we learn a\nshape model of 3D heads from thousands of incomplete raw scans using implicit\nrepresentations. At test time, we jointly overfit two coordinate-based neural\nnetworks to the scene, one modeling the geometry and another estimating the\nsurface radiance, using implicit differentiable rendering. We devise a\ntwo-stage optimization strategy in which the learned prior is used to\ninitialize and constrain the geometry during an initial optimization phase.\nThen, the prior is unfrozen and fine-tuned to the scene. By doing this, we\nachieve high-fidelity head reconstructions, including hair and shoulders, and\nwith a high level of detail that consistently outperforms both state-of-the-art\n3D Morphable Models methods in the few-shot scenario, and non-parametric\nmethods when large sets of views are available.",
          "link": "http://arxiv.org/abs/2107.12512",
          "publishedOn": "2021-07-28T02:02:34.240Z",
          "wordCount": 654,
          "title": "H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction. (arXiv:2107.12512v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuangjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>",
          "description": "Recent deep networks have convincingly demonstrated high capability in crowd\ncounting, which is a critical task attracting widespread attention due to its\nvarious industrial applications. Despite such progress, trained data-dependent\nmodels usually can not generalize well to unseen scenarios because of the\ninherent domain shift. To facilitate this issue, this paper proposes a novel\nadversarial scoring network (ASNet) to gradually bridge the gap across domains\nfrom coarse to fine granularity. In specific, at the coarse-grained stage, we\ndesign a dual-discriminator strategy to adapt source domain to be close to the\ntargets from the perspectives of both global and local feature space via\nadversarial learning. The distributions between two domains can thus be aligned\nroughly. At the fine-grained stage, we explore the transferability of source\ncharacteristics by scoring how similar the source samples are to target ones\nfrom multiple levels based on generative probability derived from coarse stage.\nGuided by these hierarchical scores, the transferable source features are\nproperly selected to enhance the knowledge transfer during the adaptation\nprocess. With the coarse-to-fine design, the generalization bottleneck induced\nfrom the domain discrepancy can be effectively alleviated. Three sets of\nmigration experiments show that the proposed methods achieve state-of-the-art\ncounting performance compared with major unsupervised methods.",
          "link": "http://arxiv.org/abs/2107.12858",
          "publishedOn": "2021-07-28T02:02:33.959Z",
          "wordCount": 657,
          "title": "Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network. (arXiv:2107.12858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1\">Tanmoy Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1\">Sreenatha G. Anavatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbass_H/0/1/0/all/0/1\">Hussein A. Abbass</a> (Fellow, IEEESchool of Engineering and Information Technology, University of New South Wales Canberra, Australia)",
          "description": "The Latent Space Clustering in Generative adversarial networks (ClusterGAN)\nmethod has been successful with high-dimensional data. However, the method\nassumes uniformlydistributed priors during the generation of modes, which isa\nrestrictive assumption in real-world data and cause loss ofdiversity in the\ngenerated modes. In this paper, we proposeself-augmentation information\nmaximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive\npriorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural\nnetworks: self-augmentation prior network,generator, discriminator and\nclustering inference autoencoder.The proposed method has been validated using\nseven bench-mark data sets and has shown improved performance overstate-of-the\nart methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on\nimbalanced dataset, wehave discussed two imbalanced conditions on MNIST\ndatasetswith one-class imbalance and three classes imbalanced cases.The results\nhighlight the advantages of SIMI-ClusterGAN.",
          "link": "http://arxiv.org/abs/2107.12706",
          "publishedOn": "2021-07-28T02:02:33.823Z",
          "wordCount": 582,
          "title": "Improving ClusterGAN Using Self-AugmentedInformation Maximization of Disentangling LatentSpaces. (arXiv:2107.12706v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoyu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Pin Siang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Hsing Wang</a>",
          "description": "In this work, we propose a novel two-stage framework for the efficient 3D\npoint cloud object detection. Instead of transforming point clouds into 2D bird\neye view projections, we parse the raw point cloud data directly in the 3D\nspace yet achieve impressive efficiency and accuracy. To achieve this goal, we\npropose dynamic voxelization, a method that voxellizes points at local scale\non-the-fly. By doing so, we preserve the point cloud geometry with 3D voxels,\nand therefore waive the dependence on expensive MLPs to learn from point\ncoordinates. On the other hand, we inherently still follow the same processing\npattern as point-wise methods (e.g., PointNet) and no longer suffer from the\nquantization issue like conventional convolutions. For further speed\noptimization, we propose the grid-based downsampling and voxelization method,\nand provide different CUDA implementations to accommodate to the discrepant\nrequirements during training and inference phases. We highlight our efficiency\non KITTI 3D object detection dataset with 75 FPS and on Waymo Open dataset with\n25 FPS inference speed with satisfactory accuracy.",
          "link": "http://arxiv.org/abs/2107.12707",
          "publishedOn": "2021-07-28T02:02:33.541Z",
          "wordCount": null,
          "title": "DV-Det: Efficient 3D Point Cloud Object Detection with Dynamic Voxelization. (arXiv:2107.12707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12978",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "There are many clinical contexts which require accurate detection and\nsegmentation of all focal pathologies (e.g. lesions, tumours) in patient\nimages. In cases where there are a mix of small and large lesions, standard\nbinary cross entropy loss will result in better segmentation of large lesions\nat the expense of missing small ones. Adjusting the operating point to\naccurately detect all lesions generally leads to oversegmentation of large\nlesions. In this work, we propose a novel reweighing strategy to eliminate this\nperformance gap, increasing small pathology detection performance while\nmaintaining segmentation accuracy. We show that our reweighing strategy vastly\noutperforms competing strategies based on experiments on a large scale,\nmulti-scanner, multi-center dataset of Multiple Sclerosis patient images.",
          "link": "http://arxiv.org/abs/2107.12978",
          "publishedOn": "2021-07-28T02:02:33.039Z",
          "wordCount": null,
          "title": "Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting. (arXiv:2107.12978v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.05105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhipeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peirong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>",
          "description": "For semantic segmentation, label probabilities are often uncalibrated as they\nare typically only the by-product of a segmentation task. Intersection over\nUnion (IoU) and Dice score are often used as criteria for segmentation success,\nwhile metrics related to label probabilities are not often explored. However,\nprobability calibration approaches have been studied, which match probability\noutputs with experimentally observed errors. These approaches mainly focus on\nclassification tasks, but not on semantic segmentation. Thus, we propose a\nlearning-based calibration method that focuses on multi-label semantic\nsegmentation. Specifically, we adopt a convolutional neural network to predict\nlocal temperature values for probability calibration. One advantage of our\napproach is that it does not change prediction accuracy, hence allowing for\ncalibration as a post-processing step. Experiments on the COCO, CamVid, and\nLPBA40 datasets demonstrate improved calibration performance for a range of\ndifferent metrics. We also demonstrate the good performance of our method for\nmulti-atlas brain segmentation from magnetic resonance images.",
          "link": "http://arxiv.org/abs/2008.05105",
          "publishedOn": "2021-07-28T02:02:33.038Z",
          "wordCount": 619,
          "title": "Local Temperature Scaling for Probability Calibration. (arXiv:2008.05105v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1\">Eleonora Grassucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicero_E/0/1/0/all/0/1\">Edoardo Cicero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1\">Danilo Comminiello</a>",
          "description": "Latest Generative Adversarial Networks (GANs) are gathering outstanding\nresults through a large-scale training, thus employing models composed of\nmillions of parameters requiring extensive computational capabilities. Building\nsuch huge models undermines their replicability and increases the training\ninstability. Moreover, multi-channel data, such as images or audio, are usually\nprocessed by realvalued convolutional networks that flatten and concatenate the\ninput, often losing intra-channel spatial relations. To address these issues\nrelated to complexity and information loss, we propose a family of\nquaternion-valued generative adversarial networks (QGANs). QGANs exploit the\nproperties of quaternion algebra, e.g., the Hamilton product, that allows to\nprocess channels as a single entity and capture internal latent relations,\nwhile reducing by a factor of 4 the overall number of parameters. We show how\nto design QGANs and to extend the proposed approach even to advanced models.We\ncompare the proposed QGANs with real-valued counterparts on several image\ngeneration benchmarks. Results show that QGANs are able to obtain better FID\nscores than real-valued GANs and to generate visually pleasing images.\nFurthermore, QGANs save up to 75% of the training parameters. We believe these\nresults may pave the way to novel, more accessible, GANs capable of improving\nperformance and saving computational resources.",
          "link": "http://arxiv.org/abs/2104.09630",
          "publishedOn": "2021-07-28T02:02:32.872Z",
          "wordCount": 678,
          "title": "Quaternion Generative Adversarial Networks. (arXiv:2104.09630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>",
          "description": "We propose a new method to detect deepfake images using the cue of the source\nfeature inconsistency within the forged images. It is based on the hypothesis\nthat images' distinct source features can be preserved and extracted after\ngoing through state-of-the-art deepfake generation processes. We introduce a\nnovel representation learning approach, called pair-wise self-consistency\nlearning (PCL), for training ConvNets to extract these source features and\ndetect deepfake images. It is accompanied by a new image synthesis approach,\ncalled inconsistency image generator (I2G), to provide richly annotated\ntraining data for PCL. Experimental results on seven popular datasets show that\nour models improve averaged AUC over the state of the art from 96.45% to 98.05%\nin the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset\nevaluation.",
          "link": "http://arxiv.org/abs/2012.09311",
          "publishedOn": "2021-07-28T02:02:32.807Z",
          "wordCount": 594,
          "title": "Learning Self-Consistency for Deepfake Detection. (arXiv:2012.09311v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shihao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>",
          "description": "Occlusions pose a significant challenge to optical flow algorithms that rely\non local evidences. We consider an occluded point to be one that is imaged in\nthe first frame but not in the next, a slight overloading of the standard\ndefinition since it also includes points that move out-of-frame. Estimating the\nmotion of these points is extremely difficult, particularly in the two-frame\nsetting. Previous work relies on CNNs to learn occlusions, without much\nsuccess, or requires multiple frames to reason about occlusions using temporal\nsmoothness. In this paper, we argue that the occlusion problem can be better\nsolved in the two-frame case by modelling image self-similarities. We introduce\na global motion aggregation module, a transformer-based approach to find\nlong-range dependencies between pixels in the first image, and perform global\naggregation on the corresponding motion features. We demonstrate that the\noptical flow estimates in the occluded regions can be significantly improved\nwithout damaging the performance in non-occluded regions. This approach obtains\nnew state-of-the-art results on the challenging Sintel dataset, improving the\naverage end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At\nthe time of submission, our method ranks first on these benchmarks among all\npublished and unpublished approaches. Code is available at\nhttps://github.com/zacjiang/GMA",
          "link": "http://arxiv.org/abs/2104.02409",
          "publishedOn": "2021-07-28T02:02:32.750Z",
          "wordCount": 683,
          "title": "Learning to Estimate Hidden Motions with Global Motion Aggregation. (arXiv:2104.02409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zixuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haizhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>",
          "description": "The challenge of the Class Incremental Learning (CIL) lies in difficulty for\na learner to discern the old classes' data from the new while no previous data\nis preserved. Namely, the representation distribution of different phases\noverlaps with each other. In this paper, to alleviate the phenomenon of\nrepresentation overlapping for both memory-based and memory-free methods, we\npropose a new CIL framework, Contrastive Class Concentration for CIL (C4IL).\nOur framework leverages the class concentration effect of contrastive\nrepresentation learning, therefore yielding a representation distribution with\nbetter intra-class compactibility and inter-class separability. Quantitative\nexperiments showcase our framework that is effective in both memory-based and\nmemory-free cases: it outperforms the baseline methods of both cases by 5% in\nterms of the average and top-1 accuracy in 10-phase and 20-phase CIL.\nQualitative results also demonstrate that our method generates a more compact\nrepresentation distribution that alleviates the overlapping problem.",
          "link": "http://arxiv.org/abs/2107.12308",
          "publishedOn": "2021-07-28T02:02:32.735Z",
          "wordCount": 605,
          "title": "Alleviate Representation Overlapping in Class Incremental Learning by Contrastive Class Concentration. (arXiv:2107.12308v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Robin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>",
          "description": "Deep neural networks (DNNs) for the semantic segmentation of images are\nusually trained to operate on a predefined closed set of object classes. This\nis in contrast to the \"open world\" setting where DNNs are envisioned to be\ndeployed to. From a functional safety point of view, the ability to detect\nso-called \"out-of-distribution\" (OoD) samples, i.e., objects outside of a DNN's\nsemantic space, is crucial for many applications such as automated driving. A\nnatural baseline approach to OoD detection is to threshold on the pixel-wise\nsoftmax entropy. We present a two-step procedure that significantly improves\nthat approach. Firstly, we utilize samples from the COCO dataset as OoD proxy\nand introduce a second training objective to maximize the softmax entropy on\nthese samples. Starting from pretrained semantic segmentation networks we\nre-train a number of DNNs on different in-distribution datasets and\nconsistently observe improved OoD detection performance when evaluating on\ncompletely disjoint OoD datasets. Secondly, we perform a transparent\npost-processing step to discard false positive OoD samples by so-called \"meta\nclassification\". To this end, we apply linear models to a set of hand-crafted\nmetrics derived from the DNN's softmax probabilities. In our experiments we\nconsistently observe a clear additional gain in OoD detection performance,\ncutting down the number of detection errors by up to 52% when comparing the\nbest baseline with our results. We achieve this improvement sacrificing only\nmarginally in original segmentation performance. Therefore, our method\ncontributes to safer DNNs with more reliable overall system performance.",
          "link": "http://arxiv.org/abs/2012.06575",
          "publishedOn": "2021-07-28T02:02:32.609Z",
          "wordCount": 728,
          "title": "Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation. (arXiv:2012.06575v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1711.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_M/0/1/0/all/0/1\">Michael Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>",
          "description": "Matching images and sentences demands a fine understanding of both\nmodalities. In this paper, we propose a new system to discriminatively embed\nthe image and text to a shared visual-textual space. In this field, most\nexisting works apply the ranking loss to pull the positive image / text pairs\nclose and push the negative pairs apart from each other. However, directly\ndeploying the ranking loss is hard for network learning, since it starts from\nthe two heterogeneous features to build inter-modal relationship. To address\nthis problem, we propose the instance loss which explicitly considers the\nintra-modal data distribution. It is based on an unsupervised assumption that\neach image / text group can be viewed as a class. So the network can learn the\nfine granularity from every image/text group. The experiment shows that the\ninstance loss offers better weight initialization for the ranking loss, so that\nmore discriminative embeddings can be learned. Besides, existing works usually\napply the off-the-shelf features, i.e., word2vec and fixed visual feature. So\nin a minor contribution, this paper constructs an end-to-end dual-path\nconvolutional network to learn the image and text representations. End-to-end\nlearning allows the system to directly learn from the data and fully utilize\nthe supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),\nexperiments demonstrate that our method yields competitive accuracy compared to\nstate-of-the-art methods. Moreover, in language based person retrieval, we\nimprove the state of the art by a large margin. The code has been made publicly\navailable.",
          "link": "http://arxiv.org/abs/1711.05535",
          "publishedOn": "2021-07-28T02:02:32.601Z",
          "wordCount": 743,
          "title": "Dual-Path Convolutional Image-Text Embeddings with Instance Loss. (arXiv:1711.05535v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenhongyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiaofei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Deep learning-based dense object detectors have achieved great success in the\npast few years and have been applied to numerous multimedia applications such\nas video understanding. However, the current training pipeline for dense\ndetectors is compromised to lots of conjunctions that may not hold. In this\npaper, we investigate three such important conjunctions: 1) only samples\nassigned as positive in classification head are used to train the regression\nhead; 2) classification and regression share the same input feature and\ncomputational fields defined by the parallel head architecture; and 3) samples\ndistributed in different feature pyramid layers are treated equally when\ncomputing the loss. We first carry out a series of pilot experiments to show\ndisentangling such conjunctions can lead to persistent performance improvement.\nThen, based on these findings, we propose Disentangled Dense Object Detector\n(DDOD), in which simple and effective disentanglement mechanisms are designed\nand integrated into the current state-of-the-art dense object detectors.\nExtensive experiments on MS COCO benchmark show that our approach can lead to\n2.0 mAP, 2.4 mAP and 2.2 mAP absolute improvements on RetinaNet, FCOS, and ATSS\nbaselines with negligible extra overhead. Notably, our best model reaches 55.0\nmAP on the COCO test-dev set and 93.5 AP on the hard subset of WIDER FACE,\nachieving new state-of-the-art performance on these two competitive benchmarks.\nCode is available at https://github.com/zehuichen123/DDOD.",
          "link": "http://arxiv.org/abs/2107.02963",
          "publishedOn": "2021-07-28T02:02:32.594Z",
          "wordCount": 676,
          "title": "Disentangle Your Dense Object Detector. (arXiv:2107.02963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_R/0/1/0/all/0/1\">Robin Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_G/0/1/0/all/0/1\">Guillaume Michel</a>",
          "description": "Pruning seeks to design lightweight architectures by removing redundant\nweights in overparameterized networks. Most of the existing techniques first\nremove structured sub-networks (filters, channels,...) and then fine-tune the\nresulting networks to maintain a high accuracy. However, removing a whole\nstructure is a strong topological prior and recovering the accuracy, with\nfine-tuning, is highly cumbersome. In this paper, we introduce an \"end-to-end\"\nlightweight network design that achieves training and pruning simultaneously\nwithout fine-tuning. The design principle of our method relies on\nreparametrization that learns not only the weights but also the topological\nstructure of the lightweight sub-network. This reparametrization acts as a\nprior (or regularizer) that defines pruning masks implicitly from the weights\nof the underlying network, without increasing the number of training\nparameters. Sparsity is induced with a budget loss that provides an accurate\npruning. Extensive experiments conducted on the CIFAR10 and the TinyImageNet\ndatasets, using standard architectures (namely Conv4, VGG19 and ResNet18), show\ncompelling results without fine-tuning.",
          "link": "http://arxiv.org/abs/2107.03909",
          "publishedOn": "2021-07-28T02:02:32.587Z",
          "wordCount": 615,
          "title": "Weight Reparametrization for Budget-Aware Network Pruning. (arXiv:2107.03909v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12461",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "The U-Net architecture, built upon the fully convolutional network, has\nproven to be effective in biomedical image segmentation. However, U-Net applies\nskip connections to merge semantically different low- and high-level\nconvolutional features, resulting in not only blurred feature maps, but also\nover- and under-segmented target regions. To address these limitations, we\npropose a simple, yet effective end-to-end depthwise encoder-decoder fully\nconvolutional network architecture, called Sharp U-Net, for binary and\nmulti-class biomedical image segmentation. The key rationale of Sharp U-Net is\nthat instead of applying a plain skip connection, a depthwise convolution of\nthe encoder feature map with a sharpening kernel filter is employed prior to\nmerging the encoder and decoder features, thereby producing a sharpened\nintermediate feature map of the same size as the encoder map. Using this\nsharpening filter layer, we are able to not only fuse semantically less\ndissimilar features, but also to smooth out artifacts throughout the network\nlayers during the early stages of training. Our extensive experiments on six\ndatasets show that the proposed Sharp U-Net model consistently outperforms or\nmatches the recent state-of-the-art baselines in both binary and multi-class\nsegmentation tasks, while adding no extra learnable parameters. Furthermore,\nSharp U-Net outperforms baselines that have more than three times the number of\nlearnable parameters.",
          "link": "http://arxiv.org/abs/2107.12461",
          "publishedOn": "2021-07-28T02:02:32.568Z",
          "wordCount": 652,
          "title": "Sharp U-Net: Depthwise Convolutional Network for Biomedical Image Segmentation. (arXiv:2107.12461v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12889",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Felfeliyan_B/0/1/0/all/0/1\">Banafshe Felfeliyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuntze_G/0/1/0/all/0/1\">Gregor Kuntze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob L. Jaremko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ronsky_J/0/1/0/all/0/1\">Janet L. Ronsky</a>",
          "description": "Objective assessment of Magnetic Resonance Imaging (MRI) scans of\nosteoarthritis (OA) can address the limitation of the current OA assessment.\nSegmentation of bone, cartilage, and joint fluid is necessary for the OA\nobjective assessment. Most of the proposed segmentation methods are not\nperforming instance segmentation and suffer from class imbalance problems. This\nstudy deployed Mask R-CNN instance segmentation and improved it (improved-Mask\nR-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for\nOA-associated tissues. Training and validation of the method were performed\nusing 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI\nscans of patients with symptomatic hip OA. Three modifications to Mask R-CNN\nyielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder\nlayer to the mask-header, and connecting them by a skip connection. The results\nwere assessed using Hausdorff distance, dice score, and coefficients of\nvariation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation\ncompared to Mask RCNN as indicated with the increase in dice score from 95% to\n98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and\n81% to 82% for tibial cartilage. For the effusion detection, dice improved with\niMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection\nbetween Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2\nand Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between\ntwo readers (0.21), indicating a high agreement between the human readers and\nboth Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and\nsimultaneously extract different scale articular tissues involved in OA,\nforming the foundation for automated assessment of OA. The iMaskRCNN results\nshow that the modification improved the network performance around the edges.",
          "link": "http://arxiv.org/abs/2107.12889",
          "publishedOn": "2021-07-28T02:02:32.560Z",
          "wordCount": 755,
          "title": "Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative). (arXiv:2107.12889v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_V/0/1/0/all/0/1\">Vincent Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_K/0/1/0/all/0/1\">Karteek Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">Mirza Faisal Beg</a>",
          "description": "The latest advances in computer-assisted precision medicine are making it\nfeasible to move from population-wide models that are useful to discover\naggregate patterns that hold for group-based analysis to patient-specific\nmodels that can drive patient-specific decisions with regard to treatment\nchoices, and predictions of outcomes of treatment. Body Composition is\nrecognized as an important driver and risk factor for a wide variety of\ndiseases, as well as a predictor of individual patient-specific clinical\noutcomes to treatment choices or surgical interventions. 3D CT images are\nroutinely acquired in the oncological worklows and deliver accurate rendering\nof internal anatomy and therefore can be used opportunistically to assess the\namount of skeletal muscle and adipose tissue compartments. Powerful tools of\nartificial intelligence such as deep learning are making it feasible now to\nsegment the entire 3D image and generate accurate measurements of all internal\nanatomy. These will enable the overcoming of the severe bottleneck that existed\npreviously, namely, the need for manual segmentation, which was prohibitive to\nscale to the hundreds of 2D axial slices that made up a 3D volumetric image.\nAutomated tools such as presented here will now enable harvesting whole-body\nmeasurements from 3D CT or MRI images, leading to a new era of discovery of the\ndrivers of various diseases based on individual tissue, organ volume, shape,\nand functional status. These measurements were hitherto unavailable thereby\nlimiting the field to a very small and limited subset. These discoveries and\nthe potential to perform individual image segmentation with high speed and\naccuracy are likely to lead to the incorporation of these 3D measures into\nindividual specific treatment planning models related to nutrition, aging,\nchemotoxicity, surgery and survival after the onset of a major disease such as\ncancer.",
          "link": "http://arxiv.org/abs/2106.00652",
          "publishedOn": "2021-07-28T02:02:32.553Z",
          "wordCount": 849,
          "title": "Comprehensive Validation of Automated Whole Body Skeletal Muscle, Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition Analysis: Towards Extended Body Composition. (arXiv:2106.00652v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.10343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Ke Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Very deep Convolutional Neural Networks (CNNs) have greatly improved the\nperformance on various image restoration tasks. However, this comes at a price\nof increasing computational burden, hence limiting their practical usages. We\nobserve that some corrupted image regions are inherently easier to restore than\nothers since the distortion and content vary within an image. To leverage this,\nwe propose Path-Restore, a multi-path CNN with a pathfinder that can\ndynamically select an appropriate route for each image region. We train the\npathfinder using reinforcement learning with a difficulty-regulated reward.\nThis reward is related to the performance, complexity and \"the difficulty of\nrestoring a region\". A policy mask is further investigated to jointly process\nall the image regions. We conduct experiments on denoising and mixed\nrestoration tasks. The results show that our method achieves comparable or\nsuperior performance to existing approaches with less computational cost. In\nparticular, Path-Restore is effective for real-world denoising, where the noise\ndistribution varies across different regions on a single image. Compared to the\nstate-of-the-art RIDNet, our method achieves comparable performance and runs\n2.7x faster on the realistic Darmstadt Noise Dataset.",
          "link": "http://arxiv.org/abs/1904.10343",
          "publishedOn": "2021-07-28T02:02:32.545Z",
          "wordCount": 664,
          "title": "Path-Restore: Learning Network Path Selection for Image Restoration. (arXiv:1904.10343v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raumanns_R/0/1/0/all/0/1\">Ralf Raumanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_G/0/1/0/all/0/1\">Gerard Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joosten_M/0/1/0/all/0/1\">Max Joosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P. W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>",
          "description": "We present ENHANCE, an open dataset with multiple annotations to complement\nthe existing ISIC and PH2 skin lesion classification datasets. This dataset\ncontains annotations of visual ABC (asymmetry, border, colour) features from\nnon-expert annotation sources: undergraduate students, crowd workers from\nAmazon MTurk and classic image processing algorithms. In this paper we first\nanalyse the correlations between the annotations and the diagnostic label of\nthe lesion, as well as study the agreement between different annotation\nsources. Overall we find weak correlations of non-expert annotations with the\ndiagnostic label, and low agreement between different annotation sources. We\nthen study multi-task learning (MTL) with the annotations as additional labels,\nand show that non-expert annotations can improve (ensembles of)\nstate-of-the-art convolutional neural networks via MTL. We hope that our\ndataset can be used in further research into multiple annotations and/or MTL.\nAll data and models are available on Github:\nhttps://github.com/raumannsr/ENHANCE.",
          "link": "http://arxiv.org/abs/2107.12734",
          "publishedOn": "2021-07-28T02:02:32.525Z",
          "wordCount": 614,
          "title": "ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification. (arXiv:2107.12734v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1\">Paul Wimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1\">Jens Mehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Condurache</a>",
          "description": "State-of-the-art deep neural network (DNN) pruning techniques, applied\none-shot before training starts, evaluate sparse architectures with the help of\na single criterion -- called pruning score. Pruning weights based on a solitary\nscore works well for some architectures and pruning rates but may also fail for\nother ones. As a common baseline for pruning scores, we introduce the notion of\na generalized synaptic score (GSS). In this work we do not concentrate on a\nsingle pruning criterion, but provide a framework for combining arbitrary GSSs\nto create more powerful pruning strategies. These COmbined Pruning Scores\n(COPS) are obtained by solving a constrained optimization problem. Optimizing\nfor more than one score prevents the sparse network to overly specialize on an\nindividual task, thus COntrols Pruning before training Starts. The\ncombinatorial optimization problem given by COPS is relaxed on a linear program\n(LP). This LP is solved analytically and determines a solution for COPS.\nFurthermore, an algorithm to compute it for two scores numerically is proposed\nand evaluated. Solving COPS in such a way has lower complexity than the best\ngeneral LP solver. In our experiments we compared pruning with COPS against\nstate-of-the-art methods for different network architectures and image\nclassification tasks and obtained improved results.",
          "link": "http://arxiv.org/abs/2107.12673",
          "publishedOn": "2021-07-28T02:02:32.519Z",
          "wordCount": 647,
          "title": "COPS: Controlled Pruning Before Training Starts. (arXiv:2107.12673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Honghui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shumin Han</a>",
          "description": "Transformers with remarkable global representation capacities achieve\ncompetitive results for visual tasks, but fail to consider high-level local\npattern information in input images. In this paper, we present a generic\nDual-stream Network (DS-Net) to fully explore the representation capacity of\nlocal and global pattern features for image classification. Our DS-Net can\nsimultaneously calculate fine-grained and integrated features and efficiently\nfuse them. Specifically, we propose an Intra-scale Propagation module to\nprocess two different resolutions in each block and an Inter-Scale Alignment\nmodule to perform information interaction across features at dual scales.\nBesides, we also design a Dual-stream FPN (DS-FPN) to further enhance\ncontextual information for downstream dense predictions. Without bells and\nwhistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1\naccuracy on ImageNet-1k and achieves state-of-the-art performance over other\nVision Transformers and ResNets. For object detection and instance\nsegmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 %\nin terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art\nscheme, which significantly demonstrates its potential to be a general backbone\nin vision tasks. The code will be released soon.",
          "link": "http://arxiv.org/abs/2105.14734",
          "publishedOn": "2021-07-28T02:02:32.504Z",
          "wordCount": 660,
          "title": "Dual-stream Network for Visual Recognition. (arXiv:2105.14734v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1\">Eitan Kosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>",
          "description": "Autonomous driving gained huge traction in recent years, due to its potential\nto change the way we commute. Much effort has been put into trying to estimate\nthe state of a vehicle. Meanwhile, learning to forecast the state of a vehicle\nahead introduces new capabilities, such as predicting dangerous situations.\nMoreover, forecasting brings new supervision opportunities by learning to\npredict richer a context, expressed by multiple horizons. Intuitively, a video\nstream originated from a front-facing camera is necessary because it encodes\ninformation about the upcoming road. Besides, historical traces of the\nvehicle's states give more context. In this paper, we tackle multi-horizon\nforecasting of vehicle states by fusing the two modalities. We design and\nexperiment with 3 end-to-end architectures that exploit 3D convolutions for\nvisual features extraction and 1D convolutions for features extraction from\nspeed and steering angle traces. To demonstrate the effectiveness of our\nmethod, we perform extensive experiments on two publicly available real-world\ndatasets, Comma2k19 and the Udacity challenge. We show that we are able to\nforecast a vehicle's state to various horizons, while outperforming the current\nstate-of-the-art results on the related task of driving state estimation. We\nexamine the contribution of vision features, and find that a model fed with\nvision features achieves an error that is 56.6% and 66.9% of the error of a\nmodel that doesn't use those features, on the Udacity and Comma2k19 datasets\nrespectively.",
          "link": "http://arxiv.org/abs/2107.12674",
          "publishedOn": "2021-07-28T02:02:32.496Z",
          "wordCount": 672,
          "title": "Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gothankar_R/0/1/0/all/0/1\">Ruchira Gothankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1\">Fabio Di Troia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "YouTube videos often include captivating descriptions and intriguing\nthumbnails designed to increase the number of views, and thereby increase the\nrevenue for the person who posted the video. This creates an incentive for\npeople to post clickbait videos, in which the content might deviate\nsignificantly from the title, description, or thumbnail. In effect, users are\ntricked into clicking on clickbait videos. In this research, we consider the\nchallenging problem of detecting clickbait YouTube videos. We experiment with\nmultiple state-of-the-art machine learning techniques using a variety of\ntextual features.",
          "link": "http://arxiv.org/abs/2107.12791",
          "publishedOn": "2021-07-28T02:02:32.478Z",
          "wordCount": 517,
          "title": "Clickbait Detection in YouTube Videos. (arXiv:2107.12791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1\">Joshua L. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1\">Ramon Manzorro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1\">Peter A. Crozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1\">Eero P. Simoncelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>",
          "description": "Deep convolutional neural networks (CNNs) for image denoising are usually\ntrained on large datasets. These models achieve the current state of the art,\nbut they have difficulties generalizing when applied to data that deviate from\nthe training distribution. Recent work has shown that it is possible to train\ndenoisers on a single noisy image. These models adapt to the features of the\ntest image, but their performance is limited by the small amount of information\nused to train them. Here we propose \"GainTuning\", in which CNN models\npre-trained on large datasets are adaptively and selectively adjusted for\nindividual test images. To avoid overfitting, GainTuning optimizes a single\nmultiplicative scaling parameter (the \"Gain\") of each channel in the\nconvolutional layers of the CNN. We show that GainTuning improves\nstate-of-the-art CNNs on standard image-denoising benchmarks, boosting their\ndenoising performance on nearly every image in a held-out test set. These\nadaptive improvements are even more substantial for test images differing\nsystematically from the training data, either in noise level or image type. We\nillustrate the potential of adaptive denoising in a scientific application, in\nwhich a CNN is trained on synthetic data, and tested on real\ntransmission-electron-microscope images. In contrast to the existing\nmethodology, GainTuning is able to faithfully reconstruct the structure of\ncatalytic nanoparticles from these data at extremely low signal-to-noise\nratios.",
          "link": "http://arxiv.org/abs/2107.12815",
          "publishedOn": "2021-07-28T02:02:32.472Z",
          "wordCount": 655,
          "title": "Adaptive Denoising via GainTuning. (arXiv:2107.12815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Darafsh_S/0/1/0/all/0/1\">Sahar Darafsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidary_S/0/1/0/all/0/1\">Saeed Shiry Ghidary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_M/0/1/0/all/0/1\">Morteza Saheb Zamani</a>",
          "description": "With the rapid increase in digital technologies, most fields of study include\nrecognition of human activity and intention recognition, which are important in\nsmart environments. In this research, we introduce a real-time activity\nrecognition to recognize people's intentions to pass or not pass a door. This\nsystem, if applied in elevators and automatic doors will save energy and\nincrease efficiency. For this study, data preparation is applied to combine the\nspatial and temporal features with the help of digital image processing\nprinciples. Nevertheless, unlike previous studies, only one AlexNet neural\nnetwork is used instead of two-stream convolutional neural networks. Our\nembedded system was implemented with an accuracy of 98.78% on our Intention\nRecognition dataset. We also examined our data representation approach on other\ndatasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of\n78.48%, 97.95%, and 100%, respectively. The image recognition and neural\nnetwork models were simulated and implemented using Xilinx simulators for\nZCU102 board. The operating frequency of this embedded system is 333 MHz, and\nit works in real-time with 120 frames per second (fps).",
          "link": "http://arxiv.org/abs/2107.12744",
          "publishedOn": "2021-07-28T02:02:32.464Z",
          "wordCount": 618,
          "title": "Real-Time Activity Recognition and Intention Recognition Using a Vision-based Embedded System. (arXiv:2107.12744v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Me&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "Emotion is an inherently subjective psychophysiological human-state and to\nproduce an agreed-upon representation (gold standard) for continuous emotion\nrequires a time-consuming and costly training procedure of multiple human\nannotators. There is strong evidence in the literature that physiological\nsignals are sufficient objective markers for states of emotion, particularly\narousal. In this contribution, we utilise a dataset which includes continuous\nemotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal\nActivity (EDA), and Respiration-rate - captured during a stress induced\nscenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,\nRecurrent Neural Network to explore the benefit of fusing these physiological\nsignals with arousal as the target, learning from various audio, video, and\ntextual based features. We utilise the state-of-the-art MuSe-Toolbox to\nconsider both annotation delay and inter-rater agreement weighting when fusing\nthe target signals. An improvement in Concordance Correlation Coefficient (CCC)\nis seen across features sets when fusing EDA with arousal, compared to the\narousal only gold standard results. Additionally, BERT-based textual features'\nresults improved for arousal plus all physiological signals, obtaining up to\n.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also\nimproves overall CCC with audio plus video features obtaining up to .6157 CCC\nto recognize arousal plus EDA and BPM.",
          "link": "http://arxiv.org/abs/2107.12964",
          "publishedOn": "2021-07-28T02:02:32.456Z",
          "wordCount": 660,
          "title": "A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario. (arXiv:2107.12964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xizhou Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>",
          "description": "As a kind of generative self-supervised learning methods, generative\nadversarial nets have been widely studied in the field of anomaly detection.\nHowever, the representation learning ability of the generator is limited since\nit pays too much attention to pixel-level details, and generator is difficult\nto learn abstract semantic representations from label prediction pretext tasks\nas effective as discriminator. In order to improve the representation learning\nability of generator, we propose a self-supervised learning framework combining\ngenerative methods and discriminative methods. The generator no longer learns\nrepresentation by reconstruction error, but the guidance of discriminator, and\ncould benefit from pretext tasks designed for discriminative methods. Our\ndiscriminative-generative representation learning method has performance close\nto discriminative methods and has a great advantage in speed. Our method used\nin one-class anomaly detection task significantly outperforms several\nstate-of-the-arts on multiple benchmark data sets, increases the performance of\nthe top-performing GAN-based baseline by 6% on CIFAR-10 and 2% on MVTAD.",
          "link": "http://arxiv.org/abs/2107.12753",
          "publishedOn": "2021-07-28T02:02:32.449Z",
          "wordCount": 598,
          "title": "Discriminative-Generative Representation Learning for One-Class Anomaly Detection. (arXiv:2107.12753v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harish_A/0/1/0/all/0/1\">Abhinav Narayan Harish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagar_R/0/1/0/all/0/1\">Rajendra Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>",
          "description": "Autonomous assembly of objects is an essential task in robotics and 3D\ncomputer vision. It has been studied extensively in robotics as a problem of\nmotion planning, actuator control and obstacle avoidance. However, the task of\ndeveloping a generalized framework for assembly robust to structural variants\nremains relatively unexplored. In this work, we tackle this problem using a\nrecurrent graph learning framework considering inter-part relations and the\nprogressive update of the part pose. Our network can learn more plausible\npredictions of shape structure by accounting for priorly assembled parts.\nCompared to the current state-of-the-art, our network yields up to 10%\nimprovement in part accuracy and up to 15% improvement in connectivity accuracy\non the PartNet dataset. Moreover, our resulting latent space facilitates\nexciting applications such as shape recovery from the point-cloud components.\nWe conduct extensive experiments to justify our design choices and demonstrate\nthe effectiveness of the proposed framework.",
          "link": "http://arxiv.org/abs/2107.12859",
          "publishedOn": "2021-07-28T02:02:32.441Z",
          "wordCount": 598,
          "title": "RGL-NET: A Recurrent Graph Learning framework for Progressive Part Assembly. (arXiv:2107.12859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohui Hu</a>",
          "description": "Since the superiority of Transformer in learning long-term dependency, the\nsign language Transformer model achieves remarkable progress in Sign Language\nRecognition (SLR) and Translation (SLT). However, there are several issues with\nthe Transformer that prevent it from better sign language understanding. The\nfirst issue is that the self-attention mechanism learns sign video\nrepresentation in a frame-wise manner, neglecting the temporal semantic\nstructure of sign gestures. Secondly, the attention mechanism with absolute\nposition encoding is direction and distance unaware, thus limiting its ability.\nTo address these issues, we propose a new model architecture, namely PiSLTRc,\nwith two distinctive characteristics: (i) content-aware and position-aware\nconvolution layers. Specifically, we explicitly select relevant features using\na novel content-aware neighborhood gathering method. Then we aggregate these\nfeatures with position-informed temporal convolution layers, thus generating\nrobust neighborhood-enhanced sign representation. (ii) injecting the relative\nposition information to the attention mechanism in the encoder, decoder, and\neven encoder-decoder cross attention. Compared with the vanilla Transformer\nmodel, our model performs consistently better on three large-scale sign\nlanguage benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL. Furthermore,\nextensive experiments demonstrate that the proposed method achieves\nstate-of-the-art performance on translation quality with $+1.6$ BLEU\nimprovements.",
          "link": "http://arxiv.org/abs/2107.12600",
          "publishedOn": "2021-07-28T02:02:32.420Z",
          "wordCount": 624,
          "title": "PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution. (arXiv:2107.12600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.11091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlett_T/0/1/0/all/0/1\">Torsten Schlett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Face recognition can benefit from the utilization of depth data captured\nusing low-cost cameras, in particular for presentation attack detection\npurposes. Depth video output from these capture devices can however contain\ndefects such as holes or general depth inaccuracies. This work proposes a deep\nlearning face depth enhancement method in this context of facial biometrics,\nwhich adds a security aspect to the topic. U-Net-like architectures are\nutilized, and the networks are compared against hand-crafted enhancer types, as\nwell as a similar depth enhancer network from related work trained for an\nadjacent application scenario. All tested enhancer types exclusively use depth\ndata as input, which differs from methods that enhance depth based on\nadditional input data such as visible light color images. Synthetic face depth\nground truth images and degraded forms thereof are created with help of PRNet,\nto train multiple deep learning enhancer models with different network sizes\nand training configurations. Evaluations are carried out on the synthetic data,\non Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435\nimages. These evaluations include an assessment of the falsification for\noccluded face depth input, which is relevant to biometric security. The\nproposed deep learning enhancers yield noticeably better results than the\ntested preexisting enhancers, without overly falsifying depth data when\nnon-face input is provided, and are shown to reduce the error of a simple\nlandmark-based PAD method.",
          "link": "http://arxiv.org/abs/2006.11091",
          "publishedOn": "2021-07-28T02:02:32.413Z",
          "wordCount": 698,
          "title": "Deep Learning-based Single Image Face Depth Data Enhancement. (arXiv:2006.11091v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xin Du</a>",
          "description": "Image enhancement is a subjective process whose targets vary with user\npreferences. In this paper, we propose a deep learning-based image enhancement\nmethod covering multiple tonal styles using only a single model dubbed\nStarEnhancer. It can transform an image from one tonal style to another, even\nif that style is unseen. With a simple one-time setting, users can customize\nthe model to make the enhanced images more in line with their aesthetics. To\nmake the method more practical, we propose a well-designed enhancer that can\nprocess a 4K-resolution image over 200 FPS but surpasses the contemporaneous\nsingle style image enhancement methods in terms of PSNR, SSIM, and LPIPS.\nFinally, our proposed enhancement method has good interactability, which allows\nthe user to fine-tune the enhanced image using intuitive options.",
          "link": "http://arxiv.org/abs/2107.12898",
          "publishedOn": "2021-07-28T02:02:32.384Z",
          "wordCount": 560,
          "title": "StarEnhancer: Learning Real-Time and Style-Aware Image Enhancement. (arXiv:2107.12898v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1\">Ross Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1\">Pujitha Gunaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "Understanding occupant-vehicle interactions by modeling control transitions\nis important to ensure safe approaches to passenger vehicle automation. Models\nwhich contain contextual, semantically meaningful representations of driver\nstates can be used to determine the appropriate timing and conditions for\ntransfer of control between driver and vehicle. However, such models rely on\nreal-world control take-over data from drivers engaged in distracting\nactivities, which is costly to collect. Here, we introduce a scheme for data\naugmentation for such a dataset. Using the augmented dataset, we develop and\ntrain take-over time (TOT) models that operate sequentially on mid and\nhigh-level features produced by computer vision algorithms operating on\ndifferent driver-facing camera views, showing models trained on the augmented\ndataset to outperform the initial dataset. The demonstrated model features\nencode different aspects of the driver state, pertaining to the face, hands,\nfoot and upper body of the driver. We perform ablative experiments on feature\ncombinations as well as model architectures, showing that a TOT model supported\nby augmented data can be used to produce continuous estimates of take-over\ntimes without delay, suitable for complex real-world scenarios.",
          "link": "http://arxiv.org/abs/2107.12932",
          "publishedOn": "2021-07-28T02:02:32.345Z",
          "wordCount": 639,
          "title": "Predicting Take-over Time for Autonomous Driving with Real-World Data: Robust Data Augmentation, Models, and Evaluation. (arXiv:2107.12932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>",
          "description": "This paper presents the evaluation of effects of image size on deep learning\nperformance via semantic segmentation of magnetic resonance heart images with\nU-net for fully automated quantification of myocardial infarction. Both\nnon-extra pixel and extra pixel interpolation algorithms are used to change the\nsize of images in datasets of interest. Extra class labels, in interpolated\nground truth segmentation images, are removed using thresholding, median\nfiltering, and subtraction strategies. Common class metrics are used to\nevaluate the quality of semantic segmentation with U-net against the ground\ntruth segmentation while arbitrary threshold, comparison of the sums, and sums\nof differences between medical experts and fully automated results are options\nused to estimate the relationship between medical experts-based quantification\nand fully automated quantification results.",
          "link": "http://arxiv.org/abs/2101.11508",
          "publishedOn": "2021-07-28T02:02:32.326Z",
          "wordCount": 592,
          "title": "Effects of Image Size on Deep Learning. (arXiv:2101.11508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of estimating frame-level full human body meshes\ngiven a video of a person with natural motion dynamics. While much progress in\nthis field has been in single image-based mesh estimation, there has been a\nrecent uptick in efforts to infer mesh dynamics from video given its role in\nalleviating issues such as depth ambiguity and occlusions. However, a key\nlimitation of existing work is the assumption that all the observed motion\ndynamics can be modeled using one dynamical/recurrent model. While this may\nwork well in cases with relatively simplistic dynamics, inference with\nin-the-wild videos presents many challenges. In particular, it is typically the\ncase that different body parts of a person undergo different dynamics in the\nvideo, e.g., legs may move in a way that may be dynamically different from\nhands (e.g., a person dancing). To address these issues, we present a new\nmethod for video mesh recovery that divides the human mesh into several local\nparts following the standard skeletal model. We then model the dynamics of each\nlocal part with separate recurrent models, with each model conditioned\nappropriately based on the known kinematic structure of the human body. This\nresults in a structure-informed local recurrent learning architecture that can\nbe trained in an end-to-end fashion with available annotations. We conduct a\nvariety of experiments on standard video mesh recovery benchmark datasets such\nas Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design\nof modeling local dynamics as well as establishing state-of-the-art results\nbased on standard evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.12847",
          "publishedOn": "2021-07-28T02:02:32.316Z",
          "wordCount": 708,
          "title": "Learning Local Recurrent Models for Human Mesh Recovery. (arXiv:2107.12847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-07-28T02:02:32.301Z",
          "wordCount": 577,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Andr&#xe9;s G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genevois_T/0/1/0/all/0/1\">Thomas Genevois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lussereau_J/0/1/0/all/0/1\">Jerome Lussereau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laugier_C/0/1/0/all/0/1\">Christian Laugier</a>",
          "description": "Object detection is a critical problem for the safe interaction between\nautonomous vehicles and road users. Deep-learning methodologies allowed the\ndevelopment of object detection approaches with better performance. However,\nthere is still the challenge to obtain more characteristics from the objects\ndetected in real-time. The main reason is that more information from the\nenvironment's objects can improve the autonomous vehicle capacity to face\ndifferent urban situations. This paper proposes a new approach to detect static\nand dynamic objects in front of an autonomous vehicle. Our approach can also\nget other characteristics from the objects detected, like their position,\nvelocity, and heading. We develop our proposal fusing results of the\nenvironment's interpretations achieved of YoloV3 and a Bayesian filter. To\ndemonstrate our proposal's performance, we asses it through a benchmark dataset\nand real-world data obtained from an autonomous platform. We compared the\nresults achieved with another approach.",
          "link": "http://arxiv.org/abs/2107.12692",
          "publishedOn": "2021-07-28T02:02:31.716Z",
          "wordCount": 599,
          "title": "Dynamic and Static Object Detection Considering Fusion Regions and Point-wise Features. (arXiv:2107.12692v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesaire_M/0/1/0/all/0/1\">Manon C&#xe9;saire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1\">Hatem Hajri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "This paper introduces stochastic sparse adversarial attacks (SSAA), simple,\nfast and purely noise-based targeted and untargeted $L_0$ attacks of neural\nnetwork classifiers (NNC). SSAA are devised by exploiting a simple small-time\nexpansion idea widely used for Markov processes and offer new examples of $L_0$\nattacks whose studies have been limited. They are designed to solve the known\nscalability issue of the family of Jacobian-based saliency maps attacks to\nlarge datasets and they succeed in solving it. Experiments on small and large\ndatasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in\ncomparison with the-state-of-the-art methods. For instance, in the untargeted\ncase, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently\nto ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up\nto $\\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better\n$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful\non a large number of samples. Codes are publicly available through the link\nhttps://github.com/SSAA3/stochastic-sparse-adv-attacks",
          "link": "http://arxiv.org/abs/2011.12423",
          "publishedOn": "2021-07-28T02:02:31.697Z",
          "wordCount": 644,
          "title": "Stochastic sparse adversarial attacks. (arXiv:2011.12423v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Bost_X/0/1/0/all/0/1\">Xavier Bost</a> (LIA)",
          "description": "A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.",
          "link": "http://arxiv.org/abs/1907.02704",
          "publishedOn": "2021-07-28T02:02:31.671Z",
          "wordCount": 714,
          "title": "Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohui Hu</a>",
          "description": "Continuous sign language recognition (cSLR) is a public significant task that\ntranscribes a sign language video into an ordered gloss sequence. It is\nimportant to capture the fine-grained gloss-level details, since there is no\nexplicit alignment between sign video frames and the corresponding glosses.\nAmong the past works, one promising way is to adopt a one-dimensional\nconvolutional network (1D-CNN) to temporally fuse the sequential frames.\nHowever, CNNs are agnostic to similarity or dissimilarity, and thus are unable\nto capture local consistent semantics within temporally neighboring frames. To\naddress the issue, we propose to adaptively fuse local features via temporal\nsimilarity for this task. Specifically, we devise a Multi-scale Local-Temporal\nSimilarity Fusion Network (mLTSF-Net) as follows: 1) In terms of a specific\nvideo frame, we firstly select its similar neighbours with multi-scale\nreceptive regions to accommodate different lengths of glosses. 2) To ensure\ntemporal consistency, we then use position-aware convolution to temporally\nconvolve each scale of selected frames. 3) To obtain a local-temporally\nenhanced frame-wise representation, we finally fuse the results of different\nscales using a content-dependent aggregator. We train our model in an\nend-to-end fashion, and the experimental results on RWTH-PHOENIX-Weather 2014\ndatasets (RWTH) demonstrate that our model achieves competitive performance\ncompared with several state-of-the-art models.",
          "link": "http://arxiv.org/abs/2107.12762",
          "publishedOn": "2021-07-28T02:02:31.663Z",
          "wordCount": 649,
          "title": "Multi-Scale Local-Temporal Similarity Fusion for Continuous Sign Language Recognition. (arXiv:2107.12762v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yabang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1\">Andrew Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>",
          "description": "3D deep learning has been increasingly more popular for a variety of tasks\nincluding many safety-critical applications. However, recently several works\nraise the security issues of 3D deep nets. Although most of these works\nconsider adversarial attacks, we identify that backdoor attack is indeed a more\nserious threat to 3D deep learning systems but remains unexplored. We present\nthe backdoor attacks in 3D with a unified framework that exploits the unique\nproperties of 3D data and networks. In particular, we design two attack\napproaches: the poison-label attack and the clean-label attack. The first one\nis straightforward and effective in practice, while the second one is more\nsophisticated assuming there are certain data inspections. The attack\nalgorithms are mainly motivated and developed by 1) the recent discovery of 3D\nadversarial samples which demonstrate the vulnerability of 3D deep nets under\nspatial transformations; 2) the proposed feature disentanglement technique that\nmanipulates the feature of the data through optimization methods and its\npotential to embed a new task. Extensive experiments show the efficacy of the\npoison-label attack with over 95% success rate across several 3D datasets and\nmodels, and the ability of clean-label attack against data filtering with\naround 50% success rate. Our proposed backdoor attack in 3D point cloud is\nexpected to perform as a baseline for improving the robustness of 3D deep\nmodels.",
          "link": "http://arxiv.org/abs/2103.16074",
          "publishedOn": "2021-07-28T02:02:31.649Z",
          "wordCount": 702,
          "title": "PointBA: Towards Backdoor Attacks in 3D Point Cloud. (arXiv:2103.16074v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-07-28T02:02:31.629Z",
          "wordCount": 641,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Traditional learning systems are trained in closed-world for a fixed number\nof classes, and need pre-collected datasets in advance. However, new classes\noften emerge in real-world applications and should be learned incrementally.\nFor example, in electronic commerce, new types of products appear daily, and in\na social media community, new topics emerge frequently. Under such\ncircumstances, incremental models should learn several new classes at a time\nwithout forgetting. We find a strong correlation between old and new classes in\nincremental learning, which can be applied to relate and facilitate different\nlearning stages mutually. As a result, we propose CO-transport for class\nIncremental Learning (COIL), which learns to relate across incremental tasks\nwith the class-wise semantic relationship. In detail, co-transport has two\naspects: prospective transport tries to augment the old classifier with optimal\ntransported knowledge as fast model adaptation. Retrospective transport aims to\ntransport new class classifiers backward as old ones to overcome forgetting.\nWith these transports, COIL efficiently adapts to new tasks, and stably resists\nforgetting. Experiments on benchmark and real-world multimedia datasets\nvalidate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.12654",
          "publishedOn": "2021-07-28T02:02:31.622Z",
          "wordCount": 616,
          "title": "Co-Transport for Class-Incremental Learning. (arXiv:2107.12654v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponomarev_E/0/1/0/all/0/1\">Evgeny Ponomarev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matveev_S/0/1/0/all/0/1\">Sergey Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan Oseledets</a>",
          "description": "A lot of deep learning applications are desired to be run on mobile devices.\nBoth accuracy and inference time are meaningful for a lot of them. While the\nnumber of FLOPs is usually used as a proxy for neural network latency, it may\nbe not the best choice. In order to obtain a better approximation of latency,\nresearch community uses look-up tables of all possible layers for latency\ncalculation for the final prediction of the inference on mobile CPU. It\nrequires only a small number of experiments. Unfortunately, on mobile GPU this\nmethod is not applicable in a straight-forward way and shows low precision. In\nthis work, we consider latency approximation on mobile GPU as a data and\nhardware-specific problem. Our main goal is to construct a convenient latency\nestimation tool for investigation(LETI) of neural network inference and\nbuilding robust and accurate latency prediction models for each specific task.\nTo achieve this goal, we build open-source tools which provide a convenient way\nto conduct massive experiments on different target devices focusing on mobile\nGPU. After evaluation of the dataset, we learn the regression model on\nexperimental data and use it for future latency prediction and analysis. We\nexperimentally demonstrate the applicability of such an approach on a subset of\npopular NAS-Benchmark 101 dataset and also evaluate the most popular neural\nnetwork architectures for two mobile GPUs. As a result, we construct latency\nprediction model with good precision on the target evaluation subset. We\nconsider LETI as a useful tool for neural architecture search or massive\nlatency evaluation. The project is available at https://github.com/leti-ai",
          "link": "http://arxiv.org/abs/2010.02871",
          "publishedOn": "2021-07-28T02:02:31.615Z",
          "wordCount": 758,
          "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU. (arXiv:2010.02871v2 [cs.PF] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1\">Byeongjoon Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hansaem Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>",
          "description": "Traffic accidents are a threat to human lives, particularly pedestrians\ncausing premature deaths. Therefore, it is necessary to devise systems to\nprevent accidents in advance and respond proactively, using potential risky\nsituations as one of the surrogate safety measurements. This study introduces a\nnew concept of a pedestrian safety system that combines the field and the\ncentralized processes. The system can warn of upcoming risks immediately in the\nfield and improve the safety of risk frequent areas by assessing the safety\nlevels of roads without actual collisions. In particular, this study focuses on\nthe latter by introducing a new analytical framework for a crosswalk safety\nassessment with behaviors of vehicle/pedestrian and environmental features. We\nobtain these behavioral features from actual traffic video footage in the city\nwith complete automatic processing. The proposed framework mainly analyzes\nthese behaviors in multidimensional perspectives by constructing a data cube\nstructure, which combines the LSTM based predictive collision risk estimation\nmodel and the on line analytical processing operations. From the PCR estimation\nmodel, we categorize the severity of risks as four levels and apply the\nproposed framework to assess the crosswalk safety with behavioral features. Our\nanalytic experiments are based on two scenarios, and the various descriptive\nresults are harvested the movement patterns of vehicles and pedestrians by road\nenvironment and the relationships between risk levels and car speeds. Thus, the\nproposed framework can support decision makers by providing valuable\ninformation to improve pedestrian safety for future accidents, and it can help\nus better understand their behaviors near crosswalks proactively. In order to\nconfirm the feasibility and applicability of the proposed framework, we\nimplement and apply it to actual operating CCTVs in Osan City, Korea.",
          "link": "http://arxiv.org/abs/2107.12507",
          "publishedOn": "2021-07-28T02:02:31.608Z",
          "wordCount": 734,
          "title": "Analyzing vehicle pedestrian interactions combining data cube structure and predictive collision risk estimation model. (arXiv:2107.12507v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thai-Son Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stueker_S/0/1/0/all/0/1\">Sebastian Stueker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>",
          "description": "Achieving super-human performance in recognizing human speech has been a goal\nfor several decades, as researchers have worked on increasingly challenging\ntasks. In the 1990's it was discovered, that conversational speech between two\nhumans turns out to be considerably more difficult than read speech as\nhesitations, disfluencies, false starts and sloppy articulation complicate\nacoustic processing and require robust handling of acoustic, lexical and\nlanguage context, jointly. Early attempts with statistical models could only\nreach error rates over 50% and far from human performance (WER of around 5.5%).\nNeural hybrid models and recent attention-based encoder-decoder models have\nconsiderably improved performance as such contexts can now be learned in an\nintegral fashion. However, processing such contexts requires an entire\nutterance presentation and thus introduces unwanted delays before a recognition\nresult can be output. In this paper, we address performance as well as latency.\nWe present results for a system that can achieve super-human performance (at a\nWER of 5.0%, over the Switchboard conversational benchmark) at a word based\nlatency of only 1 second behind a speaker's speech. The system uses multiple\nattention-based encoder-decoder networks integrated within a novel low latency\nincremental inference approach.",
          "link": "http://arxiv.org/abs/2010.03449",
          "publishedOn": "2021-07-28T02:02:31.599Z",
          "wordCount": 689,
          "title": "Super-Human Performance in Online Low-latency Recognition of Conversational Speech. (arXiv:2010.03449v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Secci_F/0/1/0/all/0/1\">Francesco Secci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1\">Andrea Ceccarelli</a>",
          "description": "RGB cameras are one of the most relevant sensors for autonomous driving\napplications. It is undeniable that failures of vehicle cameras may compromise\nthe autonomous driving task, possibly leading to unsafe behaviors when images\nthat are subsequently processed by the driving system are altered. To support\nthe definition of safe and robust vehicle architectures and intelligent\nsystems, in this paper we define the failure modes of a vehicle camera,\ntogether with an analysis of effects and known mitigations. Further, we build a\nsoftware library for the generation of the corresponding failed images and we\nfeed them to six object detectors for mono and stereo cameras and to the\nself-driving agent of an autonomous driving simulator. The resulting\nmisbehaviors with respect to operating with clean images allow a better\nunderstanding of failures effects and the related safety risks in image-based\napplications.",
          "link": "http://arxiv.org/abs/2008.05938",
          "publishedOn": "2021-07-28T02:02:31.581Z",
          "wordCount": 611,
          "title": "RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08158",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1\">Michael Gadermayr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1\">Maximilian Tschuchnig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stangassinger_L/0/1/0/all/0/1\">Lea Maria Stangassinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreutzer_C/0/1/0/all/0/1\">Christina Kreutzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couillard_Despres_S/0/1/0/all/0/1\">Sebastien Couillard-Despres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oostingh_G/0/1/0/all/0/1\">Gertie Janneke Oostingh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hittmair_A/0/1/0/all/0/1\">Anton Hittmair</a>",
          "description": "In contrast to paraffin sections, frozen sections can be quickly generated\nduring surgical interventions. This procedure allows surgeons to wait for\nhistological findings during the intervention to base intra-operative decisions\non the outcome of the histology. However, compared to paraffin sections, the\nquality of frozen sections is typically lower, leading to a higher ratio of\nmiss-classification. In this work, we investigated the effect of the section\ntype on automated decision support approaches for classification of thyroid\ncancer. This was enabled by a data set consisting of pairs of sections for\nindividual patients. Moreover, we investigated, whether a frozen-to-paraffin\ntranslation could help to optimize classification scores. Finally, we propose a\nspecific data augmentation strategy to deal with a small amount of training\ndata and to increase classification accuracy even further.",
          "link": "http://arxiv.org/abs/2012.08158",
          "publishedOn": "2021-07-28T02:02:31.574Z",
          "wordCount": 622,
          "title": "Frozen-to-Paraffin: Categorization of Histological Frozen Sections by the Aid of Paraffin Sections and Generative Adversarial Networks. (arXiv:2012.08158v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sohee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungkyu Lee</a>",
          "description": "Continual learning is a concept of online learning with multiple sequential\ntasks. One of the critical barriers of continual learning is that a network\nshould learn a new task keeping the knowledge of old tasks without access to\nany data of the old tasks. In this paper, we propose a neuron activation\nimportance-based regularization method for stable continual learning regardless\nof the order of tasks. We conduct comprehensive experiments on existing\nbenchmark data sets to evaluate not just the stability and plasticity of our\nmethod with improved classification accuracy also the robustness of the\nperformance along the changes of task order.",
          "link": "http://arxiv.org/abs/2107.12657",
          "publishedOn": "2021-07-28T02:02:31.567Z",
          "wordCount": 529,
          "title": "Continual Learning with Neuron Activation Importance. (arXiv:2107.12657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunxia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>",
          "description": "The combination of a small unmanned ground vehicle (UGV) and a large unmanned\ncarrier vehicle allows more flexibility in real applications such as rescue in\ndangerous scenarios. The autonomous recovery system, which is used to guide the\nsmall UGV back to the carrier vehicle, is an essential component to achieve a\nseamless combination of the two vehicles. This paper proposes a novel\nautonomous recovery framework with a low-cost monocular vision system to\nprovide accurate positioning and attitude estimation of the UGV during\nnavigation. First, we introduce a light-weight convolutional neural network\ncalled UGV-KPNet to detect the keypoints of the small UGV from the images\ncaptured by a monocular camera. UGV-KPNet is computationally efficient with a\nsmall number of parameters and provides pixel-level accurate keypoints\ndetection results in real-time. Then, six degrees of freedom pose is estimated\nusing the detected keypoints to obtain positioning and attitude information of\nthe UGV. Besides, we are the first to create a large-scale real-world keypoints\ndataset of the UGV. The experimental results demonstrate that the proposed\nsystem achieves state-of-the-art performance in terms of both accuracy and\nspeed on UGV keypoint detection, and can further boost the 6-DoF pose\nestimation for the UGV.",
          "link": "http://arxiv.org/abs/2107.12852",
          "publishedOn": "2021-07-28T02:02:31.560Z",
          "wordCount": 653,
          "title": "Real-time Keypoints Detection for Autonomous Recovery of the Unmanned Ground Vehicle. (arXiv:2107.12852v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benny_Y/0/1/0/all/0/1\">Yaniv Benny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekar_N/0/1/0/all/0/1\">Niv Pekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "We consider the abstract relational reasoning task, which is commonly used as\nan intelligence test. Since some patterns have spatial rationales, while others\nare only semantic, we propose a multi-scale architecture that processes each\nquery in multiple resolutions. We show that indeed different rules are solved\nby different resolutions and a combined multi-scale approach outperforms the\nexisting state of the art in this task on all benchmarks by 5-54%. The success\nof our method is shown to arise from multiple novelties. First, it searches for\nrelational patterns in multiple resolutions, which allows it to readily detect\nvisual relations, such as location, in higher resolution, while allowing the\nlower resolution module to focus on semantic relations, such as shape type.\nSecond, we optimize the reasoning network of each resolution proportionally to\nits performance, hereby we motivate each resolution to specialize on the rules\nfor which it performs better than the others and ignore cases that are already\nsolved by the other resolutions. Third, we propose a new way to pool\ninformation along the rows and the columns of the illustration-grid of the\nquery. Our work also analyses the existing benchmarks, demonstrating that the\nRAVEN dataset selects the negative examples in a way that is easily exploited.\nWe, therefore, propose a modified version of the RAVEN dataset, named\nRAVEN-FAIR. Our code and pretrained models are available at\nhttps://github.com/yanivbenny/MRNet.",
          "link": "http://arxiv.org/abs/2009.09405",
          "publishedOn": "2021-07-28T02:02:31.553Z",
          "wordCount": 689,
          "title": "Scale-Localized Abstract Reasoning. (arXiv:2009.09405v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhaoyu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Pin Siang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_J/0/1/0/all/0/1\">Junkang Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jimmy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheong_Y/0/1/0/all/0/1\">Yehur Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Hsing Wang</a>",
          "description": "3D point cloud interpretation is a challenging task due to the randomness and\nsparsity of the component points. Many of the recently proposed methods like\nPointNet and PointCNN have been focusing on learning shape descriptions from\npoint coordinates as point-wise input features, which usually involves\ncomplicated network architectures. In this work, we draw attention back to the\nstandard 3D convolutions towards an efficient 3D point cloud interpretation.\nInstead of converting the entire point cloud into voxel representations like\nthe other volumetric methods, we voxelize the sub-portions of the point cloud\nonly at necessary locations within each convolution layer on-the-fly, using our\ndynamic voxelization operation with self-adaptive voxelization resolution. In\naddition, we incorporate 3D group convolution into our dense convolution kernel\nimplementation to further exploit the rotation invariant features of point\ncloud. Benefiting from its simple fully-convolutional architecture, our network\nis able to run and converge at a considerably fast speed, while yields on-par\nor even better performance compared with the state-of-the-art methods on\nseveral benchmark datasets.",
          "link": "http://arxiv.org/abs/2009.02918",
          "publishedOn": "2021-07-28T02:02:31.534Z",
          "wordCount": 647,
          "title": "DV-ConvNet: Fully Convolutional Deep Learning on Point Clouds with Dynamic Voxelization and 3D Group Convolution. (arXiv:2009.02918v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabo_A/0/1/0/all/0/1\">Andrea Sabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdizadeh_S/0/1/0/all/0/1\">Sina Mehdizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iaboni_A/0/1/0/all/0/1\">Andrea Iaboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1\">Babak Taati</a>",
          "description": "Drug-induced parkinsonism affects many older adults with dementia, often\ncausing gait disturbances. New advances in vision-based human pose-estimation\nhave opened possibilities for frequent and unobtrusive analysis of gait in\nresidential settings. This work proposes novel spatial-temporal graph\nconvolutional network (ST-GCN) architectures and training procedures to predict\nclinical scores of parkinsonism in gait from video of individuals with\ndementia. We propose a two-stage training approach consisting of a\nself-supervised pretraining stage that encourages the ST-GCN model to learn\nabout gait patterns before predicting clinical scores in the finetuning stage.\nThe proposed ST-GCN models are evaluated on joint trajectories extracted from\nvideo and are compared against traditional (ordinal, linear, random forest)\nregression models and temporal convolutional network baselines. Three 2D human\npose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft\nKinect (2D and 3D) are used to extract joint trajectories of 4787 natural\nwalking bouts from 53 older adults with dementia. A subset of 399 walks from 14\nparticipants is annotated with scores of parkinsonism severity on the gait\ncriteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the\nSimpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating\non 3D joint trajectories extracted from the Kinect consistently outperform all\nother models and feature sets. Prediction of parkinsonism scores in natural\nwalking bouts of unseen participants remains a challenging task, with the best\nmodels achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02\nfor UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for\nthis work is available:\nhttps://github.com/TaatiTeam/stgcn_parkinsonism_prediction.",
          "link": "http://arxiv.org/abs/2105.03464",
          "publishedOn": "2021-07-28T02:02:31.527Z",
          "wordCount": 723,
          "title": "Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia. (arXiv:2105.03464v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-07-28T02:02:31.518Z",
          "wordCount": 654,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zefeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhiyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Text-to-image person re-identification (ReID) aims to search for images\ncontaining a person of interest using textual descriptions. However, due to the\nsignificant modality gap and the large intra-class variance in textual\ndescriptions, text-to-image ReID remains a challenging problem. Accordingly, in\nthis paper, we propose a Semantically Self-Aligned Network (SSAN) to handle the\nabove problems. First, we propose a novel method that automatically extracts\nsemantically aligned part-level features from the two modalities. Second, we\ndesign a multi-view non-local network that captures the relationships between\nbody parts, thereby establishing better correspondences between body parts and\nnoun phrases. Third, we introduce a Compound Ranking (CR) loss that makes use\nof textual descriptions for other images of the same identity to provide extra\nsupervision, thereby effectively reducing the intra-class variance in textual\nfeatures. Finally, to expedite future research in text-to-image ReID, we build\na new database named ICFG-PEDES. Extensive experiments demonstrate that SSAN\noutperforms state-of-the-art approaches by significant margins. Both the new\nICFG-PEDES database and the SSAN code are available at\nhttps://github.com/zifyloo/SSAN.",
          "link": "http://arxiv.org/abs/2107.12666",
          "publishedOn": "2021-07-28T02:02:31.511Z",
          "wordCount": 616,
          "title": "Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification. (arXiv:2107.12666v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>",
          "description": "Arbitrary shape text detection is a challenging task due to the high\ncomplexity and variety of scene texts. In this work, we propose a novel\nadaptive boundary proposal network for arbitrary shape text detection, which\ncan learn to directly produce accurate boundary for arbitrary shape text\nwithout any post-processing. Our method mainly consists of a boundary proposal\nmodel and an innovative adaptive boundary deformation model. The boundary\nproposal model constructed by multi-layer dilated convolutions is adopted to\nproduce prior information (including classification map, distance field, and\ndirection field) and coarse boundary proposals. The adaptive boundary\ndeformation model is an encoder-decoder network, in which the encoder mainly\nconsists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network\n(RNN). It aims to perform boundary deformation in an iterative way for\nobtaining text instance shape guided by prior information from the boundary\nproposal model.In this way, our method can directly and efficiently generate\naccurate text boundaries without complex post-processing. Extensive experiments\non publicly available datasets demonstrate the state-of-the-art performance of\nour method.",
          "link": "http://arxiv.org/abs/2107.12664",
          "publishedOn": "2021-07-28T02:02:31.489Z",
          "wordCount": 633,
          "title": "Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection. (arXiv:2107.12664v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Localizing individuals in crowds is more in accordance with the practical\ndemands of subsequent high-level crowd analysis tasks than simply counting.\nHowever, existing localization based methods relying on intermediate\nrepresentations (\\textit{i.e.}, density maps or pseudo boxes) serving as\nlearning targets are counter-intuitive and error-prone. In this paper, we\npropose a purely point-based framework for joint crowd counting and individual\nlocalization. For this framework, instead of merely reporting the absolute\ncounting error at image level, we propose a new metric, called density\nNormalized Average Precision (nAP), to provide more comprehensive and more\nprecise performance evaluation. Moreover, we design an intuitive solution under\nthis framework, which is called Point to Point Network (P2PNet). P2PNet\ndiscards superfluous steps and directly predicts a set of point proposals to\nrepresent heads in an image, being consistent with the human annotation\nresults. By thorough analysis, we reveal the key step towards implementing such\na novel idea is to assign optimal learning targets for these proposals.\nTherefore, we propose to conduct this crucial association in an one-to-one\nmatching manner using the Hungarian algorithm. The P2PNet not only\nsignificantly surpasses state-of-the-art methods on popular counting\nbenchmarks, but also achieves promising localization accuracy. The codes will\nbe available at: https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet.",
          "link": "http://arxiv.org/abs/2107.12746",
          "publishedOn": "2021-07-28T02:02:31.462Z",
          "wordCount": 658,
          "title": "Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework. (arXiv:2107.12746v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiayu Cao</a>",
          "description": "Agriculture is an essential industry in the both society and economy of a\ncountry. However, the pests and diseases cause a great amount of reduction in\nagricultural production while there is not sufficient guidance for farmers to\navoid this disaster. To address this problem, we apply CNNs to plant disease\nrecognition by building a classification model. Within the dataset of 3,642\nimages of apple leaves, We use a pre-trained image classification model\nRestnet34 based on a Convolutional neural network (CNN) with the Fastai\nframework in order to save the training time. Overall, the accuracy of\nclassification is 93.765%.",
          "link": "http://arxiv.org/abs/2107.12598",
          "publishedOn": "2021-07-28T02:02:31.454Z",
          "wordCount": 535,
          "title": "Identify Apple Leaf Diseases Using Deep Learning Algorithm. (arXiv:2107.12598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1\">Denis Gudovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishizaka_S/0/1/0/all/0/1\">Shun Ishizaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1\">Kazuki Kozuka</a>",
          "description": "Unsupervised anomaly detection with localization has many practical\napplications when labeling is infeasible and, moreover, when anomaly examples\nare completely missing in the train data. While recently proposed models for\nsuch data setup achieve high accuracy metrics, their complexity is a limiting\nfactor for real-time processing. In this paper, we propose a real-time model\nand analytically derive its relationship to prior methods. Our CFLOW-AD model\nis based on a conditional normalizing flow framework adopted for anomaly\ndetection with localization. In particular, CFLOW-AD consists of a\ndiscriminatively pretrained encoder followed by a multi-scale generative\ndecoders where the latter explicitly estimate likelihood of the encoded\nfeatures. Our approach results in a computationally and memory-efficient model:\nCFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art\nwith the same input setting. Our experiments on the MVTec dataset show that\nCFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by\n1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source\nour code with fully reproducible experiments.",
          "link": "http://arxiv.org/abs/2107.12571",
          "publishedOn": "2021-07-28T02:02:31.447Z",
          "wordCount": 620,
          "title": "CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows. (arXiv:2107.12571v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zixin Zhu</a> (Xi&#x27;an jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a> (University of Illinois at Chicago), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a> (Wormpex AI Research)",
          "description": "Effectively tackling the problem of temporal action localization (TAL)\nnecessitates a visual representation that jointly pursues two confounding\ngoals, i.e., fine-grained discrimination for temporal localization and\nsufficient visual invariance for action classification. We address this\nchallenge by enriching both the local and global contexts in the popular\ntwo-stage temporal localization framework, where action proposals are first\ngenerated followed by action classification and temporal boundary regression.\nOur proposed model, dubbed ContextLoc, can be divided into three sub-networks:\nL-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained\nmodeling of snippet-level features, which is formulated as a\nquery-and-retrieval process. G-Net enriches the global context via higher-level\nmodeling of the video-level representation. In addition, we introduce a novel\ncontext adaptation module to adapt the global context to different proposals.\nP-Net further models the context-aware inter-proposal relations. We explore two\nexisting models to be the P-Net in our experiments. The efficacy of our\nproposed method is validated by experimental results on the THUMOS14 (54.3\\% at\nIoU@0.5) and ActivityNet v1.3 (51.24\\% at IoU@0.5) datasets, which outperforms\nrecent states of the art.",
          "link": "http://arxiv.org/abs/2107.12960",
          "publishedOn": "2021-07-28T02:02:31.433Z",
          "wordCount": 639,
          "title": "Enriching Local and Global Contexts for Temporal Action Localization. (arXiv:2107.12960v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mirza S. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deppen_S/0/1/0/all/0/1\">Steve Deppen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_K/0/1/0/all/0/1\">Kim L. Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massion_P/0/1/0/all/0/1\">Pierre P. Massion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>",
          "description": "Image Quality Assessment (IQA) is important for scientific inquiry,\nespecially in medical imaging and machine learning. Potential data quality\nissues can be exacerbated when human-based workflows use limited views of the\ndata that may obscure digital artifacts. In practice, multiple factors such as\nnetwork issues, accelerated acquisitions, motion artifacts, and imaging\nprotocol design can impede the interpretation of image collections. The medical\nimage processing community has developed a wide variety of tools for the\ninspection and validation of imaging data. Yet, IQA of computed tomography (CT)\nremains an under-recognized challenge, and no user-friendly tool is commonly\navailable to address these potential issues. Here, we create and illustrate a\npipeline specifically designed to identify and resolve issues encountered with\nlarge-scale data mining of clinically acquired CT data. Using the widely\nstudied National Lung Screening Trial (NLST), we have identified approximately\n4% of image volumes with quality concerns out of 17,392 scans. To assess\nrobustness, we applied the proposed pipeline to our internal datasets where we\nfind our tool is generalizable to clinically acquired medical images. In\nconclusion, the tool has been useful and time-saving for research study of\nclinical data, and the code and tutorials are publicly available at\nhttps://github.com/MASILab/QA_tool.",
          "link": "http://arxiv.org/abs/2107.12842",
          "publishedOn": "2021-07-28T02:02:31.423Z",
          "wordCount": 662,
          "title": "Technical Report: Quality Assessment Tool for Machine Learning with Clinical CT. (arXiv:2107.12842v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sungmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dogyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junhyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sangwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Woojin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>",
          "description": "Despite the remarkable success of deep learning, optimal convolution\noperation on point cloud remains indefinite due to its irregular data\nstructure. In this paper, we present Cubic Kernel Convolution (CKConv) that\nlearns to voxelize the features of local points by exploiting both continuous\nand discrete convolutions. Our continuous convolution uniquely employs a 3D\ncubic form of kernel weight representation that splits a feature into voxels in\nembedding space. By consecutively applying discrete 3D convolutions on the\nvoxelized features in a spatial manner, preceding continuous convolution is\nforced to learn spatial feature mapping, i.e., feature voxelization. In this\nway, geometric information can be detailed by encoding with subdivided\nfeatures, and our 3D convolutions on these fixed structured data do not suffer\nfrom discretization artifacts thanks to voxelization in embedding space.\nFurthermore, we propose a spatial attention module, Local Set Attention (LSA),\nto provide comprehensive structure awareness within the local point set and\nhence produce representative features. By learning feature voxelization with\nLSA, CKConv can extract enriched features for effective point cloud analysis.\nWe show that CKConv has great applicability to point cloud processing tasks\nincluding object classification, object part segmentation, and scene semantic\nsegmentation with state-of-the-art results.",
          "link": "http://arxiv.org/abs/2107.12655",
          "publishedOn": "2021-07-28T02:02:31.412Z",
          "wordCount": 636,
          "title": "CKConv: Learning Feature Voxelization for Point Cloud Analysis. (arXiv:2107.12655v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">Alessio Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donaher_S/0/1/0/all/0/1\">Santiago Donaher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iashin_V/0/1/0/all/0/1\">Vladimir Iashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1\">Francesca Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solak_G/0/1/0/all/0/1\">G&#xf6;khan Solak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppola_C/0/1/0/all/0/1\">Claudio Coppola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_R/0/1/0/all/0/1\">Reina Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagao_Y/0/1/0/all/0/1\">Yuichi Nagao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanlin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Rosa H. M. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christmann_G/0/1/0/all/0/1\">Guilherme Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jyun-Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeharika_G/0/1/0/all/0/1\">Gonuguntla Neeharika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chinnakotla Krishna Teja Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Dinesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehman_B/0/1/0/all/0/1\">Bakhtawar Ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>",
          "description": "Acoustic and visual sensing can support the contactless estimation of the\nweight of a container and the amount of its content when the container is\nmanipulated by a person. However, transparencies (both of the container and of\nthe content) and the variability of materials, shapes and sizes make this\nproblem challenging. In this paper, we present an open benchmarking framework\nand an in-depth comparative analysis of recent methods that estimate the\ncapacity of a container, as well as the type, mass, and amount of its content.\nThese methods use learned and handcrafted features, such as mel-frequency\ncepstrum coefficients, zero-crossing rate, spectrograms, with different types\nof classifiers to estimate the type and amount of the content with acoustic\ndata, and geometric approaches with visual data to determine the capacity of\nthe container. Results on a newly distributed dataset show that audio alone is\na strong modality and methods achieves a weighted average F1-score up to 81%\nand 97% for content type and level classification, respectively. Estimating the\ncontainer capacity with vision-only approaches and filling mass with\nmulti-modal, multi-stage algorithms reaches up to 65% weighted average capacity\nand mass scores.",
          "link": "http://arxiv.org/abs/2107.12719",
          "publishedOn": "2021-07-28T02:02:31.386Z",
          "wordCount": 692,
          "title": "Multi-modal estimation of the properties of containers and their content: survey and evaluation. (arXiv:2107.12719v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>",
          "description": "Image manipulation with natural language, which aims to manipulate images\nwith the guidance of language descriptions, has been a challenging problem in\nthe fields of computer vision and natural language processing (NLP). Currently,\na number of efforts have been made for this task, but their performances are\nstill distant away from generating realistic and text-conformed manipulated\nimages. Therefore, in this paper, we propose a memory-based Image Manipulation\nNetwork (MIM-Net), where a set of memories learned from images is introduced to\nsynthesize the texture information with the guidance of the textual\ndescription. We propose a two-stage network with an additional reconstruction\nstage to learn the latent memories efficiently. To avoid the unnecessary\nbackground changes, we propose a Target Localization Unit (TLU) to focus on the\nmanipulation of the region mentioned by the text. Moreover, to learn a robust\nmemory, we further propose a novel randomized memory training loss. Experiments\non the four popular datasets show the better performance of our method compared\nto the existing ones.",
          "link": "http://arxiv.org/abs/2107.12579",
          "publishedOn": "2021-07-28T02:02:31.377Z",
          "wordCount": 607,
          "title": "Remember What You have drawn: Semantic Image Manipulation with Memory. (arXiv:2107.12579v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-07-28T02:02:31.369Z",
          "wordCount": 614,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huyan_N/0/1/0/all/0/1\">Ning Huyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_D/0/1/0/all/0/1\">Dou Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xuefeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>",
          "description": "Outlier detection is one of the most important processes taken to create\ngood, reliable data in machine learning. The most methods of outlier detection\nleverage an auxiliary reconstruction task by assuming that outliers are more\ndifficult to be recovered than normal samples (inliers). However, it is not\nalways true, especially for auto-encoder (AE) based models. They may recover\ncertain outliers even outliers are not in the training data, because they do\nnot constrain the feature learning. Instead, we think outlier detection can be\ndone in the feature space by measuring the feature distance between outliers\nand inliers. We then propose a framework, MCOD, using a memory module and a\ncontrastive learning module. The memory module constrains the consistency of\nfeatures, which represent the normal data. The contrastive learning module\nlearns more discriminating features, which boosts the distinction between\noutliers and inliers. Extensive experiments on four benchmark datasets show\nthat our proposed MCOD achieves a considerable performance and outperforms nine\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.12642",
          "publishedOn": "2021-07-28T02:02:31.350Z",
          "wordCount": 601,
          "title": "Unsupervised Outlier Detection using Memory and Contrastive Learning. (arXiv:2107.12642v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-07-28T02:02:31.341Z",
          "wordCount": 693,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turkoz_E/0/1/0/all/0/1\">Erkin T&#xfc;rk&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olcay_E/0/1/0/all/0/1\">Ertug Olcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oksanen_T/0/1/0/all/0/1\">Timo Oksanen</a>",
          "description": "This paper proposes a concept of computer vision-based guidance assistance\nfor agricultural vehicles to increase the accuracy in plowing and reduce\ndriver's cognitive burden in long-lasting tillage operations. Plowing is a\ncommon agricultural practice to prepare the soil for planting in many countries\nand it can take place both in the spring and the fall. Since plowing operation\nrequires high traction forces, it causes increased energy consumption.\nMoreover, longer operation time due to unnecessary maneuvers leads to higher\nfuel consumption. To provide necessary information for the driver and the\ncontrol unit of the tractor, a first concept of furrow detection system based\non an RGB-D camera was developed.",
          "link": "http://arxiv.org/abs/2107.12646",
          "publishedOn": "2021-07-28T02:02:31.318Z",
          "wordCount": 566,
          "title": "Computer Vision-Based Guidance Assistance Concept for Plowing Using RGB-D Camera. (arXiv:2107.12646v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompella_R/0/1/0/all/0/1\">Ramana Kompella</a>",
          "description": "Deep Neural Network (DNN) trained object detectors are widely deployed in\nmany mission-critical systems for real time video analytics at the edge, such\nas autonomous driving and video surveillance. A common performance requirement\nin these mission-critical edge services is the near real-time latency of online\nobject detection on edge devices. However, even with well-trained DNN object\ndetectors, the online detection quality at edge may deteriorate for a number of\nreasons, such as limited capacity to run DNN object detection models on\nheterogeneous edge devices, and detection quality degradation due to random\nframe dropping when the detection processing rate is significantly slower than\nthe incoming video frame rate. This paper addresses these problems by\nexploiting multi-model multi-device detection parallelism for fast object\ndetection in edge systems with heterogeneous edge devices. First, we analyze\nthe performance bottleneck of running a well-trained DNN model at edge for real\ntime online object detection. We use the offline detection as a reference\nmodel, and examine the root cause by analyzing the mismatch among the incoming\nvideo streaming rate, video processing rate for object detection, and output\nrate for real time detection visualization of video streaming. Second, we study\nperformance optimizations by exploiting multi-model detection parallelism. We\nshow that the model-parallel detection approach can effectively speed up the\nFPS detection processing rate, minimizing the FPS disparity with the incoming\nvideo frame rate on heterogeneous edge devices. We evaluate the proposed\napproach using SSD300 and YOLOv3 on benchmark videos of different video stream\nrates. The results show that exploiting multi-model detection parallelism can\nspeed up the online object detection processing rate and deliver near real-time\nobject detection performance for efficient video analytics at edge.",
          "link": "http://arxiv.org/abs/2107.12563",
          "publishedOn": "2021-07-28T02:02:31.304Z",
          "wordCount": 720,
          "title": "Parallel Detection for Efficient Video Analytics at the Edge. (arXiv:2107.12563v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1\">Runmin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_R/0/1/0/all/0/1\">Ronghui Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingzhi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>",
          "description": "Depth map super-resolution is a task with high practical application\nrequirements in the industry. Existing color-guided depth map super-resolution\nmethods usually necessitate an extra branch to extract high-frequency detail\ninformation from RGB image to guide the low-resolution depth map\nreconstruction. However, because there are still some differences between the\ntwo modalities, direct information transmission in the feature dimension or\nedge map dimension cannot achieve satisfactory result, and may even trigger\ntexture copying in areas where the structures of the RGB-D pair are\ninconsistent. Inspired by the multi-task learning, we propose a joint learning\nnetwork of depth map super-resolution (DSR) and monocular depth estimation\n(MDE) without introducing additional supervision labels. For the interaction of\ntwo subnetworks, we adopt a differentiated guidance strategy and design two\nbridges correspondingly. One is the high-frequency attention bridge (HABdg)\ndesigned for the feature encoding process, which learns the high-frequency\ninformation of the MDE task to guide the DSR task. The other is the content\nguidance bridge (CGBdg) designed for the depth map reconstruction process,\nwhich provides the content guidance learned from DSR task for MDE task. The\nentire network architecture is highly portable and can provide a paradigm for\nassociating the DSR and MDE tasks. Extensive experiments on benchmark datasets\ndemonstrate that our method achieves competitive performance. Our code and\nmodels are available at https://rmcong.github.io/proj_BridgeNet.html.",
          "link": "http://arxiv.org/abs/2107.12541",
          "publishedOn": "2021-07-28T02:02:31.295Z",
          "wordCount": 682,
          "title": "BridgeNet: A Joint Learning Network of Depth Map Super-Resolution and Monocular Depth Estimation. (arXiv:2107.12541v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Song Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrich_N/0/1/0/all/0/1\">Norman Hendrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Fanyu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shuzhi Sam Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>",
          "description": "In the classic setting of unsupervised domain adaptation (UDA), the labeled\nsource data are available in the training phase. However, in many real-world\nscenarios, owing to some reasons such as privacy protection and information\nsecurity, the source data is inaccessible, and only a model trained on the\nsource domain is available. This paper proposes a novel deep clustering method\nfor this challenging task. Aiming at the dynamical clustering at feature-level,\nwe introduce extra constraints hidden in the geometric structure between data\nto assist the process. Concretely, we propose a geometry-based constraint,\nnamed semantic consistency on the nearest neighborhood (SCNNH), and use it to\nencourage robust clustering. To reach this goal, we construct the nearest\nneighborhood for every target data and take it as the fundamental clustering\nunit by building our objective on the geometry. Also, we develop a more\nSCNNH-compliant structure with an additional semantic credibility constraint,\nnamed semantic hyper-nearest neighborhood (SHNNH). After that, we extend our\nmethod to this new geometry. Extensive experiments on three challenging UDA\ndatasets indicate that our method achieves state-of-the-art results. The\nproposed method has significant improvement on all datasets (as we adopt SHNNH,\nthe average accuracy increases by over 3.0\\% on the large-scaled dataset). Code\nis available at https://github.com/tntek/N2DCX.",
          "link": "http://arxiv.org/abs/2107.12585",
          "publishedOn": "2021-07-28T02:02:31.288Z",
          "wordCount": 660,
          "title": "Nearest Neighborhood-Based Deep Clustering for Source Data-absent Unsupervised Domain Adaptation. (arXiv:2107.12585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junnan Liu</a>",
          "description": "Effective fusion of different types of features is the key to salient object\ndetection. The majority of existing network structure design is based on the\nsubjective experience of scholars and the process of feature fusion does not\nconsider the relationship between the fused features and highest-level\nfeatures. In this paper, we focus on the feature relationship and propose a\nnovel global attention unit, which we term the \"perception- and-regulation\"\n(PR) block, that adaptively regulates the feature fusion process by explicitly\nmodeling interdependencies between features. The perception part uses the\nstructure of fully-connected layers in classification networks to learn the\nsize and shape of objects. The regulation part selectively strengthens and\nweakens the features to be fused. An imitating eye observation module (IEO) is\nfurther employed for improving the global perception ability of the network.\nThe imitation of foveal vision and peripheral vision enables IEO to scrutinize\nhighly detailed objects and to organize the broad spatial scene to better\nsegment objects. Sufficient experiments conducted on SOD datasets demonstrate\nthat the proposed method performs favorably against 22 state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.12560",
          "publishedOn": "2021-07-28T02:02:31.238Z",
          "wordCount": 609,
          "title": "Perception-and-Regulation Network for Salient Object Detection. (arXiv:2107.12560v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santamaria_Pang_A/0/1/0/all/0/1\">Alberto Santamaria-Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianwei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Aritra Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubricht_J/0/1/0/all/0/1\">James Kubricht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1\">Peter Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naresh_I/0/1/0/all/0/1\">Iyer Naresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virani_N/0/1/0/all/0/1\">Nurali Virani</a>",
          "description": "We propose a novel framework for real-time black-box universal attacks which\ndisrupts activations of early convolutional layers in deep learning models. Our\nhypothesis is that perturbations produced in the wavelet space disrupt early\nconvolutional layers more effectively than perturbations performed in the time\ndomain. The main challenge in adversarial attacks is to preserve low frequency\nimage content while minimally changing the most meaningful high frequency\ncontent. To address this, we formulate an optimization problem using time-scale\n(wavelet) representations as a dual space in three steps. First, we project\noriginal images into orthonormal sub-spaces for low and high scales via wavelet\ncoefficients. Second, we perturb wavelet coefficients for high scale projection\nusing a generator network. Third, we generate new adversarial images by\nprojecting back the original coefficients from the low scale and the perturbed\ncoefficients from the high scale sub-space. We provide a theoretical framework\nthat guarantees a dual mapping from time and time-scale domain representations.\nWe compare our results with state-of-the-art black-box attacks from\ngenerative-based and gradient-based models. We also verify efficacy against\nmultiple defense methods such as JPEG compression, Guided Denoiser and\nComdefend. Our results show that wavelet-based perturbations consistently\noutperform time-based attacks thus providing new insights into vulnerabilities\nof deep learning models and could potentially lead to robust architectures or\nnew defense and attack mechanisms by leveraging time-scale representations.",
          "link": "http://arxiv.org/abs/2107.12473",
          "publishedOn": "2021-07-28T02:02:31.230Z",
          "wordCount": 665,
          "title": "Adversarial Attacks with Time-Scale Representations. (arXiv:2107.12473v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>",
          "description": "Self-supervised depth estimation for indoor environments is more challenging\nthan its outdoor counterpart in at least the following two aspects: (i) the\ndepth range of indoor sequences varies a lot across different frames, making it\ndifficult for the depth network to induce consistent depth cues, whereas the\nmaximum distance in outdoor scenes mostly stays the same as the camera usually\nsees the sky; (ii) the indoor sequences contain much more rotational motions,\nwhich cause difficulties for the pose network, while the motions of outdoor\nsequences are pre-dominantly translational, especially for driving datasets\nsuch as KITTI. In this paper, special considerations are given to those\nchallenges and a set of good practices are consolidated for improving the\nperformance of self-supervised monocular depth estimation in indoor\nenvironments. The proposed method mainly consists of two novel modules, \\ie, a\ndepth factorization module and a residual pose estimation module, each of which\nis designed to respectively tackle the aforementioned challenges. The\neffectiveness of each module is shown through a carefully conducted ablation\nstudy and the demonstration of the state-of-the-art performance on two indoor\ndatasets, \\ie, EuRoC and NYUv2.",
          "link": "http://arxiv.org/abs/2107.12429",
          "publishedOn": "2021-07-28T02:02:31.221Z",
          "wordCount": 632,
          "title": "MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments. (arXiv:2107.12429v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1\">Bo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>",
          "description": "We propose a self-supervised spatio-temporal matching method coined\nMotion-Aware Mask Propagation (MAMP) for semi-supervised video object\nsegmentation. During training, MAMP leverages the frame reconstruction task to\ntrain the model without the need for annotations. During inference, MAMP\nextracts high-resolution features from each frame to build a memory bank from\nthe features as well as the predicted masks of selected past frames. MAMP then\npropagates the masks from the memory bank to subsequent frames according to our\nmotion-aware spatio-temporal matching module, also proposed in this paper.\nEvaluation on DAVIS-2017 and YouTube-VOS datasets show that MAMP achieves\nstate-of-the-art performance with stronger generalization ability compared to\nexisting self-supervised methods, i.e. 4.9\\% higher mean\n$\\mathcal{J}\\&\\mathcal{F}$ on DAVIS-2017 and 4.85\\% higher mean\n$\\mathcal{J}\\&\\mathcal{F}$ on the unseen categories of YouTube-VOS than the\nnearest competitor. Moreover, MAMP performs on par with many supervised video\nobject segmentation methods. Our code is available at:\n\\url{https://github.com/bo-miao/MAMP}.",
          "link": "http://arxiv.org/abs/2107.12569",
          "publishedOn": "2021-07-28T02:02:31.204Z",
          "wordCount": 588,
          "title": "Self-Supervised Video Object Segmentation by Motion-Aware Mask Propagation. (arXiv:2107.12569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azari_B/0/1/0/all/0/1\">Bahar Azari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>",
          "description": "Despite the vast success of standard planar convolutional neural networks,\nthey are not the most efficient choice for analyzing signals that lie on an\narbitrarily curved manifold, such as a cylinder. The problem arises when one\nperforms a planar projection of these signals and inevitably causes them to be\ndistorted or broken where there is valuable information. We propose a\nCircular-symmetric Correlation Layer (CCL) based on the formalism of\nroto-translation equivariant correlation on the continuous group $S^1 \\times\n\\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier\nTransform (FFT) algorithm. We showcase the performance analysis of a general\nnetwork equipped with CCL on various recognition and classification tasks and\ndatasets. The PyTorch package implementation of CCL is provided online.",
          "link": "http://arxiv.org/abs/2107.12480",
          "publishedOn": "2021-07-28T02:02:30.923Z",
          "wordCount": 552,
          "title": "Circular-Symmetric Correlation Layer based on FFT. (arXiv:2107.12480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.",
          "link": "http://arxiv.org/abs/2107.12514",
          "publishedOn": "2021-07-28T02:02:30.915Z",
          "wordCount": 709,
          "title": "Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Haisheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_P/0/1/0/all/0/1\">Peiqin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yukun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "This technical report presents an overview of our solution used in the\nsubmission to 2021 HACS Temporal Action Localization Challenge on both\nSupervised Learning Track and Weakly-Supervised Learning Track. Temporal Action\nLocalization (TAL) requires to not only precisely locate the temporal\nboundaries of action instances, but also accurately classify the untrimmed\nvideos into specific categories. However, Weakly-Supervised TAL indicates\nlocating the action instances using only video-level class labels. In this\npaper, to train a supervised temporal action localizer, we adopt Temporal\nContext Aggregation Network (TCANet) to generate high-quality action proposals\nthrough ``local and global\" temporal context aggregation and complementary as\nwell as progressive boundary refinement. As for the WSTAL, a novel framework is\nproposed to handle the poor quality of CAS generated by simple classification\nnetwork, which can only focus on local discriminative parts, rather than locate\nthe entire interval of target actions. Further inspired by the transfer\nlearning method, we also adopt an additional module to transfer the knowledge\nfrom trimmed videos (HACS Clips dataset) to untrimmed videos (HACS Segments\ndataset), aiming at promoting the classification performance on untrimmed\nvideos. Finally, we employ a boundary regression module embedded with\nOuter-Inner-Contrastive (OIC) loss to automatically predict the boundaries\nbased on the enhanced CAS. Our proposed scheme achieves 39.91 and 29.78 average\nmAP on the challenge testing set of supervised and weakly-supervised temporal\naction localization track respectively.",
          "link": "http://arxiv.org/abs/2107.12618",
          "publishedOn": "2021-07-28T02:02:30.805Z",
          "wordCount": 699,
          "title": "Transferable Knowledge-Based Multi-Granularity Aggregation Network for Temporal Action Localization: Submission to ActivityNet Challenge 2021. (arXiv:2107.12618v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yilin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "6D pose estimation of rigid objects from a single RGB image has seen\ntremendous improvements recently by using deep learning to combat complex\nreal-world variations, but a majority of methods build models on the per-object\nlevel, failing to scale to multiple objects simultaneously. In this paper, we\npresent a novel approach for scalable 6D pose estimation, by self-supervised\nlearning on synthetic data of multiple objects using a single autoencoder. To\nhandle multiple objects and generalize to unseen objects, we disentangle the\nlatent object shape and pose representations, so that the latent shape space\nmodels shape similarities, and the latent pose code is used for rotation\nretrieval by comparison with canonical rotations. To encourage shape space\nconstruction, we apply contrastive metric learning and enable the processing of\nunseen objects by referring to similar training objects. The different\nsymmetries across objects induce inconsistent latent pose spaces, which we\ncapture with a conditioned block producing shape-dependent pose codebooks by\nre-entangling shape and pose representations. We test our method on two\nmulti-object benchmarks with real data, T-LESS and NOCS REAL275, and show it\noutperforms existing RGB-based methods in terms of pose estimation accuracy and\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.12549",
          "publishedOn": "2021-07-28T02:02:30.796Z",
          "wordCount": 639,
          "title": "Disentangled Implicit Shape and Pose Learning for Scalable 6D Pose Estimation. (arXiv:2107.12549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Miao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Siyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>",
          "description": "Advanced tensor decomposition, such as Tensor train (TT) and Tensor ring\n(TR), has been widely studied for deep neural network (DNN) model compression,\nespecially for recurrent neural networks (RNNs). However, compressing\nconvolutional neural networks (CNNs) using TT/TR always suffers significant\naccuracy loss. In this paper, we propose a systematic framework for tensor\ndecomposition-based model compression using Alternating Direction Method of\nMultipliers (ADMM). By formulating TT decomposition-based model compression to\nan optimization problem with constraints on tensor ranks, we leverage ADMM\ntechnique to systemically solve this optimization problem in an iterative way.\nDuring this procedure, the entire DNN model is trained in the original\nstructure instead of TT format, but gradually enjoys the desired low tensor\nrank characteristics. We then decompose this uncompressed model to TT format\nand fine-tune it to finally obtain a high-accuracy TT-format DNN model. Our\nframework is very general, and it works for both CNNs and RNNs, and can be\neasily modified to fit other tensor decomposition approaches. We evaluate our\nproposed framework on different DNN models for image classification and video\nrecognition tasks. Experimental results show that our ADMM-based TT-format\nmodels demonstrate very high compression performance with high accuracy.\nNotably, on CIFAR-100, with 2.3X and 2.4X compression ratios, our models have\n1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and\nResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model\nachieves 2.47X FLOPs reduction without accuracy loss.",
          "link": "http://arxiv.org/abs/2107.12422",
          "publishedOn": "2021-07-28T02:02:30.772Z",
          "wordCount": 682,
          "title": "Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework. (arXiv:2107.12422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_D/0/1/0/all/0/1\">Daniil Pakhomov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hira_S/0/1/0/all/0/1\">Sanchit Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagle_N/0/1/0/all/0/1\">Narayani Wagle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_K/0/1/0/all/0/1\">Kemar E. Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>",
          "description": "We introduce a method that allows to automatically segment images into\nsemantically meaningful regions without human supervision. Derived regions are\nconsistent across different images and coincide with human-defined semantic\nclasses on some datasets. In cases where semantic regions might be hard for\nhuman to define and consistently label, our method is still able to find\nmeaningful and consistent semantic classes. In our work, we use pretrained\nStyleGAN2~\\cite{karras2020analyzing} generative model: clustering in the\nfeature space of the generative model allows to discover semantic classes. Once\nclasses are discovered, a synthetic dataset with generated images and\ncorresponding segmentation masks can be created. After that a segmentation\nmodel is trained on the synthetic dataset and is able to generalize to real\nimages. Additionally, by using CLIP~\\cite{radford2021learning} we are able to\nuse prompts defined in a natural language to discover some desired semantic\nclasses. We test our method on publicly available datasets and show\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2107.12518",
          "publishedOn": "2021-07-28T02:02:30.749Z",
          "wordCount": 597,
          "title": "Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP. (arXiv:2107.12518v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fa-Ting Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jia-Chang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "Weakly supervised temporal action localization (WS-TAL) is a challenging task\nthat aims to localize action instances in the given video with video-level\ncategorical supervision. Both appearance and motion features are used in\nprevious works, while they do not utilize them in a proper way but apply simple\nconcatenation or score-level fusion. In this work, we argue that the features\nextracted from the pretrained extractor, e.g., I3D, are not the\nWS-TALtask-specific features, thus the feature re-calibration is needed for\nreducing the task-irrelevant information redundancy. Therefore, we propose a\ncross-modal consensus network (CO2-Net) to tackle this problem. In CO2-Net, we\nmainly introduce two identical proposed cross-modal consensus modules (CCM)\nthat design a cross-modal attention mechanism to filter out the task-irrelevant\ninformation redundancy using the global information from the main modality and\nthe cross-modal local information of the auxiliary modality. Moreover, we treat\nthe attention weights derived from each CCMas the pseudo targets of the\nattention weights derived from another CCM to maintain the consistency between\nthe predictions derived from two CCMs, forming a mutual learning manner.\nFinally, we conduct extensive experiments on two common used temporal action\nlocalization datasets, THUMOS14 and ActivityNet1.2, to verify our method and\nachieve the state-of-the-art results. The experimental results show that our\nproposed cross-modal consensus module can produce more representative features\nfor temporal action localization.",
          "link": "http://arxiv.org/abs/2107.12589",
          "publishedOn": "2021-07-28T02:02:30.724Z",
          "wordCount": 665,
          "title": "Cross-modal Consensus Network for Weakly Supervised Temporal Action Localization. (arXiv:2107.12589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaotian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>",
          "description": "There is a surge of interest in image scene graph generation (object,\nattribute and relationship detection) due to the need of building fine-grained\nimage understanding models that go beyond object detection. Due to the lack of\na good benchmark, the reported results of different scene graph generation\nmodels are not directly comparable, impeding the research progress. We have\ndeveloped a much-needed scene graph generation benchmark based on the\nmaskrcnn-benchmark and several popular models. This paper presents main\nfeatures of our benchmark and a comprehensive ablation study of scene graph\ngeneration models using the Visual Genome and OpenImages Visual relationship\ndetection datasets. Our codebase is made publicly available at\nhttps://github.com/microsoft/scene_graph_benchmark.",
          "link": "http://arxiv.org/abs/2107.12604",
          "publishedOn": "2021-07-28T02:02:30.684Z",
          "wordCount": 546,
          "title": "Image Scene Graph Generation (SGG) Benchmark. (arXiv:2107.12604v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smedsrud_P/0/1/0/all/0/1\">Pia H. Smedsrud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_D/0/1/0/all/0/1\">Dag Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_H/0/1/0/all/0/1\">H&#xe5;vard D. Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Colonoscopy is considered the gold standard for detection of colorectal\ncancer and its precursors. Existing examination methods are, however, hampered\nby high overall miss-rate, and many abnormalities are left undetected.\nComputer-Aided Diagnosis systems based on advanced machine learning algorithms\nare touted as a game-changer that can identify regions in the colon overlooked\nby the physicians during endoscopic examinations, and help detect and\ncharacterize lesions. In previous work, we have proposed the ResUNet++\narchitecture and demonstrated that it produces more efficient results compared\nwith its counterparts U-Net and ResUNet. In this paper, we demonstrate that\nfurther improvements to the overall prediction performance of the ResUNet++\narchitecture can be achieved by using conditional random field and test-time\naugmentation. We have performed extensive evaluations and validated the\nimprovements using six publicly available datasets: Kvasir-SEG, CVC-ClinicDB,\nCVC-ColonDB, ETIS-Larib Polyp DB, ASU-Mayo Clinic Colonoscopy Video Database,\nand CVC-VideoClinicDB. Moreover, we compare our proposed architecture and\nresulting model with other State-of-the-art methods. To explore the\ngeneralization capability of ResUNet++ on different publicly available polyp\ndatasets, so that it could be used in a real-world setting, we performed an\nextensive cross-dataset evaluation. The experimental results show that applying\nCRF and TTA improves the performance on various polyp segmentation datasets\nboth on the same dataset and cross-dataset.",
          "link": "http://arxiv.org/abs/2107.12435",
          "publishedOn": "2021-07-28T02:02:30.676Z",
          "wordCount": 678,
          "title": "A Comprehensive Study on Colorectal Polyp Segmentation with ResUNet++, Conditional Random Field and Test-Time Augmentation. (arXiv:2107.12435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravirathinam_P/0/1/0/all/0/1\">Praveen Ravirathinam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Ankush Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulla_D/0/1/0/all/0/1\">David Mulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "Mapping and monitoring crops is a key step towards sustainable\nintensification of agriculture and addressing global food security. A dataset\nlike ImageNet that revolutionized computer vision applications can accelerate\ndevelopment of novel crop mapping techniques. Currently, the United States\nDepartment of Agriculture (USDA) annually releases the Cropland Data Layer\n(CDL) which contains crop labels at 30m resolution for the entire United States\nof America. While CDL is state of the art and is widely used for a number of\nagricultural applications, it has a number of limitations (e.g., pixelated\nerrors, labels carried over from previous errors and absence of input imagery\nalong with class labels). In this work, we create a new semantic segmentation\nbenchmark dataset, which we call CalCROP21, for the diverse crops in the\nCentral Valley region of California at 10m spatial resolution using a Google\nEarth Engine based robust image processing pipeline and a novel attention based\nspatio-temporal semantic segmentation algorithm STATT. STATT uses re-sampled\n(interpolated) CDL labels for training, but is able to generate a better\nprediction than CDL by leveraging spatial and temporal patterns in Sentinel2\nmulti-spectral image series to effectively capture phenologic differences\namongst crops and uses attention to reduce the impact of clouds and other\natmospheric disturbances. We also present a comprehensive evaluation to show\nthat STATT has significantly better results when compared to the resampled CDL\nlabels. We have released the dataset and the processing pipeline code for\ngenerating the benchmark dataset.",
          "link": "http://arxiv.org/abs/2107.12499",
          "publishedOn": "2021-07-28T02:02:30.668Z",
          "wordCount": 690,
          "title": "CalCROP21: A Georeferenced multi-spectral dataset of Satellite Imagery and Crop Labels. (arXiv:2107.12499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thoreau_M/0/1/0/all/0/1\">Michael Thoreau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_F/0/1/0/all/0/1\">Frazer Wilson</a>",
          "description": "Access to high resolution satellite imagery has dramatically increased in\nrecent years as several new constellations have entered service. High revisit\nfrequencies as well as improved resolution has widened the use cases of\nsatellite imagery to areas such as humanitarian relief and even Search and\nRescue (SaR). We propose a novel remote sensing object detection dataset for\ndeep learning assisted SaR. This dataset contains only small objects that have\nbeen identified as potential targets as part of a live SaR response. We\nevaluate the application of popular object detection models to this dataset as\na baseline to inform further research. We also propose a novel object detection\nmetric, specifically designed to be used in a deep learning assisted SaR\nsetting.",
          "link": "http://arxiv.org/abs/2107.12469",
          "publishedOn": "2021-07-28T02:02:30.657Z",
          "wordCount": 571,
          "title": "SaRNet: A Dataset for Deep Learning Assisted Search and Rescue with Satellite Imagery. (arXiv:2107.12469v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1\">Vivien Sainte Fare Garnot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>",
          "description": "Unprecedented access to multi-temporal satellite imagery has opened new\nperspectives for a variety of Earth observation tasks. Among them,\npixel-precise panoptic segmentation of agricultural parcels has major economic\nand environmental implications. While researchers have explored this problem\nfor single images, we argue that the complex temporal patterns of crop\nphenology are better addressed with temporal sequences of images. In this\npaper, we present the first end-to-end, single-stage method for panoptic\nsegmentation of Satellite Image Time Series (SITS). This module can be combined\nwith our novel image sequence encoding network which relies on temporal\nself-attention to extract rich and adaptive multi-scale spatio-temporal\nfeatures. We also introduce PASTIS, the first open-access SITS dataset with\npanoptic annotations. We demonstrate the superiority of our encoder for\nsemantic segmentation against multiple competing architectures, and set up the\nfirst state-of-the-art of panoptic segmentation of SITS. Our implementation and\nPASTIS are publicly available.",
          "link": "http://arxiv.org/abs/2107.07933",
          "publishedOn": "2021-07-27T02:03:39.359Z",
          "wordCount": null,
          "title": "Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meltzer_P/0/1/0/all/0/1\">Peter Meltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1\">Hooman Shayani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1\">Amir Khasahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1\">Pradeep Kumar Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph Lambourne</a>",
          "description": "Boundary Representations (B-Reps) are the industry standard in 3D Computer\nAided Design/Manufacturing (CAD/CAM) and industrial design due to their\nfidelity in representing stylistic details. However, they have been ignored in\nthe 3D style research. Existing 3D style metrics typically operate on meshes or\npointclouds, and fail to account for end-user subjectivity by adopting fixed\ndefinitions of style, either through crowd-sourcing for style labels or\nhand-crafted features. We propose UVStyle-Net, a style similarity measure for\nB-Reps that leverages the style signals in the second order statistics of the\nactivations in a pre-trained (unsupervised) 3D encoder, and learns their\nrelative importance to a subjective end-user through few-shot learning. Our\napproach differs from all existing data-driven 3D style methods since it may be\nused in completely unsupervised settings, which is desirable given the lack of\npublicly available labelled B-Rep datasets. More importantly, the few-shot\nlearning accounts for the inherent subjectivity associated with style. We show\nquantitatively that our proposed method with B-Reps is able to capture stronger\nstyle signals than alternative methods on meshes and pointclouds despite its\nsignificantly greater computational efficiency. We also show it is able to\ngenerate meaningful style gradients with respect to the input shape, and that\nfew-shot learning with as few as two positive examples selected by an end-user\nis sufficient to significantly improve the style measure. Finally, we\ndemonstrate its efficacy on a large unlabeled public dataset of CAD models.\nSource code and data will be released in the future.",
          "link": "http://arxiv.org/abs/2105.02961",
          "publishedOn": "2021-07-27T02:03:39.347Z",
          "wordCount": null,
          "title": "UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps. (arXiv:2105.02961v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengbo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiaqi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>",
          "description": "The key challenge of image manipulation detection is how to learn\ngeneralizable features that are sensitive to manipulations in novel data,\nwhilst specific to prevent false alarms on authentic images. Current research\nemphasizes the sensitivity, with the specificity overlooked. In this paper we\naddress both aspects by multi-view feature learning and multi-scale\nsupervision. By exploiting noise distribution and boundary artifact surrounding\ntampered regions, the former aims to learn semantic-agnostic and thus more\ngeneralizable features. The latter allows us to learn from authentic images\nwhich are nontrivial to be taken into account by current semantic segmentation\nnetwork based methods. Our thoughts are realized by a new network which we term\nMVSS-Net. Extensive experiments on five benchmark sets justify the viability of\nMVSS-Net for both pixel-level and image-level manipulation detection.",
          "link": "http://arxiv.org/abs/2104.06832",
          "publishedOn": "2021-07-27T02:03:39.344Z",
          "wordCount": null,
          "title": "Image Manipulation Detection by Multi-View Multi-Scale Supervision. (arXiv:2104.06832v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-07-27T02:03:39.342Z",
          "wordCount": null,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhisheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "In this paper, we propose Parametric Contrastive Learning (PaCo) to tackle\nlong-tailed recognition. Based on theoretical analysis, we observe supervised\ncontrastive loss tends to bias on high-frequency classes and thus increases the\ndifficulty of imbalance learning. We introduce a set of parametric class-wise\nlearnable centers to rebalance from an optimization perspective. Further, we\nanalyze our PaCo loss under a balanced setting. Our analysis demonstrates that\nPaCo can adaptively enhance the intensity of pushing samples of the same class\nclose as more samples are pulled together with their corresponding centers and\nbenefit hard example learning. Experiments on long-tailed CIFAR, ImageNet,\nPlaces, and iNaturalist 2018 manifest the new state-of-the-art for long-tailed\nrecognition. On full ImageNet, models trained with PaCo loss surpass supervised\ncontrastive learning across various ResNet backbones. Our code is available at\n\\url{https://github.com/jiequancui/Parametric-Contrastive-Learning}.",
          "link": "http://arxiv.org/abs/2107.12028",
          "publishedOn": "2021-07-27T02:03:36.138Z",
          "wordCount": 564,
          "title": "Parametric Contrastive Learning. (arXiv:2107.12028v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yue Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prugel_Bennett_A/0/1/0/all/0/1\">Adam Pr&#xfc;gel-Bennett</a>",
          "description": "Visual Semantic Embedding (VSE) models, which map images into a rich semantic\nembedding space, have been a milestone in object recognition and zero-shot\nlearning. Current approaches to VSE heavily rely on static word em-bedding\ntechniques. In this work, we propose a Visual Se-mantic Embedding Probe (VSEP)\ndesigned to probe the semantic information of contextualized word embeddings in\nvisual semantic understanding tasks. We show that the knowledge encoded in\ntransformer language models can be exploited for tasks requiring visual\nsemantic understanding.The VSEP with contextual representations can distinguish\nword-level object representations in complicated scenes as a compositional\nzero-shot learner. We further introduce a zero-shot setting with VSEPs to\nevaluate a model's ability to associate a novel word with a novel visual\ncategory. We find that contextual representations in language mod-els\noutperform static word embeddings, when the compositional chain of object is\nshort. We notice that current visual semantic embedding models lack a mutual\nexclusivity bias which limits their performance.",
          "link": "http://arxiv.org/abs/2107.12021",
          "publishedOn": "2021-07-27T02:03:35.877Z",
          "wordCount": 588,
          "title": "Language Models as Zero-shot Visual Semantic Learners. (arXiv:2107.12021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping-Chia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively.",
          "link": "http://arxiv.org/abs/2107.11769",
          "publishedOn": "2021-07-27T02:03:35.792Z",
          "wordCount": 608,
          "title": "ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingjing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1\">Zhixiong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>",
          "description": "Encouraging progress has been made towards Visual Question Answering (VQA) in\nrecent years, but it is still challenging to enable VQA models to adaptively\ngeneralize to out-of-distribution (OOD) samples. Intuitively, recompositions of\nexisting visual concepts (i.e., attributes and objects) can generate unseen\ncompositions in the training set, which will promote VQA models to generalize\nto OOD samples. In this paper, we formulate OOD generalization in VQA as a\ncompositional generalization problem and propose a graph generative\nmodeling-based training scheme (X-GGM) to handle the problem implicitly. X-GGM\nleverages graph generative modeling to iteratively generate a relation matrix\nand node representations for the predefined graph that utilizes\nattribute-object pairs as nodes. Furthermore, to alleviate the unstable\ntraining issue in graph generative modeling, we propose a gradient distribution\nconsistency loss to constrain the data distribution with adversarial\nperturbations and the generated distribution. The baseline VQA model (LXMERT)\ntrained with the X-GGM scheme achieves state-of-the-art OOD performance on two\nstandard VQA OOD benchmarks, i.e., VQA-CP v2 and GQA-OOD. Extensive ablation\nstudies demonstrate the effectiveness of X-GGM components.",
          "link": "http://arxiv.org/abs/2107.11576",
          "publishedOn": "2021-07-27T02:03:35.568Z",
          "wordCount": 624,
          "title": "X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering. (arXiv:2107.11576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fang-Yi Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Exploring to what humans pay attention in dynamic panoramic scenes is useful\nfor many fundamental applications, including augmented reality (AR) in retail,\nAR-powered recruitment, and visual language navigation. With this goal in mind,\nwe propose PV-SOD, a new task that aims to segment salient objects from\npanoramic videos. In contrast to existing fixation-level or object-level\nsaliency detection tasks, we focus on multi-modal salient object detection\n(SOD), which mimics human attention mechanism by segmenting salient objects\nwith the guidance of audio-visual cues. To support this task, we collect the\nfirst large-scale dataset, named ASOD60K, which contains 4K-resolution video\nframes annotated with a six-level hierarchy, thus distinguishing itself with\nrichness, diversity and quality. Specifically, each sequence is marked with\nboth its super-/sub-class, with objects of each sub-class being further\nannotated with human eye fixations, bounding boxes, object-/instance-level\nmasks, and associated attributes (e.g., geometrical distortion). These\ncoarse-to-fine annotations enable detailed analysis for PV-SOD modeling, e.g.,\ndetermining the major challenges for existing SOD models, and predicting\nscanpaths to study the long-term eye fixation behaviors of humans. We\nsystematically benchmark 11 representative approaches on ASOD60K and derive\nseveral interesting findings. We hope this study could serve as a good starting\npoint for advancing SOD research towards panoramic videos.",
          "link": "http://arxiv.org/abs/2107.11629",
          "publishedOn": "2021-07-27T02:03:35.422Z",
          "wordCount": 654,
          "title": "ASOD60K: Audio-Induced Salient Object Detection in Panoramic Videos. (arXiv:2107.11629v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zi-Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liang-Jian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tai-Xiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian-Jing Zhang</a>",
          "description": "The convolution operation is a powerful tool for feature extraction and plays\na prominent role in the field of computer vision. However, when targeting the\npixel-wise tasks like image fusion, it would not fully perceive the\nparticularity of each pixel in the image if the uniform convolution kernel is\nused on different patches. In this paper, we propose a local adaptive\nconvolution (LAConv), which is dynamically adjusted to different spatial\nlocations. LAConv enables the network to pay attention to every specific local\narea in the learning process. Besides, the dynamic bias (DYB) is introduced to\nprovide more possibilities for the depiction of features and make the network\nmore flexible. We further design a residual structure network equipped with the\nproposed LAConv and DYB modules, and apply it to two image fusion tasks.\nExperiments for pansharpening and hyperspectral image super-resolution (HISR)\ndemonstrate the superiority of our method over other state-of-the-art methods.\nIt is worth mentioning that LAConv can also be competent for other\nsuper-resolution tasks with less computation effort.",
          "link": "http://arxiv.org/abs/2107.11617",
          "publishedOn": "2021-07-27T02:03:35.375Z",
          "wordCount": 608,
          "title": "LAConv: Local Adaptive Convolution for Image Fusion. (arXiv:2107.11617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Weizhi An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hehuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>",
          "description": "Recent studies imply that deep neural networks are vulnerable to adversarial\nexamples -- inputs with a slight but intentional perturbation are incorrectly\nclassified by the network. Such vulnerability makes it risky for some\nsecurity-related applications (e.g., semantic segmentation in autonomous cars)\nand triggers tremendous concerns on the model reliability. For the first time,\nwe comprehensively evaluate the robustness of existing UDA methods and propose\na robust UDA approach. It is rooted in two observations: (i) the robustness of\nUDA methods in semantic segmentation remains unexplored, which pose a security\nconcern in this field; and (ii) although commonly used self-supervision (e.g.,\nrotation and jigsaw) benefits image tasks such as classification and\nrecognition, they fail to provide the critical supervision signals that could\nlearn discriminative representation for segmentation tasks. These observations\nmotivate us to propose adversarial self-supervision UDA (or ASSUDA) that\nmaximizes the agreement between clean images and their adversarial examples by\na contrastive loss in the output space. Extensive empirical studies on commonly\nused benchmarks demonstrate that ASSUDA is resistant to adversarial attacks.",
          "link": "http://arxiv.org/abs/2105.10843",
          "publishedOn": "2021-07-27T02:03:35.246Z",
          "wordCount": 652,
          "title": "Exploring Robustness of Unsupervised Domain Adaptation in Semantic Segmentation. (arXiv:2105.10843v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1\">Larisa Markeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1\">Ilya Tolstikhin</a>",
          "description": "We introduce a generalization to the lottery ticket hypothesis in which the\nnotion of \"sparsity\" is relaxed by choosing an arbitrary basis in the space of\nparameters. We present evidence that the original results reported for the\ncanonical basis continue to hold in this broader setting. We describe how\nstructured pruning methods, including pruning units or factorizing\nfully-connected layers into products of low-rank matrices, can be cast as\nparticular instances of this \"generalized\" lottery ticket hypothesis. The\ninvestigations reported here are preliminary and are provided to encourage\nfurther research along this direction.",
          "link": "http://arxiv.org/abs/2107.06825",
          "publishedOn": "2021-07-27T02:03:35.213Z",
          "wordCount": 571,
          "title": "A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-27T02:03:35.207Z",
          "wordCount": 647,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02771",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kazemimoghadam_M/0/1/0/all/0/1\">Mahdieh Kazemimoghadam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chi_W/0/1/0/all/0/1\">Weicheng Chi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahimi_A/0/1/0/all/0/1\">Asal Rahimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_N/0/1/0/all/0/1\">Nathan Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alluri_P/0/1/0/all/0/1\">Prasanna Alluri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nwachukwu_C/0/1/0/all/0/1\">Chika Nwachukwu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1\">Weiguo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_X/0/1/0/all/0/1\">Xuejun Gu</a>",
          "description": "Efficient, reliable and reproducible target volume delineation is a key step\nin the effective planning of breast radiotherapy. However, post-operative\nbreast target delineation is challenging as the contrast between the tumor bed\nvolume (TBV) and normal breast tissue is relatively low in CT images. In this\nstudy, we propose to mimic the marker-guidance procedure in manual target\ndelineation. We developed a saliency-based deep learning segmentation (SDL-Seg)\nalgorithm for accurate TBV segmentation in post-operative breast irradiation.\nThe SDL-Seg algorithm incorporates saliency information in the form of markers'\nlocation cues into a U-Net model. The design forces the model to encode the\nlocation-related features, which underscores regions with high saliency levels\nand suppresses low saliency regions. The saliency maps were generated by\nidentifying markers on CT images. Markers' locations were then converted to\nprobability maps using a distance-transformation coupled with a Gaussian\nfilter. Subsequently, the CT images and the corresponding saliency maps formed\na multi-channel input for the SDL-Seg network. Our in-house dataset was\ncomprised of 145 prone CT images from 29 post-operative breast cancer patients,\nwho received 5-fraction partial breast irradiation (PBI) regimen on GammaPod.\nThe performance of the proposed method was compared against basic U-Net. Our\nmodel achieved mean (standard deviation) of 76.4 %, 6.76 mm, and 1.9 mm for\nDSC, HD95, and ASD respectively on the test set with computation time of below\n11 seconds per one CT volume. SDL-Seg showed superior performance relative to\nbasic U-Net for all the evaluation metrics while preserving low computation\ncost. The findings demonstrate that SDL-Seg is a promising approach for\nimproving the efficiency and accuracy of the on-line treatment planning\nprocedure of PBI, such as GammaPod based PBI.",
          "link": "http://arxiv.org/abs/2105.02771",
          "publishedOn": "2021-07-27T02:03:35.200Z",
          "wordCount": 770,
          "title": "Saliency-Guided Deep Learning Network for Automatic Tumor Bed Volume Delineation in Post-operative Breast Irradiation. (arXiv:2105.02771v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Mengyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1\">Haibin Hang</a>",
          "description": "We propose a manifold matching approach to generative models which includes a\ndistribution generator (or data generator) and a metric generator. In our\nframework, we view the real data set as some manifold embedded in a\nhigh-dimensional Euclidean space. The distribution generator aims at generating\nsamples that follow some distribution condensed around the real data manifold.\nIt is achieved by matching two sets of points using their geometric shape\ndescriptors, such as centroid and $p$-diameter, with learned distance metric;\nthe metric generator utilizes both real data and generated samples to learn a\ndistance metric which is close to some intrinsic geodesic distance on the real\ndata manifold. The produced distance metric is further used for manifold\nmatching. The two networks are learned simultaneously during the training\nprocess. We apply the approach on both unsupervised and supervised learning\ntasks: in unconditional image generation task, the proposed method obtains\ncompetitive results compared with existing generative models; in\nsuper-resolution task, we incorporate the framework in perception-based models\nand improve visual qualities by producing samples with more natural textures.\nBoth theoretical analysis and real data experiments demonstrate the feasibility\nand effectiveness of the proposed framework.",
          "link": "http://arxiv.org/abs/2106.10777",
          "publishedOn": "2021-07-27T02:03:35.176Z",
          "wordCount": 655,
          "title": "Manifold Matching via Deep Metric Learning for Generative Modeling. (arXiv:2106.10777v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-27T02:03:35.169Z",
          "wordCount": 681,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tata_G/0/1/0/all/0/1\">Gautam Tata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royer_S/0/1/0/all/0/1\">Sarah-Jeanne Royer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poirion_O/0/1/0/all/0/1\">Olivier Poirion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_J/0/1/0/all/0/1\">Jay Lowe</a>",
          "description": "The quantification of positively buoyant marine plastic debris is critical to\nunderstanding how concentrations of trash from across the world's ocean and\nidentifying high concentration garbage hotspots in dire need of trash removal.\nCurrently, the most common monitoring method to quantify floating plastic\nrequires the use of a manta trawl. Techniques requiring manta trawls (or\nsimilar surface collection devices) utilize physical removal of marine plastic\ndebris as the first step and then analyze collected samples as a second step.\nThe need for physical removal before analysis incurs high costs and requires\nintensive labor preventing scalable deployment of a real-time marine plastic\nmonitoring service across the entirety of Earth's ocean bodies. Without better\nmonitoring and sampling methods, the total impact of plastic pollution on the\nenvironment as a whole, and details of impact within specific oceanic regions,\nwill remain unknown. This study presents a highly scalable workflow that\nutilizes images captured within the epipelagic layer of the ocean as an input.\nIt produces real-time quantification of marine plastic debris for accurate\nquantification and physical removal. The workflow includes creating and\npreprocessing a domain-specific dataset, building an object detection model\nutilizing a deep neural network, and evaluating the model's performance.\nYOLOv5-S was the best performing model, which operates at a Mean Average\nPrecision (mAP) of 0.851 and an F1-Score of 0.89 while maintaining\nnear-real-time speed.",
          "link": "http://arxiv.org/abs/2105.01882",
          "publishedOn": "2021-07-27T02:03:35.162Z",
          "wordCount": 725,
          "title": "DeepPlastic: A Novel Approach to Detecting Epipelagic Bound Plastic Using Deep Visual Models. (arXiv:2105.01882v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>",
          "description": "Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled\nsource domain and an unsupervised loss in an unlabeled target domain, which\noften faces more severe overfitting (than classical supervised learning) as the\nsupervised source loss has clear domain gap and the unsupervised target loss is\noften noisy due to the lack of annotations. This paper presents RDA, a robust\ndomain adaptation technique that introduces adversarial attacking to mitigate\noverfitting in UDA. We achieve robust domain adaptation by a novel Fourier\nadversarial attacking (FAA) method that allows large magnitude of perturbation\nnoises but has minimal modification of image semantics, the former is critical\nto the effectiveness of its generated adversarial samples due to the existence\nof 'domain gaps'. Specifically, FAA decomposes images into multiple frequency\ncomponents (FCs) and generates adversarial samples by just perturbating certain\nFCs that capture little semantic information. With FAA-generated samples, the\ntraining can continue the 'random walk' and drift into an area with a flat loss\nlandscape, leading to more robust domain adaptation. Extensive experiments over\nmultiple domain adaptation tasks show that RDA can work with different computer\nvision tasks with superior performance.",
          "link": "http://arxiv.org/abs/2106.02874",
          "publishedOn": "2021-07-27T02:03:35.155Z",
          "wordCount": 661,
          "title": "RDA: Robust Domain Adaptation via Fourier Adversarial Attacking. (arXiv:2106.02874v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1\">Emilian Radoi</a>",
          "description": "The use of gait for person identification has important advantages such as\nbeing non-invasive, unobtrusive, not requiring cooperation and being less\nlikely to be obscured compared to other biometrics. Existing methods for gait\nrecognition require cooperative gait scenarios, in which a single person is\nwalking multiple times in a straight line in front of a camera. We aim to\naddress the hard challenges of real-world scenarios in which camera feeds\ncapture multiple people, who in most cases pass in front of the camera only\nonce. We address privacy concerns by using only the motion information of\nwalking individuals, with no identifiable appearance-based information. As\nsuch, we propose a novel weakly supervised learning framework, WildGait, which\nconsists of training a Spatio-Temporal Graph Convolutional Network on a large\nnumber of automatically annotated skeleton sequences obtained from raw,\nreal-world, surveillance streams to learn useful gait signatures. Our results\nshow that, with fine-tuning, we surpass in terms of recognition accuracy the\ncurrent state-of-the-art pose-based gait recognition solutions. Our proposed\nmethod is reliable in training gait recognition methods in unconstrained\nenvironments, especially in settings with scarce amounts of annotated data.",
          "link": "http://arxiv.org/abs/2105.05528",
          "publishedOn": "2021-07-27T02:03:35.148Z",
          "wordCount": 677,
          "title": "WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1\">Andrey V. Savchenko</a>",
          "description": "In this paper, the multi-task learning of lightweight convolutional neural\nnetworks is studied for face identification and classification of facial\nattributes (age, gender, ethnicity) trained on cropped faces without margins.\nThe necessity to fine-tune these networks to predict facial expressions is\nhighlighted. Several models are presented based on MobileNet, EfficientNet and\nRexNet architectures. It was experimentally demonstrated that they lead to near\nstate-of-the-art results in age, gender and race recognition on the UTKFace\ndataset and emotion classification on the AffectNet dataset. Moreover, it is\nshown that the usage of the trained models as feature extractors of facial\nregions in video frames leads to 4.5% higher accuracy than the previously known\nstate-of-the-art single models for the AFEW and the VGAF datasets from the\nEmotiW challenges. The models and source code are publicly available at\nhttps://github.com/HSE-asavchenko/face-emotion-recognition.",
          "link": "http://arxiv.org/abs/2103.17107",
          "publishedOn": "2021-07-27T02:03:35.140Z",
          "wordCount": 617,
          "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks. (arXiv:2103.17107v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1\">Marco Visca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1\">Sampo Kuutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1\">Roger Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>",
          "description": "Terrain traversability analysis plays a major role in ensuring safe robotic\nnavigation in unstructured environments. However, real-time constraints\nfrequently limit the accuracy of online tests especially in scenarios where\nrealistic robot-terrain interactions are complex to model. In this context, we\npropose a deep learning framework trained in an end-to-end fashion from\nelevation maps and trajectories to estimate the occurrence of failure events.\nThe network is first trained and tested in simulation over synthetic maps\ngenerated by the OpenSimplex algorithm. The prediction performance of the Deep\nLearning framework is illustrated by being able to retain over 94% recall of\nthe original simulator at 30% of the computational time. Finally, the network\nis transferred and tested on real elevation maps collected by the SEEKER\nconsortium during the Martian rover test trial in the Atacama desert in Chile.\nWe show that transferring and fine-tuning of an application-independent\npre-trained model retains better performance than training uniquely on scarcely\navailable real data.",
          "link": "http://arxiv.org/abs/2105.10937",
          "publishedOn": "2021-07-27T02:03:35.133Z",
          "wordCount": 639,
          "title": "Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1\">Darshan Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1\">S Balasubramanian</a>",
          "description": "Facial expression recognition (FER) in the wild is crucial for building\nreliable human-computer interactive systems. However, annotations of large\nscale datasets in FER has been a key challenge as these datasets suffer from\nnoise due to various factors like crowd sourcing, subjectivity of annotators,\npoor quality of images, automatic labelling based on key word search etc. Such\nnoisy annotations impede the performance of FER due to the memorization ability\nof deep networks. During early learning stage, deep networks fit on clean data.\nThen, eventually, they start overfitting on noisy labels due to their\nmemorization ability, which limits FER performance. This report presents\nConsensual Collaborative Training (CCT) framework used in our submission to\nexpression recognition track of the Affective Behaviour Analysis in-the-wild\n(ABAW) 2021 competition. CCT co-trains three networks jointly using a convex\ncombination of supervision loss and consistency loss, without making any\nassumption about the noise distribution. A dynamic transition mechanism is used\nto move from supervision loss in early learning to consistency loss for\nconsensus of predictions among networks in the later stage. Co-training reduces\noverall error, and consistency loss prevents overfitting to noisy samples. The\nperformance of the model is validated on challenging Aff-Wild2 dataset for\ncategorical expression classification. Our code is made publicly available at\nhttps://github.com/1980x/ABAW2021DMACS.",
          "link": "http://arxiv.org/abs/2107.05736",
          "publishedOn": "2021-07-27T02:03:35.121Z",
          "wordCount": 696,
          "title": "Affect Expression Behaviour Analysis in the Wild using Consensual Collaborative Training. (arXiv:2107.05736v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wang_S/0/1/0/all/0/1\">Song wang</a>",
          "description": "Image inpainting aims to restore the missing regions and make the recovery\nresults identical to the originally complete image, which is different from the\ncommon generative task emphasizing the naturalness of generated images.\nNevertheless, existing works usually regard it as a pure generation problem and\nemploy cutting-edge generative techniques to address it. The generative\nnetworks fill the main missing parts with realistic contents but usually\ndistort the local structures. In this paper, we formulate image inpainting as a\nmix of two problems, i.e., predictive filtering and deep generation. Predictive\nfiltering is good at preserving local structures and removing artifacts but\nfalls short to complete the large missing regions. The deep generative network\ncan fill the numerous missing pixels based on the understanding of the whole\nscene but hardly restores the details identical to the original ones. To make\nuse of their respective advantages, we propose the joint predictive filtering\nand generative network (JPGNet) that contains three branches: predictive\nfiltering & uncertainty network (PFUNet), deep generative network, and\nuncertainty-aware fusion network (UAFNet). The PFUNet can adaptively predict\npixel-wise kernels for filtering-based inpainting according to the input image\nand output an uncertainty map. This map indicates the pixels should be\nprocessed by filtering or generative networks, which is further fed to the\nUAFNet for a smart combination between filtering and generative results. Note\nthat, our method as a novel framework for the image inpainting problem can\nbenefit any existing generation-based methods. We validate our method on three\npublic datasets, i.e., Dunhuang, Places2, and CelebA, and demonstrate that our\nmethod can enhance three state-of-the-art generative methods (i.e., StructFlow,\nEdgeConnect, and RFRNet) significantly with the slightly extra time cost.",
          "link": "http://arxiv.org/abs/2107.04281",
          "publishedOn": "2021-07-27T02:03:35.098Z",
          "wordCount": 747,
          "title": "JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting. (arXiv:2107.04281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-27T02:03:35.070Z",
          "wordCount": 724,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Lin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zongyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1\">Qianyan Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yehansen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lijing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihang Li</a>",
          "description": "RGB-Infrared (IR) person re-identification aims to retrieve\nperson-of-interest from heterogeneous cameras, easily suffering from large\nimage modality discrepancy caused by different sensing wavelength ranges.\nExisting work usually minimizes such discrepancy by aligning domain\ndistribution of global features, while neglecting the intra-modality structural\nrelations between semantic parts. This could result in the network overly\nfocusing on local cues, without considering long-range body part dependencies,\nleading to meaningless region representations. In this paper, we propose a\ngraph-enabled distribution matching solution, dubbed Geometry-Guided\nDual-Alignment (G2DA) learning, for RGB-IR ReID. It can jointly encourage the\ncross-modal consistency between part semantics and structural relations for\nfine-grained modality alignment by solving a graph matching task within a\nmulti-scale skeleton graph that embeds human topology information.\nSpecifically, we propose to build a semantic-aligned complete graph into which\nall cross-modality images can be mapped via a pose-adaptive graph construction\nmechanism. This graph represents extracted whole-part features by nodes and\nexpresses the node-wise similarities with associated edges. To achieve the\ngraph-based dual-alignment learning, an Optimal Transport (OT) based structured\nmetric is further introduced to simultaneously measure point-wise relations and\ngroup-wise structural similarities across modalities. By minimizing the cost of\nan inter-modality transport plan, G2DA can learn a consistent and\ndiscriminative feature subspace for cross-modality image retrieval.\nFurthermore, we advance a Message Fusion Attention (MFA) mechanism to\nadaptively reweight the information flow of semantic propagation, effectively\nstrengthening the discriminability of extracted semantic features.",
          "link": "http://arxiv.org/abs/2106.07853",
          "publishedOn": "2021-07-27T02:03:35.063Z",
          "wordCount": 712,
          "title": "G2DA: Geometry-Guided Dual-Alignment Learning for RGB-Infrared Person Re-Identification. (arXiv:2106.07853v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>",
          "description": "This paper proposes a novel deep learning-based video object matting method\nthat can achieve temporally coherent matting results. Its key component is an\nattention-based temporal aggregation module that maximizes image matting\nnetworks' strength for video matting networks. This module computes temporal\ncorrelations for pixels adjacent to each other along the time axis in feature\nspace, which is robust against motion noises. We also design a novel loss term\nto train the attention weights, which drastically boosts the video matting\nperformance. Besides, we show how to effectively solve the trimap generation\nproblem by fine-tuning a state-of-the-art video object segmentation network\nwith a sparse set of user-annotated keyframes. To facilitate video matting and\ntrimap generation networks' training, we construct a large-scale video matting\ndataset with 80 training and 28 validation foreground video clips with\nground-truth alpha mattes. Experimental results show that our method can\ngenerate high-quality alpha mattes for various videos featuring appearance\nchange, occlusion, and fast motion. Our code and dataset can be found at:\nhttps://github.com/yunkezhang/TCVOM",
          "link": "http://arxiv.org/abs/2105.11427",
          "publishedOn": "2021-07-27T02:03:35.056Z",
          "wordCount": 650,
          "title": "Attention-guided Temporally Coherent Video Object Matting. (arXiv:2105.11427v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>",
          "description": "We propose ST-DETR, a Spatio-Temporal Transformer-based architecture for\nobject detection from a sequence of temporal frames. We treat the temporal\nframes as sequences in both space and time and employ the full attention\nmechanisms to take advantage of the features correlations over both dimensions.\nThis treatment enables us to deal with frames sequence as temporal object\nfeatures traces over every location in the space. We explore two possible\napproaches; the early spatial features aggregation over the temporal dimension,\nand the late temporal aggregation of object query spatial features. Moreover,\nwe propose a novel Temporal Positional Embedding technique to encode the time\nsequence information. To evaluate our approach, we choose the Moving Object\nDetection (MOD)task, since it is a perfect candidate to showcase the importance\nof the temporal dimension. Results show a significant 5% mAP improvement on the\nKITTI MOD dataset over the 1-step spatial baseline.",
          "link": "http://arxiv.org/abs/2107.05887",
          "publishedOn": "2021-07-27T02:03:35.027Z",
          "wordCount": 601,
          "title": "ST-DETR: Spatio-Temporal Object Traces Attention Detection Transformer. (arXiv:2107.05887v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shavit_Y/0/1/0/all/0/1\">Yoli Shavit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferens_R/0/1/0/all/0/1\">Ron Ferens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1\">Yosi Keller</a>",
          "description": "Absolute camera pose regressors estimate the position and orientation of a\ncamera from the captured image alone. Typically, a convolutional backbone with\na multi-layer perceptron head is trained with images and pose labels to embed a\nsingle reference scene at a time. Recently, this scheme was extended for\nlearning multiple scenes by replacing the MLP head with a set of fully\nconnected layers. In this work, we propose to learn multi-scene absolute camera\npose regression with Transformers, where encoders are used to aggregate\nactivation maps with self-attention and decoders transform latent features and\nscenes encoding into candidate pose predictions. This mechanism allows our\nmodel to focus on general features that are informative for localization while\nembedding multiple scenes in parallel. We evaluate our method on commonly\nbenchmarked indoor and outdoor datasets and show that it surpasses both\nmulti-scene and state-of-the-art single-scene absolute pose regressors. We make\nour code publicly available from\nhttps://github.com/yolish/multi-scene-pose-transformer.",
          "link": "http://arxiv.org/abs/2103.11468",
          "publishedOn": "2021-07-27T02:03:35.021Z",
          "wordCount": 613,
          "title": "Learning Multi-Scene Absolute Pose Regression with Transformers. (arXiv:2103.11468v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Vision and Language Navigation (VLN) requires an agent to navigate to a\ntarget location by following natural language instructions. Most of existing\nworks represent a navigation candidate by the feature of the corresponding\nsingle view where the candidate lies in. However, an instruction may mention\nlandmarks out of the single view as references, which might lead to failures of\ntextual-visual matching of existing methods. In this work, we propose a\nmulti-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate\nvisual contexts from neighbor views for better textual-visual matching.\nSpecifically, our NvEM utilizes a subject module and a reference module to\ncollect contexts from neighbor views. The subject module fuses neighbor views\nat a global level, and the reference module fuses neighbor objects at a local\nlevel. Subjects and references are adaptively determined via attention\nme'chanisms. Our model also includes an action module to utilize the strong\norientation guidance (e.g., \"turn left\") in instructions. Each module predicts\nnavigation action separately and their weighted sum is used for predicting the\nfinal action. Extensive experimental results demonstrate the effectiveness of\nthe proposed method on the R2R and R4R benchmarks against several\nstate-of-the-art navigators, and NvEM even beats some pre-training ones. Our\ncode is available at https://github.com/MarSaKi/NvEM.",
          "link": "http://arxiv.org/abs/2107.07201",
          "publishedOn": "2021-07-27T02:03:35.002Z",
          "wordCount": 669,
          "title": "Neighbor-view Enhanced Model for Vision and Language Navigation. (arXiv:2107.07201v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tingting Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Modern top-performing object detectors depend heavily on backbone networks,\nwhose advances bring consistent performance gains through exploring more\neffective network structures. In this paper, we propose a novel and flexible\nbackbone framework, namely CBNetV2, to construct high-performance detectors\nusing existing open-sourced pre-trained backbones under the pre-training\nfine-tuning paradigm. In particular, CBNetV2 architecture groups multiple\nidentical backbones, which are connected through composite connections.\nSpecifically, it integrates the high- and low-level features of multiple\nbackbone networks and gradually expands the receptive field to more efficiently\nperform object detection. We also propose a better training strategy with\nassistant supervision for CBNet-based detectors. Without additional\npre-training of the composite backbone, CBNetV2 can be adapted to various\nbackbones (CNN-based vs. Transformer-based) and head designs of most mainstream\ndetectors (one-stage vs. two-stage, anchor-based vs. anchor-free-based).\nExperiments provide strong evidence that, compared with simply increasing the\ndepth and width of the network, CBNetV2 introduces a more efficient, effective,\nand resource-friendly way to build high-performance backbone networks.\nParticularly, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO\ntest-dev under the single-model and single-scale testing protocol, which is\nsignificantly better than the state-of-the-art result (57.7% box AP and 50.2%\nmask AP) achieved by Swin-L, while the training schedule is reduced by\n6$\\times$. With multi-scale testing, we push the current best single model\nresult to a new record of 60.1% box AP and 52.3% mask AP without using extra\ntraining data. Code is available at https://github.com/VDIGPKU/CBNetV2.",
          "link": "http://arxiv.org/abs/2107.00420",
          "publishedOn": "2021-07-27T02:03:34.995Z",
          "wordCount": 735,
          "title": "CBNetV2: A Composite Backbone Network Architecture for Object Detection. (arXiv:2107.00420v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Romanengo_C/0/1/0/all/0/1\">Chiara Romanengo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffo_A/0/1/0/all/0/1\">Andrea Raffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_Y/0/1/0/all/0/1\">Yifan Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_N/0/1/0/all/0/1\">Nabil Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcidieno_B/0/1/0/all/0/1\">Bianca Falcidieno</a>",
          "description": "We propose Fit4CAD, a benchmark for the evaluation and comparison of methods\nfor fitting simple geometric primitives in point clouds representing CAD\nmodels. This benchmark is meant to help both method developers and those who\nwant to identify the best performing tools. The Fit4CAD dataset is composed by\n225 high quality point clouds, each of which has been obtained by sampling a\nCAD model. The way these elements were created by using existing platforms and\ndatasets makes the benchmark easily expandable. The dataset is already split\ninto a training set and a test set. To assess performance and accuracy of the\ndifferent primitive fitting methods, various measures are defined. To\ndemonstrate the effective use of Fit4CAD, we have tested it on two methods\nbelonging to two different categories of approaches to the primitive fitting\nproblem: a clustering method based on a primitive growing framework and a\nparametric method based on the Hough transform.",
          "link": "http://arxiv.org/abs/2105.06858",
          "publishedOn": "2021-07-27T02:03:34.987Z",
          "wordCount": 632,
          "title": "Fit4CAD: A point cloud benchmark for fitting simple geometric primitives in CAD models. (arXiv:2105.06858v2 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14758",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuwei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Daoye Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1\">Jingwei Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Zhaoxiang Ye</a>",
          "description": "Low-dose CT has been a key diagnostic imaging modality to reduce the\npotential risk of radiation overdose to patient health. Despite recent\nadvances, CNN-based approaches typically apply filters in a spatially invariant\nway and adopt similar pixel-level losses, which treat all regions of the CT\nimage equally and can be inefficient when fine-grained structures coexist with\nnon-uniformly distributed noises. To address this issue, we propose a\nStructure-preserving Kernel Prediction Network (StructKPN) that combines the\nkernel prediction network with a structure-aware loss function that utilizes\nthe pixel gradient statistics and guides the model towards spatially-variant\nfilters that enhance noise removal, prevent over-smoothing and preserve\ndetailed structures for different regions in CT imaging. Extensive experiments\ndemonstrated that our approach achieved superior performance on both synthetic\nand non-synthetic datasets, and better preserves structures that are highly\ndesired in clinical screening and low-dose protocol optimization.",
          "link": "http://arxiv.org/abs/2105.14758",
          "publishedOn": "2021-07-27T02:03:34.961Z",
          "wordCount": 623,
          "title": "Low-Dose CT Denoising Using a Structure-Preserving Kernel Prediction Network. (arXiv:2105.14758v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuechao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzhansky_M/0/1/0/all/0/1\">Michael Ruzhansky</a>",
          "description": "In this paper, we propose an interesting semi-sparsity smoothing algorithm\nbased on a novel sparsity-inducing optimization framework. This method is\nderived from the multiple observations, that is, semi-sparsity prior knowledge\nis more universally applicable, especially in areas where sparsity is not fully\nadmitted, such as polynomial-smoothing surfaces. We illustrate that this\nsemi-sparsity can be identified into a generalized $L_0$-norm minimization in\nhigher-order gradient domains, thereby giving rise to a new \"feature-aware\"\nfiltering method with a powerful simultaneous-fitting ability in both sparse\nfeatures (singularities and sharpening edges) and non-sparse regions\n(polynomial-smoothing surfaces). Notice that a direct solver is always\nunavailable due to the non-convexity and combinatorial nature of $L_0$-norm\nminimization. Instead, we solve the model based on an efficient half-quadratic\nsplitting minimization with fast Fourier transforms (FFTs) for acceleration. We\nfinally demonstrate its versatility and many benefits to a series of\nsignal/image processing and computer vision applications.",
          "link": "http://arxiv.org/abs/2107.00627",
          "publishedOn": "2021-07-27T02:03:34.955Z",
          "wordCount": 590,
          "title": "Semi-Sparsity for Smoothing Filters. (arXiv:2107.00627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07761",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mascolini_A/0/1/0/all/0/1\">Alessio Mascolini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardamone_D/0/1/0/all/0/1\">Dario Cardamone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponzio_F/0/1/0/all/0/1\">Francesco Ponzio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cataldo_S/0/1/0/all/0/1\">Santa Di Cataldo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ficarra_E/0/1/0/all/0/1\">Elisa Ficarra</a>",
          "description": "Computer-aided analysis of biological images typically requires extensive\ntraining on large-scale annotated datasets, which is not viable in many\nsituations. In this paper we present GAN-DL, a Discriminator Learner based on\nthe StyleGAN2 architecture, which we employ for self-supervised image\nrepresentation learning in the case of fluorescent biological images. We show\nthat Wasserstein Generative Adversarial Networks combined with linear Support\nVector Machines enable high-throughput compound screening based on raw images.\nWe demonstrate this by classifying active and inactive compounds tested for the\ninhibition of SARS-CoV-2 infection in VERO and HRCE cell lines. In contrast to\nprevious methods, our deep learning based approach does not require any\nannotation besides the one that is normally collected during the sample\npreparation process. We test our technique on the RxRx19a Sars-CoV-2 image\ncollection. The dataset consists of fluorescent images that were generated to\nassess the ability of regulatory-approved or in late-stage clinical trials\ncompound to modulate the in vitro infection from SARS-CoV-2 in both VERO and\nHRCE cell lines. We show that our technique can be exploited not only for\nclassification tasks, but also to effectively derive a dose response curve for\nthe tested treatments, in a self-supervised manner. Lastly, we demonstrate its\ngeneralization capabilities by successfully addressing a zero-shot learning\ntask, consisting in the categorization of four different cell types of the\nRxRx1 fluorescent images collection.",
          "link": "http://arxiv.org/abs/2107.07761",
          "publishedOn": "2021-07-27T02:03:34.948Z",
          "wordCount": 750,
          "title": "Exploiting generative self-supervised learning for the assessment of biological images with lack of annotations: a COVID-19 case-study. (arXiv:2107.07761v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Um_S/0/1/0/all/0/1\">Se-Yun Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sangshin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_K/0/1/0/all/0/1\">Kyungguen Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hong-Goo Kang</a>",
          "description": "In this paper, we propose an effective method to synthesize speaker-specific\nspeech waveforms by conditioning on videos of an individual's face. Using a\ngenerative adversarial network (GAN) with linguistic and speaker characteristic\nfeatures as auxiliary conditions, our method directly converts face images into\nspeech waveforms under an end-to-end training framework. The linguistic\nfeatures are extracted from lip movements using a lip-reading model, and the\nspeaker characteristic features are predicted from face images using\ncross-modal learning with a pre-trained acoustic model. Since these two\nfeatures are uncorrelated and controlled independently, we can flexibly\nsynthesize speech waveforms whose speaker characteristics vary depending on the\ninput face images. Therefore, our method can be regarded as a multi-speaker\nface-to-speech waveform model. We show the superiority of our proposed model\nover conventional methods in terms of both objective and subjective evaluation\nresults. Specifically, we evaluate the performances of the linguistic feature\nand the speaker characteristic generation modules by measuring the accuracy of\nautomatic speech recognition and automatic speaker/gender recognition tasks,\nrespectively. We also evaluate the naturalness of the synthesized speech\nwaveforms using a mean opinion score (MOS) test.",
          "link": "http://arxiv.org/abs/2107.12003",
          "publishedOn": "2021-07-27T02:03:34.509Z",
          "wordCount": 646,
          "title": "Facetron: Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>",
          "description": "We study the challenging task of neural network quantization without\nend-to-end retraining, called Post-training Quantization (PTQ). PTQ usually\nrequires a small subset of training data but produces less powerful quantized\nmodels than Quantization-Aware Training (QAT). In this work, we propose a novel\nPTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to\nINT2 for the first time. BRECQ leverages the basic building blocks in neural\nnetworks and reconstructs them one-by-one. In a comprehensive theoretical study\nof the second-order error, we show that BRECQ achieves a good balance between\ncross-layer dependency and generalization error. To further employ the power of\nquantization, the mixed precision technique is incorporated in our framework by\napproximating the inter-layer and intra-layer sensitivity. Extensive\nexperiments on various handcrafted and searched neural architectures are\nconducted for both image classification and object detection tasks. And for the\nfirst time we prove that, without bells and whistles, PTQ can attain 4-bit\nResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster\nproduction of quantized models. Codes are available at\nhttps://github.com/yhhhli/BRECQ.",
          "link": "http://arxiv.org/abs/2102.05426",
          "publishedOn": "2021-07-27T02:03:34.429Z",
          "wordCount": 658,
          "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction. (arXiv:2102.05426v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1\">Abhishek Bhandwaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shari Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1\">Elizabeth Spelke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer D. Ullman</a>",
          "description": "For machine agents to successfully interact with humans in real-world\nsettings, they will need to develop an understanding of human mental life.\nIntuitive psychology, the ability to reason about hidden mental variables that\ndrive observable actions, comes naturally to people: even pre-verbal infants\ncan tell agents from objects, expecting agents to act efficiently to achieve\ngoals given constraints. Despite recent interest in machine agents that reason\nabout other agents, it is not clear if such agents learn or hold the core\npsychology principles that drive human reasoning. Inspired by cognitive\ndevelopment studies on intuitive psychology, we present a benchmark consisting\nof a large dataset of procedurally generated 3D animations, AGENT (Action,\nGoal, Efficiency, coNstraint, uTility), structured around four scenarios (goal\npreferences, action efficiency, unobserved constraints, and cost-reward\ntrade-offs) that probe key concepts of core intuitive psychology. We validate\nAGENT with human-ratings, propose an evaluation protocol emphasizing\ngeneralization, and compare two strong baselines built on Bayesian inverse\nplanning and a Theory of Mind neural network. Our results suggest that to pass\nthe designed tests of core intuitive psychology at human levels, a model must\nacquire or have built-in representations of how agents plan, combining utility\ncomputations and core knowledge of objects and physics.",
          "link": "http://arxiv.org/abs/2102.12321",
          "publishedOn": "2021-07-27T02:03:34.422Z",
          "wordCount": 712,
          "title": "AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14659",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Andrews_M/0/1/0/all/0/1\">Michael Andrews</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Burkle_B/0/1/0/all/0/1\">Bjorn Burkle</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-fan Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+DiCroce_D/0/1/0/all/0/1\">Davide DiCroce</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gleyzer_S/0/1/0/all/0/1\">Sergei Gleyzer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heintz_U/0/1/0/all/0/1\">Ulrich Heintz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Narain_M/0/1/0/all/0/1\">Meenakshi Narain</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Paulini_M/0/1/0/all/0/1\">Manfred Paulini</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pervan_N/0/1/0/all/0/1\">Nikolas Pervan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shafi_Y/0/1/0/all/0/1\">Yusef Shafi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Usai_E/0/1/0/all/0/1\">Emanuele Usai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_K/0/1/0/all/0/1\">Kun Yang</a>",
          "description": "We describe a novel application of the end-to-end deep learning technique to\nthe task of discriminating top quark-initiated jets from those originating from\nthe hadronization of a light quark or a gluon. The end-to-end deep learning\ntechnique combines deep learning algorithms and low-level detector\nrepresentation of the high-energy collision event. In this study, we use\nlow-level detector information from the simulated CMS Open Data samples to\nconstruct the top jet classifiers. To optimize classifier performance we\nprogressively add low-level information from the CMS tracking detector,\nincluding pixel detector reconstructed hits and impact parameters, and\ndemonstrate the value of additional tracking information even when no new\nspatial structures are added. Relying only on calorimeter energy deposits and\nreconstructed pixel detector hits, the end-to-end classifier achieves an AUC\nscore of 0.975$\\pm$0.002 for the task of classifying boosted top quark jets.\nAfter adding derived track quantities, the classifier AUC score increases to\n0.9824$\\pm$0.0013, serving as the first performance benchmark for these CMS\nOpen Data samples. We additionally provide a timing performance comparison of\ndifferent processor unit architectures for training the network.",
          "link": "http://arxiv.org/abs/2104.14659",
          "publishedOn": "2021-07-27T02:03:34.413Z",
          "wordCount": 685,
          "title": "End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data. (arXiv:2104.14659v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Abdelrahman Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadhoud_M/0/1/0/all/0/1\">Mayada Hadhoud</a>",
          "description": "Instance segmentation has gained recently huge attention in various computer\nvision applications. It aims at providing different IDs to different objects of\nthe scene, even if they belong to the same class. Instance segmentation is\nusually performed as a two-stage pipeline. First, an object is detected, then\nsemantic segmentation within the detected box area is performed which involves\ncostly up-sampling. In this paper, we propose Insta-YOLO, a novel one-stage\nend-to-end deep learning model for real-time instance segmentation. Instead of\npixel-wise prediction, our model predicts instances as object contours\nrepresented by 2D points in Cartesian space. We evaluate our model on three\ndatasets, namely, Carvana,Cityscapes and Airbus. We compare our results to the\nstate-of-the-art models for instance segmentation. The results show our model\nachieves competitive accuracy in terms of mAP at twice the speed on GTX-1080\nGPU.",
          "link": "http://arxiv.org/abs/2102.06777",
          "publishedOn": "2021-07-27T02:03:34.406Z",
          "wordCount": 596,
          "title": "INSTA-YOLO: Real-Time Instance Segmentation. (arXiv:2102.06777v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">Inigo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferstl_D/0/1/0/all/0/1\">David Ferstl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>",
          "description": "This work presents a novel approach for semi-supervised semantic\nsegmentation. The key element of this approach is our contrastive learning\nmodule that enforces the segmentation network to yield similar pixel-level\nfeature representations for same-class samples across the whole dataset. To\nachieve this, we maintain a memory bank continuously updated with relevant and\nhigh-quality feature vectors from labeled data. In an end-to-end training, the\nfeatures from both labeled and unlabeled data are optimized to be similar to\nsame-class samples from the memory bank. Our approach outperforms the current\nstate-of-the-art for semi-supervised semantic segmentation and semi-supervised\ndomain adaptation on well-known public benchmarks, with larger improvements on\nthe most challenging scenarios, i.e., less available labeled data.\nhttps://github.com/Shathe/SemiSeg-Contrastive",
          "link": "http://arxiv.org/abs/2104.13415",
          "publishedOn": "2021-07-27T02:03:34.374Z",
          "wordCount": 601,
          "title": "Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank. (arXiv:2104.13415v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Z/0/1/0/all/0/1\">Zhibin Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1\">Mu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>",
          "description": "While CNN-based models have made remarkable progress on human pose\nestimation, what spatial dependencies they capture to localize keypoints\nremains unclear. In this work, we propose a model called \\textbf{TransPose},\nwhich introduces Transformer for human pose estimation. The attention layers\nbuilt in Transformer enable our model to capture long-range relationships\nefficiently and also can reveal what dependencies the predicted keypoints rely\non. To predict keypoint heatmaps, the last attention layer specially acts as an\naggregator, which collects contributions from image clues and forms maximum\npositions of keypoints. Such a heatmap-based localization approach via\nTransformer conforms to the principle of Activation Maximization\n\\cite{erhan2009visualizing}. And the revealed dependencies are image-specific\nand fine-grained, which also can provide evidence of how the model handles\nspecial cases, e.g., occlusion. The experiments show that TransPose achieves\n75.8 AP and 75.0 AP on COCO validation and test-dev sets with 256 $\\times$ 192\ninput resolution, while being more lightweight and faster than mainstream CNN\narchitectures. The TransPose model also transfers very well on MPII benchmark,\nyielding 93.9\\% accuracy on test set when fine-tuned with small training costs.\nCode and pre-trained models are publicly available at\n\\url{https://github.com/yangsenius/TransPose}.",
          "link": "http://arxiv.org/abs/2012.14214",
          "publishedOn": "2021-07-27T02:03:34.367Z",
          "wordCount": 662,
          "title": "TransPose: Keypoint Localization via Transformer. (arXiv:2012.14214v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Anchal Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moharana_S/0/1/0/all/0/1\">Sukumar Moharana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_D/0/1/0/all/0/1\">Debi Prasanna Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panwar_A/0/1/0/all/0/1\">Archit Panwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dewang Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thota_S/0/1/0/all/0/1\">Siva Prasad Thota</a>",
          "description": "With the advent of internet, not safe for work(NSFW) content moderation is a\nmajor problem today. Since,smartphones are now part of daily life of billions\nof people,it becomes even more important to have a solution which coulddetect\nand suggest user about potential NSFW content present ontheir phone. In this\npaper we present a novel on-device solutionfor detecting NSFW images. In\naddition to conventional porno-graphic content moderation, we have also\nincluded semi-nudecontent moderation as it is still NSFW in a large\ndemography.We have curated a dataset comprising of three major\ncategories,namely nude, semi-nude and safe images. We have created anensemble\nof object detector and classifier for filtering of nudeand semi-nude contents.\nThe solution provides unsafe body partannotations along with identification of\nsemi-nude images. Weextensively tested our proposed solution on several public\ndatasetand also on our custom dataset. The model achieves F1 scoreof 0.91 with\n95% precision and 88% recall on our customNSFW16k dataset and 0.92 MAP on NPDI\ndataset. Moreover itachieves average 0.002 false positive rate on a collection\nof safeimage open datasets.",
          "link": "http://arxiv.org/abs/2107.11845",
          "publishedOn": "2021-07-27T02:03:34.345Z",
          "wordCount": 609,
          "title": "On-Device Content Moderation. (arXiv:2107.11845v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Fan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinlong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Sanqing Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Rongqi Gu</a>",
          "description": "Point cloud registration is a fundamental problem in 3D computer vision.\nOutdoor LiDAR point clouds are typically large-scale and complexly distributed,\nwhich makes the registration challenging. In this paper, we propose an\nefficient hierarchical network named HRegNet for large-scale outdoor LiDAR\npoint cloud registration. Instead of using all points in the point clouds,\nHRegNet performs registration on hierarchically extracted keypoints and\ndescriptors. The overall framework combines the reliable features in deeper\nlayer and the precise position information in shallower layers to achieve\nrobust and precise registration. We present a correspondence network to\ngenerate correct and accurate keypoints correspondences. Moreover, bilateral\nconsensus and neighborhood consensus are introduced for keypoints matching and\nnovel similarity features are designed to incorporate them into the\ncorrespondence network, which significantly improves the registration\nperformance. Besides, the whole network is also highly efficient since only a\nsmall number of keypoints are used for registration. Extensive experiments are\nconducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate\nthe high accuracy and efficiency of the proposed HRegNet. The project website\nis https://ispc-group.github.io/hregnet.",
          "link": "http://arxiv.org/abs/2107.11992",
          "publishedOn": "2021-07-27T02:03:34.339Z",
          "wordCount": 629,
          "title": "HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration. (arXiv:2107.11992v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.04902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Lun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Sheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Yu Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>",
          "description": "We present a learning-based approach for removing unwanted obstructions, such\nas window reflections, fence occlusions, or adherent raindrops, from a short\nsequence of images captured by a moving camera. Our method leverages motion\ndifferences between the background and obstructing elements to recover both\nlayers. Specifically, we alternate between estimating dense optical flow fields\nof the two layers and reconstructing each layer from the flow-warped images via\na deep convolutional neural network. This learning-based layer reconstruction\nmodule facilitates accommodating potential errors in the flow estimation and\nbrittle assumptions, such as brightness consistency. We show that the proposed\napproach learned from synthetically generated data performs well to real\nimages. Experimental results on numerous challenging scenarios of reflection\nand fence removal demonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2008.04902",
          "publishedOn": "2021-07-27T02:03:34.319Z",
          "wordCount": 625,
          "title": "Learning to See Through Obstructions with Layered Decomposition. (arXiv:2008.04902v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1\">Jay Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>",
          "description": "Deep learning-based models are developed to automatically detect if a retina\nimage is `referable' in diabetic retinopathy (DR) screening. However, their\nclassification accuracy degrades as the input images distributionally shift\nfrom their training distribution. Further, even if the input is not a retina\nimage, a standard DR classifier produces a high confident prediction that the\nimage is `referable'. Our paper presents a Dirichlet Prior Network-based\nframework to address this issue. It utilizes an out-of-distribution (OOD)\ndetector model and a DR classification model to improve generalizability by\nidentifying OOD images. Experiments on real-world datasets indicate that the\nproposed framework can eliminate the unknown non-retina images and identify the\ndistributionally shifted retina images for human intervention.",
          "link": "http://arxiv.org/abs/2107.11822",
          "publishedOn": "2021-07-27T02:03:34.304Z",
          "wordCount": 560,
          "title": "Distributional Shifts in Automated Diabetic Retinopathy Screening. (arXiv:2107.11822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qiang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shichao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhida Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>",
          "description": "The performance of face recognition system degrades when the variability of\nthe acquired faces increases. Prior work alleviates this issue by either\nmonitoring the face quality in pre-processing or predicting the data\nuncertainty along with the face feature. This paper proposes MagFace, a\ncategory of losses that learn a universal feature embedding whose magnitude can\nmeasure the quality of the given face. Under the new loss, it can be proven\nthat the magnitude of the feature embedding monotonically increases if the\nsubject is more likely to be recognized. In addition, MagFace introduces an\nadaptive mechanism to learn a wellstructured within-class feature distributions\nby pulling easy samples to class centers while pushing hard samples away. This\nprevents models from overfitting on noisy low-quality samples and improves face\nrecognition in the wild. Extensive experiments conducted on face recognition,\nquality assessments as well as clustering demonstrate its superiority over\nstate-of-the-arts. The code is available at\nhttps://github.com/IrvingMeng/MagFace.",
          "link": "http://arxiv.org/abs/2103.06627",
          "publishedOn": "2021-07-27T02:03:34.267Z",
          "wordCount": 659,
          "title": "MagFace: A Universal Representation for Face Recognition and Quality Assessment. (arXiv:2103.06627v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>",
          "description": "Salient object detection (SOD) is a long-standing research topic in computer\nvision and has drawn an increasing amount of research interest in the past\ndecade. This paper provides the first comprehensive review and benchmark for\nlight field SOD, which has long been lacking in the saliency community.\nFirstly, we introduce preliminary knowledge on light fields, including theory\nand data forms, and then review existing studies on light field SOD, covering\nten traditional models, seven deep learning-based models, one comparative\nstudy, and one brief review. Existing datasets for light field SOD are also\nsummarized with detailed information and statistical analyses. Secondly, we\nbenchmark nine representative light field SOD models together with several\ncutting-edge RGB-D SOD models on four widely used light field datasets, from\nwhich insightful discussions and analyses, including a comparison between light\nfield SOD and RGB-D SOD models, are achieved. Besides, due to the inconsistency\nof datasets in their current forms, we further generate complete data and\nsupplement focal stacks, depth maps and multi-view images for the inconsistent\ndatasets, making them consistent and unified. Our supplemental data makes a\nuniversal benchmark possible. Lastly, because light field SOD is quite a\nspecial problem attributed to its diverse data representations and high\ndependency on acquisition hardware, making it differ greatly from other\nsaliency detection tasks, we provide nine hints into the challenges and future\ndirections, and outline several open issues. We hope our review and\nbenchmarking could help advance research in this field. All the materials\nincluding collected models, datasets, benchmarking results, and supplemented\nlight field datasets will be publicly available on our project site\nhttps://github.com/kerenfu/LFSOD-Survey.",
          "link": "http://arxiv.org/abs/2010.04968",
          "publishedOn": "2021-07-27T02:03:34.219Z",
          "wordCount": 756,
          "title": "Light Field Salient Object Detection: A Review and Benchmark. (arXiv:2010.04968v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sixiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiachen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zekun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>",
          "description": "Most recent semantic segmentation methods adopt a fully-convolutional network\n(FCN) with an encoder-decoder architecture. The encoder progressively reduces\nthe spatial resolution and learns more abstract/semantic visual concepts with\nlarger receptive fields. Since context modeling is critical for segmentation,\nthe latest efforts have been focused on increasing the receptive field, through\neither dilated/atrous convolutions or inserting attention modules. However, the\nencoder-decoder based FCN architecture remains unchanged. In this paper, we aim\nto provide an alternative perspective by treating semantic segmentation as a\nsequence-to-sequence prediction task. Specifically, we deploy a pure\ntransformer (ie, without convolution and resolution reduction) to encode an\nimage as a sequence of patches. With the global context modeled in every layer\nof the transformer, this encoder can be combined with a simple decoder to\nprovide a powerful segmentation model, termed SEgmentation TRansformer (SETR).\nExtensive experiments show that SETR achieves new state of the art on ADE20K\n(50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on\nCityscapes. Particularly, we achieve the first position in the highly\ncompetitive ADE20K test server leaderboard on the day of submission.",
          "link": "http://arxiv.org/abs/2012.15840",
          "publishedOn": "2021-07-27T02:03:34.197Z",
          "wordCount": 679,
          "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. (arXiv:2012.15840v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1\">Qi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">MengChu Zhou</a>",
          "description": "In most interactive image generation tasks, given regions of interest (ROI)\nby users, the generated results are expected to have adequate diversities in\nappearance while maintaining correct and reasonable structures in original\nimages. Such tasks become more challenging if only limited data is available.\nRecently proposed generative models complete training based on only one image.\nThey pay much attention to the monolithic feature of the sample while ignoring\nthe actual semantic information of different objects inside the sample. As a\nresult, for ROI-based generation tasks, they may produce inappropriate samples\nwith excessive randomicity and without maintaining the related objects' correct\nstructures. To address this issue, this work introduces a\nMOrphologic-structure-aware Generative Adversarial Network named MOGAN that\nproduces random samples with diverse appearances and reliable structures based\non only one image. For training for ROI, we propose to utilize the data coming\nfrom the original image being augmented and bring in a novel module to\ntransform such augmented data into knowledge containing both structures and\nappearances, thus enhancing the model's comprehension of the sample. To learn\nthe rest areas other than ROI, we employ binary masks to ensure the generation\nisolated from ROI. Finally, we set parallel and hierarchical branches of the\nmentioned learning process. Compared with other single image GAN schemes, our\napproach focuses on internal features including the maintenance of rational\nstructures and variation on appearance. Experiments confirm a better capacity\nof our model on ROI-based image generation tasks than its competitive peers.",
          "link": "http://arxiv.org/abs/2103.02997",
          "publishedOn": "2021-07-27T02:03:34.190Z",
          "wordCount": 706,
          "title": "MOGAN: Morphologic-structure-aware Generative Learning from a Single Image. (arXiv:2103.02997v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imran_S/0/1/0/all/0/1\">Saif Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1\">Daniel Morris</a>",
          "description": "Depth completion starts from a sparse set of known depth values and estimates\nthe unknown depths for the remaining image pixels. Most methods model this as\ndepth interpolation and erroneously interpolate depth pixels into the empty\nspace between spatially distinct objects, resulting in depth-smearing across\nocclusion boundaries. Here we propose a multi-hypothesis depth representation\nthat explicitly models both foreground and background depths in the difficult\nocclusion-boundary regions. Our method can be thought of as performing\ntwin-surface extrapolation, rather than interpolation, in these regions. Next\nour method fuses these extrapolated surfaces into a single depth image\nleveraging the image data. Key to our method is the use of an asymmetric loss\nfunction that operates on a novel twin-surface representation. This enables us\nto train a network to simultaneously do surface extrapolation and surface\nfusion. We characterize our loss function and compare with other common losses.\nFinally, we validate our method on three different datasets; KITTI, an outdoor\nreal-world dataset, NYU2, indoor real-world depth dataset and Virtual KITTI, a\nphoto-realistic synthetic dataset with dense groundtruth, and demonstrate\nimprovement over the state of the art.",
          "link": "http://arxiv.org/abs/2104.02253",
          "publishedOn": "2021-07-27T02:03:34.182Z",
          "wordCount": 669,
          "title": "Depth Completion with Twin Surface Extrapolation at Occlusion Boundaries. (arXiv:2104.02253v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Pranjal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goenka_S/0/1/0/all/0/1\">Shreyas Goenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaterji_S/0/1/0/all/0/1\">Somali Chaterji</a>",
          "description": "Federated learning allows a large number of devices to jointly learn a model\nwithout sharing data. In this work, we enable clients with limited computing\npower to perform action recognition, a computationally heavy task. We first\nperform model compression at the central server through knowledge distillation\non a large dataset. This allows the model to learn complex features and serves\nas an initialization for model fine-tuning. The fine-tuning is required because\nthe limited data present in smaller datasets is not adequate for action\nrecognition models to learn complex spatio-temporal features. Because the\nclients present are often heterogeneous in their computing resources, we use an\nasynchronous federated optimization and we further show a convergence bound. We\ncompare our approach to two baseline approaches: fine-tuning at the central\nserver (no clients) and fine-tuning using (heterogeneous) clients using\nsynchronous federated averaging. We empirically show on a testbed of\nheterogeneous embedded devices that we can perform action recognition with\ncomparable accuracy to the two baselines above, while our asynchronous learning\nstrategy reduces the training time by 40%, relative to synchronous learning.",
          "link": "http://arxiv.org/abs/2107.12147",
          "publishedOn": "2021-07-27T02:03:34.173Z",
          "wordCount": 632,
          "title": "Federated Action Recognition on Heterogeneous Embedded Devices. (arXiv:2107.12147v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neimark_D/0/1/0/all/0/1\">Daniel Neimark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_O/0/1/0/all/0/1\">Omri Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zohar_M/0/1/0/all/0/1\">Maya Zohar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asselmann_D/0/1/0/all/0/1\">Dotan Asselmann</a>",
          "description": "This paper presents VTN, a transformer-based framework for video recognition.\nInspired by recent developments in vision transformers, we ditch the standard\napproach in video action recognition that relies on 3D ConvNets and introduce a\nmethod that classifies actions by attending to the entire video sequence\ninformation. Our approach is generic and builds on top of any given 2D spatial\nnetwork. In terms of wall runtime, it trains $16.1\\times$ faster and runs\n$5.1\\times$ faster during inference while maintaining competitive accuracy\ncompared to other state-of-the-art methods. It enables whole video analysis,\nvia a single end-to-end pass, while requiring $1.5\\times$ fewer GFLOPs. We\nreport competitive results on Kinetics-400 and present an ablation study of VTN\nproperties and the trade-off between accuracy and inference speed. We hope our\napproach will serve as a new baseline and start a fresh line of research in the\nvideo recognition domain. Code and models are available at:\nhttps://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md",
          "link": "http://arxiv.org/abs/2102.00719",
          "publishedOn": "2021-07-27T02:03:34.155Z",
          "wordCount": 604,
          "title": "Video Transformer Network. (arXiv:2102.00719v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "Learning a good representation for space-time correspondence is the key for\nvarious computer vision tasks, including tracking object bounding boxes and\nperforming video object pixel segmentation. To learn generalizable\nrepresentation for correspondence in large-scale, a variety of self-supervised\npretext tasks are proposed to explicitly perform object-level or patch-level\nsimilarity learning. Instead of following the previous literature, we propose\nto learn correspondence using Video Frame-level Similarity (VFS) learning, i.e,\nsimply learning from comparing video frames. Our work is inspired by the recent\nsuccess in image-level contrastive learning and similarity learning for visual\nrecognition. Our hypothesis is that if the representation is good for\nrecognition, it requires the convolutional features to find correspondence\nbetween similar objects or parts. Our experiments show surprising results that\nVFS surpasses state-of-the-art self-supervised approaches for both OTB visual\nobject tracking and DAVIS video object segmentation. We perform detailed\nanalysis on what matters in VFS and reveals new properties on image and frame\nlevel similarity learning. Project page is available at https://jerryxu.net/VFS",
          "link": "http://arxiv.org/abs/2103.17263",
          "publishedOn": "2021-07-27T02:03:34.148Z",
          "wordCount": 654,
          "title": "Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective. (arXiv:2103.17263v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial examples contain carefully crafted perturbations that can fool\ndeep neural networks (DNNs) into making wrong predictions. Enhancing the\nadversarial robustness of DNNs has gained considerable interest in recent\nyears. Although image transformation-based defenses were widely considered at\nan earlier time, most of them have been defeated by adaptive attacks. In this\npaper, we propose a new image transformation defense based on error diffusion\nhalftoning, and combine it with adversarial training to defend against\nadversarial examples. Error diffusion halftoning projects an image into a 1-bit\nspace and diffuses quantization error to neighboring pixels. This process can\nremove adversarial perturbations from a given image while maintaining\nacceptable image quality in the meantime in favor of recognition. Experimental\nresults demonstrate that the proposed method is able to improve adversarial\nrobustness even under advanced adaptive attacks, while most of the other image\ntransformation-based defenses do not. We show that a proper image\ntransformation can still be an effective defense approach. Code:\nhttps://github.com/shaoyuanlo/Halftoning-Defense",
          "link": "http://arxiv.org/abs/2101.09451",
          "publishedOn": "2021-07-27T02:03:34.140Z",
          "wordCount": 654,
          "title": "Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05245",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_S/0/1/0/all/0/1\">Shunhui Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zewen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xinyu Zhou</a>",
          "description": "The pandemic of COVID-19 has caused millions of infections, which has led to\na great loss all over the world, socially and economically. Due to the\nfalse-negative rate and the time-consuming of the conventional Reverse\nTranscription Polymerase Chain Reaction (RT-PCR) tests, diagnosing based on\nX-ray images and Computed Tomography (CT) images has been widely adopted.\nTherefore, researchers of the computer vision area have developed many\nautomatic diagnosing models based on machine learning or deep learning to\nassist the radiologists and improve the diagnosing accuracy. In this paper, we\npresent a review of these recently emerging automatic diagnosing models. 70\nmodels proposed from February 14, 2020, to July 21, 2020, are involved. We\nanalyzed the models from the perspective of preprocessing, feature extraction,\nclassification, and evaluation. Based on the limitation of existing models, we\npointed out that domain adaption in transfer learning and interpretability\npromotion would be the possible future directions.",
          "link": "http://arxiv.org/abs/2006.05245",
          "publishedOn": "2021-07-27T02:03:34.133Z",
          "wordCount": 691,
          "title": "A Review of Automated Diagnosis of COVID-19 Based on Scanning Images. (arXiv:2006.05245v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1\">Fangbo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Haonan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bly_R/0/1/0/all/0/1\">Randall A. Bly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moe_K/0/1/0/all/0/1\">Kris S. Moe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannaford_B/0/1/0/all/0/1\">Blake Hannaford</a>",
          "description": "Deep learning-based methods have achieved promising results on surgical\ninstrument segmentation. However, the high computation cost may limit the\napplication of deep models to time-sensitive tasks such as online surgical\nvideo analysis for robotic-assisted surgery. Moreover, current methods may\nstill suffer from challenging conditions in surgical images such as various\nlighting conditions and the presence of blood. We propose a novel Multi-frame\nFeature Aggregation (MFFA) module to aggregate video frame features temporally\nand spatially in a recurrent mode. By distributing the computation load of deep\nfeature extraction over sequential frames, we can use a lightweight encoder to\nreduce the computation costs at each time step. Moreover, public surgical\nvideos usually are not labeled frame by frame, so we develop a method that can\nrandomly synthesize a surgical frame sequence from a single labeled frame to\nassist network training. We demonstrate that our approach achieves superior\nperformance to corresponding deeper segmentation models on two public surgery\ndatasets.",
          "link": "http://arxiv.org/abs/2011.08752",
          "publishedOn": "2021-07-27T02:03:34.126Z",
          "wordCount": 643,
          "title": "Multi-frame Feature Aggregation for Real-time Instrument Segmentation in Endoscopic Video. (arXiv:2011.08752v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>",
          "description": "We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n\nHaving all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n\nWe solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article. Just like for linear convolution a\nmorphological convolution is specified by a kernel that we train in our\nPDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling\nand ReLUs as they are already subsumed by morphological convolutions.\n\nWe present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.",
          "link": "http://arxiv.org/abs/2001.09046",
          "publishedOn": "2021-07-27T02:03:34.102Z",
          "wordCount": 767,
          "title": "PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schutte_A/0/1/0/all/0/1\">August DuMont Sch&#xfc;tte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hetzel_J/0/1/0/all/0/1\">J&#xfc;rgen Hetzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hepp_T/0/1/0/all/0/1\">Tobias Hepp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dietz_B/0/1/0/all/0/1\">Benedikt Dietz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwab_P/0/1/0/all/0/1\">Patrick Schwab</a>",
          "description": "Privacy concerns around sharing personally identifiable information are a\nmajor practical barrier to data sharing in medical research. However, in many\ncases, researchers have no interest in a particular individual's information\nbut rather aim to derive insights at the level of cohorts. Here, we utilize\nGenerative Adversarial Networks (GANs) to create derived medical imaging\ndatasets consisting entirely of synthetic patient data. The synthetic images\nideally have, in aggregate, similar statistical properties to those of a source\ndataset but do not contain sensitive personal information. We assess the\nquality of synthetic data generated by two GAN models for chest radiographs\nwith 14 different radiology findings and brain computed tomography (CT) scans\nwith six types of intracranial hemorrhages. We measure the synthetic image\nquality by the performance difference of predictive models trained on either\nthe synthetic or the real dataset. We find that synthetic data performance\ndisproportionately benefits from a reduced number of unique label combinations.\nOur open-source benchmark also indicates that at low number of samples per\nclass, label overfitting effects start to dominate GAN training. We\nadditionally conducted a reader study in which trained radiologists do not\nperform better than random on discriminating between synthetic and real medical\nimages for intermediate levels of resolutions. In accordance with our benchmark\nresults, the classification accuracy of radiologists increases at higher\nspatial resolution levels. Our study offers valuable guidelines and outlines\npractical conditions under which insights derived from synthetic medical images\nare similar to those that would have been derived from real imaging data. Our\nresults indicate that synthetic data sharing may be an attractive and\nprivacy-preserving alternative to sharing real patient-level data in the right\nsettings.",
          "link": "http://arxiv.org/abs/2012.03769",
          "publishedOn": "2021-07-27T02:03:34.095Z",
          "wordCount": 752,
          "title": "Overcoming Barriers to Data Sharing with Medical Image Generation: A Comprehensive Evaluation. (arXiv:2012.03769v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1\">Yunfeng Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1\">Tianjia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong-Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>",
          "description": "Skeletal motion plays a vital role in human activity recognition as either an\nindependent data source or a complement. The robustness of skeleton-based\nactivity recognizers has been questioned recently, which shows that they are\nvulnerable to adversarial attacks when the full-knowledge of the recognizer is\naccessible to the attacker. However, this white-box requirement is overly\nrestrictive in most scenarios and the attack is not truly threatening. In this\npaper, we show that such threats do exist under black-box settings too. To this\nend, we propose the first black-box adversarial attack method BASAR. Through\nBASAR, we show that adversarial attack is not only truly a threat but also can\nbe extremely deceitful, because on-manifold adversarial samples are rather\ncommon in skeletal motions, in contrast to the common belief that adversarial\nsamples only exist off-manifold. Through exhaustive evaluation and comparison,\nwe show that BASAR can deliver successful attacks across models, data, and\nattack modes. Through harsh perceptual studies, we show that it achieves\neffective yet imperceptible attacks. By analyzing the attack on different\nactivity recognizers, BASAR helps identify the potential causes of their\nvulnerability and provides insights on what classifiers are likely to be more\nrobust against attack. Code is available at\nhttps://github.com/realcrane/BASAR-Black-box-Attack-on-Skeletal-Action-Recognition.",
          "link": "http://arxiv.org/abs/2103.05266",
          "publishedOn": "2021-07-27T02:03:34.087Z",
          "wordCount": 709,
          "title": "BASAR:Black-box Attack on Skeletal Action Recognition. (arXiv:2103.05266v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.11265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1\">Ashkan Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monadjemi_A/0/1/0/all/0/1\">Amirhassan Monadjemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Leyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbani_H/0/1/0/all/0/1\">Hossein Rabbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noormohammadi_N/0/1/0/all/0/1\">Neda Noormohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>",
          "description": "The data-driven sparse methods such as synthesis dictionary learning (e.g.,\nK-SVD) and sparsifying transform learning have been proven effective in image\ndenoising. However, they are intrinsically single-scale which can lead to\nsuboptimal results. We propose two methods developed based on wavelet subbands\nmixing to efficiently combine the merits of both single and multiscale methods.\nWe show that an efficient multiscale method can be devised without the need for\ndenoising detail subbands which substantially reduces the runtime. The proposed\nmethods are initially derived within the framework of sparsifying transform\nlearning denoising, and then, they are generalized to propose our multiscale\nextensions for the well-known K-SVD and SAIST image denoising methods. We\nanalyze and assess the studied methods thoroughly and compare them with the\nwell-known and state-of-the-art methods. The experiments show that our methods\nare able to offer good trade-offs between performance and complexity.",
          "link": "http://arxiv.org/abs/2003.11265",
          "publishedOn": "2021-07-27T02:03:34.067Z",
          "wordCount": 643,
          "title": "Multiscale Sparsifying Transform Learning for Image Denoising. (arXiv:2003.11265v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Friedland_S/0/1/0/all/0/1\">Shmuel Friedland</a>",
          "description": "We study the optimal transport problem for $d>2$ discrete measures. This is a\nlinear programming problem on $d$-tensors. It gives a way to compute a\n\"distance\" between two sets of discrete measures. We introduce an entropic\nregularization term, which gives rise to a scaling of tensors. We give a\nvariation of the celebrated Sinkhorn scaling algorithm. We show that this\nalgorithm can be viewed as a partial minimization algorithm of a strictly\nconvex function. Under appropriate conditions the rate of convergence is\ngeometric and we estimate the rate. Our results are generalizations of known\nresults for the classical case of two discrete measures.",
          "link": "http://arxiv.org/abs/2005.00945",
          "publishedOn": "2021-07-27T02:03:34.053Z",
          "wordCount": 594,
          "title": "Tensor optimal transport, distance between sets of measures and tensor scaling. (arXiv:2005.00945v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebel_P/0/1/0/all/0/1\">Patrick Ebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Most change detection methods assume that pre-change and post-change images\nare acquired by the same sensor. However, in many real-life scenarios, e.g.,\nnatural disaster, it is more practical to use the latest available images\nbefore and after the occurrence of incidence, which may be acquired using\ndifferent sensors. In particular, we are interested in the combination of the\nimages acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR\nimages appear vastly different from the optical images even when capturing the\nsame scene. Adding to this, change detection methods are often constrained to\nuse only target image-pair, no labeled data, and no additional unlabeled data.\nSuch constraints limit the scope of traditional supervised machine learning and\nunsupervised generative approaches for multi-sensor change detection. Recent\nrapid development of self-supervised learning methods has shown that some of\nthem can even work with only few images. Motivated by this, in this work we\npropose a method for multi-sensor change detection using only the unlabeled\ntarget bi-temporal images that are used for training a network in\nself-supervised fashion by using deep clustering and contrastive learning. The\nproposed method is evaluated on four multi-modal bi-temporal scenes showing\nchange and the benefits of our self-supervised approach are demonstrated.",
          "link": "http://arxiv.org/abs/2103.05102",
          "publishedOn": "2021-07-27T02:03:33.911Z",
          "wordCount": 667,
          "title": "Self-supervised Multisensor Change Detection. (arXiv:2103.05102v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Bi-Level Optimization (BLO) is originated from the area of economic game\ntheory and then introduced into the optimization community. BLO is able to\nhandle problems with a hierarchical structure, involving two levels of\noptimization tasks, where one task is nested inside the other. In machine\nlearning and computer vision fields, despite the different motivations and\nmechanisms, a lot of complex problems, such as hyper-parameter optimization,\nmulti-task and meta-learning, neural architecture search, adversarial learning\nand deep reinforcement learning, actually all contain a series of closely\nrelated subproblms. In this paper, we first uniformly express these complex\nlearning and vision problems from the perspective of BLO. Then we construct a\nbest-response-based single-level reformulation and establish a unified\nalgorithmic framework to understand and formulate mainstream gradient-based BLO\nmethodologies, covering aspects ranging from fundamental automatic\ndifferentiation schemes to various accelerations, simplifications, extensions\nand their convergence and complexity properties. Last but not least, we discuss\nthe potentials of our unified BLO framework for designing new algorithms and\npoint out some promising directions for future research.",
          "link": "http://arxiv.org/abs/2101.11517",
          "publishedOn": "2021-07-27T02:03:33.881Z",
          "wordCount": 664,
          "title": "Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond. (arXiv:2101.11517v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hung-Yueh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chin-Tang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1\">Ching-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Most 3D neural networks are trained from scratch owing to the lack of\nlarge-scale labeled datasets. In this paper, we present a novel 3D pretraining\nmethod by leveraging 2D networks learned from rich 2D datasets. We propose the\ncontrastive pixel-to-point knowledge transfer to effectively utilize the 2D\ninformation by mapping the pixel-level and point-level features into the same\nembedding space. Due to the heterogeneous nature between 2D and 3D networks, we\nintroduce the back-projection function to align the features between 2D and 3D\nto make the transfer possible. Additionally, we devise an upsampling feature\nprojection layer to increase the spatial resolution of high-level 2D feature\nmaps, which helps learning fine-grained 3D representations. With a pretrained\n2D network, the proposed pretraining process requires no additional 2D or 3D\nlabeled data, further alleviating the expansive 3D data annotation cost. To the\nbest of our knowledge, we are the first to exploit existing 2D trained weights\nto pretrain 3D deep neural networks. Our intensive experiments show that the 3D\nmodels pretrained with 2D knowledge boost the performances across various\nreal-world 3D downstream tasks.",
          "link": "http://arxiv.org/abs/2104.04687",
          "publishedOn": "2021-07-27T02:03:33.862Z",
          "wordCount": 660,
          "title": "Learning from 2D: Contrastive Pixel-to-Point Knowledge Transfer for 3D Pretraining. (arXiv:2104.04687v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1\">Biswadeep Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_X/0/1/0/all/0/1\">Xueyuan She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1\">Saibal Mukhopadhyay</a>",
          "description": "This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for\nenergy-efficient and robust object detection in resource-constrained platforms.\nThe network architecture is based on Convolutional SNN using\nleaky-integrate-fire neuron models. The model combines unsupervised Spike\nTime-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning\nmethods and also uses Monte Carlo Dropout to get an estimate of the uncertainty\nerror. FSHNN provides better accuracy compared to DNN based object detectors\nwhile being 150X energy-efficient. It also outperforms these object detectors,\nwhen subjected to noisy input data and less labeled training data with a lower\nuncertainty error.",
          "link": "http://arxiv.org/abs/2104.10719",
          "publishedOn": "2021-07-27T02:03:33.813Z",
          "wordCount": 576,
          "title": "A Fully Spiking Hybrid Neural Network for Energy-Efficient Object Detection. (arXiv:2104.10719v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>",
          "description": "One-class novelty detection is to identify anomalous instances that do not\nconform to the expected normal instances. In this paper, the Generative\nAdversarial Networks (GANs) based on encoder-decoder-encoder pipeline are used\nfor detection and achieve state-of-the-art performance. However, deep neural\nnetworks are too over-parameterized to deploy on resource-limited devices.\nTherefore, Progressive Knowledge Distillation with GANs (PKDGAN) is proposed to\nlearn compact and fast novelty detection networks. The P-KDGAN is a novel\nattempt to connect two standard GANs by the designed distillation loss for\ntransferring knowledge from the teacher to the student. The progressive\nlearning of knowledge distillation is a two-step approach that continuously\nimproves the performance of the student GAN and achieves better performance\nthan single step methods. In the first step, the student GAN learns the basic\nknowledge totally from the teacher via guiding of the pretrained teacher GAN\nwith fixed weights. In the second step, joint fine-training is adopted for the\nknowledgeable teacher and student GANs to further improve the performance and\nstability. The experimental results on CIFAR-10, MNIST, and FMNIST show that\nour method improves the performance of the student GAN by 2.44%, 1.77%, and\n1.73% when compressing the computation at ratios of 24.45:1, 311.11:1, and\n700:1, respectively.",
          "link": "http://arxiv.org/abs/2007.06963",
          "publishedOn": "2021-07-27T02:03:33.804Z",
          "wordCount": 674,
          "title": "P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection. (arXiv:2007.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1812.04275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hangyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>",
          "description": "This paper proposes a novel approach for Sketch-Based Image Retrieval (SBIR),\nfor which the key is to bridge the gap between sketches and photos in terms of\nthe data representation. Inspired by channel-wise attention explored in recent\nyears, we present a Domain-Aware Squeeze-and-Excitation (DASE) network, which\nseamlessly incorporates the prior knowledge of sample sketch or photo into SE\nmodule and make the SE module capable of emphasizing appropriate channels\naccording to domain signal. Accordingly, the proposed network can switch its\nmode to achieve a better domain feature with lower intra-class discrepancy.\nMoreover, while previous works simply focus on minimizing intra-class distance\nand maximizing inter-class distance, we introduce a loss function, named\nMultiplicative Euclidean Margin Softmax (MEMS), which introduces multiplicative\nEuclidean margin into feature space and ensure that the maximum intra-class\ndistance is smaller than the minimum inter-class distance. This facilitates\nlearning a highly discriminative feature space and ensures a more accurate\nimage retrieval result. Extensive experiments are conducted on two widely used\nSBIR benchmark datasets. Our approach achieves better results on both datasets,\nsurpassing the state-of-the-art methods by a large margin.",
          "link": "http://arxiv.org/abs/1812.04275",
          "publishedOn": "2021-07-27T02:03:33.798Z",
          "wordCount": 664,
          "title": "Domain-Aware SE Network for Sketch-based Image Retrieval with Multiplicative Euclidean Margin Softmax. (arXiv:1812.04275v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00113",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelly_D/0/1/0/all/0/1\">Damien Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorado_I/0/1/0/all/0/1\">Ignacio Garcia Dorado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>",
          "description": "Could we compress images via standard codecs while avoiding visible\nartifacts? The answer is obvious -- this is doable as long as the bit budget is\ngenerous enough. What if the allocated bit-rate for compression is\ninsufficient? Then unfortunately, artifacts are a fact of life. Many attempts\nwere made over the years to fight this phenomenon, with various degrees of\nsuccess. In this work we aim to break the unholy connection between bit-rate\nand image quality, and propose a way to circumvent compression artifacts by\npre-editing the incoming image and modifying its content to fit the given bits.\nWe design this editing operation as a learned convolutional neural network, and\nformulate an optimization problem for its training. Our loss takes into account\na proximity between the original image and the edited one, a bit-budget penalty\nover the proposed image, and a no-reference image quality measure for forcing\nthe outcome to be visually pleasing. The proposed approach is demonstrated on\nthe popular JPEG compression, showing savings in bits and/or improvements in\nvisual quality, obtained with intricate editing effects.",
          "link": "http://arxiv.org/abs/2002.00113",
          "publishedOn": "2021-07-27T02:03:33.790Z",
          "wordCount": 649,
          "title": "Better Compression with Deep Pre-Editing. (arXiv:2002.00113v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.12618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Golts_A/0/1/0/all/0/1\">Alex Golts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schechner_Y/0/1/0/all/0/1\">Yoav Y. Schechner</a>",
          "description": "Computer vision tasks are often expected to be executed on compressed images.\nClassical image compression standards like JPEG 2000 are widely used. However,\nthey do not account for the specific end-task at hand. Motivated by works on\nrecurrent neural network (RNN)-based image compression and three-dimensional\n(3D) reconstruction, we propose unified network architectures to solve both\ntasks jointly. These joint models provide image compression tailored for the\nspecific task of 3D reconstruction. Images compressed by our proposed models,\nyield 3D reconstruction performance superior as compared to using JPEG 2000\ncompression. Our models significantly extend the range of compression rates for\nwhich 3D reconstruction is possible. We also show that this can be done highly\nefficiently at almost no additional cost to obtain compression on top of the\ncomputation already required for performing the 3D reconstruction task.",
          "link": "http://arxiv.org/abs/2003.12618",
          "publishedOn": "2021-07-27T02:03:33.772Z",
          "wordCount": 605,
          "title": "Image compression optimized for 3D reconstruction by utilizing deep neural networks. (arXiv:2003.12618v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_I/0/1/0/all/0/1\">Icaro O. de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_K/0/1/0/all/0/1\">Keiko V. O. Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "This work addresses the problem of vehicle identification through\nnon-overlapping cameras. As our main contribution, we introduce a novel dataset\nfor vehicle identification, called Vehicle-Rear, that contains more than three\nhours of high-resolution videos, with accurate information about the make,\nmodel, color and year of nearly 3,000 vehicles, in addition to the position and\nidentification of their license plates. To explore our dataset we design a\ntwo-stream CNN that simultaneously uses two of the most distinctive and\npersistent features available: the vehicle's appearance and its license plate.\nThis is an attempt to tackle a major problem: false alarms caused by vehicles\nwith similar designs or by very close license plate identifiers. In the first\nnetwork stream, shape similarities are identified by a Siamese CNN that uses a\npair of low-resolution vehicle patches recorded by two different cameras. In\nthe second stream, we use a CNN for OCR to extract textual information,\nconfidence scores, and string similarities from a pair of high-resolution\nlicense plate patches. Then, features from both streams are merged by a\nsequence of fully connected layers for decision. In our experiments, we\ncompared the two-stream network against several well-known CNN architectures\nusing single or multiple vehicle features. The architectures, trained models,\nand dataset are publicly available at https://github.com/icarofua/vehicle-rear.",
          "link": "http://arxiv.org/abs/1911.05541",
          "publishedOn": "2021-07-27T02:03:33.765Z",
          "wordCount": 721,
          "title": "Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks. (arXiv:1911.05541v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aftab_A/0/1/0/all/0/1\">Abdul Rafey Aftab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beeck_M/0/1/0/all/0/1\">Michael von der Beeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrhirsch_S/0/1/0/all/0/1\">Steven Rohrhirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diotte_B/0/1/0/all/0/1\">Benoit Diotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feld_M/0/1/0/all/0/1\">Michael Feld</a>",
          "description": "There is a growing interest in more intelligent natural user interaction with\nthe car. Hand gestures and speech are already being applied for driver-car\ninteraction. Moreover, multimodal approaches are also showing promise in the\nautomotive industry. In this paper, we utilize deep learning for a multimodal\nfusion network for referencing objects outside the vehicle. We use features\nfrom gaze, head pose and finger pointing simultaneously to precisely predict\nthe referenced objects in different car poses. We demonstrate the practical\nlimitations of each modality when used for a natural form of referencing,\nspecifically inside the car. As evident from our results, we overcome the\nmodality specific limitations, to a large extent, by the addition of other\nmodalities. This work highlights the importance of multimodal sensing,\nespecially when moving towards natural user interaction. Furthermore, our user\nbased analysis shows noteworthy differences in recognition of user behavior\ndepending upon the vehicle pose.",
          "link": "http://arxiv.org/abs/2107.12167",
          "publishedOn": "2021-07-27T02:03:33.756Z",
          "wordCount": 603,
          "title": "Multimodal Fusion Using Deep Learning Applied to Driver's Referencing of Outside-Vehicle Objects. (arXiv:2107.12167v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Modeling the hand-object (HO) interaction not only requires estimation of the\nHO pose, but also pays attention to the contact due to their interaction.\nSignificant progress has been made in estimating hand and object separately\nwith deep learning methods, simultaneous HO pose estimation and contact\nmodeling has not yet been fully explored. In this paper, we present an explicit\ncontact representation namely Contact Potential Field (CPF), and a\nlearning-fitting hybrid framework namely MIHO to Modeling the Interaction of\nHand and Object. In CPF, we treat each contacting HO vertex pair as a\nspring-mass system. Hence the whole system forms a potential field with minimal\nelastic energy at the grasp position. Extensive experiments on the two commonly\nused benchmarks have demonstrated that our method can achieve state-of-the-art\nin several reconstruction metrics, and allow us to produce more physically\nplausible HO pose even when the ground-truth exhibits severe interpenetration\nor disjointedness. Our code is available at https://github.com/lixiny/CPF.",
          "link": "http://arxiv.org/abs/2012.00924",
          "publishedOn": "2021-07-27T02:03:33.725Z",
          "wordCount": 643,
          "title": "CPF: Learning a Contact Potential Field to Model the Hand-object Interaction. (arXiv:2012.00924v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1\">Norman Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Frank Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorundo_E/0/1/0/all/0/1\">Evan Dorundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1\">Rahul Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tyler Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1\">Samyak Parajuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mike Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1\">Justin Gilmer</a>",
          "description": "We introduce four new real-world distribution shift datasets consisting of\nchanges in image style, image blurriness, geographic location, camera\noperation, and more. With our new datasets, we take stock of previously\nproposed methods for improving out-of-distribution robustness and put them to\nthe test. We find that using larger models and artificial data augmentations\ncan improve robustness on real-world distribution shifts, contrary to claims in\nprior work. We find improvements in artificial robustness benchmarks can\ntransfer to real-world distribution shifts, contrary to claims in prior work.\nMotivated by our observation that data augmentations can help with real-world\ndistribution shifts, we also introduce a new data augmentation method which\nadvances the state-of-the-art and outperforms models pretrained with 1000 times\nmore labeled data. Overall we find that some methods consistently help with\ndistribution shifts in texture and local image statistics, but these methods do\nnot help with some other distribution shifts like geographic changes. Our\nresults show that future research must study multiple distribution shifts\nsimultaneously, as we demonstrate that no evaluated method consistently\nimproves robustness.",
          "link": "http://arxiv.org/abs/2006.16241",
          "publishedOn": "2021-07-27T02:03:33.719Z",
          "wordCount": 694,
          "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. (arXiv:2006.16241v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11687",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hahne_C/0/1/0/all/0/1\">Christopher Hahne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aggoun_A/0/1/0/all/0/1\">Amar Aggoun</a>",
          "description": "Light-field cameras play a vital role for rich 3-D information retrieval in\nnarrow range depth sensing applications. The key obstacle in composing\nlight-fields from exposures taken by a plenoptic camera is to computationally\ncalibrate, align and rearrange four-dimensional image data. Several attempts\nhave been proposed to enhance the overall image quality by tailoring pipelines\ndedicated to particular plenoptic cameras and improving the consistency across\nviewpoints at the expense of high computational loads. The framework presented\nherein advances prior outcomes thanks to its novel micro image scale-space\nanalysis for generic camera calibration independent of the lens specifications\nand its parallax-invariant, cost-effective viewpoint color equalization from\noptimal transport theory. Artifacts from the sensor and micro lens grid are\ncompensated in an innovative way to enable superior quality in sub-aperture\nimage extraction, computational refocusing and Scheimpflug rendering with\nsub-sampling capabilities. Benchmark comparisons using established image\nmetrics suggest that our proposed pipeline outperforms state-of-the-art tool\nchains in the majority of cases. Results from a Wasserstein distance further\nshow that our color transfer outdoes the existing transport methods. Our\nalgorithms are released under an open-source license, offer cross-platform\ncompatibility with few dependencies and different user interfaces. This makes\nthe reproduction of results and experimentation with plenoptic camera\ntechnology convenient for peer researchers, developers, photographers, data\nscientists and others working in this field.",
          "link": "http://arxiv.org/abs/2010.11687",
          "publishedOn": "2021-07-27T02:03:33.711Z",
          "wordCount": 704,
          "title": "PlenoptiCam v1.0: A light-field imaging framework. (arXiv:2010.11687v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Ye Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zehua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>",
          "description": "Coarsely-labeled semantic segmentation annotations are easy to obtain, but\ntherefore bear the risk of losing edge details and introducing background\npixels. Impeded by the inherent noise, existing coarse annotations are only\ntaken as a bonus for model pre-training. In this paper, we try to exploit their\npotentials with a confidence-based reweighting strategy. To expand, loss-based\nreweighting strategies usually take the high loss value to identify two\ncompletely different types of pixels, namely, valuable pixels in noise-free\nannotations and mislabeled pixels in noisy annotations. This makes it\nimpossible to perform two tasks of mining valuable pixels and suppressing\nmislabeled pixels at the same time. However, with the help of the prediction\nconfidence, we successfully solve this dilemma and simultaneously perform two\nsubtasks with a single reweighting strategy. Furthermore, we generalize this\nstrategy into an Adversarial Reweighting Module (ARM) and prove its convergence\nstrictly. Experiments on standard datasets shows our ARM can bring consistent\nimprovements for both coarse annotations and fine annotations. Specifically,\nbuilt on top of DeepLabv3+, ARM improves the mIoU on the coarsely-labeled\nCityscapes by a considerable margin and increases the mIoU on the ADE20K\ndataset to 47.50.",
          "link": "http://arxiv.org/abs/2009.05205",
          "publishedOn": "2021-07-27T02:03:33.697Z",
          "wordCount": 659,
          "title": "ARM: A Confidence-Based Adversarial Reweighting Module for Coarse Semantic Segmentation. (arXiv:2009.05205v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milani_F/0/1/0/all/0/1\">Federico Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraternali_P/0/1/0/all/0/1\">Piero Fraternali</a>",
          "description": "Iconography in art is the discipline that studies the visual content of\nartworks to determine their motifs and themes andto characterize the way these\nare represented. It is a subject of active research for a variety of purposes,\nincluding the interpretation of meaning, the investigation of the origin and\ndiffusion in time and space of representations, and the study of influences\nacross artists and art works. With the proliferation of digital archives of art\nimages, the possibility arises of applying Computer Vision techniques to the\nanalysis of art images at an unprecedented scale, which may support iconography\nresearch and education. In this paper we introduce a novel paintings data set\nfor iconography classification and present the quantitativeand qualitative\nresults of applying a Convolutional Neural Network (CNN) classifier to the\nrecognition of the iconography of artworks. The proposed classifier achieves\ngood performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73%\nAverage Precision) in the task of identifying saints in Christian religious\npaintings, a task made difficult by the presence of classes with very similar\nvisual features. Qualitative analysis of the results shows that the CNN focuses\non the traditional iconic motifs that characterize the representation of each\nsaint and exploits such hints to attain correct identification. The ultimate\ngoal of our work is to enable the automatic extraction, decomposition, and\ncomparison of iconography elements to support iconographic studies and\nautomatic art work annotation.",
          "link": "http://arxiv.org/abs/2010.11697",
          "publishedOn": "2021-07-27T02:03:33.679Z",
          "wordCount": 739,
          "title": "A Data Set and a Convolutional Model for Iconography Classification in Paintings. (arXiv:2010.11697v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunfeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Ying Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>",
          "description": "Graph convolutional networks (GCNs) have been widely used and achieved\nremarkable results in skeleton-based action recognition. In GCNs, graph\ntopology dominates feature aggregation and therefore is the key to extracting\nrepresentative features. In this work, we propose a novel Channel-wise Topology\nRefinement Graph Convolution (CTR-GC) to dynamically learn different topologies\nand effectively aggregate joint features in different channels for\nskeleton-based action recognition. The proposed CTR-GC models channel-wise\ntopologies through learning a shared topology as a generic prior for all\nchannels and refining it with channel-specific correlations for each channel.\nOur refinement method introduces few extra parameters and significantly reduces\nthe difficulty of modeling channel-wise topologies. Furthermore, via\nreformulating graph convolutions into a unified form, we find that CTR-GC\nrelaxes strict constraints of graph convolutions, leading to stronger\nrepresentation capability. Combining CTR-GC with temporal modeling modules, we\ndevelop a powerful graph convolutional network named CTR-GCN which notably\noutperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and\nNW-UCLA datasets.",
          "link": "http://arxiv.org/abs/2107.12213",
          "publishedOn": "2021-07-27T02:03:33.672Z",
          "wordCount": 611,
          "title": "Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition. (arXiv:2107.12213v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardaoui_D/0/1/0/all/0/1\">Dina Mardaoui</a>",
          "description": "The performance of modern algorithms on certain computer vision tasks such as\nobject recognition is now close to that of humans. This success was achieved at\nthe price of complicated architectures depending on millions of parameters and\nit has become quite challenging to understand how particular predictions are\nmade. Interpretability methods propose to give us this understanding. In this\npaper, we study LIME, perhaps one of the most popular. On the theoretical side,\nwe show that when the number of generated examples is large, LIME explanations\nare concentrated around a limit explanation for which we give an explicit\nexpression. We further this study for elementary shape detectors and linear\nmodels. As a consequence of this analysis, we uncover a connection between LIME\nand integrated gradients, another explanation method. More precisely, the LIME\nexplanations are similar to the sum of integrated gradients over the\nsuperpixels used in the preprocessing step of LIME.",
          "link": "http://arxiv.org/abs/2102.06307",
          "publishedOn": "2021-07-27T02:03:33.665Z",
          "wordCount": 617,
          "title": "What does LIME really see in images?. (arXiv:2102.06307v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>",
          "description": "When humans solve complex problems, they rarely come up with a decision\nright-away. Instead, they start with an intuitive decision, reflect upon it,\nspot mistakes, resolve contradictions and jump between different hypotheses.\nThus, they create a sequence of ideas and follow a train of thought that\nultimately reaches a conclusive decision. Contrary to this, today's neural\nclassification models are mostly trained to map an input to one single and\nfixed output. In this paper, we investigate how we can give models the\nopportunity of a second, third and $k$-th thought. We take inspiration from\nHegel's dialectics and propose a method that turns an existing classifier's\nclass prediction (such as the image class forest) into a sequence of\npredictions (such as forest $\\rightarrow$ tree $\\rightarrow$ mushroom).\nConcretely, we propose a correction module that is trained to estimate the\nmodel's correctness as well as an iterative prediction update based on the\nprediction's gradient. Our approach results in a dynamic system over class\nprobability distributions $\\unicode{x2014}$ the thought flow. We evaluate our\nmethod on diverse datasets and tasks from computer vision and natural language\nprocessing. We observe surprisingly complex but intuitive behavior and\ndemonstrate that our method (i) can correct misclassifications, (ii)\nstrengthens model performance, (iii) is robust to high levels of adversarial\nattacks, (iv) can increase accuracy up to 4% in a label-distribution-shift\nsetting and (iv) provides a tool for model interpretability that uncovers model\nknowledge which otherwise remains invisible in a single distribution\nprediction.",
          "link": "http://arxiv.org/abs/2107.12220",
          "publishedOn": "2021-07-27T02:03:33.658Z",
          "wordCount": 694,
          "title": "Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.07999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Angelina Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ryan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleiman_A/0/1/0/all/0/1\">Anat Kleiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_L/0/1/0/all/0/1\">Leslie Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dora Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirai_I/0/1/0/all/0/1\">Iroha Shirai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Arvind Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>",
          "description": "Machine learning models are known to perpetuate and even amplify the biases\npresent in the data. However, these data biases frequently do not become\napparent until after the models are deployed. Our work tackles this issue and\nenables the preemptive analysis of large-scale datasets. REVISE (REvealing\nVIsual biaSEs) is a tool that assists in the investigation of a visual dataset,\nsurfacing potential biases along three dimensions: (1) object-based, (2)\nperson-based, and (3) geography-based. Object-based biases relate to the size,\ncontext, or diversity of the depicted objects. Person-based metrics focus on\nanalyzing the portrayal of people within the dataset. Geography-based analyses\nconsider the representation of different geographic locations. These three\ndimensions are deeply intertwined in how they interact to bias a dataset, and\nREVISE sheds light on this; the responsibility then lies with the user to\nconsider the cultural and historical context, and to determine which of the\nrevealed biases may be problematic. The tool further assists the user by\nsuggesting actionable steps that may be taken to mitigate the revealed biases.\nOverall, the key aim of our work is to tackle the machine learning bias problem\nearly in the pipeline. REVISE is available at\nhttps://github.com/princetonvisualai/revise-tool",
          "link": "http://arxiv.org/abs/2004.07999",
          "publishedOn": "2021-07-27T02:03:33.652Z",
          "wordCount": 702,
          "title": "REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets. (arXiv:2004.07999v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1\">Bilal Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Baz_A/0/1/0/all/0/1\">Ayman El-Baz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>",
          "description": "Prostate cancer (PCa) is the second deadliest form of cancer in males, and it\ncan be clinically graded by examining the structural representations of Gleason\ntissues. This paper proposes \\RV{a new method} for segmenting the Gleason\ntissues \\RV{(patch-wise) in order to grade PCa from the whole slide images\n(WSI).} Also, the proposed approach encompasses two main contributions: 1) A\nsynergy of hybrid dilation factors and hierarchical decomposition of latent\nspace representation for effective Gleason tissues extraction, and 2) A\nthree-tiered loss function which can penalize different semantic segmentation\nmodels for accurately extracting the highly correlated patterns. In addition to\nthis, the proposed framework has been extensively evaluated on a large-scale\nPCa dataset containing 10,516 whole slide scans (with around 71.7M patches),\nwhere it outperforms state-of-the-art schemes by 3.22% (in terms of mean\nintersection-over-union) for extracting the Gleason tissues and 6.91% (in terms\nof F1 score) for grading the progression of PCa.",
          "link": "http://arxiv.org/abs/2011.00527",
          "publishedOn": "2021-07-27T02:03:33.632Z",
          "wordCount": 682,
          "title": "A Dilated Residual Hierarchically Fashioned Segmentation Framework for Extracting Gleason Tissues and Grading Prostate Cancer from Whole Slide Images. (arXiv:2011.00527v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12535",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kang_H/0/1/0/all/0/1\">Hongtao Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_D/0/1/0/all/0/1\">Die Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1\">Weihua Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1\">Junbo Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_S/0/1/0/all/0/1\">Shaoqun Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quan_T/0/1/0/all/0/1\">Tingwei Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiuli Liu</a>",
          "description": "Stain normalization often refers to transferring the color distribution of\nthe source image to that of the target image and has been widely used in\nbiomedical image analysis. The conventional stain normalization is regarded as\nconstructing a pixel-by-pixel color mapping model, which only depends on one\nreference image, and can not accurately achieve the style transformation\nbetween image datasets. In principle, this style transformation can be well\nsolved by the deep learning-based methods due to its complicated network\nstructure, whereas, its complicated structure results in the low computational\nefficiency and artifacts in the style transformation, which has restricted the\npractical application. Here, we use distillation learning to reduce the\ncomplexity of deep learning methods and a fast and robust network called\nStainNet to learn the color mapping between the source image and target image.\nStainNet can learn the color mapping relationship from a whole dataset and\nadjust the color value in a pixel-to-pixel manner. The pixel-to-pixel manner\nrestricts the network size and avoids artifacts in the style transformation.\nThe results on the cytopathology and histopathology datasets show that StainNet\ncan achieve comparable performance to the deep learning-based methods.\nComputation results demonstrate StainNet is more than 40 times faster than\nStainGAN and can normalize a 100,000x100,000 whole slide image in 40 seconds.",
          "link": "http://arxiv.org/abs/2012.12535",
          "publishedOn": "2021-07-27T02:03:33.626Z",
          "wordCount": 718,
          "title": "StainNet: a fast and robust stain normalization network. (arXiv:2012.12535v6 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aiger_D/0/1/0/all/0/1\">Dror Aiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynen_S/0/1/0/all/0/1\">Simon Lynen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosang_J/0/1/0/all/0/1\">Jan Hosang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeisl_B/0/1/0/all/0/1\">Bernhard Zeisl</a>",
          "description": "Outlier rejection and equivalently inlier set optimization is a key\ningredient in numerous applications in computer vision such as filtering\npoint-matches in camera pose estimation or plane and normal estimation in point\nclouds. Several approaches exist, yet at large scale we face a combinatorial\nexplosion of possible solutions and state-of-the-art methods like RANSAC, Hough\ntransform or Branch\\&Bound require a minimum inlier ratio or prior knowledge to\nremain practical. In fact, for problems such as camera posing in very large\nscenes these approaches become useless as they have exponential runtime growth\nif these conditions aren't met. To approach the problem we present a efficient\nand general algorithm for outlier rejection based on \"intersecting\"\n$k$-dimensional surfaces in $R^d$. We provide a recipe for casting a variety of\ngeometric problems as finding a point in $R^d$ which maximizes the number of\nnearby surfaces (and thus inliers). The resulting algorithm has linear\nworst-case complexity with a better runtime dependency in the approximation\nfactor than competing algorithms while not requiring domain specific bounds.\nThis is achieved by introducing a space decomposition scheme that bounds the\nnumber of computations by successively rounding and grouping samples. Our\nrecipe (and open-source code) enables anybody to derive such fast approaches to\nnew problems across a wide range of domains. We demonstrate the versatility of\nthe approach on several camera posing problems with a high number of matches at\nlow inlier ratio achieving state-of-the-art results at significantly lower\nprocessing times.",
          "link": "http://arxiv.org/abs/2107.11810",
          "publishedOn": "2021-07-27T02:03:33.619Z",
          "wordCount": 678,
          "title": "Efficient Large Scale Inlier Voting for Geometric Vision Problems. (arXiv:2107.11810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1\">Hieu T. Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1\">Huy Q. Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>",
          "description": "Accurate insect pest recognition is significant to protect the crop or take\nthe early treatment on the infected yield, and it helps reduce the loss for the\nagriculture economy. Design an automatic pest recognition system is necessary\nbecause manual recognition is slow, time-consuming, and expensive. The\nImage-based pest classifier using the traditional computer vision method is not\nefficient due to the complexity. Insect pest classification is a difficult task\nbecause of various kinds, scales, shapes, complex backgrounds in the field, and\nhigh appearance similarity among insect species. With the rapid development of\ndeep learning technology, the CNN-based method is the best way to develop a\nfast and accurate insect pest classifier. We present different convolutional\nneural network-based models in this work, including attention, feature pyramid,\nand fine-grained models. We evaluate our methods on two public datasets: the\nlarge-scale insect pest dataset, the IP102 benchmark dataset, and a smaller\ndataset, namely D0 in terms of the macro-average precision (MPre), the\nmacro-average recall (MRec), the macro-average F1- score (MF1), the accuracy\n(Acc), and the geometric mean (GM). The experimental results show that\ncombining these convolutional neural network-based models can better perform\nthan the state-of-the-art methods on these two datasets. For instance, the\nhighest accuracy we obtained on IP102 and D0 is $74.13\\%$ and $99.78\\%$,\nrespectively, bypassing the corresponding state-of-the-art accuracy: $67.1\\%$\n(IP102) and $98.8\\%$ (D0). We also publish our codes for contributing to the\ncurrent research related to the insect pest classification problem.",
          "link": "http://arxiv.org/abs/2107.12189",
          "publishedOn": "2021-07-27T02:03:33.612Z",
          "wordCount": 695,
          "title": "An Efficient Insect Pest Classification Using Multiple Convolutional Neural Network Based Models. (arXiv:2107.12189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marek_M/0/1/0/all/0/1\">Martin Marek</a>",
          "description": "We introduce a new dataset for image-based parking space occupancy\nclassification: ACPDS. Unlike in prior datasets, each image is taken from a\nunique view, systematically annotated, and the parking lots in the train,\nvalidation, and test sets are unique. We use this dataset to propose a simple\nbaseline model for parking space occupancy classification, which achieves 98%\naccuracy on unseen parking lots, significantly outperforming existing models.\nWe share our dataset, code, and trained models under the MIT license.",
          "link": "http://arxiv.org/abs/2107.12207",
          "publishedOn": "2021-07-27T02:03:33.605Z",
          "wordCount": 508,
          "title": "Image-Based Parking Space Occupancy Classification: Dataset and Baseline. (arXiv:2107.12207v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>",
          "description": "We propose an efficient inference framework for semi-supervised video object\nsegmentation by exploiting the temporal redundancy of the video. Our method\nperforms inference on selected keyframes and makes predictions for other frames\nvia propagation based on motion vectors and residuals from the compressed video\nbitstream. Specifically, we propose a new motion vector-based warping method\nfor propagating segmentation masks from keyframes to other frames in a\nmulti-reference manner. Additionally, we propose a residual-based refinement\nmodule that can correct and add detail to the block-wise propagated\nsegmentation masks. Our approach is flexible and can be added on top of\nexisting video object segmentation algorithms. With STM with top-k filtering as\nour base model, we achieved highly competitive results on DAVIS16 and\nYouTube-VOS with substantial speedups of up to 4.9X with little loss in\naccuracy.",
          "link": "http://arxiv.org/abs/2107.12192",
          "publishedOn": "2021-07-27T02:03:33.599Z",
          "wordCount": 567,
          "title": "Efficient Video Object Segmentation with Compressed Video. (arXiv:2107.12192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12205",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bhandary_A/0/1/0/all/0/1\">Abhir Bhandary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+G_A/0/1/0/all/0/1\">Ananth Prabhu G</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basthikodi_M/0/1/0/all/0/1\">Mustafa Basthikodi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+M_C/0/1/0/all/0/1\">Chaitra K M</a>",
          "description": "Lung cancer begins in the lungs and leading to the reason of cancer demise\namid population in the creation. According to the American Cancer Society,\nwhich estimates about 27% of the deaths because of cancer. In the early phase\nof its evolution, lung cancer does not cause any symptoms usually. Many of the\npatients have been diagnosed in a developed phase where symptoms become more\nprominent, that results in poor curative treatment and high mortality rate.\nComputer Aided Detection systems are used to achieve greater accuracies for the\nlung cancer diagnosis. In this research exertion, we proposed a novel\nmethodology for lung Segmentation on the basis of Fuzzy C-Means Clustering,\nAdaptive Thresholding, and Segmentation of Active Contour Model. The\nexperimental results are analysed and presented.",
          "link": "http://arxiv.org/abs/2107.12205",
          "publishedOn": "2021-07-27T02:03:33.582Z",
          "wordCount": 608,
          "title": "Early Diagnosis of Lung Cancer Using Computer Aided Detection via Lung Segmentation Approach. (arXiv:2107.12205v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_C/0/1/0/all/0/1\">C. Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gazis_A/0/1/0/all/0/1\">A. Gazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schizas_D/0/1/0/all/0/1\">D. Schizas</a>",
          "description": "An important task at the onset of a laparoscopic cholecystectomy (LC)\noperation is the inspection of gallbladder (GB) to evaluate the thickness of\nits wall, presence of inflammation and extent of fat. Difficulty in\nvisualization of the GB wall vessels may be due to the previous factors,\npotentially as a result of chronic inflammation or other diseases. In this\npaper we propose a multiple-instance learning (MIL) technique for assessment of\nthe GB wall vascularity via computer-vision analysis of images from LC\noperations. The bags correspond to a labeled (low vs. high) vascularity dataset\nof 181 GB images, from 53 operations. The instances correspond to unlabeled\npatches extracted from these images. Each patch is represented by a vector with\ncolor, texture and statistical features. We compare various state-of-the-art\nMIL and single-instance learning approaches, as well as a proposed MIL\ntechnique based on variational Bayesian inference. The methods were compared\nfor two experimental tasks: image-based and video-based (i.e. patient-based)\nclassification. The proposed approach presents the best performance with\naccuracy 92.1% and 90.3% for the first and second task, respectively. A\nsignificant advantage of the proposed technique is that it does not require the\ntime-consuming task of manual labelling the instances.",
          "link": "http://arxiv.org/abs/2107.12093",
          "publishedOn": "2021-07-27T02:03:33.573Z",
          "wordCount": 647,
          "title": "A Multiple-Instance Learning Approach for the Assessment of Gallbladder Vascularity from Laparoscopic Images. (arXiv:2107.12093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuedong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1\">Tat-Jen Cham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "Although much progress has been made in visual emotion recognition,\nresearchers have realized that modern deep networks tend to exploit dataset\ncharacteristics to learn spurious statistical associations between the input\nand the target. Such dataset characteristics are usually treated as dataset\nbias, which damages the robustness and generalization performance of these\nrecognition systems. In this work, we scrutinize this problem from the\nperspective of causal inference, where such dataset characteristic is termed as\na confounder which misleads the system to learn the spurious correlation. To\nalleviate the negative effects brought by the dataset bias, we propose a novel\nInterventional Emotion Recognition Network (IERN) to achieve the backdoor\nadjustment, which is one fundamental deconfounding technique in causal\ninference. A series of designed tests validate the effectiveness of IERN, and\nexperiments on three emotion benchmarks demonstrate that IERN outperforms other\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.12096",
          "publishedOn": "2021-07-27T02:03:33.566Z",
          "wordCount": 580,
          "title": "Towards Unbiased Visual Emotion Recognition via Causal Intervention. (arXiv:2107.12096v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zindancioglu_A/0/1/0/all/0/1\">Alara Zindanc&#x131;o&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezgin_T/0/1/0/all/0/1\">T. Metin Sezgin</a>",
          "description": "The ability to edit facial expressions has a wide range of applications in\ncomputer graphics. The ideal facial expression editing algorithm needs to\nsatisfy two important criteria. First, it should allow precise and targeted\nediting of individual facial actions. Second, it should generate high fidelity\noutputs without artifacts. We build a solution based on StyleGAN, which has\nbeen used extensively for semantic manipulation of faces. As we do so, we add\nto our understanding of how various semantic attributes are encoded in\nStyleGAN. In particular, we show that a naive strategy to perform editing in\nthe latent space results in undesired coupling between certain action units,\neven if they are conceptually distinct. For example, although brow lowerer and\nlip tightener are distinct action units, they appear correlated in the training\ndata. Hence, StyleGAN has difficulty in disentangling them. We allow\ndisentangled editing of such action units by computing detached regions of\ninfluence for each action unit, and restrict editing to these regions. We\nvalidate the effectiveness of our local editing method through perception\nexperiments conducted with 23 subjects. The results show that our method\nprovides higher control over local editing and produces images with superior\nfidelity compared to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.12143",
          "publishedOn": "2021-07-27T02:03:33.542Z",
          "wordCount": 648,
          "title": "Perceptually Validated Precise Local Editing for Facial Action Units with StyleGAN. (arXiv:2107.12143v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects using\npoint cloud data. We present anchor design along with custom loss functions\nused in this work. A combination of spatial and channel wise attention module\nis used in this work. We use the Kitti 3D Birds Eye View dataset for\nbenchmarking and validating our results. Our method surpasses previous state of\nthe art in this domain both in terms of average precision and speed running at\n> 30 FPS. Finally, we present the ablation study to demonstrate that the\nperformance of our network is generalizable. This makes it a feasible option to\nbe deployed in real time applications like self driving cars.",
          "link": "http://arxiv.org/abs/2107.12137",
          "publishedOn": "2021-07-27T02:03:33.536Z",
          "wordCount": 605,
          "title": "AA3DNet: Attention Augmented Real Time 3D Object Detection. (arXiv:2107.12137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xi Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaomei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weiwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xiaobo Lai</a>",
          "description": "Background: Glioma is the most common brain malignant tumor, with a high\nmorbidity rate and a mortality rate of more than three percent, which seriously\nendangers human health. The main method of acquiring brain tumors in the clinic\nis MRI. Segmentation of brain tumor regions from multi-modal MRI scan images is\nhelpful for treatment inspection, post-diagnosis monitoring, and effect\nevaluation of patients. However, the common operation in clinical brain tumor\nsegmentation is still manual segmentation, lead to its time-consuming and large\nperformance difference between different operators, a consistent and accurate\nautomatic segmentation method is urgently needed. Methods: To meet the above\nchallenges, we propose an automatic brain tumor MRI data segmentation framework\nwhich is called AGSE-VNet. In our study, the Squeeze and Excite (SE) module is\nadded to each encoder, the Attention Guide Filter (AG) module is added to each\ndecoder, using the channel relationship to automatically enhance the useful\ninformation in the channel to suppress the useless information, and use the\nattention mechanism to guide the edge information and remove the influence of\nirrelevant information such as noise. Results: We used the BraTS2020 challenge\nonline verification tool to evaluate our approach. The focus of verification is\nthat the Dice scores of the whole tumor (WT), tumor core (TC) and enhanced\ntumor (ET) are 0.68, 0.85 and 0.70, respectively. Conclusion: Although MRI\nimages have different intensities, AGSE-VNet is not affected by the size of the\ntumor, and can more accurately extract the features of the three regions, it\nhas achieved impressive results and made outstanding contributions to the\nclinical diagnosis and treatment of brain tumor patients.",
          "link": "http://arxiv.org/abs/2107.12046",
          "publishedOn": "2021-07-27T02:03:33.448Z",
          "wordCount": 725,
          "title": "3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework. (arXiv:2107.12046v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangteng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yiliang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>",
          "description": "Video-text retrieval is an important yet challenging task in vision-language\nunderstanding, which aims to learn a joint embedding space where related video\nand text instances are close to each other. Most current works simply measure\nthe video-text similarity based on video-level and text-level embeddings.\nHowever, the neglect of more fine-grained or local information causes the\nproblem of insufficient representation. Some works exploit the local details by\ndisentangling sentences, but overlook the corresponding videos, causing the\nasymmetry of video-text representation. To address the above limitations, we\npropose a Hierarchical Alignment Network (HANet) to align different level\nrepresentations for video-text matching. Specifically, we first decompose video\nand text into three semantic levels, namely event (video and text), action\n(motion and verb), and entity (appearance and noun). Based on these, we\nnaturally construct hierarchical representations in the individual-local-global\nmanner, where the individual level focuses on the alignment between frame and\nword, local level focuses on the alignment between video clip and textual\ncontext, and global level focuses on the alignment between the whole video and\ntext. Different level alignments capture fine-to-coarse correlations between\nvideo and text, as well as take the advantage of the complementary information\namong three semantic levels. Besides, our HANet is also richly interpretable by\nexplicitly learning key semantic concepts. Extensive experiments on two public\ndatasets, namely MSR-VTT and VATEX, show the proposed HANet outperforms other\nstate-of-the-art methods, which demonstrates the effectiveness of hierarchical\nrepresentation and alignment. Our code is publicly available.",
          "link": "http://arxiv.org/abs/2107.12059",
          "publishedOn": "2021-07-27T02:03:33.440Z",
          "wordCount": 687,
          "title": "HANet: Hierarchical Alignment Networks for Video-Text Retrieval. (arXiv:2107.12059v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yue Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prugel_Bennett_A/0/1/0/all/0/1\">Adam Pr&#xfc;gel-Bennett</a>",
          "description": "Zero shot learning (ZSL) has seen a surge in interest over the decade for its\ntight links with the mechanism making young children recognize novel objects.\nAlthough different paradigms of visual semantic embedding models are designed\nto align visual features and distributed word representations, it is unclear to\nwhat extent current ZSL models encode semantic information from distributed\nword representations. In this work, we introduce the split of tiered-ImageNet\nto the ZSL task, in order to avoid the structural flaws in the standard\nImageNet benchmark. We build a unified framework for ZSL with contrastive\nlearning as pre-training, which guarantees no semantic information leakage and\nencourages linearly separable visual features. Our work makes it fair for\nevaluating visual semantic embedding models on a ZSL setting in which semantic\ninference is decisive. With this framework, we show that current ZSL models\nstruggle with encoding semantic relationships from word analogy and word\nhierarchy. Our analyses provide motivation for exploring the role of context\nlanguage representations in ZSL tasks.",
          "link": "http://arxiv.org/abs/2107.11991",
          "publishedOn": "2021-07-27T02:03:33.433Z",
          "wordCount": 594,
          "title": "What Remains of Visual Semantic Embeddings. (arXiv:2107.11991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12038",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mentzer_F/0/1/0/all/0/1\">Fabian Mentzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agustsson_E/0/1/0/all/0/1\">Eirikur Agustsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minnen_D/0/1/0/all/0/1\">David Minnen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johnston_N/0/1/0/all/0/1\">Nick Johnston</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toderici_G/0/1/0/all/0/1\">George Toderici</a>",
          "description": "We present a neural video compression method based on generative adversarial\nnetworks (GANs) that outperforms previous neural video compression methods and\nis comparable to HEVC in a user study. We propose a technique to mitigate\ntemporal error accumulation caused by recursive frame compression that uses\nrandomized shifting and un-shifting, motivated by a spectral analysis. We\npresent in detail the network design choices, their relative importance, and\nelaborate on the challenges of evaluating video compression methods in user\nstudies.",
          "link": "http://arxiv.org/abs/2107.12038",
          "publishedOn": "2021-07-27T02:03:33.425Z",
          "wordCount": 519,
          "title": "Towards Generative Video Compression. (arXiv:2107.12038v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "Convolutional neural networks use regular quadrilateral convolution kernels\nto extract features. Since the number of parameters increases quadratically\nwith the size of the convolution kernel, many popular models use small\nconvolution kernels, resulting in small local receptive fields in lower layers.\nThis paper proposes a novel log-polar space convolution (LPSC) method, where\nthe convolution kernel is elliptical and adaptively divides its local receptive\nfield into different regions according to the relative directions and\nlogarithmic distances. The local receptive field grows exponentially with the\nnumber of distance levels. Therefore, the proposed LPSC not only naturally\nencodes local spatial structures, but also greatly increases the single-layer\nreceptive field while maintaining the number of parameters. We show that LPSC\ncan be implemented with conventional convolution via log-polar space pooling\nand can be applied in any network architecture to replace conventional\nconvolutions. Experiments on different tasks and datasets demonstrate the\neffectiveness of the proposed LPSC. Code is available at\nhttps://github.com/BingSu12/Log-Polar-Space-Convolution.",
          "link": "http://arxiv.org/abs/2107.11943",
          "publishedOn": "2021-07-27T02:03:33.372Z",
          "wordCount": 588,
          "title": "Log-Polar Space Convolution for Convolutional Neural Networks. (arXiv:2107.11943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuqian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "A recent study finds that existing few-shot learning methods, trained on the\nsource domain, fail to generalize to the novel target domain when a domain gap\nis observed. This motivates the task of Cross-Domain Few-Shot Learning\n(CD-FSL). In this paper, we realize that the labeled target data in CD-FSL has\nnot been leveraged in any way to help the learning process. Thus, we advocate\nutilizing few labeled target data to guide the model learning. Technically, a\nnovel meta-FDMixup network is proposed. We tackle this problem mainly from two\naspects. Firstly, to utilize the source and the newly introduced target data of\ntwo different class sets, a mixup module is re-proposed and integrated into the\nmeta-learning mechanism. Secondly, a novel disentangle module together with a\ndomain classifier is proposed to extract the disentangled domain-irrelevant and\ndomain-specific features. These two modules together enable our model to narrow\nthe domain gap thus generalizing well to the target datasets. Additionally, a\ndetailed feasibility and pilot study is conducted to reflect the intuitive\nunderstanding of CD-FSL under our new setting. Experimental results show the\neffectiveness of our new setting and the proposed method. Codes and models are\navailable at https://github.com/lovelyqian/Meta-FDMixup.",
          "link": "http://arxiv.org/abs/2107.11978",
          "publishedOn": "2021-07-27T02:03:33.355Z",
          "wordCount": 641,
          "title": "Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data. (arXiv:2107.11978v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>",
          "description": "Visual text recognition is undoubtedly one of the most extensively researched\ntopics in computer vision. Great progress have been made to date, with the\nlatest models starting to focus on the more practical \"in-the-wild\" setting.\nHowever, a salient problem still hinders practical deployment -- prior arts\nmostly struggle with recognising unseen (or rarely seen) character sequences.\nIn this paper, we put forward a novel framework to specifically tackle this\n\"unseen\" problem. Our framework is iterative in nature, in that it utilises\npredicted knowledge of character sequences from a previous iteration, to\naugment the main network in improving the next prediction. Key to our success\nis a unique cross-modal variational autoencoder to act as a feedback module,\nwhich is trained with the presence of textual error distribution data. This\nmodule importantly translate a discrete predicted character space, to a\ncontinuous affine transformation parameter space used to condition the visual\nfeature map at next iteration. Experiments on common datasets have shown\ncompetitive performance over state-of-the-arts under the conventional setting.\nMost importantly, under the new disjoint setup where train-test labels are\nmutually exclusive, ours offers the best performance thus showcasing the\ncapability of generalising onto unseen words.",
          "link": "http://arxiv.org/abs/2107.12081",
          "publishedOn": "2021-07-27T02:03:33.335Z",
          "wordCount": 645,
          "title": "Towards the Unseen: Iterative Text Recognition by Distilling from Errors. (arXiv:2107.12081v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11882",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kaiwen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Ho Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deppen_S/0/1/0/all/0/1\">Steve Deppen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandler_K/0/1/0/all/0/1\">Kim Sandler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Massion_P/0/1/0/all/0/1\">Pierre Massion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lasko_T/0/1/0/all/0/1\">Thomas A. Lasko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>",
          "description": "Data from multi-modality provide complementary information in clinical\nprediction, but missing data in clinical cohorts limits the number of subjects\nin multi-modal learning context. Multi-modal missing imputation is challenging\nwith existing methods when 1) the missing data span across heterogeneous\nmodalities (e.g., image vs. non-image); or 2) one modality is largely missing.\nIn this paper, we address imputation of missing data by modeling the joint\ndistribution of multi-modal data. Motivated by partial bidirectional generative\nadversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method\nthat imputes one modality combining the conditional knowledge from another\nmodality. Specifically, C-PBiGAN introduces a conditional latent space in a\nmissing imputation framework that jointly encodes the available multi-modal\ndata, along with a class regularization loss on imputed data to recover\ndiscriminative information. To our knowledge, it is the first generative\nadversarial model that addresses multi-modal missing imputation by modeling the\njoint distribution of image and non-image data. We validate our model with both\nthe national lung screening trial (NLST) dataset and an external clinical\nvalidation cohort. The proposed C-PBiGAN achieves significant improvements in\nlung cancer risk estimation compared with representative imputation methods\n(e.g., AUC values increase in both NLST (+2.9\\%) and in-house dataset (+4.3\\%)\ncompared with PBiGAN, p$<$0.05).",
          "link": "http://arxiv.org/abs/2107.11882",
          "publishedOn": "2021-07-27T02:03:33.322Z",
          "wordCount": 683,
          "title": "Lung Cancer Risk Estimation with Incomplete Data: A Joint Missing Imputation Perspective. (arXiv:2107.11882v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wentian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heda Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinxiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Entity-aware image captioning aims to describe named entities and events\nrelated to the image by utilizing the background knowledge in the associated\narticle. This task remains challenging as it is difficult to learn the\nassociation between named entities and visual cues due to the long-tail\ndistribution of named entities. Furthermore, the complexity of the article\nbrings difficulty in extracting fine-grained relationships between entities to\ngenerate informative event descriptions about the image. To tackle these\nchallenges, we propose a novel approach that constructs a multi-modal knowledge\ngraph to associate the visual objects with named entities and capture the\nrelationship between entities simultaneously with the help of external\nknowledge collected from the web. Specifically, we build a text sub-graph by\nextracting named entities and their relationships from the article, and build\nan image sub-graph by detecting the objects in the image. To connect these two\nsub-graphs, we propose a cross-modal entity matching module trained using a\nknowledge base that contains Wikipedia entries and the corresponding images.\nFinally, the multi-modal knowledge graph is integrated into the captioning\nmodel via a graph attention mechanism. Extensive experiments on both GoodNews\nand NYTimes800k datasets demonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2107.11970",
          "publishedOn": "2021-07-27T02:03:33.315Z",
          "wordCount": 632,
          "title": "Boosting Entity-aware Image Captioning with Multi-modal Knowledge Graph. (arXiv:2107.11970v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laugros_A/0/1/0/all/0/1\">Alfred Laugros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caplier_A/0/1/0/all/0/1\">Alice Caplier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ospici_M/0/1/0/all/0/1\">Matthieu Ospici</a>",
          "description": "Synthetic corruptions gathered into a benchmark are frequently used to\nmeasure neural network robustness to distribution shifts. However, robustness\nto synthetic corruption benchmarks is not always predictive of robustness to\ndistribution shifts encountered in real-world applications. In this paper, we\npropose a methodology to build synthetic corruption benchmarks that make\nrobustness estimations more correlated with robustness to real-world\ndistribution shifts. Using the overlapping criterion, we split synthetic\ncorruptions into categories that help to better understand neural network\nrobustness. Based on these categories, we identify three parameters that are\nrelevant to take into account when constructing a corruption benchmark: number\nof represented categories, balance among categories and size of benchmarks.\nApplying the proposed methodology, we build a new benchmark called\nImageNet-Syn2Nat to predict image classifier robustness.",
          "link": "http://arxiv.org/abs/2107.12052",
          "publishedOn": "2021-07-27T02:03:33.308Z",
          "wordCount": 563,
          "title": "Using Synthetic Corruptions to Measure Robustness to Natural Distribution Shifts. (arXiv:2107.12052v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lujia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Road curb detection is important for autonomous driving. It can be used to\ndetermine road boundaries to constrain vehicles on roads, so that potential\naccidents could be avoided. Most of the current methods detect road curbs\nonline using vehicle-mounted sensors, such as cameras or 3-D Lidars. However,\nthese methods usually suffer from severe occlusion issues. Especially in\nhighly-dynamic traffic environments, most of the field of view is occupied by\ndynamic objects. To alleviate this issue, we detect road curbs offline using\nhigh-resolution aerial images in this paper. Moreover, the detected road curbs\ncan be used to create high-definition (HD) maps for autonomous vehicles.\nSpecifically, we first predict the pixel-wise segmentation map of road curbs,\nand then conduct a series of post-processing steps to extract the graph\nstructure of road curbs. To tackle the disconnectivity issue in the\nsegmentation maps, we propose an innovative connectivity-preserving loss\n(CP-loss) to improve the segmentation performance. The experimental results on\na public dataset demonstrate the effectiveness of our proposed loss function.\nThis paper is accompanied with a demonstration video and a supplementary\ndocument, which are available at\n\\texttt{\\url{https://sites.google.com/view/cp-loss}}.",
          "link": "http://arxiv.org/abs/2107.11920",
          "publishedOn": "2021-07-27T02:03:33.290Z",
          "wordCount": 648,
          "title": "CP-loss: Connectivity-preserving Loss for Road Curb Detection in Autonomous Driving with Aerial Images. (arXiv:2107.11920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunlei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>",
          "description": "Few-shot learning aims to train a classifier that can generalize well when\njust a small number of labeled samples per class are given. We introduce\nTransductive Maximum Margin Classifier (TMMC) for few-shot learning. The basic\nidea of the classical maximum margin classifier is to solve an optimal\nprediction function that the corresponding separating hyperplane can correctly\ndivide the training data and the resulting classifier has the largest geometric\nmargin. In few-shot learning scenarios, the training samples are scarce, not\nenough to find a separating hyperplane with good generalization ability on\nunseen data. TMMC is constructed using a mixture of the labeled support set and\nthe unlabeled query set in a given task. The unlabeled samples in the query set\ncan adjust the separating hyperplane so that the prediction function is optimal\non both the labeled and unlabeled samples. Furthermore, we leverage an\nefficient and effective quasi-Newton algorithm, the L-BFGS method to optimize\nTMMC. Experimental results on three standard few-shot learning benchmarks\nincluding miniImagenet, tieredImagenet and CUB suggest that our TMMC achieves\nstate-of-the-art accuracies.",
          "link": "http://arxiv.org/abs/2107.11975",
          "publishedOn": "2021-07-27T02:03:33.284Z",
          "wordCount": 607,
          "title": "Transductive Maximum Margin Classifier for Few-Shot Learning. (arXiv:2107.11975v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11945",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Heran Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Liwei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zongben Xu</a>",
          "description": "Cross-contrast image translation is an important task for completing missing\ncontrasts in clinical diagnosis. However, most existing methods learn separate\ntranslator for each pair of contrasts, which is inefficient due to many\npossible contrast pairs in real scenarios. In this work, we propose a unified\nHyper-GAN model for effectively and efficiently translating between different\ncontrast pairs. Hyper-GAN consists of a pair of hyper-encoder and hyper-decoder\nto first map from the source contrast to a common feature space, and then\nfurther map to the target contrast image. To facilitate the translation between\ndifferent contrast pairs, contrast-modulators are designed to tune the\nhyper-encoder and hyper-decoder adaptive to different contrasts. We also design\na common space loss to enforce that multi-contrast images of a subject share a\ncommon feature space, implicitly modeling the shared underlying anatomical\nstructures. Experiments on two datasets of IXI and BraTS 2019 show that our\nHyper-GAN achieves state-of-the-art results in both accuracy and efficiency,\ne.g., improving more than 1.47 and 1.09 dB in PSNR on two datasets with less\nthan half the amount of parameters.",
          "link": "http://arxiv.org/abs/2107.11945",
          "publishedOn": "2021-07-27T02:03:33.275Z",
          "wordCount": 634,
          "title": "A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image Translation. (arXiv:2107.11945v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>",
          "description": "In spite of the successful application in many fields, machine learning\nalgorithms today suffer from notorious problems like vulnerability to\nadversarial examples. Beyond falling into the cat-and-mouse game between\nadversarial attack and defense, this paper provides alternative perspective to\nconsider adversarial example and explore whether we can exploit it in benign\napplications. We first propose a novel taxonomy of visual information along\ntask-relevance and semantic-orientation. The emergence of adversarial example\nis attributed to algorithm's utilization of task-relevant non-semantic\ninformation. While largely ignored in classical machine learning mechanisms,\ntask-relevant non-semantic information enjoys three interesting characteristics\nas (1) exclusive to algorithm, (2) reflecting common weakness, and (3)\nutilizable as features. Inspired by this, we present brave new idea called\nbenign adversarial attack to exploit adversarial examples for goodness in three\ndirections: (1) adversarial Turing test, (2) rejecting malicious algorithm, and\n(3) adversarial data augmentation. Each direction is positioned with motivation\nelaboration, justification analysis and prototype applications to showcase its\npotential.",
          "link": "http://arxiv.org/abs/2107.11986",
          "publishedOn": "2021-07-27T02:03:33.268Z",
          "wordCount": 597,
          "title": "Benign Adversarial Attack: Tricking Algorithm for Goodness. (arXiv:2107.11986v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziyi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianjun Zhao</a>",
          "description": "Motion blur caused by the moving of the object or camera during the exposure\ncan be a key challenge for visual object tracking, affecting tracking accuracy\nsignificantly. In this work, we explore the robustness of visual object\ntrackers against motion blur from a new angle, i.e., adversarial blur attack\n(ABA). Our main objective is to online transfer input frames to their natural\nmotion-blurred counterparts while misleading the state-of-the-art trackers\nduring the tracking process. To this end, we first design the motion blur\nsynthesizing method for visual tracking based on the generation principle of\nmotion blur, considering the motion information and the light accumulation\nprocess. With this synthetic method, we propose \\textit{optimization-based ABA\n(OP-ABA)} by iteratively optimizing an adversarial objective function against\nthe tracking w.r.t. the motion and light accumulation parameters. The OP-ABA is\nable to produce natural adversarial examples but the iteration can cause heavy\ntime cost, making it unsuitable for attacking real-time trackers. To alleviate\nthis issue, we further propose \\textit{one-step ABA (OS-ABA)} where we design\nand train a joint adversarial motion and accumulation predictive network\n(JAMANet) with the guidance of OP-ABA, which is able to efficiently estimate\nthe adversarial motion and accumulation parameters in a one-step way. The\nexperiments on four popular datasets (\\eg, OTB100, VOT2018, UAV123, and LaSOT)\ndemonstrate that our methods are able to cause significant accuracy drops on\nfour state-of-the-art trackers with high transferability. Please find the\nsource code at https://github.com/tsingqguo/ABA",
          "link": "http://arxiv.org/abs/2107.12085",
          "publishedOn": "2021-07-27T02:03:33.261Z",
          "wordCount": 694,
          "title": "Learning to Adversarially Blur Visual Object Tracking. (arXiv:2107.12085v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Shuvozit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>",
          "description": "Although text recognition has significantly evolved over the years,\nstate-of-the-art (SOTA) models still struggle in the wild scenarios due to\ncomplex backgrounds, varying fonts, uncontrolled illuminations, distortions and\nother artefacts. This is because such models solely depend on visual\ninformation for text recognition, thus lacking semantic reasoning capabilities.\nIn this paper, we argue that semantic information offers a complementary role\nin addition to visual only. More specifically, we additionally utilize semantic\ninformation by proposing a multi-stage multi-scale attentional decoder that\nperforms joint visual-semantic reasoning. Our novelty lies in the intuition\nthat for text recognition, the prediction should be refined in a stage-wise\nmanner. Therefore our key contribution is in designing a stage-wise unrolling\nattentional decoder where non-differentiability, invoked by discretely\npredicted character labels, needs to be bypassed for end-to-end training. While\nthe first stage predicts using visual features, subsequent stages refine on top\nof it using joint visual-semantic information. Additionally, we introduce\nmulti-scale 2D attention along with dense and residual connections between\ndifferent stages to deal with varying scales of character sizes, for better\nperformance and faster convergence during training. Experimental results show\nour approach to outperform existing SOTA methods by a considerable margin.",
          "link": "http://arxiv.org/abs/2107.12090",
          "publishedOn": "2021-07-27T02:03:33.242Z",
          "wordCount": 647,
          "title": "Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition. (arXiv:2107.12090v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yalong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mohan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Data augmentation is practically helpful for visual recognition, especially\nat the time of data scarcity. However, such success is only limited to quite a\nfew light augmentations (e.g., random crop, flip). Heavy augmentations (e.g.,\ngray, grid shuffle) are either unstable or show adverse effects during\ntraining, owing to the big gap between the original and augmented images. This\npaper introduces a novel network design, noted as Augmentation Pathways (AP),\nto systematically stabilize training on a much wider range of augmentation\npolicies. Notably, AP tames heavy data augmentations and stably boosts\nperformance without a careful selection among augmentation policies. Unlike\ntraditional single pathway, augmented images are processed in different neural\npaths. The main pathway handles light augmentations, while other pathways focus\non heavy augmentations. By interacting with multiple paths in a dependent\nmanner, the backbone network robustly learns from shared visual patterns among\naugmentations, and suppresses noisy patterns at the same time. Furthermore, we\nextend AP to a homogeneous version and a heterogeneous version for high-order\nscenarios, demonstrating its robustness and flexibility in practical usage.\nExperimental results on ImageNet benchmarks demonstrate the compatibility and\neffectiveness on a much wider range of augmentations (e.g., Crop, Gray, Grid\nShuffle, RandAugment), while consuming fewer parameters and lower computational\ncosts at inference time. Source code:https://github.com/ap-conv/ap-net.",
          "link": "http://arxiv.org/abs/2107.11990",
          "publishedOn": "2021-07-27T02:03:33.236Z",
          "wordCount": 648,
          "title": "Augmentation Pathways Network for Visual Recognition. (arXiv:2107.11990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12009",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cahan_N/0/1/0/all/0/1\">Noa Cahan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marom_E/0/1/0/all/0/1\">Edith M. Marom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soffer_S/0/1/0/all/0/1\">Shelly Soffer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barash_Y/0/1/0/all/0/1\">Yiftach Barash</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konen_E/0/1/0/all/0/1\">Eli Konen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klang_E/0/1/0/all/0/1\">Eyal Klang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greenspan_H/0/1/0/all/0/1\">Hayit Greenspan</a>",
          "description": "Pulmonary embolus (PE) refers to obstruction of pulmonary arteries by blood\nclots. PE accounts for approximately 100,000 deaths per year in the United\nStates alone. The clinical presentation of PE is often nonspecific, making the\ndiagnosis challenging. Thus, rapid and accurate risk stratification is of\nparamount importance. High-risk PE is caused by right ventricular (RV)\ndysfunction from acute pressure overload, which in return can help identify\nwhich patients require more aggressive therapy. Reconstructed four-chamber\nviews of the heart on chest CT can detect right ventricular enlargement. CT\npulmonary angiography (CTPA) is the golden standard in the diagnostic workup of\nsuspected PE. Therefore, it can link between diagnosis and risk stratification\nstrategies. We developed a weakly supervised deep learning algorithm, with an\nemphasis on a novel attention mechanism, to automatically classify RV strain on\nCTPA. Our method is a 3D DenseNet model with integrated 3D residual attention\nblocks. We evaluated our model on a dataset of CTPAs of emergency department\n(ED) PE patients. This model achieved an area under the receiver operating\ncharacteristic curve (AUC) of 0.88 for classifying RV strain. The model showed\na sensitivity of 87% and specificity of 83.7%. Our solution outperforms\nstate-of-the-art 3D CNN networks. The proposed design allows for a fully\nautomated network that can be trained easily in an end-to-end manner without\nrequiring computationally intensive and time-consuming preprocessing or\nstrenuous labeling of the data.We infer that unmarked CTPAs can be used for\neffective RV strain classification. This could be used as a second reader,\nalerting for high-risk PE patients. To the best of our knowledge, there are no\nprevious deep learning-based studies that attempted to solve this problem.",
          "link": "http://arxiv.org/abs/2107.12009",
          "publishedOn": "2021-07-27T02:03:33.229Z",
          "wordCount": 751,
          "title": "Weakly Supervised Attention Model for RV StrainClassification from volumetric CTPA Scans. (arXiv:2107.12009v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maureira_J/0/1/0/all/0/1\">Jose Maureira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1\">Juan Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arellano_C/0/1/0/all/0/1\">Claudia Arellano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Biometric has been increasing in relevance these days since it can be used\nfor several applications such as access control for instance. Unfortunately,\nwith the increased deployment of biometric applications, we observe an increase\nof attacks. Therefore, algorithms to detect such attacks (Presentation Attack\nDetection (PAD)) have been increasing in relevance. The LivDet-2020 competition\nwhich focuses on Presentation Attacks Detection (PAD) algorithms have shown\nstill open problems, specially for unknown attacks scenarios. In order to\nimprove the robustness of biometric systems, it is crucial to improve PAD\nmethods. This can be achieved by augmenting the number of presentation attack\ninstruments (PAI) and bona fide images that are used to train such algorithms.\nUnfortunately, the capture and creation of presentation attack instruments and\neven the capture of bona fide images is sometimes complex to achieve. This\npaper proposes a novel PAI synthetically created (SPI-PAI) using four\nstate-of-the-art GAN algorithms (cGAN, WGAN, WGAN-GP, and StyleGAN2) and a\nsmall set of periocular NIR images. A benchmark between GAN algorithms is\nperformed using the Frechet Inception Distance (FID) between the generated\nimages and the original images used for training. The best PAD algorithm\nreported by the LivDet-2020 competition was tested for us using the synthetic\nPAI which was obtained with the StyleGAN2 algorithm. Surprisingly, The PAD\nalgorithm was not able to detect the synthetic images as a Presentation Attack,\ncategorizing all of them as bona fide. Such results demonstrated the\nfeasibility of synthetic images to fool presentation attacks detection\nalgorithms and the need for such algorithms to be constantly updated and\ntrained with a larger number of images and PAI scenarios.",
          "link": "http://arxiv.org/abs/2107.12014",
          "publishedOn": "2021-07-27T02:03:33.216Z",
          "wordCount": 707,
          "title": "Synthetic Periocular Iris PAI from a Small Set of Near-Infrared-Images. (arXiv:2107.12014v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>",
          "description": "Text recognition remains a fundamental and extensively researched topic in\ncomputer vision, largely owing to its wide array of commercial applications.\nThe challenging nature of the very problem however dictated a fragmentation of\nresearch efforts: Scene Text Recognition (STR) that deals with text in everyday\nscenes, and Handwriting Text Recognition (HTR) that tackles hand-written text.\nIn this paper, for the first time, we argue for their unification -- we aim for\na single model that can compete favourably with two separate state-of-the-art\nSTR and HTR models. We first show that cross-utilisation of STR and HTR models\ntrigger significant performance drops due to differences in their inherent\nchallenges. We then tackle their union by introducing a knowledge distillation\n(KD) based framework. This is however non-trivial, largely due to the\nvariable-length and sequential nature of text sequences, which renders\noff-the-shelf KD techniques that mostly works with global fixed-length data\ninadequate. For that, we propose three distillation losses all of which are\nspecifically designed to cope with the aforementioned unique characteristics of\ntext recognition. Empirical evidence suggests that our proposed unified model\nperforms on par with individual models, even surpassing them in certain cases.\nAblative studies demonstrate that naive baselines such as a two-stage\nframework, and domain adaption/generalisation alternatives do not work as well,\nfurther verifying the appropriateness of our design.",
          "link": "http://arxiv.org/abs/2107.12087",
          "publishedOn": "2021-07-27T02:03:33.208Z",
          "wordCount": 673,
          "title": "Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation. (arXiv:2107.12087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "Despite much recent progress in video-based person re-identification (re-ID),\nthe current state-of-the-art still suffers from common real-world challenges\nsuch as appearance similarity among various people, occlusions, and frame\nmisalignment. To alleviate these problems, we propose Spatio-Temporal\nRepresentation Factorization module (STRF), a flexible new computational unit\nthat can be used in conjunction with most existing 3D convolutional neural\nnetwork architectures for re-ID. The key innovations of STRF over prior work\ninclude explicit pathways for learning discriminative temporal and spatial\nfeatures, with each component further factorized to capture complementary\nperson-specific appearance and motion information. Specifically, temporal\nfactorization comprises two branches, one each for static features (e.g., the\ncolor of clothes) that do not change much over time, and dynamic features\n(e.g., walking patterns) that change over time. Further, spatial factorization\nalso comprises two branches to learn both global (coarse segments) as well as\nlocal (finer segments) appearance features, with the local features\nparticularly useful in cases of occlusion or spatial misalignment. These two\nfactorization operations taken together result in a modular architecture for\nour parameter-wise economic STRF unit that can be plugged in between any two 3D\nconvolutional layers, resulting in an end-to-end learning framework. We\nempirically show that STRF improves performance of various existing baseline\narchitectures while demonstrating new state-of-the-art results using standard\nperson re-identification evaluation protocols on three benchmarks.",
          "link": "http://arxiv.org/abs/2107.11878",
          "publishedOn": "2021-07-27T02:03:33.173Z",
          "wordCount": 666,
          "title": "Spatio-Temporal Representation Factorization for Video-based Person Re-Identification. (arXiv:2107.11878v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunlei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>",
          "description": "The goal of few-shot video classification is to learn a classification model\nwith good generalization ability when trained with only a few labeled videos.\nHowever, it is difficult to learn discriminative feature representations for\nvideos in such a setting. In this paper, we propose Temporal Alignment\nPrediction (TAP) based on sequence similarity learning for few-shot video\nclassification. In order to obtain the similarity of a pair of videos, we\npredict the alignment scores between all pairs of temporal positions in the two\nvideos with the temporal alignment prediction function. Besides, the inputs to\nthis function are also equipped with the context information in the temporal\ndomain. We evaluate TAP on two video classification benchmarks including\nKinetics and Something-Something V2. The experimental results verify the\neffectiveness of TAP and show its superiority over state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.11960",
          "publishedOn": "2021-07-27T02:03:33.165Z",
          "wordCount": 568,
          "title": "Temporal Alignment Prediction for Few-Shot Video Classification. (arXiv:2107.11960v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1\">Baorui Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>",
          "description": "Scene video text spotting (SVTS) is a very important research topic because\nof many real-life applications. However, only a little effort has put to\nspotting scene video text, in contrast to massive studies of scene text\nspotting in static images. Due to various environmental interferences like\nmotion blur, spotting scene video text becomes very challenging. To promote\nthis research area, this competition introduces a new challenge dataset\ncontaining 129 video clips from 21 natural scenarios in full annotations. The\ncompetition containts three tasks, that is, video text detection (Task 1),\nvideo text tracking (Task 2) and end-to-end video text spotting (Task3). During\nthe competition period (opened on 1st March, 2021 and closed on 11th April,\n2021), a total of 24 teams participated in the three proposed tasks with 46\nvalid submissions, respectively. This paper includes dataset descriptions, task\ndefinitions, evaluation protocols and results summaries of the ICDAR 2021 on\nSVTS competition. Thanks to the healthy number of teams as well as submissions,\nwe consider that the SVTS competition has been successfully held, drawing much\nattention from the community and promoting the field research and its\ndevelopment.",
          "link": "http://arxiv.org/abs/2107.11919",
          "publishedOn": "2021-07-27T02:03:33.092Z",
          "wordCount": 631,
          "title": "ICDAR 2021 Competition on Scene Video Text Spotting. (arXiv:2107.11919v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1\">Oscar Mendez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vowels_M/0/1/0/all/0/1\">Matthew Vowels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "Attention is an important component of modern deep learning. However, less\nemphasis has been put on its inverse: ignoring distraction. Our daily lives\nrequire us to explicitly avoid giving attention to salient visual features that\nconfound the task we are trying to accomplish. This visual prioritisation\nallows us to concentrate on important tasks while ignoring visual distractors.\n\nIn this work, we introduce Neural Blindness, which gives an agent the ability\nto completely ignore objects or classes that are deemed distractors. More\nexplicitly, we aim to render a neural network completely incapable of\nrepresenting specific chosen classes in its latent space. In a very real sense,\nthis makes the network \"blind\" to certain classes, allowing and agent to focus\non what is important for a given task, and demonstrates how this can be used to\nimprove localisation.",
          "link": "http://arxiv.org/abs/2107.11857",
          "publishedOn": "2021-07-27T02:03:33.067Z",
          "wordCount": 578,
          "title": "Improving Robot Localisation by Ignoring Visual Distraction. (arXiv:2107.11857v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zilun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shihao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichun Zhang</a>",
          "description": "Most few-shot learning models utilize only one modality of data. We would\nlike to investigate qualitatively and quantitatively how much will the model\nimprove if we add an extra modality (i.e. text description of the image), and\nhow it affects the learning procedure. To achieve this goal, we propose four\ntypes of fusion method to combine the image feature and text feature. To verify\nthe effectiveness of improvement, we test the fusion methods with two classical\nfew-shot learning models - ProtoNet and MAML, with image feature extractors\nsuch as ConvNet and ResNet12. The attention-based fusion method works best,\nwhich improves the classification accuracy by a large margin around 30%\ncomparing to the baseline result.",
          "link": "http://arxiv.org/abs/2107.11853",
          "publishedOn": "2021-07-27T02:03:33.061Z",
          "wordCount": 550,
          "title": "Will Multi-modal Data Improves Few-shot Learning?. (arXiv:2107.11853v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hanxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinxiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "How to model fine-grained spatial-temporal dynamics in videos has been a\nchallenging problem for action recognition. It requires learning deep and rich\nfeatures with superior distinctiveness for the subtle and abstract motions.\nMost existing methods generate features of a layer in a pure feedforward\nmanner, where the information moves in one direction from inputs to outputs.\nAnd they rely on stacking more layers to obtain more powerful features,\nbringing extra non-negligible overheads. In this paper, we propose an Adaptive\nRecursive Circle (ARC) framework, a fine-grained decorator for pure feedforward\nlayers. It inherits the operators and parameters of the original layer but is\nslightly different in the use of those operators and parameters. Specifically,\nthe input of the layer is treated as an evolving state, and its update is\nalternated with the feature generation. At each recursive step, the input state\nis enriched by the previously generated features and the feature generation is\nmade with the newly updated input state. We hope the ARC framework can\nfacilitate fine-grained action recognition by introducing deeply refined\nfeatures and multi-scale receptive fields at a low cost. Significant\nimprovements over feedforward baselines are observed on several benchmarks. For\nexample, an ARC-equipped TSM-ResNet18 outperforms TSM-ResNet50 with 48% fewer\nFLOPs and 52% model parameters on Something-Something V1 and Diving48.",
          "link": "http://arxiv.org/abs/2107.11813",
          "publishedOn": "2021-07-27T02:03:33.042Z",
          "wordCount": 646,
          "title": "Adaptive Recursive Circle Framework for Fine-grained Action Recognition. (arXiv:2107.11813v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Among numerous videos shared on the web, well-edited ones always attract more\nattention. However, it is difficult for inexperienced users to make well-edited\nvideos because it requires professional expertise and immense manual labor. To\nmeet the demands for non-experts, we present Transcript-to-Video -- a\nweakly-supervised framework that uses texts as input to automatically create\nvideo sequences from an extensive collection of shots. Specifically, we propose\na Content Retrieval Module and a Temporal Coherent Module to learn\nvisual-language representations and model shot sequencing styles, respectively.\nFor fast inference, we introduce an efficient search strategy for real-time\nvideo clip sequencing. Quantitative results and user studies demonstrate\nempirically that the proposed learning framework can retrieve content-relevant\nshots while creating plausible video sequences in terms of style. Besides, the\nrun-time performance analysis shows that our framework can support real-world\napplications.",
          "link": "http://arxiv.org/abs/2107.11851",
          "publishedOn": "2021-07-27T02:03:33.035Z",
          "wordCount": 581,
          "title": "Transcript to Video: Efficient Clip Sequencing from Texts. (arXiv:2107.11851v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abedin_T/0/1/0/all/0/1\">Thasin Abedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prottoy_K/0/1/0/all/0/1\">Khondokar S. S. Prottoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshruba_A/0/1/0/all/0/1\">Ayana Moshruba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakim_S/0/1/0/all/0/1\">Safayat Bin Hakim</a>",
          "description": "Sign language is the only medium of communication for the hearing impaired\nand the deaf and dumb community. Communication with the general mass is thus\nalways a challenge for this minority group. Especially in Bangla sign language\n(BdSL), there are 38 alphabets with some having nearly identical symbols. As a\nresult, in BdSL recognition, the posture of hand is an important factor in\naddition to visual features extracted from traditional Convolutional Neural\nNetwork (CNN). In this paper, a novel architecture \"Concatenated BdSL Network\"\nis proposed which consists of a CNN based image network and a pose estimation\nnetwork. While the image network gets the visual features, the relative\npositions of hand keypoints are taken by the pose estimation network to obtain\nthe additional features to deal with the complexity of the BdSL symbols. A\nscore of 91.51% was achieved by this novel approach in test set and the\neffectiveness of the additional pose estimation network is suggested by the\nexperimental results.",
          "link": "http://arxiv.org/abs/2107.11818",
          "publishedOn": "2021-07-27T02:03:33.028Z",
          "wordCount": 602,
          "title": "Bangla sign language recognition using concatenated BdSL network. (arXiv:2107.11818v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1\">Farid Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohel_F/0/1/0/all/0/1\">Ferdous Sohel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>",
          "description": "Semantic segmentation is a challenging task in the absence of densely\nlabelled data. Only relying on class activation maps (CAM) with image-level\nlabels provides deficient segmentation supervision. Prior works thus consider\npre-trained models to produce coarse saliency maps to guide the generation of\npseudo segmentation labels. However, the commonly used off-line heuristic\ngeneration process cannot fully exploit the benefits of these coarse saliency\nmaps. Motivated by the significant inter-task correlation, we propose a novel\nweakly supervised multi-task framework termed as AuxSegNet, to leverage\nsaliency detection and multi-label image classification as auxiliary tasks to\nimprove the primary task of semantic segmentation using only image-level\nground-truth labels. Inspired by their similar structured semantics, we also\npropose to learn a cross-task global pixel-level affinity map from the saliency\nand segmentation representations. The learned cross-task affinity can be used\nto refine saliency predictions and propagate CAM maps to provide improved\npseudo labels for both tasks. The mutual boost between pseudo label updating\nand cross-task affinity learning enables iterative improvements on segmentation\nperformance. Extensive experiments demonstrate the effectiveness of the\nproposed auxiliary learning network structure and the cross-task affinity\nlearning method. The proposed approach achieves state-of-the-art weakly\nsupervised segmentation performance on the challenging PASCAL VOC 2012 and MS\nCOCO benchmarks.",
          "link": "http://arxiv.org/abs/2107.11787",
          "publishedOn": "2021-07-27T02:03:33.021Z",
          "wordCount": 656,
          "title": "Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation. (arXiv:2107.11787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Pengwen Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>",
          "description": "Numerous scene text detection methods have been proposed in recent years.\nMost of them declare they have achieved state-of-the-art performances. However,\nthe performance comparison is unfair, due to lots of inconsistent settings\n(e.g., training data, backbone network, multi-scale feature fusion, evaluation\nprotocols, etc.). These various settings would dissemble the pros and cons of\nthe proposed core techniques. In this paper, we carefully examine and analyze\nthe inconsistent settings, and propose a unified framework for the bottom-up\nbased scene text detection methods. Under the unified framework, we ensure the\nconsistent settings for non-core modules, and mainly investigate the\nrepresentations of describing arbitrary-shape scene texts, e.g., regressing\npoints on text contours, clustering pixels with predicted auxiliary\ninformation, grouping connected components with learned linkages, etc. With the\ncomprehensive investigations and elaborate analyses, it not only cleans up the\nobstacle of understanding the performance differences between existing methods\nbut also reveals the advantages and disadvantages of previous models under fair\ncomparisons.",
          "link": "http://arxiv.org/abs/2107.11800",
          "publishedOn": "2021-07-27T02:03:33.014Z",
          "wordCount": 586,
          "title": "Comprehensive Studies for Arbitrary-shape Scene Text Detection. (arXiv:2107.11800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1\">P Preethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1\">Hrishikesh Viswanath</a>",
          "description": "This paper is a presentation of a new method for denoising images using\nHaralick features and further segmenting the characters using artificial neural\nnetworks. The image is divided into kernels, each of which is converted to a\nGLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation\nfunction is called, the result of which is an array with fourteen elements\ncorresponding to fourteen features The Haralick values and the corresponding\nnoise/text classification form a dictionary, which is then used to de-noise the\nimage through kernel comparison. Segmentation is the process of extracting\ncharacters from a document and can be used when letters are separated by white\nspace, which is an explicit boundary marker. Segmentation is the first step in\nmany Natural Language Processing problems. This paper explores the process of\nsegmentation using Neural Networks. While there have been numerous methods to\nsegment characters of a document, this paper is only concerned with the\naccuracy of doing so using neural networks. It is imperative that the\ncharacters be segmented correctly, for failing to do so will lead to incorrect\nrecognition by Natural language processing tools. Artificial Neural Networks\nwas used to attain accuracy of upto 89%. This method is suitable for languages\nwhere the characters are delimited by white space. However, this method will\nfail to provide acceptable results when the language heavily uses connected\nletters. An example would be the Devanagari script, which is predominantly used\nin northern India.",
          "link": "http://arxiv.org/abs/2107.11801",
          "publishedOn": "2021-07-27T02:03:32.994Z",
          "wordCount": 677,
          "title": "Denoising and Segmentation of Epigraphical Scripts. (arXiv:2107.11801v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1\">P Preethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1\">Hrishikesh Viswanath</a>",
          "description": "This work presents a comparison of machine learning algorithms that are\nimplemented to segment the characters of text presented as an image. The\nalgorithms are designed to work on degraded documents with text that is not\naligned in an organized fashion. The paper investigates the use of Support\nVector Machines, K-Nearest Neighbor algorithm and an Encoder Network to perform\nthe operation of character spotting. Character Spotting involves extracting\npotential characters from a stream of text by selecting regions bound by white\nspace.",
          "link": "http://arxiv.org/abs/2107.11795",
          "publishedOn": "2021-07-27T02:03:32.987Z",
          "wordCount": 518,
          "title": "Character Spotting Using Machine Learning Techniques. (arXiv:2107.11795v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11786",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ozyoruk_K/0/1/0/all/0/1\">Kutsev Bengisu Ozyoruk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Can_S/0/1/0/all/0/1\">Sermet Can</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gokceler_G/0/1/0/all/0/1\">Guliz Irem Gokceler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basak_K/0/1/0/all/0/1\">Kayhan Basak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demir_D/0/1/0/all/0/1\">Derya Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Serin_G/0/1/0/all/0/1\">Gurdeniz Serin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hacisalihoglu_U/0/1/0/all/0/1\">Uguray Payam Hacisalihoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Darbaz_B/0/1/0/all/0/1\">Berkan Darbaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_F/0/1/0/all/0/1\">Funda Yilmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turan_M/0/1/0/all/0/1\">Mehmet Turan</a>",
          "description": "Frozen sectioning (FS) is the preparation method of choice for microscopic\nevaluation of tissues during surgical operations. The high speed of procedure\nallows pathologists to rapidly assess the key microscopic features, such as\ntumor margins and malignant status to guide surgical decision-making and\nminimise disruptions to the course of the operation. However, FS is prone to\nintroducing many misleading artificial structures (histological artefacts),\nsuch as nuclear ice crystals, compression, and cutting artefacts, hindering\ntimely and accurate diagnostic judgement of the pathologist. On the other hand,\nthe gold standard tissue preparation technique of formalin-fixation and\nparaffin-embedding (FFPE) provides significantly superior image quality, but is\na very time-consuming process (12-48 hours), making it unsuitable for\nintra-operative use. In this paper, we propose an artificial intelligence (AI)\nmethod that improves FS image quality by computationally transforming\nfrozen-sectioned whole-slide images (FS-WSIs) into whole-slide FFPE-style\nimages in minutes. AI-FFPE rectifies FS artefacts with the guidance of an\nattention-mechanism that puts a particular emphasis on artefacts while\nutilising a self-regularization mechanism established between FS input image\nand synthesized FFPE-style image that preserves clinically relevant features.\nAs a result, AI-FFPE method successfully generates FFPE-style images without\nsignificantly extending tissue processing time and consequently improves\ndiagnostic accuracy.",
          "link": "http://arxiv.org/abs/2107.11786",
          "publishedOn": "2021-07-27T02:03:32.980Z",
          "wordCount": 669,
          "title": "Deep Learning-based Frozen Section to FFPE Translation. (arXiv:2107.11786v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habibpour_M/0/1/0/all/0/1\">Maryam Habibpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharoun_H/0/1/0/all/0/1\">Hassan Gharoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>",
          "description": "Defects are unavoidable in casting production owing to the complexity of the\ncasting process. While conventional human-visual inspection of casting products\nis slow and unproductive in mass productions, an automatic and reliable defect\ndetection not just enhances the quality control process but positively improves\nproductivity. However, casting defect detection is a challenging task due to\ndiversity and variation in defects' appearance. Convolutional neural networks\n(CNNs) have been widely applied in both image classification and defect\ndetection tasks. Howbeit, CNNs with frequentist inference require a massive\namount of data to train on and still fall short in reporting beneficial\nestimates of their predictive uncertainty. Accordingly, leveraging the transfer\nlearning paradigm, we first apply four powerful CNN-based models (VGG16,\nResNet50, DenseNet121, and InceptionResNetV2) on a small dataset to extract\nmeaningful features. Extracted features are then processed by various machine\nlearning algorithms to perform the classification task. Simulation results\ndemonstrate that linear support vector machine (SVM) and multi-layer perceptron\n(MLP) show the finest performance in defect detection of casting images.\nSecondly, to achieve a reliable classification and to measure epistemic\nuncertainty, we employ an uncertainty quantification (UQ) technique (ensemble\nof MLP models) using features extracted from four pre-trained CNNs. UQ\nconfusion matrix and uncertainty accuracy metric are also utilized to evaluate\nthe predictive uncertainty estimates. Comprehensive comparisons reveal that UQ\nmethod based on VGG16 outperforms others to fetch uncertainty. We believe an\nuncertainty-aware automatic defect detection solution will reinforce casting\nproductions quality assurance.",
          "link": "http://arxiv.org/abs/2107.11643",
          "publishedOn": "2021-07-27T02:03:32.974Z",
          "wordCount": 695,
          "title": "An Uncertainty-Aware Deep Learning Framework for Defect Detection in Casting Products. (arXiv:2107.11643v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Ziji Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "The transformer has recently achieved impressive results on various tasks. To\nfurther improve the effectiveness and efficiency of the transformer, there are\ntwo trains of thought among existing works: (1) going wider by scaling to more\ntrainable parameters; (2) going shallower by parameter sharing or model\ncompressing along with the depth. However, larger models usually do not scale\nwell when fewer tokens are available to train, and advanced parallelisms are\nrequired when the model is extremely large. Smaller models usually achieve\ninferior performance compared to the original transformer model due to the loss\nof representation power. In this paper, to achieve better performance with\nfewer trainable parameters, we propose a framework to deploy trainable\nparameters efficiently, by going wider instead of deeper. Specially, we scale\nalong model width by replacing feed-forward network (FFN) with\nmixture-of-experts (MoE). We then share the MoE layers across transformer\nblocks using individual layer normalization. Such deployment plays the role to\ntransform various semantic representations, which makes the model more\nparameter-efficient and effective. To evaluate our framework, we design WideNet\nand evaluate it on ImageNet-1K. Our best model outperforms Vision Transformer\n(ViT) by $1.46\\%$ with $0.72 \\times$ trainable parameters. Using $0.46 \\times$\nand $0.13 \\times$ parameters, our WideNet can still surpass ViT and ViT-MoE by\n$0.83\\%$ and $2.08\\%$, respectively.",
          "link": "http://arxiv.org/abs/2107.11817",
          "publishedOn": "2021-07-27T02:03:32.966Z",
          "wordCount": 650,
          "title": "Go Wider Instead of Deeper. (arXiv:2107.11817v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abou_Kreisha_M/0/1/0/all/0/1\">Mohamed Taha Abou-Kreisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elnashar_H/0/1/0/all/0/1\">Hany Elnashar</a>",
          "description": "Automated Vehicle License Plate (VLP) detection and recognition have ended up\nbeing a significant research issue as of late. VLP localization and recognition\nare some of the most essential techniques for managing traffic using digital\ntechniques. In this paper, four smart systems are developed to recognize\nEgyptian vehicles license plates. Two systems are based on character\nrecognition, which are (System1, Characters Recognition with Classical Machine\nLearning) and (System2, Characters Recognition with Deep Machine Learning). The\nother two systems are based on the whole plate recognition which are (System3,\nWhole License Plate Recognition with Classical Machine Learning) and (System4,\nWhole License Plate Recognition with Deep Machine Learning). We use object\ndetection algorithms, and machine learning based object recognition algorithms.\nThe performance of the developed systems has been tested on real images, and\nthe experimental results demonstrate that the best detection accuracy rate for\nVLP is provided by using the deep learning method. Where the VLP detection\naccuracy rate is better than the classical system by 32%. However, the best\ndetection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is\nprovided by using the classical method. Where VLPAC detection accuracy rate is\nbetter than the deep learning-based system by 6%. Also, the results show that\ndeep learning is better than the classical technique used in VLP recognition\nprocesses. Where the recognition accuracy rate is better than the classical\nsystem by 8%. Finally, the paper output recommends a robust VLP recognition\nsystem based on both statistical and deep machine learning.",
          "link": "http://arxiv.org/abs/2107.11640",
          "publishedOn": "2021-07-27T02:03:32.941Z",
          "wordCount": 722,
          "title": "Deep Machine Learning Based Egyptian Vehicle License Plate Recognition Systems. (arXiv:2107.11640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bose_R/0/1/0/all/0/1\">Rupak Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_S/0/1/0/all/0/1\">Shivam Pande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>",
          "description": "As the field of remote sensing is evolving, we witness the accumulation of\ninformation from several modalities, such as multispectral (MS), hyperspectral\n(HSI), LiDAR etc. Each of these modalities possess its own distinct\ncharacteristics and when combined synergistically, perform very well in the\nrecognition and classification tasks. However, fusing multiple modalities in\nremote sensing is cumbersome due to highly disparate domains. Furthermore, the\nexisting methods do not facilitate cross-modal interactions. To this end, we\npropose a novel transformer based fusion method for HSI and LiDAR modalities.\nThe model is composed of stacked auto encoders that harness the cross key-value\npairs for HSI and LiDAR, thus establishing a communication between the two\nmodalities, while simultaneously using the CNNs to extract the spectral and\nspatial information from HSI and LiDAR. We test our model on Houston (Data\nFusion Contest - 2013) and MUUFL Gulfport datasets and achieve competitive\nresults.",
          "link": "http://arxiv.org/abs/2107.11585",
          "publishedOn": "2021-07-27T02:03:32.933Z",
          "wordCount": 605,
          "title": "Two Headed Dragons: Multimodal Fusion and Cross Modal Transactions. (arXiv:2107.11585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhaohui Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>",
          "description": "Bicubic downscaling is a prevalent technique used to reduce the video storage\nburden or to accelerate the downstream processing speed. However, the inverse\nupscaling step is non-trivial, and the downscaled video may also deteriorate\nthe performance of downstream tasks. In this paper, we propose a\nself-conditioned probabilistic framework for video rescaling to learn the\npaired downscaling and upscaling procedures simultaneously. During the\ntraining, we decrease the entropy of the information lost in the downscaling by\nmaximizing its probability conditioned on the strong spatial-temporal prior\ninformation within the downscaled video. After optimization, the downscaled\nvideo by our framework preserves more meaningful information, which is\nbeneficial for both the upscaling step and the downstream tasks, e.g., video\naction recognition task. We further extend the framework to a lossy video\ncompression system, in which a gradient estimator for non-differential\nindustrial lossy codecs is proposed for the end-to-end training of the whole\nsystem. Extensive experimental results demonstrate the superiority of our\napproach on video rescaling, video compression, and efficient action\nrecognition tasks.",
          "link": "http://arxiv.org/abs/2107.11639",
          "publishedOn": "2021-07-27T02:03:32.919Z",
          "wordCount": 610,
          "title": "Self-Conditioned Probabilistic Learning of Video Rescaling. (arXiv:2107.11639v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Man_Y/0/1/0/all/0/1\">Yunze Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakuma_P/0/1/0/all/0/1\">Prasanna Kumar Sivakuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OToole_M/0/1/0/all/0/1\">Matthew O&#x27;Toole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>",
          "description": "LiDAR sensors can be used to obtain a wide range of measurement signals other\nthan a simple 3D point cloud, and those signals can be leveraged to improve\nperception tasks like 3D object detection. A single laser pulse can be\npartially reflected by multiple objects along its path, resulting in multiple\nmeasurements called echoes. Multi-echo measurement can provide information\nabout object contours and semi-transparent surfaces which can be used to better\nidentify and locate objects. LiDAR can also measure surface reflectance\n(intensity of laser pulse return), as well as ambient light of the scene\n(sunlight reflected by objects). These signals are already available in\ncommercial LiDAR devices but have not been used in most LiDAR-based detection\nmodels. We present a 3D object detection model which leverages the full\nspectrum of measurement signals provided by LiDAR. First, we propose a\nmulti-signal fusion (MSF) module to combine (1) the reflectance and ambient\nfeatures extracted with a 2D CNN, and (2) point cloud features extracted using\na 3D graph neural network (GNN). Second, we propose a multi-echo aggregation\n(MEA) module to combine the information encoded in different set of echo\npoints. Compared with traditional single echo point cloud methods, our proposed\nMulti-Signal LiDAR Detector (MSLiD) extracts richer context information from a\nwider range of sensing measurements and achieves more accurate 3D object\ndetection. Experiments show that by incorporating the multi-modality of LiDAR,\nour method outperforms the state-of-the-art by up to 9.1%.",
          "link": "http://arxiv.org/abs/2107.11470",
          "publishedOn": "2021-07-27T02:03:32.901Z",
          "wordCount": 673,
          "title": "Multi-Echo LiDAR for 3D Object Detection. (arXiv:2107.11470v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiabo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>",
          "description": "Video activity localisation has recently attained increasing attention due to\nits practical values in automatically localising the most salient visual\nsegments corresponding to their language descriptions (sentences) from\nuntrimmed and unstructured videos. For supervised model training, a temporal\nannotation of both the start and end time index of each video segment for a\nsentence (a video moment) must be given. This is not only very expensive but\nalso sensitive to ambiguity and subjective annotation bias, a much harder task\nthan image labelling. In this work, we develop a more accurate\nweakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM)\nin video moment proposal generation and matching when only a paragraph\ndescription of activities without per-sentence temporal annotation is\navailable. Specifically, we explore two cross-sentence relational constraints:\n(1) Temporal ordering and (2) semantic consistency among sentences in a\nparagraph description of video activities. Existing weakly-supervised\ntechniques only consider within-sentence video segment correlations in training\nwithout considering cross-sentence paragraph context. This can mislead due to\nambiguous expressions of individual sentences with visually indiscriminate\nvideo moment proposals in isolation. Experiments on two publicly available\nactivity localisation datasets show the advantages of our approach over the\nstate-of-the-art weakly supervised methods, especially so when the video\nactivity descriptions become more complex.",
          "link": "http://arxiv.org/abs/2107.11443",
          "publishedOn": "2021-07-27T02:03:32.893Z",
          "wordCount": 650,
          "title": "Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation. (arXiv:2107.11443v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>",
          "description": "Non-Euclidean geometry with constant negative curvature, i.e., hyperbolic\nspace, has attracted sustained attention in the community of machine learning.\nHyperbolic space, owing to its ability to embed hierarchical structures\ncontinuously with low distortion, has been applied for learning data with\ntree-like structures. Hyperbolic Neural Networks (HNNs) that operate directly\nin hyperbolic space have also been proposed recently to further exploit the\npotential of hyperbolic representations. While HNNs have achieved better\nperformance than Euclidean neural networks (ENNs) on datasets with implicit\nhierarchical structure, they still perform poorly on standard classification\nbenchmarks such as CIFAR and ImageNet. The traditional wisdom is that it is\ncritical for the data to respect the hyperbolic geometry when applying HNNs. In\nthis paper, we first conduct an empirical study showing that the inferior\nperformance of HNNs on standard recognition datasets can be attributed to the\nnotorious vanishing gradient problem. We further discovered that this problem\nstems from the hybrid architecture of HNNs. Our analysis leads to a simple yet\neffective solution called Feature Clipping, which regularizes the hyperbolic\nembedding whenever its norm exceeding a given threshold. Our thorough\nexperiments show that the proposed method can successfully avoid the vanishing\ngradient problem when training HNNs with backpropagation. The improved HNNs are\nable to achieve comparable performance with ENNs on standard image recognition\ndatasets including MNIST, CIFAR10, CIFAR100 and ImageNet, while demonstrating\nmore adversarial robustness and stronger out-of-distribution detection\ncapability.",
          "link": "http://arxiv.org/abs/2107.11472",
          "publishedOn": "2021-07-27T02:03:32.886Z",
          "wordCount": 670,
          "title": "Free Hyperbolic Neural Networks with Limited Radii. (arXiv:2107.11472v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>",
          "description": "Deep learning methods are the de-facto solutions to a multitude of medical\nimage analysis tasks. Cardiac MRI segmentation is one such application which,\nlike many others, requires a large number of annotated data so a trained\nnetwork can generalize well. Unfortunately, the process of having a large\nnumber of manually curated images by medical experts is both slow and utterly\nexpensive. In this paper, we set out to explore whether expert knowledge is a\nstrict requirement for the creation of annotated datasets that machine learning\ncan successfully train on. To do so, we gauged the performance of three\nsegmentation models, namely U-Net, Attention U-Net, and ENet, trained with\ndifferent loss functions on expert and non-expert groundtruth for cardiac\ncine-MRI segmentation. Evaluation was done with classic segmentation metrics\n(Dice index and Hausdorff distance) as well as clinical measurements, such as\nthe ventricular ejection fractions and the myocardial mass. Results reveal that\ngeneralization performances of a segmentation neural network trained on\nnon-expert groundtruth data is, to all practical purposes, as good as on expert\ngroundtruth data, in particular when the non-expert gets a decent level of\ntraining, highlighting an opportunity for the efficient and cheap creation of\nannotations for cardiac datasets.",
          "link": "http://arxiv.org/abs/2107.11447",
          "publishedOn": "2021-07-27T02:03:32.855Z",
          "wordCount": 648,
          "title": "Deep Learning Based Cardiac MRI Segmentation: Do We Need Experts?. (arXiv:2107.11447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wuzhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>",
          "description": "Accurate image segmentation plays a crucial role in medical image analysis,\nyet it faces great challenges of various shapes, diverse sizes, and blurry\nboundaries. To address these difficulties, square kernel-based encoder-decoder\narchitecture has been proposed and widely used, but its performance remains\nstill unsatisfactory. To further cope with these challenges, we present a novel\ndouble-branch encoder architecture. Our architecture is inspired by two\nobservations: 1) Since the discrimination of features learned via square\nconvolutional kernels needs to be further improved, we propose to utilize\nnon-square vertical and horizontal convolutional kernels in the double-branch\nencoder, so features learned by the two branches can be expected to complement\neach other. 2) Considering that spatial attention can help models to better\nfocus on the target region in a large-sized image, we develop an attention loss\nto further emphasize the segmentation on small-sized targets. Together, the\nabove two schemes give rise to a novel double-branch encoder segmentation\nframework for medical image segmentation, namely Crosslink-Net. The experiments\nvalidate the effectiveness of our model on four datasets. The code is released\nat https://github.com/Qianyu1226/Crosslink-Net.",
          "link": "http://arxiv.org/abs/2107.11517",
          "publishedOn": "2021-07-27T02:03:32.846Z",
          "wordCount": 644,
          "title": "Crosslink-Net: Double-branch Encoder Segmentation Network via Fusing Vertical and Horizontal Convolutions. (arXiv:2107.11517v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dao_S/0/1/0/all/0/1\">Son D.Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1\">Ethan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "Recently, as an effective way of learning latent representations, contrastive\nlearning has been increasingly popular and successful in various domains. The\nsuccess of constrastive learning in single-label classifications motivates us\nto leverage this learning framework to enhance distinctiveness for better\nperformance in multi-label image classification. In this paper, we show that a\ndirect application of contrastive learning can hardly improve in multi-label\ncases. Accordingly, we propose a novel framework for multi-label classification\nwith contrastive learning in a fully supervised setting, which learns multiple\nrepresentations of an image under the context of different labels. This\nfacilities a simple yet intuitive adaption of contrastive learning into our\nmodel to boost its performance in multi-label image classification. Extensive\nexperiments on two benchmark datasets show that the proposed framework achieves\nstate-of-the-art performance in the comparison with the advanced methods in\nmulti-label classification.",
          "link": "http://arxiv.org/abs/2107.11626",
          "publishedOn": "2021-07-27T02:03:32.823Z",
          "wordCount": 570,
          "title": "Multi-Label Image Classification with Contrastive Learning. (arXiv:2107.11626v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongseok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+GunheeKim/0/1/0/all/0/1\">GunheeKim</a>",
          "description": "We present an approach named the Cycled Composition Network that can measure\nthe semantic distance of the composition of image-text embedding. First, the\nComposition Network transit a reference image to target image in an embedding\nspace using relative caption. Second, the Correction Network calculates a\ndifference between reference and retrieved target images in the embedding space\nand match it with a relative caption. Our goal is to learn a Composition\nmapping with the Composition Network. Since this one-way mapping is highly\nunder-constrained, we couple it with an inverse relation learning with the\nCorrection Network and introduce a cycled relation for given Image We\nparticipate in Fashion IQ 2020 challenge and have won the first place with the\nensemble of our model.",
          "link": "http://arxiv.org/abs/2107.11509",
          "publishedOn": "2021-07-27T02:03:32.816Z",
          "wordCount": 566,
          "title": "Cycled Compositional Learning between Images and Text. (arXiv:2107.11509v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_I/0/1/0/all/0/1\">Ian E. Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dera_D/0/1/0/all/0/1\">Dimah Dera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1\">Ravi P. Ramachandran</a>",
          "description": "With the rise of deep neural networks, the challenge of explaining the\npredictions of these networks has become increasingly recognized. While many\nmethods for explaining the decisions of deep neural networks exist, there is\ncurrently no consensus on how to evaluate them. On the other hand, robustness\nis a popular topic for deep learning research; however, it is hardly talked\nabout in explainability until very recently. In this tutorial paper, we start\nby presenting gradient-based interpretability methods. These techniques use\ngradient signals to assign the burden of the decision on the input features.\nLater, we discuss how gradient-based methods can be evaluated for their\nrobustness and the role that adversarial robustness plays in having meaningful\nexplanations. We also discuss the limitations of gradient-based methods.\nFinally, we present the best practices and attributes that should be examined\nbefore choosing an explainability method. We conclude with the future\ndirections for research in the area at the convergence of robustness and\nexplainability.",
          "link": "http://arxiv.org/abs/2107.11400",
          "publishedOn": "2021-07-27T02:03:32.809Z",
          "wordCount": 617,
          "title": "Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks. (arXiv:2107.11400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11627",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiyuan Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chimitt_N/0/1/0/all/0/1\">Nicholas Chimitt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>",
          "description": "Fast and accurate simulation of imaging through atmospheric turbulence is\nessential for developing turbulence mitigation algorithms. Recognizing the\nlimitations of previous approaches, we introduce a new concept known as the\nphase-to-space (P2S) transform to significantly speed up the simulation. P2S is\nbuild upon three ideas: (1) reformulating the spatially varying convolution as\na set of invariant convolutions with basis functions, (2) learning the basis\nfunction via the known turbulence statistics models, (3) implementing the P2S\ntransform via a light-weight network that directly convert the phase\nrepresentation to spatial representation. The new simulator offers 300x --\n1000x speed up compared to the mainstream split-step simulators while\npreserving the essential turbulence statistics.",
          "link": "http://arxiv.org/abs/2107.11627",
          "publishedOn": "2021-07-27T02:03:32.798Z",
          "wordCount": 569,
          "title": "Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform. (arXiv:2107.11627v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11574",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lai_X/0/1/0/all/0/1\">Xuetian Lai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiongyao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_X/0/1/0/all/0/1\">Xiaopeng Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pu_J/0/1/0/all/0/1\">Jixiong Pu</a>",
          "description": "Reconstruction of image by using convolutional neural networks (CNNs) has\nbeen vigorously studied in the last decade. Until now, there have being\ndeveloped several techniques for imaging of a single object through scattering\nmedium by using neural networks, however how to reconstruct images of more than\none object simultaneously seems hard to realize. In this paper, we demonstrate\nan approach by using generative adversarial network (GAN) to reconstruct images\nof two adjacent objects through scattering media. We construct an imaging\nsystem for imaging of two adjacent objects behind the scattering media. In\ngeneral, as the light field of two adjacent object images pass through the\nscattering slab, a speckle pattern is obtained. The designed adversarial\nnetwork, which is called as YGAN, is employed to reconstruct the images\nsimultaneously. It is shown that based on the trained YGAN, we can reconstruct\nimages of two adjacent objects from one speckle pattern with high fidelity. In\naddition, we study the influence of the object image types, and the distance\nbetween the two adjacent objects on the fidelity of the reconstructed images.\nMoreover even if another scattering medium is inserted between the two objects,\nwe can also reconstruct the images of two objects from a speckle with high\nquality. The technique presented in this work can be used for applications in\nareas of medical image analysis, such as medical image classification,\nsegmentation, and studies of multi-object scattering imaging etc.",
          "link": "http://arxiv.org/abs/2107.11574",
          "publishedOn": "2021-07-27T02:03:32.791Z",
          "wordCount": 694,
          "title": "Reconstructing Images of Two Adjacent Objects through Scattering Medium Using Generative Adversarial Network. (arXiv:2107.11574v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiujun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1\">Weijian Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Cloth-changing person re-identification (re-ID) is a new rising research\ntopic that aims at retrieving pedestrians whose clothes are changed. This task\nis quite challenging and has not been fully studied to date. Current works\nmainly focus on body shape or contour sketch, but they are not robust enough\ndue to view and posture variations. The key to this task is to exploit\ncloth-irrelevant cues. This paper proposes a semantic-guided pixel sampling\napproach for the cloth-changing person re-ID task. We do not explicitly define\nwhich feature to extract but force the model to automatically learn\ncloth-irrelevant cues. Specifically, we first recognize the pedestrian's upper\nclothes and pants, then randomly change them by sampling pixels from other\npedestrians. The changed samples retain the identity labels but exchange the\npixels of clothes or pants among different pedestrians. Besides, we adopt a\nloss function to constrain the learned features to keep consistent before and\nafter changes. In this way, the model is forced to learn cues that are\nirrelevant to upper clothes and pants. We conduct extensive experiments on the\nlatest released PRCC dataset. Our method achieved 65.8% on Rank1 accuracy,\nwhich outperforms previous methods with a large margin. The code is available\nat https://github.com/shuxjweb/pixel_sampling.git.",
          "link": "http://arxiv.org/abs/2107.11522",
          "publishedOn": "2021-07-27T02:03:32.773Z",
          "wordCount": 656,
          "title": "Semantic-guided Pixel Sampling for Cloth-Changing Person Re-identification. (arXiv:2107.11522v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moskvyak_O/0/1/0/all/0/1\">Olga Moskvyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maire_F/0/1/0/all/0/1\">Frederic Maire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1\">Feras Dayoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1\">Mahsa Baktashmotlagh</a>",
          "description": "Person re-identification is the challenging task of identifying a person\nacross different camera views. Training a convolutional neural network (CNN)\nfor this task requires annotating a large dataset, and hence, it involves the\ntime-consuming manual matching of people across cameras. To reduce the need for\nlabeled data, we focus on a semi-supervised approach that requires only a\nsubset of the training data to be labeled. We conduct a comprehensive survey in\nthe area of person re-identification with limited labels. Existing works in\nthis realm are limited in the sense that they utilize features from multiple\nCNNs and require the number of identities in the unlabeled data to be known. To\novercome these limitations, we propose to employ part-based features from a\nsingle CNN without requiring the knowledge of the label space (i.e., the number\nof identities). This makes our approach more suitable for practical scenarios,\nand it significantly reduces the need for computational resources. We also\npropose a PartMixUp loss that improves the discriminative ability of learned\npart-based features for pseudo-labeling in semi-supervised settings. Our method\noutperforms the state-of-the-art results on three large-scale person re-id\ndatasets and achieves the same level of performance as fully supervised methods\nwith only one-third of labeled identities.",
          "link": "http://arxiv.org/abs/2107.11566",
          "publishedOn": "2021-07-27T02:03:32.766Z",
          "wordCount": 634,
          "title": "Going Deeper into Semi-supervised Person Re-identification. (arXiv:2107.11566v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11645",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cao_W/0/1/0/all/0/1\">Wenming Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_P/0/1/0/all/0/1\">Philip L.H. Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lui_G/0/1/0/all/0/1\">Gilbert C.S. Lui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_K/0/1/0/all/0/1\">Keith W.H. Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1\">Ho-Ming Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yanwen Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuen_M/0/1/0/all/0/1\">Man-Fung Yuen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seto_W/0/1/0/all/0/1\">Wai-Kay Seto</a>",
          "description": "In this work, we propose a new segmentation network by integrating DenseUNet\nand bidirectional LSTM together with attention mechanism, termed as\nDA-BDense-UNet. DenseUNet allows learning enough diverse features and enhancing\nthe representative power of networks by regulating the information flow.\nBidirectional LSTM is responsible to explore the relationships between the\nencoded features and the up-sampled features in the encoding and decoding\npaths. Meanwhile, we introduce attention gates (AG) into DenseUNet to diminish\nresponses of unrelated background regions and magnify responses of salient\nregions progressively. Besides, the attention in bidirectional LSTM takes into\naccount the contribution differences of the encoded features and the up-sampled\nfeatures in segmentation improvement, which can in turn adjust proper weights\nfor these two kinds of features. We conduct experiments on liver CT image data\nsets collected from multiple hospitals by comparing them with state-of-the-art\nsegmentation models. Experimental results indicate that our proposed method\nDA-BDense-UNet has achieved comparative performance in terms of dice\ncoefficient, which demonstrates its effectiveness.",
          "link": "http://arxiv.org/abs/2107.11645",
          "publishedOn": "2021-07-27T02:03:32.757Z",
          "wordCount": 618,
          "title": "Dual-Attention Enhanced BDense-UNet for Liver Lesion Segmentation. (arXiv:2107.11645v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tirupattur_P/0/1/0/all/0/1\">Praveen Tirupattur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Aayush J Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangam_T/0/1/0/all/0/1\">Tushar Sangam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1\">Shruti Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh S Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "This paper summarizes the TinyAction challenge which was organized in\nActivityNet workshop at CVPR 2021. This challenge focuses on recognizing\nreal-world low-resolution activities present in videos. Action recognition task\nis currently focused around classifying the actions from high-quality videos\nwhere the actors and the action is clearly visible. While various approaches\nhave been shown effective for recognition task in recent works, they often do\nnot deal with videos of lower resolution where the action is happening in a\ntiny region. However, many real world security videos often have the actual\naction captured in a small resolution, making action recognition in a tiny\nregion a challenging task. In this work, we propose a benchmark dataset,\nTinyVIRAT-v2, which is comprised of naturally occuring low-resolution actions.\nThis is an extension of the TinyVIRAT dataset and consists of actions with\nmultiple labels. The videos are extracted from security videos which makes them\nrealistic and more challenging. We use current state-of-the-art action\nrecognition methods on the dataset as a benchmark, and propose the TinyAction\nChallenge.",
          "link": "http://arxiv.org/abs/2107.11494",
          "publishedOn": "2021-07-27T02:03:32.750Z",
          "wordCount": 621,
          "title": "TinyAction Challenge: Recognizing Real-world Low-resolution Activities in Videos. (arXiv:2107.11494v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1\">Kien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "We propose a novel framework for image clustering that incorporates joint\nrepresentation learning and clustering. Our method consists of two heads that\nshare the same backbone network - a \"representation learning\" head and a\n\"clustering\" head. The \"representation learning\" head captures fine-grained\npatterns of objects at the instance level which serve as clues for the\n\"clustering\" head to extract coarse-grain information that separates objects\ninto clusters. The whole model is trained in an end-to-end manner by minimizing\nthe weighted sum of two sample-oriented contrastive losses applied to the\noutputs of the two heads. To ensure that the contrastive loss corresponding to\nthe \"clustering\" head is optimal, we introduce a novel critic function called\n\"log-of-dot-product\". Extensive experimental results demonstrate that our\nmethod significantly outperforms state-of-the-art single-stage clustering\nmethods across a variety of image datasets, improving over the best baseline by\nabout 5-7% in accuracy on CIFAR10/20, STL10, and ImageNet-Dogs. Further, the\n\"two-stage\" variant of our method also achieves better results than baselines\non three challenging ImageNet subsets.",
          "link": "http://arxiv.org/abs/2107.11635",
          "publishedOn": "2021-07-27T02:03:32.743Z",
          "wordCount": 606,
          "title": "Clustering by Maximizing Mutual Information Across Views. (arXiv:2107.11635v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blumer_K/0/1/0/all/0/1\">Katy Blumer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1\">Jon Kleinberg</a>",
          "description": "We analyze a dataset of retinal images using linear probes: linear regression\nmodels trained on some \"target\" task, using embeddings from a deep\nconvolutional (CNN) model trained on some \"source\" task as input. We use this\nmethod across all possible pairings of 93 tasks in the UK Biobank dataset of\nretinal images, leading to ~164k different models. We analyze the performance\nof these linear probes by source and target task and by layer depth. We observe\nthat representations from the middle layers of the network are more\ngeneralizable. We find that some target tasks are easily predicted irrespective\nof the source task, and that some other target tasks are more accurately\npredicted from correlated source tasks than from embeddings trained on the same\ntask.",
          "link": "http://arxiv.org/abs/2107.11468",
          "publishedOn": "2021-07-27T02:03:32.674Z",
          "wordCount": 596,
          "title": "Using a Cross-Task Grid of Linear Probes to Interpret CNN Model Predictions On Retinal Images. (arXiv:2107.11468v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maalouf_A/0/1/0/all/0/1\">Alaa Maalouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_O/0/1/0/all/0/1\">Oren Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1\">Dan Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "We present a novel global compression framework for deep neural networks that\nautomatically analyzes each layer to identify the optimal per-layer compression\nratio, while simultaneously achieving the desired overall compression. Our\nalgorithm hinges on the idea of compressing each convolutional (or\nfully-connected) layer by slicing its channels into multiple groups and\ndecomposing each group via low-rank decomposition. At the core of our algorithm\nis the derivation of layer-wise error bounds from the Eckart Young Mirsky\ntheorem. We then leverage these bounds to frame the compression problem as an\noptimization problem where we wish to minimize the maximum compression error\nacross layers and propose an efficient algorithm towards a solution. Our\nexperiments indicate that our method outperforms existing low-rank compression\napproaches across a wide range of networks and data sets. We believe that our\nresults open up new avenues for future research into the global\nperformance-size trade-offs of modern neural networks. Our code is available at\nhttps://github.com/lucaslie/torchprune.",
          "link": "http://arxiv.org/abs/2107.11442",
          "publishedOn": "2021-07-27T02:03:32.639Z",
          "wordCount": 602,
          "title": "Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition. (arXiv:2107.11442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yeli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1\">Daniel Jun Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1\">Arvind Easwaran</a>",
          "description": "Uncertainties in machine learning are a significant roadblock for its\napplication in safety-critical cyber-physical systems (CPS). One source of\nuncertainty arises from distribution shifts in the input data between training\nand test scenarios. Detecting such distribution shifts in real-time is an\nemerging approach to address the challenge. The high dimensional input space in\nCPS applications involving imaging adds extra difficulty to the task.\nGenerative learning models are widely adopted for the task, namely\nout-of-distribution (OoD) detection. To improve the state-of-the-art, we\nstudied existing proposals from both machine learning and CPS fields. In the\nlatter, safety monitoring in real-time for autonomous driving agents has been a\nfocus. Exploiting the spatiotemporal correlation of motion in videos, we can\nrobustly detect hazardous motion around autonomous driving agents. Inspired by\nthe latest advances in the Variational Autoencoder (VAE) theory and practice,\nwe tapped into the prior knowledge in data to further boost OoD detection's\nrobustness. Comparison studies over nuScenes and Synthia data sets show our\nmethods significantly improve detection capabilities of OoD factors unique to\ndriving scenarios, 42% better than state-of-the-art approaches. Our model also\ngeneralized near-perfectly, 97% better than the state-of-the-art across the\nreal-world and simulation driving data sets experimented. Finally, we\ncustomized one proposed method into a twin-encoder model that can be deployed\nto resource limited embedded devices for real-time OoD detection. Its execution\ntime was reduced over four times in low-precision 8-bit integer inference,\nwhile detection capability is comparable to its corresponding floating-point\nmodel.",
          "link": "http://arxiv.org/abs/2107.11750",
          "publishedOn": "2021-07-27T02:03:32.602Z",
          "wordCount": 686,
          "title": "Improving Variational Autoencoder based Out-of-Distribution Detection for Embedded Real-time Applications. (arXiv:2107.11750v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1\">Nasibullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>",
          "description": "Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning-based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. This paper addresses the\ndrawback by introducing a dynamic loss network (DLN), which provides an\nadditional feedback signal that directly reflects the evaluation metrics. Our\nresults on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to\nText (MSRVTT) datasets outperform previous methods.",
          "link": "http://arxiv.org/abs/2107.11707",
          "publishedOn": "2021-07-27T02:03:32.583Z",
          "wordCount": 585,
          "title": "Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuqian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "Given a video demonstration, can we imitate the action contained in this\nvideo? In this paper, we introduce a novel task, dubbed mesh-based action\nimitation. The goal of this task is to enable an arbitrary target human mesh to\nperform the same action shown on the video demonstration. To achieve this, a\nnovel Mesh-based Video Action Imitation (M-VAI) method is proposed by us. M-VAI\nfirst learns to reconstruct the meshes from the given source image frames, then\nthe initial recovered mesh sequence is fed into mesh2mesh, a mesh sequence\nsmooth module proposed by us, to improve the temporal consistency. Finally, we\nimitate the actions by transferring the pose from the constructed human body to\nour target identity mesh. High-quality and detailed human body meshes can be\ngenerated by using our M-VAI. Extensive experiments demonstrate the feasibility\nof our task and the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.11756",
          "publishedOn": "2021-07-27T02:03:32.576Z",
          "wordCount": 599,
          "title": "Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos. (arXiv:2107.11756v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Man Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Huanhuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangshe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaoxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>",
          "description": "How to effectively and efficiently deal with spatio-temporal event streams,\nwhere the events are generally sparse and non-uniform and have the microsecond\ntemporal resolution, is of great value and has various real-life applications.\nSpiking neural network (SNN), as one of the brain-inspired event-triggered\ncomputing models, has the potential to extract effective spatio-temporal\nfeatures from the event streams. However, when aggregating individual events\ninto frames with a new higher temporal resolution, existing SNN models do not\nattach importance to that the serial frames have different signal-to-noise\nratios since event streams are sparse and non-uniform. This situation\ninterferes with the performance of existing SNNs. In this work, we propose a\ntemporal-wise attention SNN (TA-SNN) model to learn frame-based representation\nfor processing event streams. Concretely, we extend the attention concept to\ntemporal-wise input to judge the significance of frames for the final decision\nat the training stage, and discard the irrelevant frames at the inference\nstage. We demonstrate that TA-SNN models improve the accuracy of event streams\nclassification tasks. We also study the impact of multiple-scale temporal\nresolutions for frame-based representation. Our approach is tested on three\ndifferent classification tasks: gesture recognition, image classification, and\nspoken digit recognition. We report the state-of-the-art results on these\ntasks, and get the essential improvement of accuracy (almost 19\\%) for gesture\nrecognition with only 60 ms.",
          "link": "http://arxiv.org/abs/2107.11711",
          "publishedOn": "2021-07-27T02:03:32.569Z",
          "wordCount": 668,
          "title": "Temporal-wise Attention Spiking Neural Networks for Event Streams Classification. (arXiv:2107.11711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>",
          "description": "In this paper, we focus on the challenging multicategory instance\nsegmentation problem in remote sensing images (RSIs), which aims at predicting\nthe categories of all instances and localizing them with pixel-level masks.\nAlthough many landmark frameworks have demonstrated promising performance in\ninstance segmentation, the complexity in the background and scale variability\ninstances still remain challenging for instance segmentation of RSIs. To\naddress the above problems, we propose an end-to-end multi-category instance\nsegmentation model, namely Semantic Attention and Scale Complementary Network,\nwhich mainly consists of a Semantic Attention (SEA) module and a Scale\nComplementary Mask Branch (SCMB). The SEA module contains a simple fully\nconvolutional semantic segmentation branch with extra supervision to strengthen\nthe activation of interest instances on the feature map and reduce the\nbackground noise's interference. To handle the under-segmentation of geospatial\ninstances with large varying scales, we design the SCMB that extends the\noriginal single mask branch to trident mask branches and introduces\ncomplementary mask supervision at different scales to sufficiently leverage the\nmulti-scale information. We conduct comprehensive experiments to evaluate the\neffectiveness of our proposed method on the iSAID dataset and the NWPU Instance\nSegmentation dataset and achieve promising performance.",
          "link": "http://arxiv.org/abs/2107.11758",
          "publishedOn": "2021-07-27T02:03:32.551Z",
          "wordCount": 648,
          "title": "Semantic Attention and Scale Complementary Network for Instance Segmentation in Remote Sensing Images. (arXiv:2107.11758v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1\">Ali Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1\">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Huaiyu Dai</a>",
          "description": "Adversarial training has been shown as an effective approach to improve the\nrobustness of image classifiers against white-box attacks. However, its\neffectiveness against black-box attacks is more nuanced. In this work, we\ndemonstrate that some geometric consequences of adversarial training on the\ndecision boundary of deep networks give an edge to certain types of black-box\nattacks. In particular, we define a metric called robustness gain to show that\nwhile adversarial training is an effective method to dramatically improve the\nrobustness in white-box scenarios, it may not provide such a good robustness\ngain against the more realistic decision-based black-box attacks. Moreover, we\nshow that even the minimal perturbation white-box attacks can converge faster\nagainst adversarially-trained neural networks compared to the regular ones.",
          "link": "http://arxiv.org/abs/2107.11671",
          "publishedOn": "2021-07-27T02:03:32.544Z",
          "wordCount": 569,
          "title": "Adversarial training may be a double-edged sword. (arXiv:2107.11671v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qiang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yang Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yunxiao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenxu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>",
          "description": "Despite the great success achieved by deep learning methods in face\nrecognition, severe performance drops are observed for large pose variations in\nunconstrained environments (e.g., in cases of surveillance and photo-tagging).\nTo address it, current methods either deploy pose-specific models or frontalize\nfaces by additional modules. Still, they ignore the fact that identity\ninformation should be consistent across poses and are not realizing the data\nimbalance between frontal and profile face images during training. In this\npaper, we propose an efficient PoseFace framework which utilizes the facial\nlandmarks to disentangle the pose-invariant features and exploits a\npose-adaptive loss to handle the imbalance issue adaptively. Extensive\nexperimental results on the benchmarks of Multi-PIE, CFP, CPLFW and IJB have\ndemonstrated the superiority of our method over the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2107.11721",
          "publishedOn": "2021-07-27T02:03:32.537Z",
          "wordCount": 574,
          "title": "PoseFace: Pose-Invariant Features and Pose-Adaptive Loss for Face Recognition. (arXiv:2107.11721v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhang Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongsheng_H/0/1/0/all/0/1\">Huang Hongsheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jianchao_T/0/1/0/all/0/1\">Tan Jianchao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongmin_X/0/1/0/all/0/1\">Xu Hongmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guozhu_P/0/1/0/all/0/1\">Peng Guozhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Liu Ji</a>",
          "description": "Analyzing and understanding hand information from multimedia materials like\nimages or videos is important for many real world applications and remains\nactive in research community. There are various works focusing on recovering\nhand information from single image, however, they usually solve a single task,\nfor example, hand mask segmentation, 2D/3D hand pose estimation, or hand mesh\nreconstruction and perform not well in challenging scenarios. To further\nimprove the performance of these tasks, we propose a novel Hand Image\nUnderstanding (HIU) framework to extract comprehensive information of the hand\nobject from a single RGB image, by jointly considering the relationships\nbetween these tasks. To achieve this goal, a cascaded multi-task learning (MTL)\nbackbone is designed to estimate the 2D heat maps, to learn the segmentation\nmask, and to generate the intermediate 3D information encoding, followed by a\ncoarse-to-fine learning paradigm and a self-supervised learning strategy.\nQualitative experiments demonstrate that our approach is capable of recovering\nreasonable mesh representations even in challenging situations. Quantitatively,\nour method significantly outperforms the state-of-the-art approaches on various\nwidely-used datasets, in terms of diverse evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.11646",
          "publishedOn": "2021-07-27T02:03:32.530Z",
          "wordCount": 626,
          "title": "Hand Image Understanding via Deep Multi-Task Learning. (arXiv:2107.11646v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11696",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Calderon_Ramirez_S/0/1/0/all/0/1\">Saul Calderon-Ramirez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murillo_Hernandez_D/0/1/0/all/0/1\">Diego Murillo-Hernandez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rojas_Salazar_K/0/1/0/all/0/1\">Kevin Rojas-Salazar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elizondo_D/0/1/0/all/0/1\">David Elizondo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shengxiang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Molina_Cabello_M/0/1/0/all/0/1\">Miguel Molina-Cabello</a>",
          "description": "The implementation of deep learning based computer aided diagnosis systems\nfor the classification of mammogram images can help in improving the accuracy,\nreliability, and cost of diagnosing patients. However, training a deep learning\nmodel requires a considerable amount of labeled images, which can be expensive\nto obtain as time and effort from clinical practitioners is required. A number\nof publicly available datasets have been built with data from different\nhospitals and clinics. However, using models trained on these datasets for\nlater work on images sampled from a different hospital or clinic might result\nin lower performance. This is due to the distribution mismatch of the datasets,\nwhich include different patient populations and image acquisition protocols.\nThe scarcity of labeled data can also bring a challenge towards the application\nof transfer learning with models trained using these source datasets. In this\nwork, a real world scenario is evaluated where a novel target dataset sampled\nfrom a private Costa Rican clinic is used, with few labels and heavily\nimbalanced data. The use of two popular and publicly available datasets\n(INbreast and CBIS-DDSM) as source data, to train and test the models on the\nnovel target dataset, is evaluated. The use of the semi-supervised deep\nlearning approach known as MixMatch, to leverage the usage of unlabeled data\nfrom the target dataset, is proposed and evaluated. In the tests, the\nperformance of models is extensively measured, using different metrics to\nassess the performance of a classifier under heavy data imbalance conditions.\nIt is shown that the use of semi-supervised deep learning combined with\nfine-tuning can provide a meaningful advantage when using scarce labeled\nobservations. We make available the novel dataset for the benefit of the\ncommunity.",
          "link": "http://arxiv.org/abs/2107.11696",
          "publishedOn": "2021-07-27T02:03:32.522Z",
          "wordCount": 749,
          "title": "A Real Use Case of Semi-Supervised Learning for Mammogram Classification in a Local Clinic of Costa Rica. (arXiv:2107.11696v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1\">Kemal Oksuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cam_B/0/1/0/all/0/1\">Baris Can Cam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1\">Sinan Kalkan</a>",
          "description": "We propose Rank & Sort (RS) Loss, as a ranking-based loss function to train\ndeep object detection and instance segmentation methods (i.e. visual\ndetectors). RS Loss supervises the classifier, a sub-network of these methods,\nto rank each positive above all negatives as well as to sort positives among\nthemselves with respect to (wrt.) their continuous localisation qualities (e.g.\nIntersection-over-Union - IoU). To tackle the non-differentiable nature of\nranking and sorting, we reformulate the incorporation of error-driven update\nwith backpropagation as Identity Update, which enables us to model our novel\nsorting error among positives. With RS Loss, we significantly simplify\ntraining: (i) Thanks to our sorting objective, the positives are prioritized by\nthe classifier without an additional auxiliary head (e.g. for centerness, IoU,\nmask-IoU), (ii) due to its ranking-based nature, RS Loss is robust to class\nimbalance, and thus, no sampling heuristic is required, and (iii) we address\nthe multi-task nature of visual detectors using tuning-free task-balancing\ncoefficients. Using RS Loss, we train seven diverse visual detectors only by\ntuning the learning rate, and show that it consistently outperforms baselines:\ne.g. our RS Loss improves (i) Faster R-CNN by ~ 3 box AP and aLRP Loss\n(ranking-based baseline) by ~ 2 box AP on COCO dataset, (ii) Mask R-CNN with\nrepeat factor sampling (RFS) by 3.5 mask AP (~ 7 AP for rare classes) on LVIS\ndataset; and also outperforms all counterparts. Code available at\nhttps://github.com/kemaloksuz/RankSortLoss",
          "link": "http://arxiv.org/abs/2107.11669",
          "publishedOn": "2021-07-27T02:03:32.503Z",
          "wordCount": 682,
          "title": "Rank & Sort Loss for Object Detection and Instance Segmentation. (arXiv:2107.11669v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gihyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "One of the important research topics in image generative models is to\ndisentangle the spatial contents and styles for their separate control.\nAlthough StyleGAN can generate content feature vectors from random noises, the\nresulting spatial content control is primarily intended for minor spatial\nvariations, and the disentanglement of global content and styles is by no means\ncomplete. Inspired by a mathematical understanding of normalization and\nattention, here we present a novel hierarchical adaptive Diagonal spatial\nATtention (DAT) layers to separately manipulate the spatial contents from\nstyles in a hierarchical manner. Using DAT and AdaIN, our method enables\ncoarse-to-fine level disentanglement of spatial contents and styles. In\naddition, our generator can be easily integrated into the GAN inversion\nframework so that the content and style of translated images from multi-domain\nimage translation tasks can be flexibly controlled. By using various datasets,\nwe confirm that the proposed method not only outperforms the existing models in\ndisentanglement scores, but also provides more flexible control over spatial\nfeatures in the generated images.",
          "link": "http://arxiv.org/abs/2103.16146",
          "publishedOn": "2021-07-26T02:00:58.639Z",
          "wordCount": 642,
          "title": "Diagonal Attention and Style-based GAN for Content-Style Disentanglement in Image Generation and Translation. (arXiv:2103.16146v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1\">Rawal Khirodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chari_V/0/1/0/all/0/1\">Visesh Chari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Amit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Ambrish Tyagi</a>",
          "description": "A key assumption of top-down human pose estimation approaches is their\nexpectation of having a single person/instance present in the input bounding\nbox. This often leads to failures in crowded scenes with occlusions. We propose\na novel solution to overcome the limitations of this fundamental assumption.\nOur Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose\ninstances within a given bounding box. We introduce a Multi-Instance Modulation\nBlock (MIMB) that can adaptively modulate channel-wise feature responses for\neach instance and is parameter efficient. We demonstrate the efficacy of our\napproach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically,\nwe achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant\nimprovement of 2.4 AP and 6.5 AP over the prior art, respectively. When using\nground truth bounding boxes for inference, MIPNet achieves an improvement of\n0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets\ncompared to HRNet. Interestingly, when fewer, high confidence bounding boxes\nare used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet\nmaintains a relatively stable performance (drop of 1 AP) for the same inputs.",
          "link": "http://arxiv.org/abs/2101.11223",
          "publishedOn": "2021-07-26T02:00:58.630Z",
          "wordCount": 656,
          "title": "Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation. (arXiv:2101.11223v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Song Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhihao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakker_E/0/1/0/all/0/1\">Erwin M.Bakker</a>",
          "description": "Due to its effectivity and efficiency, deep hashing approaches are widely\nused for large-scale visual search. However, it is still challenging to produce\ncompact and discriminative hash codes for images associated with multiple\nsemantics for two main reasons, 1) similarity constraints designed in most of\nthe existing methods are based upon an oversimplified similarity\nassignment(i.e., 0 for instance pairs sharing no label, 1 for instance pairs\nsharing at least 1 label), 2) the exploration in multi-semantic relevance are\ninsufficient or even neglected in many of the existing methods. These problems\nsignificantly limit the discrimination of generated hash codes. In this paper,\nwe propose a novel self-supervised asymmetric deep hashing method with a\nmargin-scalable constraint(SADH) approach to cope with these problems. SADH\nimplements a self-supervised network to sufficiently preserve semantic\ninformation in a semantic feature dictionary and a semantic code dictionary for\nthe semantics of the given dataset, which efficiently and precisely guides a\nfeature learning network to preserve multilabel semantic information using an\nasymmetric learning strategy. By further exploiting semantic dictionaries, a\nnew margin-scalable constraint is employed for both precise similarity\nsearching and robust hash code generation. Extensive empirical research on four\npopular benchmarks validates the proposed method and shows it outperforms\nseveral state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2012.03820",
          "publishedOn": "2021-07-26T02:00:58.623Z",
          "wordCount": 674,
          "title": "Self-supervised asymmetric deep hashing with margin-scalable constraint. (arXiv:2012.03820v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaofo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>",
          "description": "Convolutional Neural Networks (CNNs) have achieved great success due to the\npowerful feature learning ability of convolution layers. Specifically, the\nstandard convolution traverses the input images/features using a sliding window\nscheme to extract features. However, not all the windows contribute equally to\nthe prediction results of CNNs. In practice, the convolutional operation on\nsome of the windows (e.g., smooth windows that contain very similar pixels) can\nbe very redundant and may introduce noises into the computation. Such\nredundancy may not only deteriorate the performance but also incur the\nunnecessary computational cost. Thus, it is important to reduce the\ncomputational redundancy of convolution to improve the performance. To this\nend, we propose a Content-aware Convolution (CAC) that automatically detects\nthe smooth windows and applies a 1x1 convolutional kernel to replace the\noriginal large kernel. In this sense, we are able to effectively avoid the\nredundant computation on similar pixels. By replacing the standard convolution\nin CNNs with our CAC, the resultant models yield significantly better\nperformance and lower computational cost than the baseline models with the\nstandard convolution. More critically, we are able to dynamically allocate\nsuitable computation resources according to the data smoothness of different\nimages, making it possible for content-aware computation. Extensive experiments\non various computer vision tasks demonstrate the superiority of our method over\nexisting methods.",
          "link": "http://arxiv.org/abs/2106.15797",
          "publishedOn": "2021-07-26T02:00:58.616Z",
          "wordCount": 672,
          "title": "Content-Aware Convolutional Neural Networks. (arXiv:2106.15797v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1\">Ross Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1\">Pujitha Gunaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "With increasing automation in passenger vehicles, the study of safe and\nsmooth occupant-vehicle interaction and control transitions is key. In this\nstudy, we focus on the development of contextual, semantically meaningful\nrepresentations of the driver state, which can then be used to determine the\nappropriate timing and conditions for transfer of control between driver and\nvehicle. To this end, we conduct a large-scale real-world controlled data study\nwhere participants are instructed to take-over control from an autonomous agent\nunder different driving conditions while engaged in a variety of distracting\nactivities. These take-over events are captured using multiple driver-facing\ncameras, which when labelled result in a dataset of control transitions and\ntheir corresponding take-over times (TOTs). We then develop and train TOT\nmodels that operate sequentially on mid to high-level features produced by\ncomputer vision algorithms operating on different driver-facing camera views.\nThe proposed TOT model produces continuous predictions of take-over times\nwithout delay, and shows promising qualitative and quantitative results in\ncomplex real-world scenarios.",
          "link": "http://arxiv.org/abs/2104.11489",
          "publishedOn": "2021-07-26T02:00:58.608Z",
          "wordCount": 648,
          "title": "Autonomous Vehicles that Alert Humans to Take-Over Controls: Modeling with Real-World Data. (arXiv:2104.11489v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borowa_A/0/1/0/all/0/1\">Adriana Borowa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rymarczyk_D/0/1/0/all/0/1\">Dawid Rymarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochonska_D/0/1/0/all/0/1\">Dorota Ocho&#x144;ska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brzychczy_Wloch_M/0/1/0/all/0/1\">Monika Brzychczy-W&#x142;och</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1\">Bartosz Zieli&#x144;ski</a>",
          "description": "In this work, we analyze if it is possible to distinguish between different\nclones of the same bacteria species (Klebsiella pneumoniae) based only on\nmicroscopic images. It is a challenging task, previously considered impossible\ndue to the high clones similarity. For this purpose, we apply a multi-step\nalgorithm with attention-based multiple instance learning. Except for obtaining\naccuracy at the level of 0.9, we introduce extensive interpretability based on\nCellProfiler and persistence homology, increasing the understandability and\ntrust in the model.",
          "link": "http://arxiv.org/abs/2012.01189",
          "publishedOn": "2021-07-26T02:00:58.588Z",
          "wordCount": 573,
          "title": "Classifying bacteria clones using attention-based deep multiple instance learning interpreted by persistence homology. (arXiv:2012.01189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loddo_A/0/1/0/all/0/1\">Andrea Loddo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruberto_C/0/1/0/all/0/1\">Cecilia Di Ruberto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vale_A/0/1/0/all/0/1\">A.M.P.G. Vale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ucchesu_M/0/1/0/all/0/1\">Mariano Ucchesu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_J/0/1/0/all/0/1\">J.M. Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacchetta_G/0/1/0/all/0/1\">Gianluigi Bacchetta</a>",
          "description": "Image analysis is an essential field for several topics in the life sciences,\nsuch as biology or botany. In particular, the analysis of seeds (e.g. fossil\nresearch) can provide significant information on their evolution, the history\nof agriculture, plant domestication and knowledge of diets in ancient times.\nThis work aims to present software that performs image analysis for feature\nextraction and classification from images containing seeds through a novel and\nunique framework. In detail, we propose two plugins \\emph{ImageJ}, one able to\nextract morphological, textual and colour features from seed images, and\nanother to classify seeds into categories using the extracted features. The\nexperimental results demonstrated the correctness and validity of both the\nextracted features and the classification predictions. The proposed tool is\neasily extendable to other fields of image analysis.",
          "link": "http://arxiv.org/abs/2103.17213",
          "publishedOn": "2021-07-26T02:00:58.581Z",
          "wordCount": 601,
          "title": "An effective and friendly tool for seed image analysis. (arXiv:2103.17213v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popli_A/0/1/0/all/0/1\">Additya Popli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_S/0/1/0/all/0/1\">Saraansh Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoe_N/0/1/0/all/0/1\">Naoyuki Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okubo_A/0/1/0/all/0/1\">Atsushi Okubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_A/0/1/0/all/0/1\">Anoop Namboodiri</a>",
          "description": "Typical fingerprint recognition systems are comprised of a spoof detection\nmodule and a subsequent recognition module, running one after the other. In\nthis paper, we reformulate the workings of a typical fingerprint recognition\nsystem. In particular, we posit that both spoof detection and fingerprint\nrecognition are correlated tasks. Therefore, rather than performing the two\ntasks separately, we propose a joint model for spoof detection and matching to\nsimultaneously perform both tasks without compromising the accuracy of either\ntask. We demonstrate the capability of our joint model to obtain an\nauthentication accuracy (1:1 matching) of TAR = 100% @ FAR = 0.1% on the FVC\n2006 DB2A dataset while achieving a spoof detection ACE of 1.44% on the LiveDet\n2015 dataset, both maintaining the performance of stand-alone methods. In\npractice, this reduces the time and memory requirements of the fingerprint\nrecognition system by 50% and 40%, respectively; a significant advantage for\nrecognition systems running on resource-constrained devices and communication\nchannels. The project page for our work is available at\nhttps://www.bit.ly/ijcb2021-unified .",
          "link": "http://arxiv.org/abs/2104.03255",
          "publishedOn": "2021-07-26T02:00:58.574Z",
          "wordCount": 659,
          "title": "A Unified Model for Fingerprint Authentication and Presentation Attack Detection. (arXiv:2104.03255v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saksham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>",
          "description": "Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.",
          "link": "http://arxiv.org/abs/2101.06069",
          "publishedOn": "2021-07-26T02:00:58.567Z",
          "wordCount": 700,
          "title": "Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.11134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1\">Utku Evci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1\">Trevor Gale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>",
          "description": "Many applications require sparse neural networks due to space or inference\ntime restrictions. There is a large body of work on training dense networks to\nyield sparse networks for inference, but this limits the size of the largest\ntrainable sparse model to that of the largest trainable dense model. In this\npaper we introduce a method to train sparse neural networks with a fixed\nparameter count and a fixed computational cost throughout training, without\nsacrificing accuracy relative to existing dense-to-sparse training methods. Our\nmethod updates the topology of the sparse network during training by using\nparameter magnitudes and infrequent gradient calculations. We show that this\napproach requires fewer floating-point operations (FLOPs) to achieve a given\nlevel of accuracy compared to prior techniques. We demonstrate state-of-the-art\nsparse training results on a variety of networks and datasets, including\nResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we\nprovide some insights into why allowing the topology to change during the\noptimization can overcome local minima encountered when the topology remains\nstatic. Code used in our work can be found in github.com/google-research/rigl.",
          "link": "http://arxiv.org/abs/1911.11134",
          "publishedOn": "2021-07-26T02:00:58.559Z",
          "wordCount": 696,
          "title": "Rigging the Lottery: Making All Tickets Winners. (arXiv:1911.11134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zehua Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>",
          "description": "Human Action Recognition (HAR) aims to understand human behavior and assign a\nlabel to each action. It has a wide range of applications, and therefore has\nbeen attracting increasing attention in the field of computer vision. Human\nactions can be represented using various data modalities, such as RGB,\nskeleton, depth, infrared, point cloud, event stream, audio, acceleration,\nradar, and WiFi signal, which encode different sources of useful yet distinct\ninformation and have various advantages depending on the application scenarios.\nConsequently, lots of existing works have attempted to investigate different\ntypes of approaches for HAR using various modalities. In this paper, we present\na comprehensive survey of recent progress in deep learning methods for HAR\nbased on the type of input data modality. Specifically, we review the current\nmainstream deep learning methods for single data modalities and multiple data\nmodalities, including the fusion-based and the co-learning-based frameworks. We\nalso present comparative results on several benchmark datasets for HAR,\ntogether with insightful observations and inspiring future research directions.",
          "link": "http://arxiv.org/abs/2012.11866",
          "publishedOn": "2021-07-26T02:00:58.522Z",
          "wordCount": 656,
          "title": "Human Action Recognition from Various Data Modalities: A Review. (arXiv:2012.11866v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Anand Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zylich_B/0/1/0/all/0/1\">Brian Zylich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottmar_E/0/1/0/all/0/1\">Erin Ottmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LoCasale_Crouch_J/0/1/0/all/0/1\">Jennifer LoCasale-Crouch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob Whitehill</a>",
          "description": "In this work we present a multi-modal machine learning-based system, which we\ncall ACORN, to analyze videos of school classrooms for the Positive Climate\n(PC) and Negative Climate (NC) dimensions of the CLASS observation protocol\nthat is widely used in educational research. ACORN uses convolutional neural\nnetworks to analyze spectral audio features, the faces of teachers and\nstudents, and the pixels of each image frame, and then integrates this\ninformation over time using Temporal Convolutional Networks. The audiovisual\nACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$\nwith ground-truth scores provided by expert CLASS coders on the UVA Toddler\ndataset (cross-validation on $n=300$ 15-min video segments), and a purely\nauditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the\nMET dataset (test set of $n=2000$ videos segments). These numbers are similar\nto inter-coder reliability of human coders. Finally, using Graph Convolutional\nNetworks we make early strides (AUC=$0.70$) toward predicting the specific\nmoments (45-90sec clips) when the PC is particularly weak/strong. Our findings\ninform the design of automatic classroom observation and also more general\nvideo activity recognition and summary recognition systems.",
          "link": "http://arxiv.org/abs/2005.09525",
          "publishedOn": "2021-07-26T02:00:58.510Z",
          "wordCount": 712,
          "title": "Toward Automated Classroom Observation: Multimodal Machine Learning to Estimate CLASS Positive Climate and Negative Climate. (arXiv:2005.09525v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1\">Poorya Aghdaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1\">Baaria Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "Morphed images have exploited loopholes in the face recognition checkpoints,\ne.g., Credential Authentication Technology (CAT), used by Transportation\nSecurity Administration (TSA), which is a non-trivial security concern. To\novercome the risks incurred due to morphed presentations, we propose a\nwavelet-based morph detection methodology which adopts an end-to-end trainable\nsoft attention mechanism . Our attention-based deep neural network (DNN)\nfocuses on the salient Regions of Interest (ROI) which have the most spatial\nsupport for morph detector decision function, i.e, morph class binary softmax\noutput. A retrospective of morph synthesizing procedure aids us to speculate\nthe ROI as regions around facial landmarks , particularly for the case of\nlandmark-based morphing techniques. Moreover, our attention-based DNN is\nadapted to the wavelet space, where inputs of the network are coarse-to-fine\nspectral representations, 48 stacked wavelet sub-bands to be exact. We evaluate\nperformance of the proposed framework using three datasets, VISAPP17, LMA, and\nMorGAN. In addition, as attention maps can be a robust indicator whether a\nprobe image under investigation is genuine or counterfeit, we analyze the\nestimated attention maps for both a bona fide image and its corresponding\nmorphed image. Finally, we present an ablation study on the efficacy of\nutilizing attention mechanism for the sake of morph detection.",
          "link": "http://arxiv.org/abs/2106.15686",
          "publishedOn": "2021-07-26T02:00:58.502Z",
          "wordCount": 664,
          "title": "Attention Aware Wavelet-based Detection of Morphed Face Images. (arXiv:2106.15686v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Santaquiteria_J/0/1/0/all/0/1\">Jesus Ruiz-Santaquiteria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasco_Mata_A/0/1/0/all/0/1\">Alberto Velasco-Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallez_N/0/1/0/all/0/1\">Noelia Vallez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bueno_G/0/1/0/all/0/1\">Gloria Bueno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Garcia_J/0/1/0/all/0/1\">Juan A. &#xc1;lvarez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deniz_O/0/1/0/all/0/1\">Oscar Deniz</a>",
          "description": "Closed-circuit television (CCTV) systems are essential nowadays to prevent\nsecurity threats or dangerous situations, in which early detection is crucial.\nNovel deep learning-based methods have allowed to develop automatic weapon\ndetectors with promising results. However, these approaches are mainly based on\nvisual weapon appearance only. For handguns, body pose may be a useful cue,\nespecially in cases where the gun is barely visible. In this work, a novel\nmethod is proposed to combine, in a single architecture, both weapon appearance\nand human pose information. First, pose keypoints are estimated to extract hand\nregions and generate binary pose images, which are the model inputs. Then, each\ninput is processed in different subnetworks and combined to produce the handgun\nbounding box. Results obtained show that the combined model improves the\nhandgun detection state of the art, achieving from 4.23 to 18.9 AP points more\nthan the best previous approach.",
          "link": "http://arxiv.org/abs/2010.13753",
          "publishedOn": "2021-07-26T02:00:58.481Z",
          "wordCount": 644,
          "title": "Handgun detection using combined human pose and weapon appearance. (arXiv:2010.13753v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xiaohang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "Unsupervised contrastive learning achieves great success in learning image\nrepresentations with CNN. Unlike most recent methods that focused on improving\naccuracy of image classification, we present a novel contrastive learning\napproach, named DetCo, which fully explores the contrasts between global image\nand local image patches to learn discriminative representations for object\ndetection. DetCo has several appealing benefits. (1) It is carefully designed\nby investigating the weaknesses of current self-supervised methods, which\ndiscard important representations for object detection. (2) DetCo builds\nhierarchical intermediate contrastive losses between global image and local\npatches to improve object detection, while maintaining global representations\nfor image recognition. Theoretical analysis shows that the local patches\nactually remove the contextual information of an image, improving the lower\nbound of mutual information for better contrastive learning. (3) Extensive\nexperiments on PASCAL VOC, COCO and Cityscapes demonstrate that DetCo not only\noutperforms state-of-the-art methods on object detection, but also on\nsegmentation, pose estimation, and 3D shape prediction, while it is still\ncompetitive on image classification. For example, on PASCAL VOC, DetCo-100ep\nachieves 57.4 mAP, which is on par with the result of MoCov2-800ep. Moreover,\nDetCo consistently outperforms supervised method by 1.6/1.2/1.0 AP on Mask\nRCNN-C4/FPN/RetinaNet with 1x schedule. Code will be released at\n\\href{https://github.com/xieenze/DetCo}{\\color{blue}{\\tt\ngithub.com/xieenze/DetCo}}.",
          "link": "http://arxiv.org/abs/2102.04803",
          "publishedOn": "2021-07-26T02:00:58.474Z",
          "wordCount": 685,
          "title": "DetCo: Unsupervised Contrastive Learning for Object Detection. (arXiv:2102.04803v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07950",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kamraoui_R/0/1/0/all/0/1\">Reda Abdellah Kamraoui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ta_V/0/1/0/all/0/1\">Vinh-Thong Ta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tourdias_T/0/1/0/all/0/1\">Thomas Tourdias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansencal_B/0/1/0/all/0/1\">Boris Mansencal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manjon_J/0/1/0/all/0/1\">Jos&#xe9; V Manjon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coupe_P/0/1/0/all/0/1\">Pierrick Coup&#xe9;</a>",
          "description": "Recently, segmentation methods based on Convolutional Neural Networks (CNNs)\nshowed promising performance in automatic Multiple Sclerosis (MS) lesions\nsegmentation. These techniques have even outperformed human experts in\ncontrolled evaluation conditions such as Longitudinal MS Lesion Segmentation\nChallenge (ISBI Challenge). However state-of-the-art approaches trained to\nperform well on highly-controlled datasets fail to generalize on clinical data\nfrom unseen datasets. Instead of proposing another improvement of the\nsegmentation accuracy, we propose a novel method robust to domain shift and\nperforming well on unseen datasets, called DeepLesionBrain (DLB). This\ngeneralization property results from three main contributions. First, DLB is\nbased on a large group of compact 3D CNNs. This spatially distributed strategy\nensures a robust prediction despite the risk of generalization failure of some\nindividual networks. Second, DLB includes a new image quality data augmentation\nto reduce dependency to training data specificity (e.g., acquisition protocol).\nFinally, to learn a more generalizable representation of MS lesions, we propose\na hierarchical specialization learning (HSL). HSL is performed by pre-training\na generic network over the whole brain, before using its weights as\ninitialization to locally specialized networks. By this end, DLB learns both\ngeneric features extracted at global image level and specific features\nextracted at local image level. DLB generalization was validated in\ncross-dataset experiments on MSSEG'16, ISBI challenge, and in-house datasets.\nDuring experiments, DLB showed higher segmentation accuracy, better\nsegmentation consistency and greater generalization performance compared to\nstate-of-the-art methods. Therefore, DLB offers a robust framework well-suited\nfor clinical practice.",
          "link": "http://arxiv.org/abs/2012.07950",
          "publishedOn": "2021-07-26T02:00:58.466Z",
          "wordCount": 718,
          "title": "DeepLesionBrain: Towards a broader deep-learning generalization for multiple sclerosis lesion segmentation. (arXiv:2012.07950v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zequn Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "Attention mechanism, especially channel attention, has gained great success\nin the computer vision field. Many works focus on how to design efficient\nchannel attention mechanisms while ignoring a fundamental problem, i.e.,\nchannel attention mechanism uses scalar to represent channel, which is\ndifficult due to massive information loss. In this work, we start from a\ndifferent view and regard the channel representation problem as a compression\nprocess using frequency analysis. Based on the frequency analysis, we\nmathematically prove that the conventional global average pooling is a special\ncase of the feature decomposition in the frequency domain. With the proof, we\nnaturally generalize the compression of the channel attention mechanism in the\nfrequency domain and propose our method with multi-spectral channel attention,\ntermed as FcaNet. FcaNet is simple but effective. We can change a few lines of\ncode in the calculation to implement our method within existing channel\nattention methods. Moreover, the proposed method achieves state-of-the-art\nresults compared with other channel attention methods on image classification,\nobject detection, and instance segmentation tasks. Our method could\nconsistently outperform the baseline SENet, with the same number of parameters\nand the same computational cost. Our code and models will are publicly\navailable at https://github.com/cfzd/FcaNet.",
          "link": "http://arxiv.org/abs/2012.11879",
          "publishedOn": "2021-07-26T02:00:58.458Z",
          "wordCount": 679,
          "title": "FcaNet: Frequency Channel Attention Networks. (arXiv:2012.11879v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1\">Ben Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camgoz_N/0/1/0/all/0/1\">Necati Cihan Camgoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "It is common practice to represent spoken languages at their phonetic level.\nHowever, for sign languages, this implies breaking motion into its constituent\nmotion primitives. Avatar based Sign Language Production (SLP) has\ntraditionally done just this, building up animation from sequences of hand\nmotions, shapes and facial expressions. However, more recent deep learning\nbased solutions to SLP have tackled the problem using a single network that\nestimates the full skeletal structure.\n\nWe propose splitting the SLP task into two distinct jointly-trained\nsub-tasks. The first translation sub-task translates from spoken language to a\nlatent sign language representation, with gloss supervision. Subsequently, the\nanimation sub-task aims to produce expressive sign language sequences that\nclosely resemble the learnt spatio-temporal representation. Using a progressive\ntransformer for the translation sub-task, we propose a novel Mixture of Motion\nPrimitives (MoMP) architecture for sign language animation. A set of distinct\nmotion primitives are learnt during training, that can be temporally combined\nat inference to animate continuous sign language sequences.\n\nWe evaluate on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T)\ndataset, presenting extensive ablation studies and showing that MoMP\noutperforms baselines in user evaluations. We achieve state-of-the-art back\ntranslation performance with an 11% improvement over competing results.\nImportantly, and for the first time, we showcase stronger performance for a\nfull translation pipeline going from spoken language to sign, than from gloss\nto sign.",
          "link": "http://arxiv.org/abs/2107.11317",
          "publishedOn": "2021-07-26T02:00:58.448Z",
          "wordCount": 671,
          "title": "Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives. (arXiv:2107.11317v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Global covariance pooling (GCP) aims at exploiting the second-order\nstatistics of the convolutional feature. Its effectiveness has been\ndemonstrated in boosting the classification performance of Convolutional Neural\nNetworks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute\nthe matrix square root. However, the approximate matrix square root calculated\nusing Newton-Schulz iteration \\cite{li2018towards} outperforms the accurate one\ncomputed via SVD \\cite{li2017second}. We empirically analyze the reason behind\nthe performance gap from the perspectives of data precision and gradient\nsmoothness. Various remedies for computing smooth SVD gradients are\ninvestigated. Based on our observation and analyses, a hybrid training protocol\nis proposed for SVD-based GCP meta-layers such that competitive performances\ncan be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP\nmeta-layer that uses SVD in the forward pass, and Pad\\'e Approximants in the\nbackward propagation to compute the gradients. The proposed meta-layer has been\nintegrated into different CNN models and achieves state-of-the-art performances\non both large-scale and fine-grained datasets.",
          "link": "http://arxiv.org/abs/2105.02498",
          "publishedOn": "2021-07-26T02:00:58.427Z",
          "wordCount": 639,
          "title": "Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?. (arXiv:2105.02498v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zaiping Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wei An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>",
          "description": "Recently, the performance of single image super-resolution (SR) has been\nsignificantly improved with powerful networks. However, these networks are\ndeveloped for image SR with a single specific integer scale (e.g., x2;x3,x4),\nand cannot be used for non-integer and asymmetric SR. In this paper, we propose\nto learn a scale-arbitrary image SR network from scale-specific networks.\nSpecifically, we propose a plug-in module for existing SR networks to perform\nscale-arbitrary SR, which consists of multiple scale-aware feature adaption\nblocks and a scale-aware upsampling layer. Moreover, we introduce a scale-aware\nknowledge transfer paradigm to transfer knowledge from scale-specific networks\nto the scale-arbitrary network. Our plug-in module can be easily adapted to\nexisting networks to achieve scale-arbitrary SR. These networks plugged with\nour module can achieve promising results for non-integer and asymmetric SR\nwhile maintaining state-of-the-art performance for SR with integer scale\nfactors. Besides, the additional computational and memory cost of our module is\nvery small.",
          "link": "http://arxiv.org/abs/2004.03791",
          "publishedOn": "2021-07-26T02:00:58.420Z",
          "wordCount": 625,
          "title": "Learning A Single Network for Scale-Arbitrary Super-Resolution. (arXiv:2004.03791v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gongjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Deep learning-based 3D object detection has achieved unprecedented success\nwith the advent of large-scale autonomous driving datasets. However, drastic\nperformance degradation remains a critical challenge for cross-domain\ndeployment. In addition, existing 3D domain adaptive detection methods often\nassume prior access to the target domain annotations, which is rarely feasible\nin the real world. To address this challenge, we study a more realistic\nsetting, unsupervised 3D domain adaptive detection, which only utilizes source\ndomain annotations. 1) We first comprehensively investigate the major\nunderlying factors of the domain gap in 3D detection. Our key insight is that\ngeometric mismatch is the key factor of domain shift. 2) Then, we propose a\nnovel and unified framework, Multi-Level Consistency Network (MLC-Net), which\nemploys a teacher-student paradigm to generate adaptive and reliable\npseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level\nconsistency to facilitate cross-domain transfer. Extensive experiments\ndemonstrate that MLC-Net outperforms existing state-of-the-art methods\n(including those using additional target domain information) on standard\nbenchmarks. Notably, our approach is detector-agnostic, which achieves\nconsistent gains on both single- and two-stage 3D detectors.",
          "link": "http://arxiv.org/abs/2107.11355",
          "publishedOn": "2021-07-26T02:00:58.412Z",
          "wordCount": 623,
          "title": "Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency. (arXiv:2107.11355v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiacheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_K/0/1/0/all/0/1\">Kim-Hui Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1\">Tzu-Yi Hung</a>",
          "description": "Semantic segmentation on 3D point clouds is an important task for 3D scene\nunderstanding. While dense labeling on 3D data is expensive and time-consuming,\nonly a few works address weakly supervised semantic point cloud segmentation\nmethods to relieve the labeling cost by learning from simpler and cheaper\nlabels. Meanwhile, there are still huge performance gaps between existing\nweakly supervised methods and state-of-the-art fully supervised methods. In\nthis paper, we train a semantic point cloud segmentation network with only a\nsmall portion of points being labeled. We argue that we can better utilize the\nlimited supervision information as we densely propagate the supervision signal\nfrom the labeled points to other points within and across the input samples.\nSpecifically, we propose a cross-sample feature reallocating module to transfer\nsimilar features and therefore re-route the gradients across two samples with\ncommon classes and an intra-sample feature redistribution module to propagate\nsupervision signals on unlabeled points across and within point cloud samples.\nWe conduct extensive experiments on public datasets S3DIS and ScanNet. Our\nweakly supervised method with only 10\\% and 1\\% of labels can produce\ncompatible results with the fully supervised counterpart.",
          "link": "http://arxiv.org/abs/2107.11267",
          "publishedOn": "2021-07-26T02:00:58.405Z",
          "wordCount": 633,
          "title": "Dense Supervision Propagation for Weakly Supervised Semantic Segmentation on 3D Point Clouds. (arXiv:2107.11267v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1\">Ashkan Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1\">Kiran Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1\">Devin Gaffney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>",
          "description": "There is currently no easy way to fact-check content on WhatsApp and other\nend-to-end encrypted platforms at scale. In this paper, we analyze the\nusefulness of a crowd-sourced \"tipline\" through which users can submit content\n(\"tips\") that they want fact-checked. We compare the tips sent to a WhatsApp\ntipline run during the 2019 Indian national elections with the messages\ncirculating in large, public groups on WhatsApp and other social media\nplatforms during the same period. We find that tiplines are a very useful lens\ninto WhatsApp conversations: a significant fraction of messages and images sent\nto the tipline match with the content being shared on public WhatsApp groups\nand other social media. Our analysis also shows that tiplines cover the most\npopular content well, and a majority of such content is often shared to the\ntipline before appearing in large, public WhatsApp groups. Overall, our\nfindings suggest tiplines can be an effective source for discovering content to\nfact-check.",
          "link": "http://arxiv.org/abs/2106.04726",
          "publishedOn": "2021-07-26T02:00:58.398Z",
          "wordCount": 655,
          "title": "Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ito_K/0/1/0/all/0/1\">Koichi Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>",
          "description": "Studies evaluating bikeability usually compute spatial indicators shaping\ncycling conditions and conflate them in a quantitative index. Much research\ninvolves site visits or conventional geospatial approaches, and few studies\nhave leveraged street view imagery (SVI) for conducting virtual audits. These\nhave assessed a limited range of aspects, and not all have been automated using\ncomputer vision (CV). Furthermore, studies have not yet zeroed in on gauging\nthe usability of these technologies thoroughly. We investigate, with\nexperiments at a fine spatial scale and across multiple geographies (Singapore\nand Tokyo), whether we can use SVI and CV to assess bikeability\ncomprehensively. Extending related work, we develop an exhaustive index of\nbikeability composed of 34 indicators. The results suggest that SVI and CV are\nadequate to evaluate bikeability in cities comprehensively. As they\noutperformed non-SVI counterparts by a wide margin, SVI indicators are also\nfound to be superior in assessing urban bikeability, and potentially can be\nused independently, replacing traditional techniques. However, the paper\nexposes some limitations, suggesting that the best way forward is combining\nboth SVI and non-SVI approaches. The new bikeability index presents a\ncontribution in transportation and urban analytics, and it is scalable to\nassess cycling appeal widely.",
          "link": "http://arxiv.org/abs/2105.08499",
          "publishedOn": "2021-07-26T02:00:58.376Z",
          "wordCount": 656,
          "title": "Assessing bikeability with street view imagery and computer vision. (arXiv:2105.08499v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Heatmap-based methods dominate in the field of human pose estimation by\nmodelling the output distribution through likelihood heatmaps. In contrast,\nregression-based methods are more efficient but suffer from inferior\nperformance. In this work, we explore maximum likelihood estimation (MLE) to\ndevelop an efficient and effective regression-based methods. From the\nperspective of MLE, adopting different regression losses is making different\nassumptions about the output density function. A density function closer to the\ntrue distribution leads to a better regression performance. In light of this,\nwe propose a novel regression paradigm with Residual Log-likelihood Estimation\n(RLE) to capture the underlying output distribution. Concretely, RLE learns the\nchange of the distribution instead of the unreferenced underlying distribution\nto facilitate the training process. With the proposed reparameterization\ndesign, our method is compatible with off-the-shelf flow models. The proposed\nmethod is effective, efficient and flexible. We show its potential in various\nhuman pose estimation tasks with comprehensive experiments. Compared to the\nconventional regression paradigm, regression with RLE bring 12.4 mAP\nimprovement on MSCOCO without any test-time overhead. Moreover, for the first\ntime, especially on multi-person pose estimation, our regression method is\nsuperior to the heatmap-based methods. Our code is available at\nhttps://github.com/Jeff-sjtu/res-loglikelihood-regression",
          "link": "http://arxiv.org/abs/2107.11291",
          "publishedOn": "2021-07-26T02:00:58.368Z",
          "wordCount": 647,
          "title": "Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vecchio_G/0/1/0/all/0/1\">Giuseppe Vecchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palazzo_S/0/1/0/all/0/1\">Simone Palazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1\">Concetto Spampinato</a>",
          "description": "In this paper we present SurfaceNet, an approach for estimating\nspatially-varying bidirectional reflectance distribution function (SVBRDF)\nmaterial properties from a single image. We pose the problem as an image\ntranslation task and propose a novel patch-based generative adversarial network\n(GAN) that is able to produce high-quality, high-resolution surface reflectance\nmaps. The employment of the GAN paradigm has a twofold objective: 1) allowing\nthe model to recover finer details than standard translation models; 2)\nreducing the domain shift between synthetic and real data distributions in an\nunsupervised way. An extensive evaluation, carried out on a public benchmark of\nsynthetic and real images under different illumination conditions, shows that\nSurfaceNet largely outperforms existing SVBRDF reconstruction methods, both\nquantitatively and qualitatively. Furthermore, SurfaceNet exhibits a remarkable\nability in generating high-quality maps from real samples without any\nsupervision at training time.",
          "link": "http://arxiv.org/abs/2107.11298",
          "publishedOn": "2021-07-26T02:00:58.361Z",
          "wordCount": 570,
          "title": "SurfaceNet: Adversarial SVBRDF Estimation from a Single Image. (arXiv:2107.11298v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07240",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong Tae Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goli_L/0/1/0/all/0/1\">Leili Goli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paschali_M/0/1/0/all/0/1\">Magdalini Paschali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keicher_M/0/1/0/all/0/1\">Matthias Keicher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Czempiel_T/0/1/0/all/0/1\">Tobias Czempiel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burian_E/0/1/0/all/0/1\">Egon Burian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wendler_T/0/1/0/all/0/1\">Thomas Wendler</a>",
          "description": "Chest computed tomography (CT) has played an essential diagnostic role in\nassessing patients with COVID-19 by showing disease-specific image features\nsuch as ground-glass opacity and consolidation. Image segmentation methods have\nproven to help quantify the disease burden and even help predict the outcome.\nThe availability of longitudinal CT series may also result in an efficient and\neffective method to reliably assess the progression of COVID-19, monitor the\nhealing process and the response to different therapeutic strategies. In this\npaper, we propose a new framework to identify infection at a voxel level\n(identification of healthy lung, consolidation, and ground-glass opacity) and\nvisualize the progression of COVID-19 using sequential low-dose non-contrast CT\nscans. In particular, we devise a longitudinal segmentation network that\nutilizes the reference scan information to improve the performance of disease\nidentification. Experimental results on a clinical longitudinal dataset\ncollected in our institution show the effectiveness of the proposed method\ncompared to the static deep neural networks for disease quantification.",
          "link": "http://arxiv.org/abs/2103.07240",
          "publishedOn": "2021-07-26T02:00:58.352Z",
          "wordCount": 688,
          "title": "Longitudinal Quantitative Assessment of COVID-19 Infection Progression from Chest CTs. (arXiv:2103.07240v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_R/0/1/0/all/0/1\">Reena Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shubham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Anand Raghunathan</a>",
          "description": "Precision scaling has emerged as a popular technique to optimize the compute\nand storage requirements of Deep Neural Networks (DNNs). Efforts toward\ncreating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum\nprecision required to achieve a given network-level accuracy varies\nconsiderably across networks, and even across layers within a network,\nrequiring support for variable precision in DNN hardware. Previous proposals\nsuch as bit-serial hardware incur high overheads, significantly diminishing the\nbenefits of lower precision. To efficiently support precision\nre-configurability in DNN accelerators, we introduce an approximate computing\nmethod wherein DNN computations are performed block-wise (a block is a group of\nbits) and re-configurability is supported at the granularity of blocks. Results\nof block-wise computations are composed in an approximate manner to enable\nefficient re-configurability. We design a DNN accelerator that embodies\napproximate blocked computation and propose a method to determine a suitable\napproximation configuration for a given DNN. By varying the approximation\nconfigurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement\nin system energy and performance respectively, over an 8-bit fixed-point (FxP8)\nbaseline, with negligible loss in classification accuracy. Further, by varying\nthe approximation configurations across layers and data-structures within DNNs,\nwe achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and\nperformance respectively, with negligible accuracy loss.",
          "link": "http://arxiv.org/abs/2011.13000",
          "publishedOn": "2021-07-26T02:00:58.344Z",
          "wordCount": 676,
          "title": "Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration. (arXiv:2011.13000v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sanghun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwak_D/0/1/0/all/0/1\">Daehoon Gwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungha Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Identifying unexpected objects on roads in semantic segmentation (e.g.,\nidentifying dogs on roads) is crucial in safety-critical applications. Existing\napproaches use images of unexpected objects from external datasets or require\nadditional training (e.g., retraining segmentation networks or training an\nextra network), which necessitate a non-trivial amount of labor intensity or\nlengthy inference time. One possible alternative is to use prediction scores of\na pre-trained network such as the max logits (i.e., maximum values among\nclasses before the final softmax layer) for detecting such objects. However,\nthe distribution of max logits of each predicted class is significantly\ndifferent from each other, which degrades the performance of identifying\nunexpected objects in urban-scene segmentation. To address this issue, we\npropose a simple yet effective approach that standardizes the max logits in\norder to align the different distributions and reflect the relative meanings of\nmax logits within each predicted class. Moreover, we consider the local regions\nfrom two different perspectives based on the intuition that neighboring pixels\nshare similar semantic information. In contrast to previous approaches, our\nmethod does not utilize any external datasets or require additional training,\nwhich makes our method widely applicable to existing pre-trained segmentation\nmodels. Such a straightforward approach achieves a new state-of-the-art\nperformance on the publicly available Fishyscapes Lost & Found leaderboard with\na large margin.",
          "link": "http://arxiv.org/abs/2107.11264",
          "publishedOn": "2021-07-26T02:00:58.313Z",
          "wordCount": 677,
          "title": "Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation. (arXiv:2107.11264v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1\">Gyri Reiersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1\">David Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Forest carbon offsets are increasingly popular and can play a significant\nrole in financing climate mitigation, forest conservation, and reforestation.\nMeasuring how much carbon is stored in forests is, however, still largely done\nvia expensive, time-consuming, and sometimes unaccountable field measurements.\nTo overcome these limitations, many verification bodies are leveraging machine\nlearning (ML) algorithms to estimate forest carbon from satellite or aerial\nimagery. Aerial imagery allows for tree species or family classification, which\nimproves the satellite imagery-based forest type classification. However,\naerial imagery is significantly more expensive to collect and it is unclear by\nhow much the higher resolution improves the forest carbon estimation. This\nproposal paper describes the first systematic comparison of forest carbon\nestimation from aerial imagery, satellite imagery, and ground-truth field\nmeasurements via deep learning-based algorithms for a tropical reforestation\nproject. Our initial results show that forest carbon estimates from satellite\nimagery can overestimate above-ground biomass by more than 10-times for\ntropical reforestation projects. The significant difference between aerial and\nsatellite-derived forest carbon measurements shows the potential for aerial\nimagery-based ML algorithms and raises the importance to extend this study to a\nglobal benchmark between options for carbon measurements.",
          "link": "http://arxiv.org/abs/2107.11320",
          "publishedOn": "2021-07-26T02:00:58.280Z",
          "wordCount": 663,
          "title": "Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ewecker_L/0/1/0/all/0/1\">Lukas Ewecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asan_E/0/1/0/all/0/1\">Ebubekir Asan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohnemus_L/0/1/0/all/0/1\">Lars Ohnemus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saralajew_S/0/1/0/all/0/1\">Sascha Saralajew</a>",
          "description": "In recent years, computer vision algorithms have become more and more\npowerful, which enabled technologies such as autonomous driving to evolve with\nrapid pace. However, current algorithms mainly share one limitation: They rely\non directly visible objects. This is a major drawback compared to human\nbehavior, where indirect visual cues caused by the actual object (e.g.,\nshadows) are already used intuitively to retrieve information or anticipate\noccurring objects. While driving at night, this performance deficit becomes\neven more obvious: Humans already process the light artifacts caused by\noncoming vehicles to assume their future appearance, whereas current object\ndetection systems rely on the oncoming vehicle's direct visibility. Based on\nprevious work in this subject, we present with this paper a complete system\ncapable of solving the task to providently detect oncoming vehicles at\nnighttime based on their caused light artifacts. For that, we outline the full\nalgorithm architecture ranging from the detection of light artifacts in the\nimage space, localizing the objects in the three-dimensional space, and\nverifying the objects over time. To demonstrate the applicability, we deploy\nthe system in a test vehicle and use the information of providently detected\nvehicles to control the glare-free high beam system proactively. Using this\nexperimental setting, we quantify the time benefit that the provident vehicle\ndetection system provides compared to an in-production computer vision system.\nAdditionally, the glare-free high beam use case provides a real-time and\nreal-world visualization interface of the detection results. With this\ncontribution, we want to put awareness on the unconventional sensing task of\nprovident object detection and further close the performance gap between human\nbehavior and computer vision algorithms in order to bring autonomous and\nautomated driving a step forward.",
          "link": "http://arxiv.org/abs/2107.11302",
          "publishedOn": "2021-07-26T02:00:58.272Z",
          "wordCount": 724,
          "title": "Provident Vehicle Detection at Night for Advanced Driver Assistance Systems. (arXiv:2107.11302v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruifei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>",
          "description": "While self-training has advanced semi-supervised semantic segmentation, it\nseverely suffers from the long-tailed class distribution on real-world semantic\nsegmentation datasets that make the pseudo-labeled data bias toward majority\nclasses. In this paper, we present a simple and yet effective Distribution\nAlignment and Random Sampling (DARS) method to produce unbiased pseudo labels\nthat match the true class distribution estimated from the labeled data.\nBesides, we also contribute a progressive data augmentation and labeling\nstrategy to facilitate model training with pseudo-labeled data. Experiments on\nboth Cityscapes and PASCAL VOC 2012 datasets demonstrate the effectiveness of\nour approach. Albeit simple, our method performs favorably in comparison with\nstate-of-the-art approaches. Code will be available at\nhttps://github.com/CVMI-Lab/DARS.",
          "link": "http://arxiv.org/abs/2107.11279",
          "publishedOn": "2021-07-26T02:00:58.244Z",
          "wordCount": 558,
          "title": "Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation. (arXiv:2107.11279v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abid_M/0/1/0/all/0/1\">Mohamed Abderrahmen Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedhli_I/0/1/0/all/0/1\">Ihsen Hedhli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Lalonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Christian Gagne</a>",
          "description": "Most image-to-image translation methods focus on learning mappings across\ndomains with the assumption that images share content (e.g., pose) but have\ntheir own domain-specific information known as style. When conditioned on a\ntarget image, such methods aim to extract the style of the target and combine\nit with the content of the source image. In this work, we consider the scenario\nwhere the target image has a very low resolution. More specifically, our\napproach aims at transferring fine details from a high resolution (HR) source\nimage to fit a coarse, low resolution (LR) image representation of the target.\nWe therefore generate HR images that share features from both HR and LR inputs.\nThis differs from previous methods that focus on translating a given image\nstyle into a target content, our translation approach being able to\nsimultaneously imitate the style and merge the structural information of the LR\ntarget. Our approach relies on training the generative model to produce HR\ntarget images that both 1) share distinctive information of the associated\nsource image; 2) correctly match the LR target image when downscaled. We\nvalidate our method on the CelebA-HQ and AFHQ datasets by demonstrating\nimprovements in terms of visual quality, diversity and coverage. Qualitative\nand quantitative results show that when dealing with intra-domain image\ntranslation, our method generates more realistic samples compared to\nstate-of-the-art methods such as Stargan-v2",
          "link": "http://arxiv.org/abs/2107.11262",
          "publishedOn": "2021-07-26T02:00:58.233Z",
          "wordCount": 667,
          "title": "Image-to-Image Translation with Low Resolution Conditioning. (arXiv:2107.11262v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bingqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yanxin Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Language instruction plays an essential role in the natural language grounded\nnavigation tasks. However, navigators trained with limited human-annotated\ninstructions may have difficulties in accurately capturing key information from\nthe complicated instruction at different timesteps, leading to poor navigation\nperformance. In this paper, we exploit to train a more robust navigator which\nis capable of dynamically extracting crucial factors from the long instruction,\nby using an adversarial attacking paradigm. Specifically, we propose a Dynamic\nReinforced Instruction Attacker (DR-Attacker), which learns to mislead the\nnavigator to move to the wrong target by destroying the most instructive\ninformation in instructions at different timesteps. By formulating the\nperturbation generation as a Markov Decision Process, DR-Attacker is optimized\nby the reinforcement learning algorithm to generate perturbed instructions\nsequentially during the navigation, according to a learnable attack score.\nThen, the perturbed instructions, which serve as hard samples, are used for\nimproving the robustness of the navigator with an effective adversarial\ntraining strategy and an auxiliary self-supervised reasoning task. Experimental\nresults on both Vision-and-Language Navigation (VLN) and Navigation from Dialog\nHistory (NDH) tasks show the superiority of our proposed method over\nstate-of-the-art methods. Moreover, the visualization analysis shows the\neffectiveness of the proposed DR-Attacker, which can successfully attack\ncrucial information in the instructions at different timesteps. Code is\navailable at https://github.com/expectorlin/DR-Attacker.",
          "link": "http://arxiv.org/abs/2107.11252",
          "publishedOn": "2021-07-26T02:00:58.226Z",
          "wordCount": 684,
          "title": "Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation. (arXiv:2107.11252v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>",
          "description": "We propose the adjacency adaptive graph convolutional long-short term memory\nnetwork (AAGC-LSTM) for human pose estimation from sparse inertial\nmeasurements, obtained from only 6 measurement units. The AAGC-LSTM combines\nboth spatial and temporal dependency in a single network operation. This is\nmade possible by equipping graph convolutions with adjacency adaptivity, which\nalso allows for learning unknown dependencies of the human body joints. To\nfurther boost accuracy, we propose longitudinal loss weighting to consider\nnatural movement patterns, as well as body-aware contralateral data\naugmentation. By combining these contributions, we are able to utilize the\ninherent graph nature of the human body, and can thus outperform the state of\nthe art for human pose estimation from sparse inertial measurements.",
          "link": "http://arxiv.org/abs/2107.11214",
          "publishedOn": "2021-07-26T02:00:58.205Z",
          "wordCount": 558,
          "title": "Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Estienne_T/0/1/0/all/0/1\">Th&#xe9;o Estienne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulidis_S/0/1/0/all/0/1\">Stergios Christodoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battistella_E/0/1/0/all/0/1\">Enzo Battistella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Th&#xe9;ophraste Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerousseau_M/0/1/0/all/0/1\">Marvin Lerousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_A/0/1/0/all/0/1\">Amaury Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_E/0/1/0/all/0/1\">Eric Deutsch</a>",
          "description": "Explainability of deep neural networks is one of the most challenging and\ninteresting problems in the field. In this study, we investigate the topic\nfocusing on the interpretability of deep learning-based registration methods.\nIn particular, with the appropriate model architecture and using a simple\nlinear projection, we decompose the encoding space, generating a new basis, and\nwe empirically show that this basis captures various decomposed anatomically\naware geometrical transformations. We perform experiments using two different\ndatasets focusing on lungs and hippocampus MRI. We show that such an approach\ncan decompose the highly convoluted latent spaces of registration pipelines in\nan orthogonal space with several interesting properties. We hope that this work\ncould shed some light on a better understanding of deep learning-based\nregistration methods.",
          "link": "http://arxiv.org/abs/2107.11238",
          "publishedOn": "2021-07-26T02:00:58.182Z",
          "wordCount": 591,
          "title": "Exploring Deep Registration Latent Spaces. (arXiv:2107.11238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wanchaitanawong_N/0/1/0/all/0/1\">Napat Wanchaitanawong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1\">Masayuki Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibata_T/0/1/0/all/0/1\">Takashi Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>",
          "description": "The combined use of multiple modalities enables accurate pedestrian detection\nunder poor lighting conditions by using the high visibility areas from these\nmodalities together. The vital assumption for the combination use is that there\nis no or only a weak misalignment between the two modalities. In general,\nhowever, this assumption often breaks in actual situations. Due to this\nassumption's breakdown, the position of the bounding boxes does not match\nbetween the two modalities, resulting in a significant decrease in detection\naccuracy, especially in regions where the amount of misalignment is large. In\nthis paper, we propose a multi-modal Faster-RCNN that is robust against large\nmisalignment. The keys are 1) modal-wise regression and 2) multi-modal IoU for\nmini-batch sampling. To deal with large misalignment, we perform bounding box\nregression for both the RPN and detection-head with both modalities. We also\npropose a new sampling strategy called \"multi-modal mini-batch sampling\" that\nintegrates the IoU for both modalities. We demonstrate that the proposed\nmethod's performance is much better than that of the state-of-the-art methods\nfor data with large misalignment through actual image experiments.",
          "link": "http://arxiv.org/abs/2107.11196",
          "publishedOn": "2021-07-26T02:00:58.174Z",
          "wordCount": 630,
          "title": "Multi-Modal Pedestrian Detection with Large Misalignment Based on Modal-Wise Regression and Multi-Modal IoU. (arXiv:2107.11196v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1\">Yotam Nitzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1\">Rinon Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_O/0/1/0/all/0/1\">Ofir Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>",
          "description": "We propose a novel method for solving regression tasks using few-shot or weak\nsupervision. At the core of our method is the fundamental observation that GANs\nare incredibly successful at encoding semantic information within their latent\nspace, even in a completely unsupervised setting. For modern generative\nframeworks, this semantic encoding manifests as smooth, linear directions which\naffect image attributes in a disentangled manner. These directions have been\nwidely used in GAN-based image editing. We show that such directions are not\nonly linear, but that the magnitude of change induced on the respective\nattribute is approximately linear with respect to the distance traveled along\nthem. By leveraging this observation, our method turns a pre-trained GAN into a\nregression model, using as few as two labeled samples. This enables solving\nregression tasks on datasets and attributes which are difficult to produce\nquality supervision for. Additionally, we show that the same latent-distances\ncan be used to sort collections of images by the strength of given attributes,\neven in the absence of explicit supervision. Extensive experimental evaluations\ndemonstrate that our method can be applied across a wide range of domains,\nleverage multiple latent direction discovery frameworks, and achieve\nstate-of-the-art results in few-shot and low-supervision settings, even when\ncompared to methods designed to tackle a single task.",
          "link": "http://arxiv.org/abs/2107.11186",
          "publishedOn": "2021-07-26T02:00:58.157Z",
          "wordCount": 657,
          "title": "LARGE: Latent-Based Regression through GAN Semantics. (arXiv:2107.11186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11191",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Duff_M/0/1/0/all/0/1\">Margaret Duff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_N/0/1/0/all/0/1\">Neill D. F. Campbell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehrhardt_M/0/1/0/all/0/1\">Matthias J. Ehrhardt</a>",
          "description": "Deep neural network approaches to inverse imaging problems have produced\nimpressive results in the last few years. In this paper, we consider the use of\ngenerative models in a variational regularisation approach to inverse problems.\nThe considered regularisers penalise images that are far from the range of a\ngenerative model that has learned to produce images similar to a training\ndataset. We name this family \\textit{generative regularisers}. The success of\ngenerative regularisers depends on the quality of the generative model and so\nwe propose a set of desired criteria to assess models and guide future\nresearch. In our numerical experiments, we evaluate three common generative\nmodels, autoencoders, variational autoencoders and generative adversarial\nnetworks, against our desired criteria. We also test three different generative\nregularisers on the inverse problems of deblurring, deconvolution, and\ntomography. We show that the success of solutions restricted to lie exactly in\nthe range of the generator is highly dependent on the ability of the generative\nmodel but that allowing small deviations from the range of the generator\nproduces more consistent results.",
          "link": "http://arxiv.org/abs/2107.11191",
          "publishedOn": "2021-07-26T02:00:58.149Z",
          "wordCount": 631,
          "title": "Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baumgartl_H/0/1/0/all/0/1\">Hermann Baumgartl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buettner_R/0/1/0/all/0/1\">Ricardo Buettner</a>",
          "description": "We present four different robust transfer learning and data augmentation\nstrategies for robust mobile scene recognition. By training three mobile-ready\n(EfficientNetB0, MobileNetV2, MobileNetV3) and two large-scale baseline (VGG16,\nResNet50) convolutional neural network architectures on the widely available\nEvent8, Scene15, Stanford40, and MIT67 datasets, we show the generalization\nability of our transfer learning strategies. Furthermore, we tested the\nrobustness of our transfer learning strategies under viewpoint and lighting\nchanges using the KTH-Idol2 database. Also, the impact of inference\noptimization techniques on the general performance and the robustness under\ndifferent transfer learning strategies is evaluated. Experimental results show\nthat when employing transfer learning, Fine-Tuning in combination with\nextensive data augmentation improves the general accuracy and robustness in\nmobile scene recognition. We achieved state-of-the-art results using various\nbaseline convolutional neural networks and showed the robustness against\nlighting and viewpoint changes in challenging mobile robot place recognition.",
          "link": "http://arxiv.org/abs/2107.11187",
          "publishedOn": "2021-07-26T02:00:58.141Z",
          "wordCount": 610,
          "title": "Developing efficient transfer learning strategies for robust scene recognition in mobile robotics using pre-trained convolutional neural networks. (arXiv:2107.11187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>",
          "description": "Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.",
          "link": "http://arxiv.org/abs/2107.11170",
          "publishedOn": "2021-07-26T02:00:58.129Z",
          "wordCount": 709,
          "title": "Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11099",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Jing_Y/0/1/0/all/0/1\">Yu Jing</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wu_C/0/1/0/all/0/1\">Chonghang Wu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Fu_W/0/1/0/all/0/1\">Wenbing Fu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1\">Xiaogang Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>",
          "description": "With the rapid growth of qubit numbers and coherence times in quantum\nhardware technology, implementing shallow neural networks on the so-called\nNoisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of\ninterest. Many quantum (convolutional) circuit ansaetze are proposed for\ngrayscale images classification tasks with promising empirical results.\nHowever, when applying these ansaetze on RGB images, the intra-channel\ninformation that is useful for vision tasks is not extracted effectively. In\nthis paper, we propose two types of quantum circuit ansaetze to simulate\nconvolution operations on RGB images, which differ in the way how inter-channel\nand intra-channel information are extracted. To the best of our knowledge, this\nis the first work of a quantum convolutional circuit to deal with RGB images\neffectively, with a higher test accuracy compared to the purely classical CNNs.\nWe also investigate the relationship between the size of quantum circuit ansatz\nand the learnability of the hybrid quantum-classical convolutional neural\nnetwork. Through experiments based on CIFAR-10 and MNIST datasets, we\ndemonstrate that a larger size of the quantum circuit ansatz improves\npredictive performance in multiclass classification tasks, providing useful\ninsights for near term quantum algorithm developments.",
          "link": "http://arxiv.org/abs/2107.11099",
          "publishedOn": "2021-07-26T02:00:58.113Z",
          "wordCount": 633,
          "title": "RGB Image Classification with Quantum Convolutional Ansaetze. (arXiv:2107.11099v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sanghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">SeungKyu Lee</a>",
          "description": "Due to the unstable nature of minimax game between generator and\ndiscriminator, improving the performance of GANs is a challenging task. Recent\nstudies have shown that selected high-quality samples in training improve the\nperformance of GANs. However, sampling approaches which discard samples show\nlimitations in some aspects such as the speed of training and optimality of the\nnetworks. In this paper we propose unrealistic feature suppression (UFS) module\nthat keeps high-quality features and suppresses unrealistic features. UFS\nmodule keeps the training stability of networks and improves the quality of\ngenerated images. We demonstrate the effectiveness of UFS module on various\nmodels such as WGAN-GP, SNGAN, and BigGAN. By using UFS module, we achieved\nbetter Frechet inception distance and inception score compared to various\nbaseline models. We also visualize how effectively our UFS module suppresses\nunrealistic features through class activation maps.",
          "link": "http://arxiv.org/abs/2107.11047",
          "publishedOn": "2021-07-26T02:00:58.106Z",
          "wordCount": 575,
          "title": "Unrealistic Feature Suppression for Generative Adversarial Networks. (arXiv:2107.11047v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jianxiong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Feiying Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "Image inpainting aims to complete the missing or corrupted regions of images\nwith realistic contents. The prevalent approaches adopt a hybrid objective of\nreconstruction and perceptual quality by using generative adversarial networks.\nHowever, the reconstruction loss and adversarial loss focus on synthesizing\ncontents of different frequencies and simply applying them together often leads\nto inter-frequency conflicts and compromised inpainting. This paper presents\nWaveFill, a wavelet-based inpainting network that decomposes images into\nmultiple frequency bands and fills the missing regions in each frequency band\nseparately and explicitly. WaveFill decomposes images by using discrete wavelet\ntransform (DWT) that preserves spatial information naturally. It applies L1\nreconstruction loss to the decomposed low-frequency bands and adversarial loss\nto high-frequency bands, hence effectively mitigate inter-frequency conflicts\nwhile completing images in spatial domain. To address the inpainting\ninconsistency in different frequency bands and fuse features with distinct\nstatistics, we design a novel normalization scheme that aligns and fuses the\nmulti-frequency features effectively. Extensive experiments over multiple\ndatasets show that WaveFill achieves superior image inpainting qualitatively\nand quantitatively.",
          "link": "http://arxiv.org/abs/2107.11027",
          "publishedOn": "2021-07-26T02:00:58.098Z",
          "wordCount": 625,
          "title": "WaveFill: A Wavelet-based Generation Network for Image Inpainting. (arXiv:2107.11027v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassanin_M/0/1/0/all/0/1\">Mohammed Hassanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radwan_I/0/1/0/all/0/1\">Ibrahim Radwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahtali_M/0/1/0/all/0/1\">Murat Tahtali</a>",
          "description": "Multi-label recognition is a fundamental, and yet is a challenging task in\ncomputer vision. Recently, deep learning models have achieved great progress\ntowards learning discriminative features from input images. However,\nconventional approaches are unable to model the inter-class discrepancies among\nfeatures in multi-label images, since they are designed to work for image-level\nfeature discrimination. In this paper, we propose a unified deep network to\nlearn discriminative features for the multi-label task. Given a multi-label\nimage, the proposed method first disentangles features corresponding to\ndifferent classes. Then, it discriminates between these classes via increasing\nthe inter-class distance while decreasing the intra-class differences in the\noutput space. By regularizing the whole network with the proposed loss, the\nperformance of applying the wellknown ResNet-101 is improved significantly.\nExtensive experiments have been performed on COCO-2014, VOC2007 and VOC2012\ndatasets, which demonstrate that the proposed method outperforms\nstate-of-the-art approaches by a significant margin of 3:5% on large-scale COCO\ndataset. Moreover, analysis of the discriminative feature learning approach\nshows that it can be plugged into various types of multi-label methods as a\ngeneral module.",
          "link": "http://arxiv.org/abs/2107.11159",
          "publishedOn": "2021-07-26T02:00:58.091Z",
          "wordCount": 611,
          "title": "Learning Discriminative Representations for Multi-Label Image Recognition. (arXiv:2107.11159v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1\">Inkyu Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kwanyong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sanghyun Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "Unsupervised Domain Adaptation for semantic segmentation has gained immense\npopularity since it can transfer knowledge from simulation to real (Sim2Real)\nby largely cutting out the laborious per pixel labeling efforts at real. In\nthis work, we present a new video extension of this task, namely Unsupervised\nDomain Adaptation for Video Semantic Segmentation. As it became easy to obtain\nlarge-scale video labels through simulation, we believe attempting to maximize\nSim2Real knowledge transferability is one of the promising directions for\nresolving the fundamental data-hungry issue in the video. To tackle this new\nproblem, we present a novel two-phase adaptation scheme. In the first step, we\nexhaustively distill source domain knowledge using supervised loss functions.\nSimultaneously, video adversarial training (VAT) is employed to align the\nfeatures from source to target utilizing video context. In the second step, we\napply video self-training (VST), focusing only on the target data. To construct\nrobust pseudo labels, we exploit the temporal information in the video, which\nhas been rarely explored in the previous image-based self-training approaches.\nWe set strong baseline scores on 'VIPER to CityscapeVPS' adaptation scenario.\nWe show that our proposals significantly outperform previous image-based UDA\nmethods both on image-level (mIoU) and video-level (VPQ) evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.11052",
          "publishedOn": "2021-07-26T02:00:58.057Z",
          "wordCount": 634,
          "title": "Unsupervised Domain Adaptation for Video Semantic Segmentation. (arXiv:2107.11052v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11119",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xinyang Wu</a>",
          "description": "Before analy z ing the CT image, it is very important to segment the heart\nimage, and the left ve ntricular (LV) inner and outer membrane segmentation is\none of the most important contents. However, manual segmentation is tedious and\ntime consuming. In order to facilitate doctors to focus on high tech tasks such\nas disease analysis and diagnosis, it is crucial to develop a fast and accurate\nsegmentation method [1]. In view of this phenomenon, this paper uses distance\nregularized level set (DRL SE) to explore the segmentation effect of epicardium\nand endocardium 2 ]], which includes a distance regula riz ed t erm and an\nexternal energy term. Finally, five CT images are used to verify the proposed\nmethod, and image quality evaluation indexes such as dice score and Hausdorff\ndistance are used to evaluate the segmentation effect. The results showed that\nthe me tho d could separate the inner and outer membrane very well (endocardium\ndice = 0.9253, Hausdorff = 7.8740; epicardium Hausdorff = 0.9687, Hausdorff = 6 .",
          "link": "http://arxiv.org/abs/2107.11119",
          "publishedOn": "2021-07-26T02:00:58.039Z",
          "wordCount": 611,
          "title": "Cardiac CT segmentation based on distance regularized level set. (arXiv:2107.11119v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatch_K/0/1/0/all/0/1\">Kyle Beltran Hatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jason Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Future urban transportation concepts include a mixture of ground and air\nvehicles with varying degrees of autonomy in a congested environment. In such\ndynamic environments, occupancy maps alone are not sufficient for safe path\nplanning. Safe and efficient transportation requires reasoning about the 3D\nflow of traffic and properly modeling uncertainty. Several different approaches\ncan be taken for developing 3D velocity maps. This paper explores a Bayesian\napproach that captures our uncertainty in the map given training data. The\napproach involves projecting spatial coordinates into a high-dimensional\nfeature space and then applying Bayesian linear regression to make predictions\nand quantify uncertainty in our estimates. On a collection of air and ground\ndatasets, we demonstrate that this approach is effective and more scalable than\nseveral alternative approaches.",
          "link": "http://arxiv.org/abs/2107.11039",
          "publishedOn": "2021-07-26T02:00:58.022Z",
          "wordCount": 582,
          "title": "3D Radar Velocity Maps for Uncertain Dynamic Environments. (arXiv:2107.11039v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermosilla_P/0/1/0/all/0/1\">Pedro Hermosilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>",
          "description": "Density estimation plays a crucial role in many data analysis tasks, as it\ninfers a continuous probability density function (PDF) from discrete samples.\nThus, it is used in tasks as diverse as analyzing population data, spatial\nlocations in 2D sensor readings, or reconstructing scenes from 3D scans. In\nthis paper, we introduce a learned, data-driven deep density estimation (DDE)\nto infer PDFs in an accurate and efficient manner, while being independent of\ndomain dimensionality or sample size. Furthermore, we do not require access to\nthe original PDF during estimation, neither in parametric form, nor as priors,\nor in the form of many samples. This is enabled by training an unstructured\nconvolutional neural network on an infinite stream of synthetic PDFs, as\nunbound amounts of synthetic training data generalize better across a deck of\nnatural PDFs than any natural finite training data will do. Thus, we hope that\nour publicly available DDE method will be beneficial in many areas of data\nanalysis, where continuous models are to be estimated from discrete\nobservations.",
          "link": "http://arxiv.org/abs/2107.11085",
          "publishedOn": "2021-07-26T02:00:58.013Z",
          "wordCount": 629,
          "title": "Data-driven deep density estimation. (arXiv:2107.11085v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1\">Pinzhuo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yao Gao</a>",
          "description": "Meta-learning provides a promising way for learning to efficiently learn and\nachieves great success in many applications. However, most meta-learning\nliterature focuses on dealing with tasks from a same domain, making it brittle\nto generalize to tasks from the other unseen domains. In this work, we address\nthis problem by simulating tasks from the other unseen domains to improve the\ngeneralization and robustness of meta-learning method. Specifically, we propose\na model-agnostic shift layer to learn how to simulate the domain shift and\ngenerate pseudo tasks, and develop a new adversarial learning-to-learn\nmechanism to train it. Based on the pseudo tasks, the meta-learning model can\nlearn cross-domain meta-knowledge, which can generalize well on unseen domains.\nWe conduct extensive experiments under the domain generalization setting.\nExperimental results demonstrate that the proposed shift layer is applicable to\nvarious meta-learning frameworks. Moreover, our method also leads to\nstate-of-the-art performance on different cross-domain few-shot classification\nbenchmarks and produces good results on cross-domain few-shot regression.",
          "link": "http://arxiv.org/abs/2107.11056",
          "publishedOn": "2021-07-26T02:00:58.005Z",
          "wordCount": 600,
          "title": "Improving the Generalization of Meta-learning on Unseen Domains via Adversarial Shift. (arXiv:2107.11056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengya Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1\">Chwee Ming Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>",
          "description": "Generating surgical reports aimed at surgical scene understanding in\nrobot-assisted surgery can contribute to documenting entry tasks and\npost-operative analysis. Despite the impressive outcome, the deep learning\nmodel degrades the performance when applied to different domains encountering\ndomain shifts. In addition, there are new instruments and variations in\nsurgical tissues appeared in robotic surgery. In this work, we propose\nclass-incremental domain adaptation (CIDA) with a multi-layer transformer-based\nmodel to tackle the new classes and domain shift in the target domain to\ngenerate surgical reports during robotic surgery. To adapt incremental classes\nand extract domain invariant features, a class-incremental (CI) learning method\nwith supervised contrastive (SupCon) loss is incorporated with a feature\nextractor. To generate caption from the extracted feature, curriculum by\none-dimensional gaussian smoothing (CBS) is integrated with a multi-layer\ntransformer-based caption prediction model. CBS smoothes the features embedding\nusing anti-aliasing and helps the model to learn domain invariant features. We\nalso adopt label smoothing (LS) to calibrate prediction probability and obtain\nbetter feature representation with both feature extractor and captioning model.\nThe proposed techniques are empirically evaluated by using the datasets of two\nsurgical domains, such as nephrectomy operations and transoral robotic surgery.\nWe observe that domain invariant feature learning and the well-calibrated\nnetwork improves the surgical report generation performance in both source and\ntarget domain under domain shift and unseen classes in the manners of one-shot\nand few-shot learning. The code is publicly available at\nhttps://github.com/XuMengyaAmy/CIDACaptioning.",
          "link": "http://arxiv.org/abs/2107.11091",
          "publishedOn": "2021-07-26T02:00:57.979Z",
          "wordCount": 695,
          "title": "Class-Incremental Domain Adaptation with Smoothing and Calibration for Surgical Report Generation. (arXiv:2107.11091v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koprinkova_Hristova_P/0/1/0/all/0/1\">Petia Koprinkova-Hristova</a>",
          "description": "The paper proposes a novel approach for gray scale images segmentation. It is\nbased on multiple features extraction from single feature per image pixel,\nnamely its intensity value, using Echo state network. The newly extracted\nfeatures -- reservoir equilibrium states -- reveal hidden image characteristics\nthat improve its segmentation via a clustering algorithm. Moreover, it was\ndemonstrated that the intrinsic plasticity tuning of reservoir fits its\nequilibrium states to the original image intensity distribution thus allowing\nfor its better segmentation. The proposed approach is tested on the benchmark\nimage Lena.",
          "link": "http://arxiv.org/abs/2107.11077",
          "publishedOn": "2021-07-26T02:00:57.971Z",
          "wordCount": 541,
          "title": "Reservoir Computing Approach for Gray Images Segmentation. (arXiv:2107.11077v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shasha Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guanghui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_S/0/1/0/all/0/1\">Shuiping Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Lin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>",
          "description": "By utilizing label distribution learning, a probability distribution is\nassigned for a facial image to express a compound emotion, which effectively\nimproves the problem of label uncertainties and noises occurred in one-hot\nlabels. In practice, it is observed that correlations among emotions are\ninherently different, such as surprised and happy emotions are more possibly\nsynchronized than surprised and neutral. It indicates the correlation may be\ncrucial for obtaining a reliable label distribution. Based on this, we propose\na new method that amends the label distribution of each facial image by\nleveraging correlations among expressions in the semantic space. Inspired by\ninherently diverse correlations among word2vecs, the topological information\namong facial expressions is firstly explored in the semantic space, and each\nimage is embedded into the semantic space. Specially, a class-relation graph is\nconstructed to transfer the semantic correlation among expressions into the\ntask space. By comparing semantic and task class-relation graphs of each image,\nthe confidence of its label distribution is evaluated. Based on the confidence,\nthe label distribution is amended by enhancing samples with higher confidence\nand weakening samples with lower confidence. Experimental results demonstrate\nthe proposed method is more effective than compared state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.11061",
          "publishedOn": "2021-07-26T02:00:57.963Z",
          "wordCount": 644,
          "title": "Label Distribution Amendment with Emotional Semantic Correlations for Facial Expression Recognition. (arXiv:2107.11061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhongqi Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Existing Unsupervised Domain Adaptation (UDA) literature adopts the covariate\nshift and conditional shift assumptions, which essentially encourage models to\nlearn common features across domains. However, due to the lack of supervision\nin the target domain, they suffer from the semantic loss: the feature will\ninevitably lose non-discriminative semantics in source domain, which is however\ndiscriminative in target domain. We use a causal view -- transportability\ntheory -- to identify that such loss is in fact a confounding effect, which can\nonly be removed by causal intervention. However, the theoretical solution\nprovided by transportability is far from practical for UDA, because it requires\nthe stratification and representation of an unobserved confounder that is the\ncause of the domain gap. To this end, we propose a practical solution:\nTransporting Causal Mechanisms (TCM), to identify the confounder stratum and\nrepresentations by using the domain-invariant disentangled causal mechanisms,\nwhich are discovered in an unsupervised fashion. Our TCM is both theoretically\nand empirically grounded. Extensive experiments show that TCM achieves\nstate-of-the-art performance on three challenging UDA benchmarks: ImageCLEF-DA,\nOffice-Home, and VisDA-2017. Codes are available in Appendix.",
          "link": "http://arxiv.org/abs/2107.11055",
          "publishedOn": "2021-07-26T02:00:57.955Z",
          "wordCount": 619,
          "title": "Transporting Causal Mechanisms for Unsupervised Domain Adaptation. (arXiv:2107.11055v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11010",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_B/0/1/0/all/0/1\">Bowen Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_M/0/1/0/all/0/1\">Min Gan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bingchuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuqiang Wang</a>",
          "description": "3D shape reconstruction is essential in the navigation of minimally-invasive\nand auto robot-guided surgeries whose operating environments are indirect and\nnarrow, and there have been some works that focused on reconstructing the 3D\nshape of the surgical organ through limited 2D information available. However,\nthe lack and incompleteness of such information caused by intraoperative\nemergencies (such as bleeding) and risk control conditions have not been\nconsidered. In this paper, a novel hierarchical shape-perception network (HSPN)\nis proposed to reconstruct the 3D point clouds (PCs) of specific brains from\none single incomplete image with low latency. A tree-structured predictor and\nseveral hierarchical attention pipelines are constructed to generate point\nclouds that accurately describe the incomplete images and then complete these\npoint clouds with high quality. Meanwhile, attention gate blocks (AGBs) are\ndesigned to efficiently aggregate geometric local features of incomplete PCs\ntransmitted by hierarchical attention pipelines and internal features of\nreconstructing point clouds. With the proposed HSPN, 3D shape perception and\ncompletion can be achieved spontaneously. Comprehensive results measured by\nChamfer distance and PC-to-PC error demonstrate that the performance of the\nproposed HSPN outperforms other competitive methods in terms of qualitative\ndisplays, quantitative experiment, and classification evaluation.",
          "link": "http://arxiv.org/abs/2107.11010",
          "publishedOn": "2021-07-26T02:00:57.947Z",
          "wordCount": 653,
          "title": "3D Brain Reconstruction by Hierarchical Shape-Perception Network from a Single Incomplete Image. (arXiv:2107.11010v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yunjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "Recent state-of-the-art active learning methods have mostly leveraged\nGenerative Adversarial Networks (GAN) for sample acquisition; however, GAN is\nusually known to suffer from instability and sensitivity to hyper-parameters.\nIn contrast to these methods, we propose in this paper a novel active learning\nframework that we call Maximum Classifier Discrepancy for Active Learning\n(MCDAL) which takes the prediction discrepancies between multiple classifiers.\nIn particular, we utilize two auxiliary classification layers that learn\ntighter decision boundaries by maximizing the discrepancies among them.\nIntuitively, the discrepancies in the auxiliary classification layers'\npredictions indicate the uncertainty in the prediction. In this regard, we\npropose a novel method to leverage the classifier discrepancies for the\nacquisition function for active learning. We also provide an interpretation of\nour idea in relation to existing GAN based active learning methods and domain\nadaptation frameworks. Moreover, we empirically demonstrate the utility of our\napproach where the performance of our approach exceeds the state-of-the-art\nmethods on several image classification and semantic segmentation datasets in\nactive learning setups.",
          "link": "http://arxiv.org/abs/2107.11049",
          "publishedOn": "2021-07-26T02:00:57.938Z",
          "wordCount": 614,
          "title": "MCDAL: Maximum Classifier Discrepancy for Active Learning. (arXiv:2107.11049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jie Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jude_C/0/1/0/all/0/1\">Curran Jude</a>",
          "description": "We consider unsupervised cell nuclei segmentation in this paper. Exploiting\nthe recently-proposed unpaired image-to-image translation between cell nuclei\nimages and randomly synthetic masks, existing approaches, e.g., CycleGAN, have\nachieved encouraging results. However, these methods usually take a two-stage\npipeline and fail to learn end-to-end in cell nuclei images. More seriously,\nthey could lead to the lossy transformation problem, i.e., the content\ninconsistency between the original images and the corresponding segmentation\noutput. To address these limitations, we propose a novel end-to-end\nunsupervised framework called Aligned Disentangling Generative Adversarial\nNetwork (AD-GAN). Distinctively, AD-GAN introduces representation\ndisentanglement to separate content representation (the underling spatial\nstructure) from style representation (the rendering of the structure). With\nthis framework, spatial structure can be preserved explicitly, enabling a\nsignificant reduction of macro-level lossy transformation. We also propose a\nnovel training algorithm able to align the disentangled content in the latent\nspace to reduce micro-level lossy transformation. Evaluations on real-world 2D\nand 3D datasets show that AD-GAN substantially outperforms the other comparison\nmethods and the professional software both quantitatively and qualitatively.\nSpecifically, the proposed AD-GAN leads to significant improvement over the\ncurrent best unsupervised methods by an average 17.8% relatively (w.r.t. the\nmetric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN\neven performs competitive with the best supervised models, taking a further\nleap towards end-to-end unsupervised nuclei segmentation.",
          "link": "http://arxiv.org/abs/2107.11022",
          "publishedOn": "2021-07-26T02:00:57.916Z",
          "wordCount": 674,
          "title": "AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_M/0/1/0/all/0/1\">Manish Bhattarai</a>",
          "description": "We present a new four-pronged approach to build firefighter's situational\nawareness for the first time in the literature. We construct a series of deep\nlearning frameworks built on top of one another to enhance the safety,\nefficiency, and successful completion of rescue missions conducted by\nfirefighters in emergency first response settings. First, we used a deep\nConvolutional Neural Network (CNN) system to classify and identify objects of\ninterest from thermal imagery in real-time. Next, we extended this CNN\nframework for object detection, tracking, segmentation with a Mask RCNN\nframework, and scene description with a multimodal natural language\nprocessing(NLP) framework. Third, we built a deep Q-learning-based agent,\nimmune to stress-induced disorientation and anxiety, capable of making clear\nnavigation decisions based on the observed and stored facts in live-fire\nenvironments. Finally, we used a low computational unsupervised learning\ntechnique called tensor decomposition to perform meaningful feature extraction\nfor anomaly detection in real-time. With these ad-hoc deep learning structures,\nwe built the artificial intelligence system's backbone for firefighters'\nsituational awareness. To bring the designed system into usage by firefighters,\nwe designed a physical structure where the processed results are used as inputs\nin the creation of an augmented reality capable of advising firefighters of\ntheir location and key features around them, which are vital to the rescue\noperation at hand, as well as a path planning feature that acts as a virtual\nguide to assist disoriented first responders in getting back to safety. When\ncombined, these four approaches present a novel approach to information\nunderstanding, transfer, and synthesis that could dramatically improve\nfirefighter response and efficacy and reduce life loss.",
          "link": "http://arxiv.org/abs/2107.11043",
          "publishedOn": "2021-07-26T02:00:57.908Z",
          "wordCount": 709,
          "title": "Integrating Deep Learning and Augmented Reality to Enhance Situational Awareness in Firefighting Environments. (arXiv:2107.11043v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_M/0/1/0/all/0/1\">Moonbin Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gayoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>",
          "description": "Scene text editing (STE), which converts a text in a scene image into the\ndesired text while preserving an original style, is a challenging task due to a\ncomplex intervention between text and style. To address this challenge, we\npropose a novel representational learning-based STE model, referred to as\nRewriteNet that employs textual information as well as visual information. We\nassume that the scene text image can be decomposed into content and style\nfeatures where the former represents the text information and style represents\nscene text characteristics such as font, alignment, and background. Under this\nassumption, we propose a method to separately encode content and style features\nof the input image by introducing the scene text recognizer that is trained by\ntext information. Then, a text-edited image is generated by combining the style\nfeature from the original image and the content feature from the target text.\nUnlike previous works that are only able to use synthetic images in the\ntraining phase, we also exploit real-world images by proposing a\nself-supervised training scheme, which bridges the domain gap between synthetic\nand real data. Our experiments demonstrate that RewriteNet achieves better\nquantitative and qualitative performance than other comparisons. Moreover, we\nvalidate that the use of text information and the self-supervised training\nscheme improves text switching performance. The implementation and dataset will\nbe publicly available.",
          "link": "http://arxiv.org/abs/2107.11041",
          "publishedOn": "2021-07-26T02:00:57.900Z",
          "wordCount": 671,
          "title": "RewriteNet: Realistic Scene Text Image Generation via Editing Text in Real-world Image. (arXiv:2107.11041v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11007",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yixiao Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_K/0/1/0/all/0/1\">Kaixuan Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>",
          "description": "Recovering an underlying image from under-sampled measurements, Compressive\nSensing Imaging (CSI) is a challenging problem and has many practical\napplications. Recently, deep neural networks have been applied to this problem\nwith promising results, owing to its implicitly learned prior to alleviate the\nill-poseness of CSI. However, existing neural network approaches require\nseparate models for each imaging parameter like sampling ratios, leading to\ntraining difficulties and overfitting to specific settings. In this paper, we\npresent a dynamic proximal unrolling network (dubbed DPUNet), which can handle\na variety of measurement matrices via one single model without retraining.\nSpecifically, DPUNet can exploit both embedded physical model via gradient\ndescent and imposing image prior with learned dynamic proximal mapping leading\nto joint reconstruction. A key component of DPUNet is a dynamic proximal\nmapping module, whose parameters can be dynamically adjusted at inference stage\nand make it adapt to any given imaging setting. Experimental results\ndemonstrate that the proposed DPUNet can effectively handle multiple CSI\nmodalities under varying sampling ratios and noise levels with only one model,\nand outperform the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.11007",
          "publishedOn": "2021-07-26T02:00:57.877Z",
          "wordCount": 622,
          "title": "Dynamic Proximal Unrolling Network for Compressive Sensing Imaging. (arXiv:2107.11007v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zobeidi_E/0/1/0/all/0/1\">Ehsan Zobeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>",
          "description": "Neural networks that map 3D coordinates to signed distance function (SDF) or\noccupancy values have enabled high-fidelity implicit representations of object\nshape. This paper develops a new shape model that allows synthesizing novel\ndistance views by optimizing a continuous signed directional distance function\n(SDDF). Similar to deep SDF models, our SDDF formulation can represent whole\ncategories of shapes and complete or interpolate across shapes from partial\ninput data. Unlike an SDF, which measures distance to the nearest surface in\nany direction, an SDDF measures distance in a given direction. This allows\ntraining an SDDF model without 3D shape supervision, using only distance\nmeasurements, readily available from depth camera or Lidar sensors. Our model\nalso removes post-processing steps like surface extraction or rendering by\ndirectly predicting distance at arbitrary locations and viewing directions.\nUnlike deep view-synthesis techniques, such as Neural Radiance Fields, which\ntrain high-capacity black-box models, our model encodes by construction the\nproperty that SDDF values decrease linearly along the viewing direction. This\nstructure constraint not only results in dimensionality reduction but also\nprovides analytical confidence about the accuracy of SDDF predictions,\nregardless of the distance to the object surface.",
          "link": "http://arxiv.org/abs/2107.11024",
          "publishedOn": "2021-07-26T02:00:57.868Z",
          "wordCount": 626,
          "title": "A Deep Signed Directional Distance Function for Object Shape Representation. (arXiv:2107.11024v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11001",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Goyal_B/0/1/0/all/0/1\">Bhavya Goyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>",
          "description": "Scene understanding under low-light conditions is a challenging problem. This\nis due to the small number of photons captured by the camera and the resulting\nlow signal-to-noise ratio (SNR). Single-photon cameras (SPCs) are an emerging\nsensing modality that are capable of capturing images with high sensitivity.\nDespite having minimal read-noise, images captured by SPCs in photon-starved\nconditions still suffer from strong shot noise, preventing reliable scene\ninference. We propose photon scale-space a collection of high-SNR images\nspanning a wide range of photons-per-pixel (PPP) levels (but same scene\ncontent) as guides to train inference model on low photon flux images. We\ndevelop training techniques that push images with different illumination levels\ncloser to each other in feature representation space. The key idea is that\nhaving a spectrum of different brightness levels during training enables\neffective guidance, and increases robustness to shot noise even in extreme\nnoise cases. Based on the proposed approach, we demonstrate, via simulations\nand real experiments with a SPAD camera, high-performance on various inference\ntasks such as image classification and monocular depth estimation under ultra\nlow-light, down to < 1 PPP.",
          "link": "http://arxiv.org/abs/2107.11001",
          "publishedOn": "2021-07-26T02:00:57.849Z",
          "wordCount": 621,
          "title": "Photon-Starved Scene Inference using Single Photon Cameras. (arXiv:2107.11001v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_M/0/1/0/all/0/1\">Mehdi Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estrada_R/0/1/0/all/0/1\">Rolando Estrada</a>",
          "description": "Transparent objects are a very challenging problem in computer vision. They\nare hard to segment or classify due to their lack of precise boundaries, and\nthere is limited data available for training deep neural networks. As such,\ncurrent solutions for this problem employ rigid synthetic datasets, which lack\nflexibility and lead to severe performance degradation when deployed on\nreal-world scenarios. In particular, these synthetic datasets omit features\nsuch as refraction, dispersion and caustics due to limitations in the rendering\npipeline. To address this issue, we present SuperCaustics, a real-time,\nopen-source simulation of transparent objects designed for deep learning\napplications. SuperCaustics features extensive modules for stochastic\nenvironment creation; uses hardware ray-tracing to support caustics,\ndispersion, and refraction; and enables generating massive datasets with\nmulti-modal, pixel-perfect ground truth annotations. To validate our proposed\nsystem, we trained a deep neural network from scratch to segment transparent\nobjects in difficult lighting scenarios. Our neural network achieved\nperformance comparable to the state-of-the-art on a real-world dataset using\nonly 10% of the training data and in a fraction of the training time. Further\nexperiments show that a model trained with SuperCaustics can segment different\ntypes of caustics, even in images with multiple overlapping transparent\nobjects. To the best of our knowledge, this is the first such result for a\nmodel trained on synthetic data. Both our open-source code and experimental\ndata are freely available online.",
          "link": "http://arxiv.org/abs/2107.11008",
          "publishedOn": "2021-07-26T02:00:57.838Z",
          "wordCount": 665,
          "title": "SuperCaustics: Real-time, open-source simulation of transparent objects for deep learning applications. (arXiv:2107.11008v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>",
          "description": "Video semantic segmentation is an essential task for the analysis and\nunderstanding of videos. Recent efforts largely focus on supervised video\nsegmentation by learning from fully annotated data, but the learnt models often\nexperience clear performance drop while applied to videos of a different\ndomain. This paper presents DA-VSN, a domain adaptive video segmentation\nnetwork that addresses domain gaps in videos by temporal consistency\nregularization (TCR) for consecutive frames of target-domain videos. DA-VSN\nconsists of two novel and complementary designs. The first is cross-domain TCR\nthat guides the prediction of target frames to have similar temporal\nconsistency as that of source frames (learnt from annotated source data) via\nadversarial learning. The second is intra-domain TCR that guides unconfident\npredictions of target frames to have similar temporal consistency as confident\npredictions of target frames. Extensive experiments demonstrate the superiority\nof our proposed domain adaptive video segmentation network which outperforms\nmultiple baselines consistently by large margins.",
          "link": "http://arxiv.org/abs/2107.11004",
          "publishedOn": "2021-07-26T02:00:57.815Z",
          "wordCount": 601,
          "title": "Domain Adaptive Video Segmentation via Temporal Consistency Regularization. (arXiv:2107.11004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>",
          "description": "We propose pruning ternary quantization (PTQ), a simple, yet effective,\nsymmetric ternary quantization method. The method significantly compresses\nneural network weights to a sparse ternary of [-1,0,1] and thus reduces\ncomputational, storage, and memory footprints. We show that PTQ can convert\nregular weights to ternary orthonormal bases by simply using pruning and L2\nprojection. In addition, we introduce a refined straight-through estimator to\nfinalize and stabilize the quantized weights. Our method can provide at most\n46x compression ratio on the ResNet-18 structure, with an acceptable accuracy\nof 65.36%, outperforming leading methods. Furthermore, PTQ can compress a\nResNet-18 model from 46 MB to 955KB (~48x) and a ResNet-50 model from 99 MB to\n3.3MB (~30x), while the top-1 accuracy on ImageNet drops slightly from 69.7% to\n65.3% and from 76.15% to 74.47%, respectively. Our method unifies pruning and\nquantization and thus provides a range of size-accuracy trade-off.",
          "link": "http://arxiv.org/abs/2107.10998",
          "publishedOn": "2021-07-26T02:00:57.807Z",
          "wordCount": 575,
          "title": "Pruning Ternary Quantization. (arXiv:2107.10998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shitong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>",
          "description": "Point clouds acquired from scanning devices are often perturbed by noise,\nwhich affects downstream tasks such as surface reconstruction and analysis. The\ndistribution of a noisy point cloud can be viewed as the distribution of a set\nof noise-free samples $p(x)$ convolved with some noise model $n$, leading to\n$(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy\npoint cloud, we propose to increase the log-likelihood of each point from $p *\nn$ via gradient ascent -- iteratively updating each point's position. Since $p\n* n$ is unknown at test-time, and we only need the score (i.e., the gradient of\nthe log-probability function) to perform gradient ascent, we propose a neural\nnetwork architecture to estimate the score of $p * n$ given only noisy point\nclouds as input. We derive objective functions for training the network and\ndevelop a denoising algorithm leveraging on the estimated scores. Experiments\ndemonstrate that the proposed model outperforms state-of-the-art methods under\na variety of noise models, and shows the potential to be applied in other tasks\nsuch as point cloud upsampling.",
          "link": "http://arxiv.org/abs/2107.10981",
          "publishedOn": "2021-07-26T02:00:57.799Z",
          "wordCount": 610,
          "title": "Score-Based Point Cloud Denoising. (arXiv:2107.10981v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Touqeer Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emami_E/0/1/0/all/0/1\">Ebrahim Emami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadik_M/0/1/0/all/0/1\">Martin &#x10c;ad&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bebis_G/0/1/0/all/0/1\">George Bebis</a>",
          "description": "Skyline plays a pivotal role in mountainous visual geo-localization and\nlocalization/navigation of planetary rovers/UAVs and virtual/augmented reality\napplications. We present a novel mountainous skyline detection approach where\nwe adapt a shallow learning approach to learn a set of filters to discriminate\nbetween edges belonging to sky-mountain boundary and others coming from\ndifferent regions. Unlike earlier approaches, which either rely on extraction\nof explicit feature descriptors and their classification, or fine-tuning\ngeneral scene parsing deep networks for sky segmentation, our approach learns\nlinear filters based on local structure analysis. At test time, for every\ncandidate edge pixel, a single filter is chosen from the set of learned filters\nbased on pixel's structure tensor, and then applied to the patch around it. We\nthen employ dynamic programming to solve the shortest path problem for the\nresultant multistage graph to get the sky-mountain boundary. The proposed\napproach is computationally faster than earlier methods while providing\ncomparable performance and is more suitable for resource constrained platforms\ne.g., mobile devices, planetary rovers and UAVs. We compare our proposed\napproach against earlier skyline detection methods using four different data\nsets. Our code is available at\n\\url{https://github.com/TouqeerAhmad/skyline_detection}.",
          "link": "http://arxiv.org/abs/2107.10997",
          "publishedOn": "2021-07-26T02:00:57.772Z",
          "wordCount": 648,
          "title": "Resource Efficient Mountainous Skyline Extraction using Shallow Learning. (arXiv:2107.10997v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_L/0/1/0/all/0/1\">Libo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_J/0/1/0/all/0/1\">Jochen Lang</a>",
          "description": "Feature pyramids and iterative refinement have recently led to great progress\nin optical flow estimation. However, downsampling in feature pyramids can cause\nblending of foreground objects with the background, which will mislead\nsubsequent decisions in the iterative processing. The results are missing\ndetails especially in the flow of thin and of small structures. We propose a\nnovel Residual Feature Pyramid Module (RFPM) which retains important details in\nthe feature map without changing the overall iterative refinement design of the\noptical flow estimation. RFPM incorporates a residual structure between\nmultiple feature pyramids into a downsampling module that corrects the blending\nof objects across boundaries. We demonstrate how to integrate our module with\ntwo state-of-the-art iterative refinement architectures. Results show that our\nRFPM visibly reduces flow errors and improves state-of-art performance in the\nclean pass of Sintel, and is one of the top-performing methods in KITTI.\nAccording to the particular modular structure of RFPM, we introduce a special\ntransfer learning approach that can dramatically decrease the training time\ncompared to a typical full optical flow training schedule on multiple datasets.",
          "link": "http://arxiv.org/abs/2107.10990",
          "publishedOn": "2021-07-26T02:00:57.748Z",
          "wordCount": 611,
          "title": "Detail Preserving Residual Feature Pyramid Modules for Optical Flow. (arXiv:2107.10990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10912",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Velden_B/0/1/0/all/0/1\">Bas H.M. van der Velden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gilhuijs_K/0/1/0/all/0/1\">Kenneth G.A. Gilhuijs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viergever_M/0/1/0/all/0/1\">Max A. Viergever</a>",
          "description": "With an increase in deep learning-based methods, the call for explainability\nof such methods grows, especially in high-stakes decision making areas such as\nmedical image analysis. This survey presents an overview of eXplainable\nArtificial Intelligence (XAI) used in deep learning-based medical image\nanalysis. A framework of XAI criteria is introduced to classify deep\nlearning-based medical image analysis methods. Papers on XAI techniques in\nmedical image analysis are then surveyed and categorized according to the\nframework and according to anatomical location. The paper concludes with an\noutlook of future opportunities for XAI in medical image analysis.",
          "link": "http://arxiv.org/abs/2107.10912",
          "publishedOn": "2021-07-26T02:00:57.739Z",
          "wordCount": 561,
          "title": "Explainable artificial intelligence (XAI) in deep learning-based medical image analysis. (arXiv:2107.10912v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coenen_M/0/1/0/all/0/1\">Max Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottensteiner_F/0/1/0/all/0/1\">Franz Rottensteiner</a>",
          "description": "The 3D reconstruction of objects is a prerequisite for many highly relevant\napplications of computer vision such as mobile robotics or autonomous driving.\nTo deal with the inverse problem of reconstructing 3D objects from their 2D\nprojections, a common strategy is to incorporate prior object knowledge into\nthe reconstruction approach by establishing a 3D model and aligning it to the\n2D image plane. However, current approaches are limited due to inadequate shape\npriors and the insufficiency of the derived image observations for a reliable\nalignment with the 3D model. The goal of this paper is to show how 3D object\nreconstruction can profit from a more sophisticated shape prior and from a\ncombined incorporation of different observation types inferred from the images.\nWe introduce a subcategory-aware deformable vehicle model that makes use of a\nprediction of the vehicle type for a more appropriate regularisation of the\nvehicle shape. A multi-branch CNN is presented to derive predictions of the\nvehicle type and orientation. This information is also introduced as prior\ninformation for model fitting. Furthermore, the CNN extracts vehicle keypoints\nand wireframes, which are well-suited for model-to-image association and model\nfitting. The task of pose estimation and reconstruction is addressed by a\nversatile probabilistic model. Extensive experiments are conducted using two\nchallenging real-world data sets on both of which the benefit of the developed\nshape prior can be shown. A comparison to state-of-the-art methods for vehicle\npose estimation shows that the proposed approach performs on par or better,\nconfirming the suitability of the developed shape prior and probabilistic model\nfor vehicle reconstruction.",
          "link": "http://arxiv.org/abs/2107.10898",
          "publishedOn": "2021-07-26T02:00:57.712Z",
          "wordCount": 704,
          "title": "Pose Estimation and 3D Reconstruction of Vehicles from Stereo-Images Using a Subcategory-Aware Shape Prior. (arXiv:2107.10898v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaojun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Recent studies show that deep neural networks (DNN) are vulnerable to\nadversarial examples, which aim to mislead DNNs by adding perturbations with\nsmall magnitude. To defend against such attacks, both empirical and theoretical\ndefense approaches have been extensively studied for a single ML model. In this\nwork, we aim to analyze and provide the certified robustness for ensemble ML\nmodels, together with the sufficient and necessary conditions of robustness for\ndifferent ensemble protocols. Although ensemble models are shown more robust\nthan a single model empirically; surprisingly, we find that in terms of the\ncertified robustness the standard ensemble models only achieve marginal\nimprovement compared to a single model. Thus, to explore the conditions that\nguarantee to provide certifiably robust ensemble ML models, we first prove that\ndiversified gradient and large confidence margin are sufficient and necessary\nconditions for certifiably robust ensemble models under the model-smoothness\nassumption. We then provide the bounded model-smoothness analysis based on the\nproposed Ensemble-before-Smoothing strategy. We also prove that an ensemble\nmodel can always achieve higher certified robustness than a single base model\nunder mild conditions. Inspired by the theoretical findings, we propose the\nlightweight Diversity Regularized Training (DRT) to train certifiably robust\nensemble ML models. Extensive experiments show that our DRT enhanced ensembles\ncan consistently achieve higher certified robustness than existing single and\nensemble ML models, demonstrating the state-of-the-art certified L2-robustness\non MNIST, CIFAR-10, and ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.10873",
          "publishedOn": "2021-07-26T02:00:57.697Z",
          "wordCount": 691,
          "title": "On the Certified Robustness for Ensemble Models and Beyond. (arXiv:2107.10873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chengxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zheng Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Gangyi Ding</a>",
          "description": "Deep generative models have made great progress in synthesizing images with\narbitrary human poses and transferring poses of one person to others. However,\nmost existing approaches explicitly leverage the pose information extracted\nfrom the source images as a conditional input for the generative networks.\nMeanwhile, they usually focus on the visual fidelity of the synthesized images\nbut neglect the inherent consistency, which further confines their performance\nof pose transfer. To alleviate the current limitations and improve the quality\nof the synthesized images, we propose a pose transfer network with Disentangled\nFeature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair\nof images containing the source and target person, DFC-Net extracts pose and\nstatic information from the source and target respectively, then synthesizes an\nimage of the target person with the desired pose from the source. Moreover,\nDFC-Net leverages disentangled feature consistency losses in the adversarial\ntraining to strengthen the transfer coherence and integrates the keypoint\namplifier to enhance the pose feature extraction. Additionally, an unpaired\nsupport dataset Mixamo-Sup providing more extra pose information has been\nfurther utilized during the training to improve the generality and robustness\nof DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have\ndemonstrated DFC-Net achieves state-of-the-art performance on pose transfer.",
          "link": "http://arxiv.org/abs/2107.10984",
          "publishedOn": "2021-07-26T02:00:57.670Z",
          "wordCount": 650,
          "title": "Human Pose Transfer with Disentangled Feature Consistency. (arXiv:2107.10984v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nelson_H/0/1/0/all/0/1\">Henry J. Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papanikolopoulos_N/0/1/0/all/0/1\">Nikolaos Papanikolopoulos</a>",
          "description": "In order to apply the recent successes of automated plant phenotyping and\nmachine learning on a large scale, efficient and general algorithms must be\ndesigned to intelligently split crop fields into small, yet actionable,\nportions that can then be processed by more complex algorithms. In this paper\nwe notice a similarity between the current state-of-the-art for this problem\nand a commonly used density-based clustering algorithm, Quickshift. Exploiting\nthis similarity we propose a number of novel, application specific algorithms\nwith the goal of producing a general and scalable plant segmentation algorithm.\nThe novel algorithms proposed in this work are shown to produce quantitatively\nbetter results than the current state-of-the-art while being less sensitive to\ninput parameters and maintaining the same algorithmic time complexity. When\nincorporated into field-scale phenotyping systems, the proposed algorithms\nshould work as a drop in replacement that can greatly improve the accuracy of\nresults while ensuring that performance and scalability remain undiminished.",
          "link": "http://arxiv.org/abs/2107.10950",
          "publishedOn": "2021-07-26T02:00:57.648Z",
          "wordCount": 589,
          "title": "Pre-Clustering Point Clouds of Crop Fields Using Scalable Methods. (arXiv:2107.10950v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>",
          "description": "Conditional computation and modular networks have been recently proposed for\nmultitask learning and other problems as a way to decompose problem solving\ninto multiple reusable computational blocks. We propose a new approach for\nlearning modular networks based on the isometric version of ResNet with all\nresidual blocks having the same configuration and the same number of\nparameters. This architectural choice allows adding, removing and changing the\norder of residual blocks. In our method, the modules can be invoked repeatedly\nand allow knowledge transfer to novel tasks by adjusting the order of\ncomputation. This allows soft weight sharing between tasks with only a small\nincrease in the number of parameters. We show that our method leads to\ninterpretable self-organization of modules in case of multi-task learning,\ntransfer learning and domain adaptation while achieving competitive results on\nthose tasks. From practical perspective, our approach allows to: (a) reuse\nexisting modules for learning new task by adjusting the computation order, (b)\nuse it for unsupervised multi-source domain adaptation to illustrate that\nadaptation to unseen data can be achieved by only manipulating the order of\npretrained modules, (c) show how our approach can be used to increase accuracy\nof existing architectures for image classification tasks such as ImageNet,\nwithout any parameter increase, by reusing the same block multiple times.",
          "link": "http://arxiv.org/abs/2107.10963",
          "publishedOn": "2021-07-26T02:00:57.640Z",
          "wordCount": 653,
          "title": "Compositional Models: Multi-Task Learning and Knowledge Transfer with Modular Networks. (arXiv:2107.10963v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Linghao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jinsong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges EL Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose a domain generalization (DG) approach to learn on\nseveral labeled source domains and transfer knowledge to a target domain that\nis inaccessible in training. Considering the inherent conditional and label\nshifts, we would expect the alignment of $p(x|y)$ and $p(y)$. However, the\nwidely used domain invariant feature learning (IFL) methods relies on aligning\nthe marginal concept shift w.r.t. $p(x)$, which rests on an unrealistic\nassumption that $p(y)$ is invariant across domains. We thereby propose a novel\nvariational Bayesian inference framework to enforce the conditional\ndistribution alignment w.r.t. $p(x|y)$ via the prior distribution matching in a\nlatent space, which also takes the marginal label shift w.r.t. $p(y)$ into\nconsideration with the posterior alignment. Extensive experiments on various\nbenchmarks demonstrate that our framework is robust to the label shift and the\ncross-domain accuracy is significantly improved, thereby achieving superior\nperformance over the conventional IFL counterparts.",
          "link": "http://arxiv.org/abs/2107.10931",
          "publishedOn": "2021-07-26T02:00:57.623Z",
          "wordCount": 615,
          "title": "Domain Generalization under Conditional and Label Shifts via Variational Bayesian Inference. (arXiv:2107.10931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10895",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Malawade_A/0/1/0/all/0/1\">Arnav Malawade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Odema_M/0/1/0/all/0/1\">Mohanad Odema</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lajeunesse_DeGroot_S/0/1/0/all/0/1\">Sebastien Lajeunesse-DeGroot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>",
          "description": "Autonomous vehicles (AV) are expected to revolutionize transportation and\nimprove road safety significantly. However, these benefits do not come without\ncost; AVs require large Deep-Learning (DL) models and powerful hardware\nplatforms to operate reliably in real-time, requiring between several hundred\nwatts to one kilowatt of power. This power consumption can dramatically reduce\nvehicles' driving range and affect emissions. To address this problem, we\npropose SAGE: a methodology for selectively offloading the key energy-consuming\nmodules of DL architectures to the cloud to optimize edge energy usage while\nmeeting real-time latency constraints. Furthermore, we leverage Head Network\nDistillation (HND) to introduce efficient bottlenecks within the DL\narchitecture in order to minimize the network overhead costs of offloading with\nalmost no degradation in the model's performance. We evaluate SAGE using an\nNvidia Jetson TX2 and an industry-standard Nvidia Drive PX2 as the AV edge\ndevices and demonstrate that our offloading strategy is practical for a wide\nrange of DL models and internet connection bandwidths on 3G, 4G LTE, and WiFi\ntechnologies. Compared to edge-only computation, SAGE reduces energy\nconsumption by an average of 36.13%, 47.07%, and 55.66% for an AV with one\nlow-resolution camera, one high-resolution camera, and three high-resolution\ncameras, respectively. SAGE also reduces upload data size by up to 98.40%\ncompared to direct camera offloading.",
          "link": "http://arxiv.org/abs/2107.10895",
          "publishedOn": "2021-07-26T02:00:57.603Z",
          "wordCount": 689,
          "title": "SAGE: A Split-Architecture Methodology for Efficient End-to-End Autonomous Vehicle Control. (arXiv:2107.10895v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mommert_M/0/1/0/all/0/1\">Michael Mommert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheibenreif_L/0/1/0/all/0/1\">Linus Scheibenreif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1\">Jo&#xeb;lle Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1\">Damian Borth</a>",
          "description": "Satellite remote imaging enables the detailed study of land use patterns on a\nglobal scale. We investigate the possibility to improve the information content\nof traditional land use classification by identifying the nature of industrial\nsites from medium-resolution remote sensing images. In this work, we focus on\nclassifying different types of power plants from Sentinel-2 imaging data. Using\na ResNet-50 deep learning model, we are able to achieve a mean accuracy of\n90.0% in distinguishing 10 different power plant types and a background class.\nFurthermore, we are able to identify the cooling mechanisms utilized in thermal\npower plants with a mean accuracy of 87.5%. Our results enable us to\nqualitatively investigate the energy mix from Sentinel-2 imaging data, and\nprove the feasibility to classify industrial sites on a global scale from\nfreely available satellite imagery.",
          "link": "http://arxiv.org/abs/2107.10894",
          "publishedOn": "2021-07-26T02:00:57.585Z",
          "wordCount": 592,
          "title": "Power Plant Classification from Remote Imaging with Deep Learning. (arXiv:2107.10894v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Anand Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob Whitehill</a>",
          "description": "For the task of face verification, we explore the utility of harnessing\nauxiliary facial emotion labels to impose explicit geometric constraints on the\nembedding space when training deep embedding models. We introduce several novel\nloss functions that, in conjunction with a standard Triplet Loss [43], or\nArcFace loss [10], provide geometric constraints on the embedding space; the\nlabels for our loss functions can be provided using either manually annotated\nor automatically detected auxiliary emotion labels. Our method is implemented\npurely in terms of the loss function and does not require any changes to the\nneural network backbone of the embedding function.",
          "link": "http://arxiv.org/abs/2103.03862",
          "publishedOn": "2021-07-23T02:00:31.803Z",
          "wordCount": 593,
          "title": "Harnessing Geometric Constraints from Emotion Labels to improve Face Verification. (arXiv:2103.03862v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zu_K/0/1/0/all/0/1\">Keke Zu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuru Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Recently, it has been demonstrated that the performance of a deep\nconvolutional neural network can be effectively improved by embedding an\nattention module into it. In this work, a novel lightweight and effective\nattention method named Pyramid Squeeze Attention (PSA) module is proposed. By\nreplacing the 3x3 convolution with the PSA module in the bottleneck blocks of\nthe ResNet, a novel representational block named Efficient Pyramid Squeeze\nAttention (EPSA) is obtained. The EPSA block can be easily added as a\nplug-and-play component into a well-established backbone network, and\nsignificant improvements on model performance can be achieved. Hence, a simple\nand efficient backbone architecture named EPSANet is developed in this work by\nstacking these ResNet-style EPSA blocks. Correspondingly, a stronger\nmulti-scale representation ability can be offered by the proposed EPSANet for\nvarious computer vision tasks including but not limited to, image\nclassification, object detection, instance segmentation, etc. Without bells and\nwhistles, the performance of the proposed EPSANet outperforms most of the\nstate-of-the-art channel attention methods. As compared to the SENet-50, the\nTop-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of\n+2.7 box AP for object detection and an improvement of +1.7 mask AP for\ninstance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained.\nOur source code is available at:https://github.com/murufeng/EPSANet.",
          "link": "http://arxiv.org/abs/2105.14447",
          "publishedOn": "2021-07-23T02:00:31.756Z",
          "wordCount": 689,
          "title": "EPSANet: An Efficient Pyramid Squeeze Attention Block on Convolutional Neural Network. (arXiv:2105.14447v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jin-Man Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jae-Hyuk Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sahng-Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sun-Kyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_U/0/1/0/all/0/1\">Ue-Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Hwan Kim</a>",
          "description": "We present a challenging dataset, ChangeSim, aimed at online scene change\ndetection (SCD) and more. The data is collected in photo-realistic simulation\nenvironments with the presence of environmental non-targeted variations, such\nas air turbidity and light condition changes, as well as targeted object\nchanges in industrial indoor environments. By collecting data in simulations,\nmulti-modal sensor data and precise ground truth labels are obtainable such as\nthe RGB image, depth image, semantic segmentation, change segmentation, camera\nposes, and 3D reconstructions. While the previous online SCD datasets evaluate\nmodels given well-aligned image pairs, ChangeSim also provides raw unpaired\nsequences that present an opportunity to develop an online SCD model in an\nend-to-end manner, considering both pairing and detection. Experiments show\nthat even the latest pair-based SCD models suffer from the bottleneck of the\npairing process, and it gets worse when the environment contains the\nnon-targeted variations. Our dataset is available at\nthis http URL",
          "link": "http://arxiv.org/abs/2103.05368",
          "publishedOn": "2021-07-23T02:00:31.718Z",
          "wordCount": 629,
          "title": "ChangeSim: Towards End-to-End Online Scene Change Detection in Industrial Indoor Environments. (arXiv:2103.05368v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zappel_M/0/1/0/all/0/1\">Moritz Zappel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bultmann_S/0/1/0/all/0/1\">Simon Bultmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "The task of 6D object pose estimation from RGB images is an important\nrequirement for autonomous service robots to be able to interact with the real\nworld. In this work, we present a two-step pipeline for estimating the 6 DoF\ntranslation and orientation of known objects. Keypoints and Part Affinity\nFields (PAFs) are predicted from the input image adopting the OpenPose CNN\narchitecture from human pose estimation. Object poses are then calculated from\n2D-3D correspondences between detected and model keypoints via the PnP-RANSAC\nalgorithm. The proposed approach is evaluated on the YCB-Video dataset and\nachieves accuracy on par with recent methods from the literature. Using PAFs to\nassemble detected keypoints into object instances proves advantageous over only\nusing heatmaps. Models trained to predict keypoints of a single object class\nperform significantly better than models trained for several classes.",
          "link": "http://arxiv.org/abs/2107.02057",
          "publishedOn": "2021-07-23T02:00:31.711Z",
          "wordCount": 620,
          "title": "6D Object Pose Estimation using Keypoints and Part Affinity Fields. (arXiv:2107.02057v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03337",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1\">Helmut Harbrecht</a>, <a href=\"http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1\">Michael Multerer</a>",
          "description": "In this article, we introduce the concept of samplets by transferring the\nconstruction of Tausch-White wavelets to the realm of data. This way we obtain\na multilevel representation of discrete data which directly enables data\ncompression, detection of singularities and adaptivity. Applying samplets to\nrepresent kernel matrices, as they arise in kernel based learning or Gaussian\nprocess regression, we end up with quasi-sparse matrices. By thresholding small\nentries, these matrices are compressible to O(N log N) relevant entries, where\nN is the number of data points. This feature allows for the use of fill-in\nreducing reorderings to obtain a sparse factorization of the compressed\nmatrices. Besides the comprehensive introduction to samplets and their\nproperties, we present extensive numerical studies to benchmark the approach.\nOur results demonstrate that samplets mark a considerable step in the direction\nof making large data sets accessible for analysis.",
          "link": "http://arxiv.org/abs/2107.03337",
          "publishedOn": "2021-07-23T02:00:31.698Z",
          "wordCount": 597,
          "title": "Samplets: A new paradigm for data compression. (arXiv:2107.03337v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shibata_T/0/1/0/all/0/1\">Takashi Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1\">Masayuki Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okutomi_M/0/1/0/all/0/1\">Masatoshi Okutomi</a>",
          "description": "Deep convolutional networks have become the mainstream in computer vision\napplications. Although CNNs have been successful in many computer vision tasks,\nit is not free from drawbacks. The performance of CNN is dramatically degraded\nby geometric transformation, such as large rotations. In this paper, we propose\na novel CNN architecture that can improve the robustness against geometric\ntransformations without modifying the existing backbones of their CNNs. The key\nis to enclose the existing backbone with a geometric transformation (and the\ncorresponding reverse transformation) and a feature map ensemble. The proposed\nmethod can inherit the strengths of existing CNNs that have been presented so\nfar. Furthermore, the proposed method can be employed in combination with\nstate-of-the-art data augmentation algorithms to improve their performance. We\ndemonstrate the effectiveness of the proposed method using standard datasets\nsuch as CIFAR, CUB-200, and Mnist-rot-12k.",
          "link": "http://arxiv.org/abs/2107.10524",
          "publishedOn": "2021-07-23T02:00:31.692Z",
          "wordCount": 577,
          "title": "Geometric Data Augmentation Based on Feature Map Ensemble. (arXiv:2107.10524v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1805.05510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Metric learning especially deep metric learning has been widely developed for\nlarge-scale image inputs data. However, in many real-world applications, we can\nonly have access to vectorized inputs data. Moreover, on one hand, well-labeled\ndata is usually limited due to the high annotation cost. On the other hand, the\nreal data is commonly streaming data, which requires to be processed online. In\nthese scenarios, the fashionable deep metric learning is not suitable anymore.\nTo this end, we reconsider the traditional shallow online metric learning and\nnewly develop an online progressive deep metric learning (ODML) framework to\nconstruct a metric-algorithm-based deep network. Specifically, we take an\nonline metric learning algorithm as a metric-algorithm-based layer (i.e.,\nmetric layer), followed by a nonlinear layer, and then stack these layers in a\nfashion similar to deep learning. Different from the shallow online metric\nlearning, which can only learn one metric space (feature transformation), the\nproposed ODML is able to learn multiple hierarchical metric spaces.\nFurthermore, in a progressively and nonlinearly learning way, ODML has a\nstronger learning ability than traditional shallow online metric learning in\nthe case of limited available training data. To make the learning process more\nexplainable and theoretically guaranteed, we also provide theoretical analysis.\nThe proposed ODML enjoys several nice properties and can indeed learn a metric\nprogressively and performs better on the benchmark datasets. Extensive\nexperiments with different settings have been conducted to verify these\nproperties of the proposed ODML.",
          "link": "http://arxiv.org/abs/1805.05510",
          "publishedOn": "2021-07-23T02:00:31.590Z",
          "wordCount": 713,
          "title": "Online Progressive Deep Metric Learning. (arXiv:1805.05510v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duzceker_A/0/1/0/all/0/1\">Arda D&#xfc;z&#xe7;eker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galliani_S/0/1/0/all/0/1\">Silvano Galliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1\">Christoph Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speciale_P/0/1/0/all/0/1\">Pablo Speciale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1\">Mihai Dusmanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>",
          "description": "We propose an online multi-view depth prediction approach on posed video\nstreams, where the scene geometry information computed in the previous time\nsteps is propagated to the current time step in an efficient and geometrically\nplausible way. The backbone of our approach is a real-time capable, lightweight\nencoder-decoder that relies on cost volumes computed from pairs of images. We\nextend it by placing a ConvLSTM cell at the bottleneck layer, which compresses\nan arbitrary amount of past information in its states. The novelty lies in\npropagating the hidden state of the cell by accounting for the viewpoint\nchanges between time steps. At a given time step, we warp the previous hidden\nstate into the current camera plane using the previous depth prediction. Our\nextension brings only a small overhead of computation time and memory\nconsumption, while improving the depth predictions significantly. As a result,\nwe outperform the existing state-of-the-art multi-view stereo methods on most\nof the evaluated metrics in hundreds of indoor scenes while maintaining a\nreal-time performance. Code available:\nhttps://github.com/ardaduz/deep-video-mvs",
          "link": "http://arxiv.org/abs/2012.02177",
          "publishedOn": "2021-07-23T02:00:31.563Z",
          "wordCount": 660,
          "title": "DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion. (arXiv:2012.02177v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03592",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1\">Sefik Emre Eskimez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "Visual emotion expression plays an important role in audiovisual speech\ncommunication. In this work, we propose a novel approach to rendering visual\nemotion expression in speech-driven talking face generation. Specifically, we\ndesign an end-to-end talking face generation system that takes a speech\nutterance, a single face image, and a categorical emotion label as input to\nrender a talking face video synchronized with the speech and expressing the\nconditioned emotion. Objective evaluation on image quality, audiovisual\nsynchronization, and visual emotion expression shows that the proposed system\noutperforms a state-of-the-art baseline system. Subjective evaluation of visual\nemotion expression and video realness also demonstrates the superiority of the\nproposed system. Furthermore, we conduct a human emotion recognition pilot\nstudy using generated videos with mismatched emotions among the audio and\nvisual modalities. Results show that humans respond to the visual modality more\nsignificantly than the audio modality on this task.",
          "link": "http://arxiv.org/abs/2008.03592",
          "publishedOn": "2021-07-23T02:00:31.547Z",
          "wordCount": 632,
          "title": "Speech Driven Talking Face Generation from a Single Image and an Emotion Condition. (arXiv:2008.03592v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyemin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daijin Kim</a>",
          "description": "We propose Uncertainty Augmented Context Attention network (UACANet) for\npolyp segmentation which consider a uncertain area of the saliency map. We\nconstruct a modified version of U-Net shape network with additional encoder and\ndecoder and compute a saliency map in each bottom-up stream prediction module\nand propagate to the next prediction module. In each prediction module,\npreviously predicted saliency map is utilized to compute foreground, background\nand uncertain area map and we aggregate the feature map with three area maps\nfor each representation. Then we compute the relation between each\nrepresentation and each pixel in the feature map. We conduct experiments on\nfive popular polyp segmentation benchmarks, Kvasir, CVC-ClinicDB, ETIS,\nCVC-ColonDB and CVC-300, and achieve state-of-the-art performance. Especially,\nwe achieve 76.6% mean Dice on ETIS dataset which is 13.8% improvement compared\nto the previous state-of-the-art method. Source code is publicly available at\nhttps://github.com/plemeri/UACANet",
          "link": "http://arxiv.org/abs/2107.02368",
          "publishedOn": "2021-07-23T02:00:31.056Z",
          "wordCount": 640,
          "title": "UACANet: Uncertainty Augmented Context Attention for Polyp Segmentation. (arXiv:2107.02368v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Junha Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.",
          "link": "http://arxiv.org/abs/2107.03438",
          "publishedOn": "2021-07-23T02:00:30.963Z",
          "wordCount": 631,
          "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding. (arXiv:2107.03438v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.09597",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Geiger_E/0/1/0/all/0/1\">Eric Geiger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kogan_I/0/1/0/all/0/1\">Irina A. Kogan</a>",
          "description": "While the equality of differential signatures (Calabi et al, Int. J. Comput.\nVis. 26: 107-135, 1998) is known to be a necessary condition for congruence, it\nis not sufficient (Musso and Nicolodi, J. Math Imaging Vis. 35: 68-85, 2009).\nHickman (J. Math Imaging Vis. 43: 206-213, 2012, Theorem 2) claimed that for\nnon-degenerate planar curves, equality of Euclidean signatures implies\ncongruence. We prove that while Hickman's claim holds for simple, closed curves\nwith simple signatures, it fails for curves with non-simple signatures. In the\nlater case, we associate a directed graph with the signature and show how\nvarious paths along the graph give rise to a family of non-congruent,\nnon-degenerate curves with identical signatures. Using this additional\nstructure, we formulate congruence criteria for non-degenerate, closed, simple\ncurves and show how the paths reflect the global and local symmetries of the\ncorresponding curve.",
          "link": "http://arxiv.org/abs/1912.09597",
          "publishedOn": "2021-07-23T02:00:30.911Z",
          "wordCount": 699,
          "title": "Non-congruent non-degenerate curves with identical signatures. (arXiv:1912.09597v4 [math.DG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coleman_C/0/1/0/all/0/1\">Cody Coleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_E/0/1/0/all/0/1\">Edward Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Samuels_J/0/1/0/all/0/1\">Julian Katz-Samuels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culatana_S/0/1/0/all/0/1\">Sean Culatana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1\">Peter Bailis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1\">Robert Nowak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumbaly_R/0/1/0/all/0/1\">Roshan Sumbaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalniz_I/0/1/0/all/0/1\">I. Zeki Yalniz</a>",
          "description": "Many active learning and search approaches are intractable for large-scale\nindustrial settings with billions of unlabeled examples. Existing approaches\nsearch globally for the optimal examples to label, scaling linearly or even\nquadratically with the unlabeled data. In this paper, we improve the\ncomputational efficiency of active learning and search methods by restricting\nthe candidate pool for labeling to the nearest neighbors of the currently\nlabeled set instead of scanning over all of the unlabeled data. We evaluate\nseveral selection strategies in this setting on three large-scale computer\nvision datasets: ImageNet, OpenImages, and a de-identified and aggregated\ndataset of 10 billion images provided by a large internet company. Our approach\nachieved similar mean average precision and recall as the traditional global\napproach while reducing the computational cost of selection by up to three\norders of magnitude, thus enabling web-scale active learning.",
          "link": "http://arxiv.org/abs/2007.00077",
          "publishedOn": "2021-07-23T02:00:30.904Z",
          "wordCount": 638,
          "title": "Similarity Search for Efficient Active Learning and Search of Rare Concepts. (arXiv:2007.00077v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member/0/1/0/all/0/1\">Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>",
          "description": "Video moment retrieval targets at retrieving a moment in a video for a given\nlanguage query. The challenges of this task include 1) the requirement of\nlocalizing the relevant moment in an untrimmed video, and 2) bridging the\nsemantic gap between textual query and video contents. To tackle those\nproblems, early approaches adopt the sliding window or uniform sampling to\ncollect video clips first and then match each clip with the query. Obviously,\nthese strategies are time-consuming and often lead to unsatisfied accuracy in\nlocalization due to the unpredictable length of the golden moment. To avoid the\nlimitations, researchers recently attempt to directly predict the relevant\nmoment boundaries without the requirement to generate video clips first. One\nmainstream approach is to generate a multimodal feature vector for the target\nquery and video frames (e.g., concatenation) and then use a regression approach\nupon the multimodal feature vector for boundary detection. Although some\nprogress has been achieved by this approach, we argue that those methods have\nnot well captured the cross-modal interactions between the query and video\nframes.\n\nIn this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM)\nmodel which predicts the temporal boundaries based on an interaction modeling.\nIn addition, an attention module is introduced to assign higher weights to\nquery words with richer semantic cues, which are considered to be more\nimportant for finding relevant video contents. Another contribution is that we\npropose an additional predictor to utilize the internal frames in the model\ntraining to improve the localization accuracy. Extensive experiments on two\ndatasets TACoS and Charades-STA demonstrate the superiority of our method over\nseveral state-of-the-art methods. Ablation studies have been also conducted to\nexamine the effectiveness of different modules in our ACRM model.",
          "link": "http://arxiv.org/abs/2009.10434",
          "publishedOn": "2021-07-23T02:00:30.894Z",
          "wordCount": 772,
          "title": "Frame-wise Cross-modal Matching for Video Moment Retrieval. (arXiv:2009.10434v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shail Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghdadi_R/0/1/0/all/0/1\">Riyadh Baghdadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowatzki_T/0/1/0/all/0/1\">Tony Nowatzki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avancha_S/0/1/0/all/0/1\">Sasikanth Avancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Aviral Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxin Li</a>",
          "description": "Machine learning (ML) models are widely used in many important domains. For\nefficiently processing these computational- and memory-intensive applications,\ntensors of these over-parameterized models are compressed by leveraging\nsparsity, size reduction, and quantization of tensors. Unstructured sparsity\nand tensors with varying dimensions yield irregular computation, communication,\nand memory access patterns; processing them on hardware accelerators in a\nconventional manner does not inherently leverage acceleration opportunities.\nThis paper provides a comprehensive survey on the efficient execution of sparse\nand irregular tensor computations of ML models on hardware accelerators. In\nparticular, it discusses enhancement modules in the architecture design and the\nsoftware support; categorizes different hardware designs and acceleration\ntechniques and analyzes them in terms of hardware and execution costs; analyzes\nachievable accelerations for recent DNNs; highlights further opportunities in\nterms of hardware/software/model co-design optimizations (inter/intra-module).\nThe takeaways from this paper include: understanding the key challenges in\naccelerating sparse, irregular-shaped, and quantized tensors; understanding\nenhancements in accelerator systems for supporting their efficient\ncomputations; analyzing trade-offs in opting for a specific design choice for\nencoding, storing, extracting, communicating, computing, and load-balancing the\nnon-zeros; understanding how structured sparsity can improve storage efficiency\nand balance computations; understanding how to compile and map models with\nsparse tensors on the accelerators; understanding recent design trends for\nefficient accelerations and further opportunities.",
          "link": "http://arxiv.org/abs/2007.00864",
          "publishedOn": "2021-07-23T02:00:30.887Z",
          "wordCount": 725,
          "title": "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights. (arXiv:2007.00864v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Woochul Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyeon Kim</a>",
          "description": "Modern convolutional neural networks (CNNs) have massive identical\nconvolution blocks, and, hence, recursive sharing of parameters across these\nblocks has been proposed to reduce the amount of parameters. However, naive\nsharing of parameters poses many challenges such as limited representational\npower and the vanishing/exploding gradients problem of recursively shared\nparameters. In this paper, we present a recursive convolution block design and\ntraining method, in which a recursively shareable part, or a filter basis, is\nseparated and learned while effectively avoiding the vanishing/exploding\ngradients problem during training. We show that the unwieldy\nvanishing/exploding gradients problem can be controlled by enforcing the\nelements of the filter basis orthonormal, and empirically demonstrate that the\nproposed orthogonality regularization improves the flow of gradients during\ntraining. Experimental results on image classification and object detection\nshow that our approach, unlike previous parameter-sharing approaches, does not\ntrade performance to save parameters and consistently outperforms\noverparameterized counterpart networks. This superior performance demonstrates\nthat the proposed recursive convolution block design and the orthogonality\nregularization not only prevent performance degradation, but also consistently\nimprove the representation capability while a significant amount of parameters\nare recursively shared.",
          "link": "http://arxiv.org/abs/2006.05066",
          "publishedOn": "2021-07-23T02:00:30.825Z",
          "wordCount": 663,
          "title": "Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks. (arXiv:2006.05066v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Runwei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>",
          "description": "Despite great progress in 3D human pose estimation from videos, it is still\nan open problem to take full advantage of redundant 2D pose sequences to learn\nrepresentative representation for generating one single 3D pose. To this end,\nwe propose an improved Transformer-based architecture, called Strided\nTransformer, for 3D human pose estimation in videos to lift a sequence of 2D\njoint locations to a 3D pose. Specifically, a vanilla Transformer encoder (VTE)\nis adopted to model long-range dependencies of 2D pose sequences. To reduce\nredundancy of the sequence and aggregate information from local context,\nstrided convolutions are incorporated into VTE to progressively reduce the\nsequence length. The modified VTE is termed as strided Transformer encoder\n(STE) which is built upon the outputs of VTE. STE not only effectively\naggregates long-range information to a single-vector representation in a\nhierarchical global and local fashion but also significantly reduces the\ncomputation cost. Furthermore, a full-to-single supervision scheme is designed\nat both the full sequence scale and single target frame scale, applied to the\noutputs of VTE and STE, respectively. This scheme imposes extra temporal\nsmoothness constraints in conjunction with the single target frame supervision\nand improves the representation ability of features for the target frame. The\nproposed architecture is evaluated on two challenging benchmark datasets,\nHuman3.6M and HumanEva-I, and achieves state-of-the-art results with much fewer\nparameters.",
          "link": "http://arxiv.org/abs/2103.14304",
          "publishedOn": "2021-07-23T02:00:30.818Z",
          "wordCount": 743,
          "title": "Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation. (arXiv:2103.14304v7 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1\">Wangbin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liqin Huang</a>",
          "description": "Multi-modality medical images can provide relevant or complementary\ninformation for a target (organ, tumor or tissue). Registering multi-modality\nimages to a common space can fuse these comprehensive information, and bring\nconvenience for clinical application. Recently, neural networks have been\nwidely investigated to boost registration methods. However, it is still\nchallenging to develop a multi-modality registration network due to the lack of\nrobust criteria for network training. In this work, we propose a multi-modality\nregistration network (MMRegNet), which can perform registration between\nmulti-modality images. Meanwhile, we present spatially encoded gradient\ninformation to train MMRegNet in an unsupervised manner. The proposed network\nwas evaluated on MM-WHS 2017. Results show that MMRegNet can achieve promising\nperformance for left ventricle cardiac registration tasks. Meanwhile, to\ndemonstrate the versatility of MMRegNet, we further evaluate the method with a\nliver dataset from CHAOS 2019. Source code will be released\npublicly\\footnote{https://github.com/NanYoMy/mmregnet} once the manuscript is\naccepted.",
          "link": "http://arxiv.org/abs/2105.07392",
          "publishedOn": "2021-07-23T02:00:30.811Z",
          "wordCount": 627,
          "title": "Unsupervised Registration Method based on Deep Neural Network: Application to cardiac and liver MR images. (arXiv:2105.07392v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05145",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1\">Rohit Jena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singla_S/0/1/0/all/0/1\">Sumedha Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Vessel segmentation is an essential task in many clinical applications.\nAlthough supervised methods have achieved state-of-art performance, acquiring\nexpert annotation is laborious and mostly limited for two-dimensional datasets\nwith a small sample size. On the contrary, unsupervised methods rely on\nhandcrafted features to detect tube-like structures such as vessels. However,\nthose methods require complex pipelines involving several hyper-parameters and\ndesign choices rendering the procedure sensitive, dataset-specific, and not\ngeneralizable. We propose a self-supervised method with a limited number of\nhyper-parameters that is generalizable across modalities. Our method uses\ntube-like structure properties, such as connectivity, profile consistency, and\nbifurcation, to introduce inductive bias into a learning algorithm. To model\nthose properties, we generate a vector field that we refer to as a flow. Our\nexperiments on various public datasets in 2D and 3D show that our method\nperforms better than unsupervised methods while learning useful transferable\nfeatures from unlabeled data. Unlike generic self-supervised methods, the\nlearned features learn vessel-relevant features that are transferable for\nsupervised approaches, which is essential when the number of annotated data is\nlimited.",
          "link": "http://arxiv.org/abs/2101.05145",
          "publishedOn": "2021-07-23T02:00:30.804Z",
          "wordCount": 651,
          "title": "Self-Supervised Vessel Enhancement Using Flow-Based Consistencies. (arXiv:2101.05145v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oza_M/0/1/0/all/0/1\">Manan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>",
          "description": "Faces generated using generative adversarial networks (GANs) have reached\nunprecedented realism. These faces, also known as \"Deep Fakes\", appear as\nrealistic photographs with very little pixel-level distortions. While some work\nhas enabled the training of models that lead to the generation of specific\nproperties of the subject, generating a facial image based on a natural\nlanguage description has not been fully explored. For security and criminal\nidentification, the ability to provide a GAN-based system that works like a\nsketch artist would be incredibly useful. In this paper, we present a novel\napproach to generate facial images from semantic text descriptions. The learned\nmodel is provided with a text description and an outline of the type of face,\nwhich the model uses to sketch the features. Our models are trained using an\nAffine Combination Module (ACM) mechanism to combine the text embedding from\nBERT and the GAN latent space using a self-attention matrix. This avoids the\nloss of features due to inadequate \"attention\", which may happen if text\nembedding and latent vector are simply concatenated. Our approach is capable of\ngenerating images that are very accurately aligned to the exhaustive textual\ndescriptions of faces with many fine detail features of the face and helps in\ngenerating better images. The proposed method is also capable of making\nincremental changes to a previously generated image if it is provided with\nadditional textual descriptions or sentences.",
          "link": "http://arxiv.org/abs/2107.10756",
          "publishedOn": "2021-07-23T02:00:30.798Z",
          "wordCount": 672,
          "title": "Semantic Text-to-Face GAN -ST^2FG. (arXiv:2107.10756v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.10679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_M/0/1/0/all/0/1\">Manish Bhattarai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_Curtis_A/0/1/0/all/0/1\">Aura Rose Jensen-Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MartiNez_Ramon_M/0/1/0/all/0/1\">Manel Mart&#xed;Nez-Ram&#xf3;n</a>",
          "description": "Firefighting is a dynamic activity, in which numerous operations occur\nsimultaneously. Maintaining situational awareness (i.e., knowledge of current\nconditions and activities at the scene) is critical to the accurate\ndecision-making necessary for the safe and successful navigation of a fire\nenvironment by firefighters. Conversely, the disorientation caused by hazards\nsuch as smoke and extreme heat can lead to injury or even fatality. This\nresearch implements recent advancements in technology such as deep learning,\npoint cloud and thermal imaging, and augmented reality platforms to improve a\nfirefighter's situational awareness and scene navigation through improved\ninterpretation of that scene. We have designed and built a prototype embedded\nsystem that can leverage data streamed from cameras built into a firefighter's\npersonal protective equipment (PPE) to capture thermal, RGB color, and depth\nimagery and then deploy already developed deep learning models to analyze the\ninput data in real time. The embedded system analyzes and returns the processed\nimages via wireless streaming, where they can be viewed remotely and relayed\nback to the firefighter using an augmented reality platform that visualizes the\nresults of the analyzed inputs and draws the firefighter's attention to objects\nof interest, such as doors and windows otherwise invisible through smoke and\nflames.",
          "link": "http://arxiv.org/abs/2009.10679",
          "publishedOn": "2021-07-23T02:00:30.781Z",
          "wordCount": 663,
          "title": "An embedded deep learning system for augmented reality in firefighting applications. (arXiv:2009.10679v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ko_G/0/1/0/all/0/1\">Gihyuk Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1\">Gyumin Lim</a>",
          "description": "Deep Neural Networks (DNNs) have shown remarkable performance in a diverse\nrange of machine learning applications. However, it is widely known that DNNs\nare vulnerable to simple adversarial perturbations, which causes the model to\nincorrectly classify inputs. In this paper, we propose a simple yet effective\nmethod to detect adversarial examples, using methods developed to explain the\nmodel's behavior. Our key observation is that adding small, humanly\nimperceptible perturbations can lead to drastic changes in the model\nexplanations, resulting in unusual or irregular forms of explanations. From\nthis insight, we propose an unsupervised detection of adversarial examples\nusing reconstructor networks trained only on model explanations of benign\nexamples. Our evaluations with MNIST handwritten dataset show that our method\nis capable of detecting adversarial examples generated by the state-of-the-art\nalgorithms with high confidence. To the best of our knowledge, this work is the\nfirst in suggesting unsupervised defense method using model explanations.",
          "link": "http://arxiv.org/abs/2107.10480",
          "publishedOn": "2021-07-23T02:00:30.774Z",
          "wordCount": 590,
          "title": "Unsupervised Detection of Adversarial Examples with Model Explanations. (arXiv:2107.10480v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chang-Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "Label smoothing is an effective regularization tool for deep neural networks\n(DNNs), which generates soft labels by applying a weighted average between the\nuniform distribution and the hard label. It is often used to reduce the\noverfitting problem of training DNNs and further improve classification\nperformance. In this paper, we aim to investigate how to generate more reliable\nsoft labels. We present an Online Label Smoothing (OLS) strategy, which\ngenerates soft labels based on the statistics of the model prediction for the\ntarget category. The proposed OLS constructs a more reasonable probability\ndistribution between the target categories and non-target categories to\nsupervise DNNs. Experiments demonstrate that based on the same classification\nmodels, the proposed approach can effectively improve the classification\nperformance on CIFAR-100, ImageNet, and fine-grained datasets. Additionally,\nthe proposed method can significantly improve the robustness of DNN models to\nnoisy labels compared to current label smoothing approaches.",
          "link": "http://arxiv.org/abs/2011.12562",
          "publishedOn": "2021-07-23T02:00:30.768Z",
          "wordCount": 638,
          "title": "Delving Deep into Label Smoothing. (arXiv:2011.12562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polat_S/0/1/0/all/0/1\">Songuel Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremeau_A/0/1/0/all/0/1\">Alain Tremeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boochs_F/0/1/0/all/0/1\">Frank Boochs</a>",
          "description": "Due to its high spatial and spectral information content, hyperspectral\nimaging opens up new possibilities for a better understanding of data and\nscenes in a wide variety of applications. An essential part of this process of\nunderstanding is the classification part. In this article we present a general\nclassification approach based on the shape of spectral signatures. In contrast\nto classical classification approaches (e.g. SVM, KNN), not only reflectance\nvalues are considered, but also parameters such as curvature points, curvature\nvalues, and the curvature behavior of spectral signatures are used to develop\nshape-describing rules in order to use them for classification by a rule-based\nprocedure using IF-THEN queries. The flexibility and efficiency of the\nmethodology is demonstrated using datasets from two different application\nfields and leads to convincing results with good performance.",
          "link": "http://arxiv.org/abs/2107.10638",
          "publishedOn": "2021-07-23T02:00:30.759Z",
          "wordCount": 561,
          "title": "Rule-Based Classification of Hyperspectral Imaging Data. (arXiv:2107.10638v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>",
          "description": "Re-identification(ReID) aims at matching objects in surveillance cameras with\ndifferent viewpoints. It's developing very fast, but there is no processing\nmethod for the ReID task in multiple scenarios at this stage. However, this\ndose happen all the time in real life, such as the security scenarios. This\npaper explores a new scenario of Re-identification, which differs in\nperspective, background, and pose(walking or cycling).\n\nObviously, ordinary ReID processing methods cannot handle this scenario well.\nAs we all konw, the best way to deal with that it is to introduce image\ndatasets in this scanario, But this one is very expensive. To solve this\nproblem, this paper proposes a simple and effective way to generate images in\nsome new scenario, which is named Copy and Paste method based on Pose(CPP). The\nCPP is a method based on key point detection, using copy and paste, to\ncomposite a new semantic image dataset in two different semantic image\ndatasets. Such as, we can use pedestrians and bicycles to generate some images\nthat shows the same person rides on different bicycles. The CPP is suitable for\nReID tasks in new scenarios and it outperforms state-of-the-art on the original\ndatasets in original ReID tasks. Specifically, it can also have better\ngeneralization performance for third-party public datasets. Code and Datasets\nwhich composited by the CPP will be available in the future.",
          "link": "http://arxiv.org/abs/2107.10479",
          "publishedOn": "2021-07-23T02:00:30.751Z",
          "wordCount": 660,
          "title": "Copy and Paste method based on Pose for ReID. (arXiv:2107.10479v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01422",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1\">Shiqi Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1\">Wenhui Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jonsson_J/0/1/0/all/0/1\">Joakim Jonsson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Qian_R/0/1/0/all/0/1\">Ruobing Qian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Konda_P/0/1/0/all/0/1\">Pavan Chandra Konda</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhou_K/0/1/0/all/0/1\">Kevin C. Zhou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Berrocal_E/0/1/0/all/0/1\">Edouard Berrocal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Horstmeyer_R/0/1/0/all/0/1\">Roarke Horstmeyer</a>",
          "description": "Noninvasive optical imaging through dynamic scattering media has numerous\nimportant biomedical applications but still remains a challenging task. While\nstandard methods aim to form images based upon optical absorption or\nfluorescent emission, it is also well-established that the temporal correlation\nof scattered coherent light diffuses through tissue much like optical\nintensity. Few works to date, however, have aimed to experimentally measure and\nprocess such data to demonstrate deep-tissue imaging of decorrelation dynamics.\nIn this work, we take advantage of a single-photon avalanche diode (SPAD) array\ncamera, with over one thousand detectors, to simultaneously detect speckle\nfluctuations at the single-photon level from 12 different phantom tissue\nsurface locations delivered via a customized fiber bundle array. We then apply\na deep neural network to convert the acquired single-photon measurements into\nvideo of scattering dynamics beneath rapidly decorrelating liquid tissue\nphantoms. We demonstrate the ability to record video of dynamic events\noccurring 5-8 mm beneath a decorrelating tissue phantom with mm-scale\nresolution and at a 2.5-10 Hz frame rate.",
          "link": "http://arxiv.org/abs/2107.01422",
          "publishedOn": "2021-07-23T02:00:30.733Z",
          "wordCount": 648,
          "title": "Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v2 [physics.optics] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>",
          "description": "ImageNet has been arguably the most popular image classification benchmark,\nbut it is also the one with a significant level of label noise. Recent studies\nhave shown that many samples contain multiple classes, despite being assumed to\nbe a single-label benchmark. They have thus proposed to turn ImageNet\nevaluation into a multi-label task, with exhaustive multi-label annotations per\nimage. However, they have not fixed the training set, presumably because of a\nformidable annotation cost. We argue that the mismatch between single-label\nannotations and effectively multi-label images is equally, if not more,\nproblematic in the training setup, where random crops are applied. With the\nsingle-label annotations, a random crop of an image may contain an entirely\ndifferent object from the ground truth, introducing noisy or even incorrect\nsupervision during training. We thus re-label the ImageNet training set with\nmulti-labels. We address the annotation cost barrier by letting a strong image\nclassifier, trained on an extra source of data, generate the multi-labels. We\nutilize the pixel-wise multi-label predictions before the final pooling layer,\nin order to exploit the additional location-specific supervision signals.\nTraining on the re-labeled samples results in improved model performances\nacross the board. ResNet-50 attains the top-1 classification accuracy of 78.9%\non ImageNet with our localized multi-labels, which can be further boosted to\n80.2% with the CutMix regularization. We show that the models trained with\nlocalized multi-labels also outperforms the baselines on transfer learning to\nobject detection and instance segmentation tasks, and various robustness\nbenchmarks. The re-labeled ImageNet training set, pre-trained weights, and the\nsource code are available at {https://github.com/naver-ai/relabel_imagenet}.",
          "link": "http://arxiv.org/abs/2101.05022",
          "publishedOn": "2021-07-23T02:00:30.727Z",
          "wordCount": 746,
          "title": "Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels. (arXiv:2101.05022v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07044",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Emami_H/0/1/0/all/0/1\">Hajar Emami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nejad_Davarani_S/0/1/0/all/0/1\">Siamak Nejad-Davarani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glide_Hurst_C/0/1/0/all/0/1\">Carri Glide-Hurst</a>",
          "description": "In medical image synthesis, model training could be challenging due to the\ninconsistencies between images of different modalities even with the same\npatient, typically caused by internal status/tissue changes as different\nmodalities are usually obtained at a different time. This paper proposes a\nnovel deep learning method, Structure-aware Generative Adversarial Network\n(SA-GAN), that preserves the shapes and locations of in-consistent structures\nwhen generating medical images. SA-GAN is employed to generate synthetic\ncomputed tomography (synCT) images from magnetic resonance imaging (MRI) with\ntwo parallel streams: the global stream translates the input from the MRI to\nthe CT domain while the local stream automatically segments the inconsistent\norgans, maintains their locations and shapes in MRI, and translates the organ\nintensities to CT. Through extensive experiments on a pelvic dataset, we\ndemonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs\nand organ segmentation and supports MR-only treatment planning in disease sites\nwith internal organ status changes.",
          "link": "http://arxiv.org/abs/2105.07044",
          "publishedOn": "2021-07-23T02:00:30.719Z",
          "wordCount": 622,
          "title": "SA-GAN: Structure-Aware GAN for Organ-Preserving Synthetic CT Generation. (arXiv:2105.07044v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mohammad Mahmudul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mohammad Tariqul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">S. M. Mahbubur Rahman</a>",
          "description": "Head-mounted device-based human-computer interaction often requires\negocentric recognition of hand gestures and fingertips detection. In this\npaper, a unified approach of egocentric hand gesture recognition and fingertip\ndetection is introduced. The proposed algorithm uses a single convolutional\nneural network to predict the probabilities of finger class and positions of\nfingertips in one forward propagation. Instead of directly regressing the\npositions of fingertips from the fully connected layer, the ensemble of the\nposition of fingertips is regressed from the fully convolutional network.\nSubsequently, the ensemble average is taken to regress the final position of\nfingertips. Since the whole pipeline uses a single network, it is significantly\nfast in computation. Experimental results show that the proposed method\noutperforms the existing fingertip detection approaches including the Direct\nRegression and the Heatmap-based framework. The effectiveness of the proposed\nmethod is also shown in-the-wild scenario as well as in a use-case of virtual\nreality.",
          "link": "http://arxiv.org/abs/2101.02047",
          "publishedOn": "2021-07-23T02:00:30.712Z",
          "wordCount": 650,
          "title": "Unified Learning Approach for Egocentric Hand Gesture Recognition and Fingertip Detection. (arXiv:2101.02047v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quellec_G/0/1/0/all/0/1\">Gwenol&#xe9; Quellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajj_H/0/1/0/all/0/1\">Hassan Al Hajj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamard_M/0/1/0/all/0/1\">Mathieu Lamard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conze_P/0/1/0/all/0/1\">Pierre-Henri Conze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massin_P/0/1/0/all/0/1\">Pascale Massin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochener_B/0/1/0/all/0/1\">B&#xe9;atrice Cochener</a>",
          "description": "In recent years, Artificial Intelligence (AI) has proven its relevance for\nmedical decision support. However, the \"black-box\" nature of successful AI\nalgorithms still holds back their wide-spread deployment. In this paper, we\ndescribe an eXplanatory Artificial Intelligence (XAI) that reaches the same\nlevel of performance as black-box AI, for the task of classifying Diabetic\nRetinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm,\ncalled ExplAIn, learns to segment and categorize lesions in images; the final\nimage-level classification directly derives from these multivariate lesion\nsegmentations. The novelty of this explanatory framework is that it is trained\nfrom end to end, with image supervision only, just like black-box AI\nalgorithms: the concepts of lesions and lesion categories emerge by themselves.\nFor improved lesion localization, foreground/background separation is trained\nthrough self-supervision, in such a way that occluding foreground pixels\ntransforms the input image into a healthy-looking image. The advantage of such\nan architecture is that automatic diagnoses can be explained simply by an image\nand/or a few sentences. ExplAIn is evaluated at the image level and at the\npixel level on various CFP image datasets. We expect this new framework, which\njointly offers high classification performance and explainability, to\nfacilitate AI deployment.",
          "link": "http://arxiv.org/abs/2008.05731",
          "publishedOn": "2021-07-23T02:00:30.694Z",
          "wordCount": 693,
          "title": "ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy Diagnosis. (arXiv:2008.05731v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10476",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqing Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_M/0/1/0/all/0/1\">Meng Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Wenjia Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_W/0/1/0/all/0/1\">Weijing Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Optical Coherence Tomography Angiography (OCTA) is a non-invasive and\nnon-contacting imaging technique providing visualization of microvasculature of\nretina and optic nerve head in human eyes in vivo. The adequate image quality\nof OCTA is the prerequisite for the subsequent quantification of retinal\nmicrovasculature. Traditionally, the image quality score based on signal\nstrength is used for discriminating low quality. However, it is insufficient\nfor identifying artefacts such as motion and off-centration, which rely\nspecialized knowledge and need tedious and time-consuming manual\nidentification. One of the most primary issues in OCTA analysis is to sort out\nthe foveal avascular zone (FAZ) region in the retina, which highly correlates\nwith any visual acuity disease. However, the variations in OCTA visual quality\naffect the performance of deep learning in any downstream marginally. Moreover,\nfiltering the low-quality OCTA images out is both labor-intensive and\ntime-consuming. To address these issues, we develop an automated computer-aided\nOCTA image processing system using deep neural networks as the classifier and\nsegmentor to help ophthalmologists in clinical diagnosis and research. This\nsystem can be an assistive tool as it can process OCTA images of different\nformats to assess the quality and segment the FAZ area. The source code is\nfreely available at https://github.com/shanzha09/COIPS.git.\n\nAnother major contribution is the large-scale OCTA dataset, namely\nOCTA-25K-IQA-SEG we publicize for performance evaluation. It is comprised of\nfour subsets, namely sOCTA-3$\\times$3-10k, sOCTA-6$\\times$6-14k,\nsOCTA-3$\\times$3-1.1k-seg, and dOCTA-6$\\times$6-1.1k-seg, which contains a\ntotal number of 25,665 images. The large-scale OCTA dataset is available at\nhttps://doi.org/10.5281/zenodo.5111975, https://doi.org/10.5281/zenodo.5111972.",
          "link": "http://arxiv.org/abs/2107.10476",
          "publishedOn": "2021-07-23T02:00:30.569Z",
          "wordCount": 729,
          "title": "A Deep Learning-based Quality Assessment and Segmentation System with a Large-scale Benchmark Dataset for Optical Coherence Tomographic Angiography Image. (arXiv:2107.10476v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zhendong Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>",
          "description": "Crowdsourcing provides an efficient label collection schema for supervised\nmachine learning. However, to control annotation cost, each instance in the\ncrowdsourced data is typically annotated by a small number of annotators. This\ncreates a sparsity issue and limits the quality of machine learning models\ntrained on such data. In this paper, we study how to handle sparsity in\ncrowdsourced data using data augmentation. Specifically, we propose to directly\nlearn a classifier by augmenting the raw sparse annotations. We implement two\nprinciples of high-quality augmentation using Generative Adversarial Networks:\n1) the generated annotations should follow the distribution of authentic ones,\nwhich is measured by a discriminator; 2) the generated annotations should have\nhigh mutual information with the ground-truth labels, which is measured by an\nauxiliary network. Extensive experiments and comparisons against an array of\nstate-of-the-art learning from crowds methods on three real-world datasets\nproved the effectiveness of our data augmentation framework. It shows the\npotential of our algorithm for low-budget crowdsourcing in general.",
          "link": "http://arxiv.org/abs/2107.10449",
          "publishedOn": "2021-07-23T02:00:30.540Z",
          "wordCount": 601,
          "title": "Improve Learning from Crowds via Generative Augmentation. (arXiv:2107.10449v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thoma_F/0/1/0/all/0/1\">Felix Thoma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayer_J/0/1/0/all/0/1\">Johannes Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yakun Li</a>",
          "description": "The development of digitization methods for line drawings (especially in the\narea of electrical engineering) relies on the availability of publicly\navailable training and evaluation data. This paper presents such an image set\nalong with annotations. The dataset consists of 1152 images of 144 circuits by\n12 drafters and 48 563 annotations. Each of these images depicts an electrical\ncircuit diagram, taken by consumer grade cameras under varying lighting\nconditions and perspectives. A variety of different pencil types and surface\nmaterials has been used. For each image, all individual electrical components\nare annotated with bounding boxes and one out of 45 class labels. In order to\nsimplify a graph extraction process, different helper symbols like junction\npoints and crossovers are introduced, while texts are annotated as well. The\ngeometric and taxonomic problems arising from this task as well as the classes\nthemselves and statistics of their appearances are stated. The performance of a\nstandard Faster RCNN on the dataset is provided as an object detection\nbaseline.",
          "link": "http://arxiv.org/abs/2107.10373",
          "publishedOn": "2021-07-23T02:00:30.533Z",
          "wordCount": 613,
          "title": "A Public Ground-Truth Dataset for Handwritten Circuit Diagram Images. (arXiv:2107.10373v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "This paper presents a simple and effective approach to solving the\nmulti-label classification problem. The proposed approach leverages Transformer\ndecoders to query the existence of a class label. The use of Transformer is\nrooted in the need of extracting local discriminative features adaptively for\ndifferent labels, which is a strongly desired property due to the existence of\nmultiple objects in one image. The built-in cross-attention module in the\nTransformer decoder offers an effective way to use label embeddings as queries\nto probe and pool class-related features from a feature map computed by a\nvision backbone for subsequent binary classifications. Compared with prior\nworks, the new framework is simple, using standard Transformers and vision\nbackbones, and effective, consistently outperforming all previous works on five\nmulti-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE,\nand Visual Genome. Particularly, we establish $91.3\\%$ mAP on MS-COCO. We hope\nits compact structure, simple implementation, and superior performance serve as\na strong baseline for multi-label classification tasks and future studies. The\ncode will be available soon at https://github.com/SlongLiu/query2labels.",
          "link": "http://arxiv.org/abs/2107.10834",
          "publishedOn": "2021-07-23T02:00:30.527Z",
          "wordCount": 612,
          "title": "Query2Label: A Simple Transformer Way to Multi-Label Classification. (arXiv:2107.10834v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>",
          "description": "Efficiently modeling spatial-temporal information in videos is crucial for\naction recognition. To achieve this goal, state-of-the-art methods typically\nemploy the convolution operator and the dense interaction modules such as\nnon-local blocks. However, these methods cannot accurately fit the diverse\nevents in videos. On the one hand, the adopted convolutions are with fixed\nscales, thus struggling with events of various scales. On the other hand, the\ndense interaction modeling paradigm only achieves sub-optimal performance as\naction-irrelevant parts bring additional noises for the final prediction. In\nthis paper, we propose a unified action recognition framework to investigate\nthe dynamic nature of video content by introducing the following designs.\nFirst, when extracting local cues, we generate the spatial-temporal kernels of\ndynamic-scale to adaptively fit the diverse events. Second, to accurately\naggregate these cues into a global video representation, we propose to mine the\ninteractions only among a few selected foreground objects by a Transformer,\nwhich yields a sparse paradigm. We call the proposed framework as Event\nAdaptive Network (EAN) because both key designs are adaptive to the input video\ncontent. To exploit the short-term motions within local segments, we propose a\nnovel and efficient Latent Motion Code (LMC) module, further improving the\nperformance of the framework. Extensive experiments on several large-scale\nvideo datasets, e.g., Something-to-Something V1&V2, Kinetics, and Diving48,\nverify that our models achieve state-of-the-art or competitive performances at\nlow FLOPs. Codes are available at:\nhttps://github.com/tianyuan168326/EAN-Pytorch.",
          "link": "http://arxiv.org/abs/2107.10771",
          "publishedOn": "2021-07-23T02:00:30.515Z",
          "wordCount": 688,
          "title": "EAN: Event Adaptive Network for Enhanced Action Recognition. (arXiv:2107.10771v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Liangbin Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>",
          "description": "Though many attempts have been made in blind super-resolution to restore\nlow-resolution images with unknown and complex degradations, they are still far\nfrom addressing general real-world degraded images. In this work, we extend the\npowerful ESRGAN to a practical restoration application (namely, Real-ESRGAN),\nwhich is trained with pure synthetic data. Specifically, a high-order\ndegradation modeling process is introduced to better simulate complex\nreal-world degradations. We also consider the common ringing and overshoot\nartifacts in the synthesis process. In addition, we employ a U-Net\ndiscriminator with spectral normalization to increase discriminator capability\nand stabilize the training dynamics. Extensive comparisons have shown its\nsuperior visual performance than prior works on various real datasets. We also\nprovide efficient implementations to synthesize training pairs on the fly.",
          "link": "http://arxiv.org/abs/2107.10833",
          "publishedOn": "2021-07-23T02:00:30.495Z",
          "wordCount": 585,
          "title": "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data. (arXiv:2107.10833v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1\">Ben Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camgoz_N/0/1/0/all/0/1\">Necati Cihan Camgoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>",
          "description": "The visual anonymisation of sign language data is an essential task to\naddress privacy concerns raised by large-scale dataset collection. Previous\nanonymisation techniques have either significantly affected sign comprehension\nor required manual, labour-intensive work.\n\nIn this paper, we formally introduce the task of Sign Language Video\nAnonymisation (SLVA) as an automatic method to anonymise the visual appearance\nof a sign language video whilst retaining the meaning of the original sign\nlanguage sequence. To tackle SLVA, we propose AnonySign, a novel automatic\napproach for visual anonymisation of sign language data. We first extract pose\ninformation from the source video to remove the original signer appearance. We\nnext generate a photo-realistic sign language video of a novel appearance from\nthe pose sequence, using image-to-image translation methods in a conditional\nvariational autoencoder framework. An approximate posterior style distribution\nis learnt, which can be sampled from to synthesise novel human appearances. In\naddition, we propose a novel \\textit{style loss} that ensures style consistency\nin the anonymised sign language videos.\n\nWe evaluate AnonySign for the SLVA task with extensive quantitative and\nqualitative experiments highlighting both realism and anonymity of our novel\nhuman appearance synthesis. In addition, we formalise an anonymity perceptual\nstudy as an evaluation criteria for the SLVA task and showcase that video\nanonymisation using AnonySign retains the original sign language content.",
          "link": "http://arxiv.org/abs/2107.10685",
          "publishedOn": "2021-07-23T02:00:30.488Z",
          "wordCount": 664,
          "title": "AnonySIGN: Novel Human Appearance Synthesis for Sign Language Video Anonymisation. (arXiv:2107.10685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakab_T/0/1/0/all/0/1\">Tomas Jakab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1\">Christian Rupprecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>",
          "description": "Learning deformable 3D objects from 2D images is an extremely ill-posed\nproblem. Existing methods rely on explicit supervision to establish multi-view\ncorrespondences, such as template shape models and keypoint annotations, which\nrestricts their applicability on objects \"in the wild\". In this paper, we\npropose to use monocular videos, which naturally provide correspondences across\ntime, allowing us to learn 3D shapes of deformable object categories without\nexplicit keypoints or template shapes. Specifically, we present DOVE, which\nlearns to predict 3D canonical shape, deformation, viewpoint and texture from a\nsingle 2D image of a bird, given a bird video collection as well as\nautomatically obtained silhouettes and optical flows as training data. Our\nmethod reconstructs temporally consistent 3D shape and deformation, which\nallows us to animate and re-render the bird from arbitrary viewpoints from a\nsingle image.",
          "link": "http://arxiv.org/abs/2107.10844",
          "publishedOn": "2021-07-23T02:00:30.480Z",
          "wordCount": 576,
          "title": "DOVE: Learning Deformable 3D Objects by Watching Videos. (arXiv:2107.10844v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10806",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_Quilez_A/0/1/0/all/0/1\">Alvaro Fernandez-Quilez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eftestol_T/0/1/0/all/0/1\">Trygve Eftest&#xf8;l</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goodwin_M/0/1/0/all/0/1\">Morten Goodwin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kjosavik_S/0/1/0/all/0/1\">Svein Reidar Kjosavik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oppedal_K/0/1/0/all/0/1\">Ketil Oppedal</a>",
          "description": "Prostate cancer (PCa) is the second most common cancer diagnosed among men\nworldwide. The current PCa diagnostic pathway comes at the cost of substantial\noverdiagnosis, leading to unnecessary treatment and further testing.\nBi-parametric magnetic resonance imaging (bp-MRI) based on apparent diffusion\ncoefficient maps (ADC) and T2-weighted (T2w) sequences has been proposed as a\ntriage test to differentiate between clinically significant (cS) and\nnon-clinically significant (ncS) prostate lesions. However, analysis of the\nsequences relies on expertise, requires specialized training, and suffers from\ninter-observer variability. Deep learning (DL) techniques hold promise in tasks\nsuch as classification and detection. Nevertheless, they rely on large amounts\nof annotated data which is not common in the medical field. In order to\npalliate such issues, existing works rely on transfer learning (TL) and\nImageNet pre-training, which has been proven to be sub-optimal for the medical\nimaging domain. In this paper, we present a patch-based pre-training strategy\nto distinguish between cS and ncS lesions which exploit the region of interest\n(ROI) of the patched source domain to efficiently train a classifier in the\nfull-slice target domain which does not require annotations by making use of\ntransfer learning (TL). We provide a comprehensive comparison between several\nCNNs architectures and different settings which are presented as a baseline.\nMoreover, we explore cross-domain TL which exploits both MRI modalities and\nimproves single modality results. Finally, we show how our approaches\noutperform the standard approaches by a considerable margin",
          "link": "http://arxiv.org/abs/2107.10806",
          "publishedOn": "2021-07-23T02:00:30.463Z",
          "wordCount": 701,
          "title": "Self-transfer learning via patches: A prostate cancer triage approach based on bi-parametric MRI. (arXiv:2107.10806v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10718",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaggin_H/0/1/0/all/0/1\">Hanna K. Gaggin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Weichung Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Assessment of cardiovascular disease (CVD) with cine magnetic resonance\nimaging (MRI) has been used to non-invasively evaluate detailed cardiac\nstructure and function. Accurate segmentation of cardiac structures from cine\nMRI is a crucial step for early diagnosis and prognosis of CVD, and has been\ngreatly improved with convolutional neural networks (CNN). There, however, are\na number of limitations identified in CNN models, such as limited\ninterpretability and high complexity, thus limiting their use in clinical\npractice. In this work, to address the limitations, we propose a lightweight\nand interpretable machine learning model, successive subspace learning with the\nsubspace approximation with adjusted bias (Saab) transform, for accurate and\nefficient segmentation from cine MRI. Specifically, our segmentation framework\nis comprised of the following steps: (1) sequential expansion of near-to-far\nneighborhood at different resolutions; (2) channel-wise subspace approximation\nusing the Saab transform for unsupervised dimension reduction; (3) class-wise\nentropy guided feature selection for supervised dimension reduction; (4)\nconcatenation of features and pixel-wise classification with gradient boost;\nand (5) conditional random field for post-processing. Experimental results on\nthe ACDC 2017 segmentation database, showed that our framework performed better\nthan state-of-the-art U-Net models with 200$\\times$ fewer parameters in\ndelineating the left ventricle, right ventricle, and myocardium, thus showing\nits potential to be used in clinical practice.",
          "link": "http://arxiv.org/abs/2107.10718",
          "publishedOn": "2021-07-23T02:00:30.453Z",
          "wordCount": 696,
          "title": "Segmentation of Cardiac Structures via Successive Subspace Learning with Saab Transform from Cine MRI. (arXiv:2107.10718v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chenyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Ran Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>",
          "description": "Current methods of multi-person pose estimation typically treat the\nlocalization and the association of body joints separately. It is convenient\nbut inefficient, leading to additional computation and a waste of time. This\npaper, however, presents a novel framework PoseDet (Estimating Pose by\nDetection) to localize and associate body joints simultaneously at higher\ninference speed. Moreover, we propose the keypoint-aware pose embedding to\nrepresent an object in terms of the locations of its keypoints. The proposed\npose embedding contains semantic and geometric information, allowing us to\naccess discriminative and informative features efficiently. It is utilized for\ncandidate classification and body joint localization in PoseDet, leading to\nrobust predictions of various poses. This simple framework achieves an\nunprecedented speed and a competitive accuracy on the COCO benchmark compared\nwith state-of-the-art methods. Extensive experiments on the CrowdPose benchmark\nshow the robustness in the crowd scenes. Source code is available.",
          "link": "http://arxiv.org/abs/2107.10466",
          "publishedOn": "2021-07-23T02:00:30.446Z",
          "wordCount": 590,
          "title": "PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding. (arXiv:2107.10466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10563",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Herfet_T/0/1/0/all/0/1\">Thorsten Herfet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chelli_K/0/1/0/all/0/1\">Kelvin Chelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1\">Tobias Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kremer_R/0/1/0/all/0/1\">Robin Kremer</a>",
          "description": "In recent years, light field (LF) capture and processing has become an\nintegral part of media production. The richness of information available in LFs\nhas enabled novel applications like post-capture depth-of-field editing, 3D\nreconstruction, segmentation and matting, saliency detection, object detection\nand recognition, and mixed reality. The efficacy of such applications depends\non certain underlying requirements, which are often ignored. For example, some\noperations such as noise-reduction, or hyperfan-filtering are only possible if\na scene point Lambertian radiator. Some other operations such as the removal of\nobstacles or looking behind objects are only possible if there is at least one\nray capturing the required scene point. Consequently, the ray distribution\nrepresenting a certain scene point is an important characteristic for\nevaluating processing possibilities. The primary idea in this paper is to\nestablish a relation between the capturing setup and the rays of the LF. To\nthis end, we discretize the view frustum. Traditionally, a uniform\ndiscretization of the view frustum results in voxels that represents a single\nsample on a regularly spaced, 3-D grid. Instead, we use frustum-shaped voxels\n(froxels), by using depth and capturing-setup dependent discretization of the\nview frustum. Based on such discretization, we count the number of rays mapping\nto the same pixel on the capturing device(s). By means of this count, we\npropose histograms of ray-counts over the froxels (fristograms). Fristograms\ncan be used as a tool to analyze and reveal interesting aspects of the\nunderlying LF, like the number of rays originating from a scene point and the\ncolor distribution of these rays. As an example, we show its ability by\nsignificantly reducing the number of rays which enables noise reduction while\nmaintaining the realistic rendering of non-Lambertian or partially occluded\nregions.",
          "link": "http://arxiv.org/abs/2107.10563",
          "publishedOn": "2021-07-23T02:00:30.438Z",
          "wordCount": 734,
          "title": "Fristograms: Revealing and Exploiting Light Field Internals. (arXiv:2107.10563v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke-Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shice Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bangjie Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>",
          "description": "In pursuit of consolidating the face verification systems, prior face\nanti-spoofing studies excavate the hidden cues in original images to\ndiscriminate real persons and diverse attack types with the assistance of\nauxiliary supervision. However, limited by the following two inherent\ndisturbances in their training process: 1) Complete facial structure in a\nsingle image. 2) Implicit subdomains in the whole dataset, these methods are\nprone to stick on memorization of the entire training dataset and show\nsensitivity to nonhomologous domain distribution. In this paper, we propose\nStructure Destruction Module and Content Combination Module to address these\ntwo imitations separately. The former mechanism destroys images into patches to\nconstruct a non-structural input, while the latter mechanism recombines patches\nfrom different subdomains or classes into a mixup construct. Based on this\nsplitting-and-splicing operation, Local Relation Modeling Module is further\nproposed to model the second-order relationship between patches. We evaluate\nour method on extensive public datasets and promising experimental results to\ndemonstrate the reliability of our method against state-of-the-art competitors.",
          "link": "http://arxiv.org/abs/2107.10628",
          "publishedOn": "2021-07-23T02:00:30.431Z",
          "wordCount": 612,
          "title": "Structure Destruction and Content Combination for Face Anti-Spoofing. (arXiv:2107.10628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibing_M/0/1/0/all/0/1\">Moritz Ibing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_I/0/1/0/all/0/1\">Isaak Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1\">Leif Kobbelt</a>",
          "description": "Previous approaches to generate shapes in a 3D setting train a GAN on the\nlatent space of an autoencoder (AE). Even though this produces convincing\nresults, it has two major shortcomings. As the GAN is limited to reproduce the\ndataset the AE was trained on, we cannot reuse a trained AE for novel data.\nFurthermore, it is difficult to add spatial supervision into the generation\nprocess, as the AE only gives us a global representation. To remedy these\nissues, we propose to train the GAN on grids (i.e. each cell covers a part of a\nshape). In this representation each cell is equipped with a latent vector\nprovided by an AE. This localized representation enables more expressiveness\n(since the cell-based latent vectors can be combined in novel ways) as well as\nspatial control of the generation process (e.g. via bounding boxes). Our method\noutperforms the current state of the art on all established evaluation\nmeasures, proposed for quantitatively evaluating the generative capabilities of\nGANs. We show limitations of these measures and propose the adaptation of a\nrobust criterion from statistical analysis as an alternative.",
          "link": "http://arxiv.org/abs/2107.10607",
          "publishedOn": "2021-07-23T02:00:30.424Z",
          "wordCount": 626,
          "title": "3D Shape Generation with Grid-based Implicit Functions. (arXiv:2107.10607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lizhong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>",
          "description": "The Self-Rating Depression Scale (SDS) questionnaire is commonly utilized for\neffective depression preliminary screening. The uncontrolled self-administered\nmeasure, on the other hand, maybe readily influenced by insouciant or dishonest\nresponses, yielding different findings from the clinician-administered\ndiagnostic. Facial expression (FE) and behaviors are important in\nclinician-administered assessments, but they are underappreciated in\nself-administered evaluations. We use a new dataset of 200 participants to\ndemonstrate the validity of self-rating questionnaires and their accompanying\nquestion-by-question video recordings in this study. We offer an end-to-end\nsystem to handle the face video recording that is conditioned on the\nquestionnaire answers and the responding time to automatically interpret\nsadness from the SDS assessment and the associated video. We modified a 3D-CNN\nfor temporal feature extraction and compared various state-of-the-art temporal\nmodeling techniques. The superior performance of our system shows the validity\nof combining facial video recording with the SDS score for more accurate\nself-diagnose.",
          "link": "http://arxiv.org/abs/2107.10712",
          "publishedOn": "2021-07-23T02:00:30.413Z",
          "wordCount": 614,
          "title": "Deep 3D-CNN for Depression Diagnosis with Facial Video Recording of Self-Rating Depression Scale Questionnaire. (arXiv:2107.10712v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1\">Jimmy Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1\">Nicolo Fusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>",
          "description": "Given a trained network, how can we accelerate it to meet efficiency needs\nfor deployment on particular hardware? The commonly used hardware-aware network\ncompression techniques address this question with pruning, kernel fusion,\nquantization and lowering precision. However, these approaches do not change\nthe underlying network operations. In this paper, we propose hardware-aware\nnetwork transformation (HANT), which accelerates a network by replacing\ninefficient operations with more efficient alternatives using a neural\narchitecture search like approach. HANT tackles the problem in two phase: In\nthe first phase, a large number of alternative operations per every layer of\nthe teacher model is trained using layer-wise feature map distillation. In the\nsecond phase, the combinatorial selection of efficient operations is relaxed to\nan integer optimization problem that can be solved in a few seconds. We extend\nHANT with kernel fusion and quantization to improve throughput even further.\nOur experimental results on accelerating the EfficientNet family show that HANT\ncan accelerate them by up to 3.6x with <0.4% drop in the top-1 accuracy on the\nImageNet dataset. When comparing the same latency level, HANT can accelerate\nEfficientNet-B4 to the same latency as EfficientNet-B1 while having 3% higher\naccuracy. We examine a large pool of operations, up to 197 per layer, and we\nprovide insights into the selected operations and final architectures.",
          "link": "http://arxiv.org/abs/2107.10624",
          "publishedOn": "2021-07-23T02:00:30.385Z",
          "wordCount": 656,
          "title": "HANT: Hardware-Aware Network Transformation. (arXiv:2107.10624v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungsoo Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Abstract reasoning, i.e., inferring complicated patterns from given\nobservations, is a central building block of artificial general intelligence.\nWhile humans find the answer by either eliminating wrong candidates or first\nconstructing the answer, prior deep neural network (DNN)-based methods focus on\nthe former discriminative approach. This paper aims to design a framework for\nthe latter approach and bridge the gap between artificial and human\nintelligence. To this end, we propose logic-guided generation (LoGe), a novel\ngenerative DNN framework that reduces abstract reasoning as an optimization\nproblem in propositional logic. LoGe is composed of three steps: extract\npropositional variables from images, reason the answer variables with a logic\nlayer, and reconstruct the answer image from the variables. We demonstrate that\nLoGe outperforms the black box DNN frameworks for generative abstract reasoning\nunder the RAVEN benchmark, i.e., reconstructing answers based on capturing\ncorrect rules of various attributes from observations.",
          "link": "http://arxiv.org/abs/2107.10493",
          "publishedOn": "2021-07-23T02:00:30.325Z",
          "wordCount": 601,
          "title": "Abstract Reasoning via Logic-guided Generation. (arXiv:2107.10493v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toole_C/0/1/0/all/0/1\">Christopher D. Toole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maestri_J/0/1/0/all/0/1\">Joseph A. Maestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>",
          "description": "We propose a memory-based framework for real-time, data-efficient target\nanalysis in forward-looking-sonar (FLS) imagery. Our framework relies on first\nremoving non-discriminative details from the imagery using a small-scale\nDenseNet-inspired network. Doing so simplifies ensuing analyses and permits\ngeneralizing from few labeled examples. We then cascade the filtered imagery\ninto a novel NeuralRAM-based convolutional matching network, NRMN, for low-shot\ntarget recognition. We employ a small-scale FlowNet, LFN to align and register\nFLS imagery across local temporal scales. LFN enables target label consensus\nvoting across images and generally improves target detection and recognition\nrates.\n\nWe evaluate our framework using real-world FLS imagery with multiple broad\ntarget classes that have high intra-class variability and rich sub-class\nstructure. We show that few-shot learning, with anywhere from ten to thirty\nclass-specific exemplars, performs similarly to supervised deep networks\ntrained on hundreds of samples per class. Effective zero-shot learning is also\npossible. High performance is realized from the inductive-transfer properties\nof NRMNs when distractor elements are removed.",
          "link": "http://arxiv.org/abs/2107.10504",
          "publishedOn": "2021-07-23T02:00:30.262Z",
          "wordCount": 616,
          "title": "External-Memory Networks for Low-Shot Learning of Targets in Forward-Looking-Sonar Imagery. (arXiv:2107.10504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Hyukseong Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1\">Amir Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kevin G. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Amit Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_R/0/1/0/all/0/1\">Rajan Bhattacharyya</a>",
          "description": "This paper proposes the CogSense system, which is inspired by sense-making\ncognition and perception in the mammalian brain to perform perception error\ndetection and perception parameter adaptation using probabilistic signal\ntemporal logic. As a specific application, a contrast-based perception adaption\nmethod is presented and validated. The proposed method evaluates perception\nerrors using heterogeneous probe functions computed from the detected objects\nand subsequently solves a contrast optimization problem to correct perception\nerrors. The CogSense probe functions utilize the characteristics of geometry,\ndynamics, and detected blob image quality of the objects to develop axioms in a\nprobabilistic signal temporal logic framework. By evaluating these axioms, we\ncan formally verify whether the detections are valid or erroneous. Further,\nusing the CogSense axioms, we generate the probabilistic signal temporal\nlogic-based constraints to finally solve the contrast-based optimization\nproblem to reduce false positives and false negatives.",
          "link": "http://arxiv.org/abs/2107.10456",
          "publishedOn": "2021-07-23T02:00:30.244Z",
          "wordCount": 579,
          "title": "CogSense: A Cognitively Inspired Framework for Perception Adaptation. (arXiv:2107.10456v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengxiong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Erjin Zhou</a>",
          "description": "Most existing human pose estimation (HPE) methods exploit multi-scale\ninformation by fusing feature maps of four different spatial sizes, \\ie $1/4$,\n$1/8$, $1/16$, and $1/32$ of the input image. There are two drawbacks of this\nstrategy: 1) feature maps of different spatial sizes may be not well aligned\nspatially, which potentially hurts the accuracy of keypoint location; 2) these\nscales are fixed and inflexible, which may restrict the generalization ability\nover various human sizes. Towards these issues, we propose an adaptive dilated\nconvolution (ADC). It can generate and fuse multi-scale features of the same\nspatial sizes by setting different dilation rates for different channels. More\nimportantly, these dilation rates are generated by a regression module. It\nenables ADC to adaptively adjust the fused scales and thus ADC may generalize\nbetter to various human sizes. ADC can be end-to-end trained and easily plugged\ninto existing methods. Extensive experiments show that ADC can bring consistent\nimprovements to various HPE methods. The source codes will be released for\nfurther research.",
          "link": "http://arxiv.org/abs/2107.10477",
          "publishedOn": "2021-07-23T02:00:30.236Z",
          "wordCount": 604,
          "title": "Adaptive Dilated Convolution For Human Pose Estimation. (arXiv:2107.10477v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuesong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_M/0/1/0/all/0/1\">Meihao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Contrastive self-supervised learning (SSL) has achieved great success in\nunsupervised visual representation learning by maximizing the similarity\nbetween two augmented views of the same image (positive pairs) and\nsimultaneously contrasting other different images (negative pairs). However,\nthis type of methods, such as SimCLR and MoCo, relies heavily on a large number\nof negative pairs and thus requires either large batches or memory banks. In\ncontrast, some recent non-contrastive SSL methods, such as BYOL and SimSiam,\nattempt to discard negative pairs by introducing asymmetry and show remarkable\nperformance. Unfortunately, to avoid collapsed solutions caused by not using\nnegative pairs, these methods require sophisticated asymmetry designs. In this\npaper, we argue that negative pairs are still necessary but one is sufficient,\ni.e., triplet is all you need. A simple triplet-based loss can achieve\nsurprisingly good performance without requiring large batches or asymmetry.\nMoreover, we observe that unsupervised visual representation learning can gain\nsignificantly from randomness. Based on this observation, we propose a simple\nplug-in RandOm MApping (ROMA) strategy by randomly mapping samples into other\nspaces and enforcing these randomly projected samples to satisfy the same\ncorrelation requirement. The proposed ROMA strategy not only achieves the\nstate-of-the-art performance in conjunction with the triplet-based loss, but\nalso can further effectively boost other SSL methods.",
          "link": "http://arxiv.org/abs/2107.10419",
          "publishedOn": "2021-07-23T02:00:30.210Z",
          "wordCount": 665,
          "title": "Triplet is All You Need with Random Mappings for Unsupervised Visual Representation Learning. (arXiv:2107.10419v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10327",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sengupta_A/0/1/0/all/0/1\">Arindam Sengupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_S/0/1/0/all/0/1\">Siyang Cao</a>",
          "description": "In this paper we presented mmPose-NLP, a novel Natural Language Processing\n(NLP) inspired Sequence-to-Sequence (Seq2Seq) skeletal key-point estimator\nusing millimeter-wave (mmWave) radar data. To the best of the author's\nknowledge, this is the first method to precisely estimate upto 25 skeletal\nkey-points using mmWave radar data alone. Skeletal pose estimation is critical\nin several applications ranging from autonomous vehicles, traffic monitoring,\npatient monitoring, gait analysis, to defense security forensics, and aid both\npreventative and actionable decision making. The use of mmWave radars for this\ntask, over traditionally employed optical sensors, provide several advantages,\nprimarily its operational robustness to scene lighting and adverse weather\nconditions, where optical sensor performance degrade significantly. The mmWave\nradar point-cloud (PCL) data is first voxelized (analogous to tokenization in\nNLP) and $N$ frames of the voxelized radar data (analogous to a text paragraph\nin NLP) is subjected to the proposed mmPose-NLP architecture, where the voxel\nindices of the 25 skeletal key-points (analogous to keyword extraction in NLP)\nare predicted. The voxel indices are converted back to real world 3-D\ncoordinates using the voxel dictionary used during the tokenization process.\nMean Absolute Error (MAE) metrics were used to measure the accuracy of the\nproposed system against the ground truth, with the proposed mmPose-NLP offering\n<3 cm localization errors in the depth, horizontal and vertical axes. The\neffect of the number of input frames vs performance/accuracy was also studied\nfor N = {1,2,..,10}. A comprehensive methodology, results, discussions and\nlimitations are presented in this paper. All the source codes and results are\nmade available on GitHub for furthering research and development in this\ncritical yet emerging domain of skeletal key-point estimation using mmWave\nradars.",
          "link": "http://arxiv.org/abs/2107.10327",
          "publishedOn": "2021-07-23T02:00:30.198Z",
          "wordCount": 729,
          "title": "mmPose-NLP: A Natural Language Processing Approach to Precise Skeletal Pose Estimation using mmWave Radars. (arXiv:2107.10327v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>",
          "description": "Forecasting the behavior of other agents is an integral part of the modern\nrobotic autonomy stack, especially in safety-critical scenarios with\nhuman-robot interaction, such as autonomous driving. In turn, there has been a\nsignificant amount of interest and research in trajectory forecasting,\nresulting in a wide variety of approaches. Common to all works, however, is the\nuse of the same few accuracy-based evaluation metrics, e.g., displacement error\nand log-likelihood. While these metrics are informative, they are task-agnostic\nand predictions that are evaluated as equal can lead to vastly different\noutcomes, e.g., in downstream planning and decision making. In this work, we\ntake a step back and critically evaluate current trajectory forecasting\nmetrics, proposing task-aware metrics as a better measure of performance in\nsystems where prediction is being deployed. We additionally present one example\nof such a metric, incorporating planning-awareness within existing trajectory\nforecasting metrics.",
          "link": "http://arxiv.org/abs/2107.10297",
          "publishedOn": "2021-07-23T02:00:30.189Z",
          "wordCount": 583,
          "title": "Rethinking Trajectory Forecasting Evaluation. (arXiv:2107.10297v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minghan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huei Peng</a>",
          "description": "This paper proposes a correspondence-free method for point cloud rotational\nregistration. We learn an embedding for each point cloud in a feature space\nthat preserves the SO(3)-equivariance property, enabled by recent developments\nin equivariant neural networks. The proposed shape registration method achieves\nthree major advantages through combining equivariant feature learning with\nimplicit shape models. First, the necessity of data association is removed\nbecause of the permutation-invariant property in network architectures similar\nto PointNet. Second, the registration in feature space can be solved in\nclosed-form using Horn's method due to the SO(3)-equivariance property. Third,\nthe registration is robust to noise in the point cloud because of implicit\nshape learning. The experimental results show superior performance compared\nwith existing correspondence-free deep registration methods.",
          "link": "http://arxiv.org/abs/2107.10296",
          "publishedOn": "2021-07-23T02:00:30.171Z",
          "wordCount": 570,
          "title": "Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit Shape Representations. (arXiv:2107.10296v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>",
          "description": "Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",
          "link": "http://arxiv.org/abs/2107.10300",
          "publishedOn": "2021-07-23T02:00:30.160Z",
          "wordCount": 723,
          "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nalaie_K/0/1/0/all/0/1\">Keivan Nalaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rong Zheng</a>",
          "description": "In surveillance and search and rescue applications, it is important to\nperform multi-target tracking (MOT) in real-time on low-end devices. Today's\nMOT solutions employ deep neural networks, which tend to have high computation\ncomplexity. Recognizing the effects of frame sizes on tracking performance, we\npropose DeepScale, a model agnostic frame size selection approach that operates\non top of existing fully convolutional network-based trackers to accelerate\ntracking throughput. In the training stage, we incorporate detectability scores\ninto a one-shot tracker architecture so that DeepScale can learn representation\nestimations for different frame sizes in a self-supervised manner. During\ninference, based on user-controlled parameters, it can find a suitable\ntrade-off between tracking accuracy and speed by adapting frame sizes at run\ntime. Extensive experiments and benchmark tests on MOT datasets demonstrate the\neffectiveness and flexibility of DeepScale. Compared to a state-of-the-art\ntracker, DeepScale++, a variant of DeepScale achieves 1.57X accelerated with\nonly moderate degradation (~ 2.4) in tracking accuracy on the MOT15 dataset in\none configuration.",
          "link": "http://arxiv.org/abs/2107.10404",
          "publishedOn": "2021-07-23T02:00:30.108Z",
          "wordCount": 603,
          "title": "DeepScale: An Online Frame Size Adaptation Framework to Accelerate Visual Multi-object Tracking. (arXiv:2107.10404v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhimireddy_A/0/1/0/all/0/1\">Ananth Reddy Bhimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_J/0/1/0/all/0/1\">John L. Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1\">Leo Anthony Celi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li-Ching Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_R/0/1/0/all/0/1\">Ramon Correa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dullerud_N/0/1/0/all/0/1\">Natalie Dullerud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Cheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_P/0/1/0/all/0/1\">Po-Chih Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_L/0/1/0/all/0/1\">Lyle Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brandon J Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyrros_A/0/1/0/all/0/1\">Ayis Pyrros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oakden_Rayner_L/0/1/0/all/0/1\">Luke Oakden-Rayner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okechukwu_C/0/1/0/all/0/1\">Chima Okechukwu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyyed_Kalantari_L/0/1/0/all/0/1\">Laleh Seyyed-Kalantari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Hari Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ryan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiman_Z/0/1/0/all/0/1\">Zachary Zaiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy W Gichoya</a>",
          "description": "Background: In medical imaging, prior studies have demonstrated disparate AI\nperformance by race, yet there is no known correlation for race on medical\nimaging that would be obvious to the human expert interpreting the images.\n\nMethods: Using private and public datasets we evaluate: A) performance\nquantification of deep learning models to detect race from medical images,\nincluding the ability of these models to generalize to external environments\nand across multiple imaging modalities, B) assessment of possible confounding\nanatomic and phenotype population features, such as disease distribution and\nbody habitus as predictors of race, and C) investigation into the underlying\nmechanism by which AI models can recognize race.\n\nFindings: Standard deep learning models can be trained to predict race from\nmedical images with high performance across multiple imaging modalities. Our\nfindings hold under external validation conditions, as well as when models are\noptimized to perform clinically motivated tasks. We demonstrate this detection\nis not due to trivial proxies or imaging-related surrogate covariates for race,\nsuch as underlying disease distribution. Finally, we show that performance\npersists over all anatomical regions and frequency spectrum of the images\nsuggesting that mitigation efforts will be challenging and demand further\nstudy.\n\nInterpretation: We emphasize that model ability to predict self-reported race\nis itself not the issue of importance. However, our findings that AI can\ntrivially predict self-reported race -- even from corrupted, cropped, and\nnoised medical images -- in a setting where clinical experts cannot, creates an\nenormous risk for all model deployments in medical imaging: if an AI model\nsecretly used its knowledge of self-reported race to misclassify all Black\npatients, radiologists would not be able to tell using the same data the model\nhas access to.",
          "link": "http://arxiv.org/abs/2107.10356",
          "publishedOn": "2021-07-23T02:00:30.095Z",
          "wordCount": 787,
          "title": "Reading Race: AI Recognises Patient's Racial Identity In Medical Images. (arXiv:2107.10356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiujun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Many RGB-T trackers attempt to attain robust feature representation by\nutilizing an adaptive weighting scheme (or attention mechanism). Different from\nthese works, we propose a new dynamic modality-aware filter generation module\n(named MFGNet) to boost the message communication between visible and thermal\ndata by adaptively adjusting the convolutional kernels for various input images\nin practical tracking. Given the image pairs as input, we first encode their\nfeatures with the backbone network. Then, we concatenate these feature maps and\ngenerate dynamic modality-aware filters with two independent networks. The\nvisible and thermal filters will be used to conduct a dynamic convolutional\noperation on their corresponding input feature maps respectively. Inspired by\nresidual connection, both the generated visible and thermal feature maps will\nbe summarized with input feature maps. The augmented feature maps will be fed\ninto the RoI align module to generate instance-level features for subsequent\nclassification. To address issues caused by heavy occlusion, fast motion, and\nout-of-view, we propose to conduct a joint local and global search by\nexploiting a new direction-aware target-driven attention mechanism. The spatial\nand temporal recurrent neural network is used to capture the direction-aware\ncontext for accurate global attention prediction. Extensive experiments on\nthree large-scale RGB-T tracking benchmark datasets validated the effectiveness\nof our proposed algorithm. The project page of this paper is available at\nhttps://sites.google.com/view/mfgrgbttrack/.",
          "link": "http://arxiv.org/abs/2107.10433",
          "publishedOn": "2021-07-23T02:00:30.082Z",
          "wordCount": 671,
          "title": "MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking. (arXiv:2107.10433v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.00719",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_P/0/1/0/all/0/1\">Po-Yu Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_S/0/1/0/all/0/1\">Shu-Min Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_N/0/1/0/all/0/1\">Nan-Lan Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chu Lin</a>",
          "description": "Drug-target interaction (DTI) prediction plays a crucial role in drug\ndiscovery, and deep learning approaches have achieved state-of-the-art\nperformance in this field. We introduce an ensemble of deep learning models\n(EnsembleDLM) for DTI prediction. EnsembleDLM only uses the sequence\ninformation of chemical compounds and proteins, and it aggregates the\npredictions from multiple deep neural networks. This approach not only achieves\nstate-of-the-art performance in Davis and KIBA datasets but also reaches\ncutting-edge performance in the cross-domain applications across different\nbio-activity types and different protein classes. We also demonstrate that\nEnsembleDLM achieves a good performance (Pearson correlation coefficient and\nconcordance index > 0.8) in the new domain with approximately 50% transfer\nlearning data, i.e., the training set has twice as much data as the test set.",
          "link": "http://arxiv.org/abs/2107.00719",
          "publishedOn": "2021-07-29T02:00:11.235Z",
          "wordCount": 588,
          "title": "Toward Drug-Target Interaction Prediction via Ensemble Modeling and Transfer Learning. (arXiv:2107.00719v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1\">Zach Nussbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "As machine learning models are increasingly employed to assist human\ndecision-makers, it becomes critical to communicate the uncertainty associated\nwith these model predictions. However, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking approaches - where the model\nassigns low probabilities or scores to uncertain examples. While this captures\nwhat examples are challenging for the model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek to identify examples the model\nis uncertain about and characterize the source of said uncertainty. We explore\nthe benefits of designing a targeted intervention - targeted data augmentation\nof the examples where the model is uncertain over the course of training. We\ninvestigate whether the rate of learning in the presence of additional\ninformation differs between atypical and noisy examples? Our results show that\nthis is indeed the case, suggesting that well-designed interventions over the\ncourse of training can be an effective way to characterize and distinguish\nbetween different sources of uncertainty.",
          "link": "http://arxiv.org/abs/2107.13098",
          "publishedOn": "2021-07-29T02:00:11.209Z",
          "wordCount": 618,
          "title": "A Tale Of Two Long Tails. (arXiv:2107.13098v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward S. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1\">Dinesh Jayaraman</a>",
          "description": "Training visuomotor robot controllers from scratch on a new robot typically\nrequires generating large amounts of robot-specific data. Could we leverage\ndata previously collected on another robot to reduce or even completely remove\nthis need for robot-specific data? We propose a \"robot-aware\" solution paradigm\nthat exploits readily available robot \"self-knowledge\" such as proprioception,\nkinematics, and camera calibration to achieve this. First, we learn modular\ndynamics models that pair a transferable, robot-agnostic world dynamics module\nwith a robot-specific, analytical robot dynamics module. Next, we set up visual\nplanning costs that draw a distinction between the robot self and the world.\nOur experiments on tabletop manipulation tasks in simulation and on real robots\ndemonstrate that these plug-in improvements dramatically boost the\ntransferability of visuomotor controllers, even permitting zero-shot transfer\nonto new robots for the very first time. Project website:\nhttps://hueds.github.io/rac/",
          "link": "http://arxiv.org/abs/2107.09047",
          "publishedOn": "2021-07-29T02:00:11.202Z",
          "wordCount": 603,
          "title": "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness. (arXiv:2107.09047v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuangjia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Ying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jiahua Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>",
          "description": "Constructing appropriate representations of molecules lies at the core of\nnumerous tasks such as material science, chemistry and drug designs. Recent\nresearches abstract molecules as attributed graphs and employ graph neural\nnetworks (GNN) for molecular representation learning, which have made\nremarkable achievements in molecular graph modeling. Albeit powerful, current\nmodels either are based on local aggregation operations and thus miss\nhigher-order graph properties or focus on only node information without fully\nusing the edge information. For this sake, we propose a Communicative Message\nPassing Transformer (CoMPT) neural network to improve the molecular graph\nrepresentation by reinforcing message interactions between nodes and edges\nbased on the Transformer architecture. Unlike the previous transformer-style\nGNNs that treat molecules as fully connected graphs, we introduce a message\ndiffusion mechanism to leverage the graph connectivity inductive bias and\nreduce the message enrichment explosion. Extensive experiments demonstrated\nthat the proposed model obtained superior performances (around 4$\\%$ on\naverage) against state-of-the-art baselines on seven chemical property datasets\n(graph-level tasks) and two chemical shift datasets (node-level tasks). Further\nvisualization studies also indicated a better representation capacity achieved\nby our model.",
          "link": "http://arxiv.org/abs/2107.08773",
          "publishedOn": "2021-07-29T02:00:11.168Z",
          "wordCount": 649,
          "title": "Learning Attributed Graph Representations with Communicative Message Passing Transformer. (arXiv:2107.08773v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "How can deep neural networks encode information that corresponds to words in\nhuman speech into raw acoustic data? This paper proposes two neural network\narchitectures for modeling unsupervised lexical learning from raw acoustic\ninputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),\nthat combine a Deep Convolutional GAN architecture for audio data (WaveGAN;\narXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN\n(arXiv:1606.03657), and propose a new latent space structure that can model\nfeatural learning simultaneously with a higher level classification and allows\nfor a very low-dimension vector representation of lexical items. Lexical\nlearning is modeled as emergent from an architecture that forces a deep neural\nnetwork to output data such that unique information is retrievable from its\nacoustic outputs. The networks trained on lexical items from TIMIT learn to\nencode unique information corresponding to lexical items in the form of\ncategorical variables in their latent space. By manipulating these variables,\nthe network outputs specific lexical items. The network occasionally outputs\ninnovative lexical items that violate training data, but are linguistically\ninterpretable and highly informative for cognitive modeling and neural network\ninterpretability. Innovative outputs suggest that phonetic and phonological\nrepresentations learned by the network can be productively recombined and\ndirectly paralleled to productivity in human speech: a fiwGAN network trained\non `suit' and `dark' outputs innovative `start', even though it never saw\n`start' or even a [st] sequence in the training data. We also argue that\nsetting latent featural codes to values well beyond training range results in\nalmost categorical generation of prototypical lexical items and reveals\nunderlying values of each latent code.",
          "link": "http://arxiv.org/abs/2006.02951",
          "publishedOn": "2021-07-29T02:00:11.091Z",
          "wordCount": 756,
          "title": "CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks. (arXiv:2006.02951v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub. To the best of our knowledge we believe\nthis the first works to try out semi-supervised learning to improve flood\nsegmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-29T02:00:11.083Z",
          "wordCount": 774,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_B_J/0/1/0/all/0/1\">J.-Emeterio Navarro-B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebert_M/0/1/0/all/0/1\">Martin Gebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielig_R/0/1/0/all/0/1\">Ralf Bielig</a>",
          "description": "This article proposes two different approaches to automatically create a map\nfor valid on-street car parking spaces. For this, we use car sharing park-out\nevents data. The first one uses spatial aggregation and the second a machine\nlearning algorithm. For the former, we chose rasterization and road sectioning;\nfor the latter we chose decision trees. We compare the results of these\napproaches and discuss their advantages and disadvantages. Furthermore, we show\nour results for a neighborhood in the city of Berlin and report a\nclassification accuracy of 91.6\\% on the original imbalanced data. Finally, we\ndiscuss further work; from gathering more data over a longer period of time to\nfitting spatial Gaussian densities to the data and the usage of apps for manual\nvalidation and annotation of parking spaces to improve ground truth data.",
          "link": "http://arxiv.org/abs/2102.06758",
          "publishedOn": "2021-07-29T02:00:11.075Z",
          "wordCount": 621,
          "title": "On automatic extraction of on-street parking spaces using park-out events data. (arXiv:2102.06758v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00773",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Visani_G/0/1/0/all/0/1\">Gian Marco Visani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1\">Alexandra Hope Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kent_D/0/1/0/all/0/1\">David M. Kent</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wong_J/0/1/0/all/0/1\">John B. Wong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cohen_J/0/1/0/all/0/1\">Joshua T. Cohen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "We address the problem of modeling constrained hospital resources in the\nmidst of the COVID-19 pandemic in order to inform decision-makers of future\ndemand and assess the societal value of possible interventions. For broad\napplicability, we focus on the common yet challenging scenario where\npatient-level data for a region of interest are not available. Instead, given\ndaily admissions counts, we model aggregated counts of observed resource use,\nsuch as the number of patients in the general ward, in the intensive care unit,\nor on a ventilator. In order to explain how individual patient trajectories\nproduce these counts, we propose an aggregate count explicit-duration hidden\nMarkov model, nicknamed the ACED-HMM, with an interpretable, compact\nparameterization. We develop an Approximate Bayesian Computation approach that\ndraws samples from the posterior distribution over the model's transition and\nduration parameters given aggregate counts from a specific location, thus\nadapting the model to a region or individual hospital site of interest. Samples\nfrom this posterior can then be used to produce future forecasts of any counts\nof interest. Using data from the United States and the United Kingdom, we show\nour mechanistic approach provides competitive probabilistic forecasts for the\nfuture even as the dynamics of the pandemic shift. Furthermore, we show how our\nmodel provides insight about recovery probabilities or length of stay\ndistributions, and we suggest its potential to answer challenging what-if\nquestions about the societal value of possible interventions.",
          "link": "http://arxiv.org/abs/2105.00773",
          "publishedOn": "2021-07-29T02:00:11.068Z",
          "wordCount": 782,
          "title": "Approximate Bayesian Computation for an Explicit-Duration Hidden Markov Model of COVID-19 Hospital Trajectories. (arXiv:2105.00773v2 [stat.AP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00533",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hall_G/0/1/0/all/0/1\">Georgina Hall</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>",
          "description": "In this paper, we consider the graph alignment problem, which is the problem\nof recovering, given two graphs, a one-to-one mapping between nodes that\nmaximizes edge overlap. This problem can be viewed as a noisy version of the\nwell-known graph isomorphism problem and appears in many applications,\nincluding social network deanonymization and cellular biology. Our focus here\nis on partial recovery, i.e., we look for a one-to-one mapping which is correct\non a fraction of the nodes of the graph rather than on all of them, and we\nassume that the two input graphs to the problem are correlated\nErd\\H{o}s-R\\'enyi graphs of parameters $(n,q,s)$. Our main contribution is then\nto give necessary and sufficient conditions on $(n,q,s)$ under which partial\nrecovery is possible with high probability as the number of nodes $n$ goes to\ninfinity. In particular, we show that it is possible to achieve partial\nrecovery in the $nqs=\\Theta(1)$ regime under certain additional assumptions. An\ninteresting byproduct of the analysis techniques we develop to obtain the\nsufficiency result in the partial recovery setting is a tighter analysis of the\nmaximum likelihood estimator for the graph alignment problem, which leads to\nimproved sufficient conditions for exact recovery.",
          "link": "http://arxiv.org/abs/2007.00533",
          "publishedOn": "2021-07-29T02:00:11.061Z",
          "wordCount": 677,
          "title": "Partial Recovery in the Graph Alignment Problem. (arXiv:2007.00533v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamford_C/0/1/0/all/0/1\">Chris Bamford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grela_L/0/1/0/all/0/1\">Lukasz Grela</a>",
          "description": "In recent years, researchers have achieved great success in applying Deep\nReinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games,\ncreating strong autonomous agents that could defeat professional players in\nStarCraft~II. However, existing approaches to tackle full games have high\ncomputational costs, usually requiring the use of thousands of GPUs and CPUs\nfor weeks. This paper has two main contributions to address this issue: 1) We\nintroduce Gym-$\\mu$RTS (pronounced \"gym-micro-RTS\") as a fast-to-run RL\nenvironment for full-game RTS research and 2) we present a collection of\ntechniques to scale DRL to play full-game $\\mu$RTS as well as ablation studies\nto demonstrate their empirical importance. Our best-trained bot can defeat\nevery $\\mu$RTS bot we tested from the past $\\mu$RTS competitions when working\nin a single-map setting, resulting in a state-of-the-art DRL agent while only\ntaking about 60 hours of training using a single machine (one GPU, three vCPU,\n16GB RAM). See the blog post at\nhttps://wandb.ai/vwxyzjn/gym-microrts-paper/reports/Gym-RTS-Toward-Affordable-Deep-Reinforcement-Learning-Research-in-Real-Time-Strategy-Games--Vmlldzo2MDIzMTg\nand the source code at https://github.com/vwxyzjn/gym-microrts-paper",
          "link": "http://arxiv.org/abs/2105.13807",
          "publishedOn": "2021-07-29T02:00:11.038Z",
          "wordCount": 666,
          "title": "Gym-$\\mu$RTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning. (arXiv:2105.13807v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.11988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fornasier_M/0/1/0/all/0/1\">Massimo Fornasier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pareschi_L/0/1/0/all/0/1\">Lorenzo Pareschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunnen_P/0/1/0/all/0/1\">Philippe S&#xfc;nnen</a>",
          "description": "We investigate the implementation of a new stochastic Kuramoto-Vicsek-type\nmodel for global optimization of nonconvex functions on the sphere. This model\nbelongs to the class of Consensus-Based Optimization. In fact, particles move\non the sphere driven by a drift towards an instantaneous consensus point, which\nis computed as a convex combination of particle locations, weighted by the cost\nfunction according to Laplace's principle, and it represents an approximation\nto a global minimizer. The dynamics is further perturbed by a random vector\nfield to favor exploration, whose variance is a function of the distance of the\nparticles to the consensus point. In particular, as soon as the consensus is\nreached the stochastic component vanishes. The main results of this paper are\nabout the proof of convergence of the numerical scheme to global minimizers\nprovided conditions of well-preparation of the initial datum. The proof\ncombines previous results of mean-field limit with a novel asymptotic analysis,\nand classical convergence results of numerical methods for SDE. We present\nseveral numerical experiments, which show that the algorithm proposed in the\npresent paper scales well with the dimension and is extremely versatile. To\nquantify the performances of the new approach, we show that the algorithm is\nable to perform essentially as good as ad hoc state of the art methods in\nchallenging problems in signal processing and machine learning, namely the\nphase retrieval problem and the robust subspace detection.",
          "link": "http://arxiv.org/abs/2001.11988",
          "publishedOn": "2021-07-29T02:00:11.020Z",
          "wordCount": 747,
          "title": "Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning. (arXiv:2001.11988v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1\">Amira Guesmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1\">Ihsen Alouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasawneh_K/0/1/0/all/0/1\">Khaled Khasawneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baklouti_M/0/1/0/all/0/1\">Mouna Baklouti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frikha_T/0/1/0/all/0/1\">Tarek Frikha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_M/0/1/0/all/0/1\">Mohamed Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1\">Nael Abu-Ghazaleh</a>",
          "description": "In the past few years, an increasing number of machine-learning and deep\nlearning structures, such as Convolutional Neural Networks (CNNs), have been\napplied to solving a wide range of real-life problems. However, these\narchitectures are vulnerable to adversarial attacks. In this paper, we propose\nfor the first time to use hardware-supported approximate computing to improve\nthe robustness of machine learning classifiers. We show that our approximate\ncomputing implementation achieves robustness across a wide range of attack\nscenarios. Specifically, for black-box and grey-box attack scenarios, we show\nthat successful adversarial attacks against the exact classifier have poor\ntransferability to the approximate implementation. Surprisingly, the robustness\nadvantages also apply to white-box attacks where the attacker has access to the\ninternal implementation of the approximate classifier. We explain some of the\npossible reasons for this robustness through analysis of the internal operation\nof the approximate implementation. Furthermore, our approximate computing model\nmaintains the same level in terms of classification accuracy, does not require\nretraining, and reduces resource utilization and energy consumption of the CNN.\nWe conducted extensive experiments on a set of strong adversarial attacks; We\nempirically show that the proposed implementation increases the robustness of a\nLeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong\ngrey-box adversarial attacks along with up to 67% saving in energy consumption\ndue to the simpler nature of the approximate logic. We also show that a\nwhite-box attack requires a remarkably higher noise budget to fool the\napproximate classifier, causing an average of 4db degradation of the PSNR of\nthe input image relative to the images that succeed in fooling the exact\nclassifier",
          "link": "http://arxiv.org/abs/2006.07700",
          "publishedOn": "2021-07-29T02:00:11.003Z",
          "wordCount": 761,
          "title": "Defensive Approximation: Enhancing CNNs Security through Approximate Computing. (arXiv:2006.07700v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>",
          "description": "Near out-of-distribution detection (OOD) is a major challenge for deep neural\nnetworks. We demonstrate that large-scale pre-trained transformers can\nsignificantly improve the state-of-the-art (SOTA) on a range of near OOD tasks\nacross different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD\ndetection, we improve the AUROC from 85% (current SOTA) to more than 96% using\nVision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD\ndetection benchmark, we improve the AUROC from 66% to 77% using transformers\nand unsupervised pre-training. To further improve performance, we explore the\nfew-shot outlier exposure setting where a few examples from outlier classes may\nbe available; we show that pre-trained transformers are particularly\nwell-suited for outlier exposure, and that the AUROC of OOD detection on\nCIFAR-100 vs CIFAR-10 can be improved to 98.7% with just 1 image per OOD class,\nand 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained\ntransformers such as CLIP, we explore a new way of using just the names of\noutlier classes as a sole source of information without any accompanying\nimages, and show that this outperforms previous SOTA on standard vision OOD\nbenchmark tasks.",
          "link": "http://arxiv.org/abs/2106.03004",
          "publishedOn": "2021-07-29T02:00:10.994Z",
          "wordCount": 646,
          "title": "Exploring the Limits of Out-of-Distribution Detection. (arXiv:2106.03004v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>",
          "description": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
          "link": "http://arxiv.org/abs/2107.02192",
          "publishedOn": "2021-07-29T02:00:10.967Z",
          "wordCount": 700,
          "title": "Long-Short Transformer: Efficient Transformers for Language and Vision. (arXiv:2107.02192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Minping Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1\">Qiuhua Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yudong Cao</a>",
          "description": "The scope of data-driven fault diagnosis models is greatly improved through\ndeep learning (DL). However, the classical convolution and recurrent structure\nhave their defects in computational efficiency and feature representation,\nwhile the latest Transformer architecture based on attention mechanism has not\nbeen applied in this field. To solve these problems, we propose a novel\ntime-frequency Transformer (TFT) model inspired by the massive success of\nstandard Transformer in sequence processing. Specially, we design a fresh\ntokenizer and encoder module to extract effective abstractions from the\ntime-frequency representation (TFR) of vibration signals. On this basis, a new\nend-to-end fault diagnosis framework based on time-frequency Transformer is\npresented in this paper. Through the case studies on bearing experimental\ndatasets, we constructed the optimal Transformer structure and verified the\nperformance of the diagnostic method. The superiority of the proposed method is\ndemonstrated in comparison with the benchmark model and other state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2104.09079",
          "publishedOn": "2021-07-29T02:00:10.915Z",
          "wordCount": 623,
          "title": "A novel Time-frequency Transformer and its Application in Fault Diagnosis of Rolling Bearings. (arXiv:2104.09079v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02604",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Sabando_M/0/1/0/all/0/1\">Mar&#xed;a Virginia Sabando</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ponzoni_I/0/1/0/all/0/1\">Ignacio Ponzoni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos E. Milios</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Soto_A/0/1/0/all/0/1\">Axel J. Soto</a>",
          "description": "With the consolidation of deep learning in drug discovery, several novel\nalgorithms for learning molecular representations have been proposed. Despite\nthe interest of the community in developing new methods for learning molecular\nembeddings and their theoretical benefits, comparing molecular embeddings with\neach other and with traditional representations is not straightforward, which\nin turn hinders the process of choosing a suitable representation for QSAR\nmodeling. A reason behind this issue is the difficulty of conducting a fair and\nthorough comparison of the different existing embedding approaches, which\nrequires numerous experiments on various datasets and training scenarios. To\nclose this gap, we reviewed the literature on methods for molecular embeddings\nand reproduced three unsupervised and two supervised molecular embedding\ntechniques recently proposed in the literature. We compared these five methods\nconcerning their performance in QSAR scenarios using different classification\nand regression datasets. We also compared these representations to traditional\nmolecular representations, namely molecular descriptors and fingerprints. As\nopposed to the expected outcome, our experimental setup consisting of over\n25,000 trained models and statistical tests revealed that the predictive\nperformance using molecular embeddings did not significantly surpass that of\ntraditional representations. While supervised embeddings yielded competitive\nresults compared to those using traditional molecular representations,\nunsupervised embeddings tended to perform worse than traditional\nrepresentations. Our results highlight the need for conducting a careful\ncomparison and analysis of the different embedding techniques prior to using\nthem in drug design tasks, and motivate a discussion about the potential of\nmolecular embeddings in computer-aided drug design.",
          "link": "http://arxiv.org/abs/2104.02604",
          "publishedOn": "2021-07-29T02:00:10.908Z",
          "wordCount": 714,
          "title": "Using Molecular Embeddings in QSAR Modeling: Does it Make a Difference?. (arXiv:2104.02604v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1\">Ananth Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1\">Michael Mathioudakis</a>",
          "description": "Machine unlearning is the task of updating machine learning (ML) models after\na subset of the training data they were trained on is deleted. Methods for the\ntask are desired to combine effectiveness and efficiency, i.e., they should\neffectively \"unlearn\" deleted data, but in a way that does not require\nexcessive computation effort (e.g., a full retraining) for a small amount of\ndeletions. Such a combination is typically achieved by tolerating some amount\nof approximation in the unlearning. In addition, laws and regulations in the\nspirit of \"the right to be forgotten\" have given rise to requirements for\ncertifiability, i.e., the ability to demonstrate that the deleted data has\nindeed been unlearned by the ML model.\n\nIn this paper, we present an experimental study of the three state-of-the-art\napproximate unlearning methods for linear models and demonstrate the trade-offs\nbetween efficiency, effectiveness and certifiability offered by each method. In\nimplementing the study, we extend some of the existing works and describe a\ncommon ML pipeline to compare and evaluate the unlearning methods on six\nreal-world datasets and a variety of settings. We provide insights into the\neffect of the quantity and distribution of the deleted data on ML models and\nthe performance of each unlearning method in different settings. We also\npropose a practical online strategy to determine when the accumulated error\nfrom approximate unlearning is large enough to warrant a full retrain of the ML\nmodel.",
          "link": "http://arxiv.org/abs/2106.15093",
          "publishedOn": "2021-07-29T02:00:10.886Z",
          "wordCount": 683,
          "title": "Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14331",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1\">Abhranil Das</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Geisler_W/0/1/0/all/0/1\">Wilson S Geisler</a>",
          "description": "Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.",
          "link": "http://arxiv.org/abs/2012.14331",
          "publishedOn": "2021-07-29T02:00:10.854Z",
          "wordCount": 681,
          "title": "A method to integrate and classify normal distributions. (arXiv:2012.14331v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengbo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zitang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weilian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamata_S/0/1/0/all/0/1\">Sei-ichiro Kamata</a>",
          "description": "Various deep neural network architectures (DNNs) maintain massive vital\nrecords in computer vision. While drawing attention worldwide, the design of\nthe overall structure lacks general guidance. Based on the relationship between\nDNN design and numerical differential equations, we performed a fair comparison\nof the residual design with higher-order perspectives. We show that the widely\nused DNN design strategy, constantly stacking a small design (usually 2-3\nlayers), could be easily improved, supported by solid theoretical knowledge and\nwith no extra parameters needed. We reorganise the residual design in\nhigher-order ways, which is inspired by the observation that many effective\nnetworks can be interpreted as different numerical discretisations of\ndifferential equations. The design of ResNet follows a relatively simple\nscheme, which is Euler forward; however, the situation becomes complicated\nrapidly while stacking. We suppose that stacked ResNet is somehow equalled to a\nhigher-order scheme; then, the current method of forwarding propagation might\nbe relatively weak compared with a typical high-order method such as\nRunge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV\nbenchmarks with sufficient experiments. Stable and noticeable increases in\nperformance are observed, and convergence and robustness are also improved. Our\nstacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per\ncent on CIFAR-10, with the same settings and parameters. The proposed strategy\nis fundamental and theoretical and can therefore be applied to any network as a\ngeneral guideline.",
          "link": "http://arxiv.org/abs/2103.15244",
          "publishedOn": "2021-07-29T02:00:10.846Z",
          "wordCount": 727,
          "title": "Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. (arXiv:2103.15244v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarussi_R/0/1/0/all/0/1\">Roei Sarussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brutzkus_A/0/1/0/all/0/1\">Alon Brutzkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>",
          "description": "Can a neural network minimizing cross-entropy learn linearly separable data?\nDespite progress in the theory of deep learning, this question remains\nunsolved. Here we prove that SGD globally optimizes this learning problem for a\ntwo-layer network with Leaky ReLU activations. The learned network can in\nprinciple be very complex. However, empirical evidence suggests that it often\nturns out to be approximately linear. We provide theoretical support for this\nphenomenon by proving that if network weights converge to two weight clusters,\nthis will imply an approximately linear decision boundary. Finally, we show a\ncondition on the optimization that leads to weight clustering. We provide\nempirical results that validate our theoretical analysis.",
          "link": "http://arxiv.org/abs/2101.02533",
          "publishedOn": "2021-07-29T02:00:10.759Z",
          "wordCount": 567,
          "title": "Towards Understanding Learning in Neural Networks with Linear Teachers. (arXiv:2101.02533v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Minto_L/0/1/0/all/0/1\">Lorenzo Minto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_M/0/1/0/all/0/1\">Moritz Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livshits_B/0/1/0/all/0/1\">Benjamin Livshits</a>",
          "description": "Recommender systems are commonly trained on centrally collected user\ninteraction data like views or clicks. This practice however raises serious\nprivacy concerns regarding the recommender's collection and handling of\npotentially sensitive data. Several privacy-aware recommender systems have been\nproposed in recent literature, but comparatively little attention has been\ngiven to systems at the intersection of implicit feedback and privacy. To\naddress this shortcoming, we propose a practical federated recommender system\nfor implicit data under user-level local differential privacy (LDP). The\nprivacy-utility trade-off is controlled by parameters $\\epsilon$ and $k$,\nregulating the per-update privacy budget and the number of $\\epsilon$-LDP\ngradient updates sent by each user respectively. To further protect the user's\nprivacy, we introduce a proxy network to reduce the fingerprinting surface by\nanonymizing and shuffling the reports before forwarding them to the\nrecommender. We empirically demonstrate the effectiveness of our framework on\nthe MovieLens dataset, achieving up to Hit Ratio with K=10 (HR@10) 0.68 on 50k\nusers with 5k items. Even on the full dataset, we show that it is possible to\nachieve reasonable utility with HR@10>0.5 without compromising user privacy.",
          "link": "http://arxiv.org/abs/2105.03941",
          "publishedOn": "2021-07-29T02:00:10.747Z",
          "wordCount": 666,
          "title": "Stronger Privacy for Federated Collaborative Filtering with Implicit Feedback. (arXiv:2105.03941v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_G/0/1/0/all/0/1\">Guannan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yujie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_S/0/1/0/all/0/1\">Steven Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>",
          "description": "With large-scale integration of renewable generation and distributed energy\nresources (DERs), modern power systems are confronted with new operational\nchallenges, such as growing complexity, increasing uncertainty, and aggravating\nvolatility. Meanwhile, more and more data are becoming available owing to the\nwidespread deployment of smart meters, smart sensors, and upgraded\ncommunication networks. As a result, data-driven control techniques, especially\nreinforcement learning (RL), have attracted surging attention in recent years.\nIn this paper, we provide a tutorial on various RL techniques and how they can\nbe applied to decision-making in power systems. We illustrate RL-based models\nand solutions in three key applications, frequency regulation, voltage control,\nand energy management. We conclude with three critical issues in the\napplication of RL, i.e., safety, scalability, and data. Several potential\nfuture directions are discussed as well.",
          "link": "http://arxiv.org/abs/2102.01168",
          "publishedOn": "2021-07-29T02:00:10.704Z",
          "wordCount": 632,
          "title": "Reinforcement Learning for Decision-Making and Control in Power Systems: Tutorial, Review, and Vision. (arXiv:2102.01168v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramabathiran_A/0/1/0/all/0/1\">Amuthan A. Ramabathiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1\">Prabhu Ramachandran</a>",
          "description": "We introduce a class of Sparse, Physics-based, and partially Interpretable\nNeural Networks (SPINN) for solving ordinary and partial differential equations\n(PDEs). By reinterpreting a traditional meshless representation of solutions of\nPDEs we develop a class of sparse neural network architectures that are\npartially interpretable. The SPINN model we propose here serves as a seamless\nbridge between two extreme modeling tools for PDEs, namely dense neural network\nbased methods like Physics Informed Neural Networks (PINNs) and traditional\nmesh-free numerical methods, thereby providing a novel means to develop a new\nclass of hybrid algorithms that build on the best of both these viewpoints. A\nunique feature of the SPINN model that distinguishes it from other neural\nnetwork based approximations proposed earlier is that it is (i) interpretable,\nin a particular sense made precise in the work, and (ii) sparse in the sense\nthat it has much fewer connections than typical dense neural networks used for\nPDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is\nable to handle discontinuities in the solutions. In addition, we demonstrate\nthat Fourier series representations can also be expressed as a special class of\nSPINN and propose generalized neural network analogues of Fourier\nrepresentations. We illustrate the utility of the proposed method with a\nvariety of examples involving ordinary differential equations, elliptic,\nparabolic, hyperbolic and nonlinear partial differential equations, and an\nexample in fluid dynamics.",
          "link": "http://arxiv.org/abs/2102.13037",
          "publishedOn": "2021-07-29T02:00:10.684Z",
          "wordCount": 718,
          "title": "SPINN: Sparse, Physics-based, and partially Interpretable Neural Networks for PDEs. (arXiv:2102.13037v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yunsong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stearrett_R/0/1/0/all/0/1\">Ryan Stearrett</a>",
          "description": "A cross-benchmark has been done on three critical aspects, data imputing,\nfeature selection and regression algorithms, for machine learning based\nchemical vapor deposition (CVD) virtual metrology (VM). The result reveals that\nlinear feature selection regression algorithm would extensively under-fit the\nVM data. Data imputing is also necessary to achieve a higher prediction\naccuracy as the data availability is only ~70% when optimal accuracy is\nobtained. This work suggests a nonlinear feature selection and regression\nalgorithm combined with nearest data imputing algorithm would provide a\nprediction accuracy as high as 0.7. This would lead to 70% reduced CVD\nprocessing variation, which is believed to will lead to reduced frequency of\nphysical metrology as well as more reliable mass-produced wafer with improved\nquality.",
          "link": "http://arxiv.org/abs/2107.05071",
          "publishedOn": "2021-07-29T02:00:10.616Z",
          "wordCount": 574,
          "title": "Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process. (arXiv:2107.05071v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pre-trained vision models such as VGG16 and ResNet\n\nIndex Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-07-29T02:00:10.346Z",
          "wordCount": 706,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waller_I/0/1/0/all/0/1\">Isaac Waller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Ashton Anderson</a>",
          "description": "Optimism about the Internet's potential to bring the world together has been\ntempered by concerns about its role in inflaming the 'culture wars'. Via mass\nselection into like-minded groups, online society may be becoming more\nfragmented and polarized, particularly with respect to partisan differences.\nHowever, our ability to measure the social makeup of online communities, and in\nturn understand the social organization of online platforms, is limited by the\npseudonymous, unstructured, and large-scale nature of digital discussion. We\ndevelop a neural embedding methodology to quantify the positioning of online\ncommunities along social dimensions by leveraging large-scale patterns of\naggregate behaviour. Applying our methodology to 5.1B Reddit comments made in\n10K communities over 14 years, we measure how the macroscale community\nstructure is organized with respect to age, gender, and U.S. political\npartisanship. Examining political content, we find Reddit underwent a\nsignificant polarization event around the 2016 U.S. presidential election, and\nremained highly polarized for years afterward. Contrary to conventional wisdom,\nhowever, individual-level polarization is rare; the system-level shift in 2016\nwas disproportionately driven by the arrival of new and newly political users.\nPolitical polarization on Reddit is unrelated to previous activity on the\nplatform, and is instead temporally aligned with external events. We also\nobserve a stark ideological asymmetry, with the sharp increase in 2016 being\nentirely attributable to changes in right-wing activity. Our methodology is\nbroadly applicable to the study of online interaction, and our findings have\nimplications for the design of online platforms, understanding the social\ncontexts of online behaviour, and quantifying the dynamics and mechanisms of\nonline polarization.",
          "link": "http://arxiv.org/abs/2010.00590",
          "publishedOn": "2021-07-29T02:00:10.318Z",
          "wordCount": 746,
          "title": "Quantifying social organization and political polarization in online platforms. (arXiv:2010.00590v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smedt_J/0/1/0/all/0/1\">Johannes De Smedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeshchenko_A/0/1/0/all/0/1\">Anton Yeshchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyvyanyy_A/0/1/0/all/0/1\">Artem Polyvyanyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerdt_J/0/1/0/all/0/1\">Jochen De Weerdt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendling_J/0/1/0/all/0/1\">Jan Mendling</a>",
          "description": "Process analytics is an umbrella of data-driven techniques which includes\nmaking predictions for individual process instances or overall process models.\nAt the instance level, various novel techniques have been recently devised,\ntackling next activity, remaining time, and outcome prediction. At the model\nlevel, there is a notable void. It is the ambition of this paper to fill this\ngap. To this end, we develop a technique to forecast the entire process model\nfrom historical event data. A forecasted model is a will-be process model\nrepresenting a probable future state of the overall process. Such a forecast\nhelps to investigate the consequences of drift and emerging bottlenecks. Our\ntechnique builds on a representation of event data as multiple time series,\neach capturing the evolution of a behavioural aspect of the process model, such\nthat corresponding forecasting techniques can be applied. Our implementation\ndemonstrates the accuracy of our technique on real-world event log data.",
          "link": "http://arxiv.org/abs/2105.01092",
          "publishedOn": "2021-07-29T02:00:10.311Z",
          "wordCount": 634,
          "title": "Process Model Forecasting Using Time Series Analysis of Event Sequence Data. (arXiv:2105.01092v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper argues that training GANs on local and non-local dependencies in\nspeech data offers insights into how deep neural networks discretize continuous\ndata and how symbolic-like rule-based morphophonological processes emerge in a\ndeep convolutional architecture. Acquisition of speech has recently been\nmodeled as a dependency between latent space and data generated by GANs in\nBegu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local\nallophonic distribution. We extend this approach to test learning of local and\nnon-local phonological processes that include approximations of morphological\nprocesses. We further parallel outputs of the model to results of a behavioral\nexperiment where human subjects are trained on the data used for training the\nGAN network. Four main conclusions emerge: (i) the networks provide useful\ninformation for computational models of speech acquisition even if trained on a\ncomparatively small dataset of an artificial grammar learning experiment; (ii)\nlocal processes are easier to learn than non-local processes, which matches\nboth behavioral data in human subjects and typology in the world's languages.\nThis paper also proposes (iii) how we can actively observe the network's\nprogress in learning and explore the effect of training steps on learning\nrepresentations by keeping latent space constant across different training\nsteps. Finally, this paper shows that (iv) the network learns to encode the\npresence of a prefix with a single latent variable; by interpolating this\nvariable, we can actively observe the operation of a non-local phonological\nprocess. The proposed technique for retrieving learning representations has\ngeneral implications for our understanding of how GANs discretize continuous\nspeech data and suggests that rule-like generalizations in the training data\nare represented as an interaction between variables in the network's latent\nspace.",
          "link": "http://arxiv.org/abs/2009.12711",
          "publishedOn": "2021-07-29T02:00:10.300Z",
          "wordCount": 761,
          "title": "Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks. (arXiv:2009.12711v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07757",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Musso_D/0/1/0/all/0/1\">Daniele Musso</a>",
          "description": "Local entropic loss functions provide a versatile framework to define\narchitecture-aware regularization procedures. Besides the possibility of being\nanisotropic in the synaptic space, the local entropic smoothening of the loss\nfunction can vary during training, thus yielding a tunable model complexity. A\nscoping protocol where the regularization is strong in the early-stage of the\ntraining and then fades progressively away constitutes an alternative to\nstandard initialization procedures for deep convolutional neural networks,\nnonetheless, it has wider applicability. We analyze anisotropic, local entropic\nsmoothenings in the language of statistical physics and information theory,\nproviding insight into both their interpretation and workings. We comment some\naspects related to the physics of renormalization and the spacetime structure\nof convolutional networks.",
          "link": "http://arxiv.org/abs/2107.07757",
          "publishedOn": "2021-07-29T02:00:10.291Z",
          "wordCount": 585,
          "title": "Entropic alternatives to initialization. (arXiv:2107.07757v2 [cond-mat.dis-nn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1\">Akshat Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "In this paper, we demonstrated a practical application of realistic river\nimage generation using deep learning. Specifically, we explored a generative\nadversarial network (GAN) model capable of generating high-resolution and\nrealistic river images that can be used to support modeling and analysis in\nsurface water estimation, river meandering, wetland loss, and other\nhydrological research studies. First, we have created an extensive repository\nof overhead river images to be used in training. Second, we incorporated the\nProgressive Growing GAN (PGGAN), a network architecture that iteratively trains\nsmaller-resolution GANs to gradually build up to a very high resolution to\ngenerate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\nGAN architectures, difficulties arose in terms of exponential increase of\ntraining time and vanishing/exploding gradient issues, which the PGGAN\nimplementation seemed to significantly reduce. The results presented in this\nstudy show great promise in generating high-quality images and capturing the\ndetails of river structure and flow to support hydrological research, which\noften requires extensive imagery for model performance.",
          "link": "http://arxiv.org/abs/2003.00826",
          "publishedOn": "2021-07-29T02:00:10.284Z",
          "wordCount": 644,
          "title": "Realistic River Image Synthesis using Deep Generative Adversarial Networks. (arXiv:2003.00826v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Clarice Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kathryn Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_A/0/1/0/all/0/1\">Andrew Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1\">Rashidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keya_K/0/1/0/all/0/1\">Kamrun Naher Keya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1\">James Foulds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shimei Pan</a>",
          "description": "Currently, there is a surge of interest in fair Artificial Intelligence (AI)\nand Machine Learning (ML) research which aims to mitigate discriminatory bias\nin AI algorithms, e.g. along lines of gender, age, and race. While most\nresearch in this domain focuses on developing fair AI algorithms, in this work,\nwe show that a fair AI algorithm on its own may be insufficient to achieve its\nintended results in the real world. Using career recommendation as a case\nstudy, we build a fair AI career recommender by employing gender debiasing\nmachine learning techniques. Our offline evaluation showed that the debiased\nrecommender makes fairer career recommendations without sacrificing its\naccuracy. Nevertheless, an online user study of more than 200 college students\nrevealed that participants on average prefer the original biased system over\nthe debiased system. Specifically, we found that perceived gender disparity is\na determining factor for the acceptance of a recommendation. In other words,\nour results demonstrate we cannot fully address the gender bias issue in AI\nrecommendations without addressing the gender bias in humans.",
          "link": "http://arxiv.org/abs/2106.07112",
          "publishedOn": "2021-07-29T02:00:10.232Z",
          "wordCount": 649,
          "title": "User Acceptance of Gender Stereotypes in Automated Career Recommendations. (arXiv:2106.07112v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09747",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Basak_S/0/1/0/all/0/1\">Subhasish Basak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Petit_S/0/1/0/all/0/1\">S&#xe9;bastien Petit</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bect_J/0/1/0/all/0/1\">Julien Bect</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vazquez_E/0/1/0/all/0/1\">Emmanuel Vazquez</a>",
          "description": "This article investigates the origin of numerical issues in maximum\nlikelihood parameter estimation for Gaussian process (GP) interpolation and\ninvestigates simple but effective strategies for improving commonly used\nopen-source software implementations. This work targets a basic problem but a\nhost of studies, particularly in the literature of Bayesian optimization, rely\non off-the-shelf GP implementations. For the conclusions of these studies to be\nreliable and reproducible, robust GP implementations are critical.",
          "link": "http://arxiv.org/abs/2101.09747",
          "publishedOn": "2021-07-29T02:00:10.225Z",
          "wordCount": 530,
          "title": "Numerical issues in maximum likelihood parameter estimation for Gaussian process interpolation. (arXiv:2101.09747v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1\">Tommaso R. Cesari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vecchia_R/0/1/0/all/0/1\">Riccardo Della Vecchia</a>",
          "description": "In this preliminary (and unpolished) version of the paper, we study an\nasynchronous online learning setting with a network of agents. At each time\nstep, some of the agents are activated, requested to make a prediction, and pay\nthe corresponding loss. Some feedback is then revealed to these agents and is\nlater propagated through the network. We consider the case of full, bandit, and\nsemi-bandit feedback. In particular, we construct a reduction to delayed\nsingle-agent learning that applies to both the full and the bandit feedback\ncase and allows to obtain regret guarantees for both settings. We complement\nthese results with a near-matching lower bound.",
          "link": "http://arxiv.org/abs/2106.04982",
          "publishedOn": "2021-07-29T02:00:10.218Z",
          "wordCount": 556,
          "title": "Cooperative Online Learning. (arXiv:2106.04982v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-07-29T02:00:10.197Z",
          "wordCount": 658,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.12278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castera_C/0/1/0/all/0/1\">Camille Castera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Bolte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a>",
          "description": "We introduce a new second-order inertial optimization method for machine\nlearning called INNA. It exploits the geometry of the loss function while only\nrequiring stochastic approximations of the function values and the generalized\ngradients. This makes INNA fully implementable and adapted to large-scale\noptimization problems such as the training of deep neural networks. The\nalgorithm combines both gradient-descent and Newton-like behaviors as well as\ninertia. We prove the convergence of INNA for most deep learning problems. To\ndo so, we provide a well-suited framework to analyze deep learning loss\nfunctions involving tame optimization in which we study a continuous dynamical\nsystem together with its discrete stochastic approximations. We prove sublinear\nconvergence for the continuous-time differential inclusion which underlies our\nalgorithm. Additionally, we also show how standard optimization mini-batch\nmethods applied to non-smooth non-convex problems can yield a certain type of\nspurious stationary points never discussed before. We address this issue by\nproviding a theoretical framework around the new idea of $D$-criticality; we\nthen give a simple asymptotic analysis of INNA. Our algorithm allows for using\nan aggressive learning rate of $o(1/\\log k)$. From an empirical viewpoint, we\nshow that INNA returns competitive results with respect to state of the art\n(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark\nproblems.",
          "link": "http://arxiv.org/abs/1905.12278",
          "publishedOn": "2021-07-29T02:00:10.190Z",
          "wordCount": 733,
          "title": "An Inertial Newton Algorithm for Deep Learning. (arXiv:1905.12278v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.01845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijun Xu</a>",
          "description": "One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online this http URL .",
          "link": "http://arxiv.org/abs/1907.01845",
          "publishedOn": "2021-07-29T02:00:10.158Z",
          "wordCount": 672,
          "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. (arXiv:1907.01845v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habibpour_M/0/1/0/all/0/1\">Maryam Habibpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharoun_H/0/1/0/all/0/1\">Hassan Gharoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdipour_M/0/1/0/all/0/1\">Mohammadreza Mehdipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafie_Khah_M/0/1/0/all/0/1\">Miadreza Shafie-Khah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catalao_J/0/1/0/all/0/1\">Joao P.S. Catalao</a>",
          "description": "Countless research works of deep neural networks (DNNs) in the task of credit\ncard fraud detection have focused on improving the accuracy of point\npredictions and mitigating unwanted biases by building different network\narchitectures or learning models. Quantifying uncertainty accompanied by point\nestimation is essential because it mitigates model unfairness and permits\npractitioners to develop trustworthy systems which abstain from suboptimal\ndecisions due to low confidence. Explicitly, assessing uncertainties associated\nwith DNNs predictions is critical in real-world card fraud detection settings\nfor characteristic reasons, including (a) fraudsters constantly change their\nstrategies, and accordingly, DNNs encounter observations that are not generated\nby the same process as the training distribution, (b) owing to the\ntime-consuming process, very few transactions are timely checked by\nprofessional experts to update DNNs. Therefore, this study proposes three\nuncertainty quantification (UQ) techniques named Monte Carlo dropout, ensemble,\nand ensemble Monte Carlo dropout for card fraud detection applied on\ntransaction data. Moreover, to evaluate the predictive uncertainty estimates,\nUQ confusion matrix and several performance metrics are utilized. Through\nexperimental results, we show that the ensemble is more effective in capturing\nuncertainty corresponding to generated predictions. Additionally, we\ndemonstrate that the proposed UQ methods provide extra insight to the point\npredictions, leading to elevate the fraud prevention process.",
          "link": "http://arxiv.org/abs/2107.13508",
          "publishedOn": "2021-07-29T02:00:10.077Z",
          "wordCount": 663,
          "title": "Uncertainty-Aware Credit Card Fraud Detection Using Deep Learning. (arXiv:2107.13508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13522",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Hasyim_M/0/1/0/all/0/1\">Muhammad R. Hasyim</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Batton_C/0/1/0/all/0/1\">Clay H. Batton</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Mandadapu_K/0/1/0/all/0/1\">Kranthi K. Mandadapu</a>",
          "description": "A central object in the computational studies of rare events is the committor\nfunction. Though costly to compute, the committor function encodes complete\nmechanistic information of the processes involving rare events, including\nreaction rates and transition-state ensembles. Under the framework of\ntransition path theory (TPT), recent work [1] proposes an algorithm where a\nfeedback loop couples a neural network that models the committor function with\nimportance sampling, mainly umbrella sampling, which collects data needed for\nadaptive training. In this work, we show additional modifications are needed to\nimprove the accuracy of the algorithm. The first modification adds elements of\nsupervised learning, which allows the neural network to improve its prediction\nby fitting to sample-mean estimates of committor values obtained from short\nmolecular dynamics trajectories. The second modification replaces the\ncommittor-based umbrella sampling with the finite-temperature string (FTS)\nmethod, which enables homogeneous sampling in regions where transition pathways\nare located. We test our modifications on low-dimensional systems with\nnon-convex potential energy where reference solutions can be found via\nanalytical or the finite element methods, and show how combining supervised\nlearning and the FTS method yields accurate computation of committor functions\nand reaction rates. We also provide an error analysis for algorithms that use\nthe FTS method, using which reaction rates can be accurately estimated during\ntraining with a small number of samples.",
          "link": "http://arxiv.org/abs/2107.13522",
          "publishedOn": "2021-07-29T02:00:10.028Z",
          "wordCount": 682,
          "title": "Supervised Learning and the Finite-Temperature String Method for Computing Committor Functions and Reaction Rates. (arXiv:2107.13522v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>",
          "description": "EEG-based emotion recognition often requires sufficient labeled training\nsamples to build an effective computational model. Labeling EEG data, on the\nother hand, is often expensive and time-consuming. To tackle this problem and\nreduce the need for output labels in the context of EEG-based emotion\nrecognition, we propose a semi-supervised pipeline to jointly exploit both\nunlabeled and labeled data for learning EEG representations. Our\nsemi-supervised framework consists of both unsupervised and supervised\ncomponents. The unsupervised part maximizes the consistency between original\nand reconstructed input data using an autoencoder, while simultaneously the\nsupervised part minimizes the cross-entropy between the input and output\nlabels. We evaluate our framework using both a stacked autoencoder and an\nattention-based recurrent autoencoder. We test our framework on the large-scale\nSEED EEG dataset and compare our results with several other popular\nsemi-supervised methods. Our semi-supervised framework with a deep\nattention-based recurrent autoencoder consistently outperforms the benchmark\nmethods, even when small sub-sets (3\\%, 5\\% and 10\\%) of the output labels are\navailable during training, achieving a new state-of-the-art semi-supervised\nperformance.",
          "link": "http://arxiv.org/abs/2107.13505",
          "publishedOn": "2021-07-29T02:00:10.020Z",
          "wordCount": 618,
          "title": "Deep Recurrent Semi-Supervised EEG Representation Learning for Emotion Recognition. (arXiv:2107.13505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helou_B/0/1/0/all/0/1\">Bassam Helou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusi_A/0/1/0/all/0/1\">Aditya Dusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collin_A/0/1/0/all/0/1\">Anne Collin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdipour_N/0/1/0/all/0/1\">Noushin Mehdipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lizarazo_C/0/1/0/all/0/1\">Cristhian Lizarazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belta_C/0/1/0/all/0/1\">Calin Belta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wongpiromsarn_T/0/1/0/all/0/1\">Tichakorn Wongpiromsarn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tebbens_R/0/1/0/all/0/1\">Radboud Duintjer Tebbens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>",
          "description": "Autonomous vehicles must balance a complex set of objectives. There is no\nconsensus on how they should do so, nor on a model for specifying a desired\ndriving behavior. We created a dataset to help address some of these questions\nin a limited operating domain. The data consists of 92 traffic scenarios, with\nmultiple ways of traversing each scenario. Multiple annotators expressed their\npreference between pairs of scenario traversals. We used the data to compare an\ninstance of a rulebook, carefully hand-crafted independently of the dataset,\nwith several interpretable machine learning models such as Bayesian networks,\ndecision trees, and logistic regression trained on the dataset. To compare\ndriving behavior, these models use scores indicating by how much different\nscenario traversals violate each of 14 driving rules. The rules are\ninterpretable and designed by subject-matter experts. First, we found that\nthese rules were enough for these models to achieve a high classification\naccuracy on the dataset. Second, we found that the rulebook provides high\ninterpretability without excessively sacrificing performance. Third, the data\npointed to possible improvements in the rulebook and the rules, and to\npotential new rules. Fourth, we explored the interpretability vs performance\ntrade-off by also training non-interpretable models such as a random forest.\nFinally, we make the dataset publicly available to encourage a discussion from\nthe wider community on behavior specification for AVs. Please find it at\ngithub.com/bassam-motional/Reasonable-Crowd.",
          "link": "http://arxiv.org/abs/2107.13507",
          "publishedOn": "2021-07-29T02:00:09.974Z",
          "wordCount": 693,
          "title": "The Reasonable Crowd: Towards evidence-based and interpretable models of driving behavior. (arXiv:2107.13507v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1901.09997",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Berahas_A/0/1/0/all/0/1\">Albert S. Berahas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jahani_M/0/1/0/all/0/1\">Majid Jahani</a>, <a href=\"http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Takac_M/0/1/0/all/0/1\">Martin Tak&#xe1;&#x10d;</a>",
          "description": "We present two sampled quasi-Newton methods (sampled LBFGS and sampled LSR1)\nfor solving empirical risk minimization problems that arise in machine\nlearning. Contrary to the classical variants of these methods that sequentially\nbuild Hessian or inverse Hessian approximations as the optimization progresses,\nour proposed methods sample points randomly around the current iterate at every\niteration to produce these approximations. As a result, the approximations\nconstructed make use of more reliable (recent and local) information, and do\nnot depend on past iterate information that could be significantly stale. Our\nproposed algorithms are efficient in terms of accessed data points (epochs) and\nhave enough concurrency to take advantage of parallel/distributed computing\nenvironments. We provide convergence guarantees for our proposed methods.\nNumerical tests on a toy classification problem as well as on popular\nbenchmarking binary classification and neural network training tasks reveal\nthat the methods outperform their classical variants.",
          "link": "http://arxiv.org/abs/1901.09997",
          "publishedOn": "2021-07-29T02:00:09.967Z",
          "wordCount": 641,
          "title": "Quasi-Newton Methods for Machine Learning: Forget the Past, Just Sample. (arXiv:1901.09997v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Charles Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbik_J/0/1/0/all/0/1\">J&#x119;drzej Orbik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devin_C/0/1/0/all/0/1\">Coline Devin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Brian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1\">Glen Berseth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "In this paper, we study how robots can autonomously learn skills that require\na combination of navigation and grasping. Learning robotic skills in the real\nworld remains challenging without large-scale data collection and supervision.\nOur aim is to devise a robotic reinforcement learning system for learning\nnavigation and manipulation together, in an \\textit{autonomous} way without\nhuman intervention, enabling continual learning under realistic assumptions.\nSpecifically, our system, ReLMM, can learn continuously on a real-world\nplatform without any environment instrumentation, without human intervention,\nand without access to privileged information, such as maps, objects positions,\nor a global view of the environment. Our method employs a modularized policy\nwith components for manipulation and navigation, where uncertainty over the\nmanipulation success drives exploration for the navigation controller, and the\nmanipulation module provides rewards for navigation. We evaluate our method on\na room cleanup task, where the robot must navigate to and pick up items of\nscattered on the floor. After a grasp curriculum training phase, ReLMM can\nlearn navigation and grasping together fully automatically, in around 40 hours\nof real-world training.",
          "link": "http://arxiv.org/abs/2107.13545",
          "publishedOn": "2021-07-29T02:00:09.959Z",
          "wordCount": 626,
          "title": "ReLMM: Practical RL for Learning Mobile Manipulation Skills Using Only Onboard Sensors. (arXiv:2107.13545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13473",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Valenchon_N/0/1/0/all/0/1\">Nicolas Valenchon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouteiller_Y/0/1/0/all/0/1\">Yann Bouteiller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jourde_H/0/1/0/all/0/1\">Hugo R. Jourde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coffey_E/0/1/0/all/0/1\">Emily B.J. Coffey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beltrame_G/0/1/0/all/0/1\">Giovanni Beltrame</a>",
          "description": "Electroencephalography (EEG) is a method of measuring the brain's electrical\nactivity, using non-invasive scalp electrodes. In this article, we propose the\nPortiloop, a deep learning-based portable and low-cost device enabling the\nneuroscience community to capture EEG, process it in real time, detect patterns\nof interest, and respond with precisely-timed stimulation. The core of the\nPortiloop is a System on Chip composed of an Analog to Digital Converter (ADC)\nand a Field-Programmable Gate Array (FPGA). After being converted to digital by\nthe ADC, the EEG signal is processed in the FPGA. The FPGA contains an ad-hoc\nArtificial Neural Network (ANN) with convolutional and recurrent units,\ndirectly implemented in hardware. The output of the ANN is then used to trigger\nthe user-defined feedback. We use the Portiloop to develop a real-time sleep\nspindle stimulating application, as a case study. Sleep spindles are a specific\ntype of transient oscillation ($\\sim$2.5 s, 12-16 Hz) that are observed in EEG\nrecordings, and are related to memory consolidation during sleep. We tested the\nPortiloop's capacity to detect and stimulate sleep spindles in real time using\nan existing database of EEG sleep recordings. With 71% for both precision and\nrecall as compared with expert labels, the system is able to stimulate spindles\nwithin $\\sim$300 ms of their onset, enabling experimental manipulation of early\nthe entire spindle. The Portiloop can be extended to detect and stimulate other\nneural events in EEG. It is fully available to the research community as an\nopen science project.",
          "link": "http://arxiv.org/abs/2107.13473",
          "publishedOn": "2021-07-29T02:00:09.840Z",
          "wordCount": 721,
          "title": "The Portiloop: a deep learning-based open science tool for closed-loop brain stimulation. (arXiv:2107.13473v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/1811.11891",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koelle_S/0/1/0/all/0/1\">Samson Koelle</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Hanyu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meila</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>",
          "description": "Manifold embedding algorithms map high-dimensional data down to coordinates\nin a much lower-dimensional space. One of the aims of dimension reduction is to\nfind intrinsic coordinates that describe the data manifold. The coordinates\nreturned by the embedding algorithm are abstract, and finding their physical or\ndomain-related meaning is not formalized and often left to domain experts. This\npaper studies the problem of recovering the meaning of the new low-dimensional\nrepresentation in an automatic, principled fashion. We propose a method to\nexplain embedding coordinates of a manifold as non-linear compositions of\nfunctions from a user-defined dictionary. We show that this problem can be set\nup as a sparse linear Group Lasso recovery problem, find sufficient recovery\nconditions, and demonstrate its effectiveness on data.",
          "link": "http://arxiv.org/abs/1811.11891",
          "publishedOn": "2021-07-29T02:00:09.832Z",
          "wordCount": 567,
          "title": "Manifold Coordinates with Physical Meaning. (arXiv:1811.11891v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Ying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jong-Shi Pang</a>",
          "description": "The non-negative matrix factorization (NMF) model with an additional\northogonality constraint on one of the factor matrices, called the orthogonal\nNMF (ONMF), has been found a promising clustering model and can outperform the\nclassical K-means. However, solving the ONMF model is a challenging\noptimization problem because the coupling of the orthogonality and\nnon-negativity constraints introduces a mixed combinatorial aspect into the\nproblem due to the determination of the correct status of the variables\n(positive or zero). Most of the existing methods directly deal with the\northogonality constraint in its original form via various optimization\ntechniques, but are not scalable for large-scale problems. In this paper, we\npropose a new ONMF based clustering formulation that equivalently transforms\nthe orthogonality constraint into a set of norm-based non-convex equality\nconstraints. We then apply a non-convex penalty (NCP) approach to add them to\nthe objective as penalty terms, leading to a problem that is efficiently\nsolvable. One smooth penalty formulation and one non-smooth penalty formulation\nare respectively studied. We build theoretical conditions for the penalized\nproblems to provide feasible stationary solutions to the ONMF based clustering\nproblem, as well as proposing efficient algorithms for solving the penalized\nproblems of the two NCP methods. Experimental results based on both synthetic\nand real datasets are presented to show that the proposed NCP methods are\ncomputationally time efficient, and either match or outperform the existing\nK-means and ONMF based methods in terms of the clustering performance.",
          "link": "http://arxiv.org/abs/1906.00570",
          "publishedOn": "2021-07-29T02:00:09.825Z",
          "wordCount": 732,
          "title": "Clustering by Orthogonal NMF Model and Non-Convex Penalty Optimization. (arXiv:1906.00570v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polgreen_E/0/1/0/all/0/1\">Elizabeth Polgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_A/0/1/0/all/0/1\">Andrew Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1\">Sanjit A. Seshia</a>",
          "description": "In classic program synthesis algorithms, such as counterexample-guided\ninductive synthesis (CEGIS), the algorithms alternate between a synthesis phase\nand an oracle (verification) phase. Many synthesis algorithms use a white-box\noracle based on satisfiability modulo theory (SMT) solvers to provide\ncounterexamples. But what if a white-box oracle is either not available or not\neasy to work with? We present a framework for solving a general class of\noracle-guided synthesis problems which we term synthesis modulo oracles. In\nthis setting, oracles may be black boxes with a query-response interface\ndefined by the synthesis problem. As a necessary component of this framework,\nwe also formalize the problem of satisfiability modulo theories and oracles,\nand present an algorithm for solving this problem. We implement a prototype\nsolver for satisfiability and synthesis modulo oracles and demonstrate that, by\nusing oracles that execute functions not easily modeled in SMT-constraints,\nsuch as recursive functions or oracles that incorporate compilation and\nexecution of code, SMTO and SyMO are able to solve problems beyond the\nabilities of standard SMT and synthesis solvers.",
          "link": "http://arxiv.org/abs/2107.13477",
          "publishedOn": "2021-07-29T02:00:09.813Z",
          "wordCount": 613,
          "title": "Satisfiability and Synthesis Modulo Oracles. (arXiv:2107.13477v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1\">Lorenz Kummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidak_K/0/1/0/all/0/1\">Kevin Sidak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichmann_T/0/1/0/all/0/1\">Tabea Reichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gansterer_W/0/1/0/all/0/1\">Wilfried Gansterer</a>",
          "description": "Quantization is a technique for reducing deep neural networks (DNNs) training\nand inference times, which is crucial for training in resource constrained\nenvironments or time critical inference applications. State-of-the-art (SOTA)\nquantization approaches focus on post-training quantization, i.e. quantization\nof pre-trained DNNs for speeding up inference. Very little work on quantized\ntraining exists, which neither al-lows dynamic intra-epoch precision switches\nnor em-ploys an information theory based switching heuristic. Usually, existing\napproaches require full precision refinement afterwards and enforce a global\nword length across the whole DNN. This leads to suboptimal quantization\nmappings and resource usage. Recognizing these limits, we introduce MARViN, a\nnew quantized training strategy using information theory-based intra-epoch\nprecision switching, which decides on a per-layer basis which precision should\nbe used in order to minimize quantization-induced information loss. Note that\nany quantization must leave enough precision such that future learning steps do\nnot suffer from vanishing gradients. We achieve an average speedup of 1.86\ncompared to a float32 basis while limiting mean accuracy degradation on\nAlexNet/ResNet to only -0.075%.",
          "link": "http://arxiv.org/abs/2107.13490",
          "publishedOn": "2021-07-29T02:00:09.804Z",
          "wordCount": 612,
          "title": "MARViN -- Multiple Arithmetic Resolutions Vacillating in Neural Networks. (arXiv:2107.13490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogozhnikov_A/0/1/0/all/0/1\">Alex Rogozhnikov</a>",
          "description": "Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.",
          "link": "http://arxiv.org/abs/2106.03143",
          "publishedOn": "2021-07-29T02:00:09.699Z",
          "wordCount": 607,
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings. (arXiv:2106.03143v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1\">J. K. Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1\">Mario Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwis_K/0/1/0/all/0/1\">Kusal De Alwis</a>",
          "description": "The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.",
          "link": "http://arxiv.org/abs/2103.01205",
          "publishedOn": "2021-07-29T02:00:09.681Z",
          "wordCount": 691,
          "title": "Statistically Significant Stopping of Neural Network Training. (arXiv:2103.01205v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Timothy Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozier_J/0/1/0/all/0/1\">Jamell Dozier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1\">Helen Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1\">Nishchal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fr&#xe9;do Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Object recognition and viewpoint estimation lie at the heart of visual\nunderstanding. Recent works suggest that convolutional neural networks (CNNs)\nfail to generalize to out-of-distribution (OOD) category-viewpoint\ncombinations, ie. combinations not seen during training. In this paper, we\ninvestigate when and how such OOD generalization may be possible by evaluating\nCNNs trained to classify both object category and 3D viewpoint on OOD\ncombinations, and identifying the neural mechanisms that facilitate such OOD\ngeneralization. We show that increasing the number of in-distribution\ncombinations (ie. data diversity) substantially improves generalization to OOD\ncombinations, even with the same amount of training data. We compare learning\ncategory and viewpoint in separate and shared network architectures, and\nobserve starkly different trends on in-distribution and OOD combinations, ie.\nwhile shared networks are helpful in-distribution, separate networks\nsignificantly outperform shared ones at OOD combinations. Finally, we\ndemonstrate that such OOD generalization is facilitated by the neural mechanism\nof specialization, ie. the emergence of two types of neurons -- neurons\nselective to category and invariant to viewpoint, and vice versa.",
          "link": "http://arxiv.org/abs/2007.08032",
          "publishedOn": "2021-07-29T02:00:09.674Z",
          "wordCount": 654,
          "title": "When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?. (arXiv:2007.08032v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenxuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiping Huang</a>",
          "description": "The geometric structure of an optimization landscape is argued to be\nfundamentally important to support the success of deep neural network learning.\nA direct computation of the landscape beyond two layers is hard. Therefore, to\ncapture the global view of the landscape, an interpretable model of the\nnetwork-parameter (or weight) space must be established. However, the model is\nlacking so far. Furthermore, it remains unknown what the landscape looks like\nfor deep networks of binary synapses, which plays a key role in robust and\nenergy efficient neuromorphic computation. Here, we propose a statistical\nmechanics framework by directly building a least structured model of the\nhigh-dimensional weight space, considering realistic structured data,\nstochastic gradient descent training, and the computational depth of neural\nnetworks. We also consider whether the number of network parameters outnumbers\nthe number of supplied training data, namely, over- or under-parametrization.\nOur least structured model reveals that the weight spaces of the\nunder-parametrization and over-parameterization cases belong to the same class,\nin the sense that these weight spaces are well-connected without any\nhierarchical clustering structure. In contrast, the shallow-network has a\nbroken weight space, characterized by a discontinuous phase transition, thereby\nclarifying the benefit of depth in deep learning from the angle of high\ndimensional geometry. Our effective model also reveals that inside a deep\nnetwork, there exists a liquid-like central part of the architecture in the\nsense that the weights in this part behave as randomly as possible, providing\nalgorithmic implications. Our data-driven model thus provides a statistical\nmechanics insight about why deep learning is unreasonably effective in terms of\nthe high-dimensional weight space, and how deep networks are different from\nshallow ones.",
          "link": "http://arxiv.org/abs/2007.08093",
          "publishedOn": "2021-07-29T02:00:09.666Z",
          "wordCount": 754,
          "title": "Data-driven effective model shows a liquid-like deep learning. (arXiv:2007.08093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning (ML) algorithms has raised\nthe number of studies including their applicability in a variety of different\nscenarios. Among all, one of the hardest ones is the aerospace, due to its\npeculiar physical requirements. In this context, a feasibility study and a\nfirst prototype for an Artificial Intelligence (AI) model to be deployed on\nboard satellites are presented in this work. As a case study, the detection of\nvolcanic eruptions has been investigated as a method to swiftly produce alerts\nand allow immediate interventions. Two Convolutional Neural Networks (CNNs)\nhave been proposed and designed, showing how to efficiently implement them for\nidentifying the eruptions and at the same time adapting their complexity in\norder to fit on board requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-07-29T02:00:09.647Z",
          "wordCount": 602,
          "title": "On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1808.09670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fouillen_E/0/1/0/all/0/1\">Erwan Fouillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_C/0/1/0/all/0/1\">Claire Boyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1\">Maxime Sangnier</a>",
          "description": "Gradient boosting is a prediction method that iteratively combines weak\nlearners to produce a complex and accurate model. From an optimization point of\nview, the learning procedure of gradient boosting mimics a gradient descent on\na functional variable. This paper proposes to build upon the proximal point\nalgorithm, when the empirical risk to minimize is not differentiable, in order\nto introduce a novel boosting approach, called proximal boosting. Besides being\nmotivated by non-differentiable optimization, the proposed algorithm benefits\nfrom algorithmic improvements such as controlling the approximation error and\nNesterov's acceleration, in the same way as gradient boosting [Grubb and\nBagnell, 2011, Biau et al., 2018]. This leads to two variants, respectively\ncalled residual proximal boosting and accelerated proximal boosting.\nTheoretical convergence is proved for the first two procedures under different\nhypotheses on the empirical risk and advantages of leveraging proximal methods\nfor boosting are illustrated by numerical experiments on simulated and\nreal-world data. In particular, we exhibit a favorable comparison over gradient\nboosting regarding convergence rate and prediction accuracy.",
          "link": "http://arxiv.org/abs/1808.09670",
          "publishedOn": "2021-07-29T02:00:09.640Z",
          "wordCount": 631,
          "title": "Proximal boosting and variants. (arXiv:1808.09670v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammoudeh_A/0/1/0/all/0/1\">Ahmad Hammoudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedmori_S/0/1/0/all/0/1\">Sara Tedmori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obeid_N/0/1/0/all/0/1\">Nadim Obeid</a>",
          "description": "Although learning from data is effective and has achieved significant\nmilestones, it has many challenges and limitations. Learning from data starts\nfrom observations and then proceeds to broader generalizations. This framework\nis controversial in science, yet it has achieved remarkable engineering\nsuccesses. This paper reflects on some epistemological issues and some of the\nlimitations of the knowledge discovered in data. The document discusses the\ncommon perception that getting more data is the key to achieving better machine\nlearning models from theoretical and practical perspectives. The paper sheds\nsome light on the shortcomings of using generic mathematical theories to\ndescribe the process. It further highlights the need for theories specialized\nin learning from data. While more data leverages the performance of machine\nlearning models in general, the relation in practice is shown to be logarithmic\nat its best; After a specific limit, more data stabilize or degrade the machine\nlearning models. Recent work in reinforcement learning showed that the trend is\nshifting away from data-oriented approaches and relying more on algorithms. The\npaper concludes that learning from data is hindered by many limitations. Hence\nan approach that has an intensional orientation is needed.",
          "link": "http://arxiv.org/abs/2107.13270",
          "publishedOn": "2021-07-29T02:00:09.633Z",
          "wordCount": 626,
          "title": "A Reflection on Learning from Data: Epistemology Issues and Limitations. (arXiv:2107.13270v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_R/0/1/0/all/0/1\">Raluca Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepecki_J/0/1/0/all/0/1\">Jaroslaw Rzepecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuniga_E/0/1/0/all/0/1\">Evelyn Zuniga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costello_G/0/1/0/all/0/1\">Gavin Costello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1\">Guy Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_A/0/1/0/all/0/1\">Ali Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>",
          "description": "A key challenge on the path to developing agents that learn complex\nhuman-like behavior is the need to quickly and accurately quantify\nhuman-likeness. While human assessments of such behavior can be highly\naccurate, speed and scalability are limited. We address these limitations\nthrough a novel automated Navigation Turing Test (ANTT) that learns to predict\nhuman judgments of human-likeness. We demonstrate the effectiveness of our\nautomated NTT on a navigation task in a complex 3D environment. We investigate\nsix classification models to shed light on the types of architectures best\nsuited to this task, and validate them against data collected through a human\nNTT. Our best models achieve high accuracy when distinguishing true human and\nagent behavior. At the same time, we show that predicting finer-grained human\nassessment of agents' progress towards human-like behavior remains unsolved.\nOur work takes an important step towards agents that more effectively learn\ncomplex human-like behavior.",
          "link": "http://arxiv.org/abs/2105.09637",
          "publishedOn": "2021-07-29T02:00:09.626Z",
          "wordCount": 660,
          "title": "Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation. (arXiv:2105.09637v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Farwa K. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanagan_A/0/1/0/all/0/1\">Adrian Flanagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kuan E. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamgir_Z/0/1/0/all/0/1\">Zareen Alamgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammad_Ud_Din_M/0/1/0/all/0/1\">Muhammad Ammad-Ud-Din</a>",
          "description": "We introduce the payload optimization method for federated recommender\nsystems (FRS). In federated learning (FL), the global model payload that is\nmoved between the server and users depends on the number of items to recommend.\nThe model payload grows when there is an increasing number of items. This\nbecomes challenging for an FRS if it is running in production mode. To tackle\nthe payload challenge, we formulated a multi-arm bandit solution that selected\npart of the global model and transmitted it to all users. The selection process\nwas guided by a novel reward function suitable for FL systems. So far as we are\naware, this is the first optimization method that seeks to address item\ndependent payloads. The method was evaluated using three benchmark\nrecommendation datasets. The empirical validation confirmed that the proposed\nmethod outperforms the simpler methods that do not benefit from the bandits for\nthe purpose of item selection. In addition, we have demonstrated the usefulness\nof our proposed method by rigorously evaluating the effects of a payload\nreduction on the recommendation performance degradation. Our method achieved up\nto a 90\\% reduction in model payload, yielding only a $\\sim$4\\% - 8\\% loss in\nthe recommendation performance for highly sparse datasets",
          "link": "http://arxiv.org/abs/2107.13078",
          "publishedOn": "2021-07-29T02:00:09.606Z",
          "wordCount": 674,
          "title": "A Payload Optimization Method for Federated Recommender Systems. (arXiv:2107.13078v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feeney_P/0/1/0/all/0/1\">Patrick Feeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>",
          "description": "The pixelwise reconstruction error of deep autoencoders is often utilized for\nimage novelty detection and localization under the assumption that pixels with\nhigh error indicate which parts of the input image are unfamiliar and therefore\nlikely to be novel. This assumed correlation between pixels with high\nreconstruction error and novel regions of input images has not been verified\nand may limit the accuracy of these methods. In this paper we utilize saliency\nmaps to evaluate whether this correlation exists. Saliency maps reveal directly\nhow much a change in each input pixel would affect reconstruction loss, while\neach pixel's reconstruction error may be attributed to many input pixels when\nlayers are fully connected. We compare saliency maps to reconstruction error\nmaps via qualitative visualizations as well as quantitative correspondence\nbetween the top K elements of the maps for both novel and normal images. Our\nresults indicate that reconstruction error maps do not closely correlate with\nthe importance of pixels in the input images, making them insufficient for\nnovelty localization.",
          "link": "http://arxiv.org/abs/2107.13379",
          "publishedOn": "2021-07-29T02:00:09.599Z",
          "wordCount": 606,
          "title": "Evaluating the Use of Reconstruction Error for Novelty Localization. (arXiv:2107.13379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Liangxiu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleerekoper_A/0/1/0/all/0/1\">Anthony Kleerekoper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tongle Hu</a>",
          "description": "Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.",
          "link": "http://arxiv.org/abs/2107.13277",
          "publishedOn": "2021-07-29T02:00:09.592Z",
          "wordCount": 707,
          "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery. (arXiv:2107.13277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1\">Shunmei Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_X/0/1/0/all/0/1\">Xiaoxiao Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chao Yan</a>",
          "description": "Edge computing enabled smart greenhouse is a representative application of\nInternet of Things technology, which can monitor the environmental information\nin real time and employ the information to contribute to intelligent\ndecision-making. In the process, anomaly detection for wireless sensor data\nplays an important role. However, traditional anomaly detection algorithms\noriginally designed for anomaly detection in static data have not properly\nconsidered the inherent characteristics of data stream produced by wireless\nsensor such as infiniteness, correlations and concept drift, which may pose a\nconsiderable challenge on anomaly detection based on data stream, and lead to\nlow detection accuracy and efficiency. First, data stream usually generates\nquickly which means that it is infinite and enormous, so any traditional\noff-line anomaly detection algorithm that attempts to store the whole dataset\nor to scan the dataset multiple times for anomaly detection will run out of\nmemory space. Second, there exist correlations among different data streams,\nwhich traditional algorithms hardly consider. Third, the underlying data\ngeneration process or data distribution may change over time. Thus, traditional\nanomaly detection algorithms with no model update will lose their effects.\nConsidering these issues, a novel method (called DLSHiForest) on basis of\nLocality-Sensitive Hashing and time window technique in this paper is proposed\nto solve these problems while achieving accurate and efficient detection.\nComprehensive experiments are executed using real-world agricultural greenhouse\ndataset to demonstrate the feasibility of our approach. Experimental results\nshow that our proposal is practicable in addressing challenges of traditional\nanomaly detection while ensuring accuracy and efficiency.",
          "link": "http://arxiv.org/abs/2107.13353",
          "publishedOn": "2021-07-29T02:00:09.585Z",
          "wordCount": 715,
          "title": "Fast Wireless Sensor Anomaly Detection based on Data Stream in Edge Computing Enabled Smart Greenhouse. (arXiv:2107.13353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1\">Ettore Merlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marhaba_M/0/1/0/all/0/1\">Mira Marhaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braiek_H/0/1/0/all/0/1\">Houssem Ben Braiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giuliano Antoniol</a>",
          "description": "Neural network test cases are meant to exercise different reasoning paths in\nan architecture and used to validate the prediction outcomes. In this paper, we\nintroduce \"computational profiles\" as vectors of neuron activation levels. We\ninvestigate the distribution of computational profile likelihood of metamorphic\ntest cases with respect to the likelihood distributions of training, test and\nerror control cases. We estimate the non-parametric probability densities of\nneuron activation levels for each distinct output class. Probabilities are\ninferred using training cases only, without any additional knowledge about\nmetamorphic test cases. Experiments are performed by training a network on the\nMNIST Fashion library of images and comparing prediction likelihoods with those\nobtained from error control-data and from metamorphic test cases. Experimental\nresults show that the distributions of computational profile likelihood for\ntraining and test cases are somehow similar, while the distribution of the\nrandom-noise control-data is always remarkably lower than the observed one for\nthe training and testing sets. In contrast, metamorphic test cases show a\nprediction likelihood that lies in an extended range with respect to training,\ntests, and random noise. Moreover, the presented approach allows the\nindependent assessment of different training classes and experiments to show\nthat some of the classes are more sensitive to misclassifying metamorphic test\ncases than other classes. In conclusion, metamorphic test cases represent very\naggressive tests for neural network architectures. Furthermore, since\nmetamorphic test cases force a network to misclassify those inputs whose\nlikelihood is similar to that of training cases, they could also be considered\nas adversarial attacks that evade defenses based on computational profile\nlikelihood evaluation.",
          "link": "http://arxiv.org/abs/2107.13491",
          "publishedOn": "2021-07-29T02:00:09.561Z",
          "wordCount": 717,
          "title": "Models of Computational Profiles to Study the Likelihood of DNN Metamorphic Test Cases. (arXiv:2107.13491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13530",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1\">Bethan Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>",
          "description": "We present a method for continual learning of speech representations for\nmultiple languages using self-supervised learning (SSL) and applying these for\nautomatic speech recognition. There is an abundance of unannotated speech, so\ncreating self-supervised representations from raw audio and finetuning on a\nsmall annotated datasets is a promising direction to build speech recognition\nsystems. Wav2vec models perform SSL on raw audio in a pretraining phase and\nthen finetune on a small fraction of annotated data. SSL models have produced\nstate of the art results for ASR. However, these models are very expensive to\npretrain with self-supervision. We tackle the problem of learning new language\nrepresentations continually from audio without forgetting a previous language\nrepresentation. We use ideas from continual learning to transfer knowledge from\na previous task to speed up pretraining a new language task. Our\ncontinual-wav2vec2 model can decrease pretraining times by 32% when learning a\nnew language task, and learn this new audio-language representation without\nforgetting previous language representation.",
          "link": "http://arxiv.org/abs/2107.13530",
          "publishedOn": "2021-07-29T02:00:09.543Z",
          "wordCount": 634,
          "title": "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition. (arXiv:2107.13530v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Daniel Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1\">Logan Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1\">Vasilis Syrgkanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>",
          "description": "Randomized experiments can be susceptible to selection bias due to potential\nnon-compliance by the participants. While much of the existing work has studied\ncompliance as a static behavior, we propose a game-theoretic model to study\ncompliance as dynamic behavior that may change over time. In rounds, a social\nplanner interacts with a sequence of heterogeneous agents who arrive with their\nunobserved private type that determines both their prior preferences across the\nactions (e.g., control and treatment) and their baseline rewards without taking\nany treatment. The planner provides each agent with a randomized recommendation\nthat may alter their beliefs and their action selection. We develop a novel\nrecommendation mechanism that views the planner's recommendation as a form of\ninstrumental variable (IV) that only affects an agents' action selection, but\nnot the observed rewards. We construct such IVs by carefully mapping the\nhistory -- the interactions between the planner and the previous agents -- to a\nrandom recommendation. Even though the initial agents may be completely\nnon-compliant, our mechanism can incentivize compliance over time, thereby\nenabling the estimation of the treatment effect of each treatment, and\nminimizing the cumulative regret of the planner whose goal is to identify the\noptimal treatment.",
          "link": "http://arxiv.org/abs/2107.10093",
          "publishedOn": "2021-07-29T02:00:09.515Z",
          "wordCount": 671,
          "title": "Incentivizing Compliance with Algorithmic Instruments. (arXiv:2107.10093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lingam_V/0/1/0/all/0/1\">Vijay Lingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragesh_R/0/1/0/all/0/1\">Rahul Ragesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Arun Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellamanickam_S/0/1/0/all/0/1\">Sundararajan Sellamanickam</a>",
          "description": "Graph Neural Networks (GNNs) exhibit excellent performance when graphs have\nstrong homophily property, i.e. connected nodes have the same labels. However,\nthey perform poorly on heterophilic graphs. Several approaches address the\nissue of heterophily by proposing models that adapt the graph by optimizing\ntask-specific loss function using labelled data. These adaptations are made\neither via attention or by attenuating or enhancing various\nlow-frequency/high-frequency signals, as needed for the task at hand. More\nrecent approaches adapt the eigenvalues of the graph. One important\ninterpretation of this adaptation is that these models select/weigh the\neigenvectors of the graph. Based on this interpretation, we present an\neigendecomposition based approach and propose EigenNetwork models that improve\nthe performance of GNNs on heterophilic graphs. Performance improvement is\nachieved by learning flexible graph adaptation functions that modulate the\neigenvalues of the graph. Regularization of these functions via parameter\nsharing helps to improve the performance even more. Our approach achieves up to\n11% improvement in performance over the state-of-the-art methods on\nheterophilic graphs.",
          "link": "http://arxiv.org/abs/2107.13312",
          "publishedOn": "2021-07-29T02:00:09.507Z",
          "wordCount": 610,
          "title": "Effective Eigendecomposition based Graph Adaptation for Heterophilic Networks. (arXiv:2107.13312v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_A/0/1/0/all/0/1\">Anusua Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desbiens_J/0/1/0/all/0/1\">Jocelyn Desbiens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_R/0/1/0/all/0/1\">Ron Gross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Purpose: To demonstrate that retinal microvasculature per se is a reliable\nbiomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular\ndiseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to\ncolor fundus images for semantic segmentation of the blood vessels and severity\nclassification on both vascular and full images. Vessel reconstruction through\nharmonic descriptors is also used as a smoothing and de-noising tool. The\nmathematical background of the theory is also outlined. Results: For diabetic\npatients, at least 93.8% of DR No-Refer vs. Refer classification can be related\nto vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening\ncase, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the\ndisease biomarkers are related topologically to the vasculature. Translational\nRelevance: Experiments conducted on eye blood vasculature reconstruction as a\nbiomarker shows a strong correlation between vasculature shape and later stages\nof DR.",
          "link": "http://arxiv.org/abs/2107.13157",
          "publishedOn": "2021-07-29T02:00:09.498Z",
          "wordCount": 603,
          "title": "Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases. (arXiv:2107.13157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindra Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinyu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaolong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.",
          "link": "http://arxiv.org/abs/2107.13389",
          "publishedOn": "2021-07-29T02:00:09.477Z",
          "wordCount": 612,
          "title": "SimROD: A Simple Adaptation Method for Robust Object Detection. (arXiv:2107.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Devi_T/0/1/0/all/0/1\">Thangjam Clarinda Devi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thaoroijam_K/0/1/0/all/0/1\">Kabita Thaoroijam</a>",
          "description": "This paper presents a vowel-based dialect identification system for\nMeeteilon. For this work, a vowel dataset is created by using Meeteilon Speech\nCorpora available at Linguistic Data Consortium for Indian Languages (LDC-IL).\nSpectral features such as formant frequencies (F1, F1 and F3) and prosodic\nfeatures such as pitch (F0), energy, intensity and segment duration values are\nextracted from monophthong vowel sounds. Random forest classifier, a decision\ntree-based ensemble algorithm is used for classification of three major\ndialects of Meeteilon namely, Imphal, Kakching and Sekmai. Model has shown an\naverage dialect identification performance in terms of accuracy of around\n61.57%. The role of spectral and prosodic features are found to be significant\nin Meeteilon dialect classification.",
          "link": "http://arxiv.org/abs/2107.13419",
          "publishedOn": "2021-07-29T02:00:09.466Z",
          "wordCount": 579,
          "title": "Vowel-based Meeteilon dialect identification using a Random Forest classifier. (arXiv:2107.13419v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patrick_Evans_J/0/1/0/all/0/1\">James Patrick-Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannehl_M/0/1/0/all/0/1\">Moritz Dannehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinder_J/0/1/0/all/0/1\">Johannes Kinder</a>",
          "description": "Reverse engineers would benefit from identifiers like function names, but\nthese are usually unavailable in binaries. Training a machine learning model to\npredict function names automatically is promising but fundamentally hard due to\nthe enormous number of classes. In this paper, we introduce eXtreme Function\nLabeling (XFL), an extreme multi-label learning approach to selecting\nappropriate labels for binary functions. XFL splits function names into tokens,\ntreating each as an informative label akin to the problem of tagging texts in\nnatural language. To capture the semantics of binary code, we introduce DEXTER,\na novel function embedding that combines static analysis-based features with\nlocal context from the call graph and global context from the entire binary. We\ndemonstrate that XFL outperforms state-of-the-art approaches to function\nlabeling on a dataset of over 10,000 binaries from the Debian project,\nachieving a precision of 82.5%. We also study combinations of XFL with\ndifferent published embeddings for binary functions and show that DEXTER\nconsistently improves over the state of the art in information gain. As a\nresult, we are able to show that binary function labeling is best phrased in\nterms of multi-label learning, and that binary function embeddings benefit from\nmoving beyond just learning from syntax.",
          "link": "http://arxiv.org/abs/2107.13404",
          "publishedOn": "2021-07-29T02:00:09.448Z",
          "wordCount": 631,
          "title": "XFL: eXtreme Function Labeling. (arXiv:2107.13404v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_G/0/1/0/all/0/1\">Gary G. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_V/0/1/0/all/0/1\">Vincent S. Tseng</a>",
          "description": "Arrhythmia detection from ECG is an important research subject in the\nprevention and diagnosis of cardiovascular diseases. The prevailing studies\nformulate arrhythmia detection from ECG as a time series classification\nproblem. Meanwhile, early detection of arrhythmia presents a real-world demand\nfor early prevention and diagnosis. In this paper, we address a problem of\ncardiovascular disease early classification, which is a varied-length and\nlong-length time series early classification problem as well. For solving this\nproblem, we propose a deep reinforcement learning-based framework, namely\nSnippet Policy Network (SPN), consisting of four modules, snippet generator,\nbackbone network, controlling agent, and discriminator. Comparing to the\nexisting approaches, the proposed framework features flexible input length,\nsolves the dual-optimization solution of the earliness and accuracy goals.\nExperimental results demonstrate that SPN achieves an excellent performance of\nover 80\\% in terms of accuracy. Compared to the state-of-the-art methods, at\nleast 7% improvement on different metrics, including the precision, recall,\nF1-score, and harmonic mean, is delivered by the proposed SPN. To the best of\nour knowledge, this is the first work focusing on solving the cardiovascular\nearly classification problem based on varied-length ECG data. Based on these\nexcellent features from SPN, it offers a good exemplification for addressing\nall kinds of varied-length time series early classification problems.",
          "link": "http://arxiv.org/abs/2107.13361",
          "publishedOn": "2021-07-29T02:00:09.439Z",
          "wordCount": 646,
          "title": "Snippet Policy Network for Multi-class Varied-length ECG Early Classification. (arXiv:2107.13361v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13394",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Faruqui_S/0/1/0/all/0/1\">Syed Hasib Akhter Faruqui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alaeddini_A/0/1/0/all/0/1\">Adel Alaeddini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fisher_Hoch_S/0/1/0/all/0/1\">Susan P Fisher-Hoch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mccormic_J/0/1/0/all/0/1\">Joseph B Mccormic</a>",
          "description": "The emergence and progression of multiple chronic conditions (MCC) over time\noften form a dynamic network that depends on patient's modifiable risk factors\nand their interaction with non-modifiable risk factors and existing conditions.\nContinuous time Bayesian networks (CTBNs) are effective methods for modeling\nthe complex network of MCC relationships over time. However, CTBNs are not able\nto effectively formulate the dynamic impact of patient's modifiable risk\nfactors on the emergence and progression of MCC. Considering a functional CTBN\n(FCTBN) to represent the underlying structure of the MCC relationships with\nrespect to individuals' risk factors and existing conditions, we propose a\nnonlinear state-space model based on Extended Kalman filter (EKF) to capture\nthe dynamics of the patients' modifiable risk factors and existing conditions\non the MCC evolution over time. We also develop a tensor control chart to\ndynamically monitor the effect of changes in the modifiable risk factors of\nindividual patients on the risk of new chronic conditions emergence. We\nvalidate the proposed approach based on a combination of simulation and real\ndata from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC)\nover multiple years. The dataset examines the emergence of 5 chronic conditions\n(Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension)\nbased on 4 modifiable risk factors representing lifestyle behaviors (Diet,\nExercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors,\nincluding demographic information (Age, Gender, Education). The results\ndemonstrate the effectiveness of the proposed methodology for dynamic\nprediction and monitoring of the risk of MCC emergence in individual patients.",
          "link": "http://arxiv.org/abs/2107.13394",
          "publishedOn": "2021-07-29T02:00:09.432Z",
          "wordCount": 736,
          "title": "Nonlinear State Space Modeling and Control of the Impact of Patients' Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic Conditions. (arXiv:2107.13394v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lattanzi_E/0/1/0/all/0/1\">Emanuele Lattanzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calisti_L/0/1/0/all/0/1\">Lorenzo Calisti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freschi_V/0/1/0/all/0/1\">Valerio Freschi</a>",
          "description": "Current guidelines from the World Health Organization indicate that the\nSARSCoV-2 coronavirus, which results in the novel coronavirus disease\n(COVID-19), is transmitted through respiratory droplets or by contact. Contact\ntransmission occurs when contaminated hands touch the mucous membrane of the\nmouth, nose, or eyes. Moreover, pathogens can also be transferred from one\nsurface to another by contaminated hands, which facilitates transmission by\nindirect contact. Consequently, hands hygiene is extremely important to prevent\nthe spread of the SARSCoV-2 virus. Additionally, hand washing and/or hand\nrubbing disrupts also the transmission of other viruses and bacteria that cause\ncommon colds, flu and pneumonia, thereby reducing the overall disease burden.\nThe vast proliferation of wearable devices, such as smartwatches, containing\nacceleration, rotation, magnetic field sensors, etc., together with the modern\ntechnologies of artificial intelligence, such as machine learning and more\nrecently deep-learning, allow the development of accurate applications for\nrecognition and classification of human activities such as: walking, climbing\nstairs, running, clapping, sitting, sleeping, etc. In this work we evaluate the\nfeasibility of an automatic system, based on current smartwatches, which is\nable to recognize when a subject is washing or rubbing its hands, in order to\nmonitor parameters such as frequency and duration, and to evaluate the\neffectiveness of the gesture. Our preliminary results show a classification\naccuracy of about 95% and of about 94% for respectively deep and standard\nlearning techniques.",
          "link": "http://arxiv.org/abs/2107.13405",
          "publishedOn": "2021-07-29T02:00:09.424Z",
          "wordCount": 722,
          "title": "Automatic Unstructured Handwashing Recognition using Smartwatch to Reduce Contact Transmission of Pathogens. (arXiv:2107.13405v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_A/0/1/0/all/0/1\">Alishiba Dsouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1\">Nicolas Tempelmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1\">Elena Demidova</a>",
          "description": "OpenStreetMap (OSM) is one of the richest openly available sources of\nvolunteered geographic information. Although OSM includes various geographical\nentities, their descriptions are highly heterogeneous, incomplete, and do not\nfollow any well-defined ontology. Knowledge graphs can potentially provide\nvaluable semantic information to enrich OSM entities. However, interlinking OSM\nentities with knowledge graphs is inherently difficult due to the large,\nheterogeneous, ambiguous, and flat OSM schema and the annotation sparsity. This\npaper tackles the alignment of OSM tags with the corresponding knowledge graph\nclasses holistically by jointly considering the schema and instance layers. We\npropose a novel neural architecture that capitalizes upon a shared latent space\nfor tag-to-class alignment created using linked entities in OSM and knowledge\ngraphs. Our experiments performed to align OSM datasets for several countries\nwith two of the most prominent openly available knowledge graphs, namely,\nWikidata and DBpedia, demonstrate that the proposed approach outperforms the\nstate-of-the-art schema alignment baselines by up to 53 percentage points in\nterms of F1-score. The resulting alignment facilitates new semantic annotations\nfor over 10 million OSM entities worldwide, which is more than a 400% increase\ncompared to the existing semantic annotations in OSM.",
          "link": "http://arxiv.org/abs/2107.13257",
          "publishedOn": "2021-07-29T02:00:09.404Z",
          "wordCount": 625,
          "title": "Towards Neural Schema Alignment for OpenStreetMap and Knowledge Graphs. (arXiv:2107.13257v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Struski_L/0/1/0/all/0/1\">&#x141;ukasz Struski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danel_T/0/1/0/all/0/1\">Tomasz Danel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1\">Marek &#x15a;mieja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1\">Bartosz Zieli&#x144;ski</a>",
          "description": "Recent years have seen a surge in research on deep interpretable neural\nnetworks with decision trees as one of the most commonly incorporated tools.\nThere are at least three advantages of using decision trees over logistic\nregression classification models: they are easy to interpret since they are\nbased on binary decisions, they can make decisions faster, and they provide a\nhierarchy of classes. However, one of the well-known drawbacks of decision\ntrees, as compared to decision graphs, is that decision trees cannot reuse the\ndecision nodes. Nevertheless, decision graphs were not commonly used in deep\nlearning due to the lack of efficient gradient-based training techniques. In\nthis paper, we fill this gap and provide a general paradigm based on Markov\nprocesses, which allows for efficient training of the special type of decision\ngraphs, which we call Self-Organizing Neural Graphs (SONG). We provide an\nextensive theoretical study of SONG, complemented by experiments conducted on\nLetter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our\nmethod performs on par or better than existing decision models.",
          "link": "http://arxiv.org/abs/2107.13214",
          "publishedOn": "2021-07-29T02:00:09.397Z",
          "wordCount": 601,
          "title": "SONG: Self-Organizing Neural Graphs. (arXiv:2107.13214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathy_Y/0/1/0/all/0/1\">Yasmin Fathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Autoencoders are unsupervised models which have been used for detecting\nanomalies in multi-sensor environments. A typical use includes training a\npredictive model with data from sensors operating under normal conditions and\nusing the model to detect anomalies. Anomalies can come either from real\nchanges in the environment (real drift) or from faulty sensory devices (virtual\ndrift); however, the use of Autoencoders to distinguish between different\nanomalies has not yet been considered. To this end, we first propose the\ndevelopment of Bayesian Autoencoders to quantify epistemic and aleatoric\nuncertainties. We then test the Bayesian Autoencoder using a real-world\nindustrial dataset for hydraulic condition monitoring. The system is injected\nwith noise and drifts, and we have found the epistemic uncertainty to be less\nsensitive to sensor perturbations as compared to the reconstruction loss. By\nobserving the reconstructed signals with the uncertainties, we gain\ninterpretable insights, and these uncertainties offer a potential avenue for\ndistinguishing real and virtual drifts.",
          "link": "http://arxiv.org/abs/2107.13249",
          "publishedOn": "2021-07-29T02:00:09.388Z",
          "wordCount": 601,
          "title": "Bayesian Autoencoders for Drift Detection in Industrial Environments. (arXiv:2107.13249v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiyong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qianqian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shilong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>",
          "description": "The Area under the ROC curve (AUC) is a well-known ranking metric for\nproblems such as imbalanced learning and recommender systems. The vast majority\nof existing AUC-optimization-based machine learning methods only focus on\nbinary-class cases, while leaving the multiclass cases unconsidered. In this\npaper, we start an early trial to consider the problem of learning multiclass\nscoring functions via optimizing multiclass AUC metrics. Our foundation is\nbased on the M metric, which is a well-known multiclass extension of AUC. We\nfirst pay a revisit to this metric, showing that it could eliminate the\nimbalance issue from the minority class pairs. Motivated by this, we propose an\nempirical surrogate risk minimization framework to approximately optimize the M\nmetric. Theoretically, we show that: (i) optimizing most of the popular\ndifferentiable surrogate losses suffices to reach the Bayes optimal scoring\nfunction asymptotically; (ii) the training framework enjoys an imbalance-aware\ngeneralization error bound, which pays more attention to the bottleneck samples\nof minority classes compared with the traditional $O(\\sqrt{1/N})$ result.\nPractically, to deal with the low scalability of the computational operations,\nwe propose acceleration methods for three popular surrogate loss functions,\nincluding the exponential loss, squared loss, and hinge loss, to speed up loss\nand gradient evaluations. Finally, experimental results on 11 real-world\ndatasets demonstrate the effectiveness of our proposed framework.",
          "link": "http://arxiv.org/abs/2107.13171",
          "publishedOn": "2021-07-29T02:00:09.381Z",
          "wordCount": 650,
          "title": "Learning with Multiclass AUC: Theory and Algorithms. (arXiv:2107.13171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1\">Geetika Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1\">Rohit K Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kamlesh Tiwari</a>",
          "description": "This paper proposes teeth-photo, a new biometric modality for human\nauthentication on mobile and hand held devices. Biometrics samples are acquired\nusing the camera mounted on mobile device with the help of a mobile application\nhaving specific markers to register the teeth area. Region of interest (RoI) is\nthen extracted using the markers and the obtained sample is enhanced using\ncontrast limited adaptive histogram equalization (CLAHE) for better visual\nclarity. We propose a deep learning architecture and novel regularization\nscheme to obtain highly discriminative embedding for small size RoI. Proposed\ncustom loss function was able to achieve perfect classification for the tiny\nRoI of $75\\times 75$ size. The model is end-to-end and few-shot and therefore\nis very efficient in terms of time and energy requirements. The system can be\nused in many ways including device unlocking and secure authentication. To the\nbest of our understanding, this is the first work on teeth-photo based\nauthentication for mobile device. Experiments have been conducted on an\nin-house teeth-photo database collected using our application. The database is\nmade publicly available. Results have shown that the proposed system has\nperfect accuracy.",
          "link": "http://arxiv.org/abs/2107.13217",
          "publishedOn": "2021-07-29T02:00:09.373Z",
          "wordCount": 632,
          "title": "DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices. (arXiv:2107.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wei-Wei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>",
          "description": "Analyzing better time series with limited human effort is of interest to\nacademia and industry. Driven by business scenarios, we organized the first\nAutomated Time Series Regression challenge (AutoSeries) for the WSDM Cup 2020.\nWe present its design, analysis, and post-hoc experiments. The code submission\nrequirement precluded participants from any manual intervention, testing\nautomated machine learning capabilities of solutions, across many datasets,\nunder hardware and time limitations. We prepared 10 datasets from diverse\napplication domains (sales, power consumption, air quality, traffic, and\nparking), featuring missing data, mixed continuous and categorical variables,\nand various sampling rates. Each dataset was split into a training and a test\nsequence (which was streamed, allowing models to continuously adapt). The\nsetting of time series regression, differs from classical forecasting in that\ncovariates at the present time are known. Great strides were made by\nparticipants to tackle this AutoSeries problem, as demonstrated by the jump in\nperformance from the sample submission, and post-hoc comparisons with\nAutoGluon. Simple yet effective methods were used, based on feature\nengineering, LightGBM, and random search hyper-parameter tuning, addressing all\naspects of the challenge. Our post-hoc analyses revealed that providing\nadditional time did not yield significant improvements. The winners' code was\nopen-sourced https://www.4paradigm.com/competition/autoseries2020.",
          "link": "http://arxiv.org/abs/2107.13186",
          "publishedOn": "2021-07-29T02:00:09.353Z",
          "wordCount": 644,
          "title": "AutoML Meets Time Series Regression Design and Analysis of the AutoSeries Challenge. (arXiv:2107.13186v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lishuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinting Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1\">Kwok Leung Tsui</a>",
          "description": "Short-term forecasting of passenger flow is critical for transit management\nand crowd regulation. Spatial dependencies, temporal dependencies,\ninter-station correlations driven by other latent factors, and exogenous\nfactors bring challenges to the short-term forecasts of passenger flow of urban\nrail transit networks. An innovative deep learning approach, Multi-Graph\nConvolutional-Recurrent Neural Network (MGC-RNN) is proposed to forecast\npassenger flow in urban rail transit systems to incorporate these complex\nfactors. We propose to use multiple graphs to encode the spatial and other\nheterogenous inter-station correlations. The temporal dynamics of the\ninter-station correlations are also modeled via the proposed multi-graph\nconvolutional-recurrent neural network structure. Inflow and outflow of all\nstations can be collectively predicted with multiple time steps ahead via a\nsequence to sequence(seq2seq) architecture. The proposed method is applied to\nthe short-term forecasts of passenger flow in Shenzhen Metro, China. The\nexperimental results show that MGC-RNN outperforms the benchmark algorithms in\nterms of forecasting accuracy. Besides, it is found that the inter-station\ndriven by network distance, network structure, and recent flow patterns are\nsignificant factors for passenger flow forecasting. Moreover, the architecture\nof LSTM-encoder-decoder can capture the temporal dependencies well. In general,\nthe proposed framework could provide multiple views of passenger flow dynamics\nfor fine prediction and exhibit a possibility for multi-source heterogeneous\ndata fusion in the spatiotemporal forecast tasks.",
          "link": "http://arxiv.org/abs/2107.13226",
          "publishedOn": "2021-07-29T02:00:09.345Z",
          "wordCount": 659,
          "title": "Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) for Short-Term Forecasting of Transit Passenger Flow. (arXiv:2107.13226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruhe_D/0/1/0/all/0/1\">David Ruhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1\">Patrick Forr&#xe9;</a>",
          "description": "We perform approximate inference in state-space models that allow for\nnonlinear higher-order Markov chains in latent space. The conditional\nindependencies of the generative model enable us to parameterize only an\ninference model, which learns to estimate clean states in a self-supervised\nmanner using maximum likelihood. First, we propose a recurrent method that is\ntrained directly on noisy observations. Afterward, we cast the model such that\nthe optimization problem leads to an update scheme that backpropagates through\na recursion similar to the classical Kalman filter and smoother. In scientific\napplications, domain knowledge can give a linear approximation of the latent\ntransition maps. We can easily incorporate this knowledge into our model,\nleading to a hybrid inference approach. In contrast to other methods,\nexperiments show that the hybrid method makes the inferred latent states\nphysically more interpretable and accurate, especially in low-data regimes.\nFurthermore, we do not rely on an additional parameterization of the generative\nmodel or supervision via uncorrupted observations or ground truth latent\nstates. Despite our model's simplicity, we obtain competitive results on the\nchaotic Lorenz system compared to a fully supervised approach and outperform a\nmethod based on variational inference.",
          "link": "http://arxiv.org/abs/2107.13349",
          "publishedOn": "2021-07-29T02:00:09.338Z",
          "wordCount": 617,
          "title": "Self-Supervised Hybrid Inference in State-Space Models. (arXiv:2107.13349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peuter_S/0/1/0/all/0/1\">Sebastiaan De Peuter</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Oulasvirta_A/0/1/0/all/0/1\">Antti Oulasvirta</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a> (1 and 3) ((1) Department of Computer Science, Aalto University, Finland, (2) Department of Communications and Networking, Aalto University, Finland, (3) Department of Computer Science, University of Manchester, UK)",
          "description": "AI for supporting designers needs to be rethought. It should aim to\ncooperate, not automate, by supporting and leveraging the creativity and\nproblem-solving of designers. The challenge for such AI is how to infer\ndesigners' goals and then help them without being needlessly disruptive. We\npresent AI-assisted design: a framework for creating such AI, built around\ngenerative user models which enable reasoning about designers' goals,\nreasoning, and capabilities.",
          "link": "http://arxiv.org/abs/2107.13074",
          "publishedOn": "2021-07-29T02:00:09.331Z",
          "wordCount": 550,
          "title": "Toward AI Assistants That Let Designers Design. (arXiv:2107.13074v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_E/0/1/0/all/0/1\">Eric Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1\">Ann Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>",
          "description": "We present a framework for the unsupervised learning of neurosymbolic\nencoders, i.e., encoders obtained by composing neural networks with symbolic\nprograms from a domain-specific language. Such a framework can naturally\nincorporate symbolic expert knowledge into the learning process and lead to\nmore interpretable and factorized latent representations than fully neural\nencoders. Also, models learned this way can have downstream impact, as many\nanalysis workflows can benefit from having clean programmatic descriptions. We\nground our learning algorithm in the variational autoencoding (VAE) framework,\nwhere we aim to learn a neurosymbolic encoder in conjunction with a standard\ndecoder. Our algorithm integrates standard VAE-style training with modern\nprogram synthesis techniques. We evaluate our method on learning latent\nrepresentations for real-world trajectory data from animal biology and sports\nanalytics. We show that our approach offers significantly better separation\nthan standard VAEs and leads to practical gains on downstream tasks.",
          "link": "http://arxiv.org/abs/2107.13132",
          "publishedOn": "2021-07-29T02:00:09.324Z",
          "wordCount": 576,
          "title": "Unsupervised Learning of Neurosymbolic Encoders. (arXiv:2107.13132v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Recent advancements in predictive machine learning has led to its application\nin various use cases in manufacturing. Most research focused on maximising\npredictive accuracy without addressing the uncertainty associated with it.\nWhile accuracy is important, focusing primarily on it poses an overfitting\ndanger, exposing manufacturers to risk, ultimately hindering the adoption of\nthese techniques. In this paper, we determine the sources of uncertainty in\nmachine learning and establish the success criteria of a machine learning\nsystem to function well under uncertainty in a cyber-physical manufacturing\nsystem (CPMS) scenario. Then, we propose a multi-agent system architecture\nwhich leverages probabilistic machine learning as a means of achieving such\ncriteria. We propose possible scenarios for which our proposed architecture is\nuseful and discuss future work. Experimentally, we implement Bayesian Neural\nNetworks for multi-tasks classification on a public dataset for the real-time\ncondition monitoring of a hydraulic system and demonstrate the usefulness of\nthe system by evaluating the probability of a prediction being accurate given\nits uncertainty. We deploy these models using our proposed agent-based\nframework and integrate web visualisation to demonstrate its real-time\nfeasibility.",
          "link": "http://arxiv.org/abs/2107.13252",
          "publishedOn": "2021-07-29T02:00:09.318Z",
          "wordCount": 635,
          "title": "Multi Agent System for Machine Learning Under Uncertainty in Cyber Physical Manufacturing System. (arXiv:2107.13252v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canessa_G/0/1/0/all/0/1\">Gianpiero Canessa</a>",
          "description": "Support vector machines (SVM) is one of the well known supervised classes of\nlearning algorithms. Furthermore, the conic-segmentation SVM (CS-SVM) is a\nnatural multiclass analogue of the standard binary SVM, as CS-SVM models are\ndealing with the situation where the exact values of the data points are known.\nThis paper studies CS-SVM when the data points are uncertain or mislabelled.\nWith some properties known for the distributions, a chance-constrained CS-SVM\napproach is used to ensure the small probability of misclassification for the\nuncertain data. The geometric interpretation is presented to show how CS-SVM\nworks. Finally, we present experimental results to investigate the chance\nconstrained CS-SVM's performance.",
          "link": "http://arxiv.org/abs/2107.13319",
          "publishedOn": "2021-07-29T02:00:09.297Z",
          "wordCount": 547,
          "title": "Chance constrained conic-segmentation support vector machine with uncertain data. (arXiv:2107.13319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13393",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Choe_Y/0/1/0/all/0/1\">Yoonsuck Choe</a>",
          "description": "Brain science and artificial intelligence have made great progress toward the\nunderstanding and engineering of the human mind. The progress has accelerated\nsignificantly since the turn of the century thanks to new methods for probing\nthe brain (both structure and function), and rapid development in deep learning\nresearch. However, despite these new developments, there are still many open\nquestions, such as how to understand the brain at the system level, and various\nrobustness issues and limitations of deep learning. In this informal essay, I\nwill talk about some of the concepts that are central to brain science and\nartificial intelligence, such as information and memory, and discuss how a\ndifferent view on these concepts can help us move forward, beyond current\nlimits of our understanding in these fields.",
          "link": "http://arxiv.org/abs/2107.13393",
          "publishedOn": "2021-07-29T02:00:09.291Z",
          "wordCount": 576,
          "title": "Meaning Versus Information, Prediction Versus Memory, and Question Versus Answer. (arXiv:2107.13393v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chubba_e/0/1/0/all/0/1\">ennifer Chubba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Missaouib_S/0/1/0/all/0/1\">Sondess Missaouib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Concannonc_S/0/1/0/all/0/1\">Shauna Concannonc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maloneyb_L/0/1/0/all/0/1\">Liam Maloneyb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>",
          "description": "Conversational Artificial Intelligence (CAI) systems and Intelligent Personal\nAssistants (IPA), such as Alexa, Cortana, Google Home and Siri are becoming\nubiquitous in our lives, including those of children, the implications of which\nis receiving increased attention, specifically with respect to the effects of\nthese systems on children's cognitive, social and linguistic development.\nRecent advances address the implications of CAI with respect to privacy,\nsafety, security, and access. However, there is a need to connect and embed the\nethical and technical aspects in the design. Using a case-study of a research\nand development project focused on the use of CAI in storytelling for children,\nthis paper reflects on the social context within a specific case of technology\ndevelopment, as substantiated and supported by argumentation from within the\nliterature. It describes the decision making process behind the recommendations\nmade on this case for their adoption in the creative industries. Further\nresearch that engages with developers and stakeholders in the ethics of\nstorytelling through CAI is highlighted as a matter of urgency.",
          "link": "http://arxiv.org/abs/2107.13076",
          "publishedOn": "2021-07-29T02:00:09.284Z",
          "wordCount": 620,
          "title": "Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI. (arXiv:2107.13076v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masahiro Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takaaki Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>",
          "description": "With the recent rapid progress in the study of deep generative models (DGMs),\nthere is a need for a framework that can implement them in a simple and generic\nway. In this research, we focus on two features of the latest DGMs: (1) deep\nneural networks are encapsulated by probability distributions and (2) models\nare designed and learned based on an objective function. Taking these features\ninto account, we propose a new DGM library called Pixyz. We experimentally show\nthat our library is faster than existing probabilistic modeling languages in\nlearning simple DGMs and we show that our library can be used to implement\ncomplex DGMs in a simple and concise manner, which is difficult to do with\nexisting libraries.",
          "link": "http://arxiv.org/abs/2107.13109",
          "publishedOn": "2021-07-29T02:00:09.276Z",
          "wordCount": 553,
          "title": "Pixyz: a library for developing deep generative models. (arXiv:2107.13109v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curth_A/0/1/0/all/0/1\">Alicia Curth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "The machine learning toolbox for estimation of heterogeneous treatment\neffects from observational data is expanding rapidly, yet many of its\nalgorithms have been evaluated only on a very limited set of semi-synthetic\nbenchmark datasets. In this paper, we show that even in arguably the simplest\nsetting -- estimation under ignorability assumptions -- the results of such\nempirical evaluations can be misleading if (i) the assumptions underlying the\ndata-generating mechanisms in benchmark datasets and (ii) their interplay with\nbaseline algorithms are inadequately discussed. We consider two popular machine\nlearning benchmark datasets for evaluation of heterogeneous treatment effect\nestimators -- the IHDP and ACIC2016 datasets -- in detail. We identify problems\nwith their current use and highlight that the inherent characteristics of the\nbenchmark datasets favor some algorithms over others -- a fact that is rarely\nacknowledged but of immense relevance for interpretation of empirical results.\nWe close by discussing implications and possible next steps.",
          "link": "http://arxiv.org/abs/2107.13346",
          "publishedOn": "2021-07-29T02:00:09.269Z",
          "wordCount": 619,
          "title": "Doing Great at Estimating CATE? On the Neglected Assumptions in Benchmark Comparisons of Treatment Effect Estimators. (arXiv:2107.13346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.",
          "link": "http://arxiv.org/abs/2107.13469",
          "publishedOn": "2021-07-29T02:00:09.251Z",
          "wordCount": 605,
          "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate. (arXiv:2107.13469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13090",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hambly_B/0/1/0/all/0/1\">Ben Hambly</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_H/0/1/0/all/0/1\">Huining Yang</a>",
          "description": "We consider a general-sum N-player linear-quadratic game with stochastic\ndynamics over a finite horizon and prove the global convergence of the natural\npolicy gradient method to the Nash equilibrium. In order to prove the\nconvergence of the method, we require a certain amount of noise in the system.\nWe give a condition, essentially a lower bound on the covariance of the noise\nin terms of the model parameters, in order to guarantee convergence. We\nillustrate our results with numerical experiments to show that even in\nsituations where the policy gradient method may not converge in the\ndeterministic setting, the addition of noise leads to convergence.",
          "link": "http://arxiv.org/abs/2107.13090",
          "publishedOn": "2021-07-29T02:00:09.243Z",
          "wordCount": 559,
          "title": "Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games. (arXiv:2107.13090v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1\">Wil van der Aalst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockhoff_T/0/1/0/all/0/1\">Tobias Brockhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahfarokhi_A/0/1/0/all/0/1\">Anahita Farhang Ghahfarokhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourbafrani_M/0/1/0/all/0/1\">Mahsa Pourbafrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uysal_M/0/1/0/all/0/1\">Merih Seran Uysal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelst_S/0/1/0/all/0/1\">Sebastiaan van Zelst</a>",
          "description": "Operational processes in production, logistics, material handling,\nmaintenance, etc., are supported by cyber-physical systems combining hardware\nand software components. As a result, the digital and the physical world are\nclosely aligned, and it is possible to track operational processes in detail\n(e.g., using sensors). The abundance of event data generated by today's\noperational processes provides opportunities and challenges for process mining\ntechniques supporting process discovery, performance analysis, and conformance\nchecking. Using existing process mining tools, it is already possible to\nautomatically discover process models and uncover performance and compliance\nproblems. In the DFG-funded Cluster of Excellence \"Internet of Production\"\n(IoP), process mining is used to create \"digital shadows\" to improve a wide\nvariety of operational processes. However, operational processes are dynamic,\ndistributed, and complex. Driven by the challenges identified in the IoP\ncluster, we work on novel techniques for comparative process mining (comparing\nprocess variants for different products at different locations at different\ntimes), object-centric process mining (to handle processes involving different\ntypes of objects that interact), and forward-looking process mining (to explore\n\"What if?\" questions). By addressing these challenges, we aim to develop\nvaluable \"digital shadows\" that can be used to remove operational friction.",
          "link": "http://arxiv.org/abs/2107.13066",
          "publishedOn": "2021-07-29T02:00:09.236Z",
          "wordCount": 660,
          "title": "Removing Operational Friction Using Process Mining: Challenges Provided by the Internet of Production (IoP). (arXiv:2107.13066v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>",
          "description": "Neural Architecture Search (NAS) can automatically design well-performed\narchitectures of Deep Neural Networks (DNNs) for the tasks at hand. However,\none bottleneck of NAS is the prohibitively computational cost largely due to\nthe expensive performance evaluation. The neural predictors can directly\nestimate the performance without any training of the DNNs to be evaluated, thus\nhave drawn increasing attention from researchers. Despite their popularity,\nthey also suffer a severe limitation: the shortage of annotated DNN\narchitectures for effectively training the neural predictors. In this paper, we\nproposed Homogeneous Architecture Augmentation for Neural Predictor (HAAP) of\nDNN architectures to address the issue aforementioned. Specifically, a\nhomogeneous architecture augmentation algorithm is proposed in HAAP to generate\nsufficient training data taking the use of homogeneous representation.\nFurthermore, the one-hot encoding strategy is introduced into HAAP to make the\nrepresentation of DNN architectures more effective. The experiments have been\nconducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental\nresults demonstrate that the proposed HAAP algorithm outperforms the state of\nthe arts compared, yet with much less training data. In addition, the ablation\nstudies on both benchmark datasets have also shown the universality of the\nhomogeneous architecture augmentation.",
          "link": "http://arxiv.org/abs/2107.13153",
          "publishedOn": "2021-07-29T02:00:09.228Z",
          "wordCount": 631,
          "title": "Homogeneous Architecture Augmentation for Neural Predictor. (arXiv:2107.13153v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Sayantini Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivisonno_R/0/1/0/all/0/1\">Riccardo Trivisonno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carle_G/0/1/0/all/0/1\">Georg Carle</a>",
          "description": "Network automation is gaining significant attention in the development of B5G\nnetworks, primarily for reducing operational complexity, expenditures and\nimproving network efficiency. Concurrently operating closed loops aiming for\nindividual optimization targets may cause conflicts which, left unresolved,\nwould lead to significant degradation in network Key Performance Indicators\n(KPIs), thereby resulting in sub-optimal network performance. Centralized\ncoordination, albeit optimal, is impractical in large scale networks and for\ntime-critical applications. Decentralized approaches are therefore envisaged in\nthe evolution to B5G and subsequently, 6G networks. This work explores\npervasive intelligence for conflict resolution in network automation, as an\nalternative to centralized orchestration. A Q-Learning decentralized approach\nto network automation is proposed, and an application to network slice\nauto-scaling is designed and evaluated. Preliminary results highlight the\npotential of the proposed scheme and justify further research work in this\ndirection.",
          "link": "http://arxiv.org/abs/2107.13268",
          "publishedOn": "2021-07-29T02:00:09.218Z",
          "wordCount": 606,
          "title": "Q-Learning for Conflict Resolution in B5G Network Automation. (arXiv:2107.13268v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>",
          "description": "Collaborative filtering models based on matrix factorization and learned\nsimilarities using Artificial Neural Networks (ANNs) have gained significant\nattention in recent years. This is, in part, because ANNs have demonstrated\ngood results in a wide variety of recommendation tasks. The introduction of\nANNs within the recommendation ecosystem has been recently questioned, raising\nseveral comparisons in terms of efficiency and effectiveness. One aspect most\nof these comparisons have in common is their focus on accuracy, neglecting\nother evaluation dimensions important for the recommendation, such as novelty,\ndiversity, or accounting for biases. We replicate experiments from three papers\nthat compare Neural Collaborative Filtering (NCF) and Matrix Factorization\n(MF), to extend the analysis to other evaluation dimensions. Our contribution\nshows that the experiments are entirely reproducible, and we extend the study\nincluding other accuracy metrics and two statistical hypothesis tests. We\ninvestigated the Diversity and Novelty of the recommendations, showing that MF\nprovides a better accuracy also on the long tail, although NCF provides a\nbetter item coverage and more diversified recommendations. We discuss the bias\neffect generated by the tested methods. They show a relatively small bias, but\nother recommendation baselines, with competitive accuracy performance,\nconsistently show to be less affected by this issue. This is the first work, to\nthe best of our knowledge, where several evaluation dimensions have been\nexplored for an array of SOTA algorithms covering recent adaptations of ANNs\nand MF. Hence, we show the potential these techniques may have on\nbeyond-accuracy evaluation while analyzing the effect on reproducibility these\ncomplementary dimensions may spark. Available at\ngithub.com/sisinflab/Reenvisioning-the-comparison-between-Neural-Collaborative-Filtering-and-Matrix-Factorization",
          "link": "http://arxiv.org/abs/2107.13472",
          "publishedOn": "2021-07-29T02:00:09.198Z",
          "wordCount": 710,
          "title": "Reenvisioning Collaborative Filtering vs Matrix Factorization. (arXiv:2107.13472v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "There has been a growing interest in unsupervised domain adaptation (UDA) to\nalleviate the data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is proposed for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vectors that violate the poset constraint by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-07-29T02:00:09.190Z",
          "wordCount": 624,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuesong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>",
          "description": "Node features and structural information of a graph are both crucial for\nsemi-supervised node classification problems. A variety of graph neural network\n(GNN) based approaches have been proposed to tackle these problems, which\ntypically determine output labels through feature aggregation. This can be\nproblematic, as it implies conditional independence of output nodes given\nhidden representations, despite their direct connections in the graph. To learn\nthe direct influence among output nodes in a graph, we propose the Explicit\nPairwise Factorized Graph Neural Network (EPFGNN), which models the whole graph\nas a partially observed Markov Random Field. It contains explicit pairwise\nfactors to model output-output relations and uses a GNN backbone to model\ninput-output relations. To balance model complexity and expressivity, the\npairwise factors have a shared component and a separate scaling coefficient for\neach edge. We apply the EM algorithm to train our model, and utilize a\nstar-shaped piecewise likelihood for the tractable surrogate objective. We\nconduct experiments on various datasets, which shows that our model can\neffectively improve the performance for semi-supervised node classification on\ngraphs.",
          "link": "http://arxiv.org/abs/2107.13059",
          "publishedOn": "2021-07-29T02:00:09.179Z",
          "wordCount": 613,
          "title": "Explicit Pairwise Factorized Graph Neural Network for Semi-Supervised Node Classification. (arXiv:2107.13059v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Picallo_M/0/1/0/all/0/1\">Mario Alvarez-Picallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghica_D/0/1/0/all/0/1\">Dan R. Ghica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sprunger_D/0/1/0/all/0/1\">David Sprunger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanasi_F/0/1/0/all/0/1\">Fabio Zanasi</a>",
          "description": "We enhance the calculus of string diagrams for monoidal categories with\nhierarchical features in order to capture closed monoidal (and cartesian\nclosed) structure. Using this new syntax we formulate an automatic\ndifferentiation algorithm for (applied) simply typed lambda calculus in the\nstyle of [Pearlmutter and Siskind 2008] and we prove for the first time its\nsoundness. To give an efficient yet principled implementation of the AD\nalgorithm we define a sound and complete representation of hierarchical string\ndiagrams as a class of hierarchical hypergraphs we call hypernets.",
          "link": "http://arxiv.org/abs/2107.13433",
          "publishedOn": "2021-07-29T02:00:09.160Z",
          "wordCount": 523,
          "title": "Functorial String Diagrams for Reverse-Mode Automatic Differentiation. (arXiv:2107.13433v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose new explainability approaches for point cloud deep\nneural networks based on local surrogate model-based methods to show which\ncomponents make the main contribution to the classification. Moreover, we\npropose a quantitative validation method for explainability methods of point\nclouds which enhances the persuasive power of explainability by dropping the\nmost positive or negative contributing features and monitoring how the\nclassification scores of specific categories change. To enable an intuitive\nexplanation of misclassified instances, we display features with confounding\ncontributions. Our new explainability approach provides a fairly accurate, more\nintuitive and widely applicable explanation for point cloud classification\ntasks. Our code is available at https://github.com/Explain3D/Explainable3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-07-29T02:00:09.104Z",
          "wordCount": 610,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1\">Guangliang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minglei Li</a>",
          "description": "Channel estimation and signal detection are essential steps to ensure the\nquality of end-to-end communication in orthogonal frequency-division\nmultiplexing (OFDM) systems. In this paper, we develop a DDLSD approach, i.e.,\nData-driven Deep Learning for Signal Detection in OFDM systems. First, the OFDM\nsystem model is established. Then, the long short-term memory (LSTM) is\nintroduced into the OFDM system model. Wireless channel data is generated\nthrough simulation, the preprocessed time series feature information is input\ninto the LSTM to complete the offline training. Finally, the trained model is\nused for online recovery of transmitted signal. The difference between this\nscheme and existing OFDM receiver is that explicit estimated channel state\ninformation (CSI) is transformed into invisible estimated CSI, and the transmit\nsymbol is directly restored. Simulation results show that the DDLSD scheme\noutperforms the existing traditional methods in terms of improving channel\nestimation and signal detection performance.",
          "link": "http://arxiv.org/abs/2107.13423",
          "publishedOn": "2021-07-29T02:00:09.096Z",
          "wordCount": 594,
          "title": "A Signal Detection Scheme Based on Deep Learning in OFDM Systems. (arXiv:2107.13423v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dongchen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-feng Yang</a>",
          "description": "Traditional maximum entropy and sparsity-based algorithms for analytic\ncontinuation often suffer from the ill-posed kernel matrix or demand tremendous\ncomputation time for parameter tuning. Here we propose a neural network method\nby convex optimization and replace the ill-posed inverse problem by a sequence\nof well-conditioned surrogate problems. After training, the learned optimizers\nare able to give a solution of high quality with low time cost and achieve\nhigher parameter efficiency than heuristic full-connected networks. The output\ncan also be used as a neural default model to improve the maximum entropy for\nbetter performance. Our methods may be easily extended to other\nhigh-dimensional inverse problems via large-scale pretraining.",
          "link": "http://arxiv.org/abs/2107.13265",
          "publishedOn": "2021-07-29T02:00:09.087Z",
          "wordCount": 542,
          "title": "Learned Optimizers for Analytic Continuation. (arXiv:2107.13265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13449",
          "author": "<a href=\"http://arxiv.org/find/nucl-th/1/au:+Sarkar_A/0/1/0/all/0/1\">Avik Sarkar</a>, <a href=\"http://arxiv.org/find/nucl-th/1/au:+Lee_D/0/1/0/all/0/1\">Dean Lee</a>",
          "description": "Emulators that can bypass computationally expensive scientific calculations\nwith high accuracy and speed can enable new studies of fundamental science as\nwell as more potential applications. In this work we focus on solving a system\nof constraint equations efficiently using a new machine learning approach that\nwe call self-learning emulation. A self-learning emulator is an active learning\nprotocol that can rapidly solve a system of equations over some range of\ncontrol parameters. The key ingredient is a fast estimate of the emulator error\nthat becomes progressively more accurate as the emulator improves. This\nacceleration is possible because the emulator itself is used to estimate the\nerror, and we illustrate with two examples. The first uses cubic spline\ninterpolation to find the roots of a polynomial with variable coefficients. The\nsecond example uses eigenvector continuation to find the eigenvectors and\neigenvalues of a large Hamiltonian matrix that depends on several control\nparameters. We envision future applications of self-learning emulators for\nsolving systems of algebraic equations, linear and nonlinear differential\nequations, and linear and nonlinear eigenvalue problems.",
          "link": "http://arxiv.org/abs/2107.13449",
          "publishedOn": "2021-07-29T02:00:09.080Z",
          "wordCount": 625,
          "title": "Self-learning Emulators and Eigenvector Continuation. (arXiv:2107.13449v1 [nucl-th])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13148",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Ullah_A/0/1/0/all/0/1\">A. K. M. Amanat Ullah</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Imtiaz_F/0/1/0/all/0/1\">Fahim Imtiaz</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ihsan_M/0/1/0/all/0/1\">Miftah Uddin Md Ihsan</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Alam_M/0/1/0/all/0/1\">Md. Golam Rabiul Alam</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Majumdar_M/0/1/0/all/0/1\">Mahbub Majumdar</a>",
          "description": "The unpredictability and volatility of the stock market render it challenging\nto make a substantial profit using any generalized scheme. This paper intends\nto discuss our machine learning model, which can make a significant amount of\nprofit in the US stock market by performing live trading in the Quantopian\nplatform while using resources free of cost. Our top approach was to use\nensemble learning with four classifiers: Gaussian Naive Bayes, Decision Tree,\nLogistic Regression with L1 regularization and Stochastic Gradient Descent, to\ndecide whether to go long or short on a particular stock. Our best model\nperformed daily trade between July 2011 and January 2019, generating 54.35%\nprofit. Finally, our work showcased that mixtures of weighted classifiers\nperform better than any individual predictor about making trading decisions in\nthe stock market.",
          "link": "http://arxiv.org/abs/2107.13148",
          "publishedOn": "2021-07-29T02:00:09.073Z",
          "wordCount": 594,
          "title": "Combining Machine Learning Classifiers for Stock Trading with Effective Feature Extraction. (arXiv:2107.13148v1 [q-fin.TR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Divi_S/0/1/0/all/0/1\">Siddharth Divi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi-Shan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrukh_H/0/1/0/all/0/1\">Habiba Farrukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celik_Z/0/1/0/all/0/1\">Z. Berkay Celik</a>",
          "description": "In Federated Learning (FL), the clients learn a single global model (FedAvg)\nthrough a central aggregator. In this setting, the non-IID distribution of the\ndata across clients restricts the global FL model from delivering good\nperformance on the local data of each client. Personalized FL aims to address\nthis problem by finding a personalized model for each client. Recent works\nwidely report the average personalized model accuracy on a particular data\nsplit of a dataset to evaluate the effectiveness of their methods. However,\nconsidering the multitude of personalization approaches proposed, it is\ncritical to study the per-user personalized accuracy and the accuracy\nimprovements among users with an equitable notion of fairness. To address these\nissues, we present a set of performance and fairness metrics intending to\nassess the quality of personalized FL methods. We apply these metrics to four\nrecently proposed personalized FL methods, PersFL, FedPer, pFedMe, and\nPer-FedAvg, on three different data splits of the CIFAR-10 dataset. Our\nevaluations show that the personalized model with the highest average accuracy\nacross users may not necessarily be the fairest. Our code is available at\nhttps://tinyurl.com/1hp9ywfa for public use.",
          "link": "http://arxiv.org/abs/2107.13173",
          "publishedOn": "2021-07-29T02:00:09.053Z",
          "wordCount": 627,
          "title": "New Metrics to Evaluate the Performance and Fairness of Personalized Federated Learning. (arXiv:2107.13173v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yong_B/0/1/0/all/0/1\">Bang Xiang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1\">Tim Pearce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "After an autoencoder (AE) has learnt to reconstruct one dataset, it might be\nexpected that the likelihood on an out-of-distribution (OOD) input would be\nlow. This has been studied as an approach to detect OOD inputs. Recent work\nshowed this intuitive approach can fail for the dataset pairs FashionMNIST vs\nMNIST. This paper suggests this is due to the use of Bernoulli likelihood and\nanalyses why this is the case, proposing two fixes: 1) Compute the uncertainty\nof likelihood estimate by using a Bayesian version of the AE. 2) Use\nalternative distributions to model the likelihood.",
          "link": "http://arxiv.org/abs/2107.13304",
          "publishedOn": "2021-07-29T02:00:09.045Z",
          "wordCount": 550,
          "title": "Bayesian Autoencoders: Analysing and Fixing the Bernoulli likelihood for Out-of-Distribution Detection. (arXiv:2107.13304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13430",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1\">Kiheiji Nishida</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1\">Kanta Naito</a>",
          "description": "This paper studies kernel density estimation by stagewise minimization\nalgorithm with a simple dictionary on U-divergence. We randomly split an i.i.d.\nsample into the two disjoint sets, one to be used for constructing the kernels\nin the dictionary and the other for evaluating the estimator, and implement the\nalgorithm. The resulting estimator brings us data-adaptive weighting parameters\nand bandwidth matrices, and realizes a sparse representation of kernel density\nestimation. We present the non-asymptotic error bounds of our estimator and\nconfirm its performance by simulations compared with the direct plug-in\nbandwidth matrices and the reduced set density estimator.",
          "link": "http://arxiv.org/abs/2107.13430",
          "publishedOn": "2021-07-29T02:00:09.036Z",
          "wordCount": 538,
          "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary. (arXiv:2107.13430v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1\">Joseph Marino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>",
          "description": "While recent machine learning research has revealed connections between deep\ngenerative models such as VAEs and rate-distortion losses used in learned\ncompression, most of this work has focused on images. In a similar spirit, we\nview recently proposed neural video coding algorithms through the lens of deep\nautoregressive and latent variable modeling. We present recent neural video\ncodecs as instances of a generalized stochastic temporal autoregressive\ntransform, and propose new avenues for further improvements inspired by\nnormalizing flows and structured priors. We propose several architectures that\nyield state-of-the-art video compression performance on full-resolution video\nand discuss their tradeoffs and ablations. In particular, we propose (i)\nimproved temporal autoregressive transforms, (ii) improved entropy models with\nstructured and temporal dependencies, and (iii) variable bitrate versions of\nour algorithms. Since our improvements are compatible with a large class of\nexisting models, we provide further evidence that the generative modeling\nviewpoint can advance the neural video coding field.",
          "link": "http://arxiv.org/abs/2107.13136",
          "publishedOn": "2021-07-29T02:00:09.028Z",
          "wordCount": 637,
          "title": "Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "A common lens to theoretically study neural net architectures is to analyze\nthe functions they can approximate. However, the constructions from\napproximation theory often have unrealistic aspects, for example, reliance on\ninfinite precision to memorize target function values, which make these results\npotentially less meaningful. To address these issues, this work proposes a\nformal definition of statistically meaningful approximation which requires the\napproximating network to exhibit good statistical learnability. We present case\nstudies on statistically meaningful approximation for two classes of functions:\nboolean circuits and Turing machines. We show that overparameterized\nfeedforward neural nets can statistically meaningfully approximate boolean\ncircuits with sample complexity depending only polynomially on the circuit\nsize, not the size of the approximating network. In addition, we show that\ntransformers can statistically meaningfully approximate Turing machines with\ncomputation time bounded by $T$, requiring sample complexity polynomial in the\nalphabet size, state space size, and $\\log (T)$. Our analysis introduces new\ntools for generalization bounds that provide much tighter sample complexity\nguarantees than the typical VC-dimension or norm-based bounds, which may be of\nindependent interest.",
          "link": "http://arxiv.org/abs/2107.13163",
          "publishedOn": "2021-07-29T02:00:09.019Z",
          "wordCount": 619,
          "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13200",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bo Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a> (Alzheimer&#x27;s Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing), <a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Shuwei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_P/0/1/0/all/0/1\">Peng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Ronald X. Xu</a>",
          "description": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "link": "http://arxiv.org/abs/2107.13200",
          "publishedOn": "2021-07-29T02:00:08.998Z",
          "wordCount": 760,
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization. (arXiv:2107.13200v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daubechies_I/0/1/0/all/0/1\">Ingrid Daubechies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeVore_R/0/1/0/all/0/1\">Ronald DeVore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dym_N/0/1/0/all/0/1\">Nadav Dym</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faigenbaum_Golovin_S/0/1/0/all/0/1\">Shira Faigenbaum-Golovin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovalsky_S/0/1/0/all/0/1\">Shahar Z. Kovalsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Josiah Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrova_G/0/1/0/all/0/1\">Guergana Petrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sober_B/0/1/0/all/0/1\">Barak Sober</a>",
          "description": "In the desire to quantify the success of neural networks in deep learning and\nother applications, there is a great interest in understanding which functions\nare efficiently approximated by the outputs of neural networks. By now, there\nexists a variety of results which show that a wide range of functions can be\napproximated with sometimes surprising accuracy by these outputs. For example,\nit is known that the set of functions that can be approximated with exponential\naccuracy (in terms of the number of parameters used) includes, on one hand,\nvery smooth functions such as polynomials and analytic functions (see e.g.\n\\cite{E,S,Y}) and, on the other hand, very rough functions such as the\nWeierstrass function (see e.g. \\cite{EPGB,DDFHP}), which is nowhere\ndifferentiable. In this paper, we add to the latter class of rough functions by\nshowing that it also includes refinable functions. Namely, we show that\nrefinable functions are approximated by the outputs of deep ReLU networks with\na fixed width and increasing depth with accuracy exponential in terms of their\nnumber of parameters. Our results apply to functions used in the standard\nconstruction of wavelets as well as to functions constructed via subdivision\nalgorithms in Computer Aided Geometric Design.",
          "link": "http://arxiv.org/abs/2107.13191",
          "publishedOn": "2021-07-29T02:00:08.989Z",
          "wordCount": 635,
          "title": "Neural Network Approximation of Refinable Functions. (arXiv:2107.13191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahadori_M/0/1/0/all/0/1\">Mohammad Taha Bahadori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchetgen_E/0/1/0/all/0/1\">Eric Tchetgen Tchetgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David E. Heckerman</a>",
          "description": "We study the problem of observational causal inference with continuous\ntreatment. We focus on the challenge of estimating the causal response curve\nfor infrequently-observed treatment values. We design a new algorithm based on\nthe framework of entropy balancing which learns weights that directly maximize\ncausal inference accuracy using end-to-end optimization. Our weights can be\ncustomized for different datasets and causal inference algorithms. We propose a\nnew theory for consistency of entropy balancing for continuous treatments.\nUsing synthetic and real-world data, we show that our proposed algorithm\noutperforms the entropy balancing in terms of causal inference accuracy.",
          "link": "http://arxiv.org/abs/2107.13068",
          "publishedOn": "2021-07-29T02:00:08.982Z",
          "wordCount": 537,
          "title": "End-to-End Balancing for Causal Continuous Treatment-Effect Estimation. (arXiv:2107.13068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundgaard_K/0/1/0/all/0/1\">Keld T. Lundgaard</a>",
          "description": "By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.",
          "link": "http://arxiv.org/abs/2107.13054",
          "publishedOn": "2021-07-29T02:00:08.975Z",
          "wordCount": 623,
          "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning. (arXiv:2107.13054v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1\">George Kesidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">David J. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeron_M/0/1/0/all/0/1\">Maxime Bergeron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferguson_R/0/1/0/all/0/1\">Ryan Ferguson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_V/0/1/0/all/0/1\">Vladimir Lucic</a>",
          "description": "We describe a gradient-based method to discover local error maximizers of a\ndeep neural network (DNN) used for regression, assuming the availability of an\n\"oracle\" capable of providing real-valued supervision (a regression target) for\nsamples. For example, the oracle could be a numerical solver which,\noperationally, is much slower than the DNN. Given a discovered set of local\nerror maximizers, the DNN is either fine-tuned or retrained in the manner of\nactive learning.",
          "link": "http://arxiv.org/abs/2107.13124",
          "publishedOn": "2021-07-29T02:00:08.954Z",
          "wordCount": 511,
          "title": "Robust and Active Learning for Deep Neural Network Regression. (arXiv:2107.13124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13067",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ataei_M/0/1/0/all/0/1\">Mohammadmehdi Ataei</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pirmorad_E/0/1/0/all/0/1\">Erfan Pirmorad</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Costa_F/0/1/0/all/0/1\">Franco Costa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Han_S/0/1/0/all/0/1\">Sejin Han</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Park_C/0/1/0/all/0/1\">Chul B Park</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bussmann_M/0/1/0/all/0/1\">Markus Bussmann</a>",
          "description": "Piecewise Linear Interface Construction (PLIC) is frequently used to\ngeometrically reconstruct fluid interfaces in Computational Fluid Dynamics\n(CFD) modeling of two-phase flows. PLIC reconstructs interfaces from a scalar\nfield that represents the volume fraction of each phase in each computational\ncell. Given the volume fraction and interface normal, the location of a linear\ninterface is uniquely defined. For a cubic computational cell (3D), the\nposition of the planar interface is determined by intersecting the cube with a\nplane, such that the volume of the resulting truncated polyhedron cell is equal\nto the volume fraction. Yet it is geometrically complex to find the exact\nposition of the plane, and it involves calculations that can be a computational\nbottleneck of many CFD models. However, while the forward problem of 3D PLIC is\nchallenging, the inverse problem, of finding the volume of the truncated\npolyhedron cell given a defined plane, is simple. In this work, we propose a\ndeep learning model for the solution to the forward problem of PLIC by only\nmaking use of its inverse problem. The proposed model is up to several orders\nof magnitude faster than traditional schemes, which significantly reduces the\ncomputational bottleneck of PLIC in CFD simulations.",
          "link": "http://arxiv.org/abs/2107.13067",
          "publishedOn": "2021-07-29T02:00:08.947Z",
          "wordCount": 651,
          "title": "A Deep Learning Algorithm for Piecewise Linear Interface Construction (PLIC). (arXiv:2107.13067v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dallmann_A/0/1/0/all/0/1\">Alexander Dallmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoller_D/0/1/0/all/0/1\">Daniel Zoller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>",
          "description": "At the present time, sequential item recommendation models are compared by\ncalculating metrics on a small item subset (target set) to speed up\ncomputation. The target set contains the relevant item and a set of negative\nitems that are sampled from the full item set. Two well-known strategies to\nsample negative items are uniform random sampling and sampling by popularity to\nbetter approximate the item frequency distribution in the dataset. Most\nrecently published papers on sequential item recommendation rely on sampling by\npopularity to compare the evaluated models. However, recent work has already\nshown that an evaluation with uniform random sampling may not be consistent\nwith the full ranking, that is, the model ranking obtained by evaluating a\nmetric using the full item set as target set, which raises the question whether\nthe ranking obtained by sampling by popularity is equal to the full ranking. In\nthis work, we re-evaluate current state-of-the-art sequential recommender\nmodels from the point of view, whether these sampling strategies have an impact\non the final ranking of the models. We therefore train four recently proposed\nsequential recommendation models on five widely known datasets. For each\ndataset and model, we employ three evaluation strategies. First, we compute the\nfull model ranking. Then we evaluate all models on a target set sampled by the\ntwo different sampling strategies, uniform random sampling and sampling by\npopularity with the commonly used target set size of 100, compute the model\nranking for each strategy and compare them with each other. Additionally, we\nvary the size of the sampled target set. Overall, we find that both sampling\nstrategies can produce inconsistent rankings compared with the full ranking of\nthe models. Furthermore, both sampling by popularity and uniform random\nsampling do not consistently produce the same ranking ...",
          "link": "http://arxiv.org/abs/2107.13045",
          "publishedOn": "2021-07-29T02:00:08.940Z",
          "wordCount": 740,
          "title": "A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models. (arXiv:2107.13045v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1\">Reece Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1\">Mohamed H. Abdelpakey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed S. Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M.Mohamed</a>",
          "description": "Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.",
          "link": "http://arxiv.org/abs/2107.13093",
          "publishedOn": "2021-07-29T02:00:08.932Z",
          "wordCount": 720,
          "title": "Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onoufriou_G/0/1/0/all/0/1\">George Onoufriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayfield_P/0/1/0/all/0/1\">Paul Mayfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontidis_G/0/1/0/all/0/1\">Georgios Leontidis</a>",
          "description": "Fully Homomorphic Encryption (FHE) is a relatively recent advancement in the\nfield of privacy-preserving technologies. FHE allows for the arbitrary depth\ncomputation of both addition and multiplication, and thus the application of\nabelian/polynomial equations, like those found in deep learning algorithms.\nThis project investigates, derives, and proves how FHE with deep learning can\nbe used at scale, with relatively low time complexity, the problems that such a\nsystem incurs, and mitigations/solutions for such problems. In addition, we\ndiscuss how this could have an impact on the future of data privacy and how it\ncan enable data sharing across various actors in the agri-food supply chain,\nhence allowing the development of machine learning-based systems. Finally, we\nfind that although FHE incurs a high spatial complexity cost, the time\ncomplexity is within expected reasonable bounds, while allowing for absolutely\nprivate predictions to be made, in our case for milk yield prediction.",
          "link": "http://arxiv.org/abs/2107.12997",
          "publishedOn": "2021-07-29T02:00:08.920Z",
          "wordCount": 585,
          "title": "Fully Homomorphically Encrypted Deep Learning as a Service. (arXiv:2107.12997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Timothy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1\">Roman Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lechao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaehoon Lee</a>",
          "description": "The effectiveness of machine learning algorithms arises from being able to\nextract useful features from large amounts of data. As model and dataset sizes\nincrease, dataset distillation methods that compress large datasets into\nsignificantly smaller yet highly performant ones will become valuable in terms\nof training efficiency and useful feature extraction. To that end, we apply a\nnovel distributed kernel based meta-learning framework to achieve\nstate-of-the-art results for dataset distillation using infinitely wide\nconvolutional neural networks. For instance, using only 10 datapoints (0.02% of\noriginal dataset), we obtain over 64% test accuracy on CIFAR-10 image\nclassification task, a dramatic improvement over the previous best test\naccuracy of 40%. Our state-of-the-art results extend across many other settings\nfor MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we\nperform some preliminary analyses of our distilled datasets to shed light on\nhow they differ from naturally occurring data.",
          "link": "http://arxiv.org/abs/2107.13034",
          "publishedOn": "2021-07-29T02:00:08.867Z",
          "wordCount": 580,
          "title": "Dataset Distillation with Infinitely Wide Convolutional Networks. (arXiv:2107.13034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uddin_A/0/1/0/all/0/1\">A. F. M. Shahab Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monira_M/0/1/0/all/0/1\">Mst. Sirazam Monira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Wheemyung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">TaeChoong Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sung-Ho Bae</a>",
          "description": "Advanced data augmentation strategies have widely been studied to improve the\ngeneralization ability of deep learning models. Regional dropout is one of the\npopular solutions that guides the model to focus on less discriminative parts\nby randomly removing image regions, resulting in improved regularization.\nHowever, such information removal is undesirable. On the other hand, recent\nstrategies suggest to randomly cut and mix patches and their labels among\ntraining images, to enjoy the advantages of regional dropout without having any\npointless pixel in the augmented images. We argue that such random selection\nstrategies of the patches may not necessarily represent sufficient information\nabout the corresponding object and thereby mixing the labels according to that\nuninformative patch enables the model to learn unexpected feature\nrepresentation. Therefore, we propose SaliencyMix that carefully selects a\nrepresentative image patch with the help of a saliency map and mixes this\nindicative patch with the target image, thus leading the model to learn more\nappropriate feature representation. SaliencyMix achieves the best known top-1\nerror of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on\nImageNet classification, respectively, and also improves the model robustness\nagainst adversarial perturbations. Furthermore, models that are trained with\nSaliencyMix help to improve the object detection performance. Source code is\navailable at https://github.com/SaliencyMix/SaliencyMix.",
          "link": "http://arxiv.org/abs/2006.01791",
          "publishedOn": "2021-07-28T02:02:34.744Z",
          "wordCount": 708,
          "title": "SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization. (arXiv:2006.01791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.14220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mostaani_A/0/1/0/all/0/1\">Arsham Mostaani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thang X. Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzinotas_S/0/1/0/all/0/1\">Symeon Chatzinotas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottersten_B/0/1/0/all/0/1\">Bj&#xf6;rn Ottersten</a>",
          "description": "A collaborative task is assigned to a multiagent system (MAS) in which agents\nare allowed to communicate. The MAS runs over an underlying Markov decision\nprocess and its task is to maximize the averaged sum of discounted one-stage\nrewards. Although knowing the global state of the environment is necessary for\nthe optimal action selection of the MAS, agents are limited to individual\nobservations. The inter-agent communication can tackle the issue of local\nobservability, however, the limited rate of the inter-agent communication\nprevents the agent from acquiring the precise global state information. To\novercome this challenge, agents need to communicate their observations in a\ncompact way such that the MAS compromises the minimum possible sum of rewards.\nWe show that this problem is equivalent to a form of rate-distortion problem\nwhich we call the task-based information compression. We introduce a scheme for\ntask-based information compression titled State aggregation for information\ncompression (SAIC), for which a state aggregation algorithm is analytically\ndesigned. The SAIC is shown to be capable of achieving near-optimal performance\nin terms of the achieved sum of discounted rewards. The proposed algorithm is\napplied to a rendezvous problem and its performance is compared with several\nbenchmarks. Numerical experiments confirm the superiority of the proposed\nalgorithm.",
          "link": "http://arxiv.org/abs/2005.14220",
          "publishedOn": "2021-07-28T02:02:34.699Z",
          "wordCount": 696,
          "title": "Task-Based Information Compression for Multi-Agent Communication Problems with Channel Rate Constraints. (arXiv:2005.14220v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Secci_F/0/1/0/all/0/1\">Francesco Secci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccarelli_A/0/1/0/all/0/1\">Andrea Ceccarelli</a>",
          "description": "RGB cameras are one of the most relevant sensors for autonomous driving\napplications. It is undeniable that failures of vehicle cameras may compromise\nthe autonomous driving task, possibly leading to unsafe behaviors when images\nthat are subsequently processed by the driving system are altered. To support\nthe definition of safe and robust vehicle architectures and intelligent\nsystems, in this paper we define the failure modes of a vehicle camera,\ntogether with an analysis of effects and known mitigations. Further, we build a\nsoftware library for the generation of the corresponding failed images and we\nfeed them to six object detectors for mono and stereo cameras and to the\nself-driving agent of an autonomous driving simulator. The resulting\nmisbehaviors with respect to operating with clean images allow a better\nunderstanding of failures effects and the related safety risks in image-based\napplications.",
          "link": "http://arxiv.org/abs/2008.05938",
          "publishedOn": "2021-07-28T02:02:34.652Z",
          "wordCount": 611,
          "title": "RGB cameras failures and their effects in autonomous driving applications. (arXiv:2008.05938v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12975",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Severin_B/0/1/0/all/0/1\">B. Severin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lennon_D/0/1/0/all/0/1\">D. T. Lennon</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Camenzind_L/0/1/0/all/0/1\">L. C. Camenzind</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Vigneau_F/0/1/0/all/0/1\">F. Vigneau</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Fedele_F/0/1/0/all/0/1\">F. Fedele</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jirovec_D/0/1/0/all/0/1\">D. Jirovec</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ballabio_A/0/1/0/all/0/1\">A. Ballabio</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chrastina_D/0/1/0/all/0/1\">D. Chrastina</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Isella_G/0/1/0/all/0/1\">G. Isella</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kruijf_M/0/1/0/all/0/1\">M. de Kruijf</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Carballido_M/0/1/0/all/0/1\">M. J. Carballido</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Svab_S/0/1/0/all/0/1\">S. Svab</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kuhlmann_A/0/1/0/all/0/1\">A. V. Kuhlmann</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Braakman_F/0/1/0/all/0/1\">F. R. Braakman</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Geyer_S/0/1/0/all/0/1\">S. Geyer</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Froning_F/0/1/0/all/0/1\">F. N. M. Froning</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Moon_H/0/1/0/all/0/1\">H. Moon</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Osborne_M/0/1/0/all/0/1\">M. A. Osborne</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Sejdinovic_D/0/1/0/all/0/1\">D. Sejdinovic</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Katsaros_G/0/1/0/all/0/1\">G. Katsaros</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zumbuhl_D/0/1/0/all/0/1\">D. M. Zumb&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Briggs_G/0/1/0/all/0/1\">G. A. D. Briggs</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ares_N/0/1/0/all/0/1\">N. Ares</a>",
          "description": "The potential of Si and SiGe-based devices for the scaling of quantum\ncircuits is tainted by device variability. Each device needs to be tuned to\noperation conditions. We give a key step towards tackling this variability with\nan algorithm that, without modification, is capable of tuning a 4-gate Si\nFinFET, a 5-gate GeSi nanowire and a 7-gate SiGe heterostructure double quantum\ndot device from scratch. We achieve tuning times of 30, 10, and 92 minutes,\nrespectively. The algorithm also provides insight into the parameter space\nlandscape for each of these devices. These results show that overarching\nsolutions for the tuning of quantum devices are enabled by machine learning.",
          "link": "http://arxiv.org/abs/2107.12975",
          "publishedOn": "2021-07-28T02:02:34.625Z",
          "wordCount": 609,
          "title": "Cross-architecture Tuning of Silicon and SiGe-based Quantum Devices Using Machine Learning. (arXiv:2107.12975v1 [cond-mat.mes-hall])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silveira_B/0/1/0/all/0/1\">B&#xe1;rbara Silveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Henrique S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Ana Paula Couto da Silva</a>",
          "description": "In recent years, Online Social Networks have become an important medium for\npeople who suffer from mental disorders to share moments of hardship, and\nreceive emotional and informational support. In this work, we analyze how\ndiscussions in Reddit communities related to mental disorders can help improve\nthe health conditions of their users. Using the emotional tone of users'\nwriting as a proxy for emotional state, we uncover relationships between user\ninteractions and state changes. First, we observe that authors of negative\nposts often write rosier comments after engaging in discussions, indicating\nthat users' emotional state can improve due to social support. Second, we build\nmodels based on SOTA text embedding techniques and RNNs to predict shifts in\nemotional tone. This differs from most of related work, which focuses primarily\non detecting mental disorders from user activity. We demonstrate the\nfeasibility of accurately predicting the users' reactions to the interactions\nexperienced in these platforms, and present some examples which illustrate that\nthe models are correctly capturing the effects of comments on the author's\nemotional tone. Our models hold promising implications for interventions to\nprovide support for people struggling with mental illnesses.",
          "link": "http://arxiv.org/abs/2005.07473",
          "publishedOn": "2021-07-28T02:02:34.616Z",
          "wordCount": 698,
          "title": "Predicting User Emotional Tone in Mental Disorder Online Communities. (arXiv:2005.07473v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12416",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jing_G/0/1/0/all/0/1\">Gangshan Jing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+George_J/0/1/0/all/0/1\">Jemin George</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakrabortty_A/0/1/0/all/0/1\">Aranya Chakrabortty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush K. Sharma</a>",
          "description": "Recently introduced distributed zeroth-order optimization (ZOO) algorithms\nhave shown their utility in distributed reinforcement learning (RL).\nUnfortunately, in the gradient estimation process, almost all of them require\nrandom samples with the same dimension as the global variable and/or require\nevaluation of the global cost function, which may induce high estimation\nvariance for large-scale networks. In this paper, we propose a novel\ndistributed zeroth-order algorithm by leveraging the network structure inherent\nin the optimization objective, which allows each agent to estimate its local\ngradient by local cost evaluation independently, without use of any consensus\nprotocol. The proposed algorithm exhibits an asynchronous update scheme, and is\ndesigned for stochastic non-convex optimization with a possibly non-convex\nfeasible domain based on the block coordinate descent method. The algorithm is\nlater employed as a distributed model-free RL algorithm for distributed linear\nquadratic regulator design, where a learning graph is designed to describe the\nrequired interaction relationship among agents in distributed learning. We\nprovide an empirical validation of the proposed algorithm to benchmark its\nperformance on convergence rate and variance against a centralized ZOO\nalgorithm.",
          "link": "http://arxiv.org/abs/2107.12416",
          "publishedOn": "2021-07-28T02:02:34.579Z",
          "wordCount": 641,
          "title": "Asynchronous Distributed Reinforcement Learning for LQR Control via Zeroth-Order Block Coordinate Descent. (arXiv:2107.12416v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Claudia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1\">Victor Veitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1\">David Blei</a>",
          "description": "The defining challenge for causal inference from observational data is the\npresence of `confounders', covariates that affect both treatment assignment and\nthe outcome. To address this challenge, practitioners collect and adjust for\nthe covariates, hoping that they adequately correct for confounding. However,\nincluding every observed covariate in the adjustment runs the risk of including\n`bad controls', variables that induce bias when they are conditioned on. The\nproblem is that we do not always know which variables in the covariate set are\nsafe to adjust for and which are not. To address this problem, we develop\nNearly Invariant Causal Estimation (NICE). NICE uses invariant risk\nminimization (IRM) [Arj19] to learn a representation of the covariates that,\nunder some assumptions, strips out bad controls but preserves sufficient\ninformation to adjust for confounding. Adjusting for the learned\nrepresentation, rather than the covariates themselves, avoids the induced bias\nand provides valid causal inferences. We evaluate NICE on both synthetic and\nsemi-synthetic data. When the covariates contain unknown collider variables and\nother bad controls, NICE performs better than adjusting for all the covariates.",
          "link": "http://arxiv.org/abs/2011.12379",
          "publishedOn": "2021-07-28T02:02:34.572Z",
          "wordCount": 637,
          "title": "Invariant Representation Learning for Treatment Effect Estimation. (arXiv:2011.12379v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1\">Wendson A. S. Barbosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1\">Aaron Griffith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowlands_G/0/1/0/all/0/1\">Graham E. Rowlands</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govia_L/0/1/0/all/0/1\">Luke C. G. Govia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeill_G/0/1/0/all/0/1\">Guilhem J. Ribeill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohki_T/0/1/0/all/0/1\">Thomas A. Ohki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1\">Daniel J. Gauthier</a>",
          "description": "We demonstrate that matching the symmetry properties of a reservoir computer\n(RC) to the data being processed dramatically increases its processing power.\nWe apply our method to the parity task, a challenging benchmark problem that\nhighlights inversion and permutation symmetries, and to a chaotic system\ninference task that presents an inversion symmetry rule. For the parity task,\nour symmetry-aware RC obtains zero error using an exponentially reduced neural\nnetwork and training data, greatly speeding up the time to result and\noutperforming hand crafted artificial neural networks. When both symmetries are\nrespected, we find that the network size $N$ necessary to obtain zero error for\n50 different RC instances scales linearly with the parity-order $n$. Moreover,\nsome symmetry-aware RC instances perform a zero error classification with only\n$N=1$ for $n\\leq7$. Furthermore, we show that a symmetry-aware RC only needs a\ntraining data set with size on the order of $(n+n/2)$ to obtain such\nperformance, an exponential reduction in comparison to a regular RC which\nrequires a training data set with size on the order of $n2^n$ to contain all\n$2^n$ possible $n-$bit-long sequences. For the inference task, we show that a\nsymmetry-aware RC presents a normalized root-mean-square error three\norders-of-magnitude smaller than regular RCs. For both tasks, our RC approach\nrespects the symmetries by adjusting only the input and the output layers, and\nnot by problem-based modifications to the neural network. We anticipate that\ngeneralizations of our procedure can be applied in information processing for\nproblems with known symmetries.",
          "link": "http://arxiv.org/abs/2102.00310",
          "publishedOn": "2021-07-28T02:02:34.553Z",
          "wordCount": 739,
          "title": "Symmetry-Aware Reservoir Computing. (arXiv:2102.00310v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Gil_D/0/1/0/all/0/1\">Diego Garc&#xed;a-Gil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1\">Salvador Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1\">Ning Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1\">Francisco Herrera</a>",
          "description": "Differences in data size per class, also known as imbalanced data\ndistribution, have become a common problem affecting data quality. Big Data\nscenarios pose a new challenge to traditional imbalanced classification\nalgorithms, since they are not prepared to work with such amount of data. Split\ndata strategies and lack of data in the minority class due to the use of\nMapReduce paradigm have posed new challenges for tackling the imbalance between\nclasses in Big Data scenarios. Ensembles have shown to be able to successfully\naddress imbalanced data problems. Smart Data refers to data of enough quality\nto achieve high performance models. The combination of ensembles and Smart\nData, achieved through Big Data preprocessing, should be a great synergy. In\nthis paper, we propose a novel methodology based on Decision Trees Ensemble\nwith Smart Data for addressing the imbalanced classification problem in Big\nData domains, namely DeTE_SD methodology. This methodology is based on the\nlearning of different decision trees using distributed quality data for the\nensemble process. This quality data is achieved by fusing Random\nDiscretization, Principal Components Analysis and clustering-based Random\nOversampling for obtaining different Smart Data versions of the original data.\nExperiments carried out in 21 binary adapted datasets have shown that our\nmethodology outperforms Random Forest.",
          "link": "http://arxiv.org/abs/2001.05759",
          "publishedOn": "2021-07-28T02:02:34.537Z",
          "wordCount": 691,
          "title": "A Methodology guided by Decision Trees Ensemble and Smart Data for Imbalanced Big Data. (arXiv:2001.05759v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yuqing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ricky Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Procedural content generation via machine learning (PCGML) is the process of\nprocedurally generating game content using models trained on existing game\ncontent. PCGML methods can struggle to capture the true variance present in\nunderlying data with a single model. In this paper, we investigated the use of\nensembles of Markov chains for procedurally generating \\emph{Mega Man} levels.\nWe conduct an initial investigation of our approach and evaluate it on measures\nof playability and stylistic similarity in comparison to a non-ensemble,\nexisting Markov chain approach.",
          "link": "http://arxiv.org/abs/2107.12524",
          "publishedOn": "2021-07-28T02:02:34.523Z",
          "wordCount": 540,
          "title": "Ensemble Learning For Mega Man Level Generation. (arXiv:2107.12524v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-07-28T02:02:34.503Z",
          "wordCount": 654,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12783",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Khurana_D/0/1/0/all/0/1\">Drona Khurana</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ravichandran_S/0/1/0/all/0/1\">Srinivasan Ravichandran</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jain_S/0/1/0/all/0/1\">Sparsh Jain</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Edakunni_N/0/1/0/all/0/1\">Narayanan Unny Edakunni</a>",
          "description": "A plug-in algorithm to estimate Bayes Optimal Classifiers for fairness-aware\nbinary classification has been proposed in (Menon & Williamson, 2018). However,\nthe statistical efficacy of their approach has not been established. We prove\nthat the plug-in algorithm is statistically consistent. We also derive finite\nsample guarantees associated with learning the Bayes Optimal Classifiers via\nthe plug-in algorithm. Finally, we propose a protocol that modifies the plug-in\napproach, so as to simultaneously guarantee fairness and differential privacy\nwith respect to a binary feature deemed sensitive.",
          "link": "http://arxiv.org/abs/2107.12783",
          "publishedOn": "2021-07-28T02:02:34.496Z",
          "wordCount": 532,
          "title": "Statistical Guarantees for Fairness Aware Plug-In Algorithms. (arXiv:2107.12783v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12102",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cartis_C/0/1/0/all/0/1\">Coralia Cartis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Massart_E/0/1/0/all/0/1\">Estelle Massart</a>, <a href=\"http://arxiv.org/find/math/1/au:+Otemissov_A/0/1/0/all/0/1\">Adilet Otemissov</a>",
          "description": "We propose a random-subspace algorithmic framework for global optimization of\nLipschitz-continuous objectives, and analyse its convergence using novel tools\nfrom conic integral geometry. X-REGO randomly projects, in a sequential or\nsimultaneous manner, the high-dimensional original problem into low-dimensional\nsubproblems that can then be solved with any global, or even local,\noptimization solver. We estimate the probability that the randomly-embedded\nsubproblem shares (approximately) the same global optimum as the original\nproblem. This success probability is then used to show convergence of X-REGO to\nan approximate global solution of the original problem, under weak assumptions\non the problem (having a strictly feasible global solution) and on the solver\n(guaranteed to find an approximate global solution of the reduced problem with\nsufficiently high probability). In the particular case of unconstrained\nobjectives with low effective dimension, that only vary over a low-dimensional\nsubspace, we propose an X-REGO variant that explores random subspaces of\nincreasing dimension until finding the effective dimension of the problem,\nleading to X-REGO globally converging after a finite number of embeddings,\nproportional to the effective dimension. We show numerically that this variant\nefficiently finds both the effective dimension and an approximate global\nminimizer of the original problem.",
          "link": "http://arxiv.org/abs/2107.12102",
          "publishedOn": "2021-07-28T02:02:34.490Z",
          "wordCount": 631,
          "title": "Global optimization using random embeddings. (arXiv:2107.12102v1 [math.OC] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shifeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>",
          "description": "Cross-speaker style transfer is crucial to the applications of multi-style\nand expressive speech synthesis at scale. It does not require the target\nspeakers to be experts in expressing all styles and to collect corresponding\nrecordings for model training. However, the performances of existing style\ntransfer methods are still far behind real application needs. The root causes\nare mainly twofold. Firstly, the style embedding extracted from single\nreference speech can hardly provide fine-grained and appropriate prosody\ninformation for arbitrary text to synthesize. Secondly, in these models the\ncontent/text, prosody, and speaker timbre are usually highly entangled, it's\ntherefore not realistic to expect a satisfied result when freely combining\nthese components, such as to transfer speaking style between speakers. In this\npaper, we propose a cross-speaker style transfer text-to-speech (TTS) model\nwith explicit prosody bottleneck. The prosody bottleneck builds up the kernels\naccounting for speaking style robustly, and disentangles the prosody from\ncontent and speaker timbre, therefore guarantees high quality cross-speaker\nstyle transfer. Evaluation result shows the proposed method even achieves\non-par performance with source speaker's speaker-dependent (SD) model in\nobjective measurement of prosody, and significantly outperforms the cycle\nconsistency and GMVAE-based baselines in objective and subjective evaluations.",
          "link": "http://arxiv.org/abs/2107.12562",
          "publishedOn": "2021-07-28T02:02:34.483Z",
          "wordCount": 640,
          "title": "Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis. (arXiv:2107.12562v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1\">Ilja Kuzborskij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivasplata_O/0/1/0/all/0/1\">Omar Rivasplata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rannen_Triki_A/0/1/0/all/0/1\">Amal Rannen-Triki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>",
          "description": "Empirically it has been observed that the performance of deep neural networks\nsteadily improves as we increase model size, contradicting the classical view\non overfitting and generalization. Recently, the double descent phenomena has\nbeen proposed to reconcile this observation with theory, suggesting that the\ntest error has a second descent when the model becomes sufficiently\noverparameterized, as the model size itself acts as an implicit regularizer. In\nthis paper we add to the growing body of work in this space, providing a\ncareful study of learning dynamics as a function of model size for the least\nsquares scenario. We show an excess risk bound for the gradient descent\nsolution of the least squares objective. The bound depends on the smallest\nnon-zero eigenvalue of the covariance matrix of the input features, via a\nfunctional form that has the double descent behavior. This gives a new\nperspective on the double descent curves reported in the literature. Our\nanalysis of the excess risk allows to decouple the effect of optimization and\ngeneralization error. In particular, we find that in case of noiseless\nregression, double descent is explained solely by optimization-related\nquantities, which was missed in studies focusing on the Moore-Penrose\npseudoinverse solution. We believe that our derivation provides an alternative\nview compared to existing work, shedding some light on a possible cause of this\nphenomena, at least in the considered least squares setting. We empirically\nexplore if our predictions hold for neural networks, in particular whether the\ncovariance of intermediary hidden activations has a similar behavior as the one\npredicted by our derivations.",
          "link": "http://arxiv.org/abs/2107.12685",
          "publishedOn": "2021-07-28T02:02:34.475Z",
          "wordCount": 711,
          "title": "On the Role of Optimization in Double Descent: A Least Squares Study. (arXiv:2107.12685v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04896",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Sakthivadivel_D/0/1/0/all/0/1\">Dalton A R Sakthivadivel</a>",
          "description": "We investigate how the activation function can be used to describe neural\nfiring in an abstract way, and in turn, why it works well in artificial neural\nnetworks. We discuss how a spike in a biological neurone belongs to a\nparticular universality class of phase transitions in statistical physics. We\nthen show that the artificial neurone is, mathematically, a mean field model of\nbiological neural membrane dynamics, which arises from modelling spiking as a\nphase transition. This allows us to treat selective neural firing in an\nabstract way, and formalise the role of the activation function in perceptron\nlearning. The resultant statistical physical model allows us to recover the\nexpressions for some known activation functions as various special cases. Along\nwith deriving this model and specifying the analogous neural case, we analyse\nthe phase transition to understand the physics of neural network learning.\nTogether, it is shown that there is not only a biological meaning, but a\nphysical justification, for the emergence and performance of typical activation\nfunctions; implications for neural learning and inference are also discussed.",
          "link": "http://arxiv.org/abs/2102.04896",
          "publishedOn": "2021-07-28T02:02:34.452Z",
          "wordCount": 649,
          "title": "Formalising the Use of the Activation Function in Neural Inference. (arXiv:2102.04896v2 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halle_A/0/1/0/all/0/1\">Alex Halle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campanile_L/0/1/0/all/0/1\">L. Flavio Campanile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasse_A/0/1/0/all/0/1\">Alexander Hasse</a>",
          "description": "Topology optimization is widely used by engineers during the initial product\ndevelopment process to get a first possible geometry design. The\nstate-of-the-art is the iterative calculation, which requires both time and\ncomputational power. Some newly developed methods use artificial intelligence\nto accelerate the topology optimization. These require conventionally\npre-optimized data and therefore are dependent on the quality and number of\navailable data. This paper proposes an AI-assisted design method for topology\noptimization, which does not require pre-optimized data. The designs are\nprovided by an artificial neural network, the predictor, on the basis of\nboundary conditions and degree of filling (the volume percentage filled by\nmaterial) as input data. In the training phase, geometries generated on the\nbasis of random input data are evaluated with respect to given criteria. The\nresults of those evaluations flow into an objective function which is minimized\nby adapting the predictor's parameters. After the training is completed, the\npresented AI-assisted design procedure supplies geometries which are similar to\nthe ones generated by conventional topology optimizers, but requires a small\nfraction of the computational effort required by those algorithms. We\nanticipate our paper to be a starting point for AI-based methods that requires\ndata, that is hard to compute or not available.",
          "link": "http://arxiv.org/abs/2012.06384",
          "publishedOn": "2021-07-28T02:02:34.444Z",
          "wordCount": 664,
          "title": "An AI-Assisted Design Method for Topology Optimization Without Pre-Optimized Training Data. (arXiv:2012.06384v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yejiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Weiming Xiang</a>",
          "description": "In this paper, a robust optimization framework is developed to train shallow\nneural networks based on reachability analysis of neural networks. To\ncharacterize noises of input data, the input training data is disturbed in the\ndescription of interval sets. Interval-based reachability analysis is then\nperformed for the hidden layer. With the reachability analysis results, a\nrobust optimization training method is developed in the framework of robust\nleast-square problems. Then, the developed robust least-square problem is\nrelaxed to a semidefinite programming problem. It has been shown that the\ndeveloped robust learning method can provide better robustness against\nperturbations at the price of loss of training accuracy to some extent. At\nlast, the proposed method is evaluated on a robot arm model learning example.",
          "link": "http://arxiv.org/abs/2107.12801",
          "publishedOn": "2021-07-28T02:02:34.437Z",
          "wordCount": 560,
          "title": "Robust Optimization Framework for Training Shallow Neural Networks Using Reachability Method. (arXiv:2107.12801v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bernini_N/0/1/0/all/0/1\">Nicola Bernini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessa_M/0/1/0/all/0/1\">Mikhail Bessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delmas_R/0/1/0/all/0/1\">R&#xe9;mi Delmas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gold_A/0/1/0/all/0/1\">Arthur Gold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goubault_E/0/1/0/all/0/1\">Eric Goubault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pennec_R/0/1/0/all/0/1\">Romain Pennec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putot_S/0/1/0/all/0/1\">Sylvie Putot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sillion_F/0/1/0/all/0/1\">Fran&#xe7;ois Sillion</a>",
          "description": "We explore the reinforcement learning approach to designing controllers by\nextensively discussing the case of a quadcopter attitude controller. We provide\nall details allowing to reproduce our approach, starting with a model of the\ndynamics of a crazyflie 2.0 under various nominal and non-nominal conditions,\nincluding partial motor failures and wind gusts. We develop a robust form of a\nsignal temporal logic to quantitatively evaluate the vehicle's behavior and\nmeasure the performance of controllers. The paper thoroughly describes the\nchoices in training algorithms, neural net architecture, hyperparameters,\nobservation space in view of the different performance metrics we have\nintroduced. We discuss the robustness of the obtained controllers, both to\npartial loss of power for one rotor and to wind gusts and finish by drawing\nconclusions on practical controller design by reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.12942",
          "publishedOn": "2021-07-28T02:02:34.428Z",
          "wordCount": 595,
          "title": "Reinforcement Learning with Formal Performance Metrics for Quadcopter Attitude Control under Non-nominal Contexts. (arXiv:2107.12942v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12421",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Talgorn_B/0/1/0/all/0/1\">Bastien Talgorn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Alarie_S/0/1/0/all/0/1\">St&#xe9;phane Alarie</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kokkolaras_M/0/1/0/all/0/1\">Michael Kokkolaras</a>",
          "description": "We consider computationally expensive blackbox optimization problems and\npresent a method that employs surrogate models and concurrent computing at the\nsearch step of the mesh adaptive direct search (MADS) algorithm. Specifically,\nwe solve a surrogate optimization problem using locally weighted scatterplot\nsmoothing (LOWESS) models to find promising candidate points to be evaluated by\nthe blackboxes. We consider several methods for selecting promising points from\na large number of points. We conduct numerical experiments to assess the\nperformance of the modified MADS algorithm with respect to available CPU\nresources by means of five engineering design problems.",
          "link": "http://arxiv.org/abs/2107.12421",
          "publishedOn": "2021-07-28T02:02:34.421Z",
          "wordCount": 528,
          "title": "Parallel Surrogate-assisted Optimization Using Mesh Adaptive Direct Search. (arXiv:2107.12421v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2004.06230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiayu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_S/0/1/0/all/0/1\">Susan Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1\">Finale Doshi-Velez</a>",
          "description": "Contextual bandits often provide simple and effective personalization in\ndecision making problems, making them popular tools to deliver personalized\ninterventions in mobile health as well as other health applications. However,\nwhen bandits are deployed in the context of a scientific study -- e.g. a\nclinical trial to test if a mobile health intervention is effective -- the aim\nis not only to personalize for an individual, but also to determine, with\nsufficient statistical power, whether or not the system's intervention is\neffective. It is essential to assess the effectiveness of the intervention\nbefore broader deployment for better resource allocation. The two objectives\nare often deployed under different model assumptions, making it hard to\ndetermine how achieving the personalization and statistical power affect each\nother. In this work, we develop general meta-algorithms to modify existing\nalgorithms such that sufficient power is guaranteed while still improving each\nuser's well-being. We also demonstrate that our meta-algorithms are robust to\nvarious model mis-specifications possibly appearing in statistical studies,\nthus providing a valuable tool to study designers.",
          "link": "http://arxiv.org/abs/2004.06230",
          "publishedOn": "2021-07-28T02:02:34.395Z",
          "wordCount": 651,
          "title": "Power Constrained Bandits. (arXiv:2004.06230v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gothankar_R/0/1/0/all/0/1\">Ruchira Gothankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1\">Fabio Di Troia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "YouTube videos often include captivating descriptions and intriguing\nthumbnails designed to increase the number of views, and thereby increase the\nrevenue for the person who posted the video. This creates an incentive for\npeople to post clickbait videos, in which the content might deviate\nsignificantly from the title, description, or thumbnail. In effect, users are\ntricked into clicking on clickbait videos. In this research, we consider the\nchallenging problem of detecting clickbait YouTube videos. We experiment with\nmultiple state-of-the-art machine learning techniques using a variety of\ntextual features.",
          "link": "http://arxiv.org/abs/2107.12791",
          "publishedOn": "2021-07-28T02:02:34.388Z",
          "wordCount": 517,
          "title": "Clickbait Detection in YouTube Videos. (arXiv:2107.12791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haibo Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>",
          "description": "Federated learning (FL) enables distributed participants to collectively\nlearn a strong global model without sacrificing their individual data privacy.\nMainstream FL approaches require each participant to share a common network\narchitecture and further assume that data are are sampled IID across\nparticipants. However, in real-world deployments participants may require\nheterogeneous network architectures; and the data distribution is almost\ncertainly non-uniform across participants. To address these issues we introduce\nFedH2L, which is agnostic to both the model architecture and robust to\ndifferent data distributions across participants. In contrast to approaches\nsharing parameters or gradients, FedH2L relies on mutual distillation,\nexchanging only posteriors on a shared seed set between participants in a\ndecentralized manner. This makes it extremely bandwidth efficient, model\nagnostic, and crucially produces models capable of performing well on the whole\ndata distribution when learning from heterogeneous silos.",
          "link": "http://arxiv.org/abs/2101.11296",
          "publishedOn": "2021-07-28T02:02:34.373Z",
          "wordCount": 615,
          "title": "FedH2L: Federated Learning with Model and Statistical Heterogeneity. (arXiv:2101.11296v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solares_J/0/1/0/all/0/1\">Jose Roberto Ayala Solares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yajie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassaine_A/0/1/0/all/0/1\">Abdelaali Hassaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Shishir Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamouei_M/0/1/0/all/0/1\">Mohammad Mamouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canoy_D/0/1/0/all/0/1\">Dexter Canoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_K/0/1/0/all/0/1\">Kazem Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_Khorshidi_G/0/1/0/all/0/1\">Gholamreza Salimi-Khorshidi</a>",
          "description": "Deep learning models have shown tremendous potential in learning\nrepresentations, which are able to capture some key properties of the data.\nThis makes them great candidates for transfer learning: Exploiting\ncommonalities between different learning tasks to transfer knowledge from one\ntask to another. Electronic health records (EHR) research is one of the domains\nthat has witnessed a growing number of deep learning techniques employed for\nlearning clinically-meaningful representations of medical concepts (such as\ndiseases and medications). Despite this growth, the approaches to benchmark and\nassess such learned representations (or, embeddings) is under-investigated;\nthis can be a big issue when such embeddings are shared to facilitate transfer\nlearning. In this study, we aim to (1) train some of the most prominent disease\nembedding techniques on a comprehensive EHR data from 3.1 million patients, (2)\nemploy qualitative and quantitative evaluation techniques to assess these\nembeddings, and (3) provide pre-trained disease embeddings for transfer\nlearning. This study can be the first comprehensive approach for clinical\nconcept embedding evaluation and can be applied to any embedding techniques and\nfor any EHR concept.",
          "link": "http://arxiv.org/abs/2107.12919",
          "publishedOn": "2021-07-28T02:02:34.352Z",
          "wordCount": 623,
          "title": "Transfer Learning in Electronic Health Records through Clinical Concept Embedding. (arXiv:2107.12919v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12775",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Che_H/0/1/0/all/0/1\">Hui Che</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramanathan_S/0/1/0/all/0/1\">Sumana Ramanathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foran_D/0/1/0/all/0/1\">David Foran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nosher_J/0/1/0/all/0/1\">John L Nosher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hacihaliloglu_I/0/1/0/all/0/1\">Ilker Hacihaliloglu</a>",
          "description": "With the success of deep learning-based methods applied in medical image\nanalysis, convolutional neural networks (CNNs) have been investigated for\nclassifying liver disease from ultrasound (US) data. However, the scarcity of\navailable large-scale labeled US data has hindered the success of CNNs for\nclassifying liver disease from US data. In this work, we propose a novel\ngenerative adversarial network (GAN) architecture for realistic diseased and\nhealthy liver US image synthesis. We adopt the concept of stacking to\nsynthesize realistic liver US data. Quantitative and qualitative evaluation is\nperformed on 550 in-vivo B-mode liver US images collected from 55 subjects. We\nalso show that the synthesized images, together with real in vivo data, can be\nused to significantly improve the performance of traditional CNN architectures\nfor Nonalcoholic fatty liver disease (NAFLD) classification.",
          "link": "http://arxiv.org/abs/2107.12775",
          "publishedOn": "2021-07-28T02:02:34.345Z",
          "wordCount": 598,
          "title": "Realistic Ultrasound Image Synthesis for Improved Classification of Liver Disease. (arXiv:2107.12775v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Robin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>",
          "description": "Deep neural networks (DNNs) for the semantic segmentation of images are\nusually trained to operate on a predefined closed set of object classes. This\nis in contrast to the \"open world\" setting where DNNs are envisioned to be\ndeployed to. From a functional safety point of view, the ability to detect\nso-called \"out-of-distribution\" (OoD) samples, i.e., objects outside of a DNN's\nsemantic space, is crucial for many applications such as automated driving. A\nnatural baseline approach to OoD detection is to threshold on the pixel-wise\nsoftmax entropy. We present a two-step procedure that significantly improves\nthat approach. Firstly, we utilize samples from the COCO dataset as OoD proxy\nand introduce a second training objective to maximize the softmax entropy on\nthese samples. Starting from pretrained semantic segmentation networks we\nre-train a number of DNNs on different in-distribution datasets and\nconsistently observe improved OoD detection performance when evaluating on\ncompletely disjoint OoD datasets. Secondly, we perform a transparent\npost-processing step to discard false positive OoD samples by so-called \"meta\nclassification\". To this end, we apply linear models to a set of hand-crafted\nmetrics derived from the DNN's softmax probabilities. In our experiments we\nconsistently observe a clear additional gain in OoD detection performance,\ncutting down the number of detection errors by up to 52% when comparing the\nbest baseline with our results. We achieve this improvement sacrificing only\nmarginally in original segmentation performance. Therefore, our method\ncontributes to safer DNNs with more reliable overall system performance.",
          "link": "http://arxiv.org/abs/2012.06575",
          "publishedOn": "2021-07-28T02:02:34.339Z",
          "wordCount": 728,
          "title": "Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation. (arXiv:2012.06575v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08158",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1\">Michael Gadermayr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1\">Maximilian Tschuchnig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stangassinger_L/0/1/0/all/0/1\">Lea Maria Stangassinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreutzer_C/0/1/0/all/0/1\">Christina Kreutzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couillard_Despres_S/0/1/0/all/0/1\">Sebastien Couillard-Despres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oostingh_G/0/1/0/all/0/1\">Gertie Janneke Oostingh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hittmair_A/0/1/0/all/0/1\">Anton Hittmair</a>",
          "description": "In contrast to paraffin sections, frozen sections can be quickly generated\nduring surgical interventions. This procedure allows surgeons to wait for\nhistological findings during the intervention to base intra-operative decisions\non the outcome of the histology. However, compared to paraffin sections, the\nquality of frozen sections is typically lower, leading to a higher ratio of\nmiss-classification. In this work, we investigated the effect of the section\ntype on automated decision support approaches for classification of thyroid\ncancer. This was enabled by a data set consisting of pairs of sections for\nindividual patients. Moreover, we investigated, whether a frozen-to-paraffin\ntranslation could help to optimize classification scores. Finally, we propose a\nspecific data augmentation strategy to deal with a small amount of training\ndata and to increase classification accuracy even further.",
          "link": "http://arxiv.org/abs/2012.08158",
          "publishedOn": "2021-07-28T02:02:34.332Z",
          "wordCount": 622,
          "title": "Frozen-to-Paraffin: Categorization of Histological Frozen Sections by the Aid of Paraffin Sections and Generative Adversarial Networks. (arXiv:2012.08158v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.",
          "link": "http://arxiv.org/abs/2107.12651",
          "publishedOn": "2021-07-28T02:02:34.285Z",
          "wordCount": 604,
          "title": "Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chung-Wei Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakimenka_Y/0/1/0/all/0/1\">Yauhen Yakimenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsuan-Yin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosnes_E/0/1/0/all/0/1\">Eirik Rosnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kliewer_J/0/1/0/all/0/1\">Joerg Kliewer</a>",
          "description": "We propose to extend the concept of private information retrieval by allowing\nfor distortion in the retrieval process and relaxing the perfect privacy\nrequirement at the same time. In particular, we study the tradeoff between\ndownload rate, distortion, and user privacy leakage, and show that in the limit\nof large file sizes this trade-off can be captured via a novel\ninformation-theoretical formulation for datasets with a known distribution.\nMoreover, for scenarios where the statistics of the dataset is unknown, we\npropose a new deep learning framework by leveraging a generative adversarial\nnetwork approach, which allows the user to learn efficient schemes from the\ndata itself, minimizing the download cost. We evaluate the performance of the\nscheme on a synthetic Gaussian dataset as well as on both the MNIST and\nCIFAR-10 datasets. For the MNIST dataset, the data-driven approach\nsignificantly outperforms a non-learning based scheme which combines source\ncoding with multiple file download, while the CIFAR-10 performance is notably\nbetter.",
          "link": "http://arxiv.org/abs/2012.03902",
          "publishedOn": "2021-07-28T02:02:34.277Z",
          "wordCount": 658,
          "title": "Generative Adversarial User Privacy in Lossy Single-Server Information Retrieval. (arXiv:2012.03902v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tingting Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1\">Ramy E. Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1\">Hanieh Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangwani_T/0/1/0/all/0/1\">Tynan Gangwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annavaram_M/0/1/0/all/0/1\">Murali Annavaram</a>",
          "description": "Stragglers, Byzantine workers, and data privacy are the main bottlenecks in\ndistributed cloud computing. Several prior works proposed coded computing\nstrategies to jointly address all three challenges. They require either a large\nnumber of workers, a significant communication cost or a significant\ncomputational complexity to tolerate malicious workers. Much of the overhead in\nprior schemes comes from the fact that they tightly couple coding for all three\nproblems into a single framework. In this work, we propose Verifiable Coded\nComputing (VCC) framework that decouples Byzantine node detection challenge\nfrom the straggler tolerance. VCC leverages coded computing just for handling\nstragglers and privacy, and then uses an orthogonal approach of verifiable\ncomputing to tackle Byzantine nodes. Furthermore, VCC dynamically adapts its\ncoding scheme to tradeoff straggler tolerance with Byzantine protection and\nvice-versa. We evaluate VCC on compute intensive distributed logistic\nregression application. Our experiments show that VCC speeds up the\nconventional uncoded implementation of distributed logistic regression by\n$3.2\\times-6.9\\times$, and also improves the test accuracy by up to $12.6\\%$.",
          "link": "http://arxiv.org/abs/2107.12958",
          "publishedOn": "2021-07-28T02:02:34.258Z",
          "wordCount": 630,
          "title": "Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning. (arXiv:2107.12958v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "This is a tutorial and survey paper on Boltzmann Machine (BM), Restricted\nBoltzmann Machine (RBM), and Deep Belief Network (DBN). We start with the\nrequired background on probabilistic graphical models, Markov random field,\nGibbs sampling, statistical physics, Ising model, and the Hopfield network.\nThen, we introduce the structures of BM and RBM. The conditional distributions\nof visible and hidden variables, Gibbs sampling in RBM for generating\nvariables, training BM and RBM by maximum likelihood estimation, and\ncontrastive divergence are explained. Then, we discuss different possible\ndiscrete and continuous distributions for the variables. We introduce\nconditional RBM and how it is trained. Finally, we explain deep belief network\nas a stack of RBM models. This paper on Boltzmann machines can be useful in\nvarious fields including data science, statistics, neural computation, and\nstatistical physics.",
          "link": "http://arxiv.org/abs/2107.12521",
          "publishedOn": "2021-07-28T02:02:34.248Z",
          "wordCount": 604,
          "title": "Restricted Boltzmann Machine and Deep Belief Network: Tutorial and Survey. (arXiv:2107.12521v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ittner_J/0/1/0/all/0/1\">Jan Ittner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolikowski_L/0/1/0/all/0/1\">Lukasz Bolikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemker_K/0/1/0/all/0/1\">Konstantin Hemker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_R/0/1/0/all/0/1\">Ricardo Kennedy</a>",
          "description": "We offer a new formalism for global explanations of pairwise feature\ndependencies and interactions in supervised models. Building upon SHAP values\nand SHAP interaction values, our approach decomposes feature contributions into\nsynergistic, redundant and independent components (S-R-I decomposition of SHAP\nvectors). We propose a geometric interpretation of the components and formally\nprove its basic properties. Finally, we demonstrate the utility of synergy,\nredundancy and independence by applying them to a constructed data set and\nmodel.",
          "link": "http://arxiv.org/abs/2107.12436",
          "publishedOn": "2021-07-28T02:02:34.232Z",
          "wordCount": 523,
          "title": "Feature Synergy, Redundancy, and Independence in Global Model Explanations using SHAP Vector Decomposition. (arXiv:2107.12436v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baudry_D/0/1/0/all/0/1\">Dorian Baudry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautron_R/0/1/0/all/0/1\">Romain Gautron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1\">Emilie Kaufmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_O/0/1/0/all/0/1\">Odalric-Ambryn Maillard</a>",
          "description": "In this paper we study a multi-arm bandit problem in which the quality of\neach arm is measured by the Conditional Value at Risk (CVaR) at some level\nalpha of the reward distribution. While existing works in this setting mainly\nfocus on Upper Confidence Bound algorithms, we introduce a new Thompson\nSampling approach for CVaR bandits on bounded rewards that is flexible enough\nto solve a variety of problems grounded on physical resources. Building on a\nrecent work by Riou & Honda (2020), we introduce B-CVTS for continuous bounded\nrewards and M-CVTS for multinomial distributions. On the theoretical side, we\nprovide a non-trivial extension of their analysis that enables to theoretically\nbound their CVaR regret minimization performance. Strikingly, our results show\nthat these strategies are the first to provably achieve asymptotic optimality\nin CVaR bandits, matching the corresponding asymptotic lower bounds for this\nsetting. Further, we illustrate empirically the benefit of Thompson Sampling\napproaches both in a realistic environment simulating a use-case in agriculture\nand on various synthetic examples.",
          "link": "http://arxiv.org/abs/2012.05754",
          "publishedOn": "2021-07-28T02:02:34.225Z",
          "wordCount": 637,
          "title": "Optimal Thompson Sampling strategies for support-aware CVaR bandits. (arXiv:2012.05754v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1711.03639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1\">Thodoris Lykouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1\">Karthik Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardos_E/0/1/0/all/0/1\">Eva Tardos</a>",
          "description": "We consider the problem of adversarial (non-stochastic) online learning with\npartial information feedback, where at each round, a decision maker selects an\naction from a finite set of alternatives. We develop a black-box approach for\nsuch problems where the learner observes as feedback only losses of a subset of\nthe actions that includes the selected action. When losses of actions are\nnon-negative, under the graph-based feedback model introduced by Mannor and\nShamir, we offer algorithms that attain the so called \"small-loss\" $o(\\alpha\nL^{\\star})$ regret bounds with high probability, where $\\alpha$ is the\nindependence number of the graph, and $L^{\\star}$ is the loss of the best\naction. Prior to our work, there was no data-dependent guarantee for general\nfeedback graphs even for pseudo-regret (without dependence on the number of\nactions, i.e. utilizing the increased information feedback). Taking advantage\nof the black-box nature of our technique, we extend our results to many other\napplications such as semi-bandits (including routing in networks), contextual\nbandits (even with an infinite comparator class), as well as learning with\nslowly changing (shifting) comparators.\n\nIn the special case of classical bandit and semi-bandit problems, we provide\noptimal small-loss, high-probability guarantees of\n$\\tilde{O}(\\sqrt{dL^{\\star}})$ for actual regret, where $d$ is the number of\nactions, answering open questions of Neu. Previous bounds for bandits and\nsemi-bandits were known only for pseudo-regret and only in expectation. We also\noffer an optimal $\\tilde{O}(\\sqrt{\\kappa L^{\\star}})$ regret guarantee for\nfixed feedback graphs with clique-partition number at most $\\kappa$.",
          "link": "http://arxiv.org/abs/1711.03639",
          "publishedOn": "2021-07-28T02:02:34.208Z",
          "wordCount": 765,
          "title": "Small-loss bounds for online learning with partial information. (arXiv:1711.03639v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1\">Somayeh Sojoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>",
          "description": "Variants of Graph Neural Networks (GNNs) for representation learning have\nbeen proposed recently and achieved fruitful results in various fields. Among\nthem, Graph Attention Network (GAT) first employs a self-attention strategy to\nlearn attention weights for each edge in the spatial domain. However, learning\nthe attentions over edges can only focus on the local information of graphs and\ngreatly increases the computational costs. In this paper, we first introduce\nthe attention mechanism in the spectral domain of graphs and present Spectral\nGraph Attention Network (SpGAT) that learns representations for different\nfrequency components regarding weighted filters and graph wavelets bases. In\nthis way, SpGAT can better capture global patterns of graphs in an efficient\nmanner with much fewer learned parameters than that of GAT. Further, to reduce\nthe computational cost of SpGAT brought by the eigen-decomposition, we propose\na fast approximation variant SpGAT-Cheby. We thoroughly evaluate the\nperformance of SpGAT and SpGAT-Cheby in semi-supervised node classification\ntasks and verify the effectiveness of the learned attentions in the spectral\ndomain.",
          "link": "http://arxiv.org/abs/2003.07450",
          "publishedOn": "2021-07-28T02:02:34.201Z",
          "wordCount": 655,
          "title": "Spectral Graph Attention Network with Fast Eigen-approximation. (arXiv:2003.07450v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonet_D/0/1/0/all/0/1\">David Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_A/0/1/0/all/0/1\">Antonio Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Hidalgo_J/0/1/0/all/0/1\">Javier Ruiz-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekkizhar_S/0/1/0/all/0/1\">Sarath Shekkizhar</a>",
          "description": "State-of-the-art neural network architectures continue to scale in size and\ndeliver impressive generalization results, although this comes at the expense\nof limited interpretability. In particular, a key challenge is to determine\nwhen to stop training the model, as this has a significant impact on\ngeneralization. Convolutional neural networks (ConvNets) comprise\nhigh-dimensional feature spaces formed by the aggregation of multiple channels,\nwhere analyzing intermediate data representations and the model's evolution can\nbe challenging owing to the curse of dimensionality. We present channel-wise\nDeepNNK (CW-DeepNNK), a novel channel-wise generalization estimate based on\nnon-negative kernel regression (NNK) graphs with which we perform local\npolytope interpolation on low-dimensional channels. This method leads to\ninstance-based interpretability of both the learned data representations and\nthe relationship between channels. Motivated by our observations, we use\nCW-DeepNNK to propose a novel early stopping criterion that (i) does not\nrequire a validation set, (ii) is based on a task performance metric, and (iii)\nallows stopping to be reached at different points for each channel. Our\nexperiments demonstrate that our proposed method has advantages as compared to\nthe standard criterion based on validation set performance.",
          "link": "http://arxiv.org/abs/2107.12972",
          "publishedOn": "2021-07-28T02:02:34.193Z",
          "wordCount": 631,
          "title": "Channel-Wise Early Stopping without a Validation Set via NNK Polytope Interpolation. (arXiv:2107.12972v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarchi_Z/0/1/0/all/0/1\">Zahra Shekarchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1\">Suzanne Stevenson</a>",
          "description": "Children learn word meanings by tapping into the commonalities across\ndifferent situations in which words are used and overcome the high level of\nuncertainty involved in early word learning experiences. We propose a modeling\nframework to investigate the role of mutual exclusivity bias - asserting\none-to-one mappings between words and their meanings - in reducing uncertainty\nin word learning. In a set of computational studies, we show that to\nsuccessfully learn word meanings in the face of uncertainty, a learner needs to\nuse two types of competition: words competing for association to a referent\nwhen learning from an observation and referents competing for a word when the\nword is used. Our work highlights the importance of an algorithmic-level\nanalysis to shed light on the utility of different mechanisms that can\nimplement the same computational-level theory.",
          "link": "http://arxiv.org/abs/2012.03370",
          "publishedOn": "2021-07-28T02:02:34.176Z",
          "wordCount": 619,
          "title": "Competition in Cross-situational Word Learning: A Computational Study. (arXiv:2012.03370v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesaire_M/0/1/0/all/0/1\">Manon C&#xe9;saire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1\">Hatem Hajri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "This paper introduces stochastic sparse adversarial attacks (SSAA), simple,\nfast and purely noise-based targeted and untargeted $L_0$ attacks of neural\nnetwork classifiers (NNC). SSAA are devised by exploiting a simple small-time\nexpansion idea widely used for Markov processes and offer new examples of $L_0$\nattacks whose studies have been limited. They are designed to solve the known\nscalability issue of the family of Jacobian-based saliency maps attacks to\nlarge datasets and they succeed in solving it. Experiments on small and large\ndatasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in\ncomparison with the-state-of-the-art methods. For instance, in the untargeted\ncase, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently\nto ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up\nto $\\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better\n$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful\non a large number of samples. Codes are publicly available through the link\nhttps://github.com/SSAA3/stochastic-sparse-adv-attacks",
          "link": "http://arxiv.org/abs/2011.12423",
          "publishedOn": "2021-07-28T02:02:34.166Z",
          "wordCount": 644,
          "title": "Stochastic sparse adversarial attacks. (arXiv:2011.12423v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Archit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Reinforcement learning (RL) promises to enable autonomous acquisition of\ncomplex behaviors for diverse agents. However, the success of current\nreinforcement learning algorithms is predicated on an often under-emphasised\nrequirement -- each trial needs to start from a fixed initial state\ndistribution. Unfortunately, resetting the environment to its initial state\nafter each trial requires substantial amount of human supervision and extensive\ninstrumentation of the environment which defeats the purpose of autonomous\nreinforcement learning. In this work, we propose Value-accelerated Persistent\nReinforcement Learning (VaPRL), which generates a curriculum of initial states\nsuch that the agent can bootstrap on the success of easier tasks to efficiently\nlearn harder tasks. The agent also learns to reach the initial states proposed\nby the curriculum, minimizing the reliance on human interventions into the\nlearning. We observe that VaPRL reduces the interventions required by three\norders of magnitude compared to episodic RL while outperforming prior\nstate-of-the art methods for reset-free RL both in terms of sample efficiency\nand asymptotic performance on a variety of simulated robotics problems.",
          "link": "http://arxiv.org/abs/2107.12931",
          "publishedOn": "2021-07-28T02:02:34.146Z",
          "wordCount": 605,
          "title": "Persistent Reinforcement Learning via Subgoal Curricula. (arXiv:2107.12931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12869",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Duong_Q/0/1/0/all/0/1\">Quan Duong</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tran_T/0/1/0/all/0/1\">Tan Tran</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pham_D/0/1/0/all/0/1\">Duc-Thinh Pham</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mai_A/0/1/0/all/0/1\">An Mai</a>",
          "description": "The volume of flight traffic gets increasing over the time, which makes the\nstrategic traffic flow management become one of the challenging problems since\nit requires a lot of computational resources to model entire traffic data. On\nthe other hand, Automatic Dependent Surveillance - Broadcast (ADS-B) technology\nhas been considered as a promising data technology to provide both flight crews\nand ground control staff the necessary information safely and efficiently about\nthe position and velocity of the airplanes in a specific area. In the attempt\nto tackle this problem, we presented in this paper a simplified framework that\ncan support to detect the typical air routes between airports based on ADS-B\ndata. Specifically, the flight traffic will be classified into major groups\nbased on similarity measures, which helps to reduce the number of flight paths\nbetween airports. As a matter of fact, our framework can be taken into account\nto reduce practically the computational cost for air flow optimization and\nevaluate the operational performance. Finally, in order to illustrate the\npotential applications of our proposed framework, an experiment was performed\nusing ADS-B traffic flight data of three different pairs of airports. The\ndetected typical routes between each couple of airports show promising results\nby virtue of combining two indices for measuring the clustering performance and\nincorporating human judgment into the visual inspection.",
          "link": "http://arxiv.org/abs/2107.12869",
          "publishedOn": "2021-07-28T02:02:34.139Z",
          "wordCount": 681,
          "title": "A Simplified Framework for Air Route Clustering Based on ADS-B Data. (arXiv:2107.12869v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12970",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifeng Zhao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Galindo_Torres_S/0/1/0/all/0/1\">S.A. Galindo-Torres</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Longitudinal Dispersion(LD) is the dominant process of scalar transport in\nnatural streams. An accurate prediction on LD coefficient(Dl) can produce a\nperformance leap in related simulation. The emerging machine learning(ML)\ntechniques provide a self-adaptive tool for this problem. However, most of the\nexisting studies utilize an unproved quaternion feature set, obtained through\nsimple theoretical deduction. Few studies have put attention on its reliability\nand rationality. Besides, due to the lack of comparative comparison, the proper\nchoice of ML models in different scenarios still remains unknown. In this\nstudy, the Feature Gradient selector was first adopted to distill the local\noptimal feature sets directly from multivariable data. Then, a global optimal\nfeature set (the channel width, the flow velocity, the channel slope and the\ncross sectional area) was proposed through numerical comparison of the\ndistilled local optimums in performance with representative ML models. The\nchannel slope is identified to be the key parameter for the prediction of LDC.\nFurther, we designed a weighted evaluation metric which enables comprehensive\nmodel comparison. With the simple linear model as the baseline, a benchmark of\nsingle and ensemble learning models was provided. Advantages and disadvantages\nof the methods involved were also discussed. Results show that the support\nvector machine has significantly better performance than other models. Decision\ntree is not suitable for this problem due to poor generalization ability.\nNotably, simple models show superiority over complicated model on this\nlow-dimensional problem, for their better balance between regression and\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.12970",
          "publishedOn": "2021-07-28T02:02:34.133Z",
          "wordCount": 691,
          "title": "A Data-driven feature selection and machine-learning model benchmark for the prediction of longitudinal dispersion coefficient. (arXiv:2107.12970v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12978",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>",
          "description": "There are many clinical contexts which require accurate detection and\nsegmentation of all focal pathologies (e.g. lesions, tumours) in patient\nimages. In cases where there are a mix of small and large lesions, standard\nbinary cross entropy loss will result in better segmentation of large lesions\nat the expense of missing small ones. Adjusting the operating point to\naccurately detect all lesions generally leads to oversegmentation of large\nlesions. In this work, we propose a novel reweighing strategy to eliminate this\nperformance gap, increasing small pathology detection performance while\nmaintaining segmentation accuracy. We show that our reweighing strategy vastly\noutperforms competing strategies based on experiments on a large scale,\nmulti-scanner, multi-center dataset of Multiple Sclerosis patient images.",
          "link": "http://arxiv.org/abs/2107.12978",
          "publishedOn": "2021-07-28T02:02:34.125Z",
          "wordCount": 584,
          "title": "Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting. (arXiv:2107.12978v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sommer_D/0/1/0/all/0/1\">David M. Sommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abfalterer_L/0/1/0/all/0/1\">Lukas Abfalterer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zingg_S/0/1/0/all/0/1\">Sheila Zingg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1\">Esfandiar Mohammadi</a>",
          "description": "Differentially private (DP) mechanisms face the challenge of providing\naccurate results while protecting their inputs: the privacy-utility trade-off.\nA simple but powerful technique for DP adds noise to sensitivity-bounded query\noutputs to blur the exact query output: additive mechanisms. While a vast body\nof work considers infinitely wide noise distributions, some applications (e.g.,\nreal-time operating systems) require hard bounds on the deviations from the\nreal query, and only limited work on such mechanisms exist. An additive\nmechanism with truncated noise (i.e., with bounded range) can offer such hard\nbounds. We introduce a gradient-descent-based tool to learn truncated noise for\nadditive mechanisms with strong utility bounds while simultaneously optimizing\nfor differential privacy under sequential composition, i.e., scenarios where\nmultiple noisy queries on the same data are revealed. Our method can learn\ndiscrete noise patterns and not only hyper-parameters of a predefined\nprobability distribution. For sensitivity bounded mechanisms, we show that it\nis sufficient to consider symmetric and that\\new{, for from the mean\nmonotonically falling noise,} ensuring privacy for a pair of representative\nquery outputs guarantees privacy for all pairs of inputs (that differ in one\nelement). We find that the utility-privacy trade-off curves of our generated\nnoise are remarkably close to truncated Gaussians and even replicate their\nshape for $l_2$ utility-loss. For a low number of compositions, we also\nimproved DP-SGD (sub-sampling). Moreover, we extend Moments Accountant to\ntruncated distributions, allowing to incorporate mechanism output events with\nvarying input-dependent zero occurrence probability.",
          "link": "http://arxiv.org/abs/2107.12957",
          "publishedOn": "2021-07-28T02:02:34.118Z",
          "wordCount": 686,
          "title": "Learning Numeric Optimal Differentially Private Truncated Additive Mechanisms. (arXiv:2107.12957v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kawakami_H/0/1/0/all/0/1\">Hiroki Kawakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_H/0/1/0/all/0/1\">Hirohisa Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Keisuke Sugiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1\">Hiroki Matsutani</a>",
          "description": "Although high-performance deep neural networks are in high demand in edge\nenvironments, computation resources are strictly limited in edge devices, and\nlight-weight neural network techniques, such as Depthwise Separable Convolution\n(DSC), have been developed. ResNet is one of conventional deep neural network\nmodels that stack a lot of layers and parameters for a higher accuracy. To\nreduce the parameter size of ResNet, by utilizing a similarity to ODE (Ordinary\nDifferential Equation), Neural ODE repeatedly uses most of weight parameters\ninstead of having a lot of different parameters. Thus, Neural ODE becomes\nsignificantly small compared to that of ResNet so that it can be implemented in\nresource-limited edge devices. In this paper, a combination of Neural ODE and\nDSC, called dsODENet, is designed and implemented for FPGAs (Field-Programmable\nGate Arrays). dsODENet is then applied to edge domain adaptation as a practical\nuse case and evaluated with image classification datasets. It is implemented on\nXilinx ZCU104 board and evaluated in terms of domain adaptation accuracy,\ntraining speed, FPGA resource utilization, and speedup rate compared to a\nsoftware execution. The results demonstrate that dsODENet is comparable to or\nslightly better than our baseline Neural ODE implementation in terms of domain\nadaptation accuracy, while the total parameter size without pre- and\npost-processing layers is reduced by 54.2% to 79.8%. The FPGA implementation\naccelerates the prediction tasks by 27.9 times faster than a software\nimplementation.",
          "link": "http://arxiv.org/abs/2107.12824",
          "publishedOn": "2021-07-28T02:02:34.103Z",
          "wordCount": 677,
          "title": "A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs. (arXiv:2107.12824v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12878",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alle_S/0/1/0/all/0/1\">Shanmukh Alle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Priyakumar_U/0/1/0/all/0/1\">U. Deva Priyakumar</a>",
          "description": "Parkinson's Disease (PD) is a chronic and progressive neurological disorder\nthat results in rigidity, tremors and postural instability. There is no\ndefinite medical test to diagnose PD and diagnosis is mostly a clinical\nexercise. Although guidelines exist, about 10-30% of the patients are wrongly\ndiagnosed with PD. Hence, there is a need for an accurate, unbiased and fast\nmethod for diagnosis. In this study, we propose LPGNet, a fast and accurate\nmethod to diagnose PD from gait. LPGNet uses Linear Prediction Residuals (LPR)\nto extract discriminating patterns from gait recordings and then uses a 1D\nconvolution neural network with depth-wise separable convolutions to perform\ndiagnosis. LPGNet achieves an AUC of 0.91 with a 21 times speedup and about 99%\nlesser parameters in the model compared to the state of the art. We also\nundertake an analysis of various cross-validation strategies used in literature\nin PD diagnosis from gait and find that most methods are affected by some form\nof data leakage between various folds which leads to unnecessarily large models\nand inflated performance due to overfitting. The analysis clears the path for\nfuture works in correctly evaluating their methods.",
          "link": "http://arxiv.org/abs/2107.12878",
          "publishedOn": "2021-07-28T02:02:34.096Z",
          "wordCount": 664,
          "title": "Linear Prediction Residual for Efficient Diagnosis of Parkinson's Disease from Gait. (arXiv:2107.12878v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hongpeng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibrahim_C/0/1/0/all/0/1\">Chahine Ibrahim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_W/0/1/0/all/0/1\">Wei Xing Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_W/0/1/0/all/0/1\">Wei Pan</a>",
          "description": "This paper proposes a sparse Bayesian treatment of deep neural networks\n(DNNs) for system identification. Although DNNs show impressive approximation\nability in various fields, several challenges still exist for system\nidentification problems. First, DNNs are known to be too complex that they can\neasily overfit the training data. Second, the selection of the input regressors\nfor system identification is nontrivial. Third, uncertainty quantification of\nthe model parameters and predictions are necessary. The proposed Bayesian\napproach offers a principled way to alleviate the above challenges by marginal\nlikelihood/model evidence approximation and structured group sparsity-inducing\npriors construction. The identification algorithm is derived as an iterative\nregularized optimization procedure that can be solved as efficiently as\ntraining typical DNNs. Furthermore, a practical calculation approach based on\nthe Monte-Carlo integration method is derived to quantify the uncertainty of\nthe parameters and predictions. The effectiveness of the proposed Bayesian\napproach is demonstrated on several linear and nonlinear systems identification\nbenchmarks with achieving good and competitive simulation accuracy.",
          "link": "http://arxiv.org/abs/2107.12910",
          "publishedOn": "2021-07-28T02:02:34.086Z",
          "wordCount": 610,
          "title": "Sparse Bayesian Deep Learning for Dynamic System Identification. (arXiv:2107.12910v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumann_L/0/1/0/all/0/1\">Lea Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Me&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>",
          "description": "Emotion is an inherently subjective psychophysiological human-state and to\nproduce an agreed-upon representation (gold standard) for continuous emotion\nrequires a time-consuming and costly training procedure of multiple human\nannotators. There is strong evidence in the literature that physiological\nsignals are sufficient objective markers for states of emotion, particularly\narousal. In this contribution, we utilise a dataset which includes continuous\nemotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal\nActivity (EDA), and Respiration-rate - captured during a stress induced\nscenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,\nRecurrent Neural Network to explore the benefit of fusing these physiological\nsignals with arousal as the target, learning from various audio, video, and\ntextual based features. We utilise the state-of-the-art MuSe-Toolbox to\nconsider both annotation delay and inter-rater agreement weighting when fusing\nthe target signals. An improvement in Concordance Correlation Coefficient (CCC)\nis seen across features sets when fusing EDA with arousal, compared to the\narousal only gold standard results. Additionally, BERT-based textual features'\nresults improved for arousal plus all physiological signals, obtaining up to\n.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also\nimproves overall CCC with audio plus video features obtaining up to .6157 CCC\nto recognize arousal plus EDA and BPM.",
          "link": "http://arxiv.org/abs/2107.12964",
          "publishedOn": "2021-07-28T02:02:34.080Z",
          "wordCount": 660,
          "title": "A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario. (arXiv:2107.12964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perekrestenko_D/0/1/0/all/0/1\">Dmytro Perekrestenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eberhard_L/0/1/0/all/0/1\">L&#xe9;andre Eberhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1\">Helmut B&#xf6;lcskei</a>",
          "description": "We show that every $d$-dimensional probability distribution of bounded\nsupport can be generated through deep ReLU networks out of a $1$-dimensional\nuniform input distribution. What is more, this is possible without incurring a\ncost - in terms of approximation error measured in Wasserstein-distance -\nrelative to generating the $d$-dimensional target distribution from $d$\nindependent random variables. This is enabled by a vast generalization of the\nspace-filling approach discovered in (Bailey & Telgarsky, 2018). The\nconstruction we propose elicits the importance of network depth in driving the\nWasserstein distance between the target distribution and its neural network\napproximation to zero. Finally, we find that, for histogram target\ndistributions, the number of bits needed to encode the corresponding generative\nnetwork equals the fundamental limit for encoding probability distributions as\ndictated by quantization theory.",
          "link": "http://arxiv.org/abs/2107.12466",
          "publishedOn": "2021-07-28T02:02:34.062Z",
          "wordCount": 558,
          "title": "High-Dimensional Distribution Generation Through Deep Neural Networks. (arXiv:2107.12466v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mimi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnell_A/0/1/0/all/0/1\">Andrew Parnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brabazon_D/0/1/0/all/0/1\">Dermot Brabazon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benavoli_A/0/1/0/all/0/1\">Alessio Benavoli</a>",
          "description": "Bayesian optimization (BO) is an approach to globally optimizing black-box\nobjective functions that are expensive to evaluate. BO-powered experimental\ndesign has found wide application in materials science, chemistry, experimental\nphysics, drug development, etc. This work aims to bring attention to the\nbenefits of applying BO in designing experiments and to provide a BO manual,\ncovering both methodology and software, for the convenience of anyone who wants\nto apply or learn BO. In particular, we briefly explain the BO technique,\nreview all the applications of BO in additive manufacturing, compare and\nexemplify the features of different open BO libraries, unlock new potential\napplications of BO to other types of data (e.g., preferential output). This\narticle is aimed at readers with some understanding of Bayesian methods, but\nnot necessarily with knowledge of additive manufacturing; the software\nperformance overview and implementation instructions are instrumental for any\nexperimental-design practitioner. Moreover, our review in the field of additive\nmanufacturing highlights the current knowledge and technological trends of BO.",
          "link": "http://arxiv.org/abs/2107.12809",
          "publishedOn": "2021-07-28T02:02:34.055Z",
          "wordCount": 606,
          "title": "Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. (arXiv:2107.12809v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strumke_I/0/1/0/all/0/1\">Inga Str&#xfc;mke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavkovik_M/0/1/0/all/0/1\">Marija Slavkovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madai_V/0/1/0/all/0/1\">Vince Madai</a>",
          "description": "While the demand for ethical artificial intelligence (AI) systems increases,\nthe number of unethical uses of AI accelerates, even though there is no\nshortage of ethical guidelines. We argue that a main underlying cause for this\nis that AI developers face a social dilemma in AI development ethics,\npreventing the widespread adaptation of ethical best practices. We define the\nsocial dilemma for AI development and describe why the current crisis in AI\ndevelopment ethics cannot be solved without relieving AI developers of their\nsocial dilemma. We argue that AI development must be professionalised to\novercome the social dilemma, and discuss how medicine can be used as a template\nin this process.",
          "link": "http://arxiv.org/abs/2107.12977",
          "publishedOn": "2021-07-28T02:02:34.046Z",
          "wordCount": 558,
          "title": "The social dilemma in AI development and why we have to solve it. (arXiv:2107.12977v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12838",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Noman_F/0/1/0/all/0/1\">Fuad Noman</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ting_C/0/1/0/all/0/1\">Chee-Ming Ting</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kang_H/0/1/0/all/0/1\">Hakmook Kang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Phan_R/0/1/0/all/0/1\">Raphael C.-W. Phan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Boyd_B/0/1/0/all/0/1\">Brian D. Boyd</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Taylor_W/0/1/0/all/0/1\">Warren D. Taylor</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ombao_H/0/1/0/all/0/1\">Hernando Ombao</a>",
          "description": "Brain functional connectivity (FC) reveals biomarkers for identification of\nvarious neuropsychiatric disorders. Recent application of deep neural networks\n(DNNs) to connectome-based classification mostly relies on traditional\nconvolutional neural networks using input connectivity matrices on a regular\nEuclidean grid. We propose a graph deep learning framework to incorporate the\nnon-Euclidean information about graph structure for classifying functional\nmagnetic resonance imaging (fMRI)- derived brain networks in major depressive\ndisorder (MDD). We design a novel graph autoencoder (GAE) architecture based on\nthe graph convolutional networks (GCNs) to embed the topological structure and\nnode content of large-sized fMRI networks into low-dimensional latent\nrepresentations. In network construction, we employ the Ledoit-Wolf (LDW)\nshrinkage method to estimate the high-dimensional FC metrics efficiently from\nfMRI data. We consider both supervised and unsupervised approaches for the\ngraph embedded learning. The learned embeddings are then used as feature inputs\nfor a deep fully-connected neural network (FCNN) to discriminate MDD from\nhealthy controls. Evaluated on a resting-state fMRI MDD dataset with 43\nsubjects, results show that the proposed GAE-FCNN model significantly\noutperforms several state-of-the-art DNN methods for brain connectome\nclassification, achieving accuracy of 72.50% using the LDW-FC metrics as node\nfeatures. The graph embeddings of fMRI FC networks learned by the GAE also\nreveal apparent group differences between MDD and HC. Our new framework\ndemonstrates feasibility of learning graph embeddings on brain networks to\nprovide discriminative information for diagnosis of brain disorders.",
          "link": "http://arxiv.org/abs/2107.12838",
          "publishedOn": "2021-07-28T02:02:34.021Z",
          "wordCount": 691,
          "title": "Graph Autoencoders for Embedding Learning in Brain Networks and Major Depressive Disorder Identification. (arXiv:2107.12838v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12889",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Felfeliyan_B/0/1/0/all/0/1\">Banafshe Felfeliyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuntze_G/0/1/0/all/0/1\">Gregor Kuntze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob L. Jaremko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ronsky_J/0/1/0/all/0/1\">Janet L. Ronsky</a>",
          "description": "Objective assessment of Magnetic Resonance Imaging (MRI) scans of\nosteoarthritis (OA) can address the limitation of the current OA assessment.\nSegmentation of bone, cartilage, and joint fluid is necessary for the OA\nobjective assessment. Most of the proposed segmentation methods are not\nperforming instance segmentation and suffer from class imbalance problems. This\nstudy deployed Mask R-CNN instance segmentation and improved it (improved-Mask\nR-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for\nOA-associated tissues. Training and validation of the method were performed\nusing 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI\nscans of patients with symptomatic hip OA. Three modifications to Mask R-CNN\nyielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder\nlayer to the mask-header, and connecting them by a skip connection. The results\nwere assessed using Hausdorff distance, dice score, and coefficients of\nvariation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation\ncompared to Mask RCNN as indicated with the increase in dice score from 95% to\n98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and\n81% to 82% for tibial cartilage. For the effusion detection, dice improved with\niMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection\nbetween Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2\nand Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between\ntwo readers (0.21), indicating a high agreement between the human readers and\nboth Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and\nsimultaneously extract different scale articular tissues involved in OA,\nforming the foundation for automated assessment of OA. The iMaskRCNN results\nshow that the modification improved the network performance around the edges.",
          "link": "http://arxiv.org/abs/2107.12889",
          "publishedOn": "2021-07-28T02:02:34.013Z",
          "wordCount": 755,
          "title": "Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative). (arXiv:2107.12889v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaeckle_F/0/1/0/all/0/1\">Florian Jaeckle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingyue Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>",
          "description": "Many available formal verification methods have been shown to be instances of\na unified Branch-and-Bound (BaB) formulation. We propose a novel machine\nlearning framework that can be used for designing an effective branching\nstrategy as well as for computing better lower bounds. Specifically, we learn\ntwo graph neural networks (GNN) that both directly treat the network we want to\nverify as a graph input and perform forward-backward passes through the GNN\nlayers. We use one GNN to simulate the strong branching heuristic behaviour and\nanother to compute a feasible dual solution of the convex relaxation, thereby\nproviding a valid lower bound.\n\nWe provide a new verification dataset that is more challenging than those\nused in the literature, thereby providing an effective alternative for testing\nalgorithmic improvements for verification. Whilst using just one of the GNNs\nleads to a reduction in verification time, we get optimal performance when\ncombining the two GNN approaches. Our combined framework achieves a 50\\%\nreduction in both the number of branches and the time required for verification\non various convolutional networks when compared to several state-of-the-art\nverification methods. In addition, we show that our GNN models generalize well\nto harder properties on larger unseen networks.",
          "link": "http://arxiv.org/abs/2107.12855",
          "publishedOn": "2021-07-28T02:02:33.965Z",
          "wordCount": 638,
          "title": "Neural Network Branch-and-Bound for Neural Network Verification. (arXiv:2107.12855v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1\">Patrik Joslin Kenfack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Adil Mehmood Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1\">Rasheed Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1\">S.M. Ahsan Kazmi</a>,",
          "description": "Training machine learning models with the only accuracy as a final goal may\npromote prejudices and discriminatory behaviors embedded in the data. One\nsolution is to learn latent representations that fulfill specific fairness\nmetrics. Different types of learning methods are employed to map data into the\nfair representational space. The main purpose is to learn a latent\nrepresentation of data that scores well on a fairness metric while maintaining\nthe usability for the downstream task. In this paper, we propose a new fair\nrepresentation learning approach that leverages different levels of\nrepresentation of data to tighten the fairness bounds of the learned\nrepresentation. Our results show that stacking different auto-encoders and\nenforcing fairness at different latent spaces result in an improvement of\nfairness compared to other existing approaches.",
          "link": "http://arxiv.org/abs/2107.12826",
          "publishedOn": "2021-07-28T02:02:33.952Z",
          "wordCount": 568,
          "title": "Adversarial Stacked Auto-Encoders for Fair Representation Learning. (arXiv:2107.12826v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12797",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kepler_M/0/1/0/all/0/1\">Michael E. Kepler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bedi_A/0/1/0/all/0/1\">Amrit Singh Bedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Stilwell_D/0/1/0/all/0/1\">Daniel J. Stilwell</a>",
          "description": "Gaussian processes (GPs) are a well-known nonparametric Bayesian inference\ntechnique, but they suffer from scalability problems for large sample sizes,\nand their performance can degrade for non-stationary or spatially heterogeneous\ndata. In this work, we seek to overcome these issues through (i) employing\nvariational free energy approximations of GPs operating in tandem with online\nexpectation propagation steps; and (ii) introducing a local splitting step\nwhich instantiates a new GP whenever the posterior distribution changes\nsignificantly as quantified by the Wasserstein metric over posterior\ndistributions. Over time, then, this yields an ensemble of sparse GPs which may\nbe updated incrementally, and adapts to locality, heterogeneity, and\nnon-stationarity in training data.",
          "link": "http://arxiv.org/abs/2107.12797",
          "publishedOn": "2021-07-28T02:02:33.936Z",
          "wordCount": 548,
          "title": "Wasserstein-Splitting Gaussian Process Regression for Heterogeneous Online Bayesian Inference. (arXiv:2107.12797v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhiwen Pan</a>",
          "description": "Nowadays, multi-sensor technologies are applied in many fields, e.g., Health\nCare (HC), Human Activity Recognition (HAR), and Industrial Control System\n(ICS). These sensors can generate a substantial amount of multivariate\ntime-series data. Unsupervised anomaly detection on multi-sensor time-series\ndata has been proven critical in machine learning researches. The key challenge\nis to discover generalized normal patterns by capturing spatial-temporal\ncorrelation in multi-sensor data. Beyond this challenge, the noisy data is\noften intertwined with the training data, which is likely to mislead the model\nby making it hard to distinguish between the normal, abnormal, and noisy data.\nFew of previous researches can jointly address these two challenges. In this\npaper, we propose a novel deep learning-based anomaly detection algorithm\ncalled Deep Convolutional Autoencoding Memory network (CAE-M). We first build a\nDeep Convolutional Autoencoder to characterize spatial dependence of\nmulti-sensor data with a Maximum Mean Discrepancy (MMD) to better distinguish\nbetween the noisy, normal, and abnormal data. Then, we construct a Memory\nNetwork consisting of linear (Autoregressive Model) and non-linear predictions\n(Bidirectional LSTM with Attention) to capture temporal dependence from\ntime-series data. Finally, CAE-M jointly optimizes these two subnetworks. We\nempirically compare the proposed approach with several state-of-the-art anomaly\ndetection methods on HAR and HC datasets. Experimental results demonstrate that\nour proposed model outperforms these existing methods.",
          "link": "http://arxiv.org/abs/2107.12626",
          "publishedOn": "2021-07-28T02:02:33.901Z",
          "wordCount": 671,
          "title": "Unsupervised Deep Anomaly Detection for Multi-Sensor Time-Series Signals. (arXiv:2107.12626v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ausset_G/0/1/0/all/0/1\">Guillaume Ausset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciffreo_T/0/1/0/all/0/1\">Tom Ciffreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portier_F/0/1/0/all/0/1\">Francois Portier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Cl&#xe9;men&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papin_T/0/1/0/all/0/1\">Timoth&#xe9;e Papin</a>",
          "description": "Survival analysis, or time-to-event modelling, is a classical statistical\nproblem that has garnered a lot of interest for its practical use in\nepidemiology, demographics or actuarial sciences. Recent advances on the\nsubject from the point of view of machine learning have been concerned with\nprecise per-individual predictions instead of population studies, driven by the\nrise of individualized medicine. We introduce here a conditional normalizing\nflow based estimate of the time-to-event density as a way to model highly\nflexible and individualized conditional survival distributions. We use a novel\nhierarchical formulation of normalizing flows to enable efficient fitting of\nflexible conditional distributions without overfitting and show how the\nnormalizing flow formulation can be efficiently adapted to the censored\nsetting. We experimentally validate the proposed approach on a synthetic\ndataset as well as four open medical datasets and an example of a common\nfinancial problem.",
          "link": "http://arxiv.org/abs/2107.12825",
          "publishedOn": "2021-07-28T02:02:33.893Z",
          "wordCount": 579,
          "title": "Individual Survival Curves with Conditional Normalizing Flows. (arXiv:2107.12825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuesheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haizhang Zhang</a>",
          "description": "We explore convergence of deep neural networks with the popular ReLU\nactivation function, as the depth of the networks tends to infinity. To this\nend, we introduce the notion of activation domains and activation matrices of a\nReLU network. By replacing applications of the ReLU activation function by\nmultiplications with activation matrices on activation domains, we obtain an\nexplicit expression of the ReLU network. We then identify the convergence of\nthe ReLU networks as convergence of a class of infinite products of matrices.\nSufficient and necessary conditions for convergence of these infinite products\nof matrices are studied. As a result, we establish necessary conditions for\nReLU networks to converge that the sequence of weight matrices converges to the\nidentity matrix and the sequence of the bias vectors converges to zero as the\ndepth of ReLU networks increases to infinity. Moreover, we obtain sufficient\nconditions in terms of the weight matrices and bias vectors at hidden layers\nfor pointwise convergence of deep ReLU networks. These results provide\nmathematical insights to the design strategy of the well-known deep residual\nnetworks in image classification.",
          "link": "http://arxiv.org/abs/2107.12530",
          "publishedOn": "2021-07-28T02:02:33.802Z",
          "wordCount": 607,
          "title": "Convergence of Deep ReLU Networks. (arXiv:2107.12530v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1\">Gourav Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1\">Massoud Pedram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>",
          "description": "Deep spiking neural networks (SNNs) have emerged as a potential alternative\nto traditional deep learning frameworks, due to their promise to provide\nincreased compute efficiency on event-driven neuromorphic hardware. However, to\nperform well on complex vision applications, most SNN training frameworks yield\nlarge inference latency which translates to increased spike activity and\nreduced energy efficiency. Hence,minimizing average spike activity while\npreserving accuracy indeep SNNs remains a significant challenge and\nopportunity.This paper presents a non-iterative SNN training technique\nthatachieves ultra-high compression with reduced spiking activitywhile\nmaintaining high inference accuracy. In particular, our framework first uses\nthe attention-maps of an un compressed meta-model to yield compressed ANNs.\nThis step can be tuned to support both irregular and structured channel pruning\nto leverage computational benefits over a broad range of platforms. The\nframework then performs sparse-learning-based supervised SNN training using\ndirect inputs. During the training, it jointly optimizes the SNN weight,\nthreshold, and leak parameters to drastically minimize the number of time steps\nrequired while retaining compression. To evaluate the merits of our approach,\nwe performed experiments with variants of VGG and ResNet, on both CIFAR-10 and\nCIFAR-100, and VGG16 on Tiny-ImageNet.The SNN models generated through the\nproposed technique yield SOTA compression ratios of up to 33.4x with no\nsignificant drops in accuracy compared to baseline unpruned counterparts.\nCompared to existing SNN pruning methods, we achieve up to 8.3x higher\ncompression with improved accuracy.",
          "link": "http://arxiv.org/abs/2107.12445",
          "publishedOn": "2021-07-28T02:02:33.598Z",
          "wordCount": null,
          "title": "Towards Low-Latency Energy-Efficient Deep SNNs via Attention-Guided Compression. (arXiv:2107.12445v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.",
          "link": "http://arxiv.org/abs/2107.12636",
          "publishedOn": "2021-07-28T02:02:33.591Z",
          "wordCount": null,
          "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koren_M/0/1/0/all/0/1\">Mark Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1\">Ahmed Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Validating the safety of autonomous systems generally requires the use of\nhigh-fidelity simulators that adequately capture the variability of real-world\nscenarios. However, it is generally not feasible to exhaustively search the\nspace of simulation scenarios for failures. Adaptive stress testing (AST) is a\nmethod that uses reinforcement learning to find the most likely failure of a\nsystem. AST with a deep reinforcement learning solver has been shown to be\neffective in finding failures across a range of different systems. This\napproach generally involves running many simulations, which can be very\nexpensive when using a high-fidelity simulator. To improve efficiency, we\npresent a method that first finds failures in a low-fidelity simulator. It then\nuses the backward algorithm, which trains a deep neural network policy using a\nsingle expert demonstration, to adapt the low-fidelity failures to\nhigh-fidelity. We have created a series of autonomous vehicle validation case\nstudies that represent some of the ways low-fidelity and high-fidelity\nsimulators can differ, such as time discretization. We demonstrate in a variety\nof case studies that this new AST approach is able to find failures with\nsignificantly fewer high-fidelity simulation steps than are needed when just\nrunning AST directly in high-fidelity. As a proof of concept, we also\ndemonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art\nhigh-fidelity simulator for finding failures in autonomous vehicles.",
          "link": "http://arxiv.org/abs/2107.12940",
          "publishedOn": "2021-07-28T02:02:33.548Z",
          "wordCount": null,
          "title": "Finding Failures in High-Fidelity Simulation using Adaptive Stress Testing and the Backward Algorithm. (arXiv:2107.12940v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1\">Christopher R. Hoyt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owen_A/0/1/0/all/0/1\">Art B. Owen</a>",
          "description": "We use graphical methods to probe neural nets that classify images. Plots of\nt-SNE outputs at successive layers in a network reveal increasingly organized\narrangement of the data points. They can also reveal how a network can diminish\nor even forget about within-class structure as the data proceeds through\nlayers. We use class-specific analogues of principal components to visualize\nhow succeeding layers separate the classes. These allow us to sort images from\na given class from most typical to least typical (in the data) and they also\nserve as very useful projection coordinates for data visualization. We find\nthem especially useful when defining versions guided tours for animated data\nvisualization.",
          "link": "http://arxiv.org/abs/2107.12547",
          "publishedOn": "2021-07-28T02:02:33.516Z",
          "wordCount": null,
          "title": "Probing neural networks with t-SNE, class-specific projections and a guided tour. (arXiv:2107.12547v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.",
          "link": "http://arxiv.org/abs/2107.12628",
          "publishedOn": "2021-07-28T02:02:33.502Z",
          "wordCount": null,
          "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration. (arXiv:2107.12628v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>",
          "description": "Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.",
          "link": "http://arxiv.org/abs/2107.12619",
          "publishedOn": "2021-07-28T02:02:33.499Z",
          "wordCount": null,
          "title": "Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menculini_L/0/1/0/all/0/1\">Lorenzo Menculini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marini_A/0/1/0/all/0/1\">Andrea Marini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proietti_M/0/1/0/all/0/1\">Massimiliano Proietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garinei_A/0/1/0/all/0/1\">Alberto Garinei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozza_A/0/1/0/all/0/1\">Alessio Bozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moretti_C/0/1/0/all/0/1\">Cecilia Moretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marconi_M/0/1/0/all/0/1\">Marcello Marconi</a>",
          "description": "Setting sale prices correctly is of great importance for firms, and the study\nand forecast of prices time series is therefore a relevant topic not only from\na data science perspective but also from an economic and applicative one. In\nthis paper we exhamine different techniques to forecast the sale prices of\nthree food products applied by an Italian food wholesaler, as a step towards\nthe automation of pricing tasks usually taken care by human workforce. We\nconsider ARIMA models and compare them to Prophet, a scalable forecasting tool\ndeveloped by Facebook and based on a generalized additive model, and to deep\nlearning models based on Long Short--Term Memory (LSTM) and Convolutional\nNeural Networks (CNNs). ARIMA models are frequently used in econometric\nanalyses, providing a good bechmark for the problem under study. Our results\nindicate that ARIMA performs similarly to LSTM neural networks for the problem\nunder study, while the combination of CNNs and LSTMs attains the best overall\naccuracy, but requires more time to be tuned. On the contrary, Prophet is very\nfast to use, but less accurate.",
          "link": "http://arxiv.org/abs/2107.12770",
          "publishedOn": "2021-07-28T02:02:33.496Z",
          "wordCount": null,
          "title": "Comparing Prophet and Deep Learning to ARIMA in Forecasting Wholesale Food Prices. (arXiv:2107.12770v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lesley Tan</a>",
          "description": "In this paper, we investigate data-driven parameterized modeling of insertion\nloss for transmission lines with respect to design parameters. We first show\nthat direct application of neural networks can lead to non-physics models with\nnegative insertion loss. To mitigate this problem, we propose two deep learning\nsolutions. One solution is to add a regulation term, which represents the\npassive condition, to the final loss function to enforce the negative quantity\nof insertion loss. In the second method, a third-order polynomial expression is\ndefined first, which ensures positiveness, to approximate the insertion loss,\nthen DeepONet neural network structure, which was proposed recently for\nfunction and system modeling, was employed to model the coefficients of\npolynomials. The resulting neural network is applied to predict the\ncoefficients of the polynomial expression. The experimental results on an\nopen-sourced SI/PI database of a PCB design show that both methods can ensure\nthe positiveness for the insertion loss. Furthermore, both methods can achieve\nsimilar prediction results, while the polynomial-based DeepONet method is\nfaster than DeepONet based method in training time.",
          "link": "http://arxiv.org/abs/2107.12527",
          "publishedOn": "2021-07-28T02:02:33.494Z",
          "wordCount": null,
          "title": "Physics-Enforced Modeling for Insertion Loss of Transmission Lines by Deep Neural Networks. (arXiv:2107.12527v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1\">Maithra Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1\">Jon Kleinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1\">Samy Bengio</a>",
          "description": "The successes of deep learning critically rely on the ability of neural\nnetworks to output meaningful predictions on unseen data -- generalization. Yet\ndespite its criticality, there remain fundamental open questions on how neural\nnetworks generalize. How much do neural networks rely on memorization -- seeing\nhighly similar training examples -- and how much are they capable of\nhuman-intelligence styled reasoning -- identifying abstract rules underlying\nthe data? In this paper we introduce a novel benchmark, Pointer Value Retrieval\n(PVR) tasks, that explore the limits of neural network generalization. While\nPVR tasks can consist of visual as well as symbolic inputs, each with varying\nlevels of difficulty, they all have a simple underlying rule. One part of the\nPVR task input acts as a pointer, giving the location of a different part of\nthe input, which forms the value (and output). We demonstrate that this task\nstructure provides a rich testbed for understanding generalization, with our\nempirical study showing large variations in neural network performance based on\ndataset size, task complexity and model architecture. The interaction of\nposition, values and the pointer rule also allow the development of nuanced\ntests of generalization, by introducing distribution shift and increasing\nfunctional complexity. These reveal both subtle failures and surprising\nsuccesses, suggesting many promising directions of exploration on this\nbenchmark.",
          "link": "http://arxiv.org/abs/2107.12580",
          "publishedOn": "2021-07-28T02:02:33.449Z",
          "wordCount": null,
          "title": "Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization. (arXiv:2107.12580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raumanns_R/0/1/0/all/0/1\">Ralf Raumanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_G/0/1/0/all/0/1\">Gerard Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joosten_M/0/1/0/all/0/1\">Max Joosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P. W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>",
          "description": "We present ENHANCE, an open dataset with multiple annotations to complement\nthe existing ISIC and PH2 skin lesion classification datasets. This dataset\ncontains annotations of visual ABC (asymmetry, border, colour) features from\nnon-expert annotation sources: undergraduate students, crowd workers from\nAmazon MTurk and classic image processing algorithms. In this paper we first\nanalyse the correlations between the annotations and the diagnostic label of\nthe lesion, as well as study the agreement between different annotation\nsources. Overall we find weak correlations of non-expert annotations with the\ndiagnostic label, and low agreement between different annotation sources. We\nthen study multi-task learning (MTL) with the annotations as additional labels,\nand show that non-expert annotations can improve (ensembles of)\nstate-of-the-art convolutional neural networks via MTL. We hope that our\ndataset can be used in further research into multiple annotations and/or MTL.\nAll data and models are available on Github:\nhttps://github.com/raumannsr/ENHANCE.",
          "link": "http://arxiv.org/abs/2107.12734",
          "publishedOn": "2021-07-28T02:02:33.448Z",
          "wordCount": null,
          "title": "ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification. (arXiv:2107.12734v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13493",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kabeli_O/0/1/0/all/0/1\">Ori Kabeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Buye Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>",
          "description": "Deep neural networks have recently shown great success in the task of blind\nsource separation, both under monaural and binaural settings. Although these\nmethods were shown to produce high-quality separations, they were mainly\napplied under offline settings, in which the model has access to the full input\nsignal while separating the signal. In this study, we convert a non-causal\nstate-of-the-art separation model into a causal and real-time model and\nevaluate its performance under both online and offline settings. We compare the\nperformance of the proposed model to several baseline methods under anechoic,\nnoisy, and noisy-reverberant recording conditions while exploring both monaural\nand binaural inputs and outputs. Our findings shed light on the relative\ndifference between causal and non-causal models when performing separation. Our\nstateful implementation for online separation leads to a minor drop in\nperformance compared to the offline model; 0.8dB for monaural inputs and 0.3dB\nfor binaural inputs while reaching a real-time factor of 0.65. Samples can be\nfound under the following link:\nhttps://kwanum.github.io/sagrnnc-stream-results/.",
          "link": "http://arxiv.org/abs/2106.13493",
          "publishedOn": "2021-07-28T02:02:33.412Z",
          "wordCount": 649,
          "title": "Online Self-Attentive Gated RNNs for Real-Time Speaker Separation. (arXiv:2106.13493v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1\">Amir Zandieh</a>",
          "description": "The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely wide\nneural nets trained under least squares loss by gradient descent. However,\ndespite its importance, the super-quadratic runtime of kernel methods limits\nthe use of NTK in large-scale learning tasks. To accelerate kernel machines\nwith NTK, we propose a near input sparsity time algorithm that maps the input\ndata to a randomized low-dimensional feature space so that the inner product of\nthe transformed data approximates their NTK evaluation. Our transformation\nworks by sketching the polynomial expansions of arc-cosine kernels.\nFurthermore, we propose a feature map for approximating the convolutional\ncounterpart of the NTK, which can transform any image using a runtime that is\nonly linear in the number of pixels. We show that in standard large-scale\nregression and classification tasks a linear regressor trained on our features\noutperforms trained Neural Nets and Nystrom approximation of NTK kernel.",
          "link": "http://arxiv.org/abs/2104.00415",
          "publishedOn": "2021-07-28T02:02:33.403Z",
          "wordCount": 611,
          "title": "Learning with Neural Tangent Kernels in Near Input Sparsity Time. (arXiv:2104.00415v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhirui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zekun Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yabang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1\">Andrew Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>",
          "description": "3D deep learning has been increasingly more popular for a variety of tasks\nincluding many safety-critical applications. However, recently several works\nraise the security issues of 3D deep nets. Although most of these works\nconsider adversarial attacks, we identify that backdoor attack is indeed a more\nserious threat to 3D deep learning systems but remains unexplored. We present\nthe backdoor attacks in 3D with a unified framework that exploits the unique\nproperties of 3D data and networks. In particular, we design two attack\napproaches: the poison-label attack and the clean-label attack. The first one\nis straightforward and effective in practice, while the second one is more\nsophisticated assuming there are certain data inspections. The attack\nalgorithms are mainly motivated and developed by 1) the recent discovery of 3D\nadversarial samples which demonstrate the vulnerability of 3D deep nets under\nspatial transformations; 2) the proposed feature disentanglement technique that\nmanipulates the feature of the data through optimization methods and its\npotential to embed a new task. Extensive experiments show the efficacy of the\npoison-label attack with over 95% success rate across several 3D datasets and\nmodels, and the ability of clean-label attack against data filtering with\naround 50% success rate. Our proposed backdoor attack in 3D point cloud is\nexpected to perform as a baseline for improving the robustness of 3D deep\nmodels.",
          "link": "http://arxiv.org/abs/2103.16074",
          "publishedOn": "2021-07-28T02:02:33.391Z",
          "wordCount": 702,
          "title": "PointBA: Towards Backdoor Attacks in 3D Point Cloud. (arXiv:2103.16074v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chernyak_B/0/1/0/all/0/1\">Bronya Roni Chernyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazan_T/0/1/0/all/0/1\">Tamir Hazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1\">Joseph Keshet</a>",
          "description": "This paper proposes an attack-independent (non-adversarial training)\ntechnique for improving adversarial robustness of neural network models, with\nminimal loss of standard accuracy. We suggest creating a neighborhood around\neach training example, such that the label is kept constant for all inputs\nwithin that neighborhood. Unlike previous work that follows a similar\nprinciple, we apply this idea by extending the training set with multiple\nperturbations for each training example, drawn from within the neighborhood.\nThese perturbations are model independent, and remain constant throughout the\nentire training process. We analyzed our method empirically on MNIST, SVHN, and\nCIFAR-10, under different attacks and conditions. Results suggest that the\nproposed approach improves standard accuracy over other defenses while having\nincreased robustness compared to vanilla adversarial training.",
          "link": "http://arxiv.org/abs/2103.08265",
          "publishedOn": "2021-07-28T02:02:33.371Z",
          "wordCount": 594,
          "title": "Constant Random Perturbations Provide Adversarial Robustness with Minimal Effect on Accuracy. (arXiv:2103.08265v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Traditional learning systems are trained in closed-world for a fixed number\nof classes, and need pre-collected datasets in advance. However, new classes\noften emerge in real-world applications and should be learned incrementally.\nFor example, in electronic commerce, new types of products appear daily, and in\na social media community, new topics emerge frequently. Under such\ncircumstances, incremental models should learn several new classes at a time\nwithout forgetting. We find a strong correlation between old and new classes in\nincremental learning, which can be applied to relate and facilitate different\nlearning stages mutually. As a result, we propose CO-transport for class\nIncremental Learning (COIL), which learns to relate across incremental tasks\nwith the class-wise semantic relationship. In detail, co-transport has two\naspects: prospective transport tries to augment the old classifier with optimal\ntransported knowledge as fast model adaptation. Retrospective transport aims to\ntransport new class classifiers backward as old ones to overcome forgetting.\nWith these transports, COIL efficiently adapts to new tasks, and stably resists\nforgetting. Experiments on benchmark and real-world multimedia datasets\nvalidate the effectiveness of our proposed method.",
          "link": "http://arxiv.org/abs/2107.12654",
          "publishedOn": "2021-07-28T02:02:33.338Z",
          "wordCount": 616,
          "title": "Co-Transport for Class-Incremental Learning. (arXiv:2107.12654v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dam_T/0/1/0/all/0/1\">Tanmoy Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1\">Sreenatha G. Anavatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbass_H/0/1/0/all/0/1\">Hussein A. Abbass</a> (Fellow, IEEESchool of Engineering and Information Technology, University of New South Wales Canberra, Australia)",
          "description": "The Latent Space Clustering in Generative adversarial networks (ClusterGAN)\nmethod has been successful with high-dimensional data. However, the method\nassumes uniformlydistributed priors during the generation of modes, which isa\nrestrictive assumption in real-world data and cause loss ofdiversity in the\ngenerated modes. In this paper, we proposeself-augmentation information\nmaximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive\npriorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural\nnetworks: self-augmentation prior network,generator, discriminator and\nclustering inference autoencoder.The proposed method has been validated using\nseven bench-mark data sets and has shown improved performance overstate-of-the\nart methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on\nimbalanced dataset, wehave discussed two imbalanced conditions on MNIST\ndatasetswith one-class imbalance and three classes imbalanced cases.The results\nhighlight the advantages of SIMI-ClusterGAN.",
          "link": "http://arxiv.org/abs/2107.12706",
          "publishedOn": "2021-07-28T02:02:33.331Z",
          "wordCount": 582,
          "title": "Improving ClusterGAN Using Self-AugmentedInformation Maximization of Disentangling LatentSpaces. (arXiv:2107.12706v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1\">Eitan Kosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>",
          "description": "Autonomous driving gained huge traction in recent years, due to its potential\nto change the way we commute. Much effort has been put into trying to estimate\nthe state of a vehicle. Meanwhile, learning to forecast the state of a vehicle\nahead introduces new capabilities, such as predicting dangerous situations.\nMoreover, forecasting brings new supervision opportunities by learning to\npredict richer a context, expressed by multiple horizons. Intuitively, a video\nstream originated from a front-facing camera is necessary because it encodes\ninformation about the upcoming road. Besides, historical traces of the\nvehicle's states give more context. In this paper, we tackle multi-horizon\nforecasting of vehicle states by fusing the two modalities. We design and\nexperiment with 3 end-to-end architectures that exploit 3D convolutions for\nvisual features extraction and 1D convolutions for features extraction from\nspeed and steering angle traces. To demonstrate the effectiveness of our\nmethod, we perform extensive experiments on two publicly available real-world\ndatasets, Comma2k19 and the Udacity challenge. We show that we are able to\nforecast a vehicle's state to various horizons, while outperforming the current\nstate-of-the-art results on the related task of driving state estimation. We\nexamine the contribution of vision features, and find that a model fed with\nvision features achieves an error that is 56.6% and 66.9% of the error of a\nmodel that doesn't use those features, on the Udacity and Comma2k19 datasets\nrespectively.",
          "link": "http://arxiv.org/abs/2107.12674",
          "publishedOn": "2021-07-28T02:02:33.323Z",
          "wordCount": 672,
          "title": "Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting. (arXiv:2107.12674v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-07-28T02:02:33.313Z",
          "wordCount": 637,
          "title": "MU-MIMO Grouping For Real-time Applications. (arXiv:2106.15262v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sohee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungkyu Lee</a>",
          "description": "Continual learning is a concept of online learning with multiple sequential\ntasks. One of the critical barriers of continual learning is that a network\nshould learn a new task keeping the knowledge of old tasks without access to\nany data of the old tasks. In this paper, we propose a neuron activation\nimportance-based regularization method for stable continual learning regardless\nof the order of tasks. We conduct comprehensive experiments on existing\nbenchmark data sets to evaluate not just the stability and plasticity of our\nmethod with improved classification accuracy also the robustness of the\nperformance along the changes of task order.",
          "link": "http://arxiv.org/abs/2107.12657",
          "publishedOn": "2021-07-28T02:02:33.294Z",
          "wordCount": 529,
          "title": "Continual Learning with Neuron Activation Importance. (arXiv:2107.12657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1\">Madhurananda Pahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1\">Thomas Niesler</a>",
          "description": "We present an experimental investigation into the automatic detection of\nCOVID-19 from coughs, breaths and speech as this type of screening is\nnon-contact, does not require specialist medical expertise or laboratory\nfacilities and can easily be deployed on inexpensive consumer hardware.\n\nSmartphone recordings of cough, breath and speech from subjects around the\nglobe are used for classification by seven standard machine learning\nclassifiers using leave-$p$-out cross-validation to provide a promising\nbaseline performance.\n\nThen, a diverse dataset of 10.29 hours of cough, sneeze, speech and noise\naudio recordings are used to pre-train a CNN, LSTM and Resnet50 classifier and\nfine tuned the model to enhance the performance even further.\n\nWe have also extracted the bottleneck features from these pre-trained models\nby removing the final-two layers and used them as an input to the LR, SVM, MLP\nand KNN classifiers to detect COVID-19 signature.\n\nThe highest AUC of 0.98 was achieved using a transfer learning based Resnet50\narchitecture on coughs from Coswara dataset.\n\nThe highest AUC of 0.94 and 0.92 was achieved from an SVM run on the\nbottleneck features extracted from the breaths from Coswara dataset and speech\nrecordings from ComParE dataset.\n\nWe conclude that among all vocal audio, coughs carry the strongest COVID-19\nsignature followed by breath and speech and using transfer learning improves\nthe classifier performance with higher AUC and lower variance across the\ncross-validation folds.\n\nAlthough these signatures are not perceivable by human ear, machine learning\nbased COVID-19 detection is possible from vocal audio recorded via smartphone.",
          "link": "http://arxiv.org/abs/2104.02477",
          "publishedOn": "2021-07-28T02:02:33.288Z",
          "wordCount": 775,
          "title": "Deep Transfer Learning based COVID-19 Detection in Cough, Breath and Speech using Bottleneck Features. (arXiv:2104.02477v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12631",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Jiguang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wymeersch_H/0/1/0/all/0/1\">Henk Wymeersch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Renzo_M/0/1/0/all/0/1\">Marco Di Renzo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Juntti_M/0/1/0/all/0/1\">Markku Juntti</a>",
          "description": "Inspired by the remarkable learning and prediction performance of deep neural\nnetworks (DNNs), we apply one special type of DNN framework, known as\nmodel-driven deep unfolding neural network, to reconfigurable intelligent\nsurface (RIS)-aided millimeter wave (mmWave) single-input multiple-output\n(SIMO) systems. We focus on uplink cascaded channel estimation, where known and\nfixed base station combining and RIS phase control matrices are considered for\ncollecting observations. To boost the estimation performance and reduce the\ntraining overhead, the inherent channel sparsity of mmWave channels is\nleveraged in the deep unfolding method. It is verified that the proposed deep\nunfolding network architecture can outperform the least squares (LS) method\nwith a relatively smaller training overhead and online computational\ncomplexity.",
          "link": "http://arxiv.org/abs/2107.12631",
          "publishedOn": "2021-07-28T02:02:33.281Z",
          "wordCount": 562,
          "title": "Learning to Estimate RIS-Aided mmWave Channels. (arXiv:2107.12631v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1\">Alexander Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huguet_G/0/1/0/all/0/1\">Guillaume Huguet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natik_A/0/1/0/all/0/1\">Amine Natik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonald_K/0/1/0/all/0/1\">Kincaid MacDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchroo_M/0/1/0/all/0/1\">Manik Kuchroo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coifman_R/0/1/0/all/0/1\">Ronald Coifman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1\">Smita Krishnaswamy</a>",
          "description": "We propose a new fast method of measuring distances between large numbers of\nrelated high dimensional datasets called the Diffusion Earth Mover's Distance\n(EMD). We model the datasets as distributions supported on common data graph\nthat is derived from the affinity matrix computed on the combined data. In such\ncases where the graph is a discretization of an underlying Riemannian closed\nmanifold, we prove that Diffusion EMD is topologically equivalent to the\nstandard EMD with a geodesic ground distance. Diffusion EMD can be computed in\n$\\tilde{O}(n)$ time and is more accurate than similarly fast algorithms such as\ntree-based EMDs. We also show Diffusion EMD is fully differentiable, making it\namenable to future uses in gradient-descent frameworks such as deep neural\nnetworks. Finally, we demonstrate an application of Diffusion EMD to single\ncell data collected from 210 COVID-19 patient samples at Yale New Haven\nHospital. Here, Diffusion EMD can derive distances between patients on the\nmanifold of cells at least two orders of magnitude faster than equally accurate\nmethods. This distance matrix between patients can be embedded into a higher\nlevel patient manifold which uncovers structure and heterogeneity in patients.\nMore generally, Diffusion EMD is applicable to all datasets that are massively\ncollected in parallel in many medical and biological systems.",
          "link": "http://arxiv.org/abs/2102.12833",
          "publishedOn": "2021-07-28T02:02:33.274Z",
          "wordCount": 727,
          "title": "Diffusion Earth Mover's Distance and Distribution Embeddings. (arXiv:2102.12833v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>",
          "description": "We propose using self-supervised discrete representations for the task of\nspeech resynthesis. To generate disentangled representation, we separately\nextract low-bitrate representations for speech content, prosodic information,\nand speaker identity. This allows to synthesize speech in a controllable\nmanner. We analyze various state-of-the-art, self-supervised representation\nlearning methods and shed light on the advantages of each method while\nconsidering reconstruction quality and disentanglement properties.\nSpecifically, we evaluate the F0 reconstruction, speaker identification\nperformance (for both resynthesis and voice conversion), recordings'\nintelligibility, and overall quality using subjective human evaluation. Lastly,\nwe demonstrate how these representations can be used for an ultra-lightweight\nspeech codec. Using the obtained representations, we can get to a rate of 365\nbits per second while providing better speech quality than the baseline\nmethods. Audio samples can be found under the following link:\nspeechbot.github.io/resynthesis.",
          "link": "http://arxiv.org/abs/2104.00355",
          "publishedOn": "2021-07-28T02:02:33.266Z",
          "wordCount": 627,
          "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations. (arXiv:2104.00355v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhiyong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yixuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huihua Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hsiao-Dong Chiang</a>",
          "description": "Recent progress on deep learning relies heavily on the quality and efficiency\nof training algorithms. In this paper, we develop a fast training method\nmotivated by the nonlinear Conjugate Gradient (CG) framework. We propose the\nConjugate Gradient with Quadratic line-search (CGQ) method. On the one hand, a\nquadratic line-search determines the step size according to current loss\nlandscape. On the other hand, the momentum factor is dynamically updated in\ncomputing the conjugate gradient parameter (like Polak-Ribiere). Theoretical\nresults to ensure the convergence of our method in strong convex settings is\ndeveloped. And experiments in image classification datasets show that our\nmethod yields faster convergence than other local solvers and has better\ngeneralization capability (test set accuracy). One major advantage of the paper\nmethod is that tedious hand tuning of hyperparameters like the learning rate\nand momentum is avoided.",
          "link": "http://arxiv.org/abs/2106.11548",
          "publishedOn": "2021-07-28T02:02:33.247Z",
          "wordCount": 603,
          "title": "Adaptive Learning Rate and Momentum for Training Deep Neural Networks. (arXiv:2106.11548v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_B/0/1/0/all/0/1\">Ben Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully\ncausal approach to neural audio synthesis which operates directly in the\nwaveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU\ninference. The NEWT uses time-distributed multilayer perceptrons with periodic\nactivations to implicitly learn nonlinear transfer functions that encode the\ncharacteristics of a target timbre. Once trained, a NEWT can produce complex\ntimbral evolutions by simple affine transformations of its input and output\nsignals. We paired the NEWT with a differentiable noise synthesiser and reverb\nand found it capable of generating realistic musical instrument performances\nwith only 260k total model parameters, conditioned on F0 and loudness features.\nWe compared our method to state-of-the-art benchmarks with a multi-stimulus\nlistening test and the Fr\\'echet Audio Distance and found it performed\ncompetitively across the tested timbral domains. Our method significantly\noutperformed the benchmarks in terms of generation speed, and achieved\nreal-time performance on a consumer CPU, both with and without FastNEWT,\nsuggesting it is a viable basis for future creative sound design tools.",
          "link": "http://arxiv.org/abs/2107.05050",
          "publishedOn": "2021-07-28T02:02:33.240Z",
          "wordCount": 631,
          "title": "Neural Waveshaping Synthesis. (arXiv:2107.05050v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hunter Lang</a>",
          "description": "Deep learning has proven effective for various application tasks, but its\napplicability is limited by the reliance on annotated examples. Self-supervised\nlearning has emerged as a promising direction to alleviate the supervision\nbottleneck, but existing work focuses on leveraging co-occurrences in unlabeled\ndata for task-agnostic representation learning, as exemplified by masked\nlanguage model pretraining. In this chapter, we explore task-specific\nself-supervision, which leverages domain knowledge to automatically annotate\nnoisy training examples for end applications, either by introducing labeling\nfunctions for annotating individual instances, or by imposing constraints over\ninterdependent label decisions. We first present deep probabilistic logic(DPL),\nwhich offers a unifying framework for task-specific self-supervision by\ncomposing probabilistic logic with deep learning. DPL represents unknown labels\nas latent variables and incorporates diverse self-supervision using\nprobabilistic logic to train a deep neural network end-to-end using variational\nEM. Next, we present self-supervised self-supervision(S4), which adds to DPL\nthe capability to learn new self-supervision automatically. Starting from an\ninitial seed self-supervision, S4 iteratively uses the deep neural network to\npropose new self supervision. These are either added directly (a form of\nstructured self-training) or verified by a human expert (as in feature-based\nactive learning). Experiments on real-world applications such as biomedical\nmachine reading and various text classification tasks show that task-specific\nself-supervision can effectively leverage domain expertise and often match the\naccuracy of supervised methods with a tiny fraction of human effort.",
          "link": "http://arxiv.org/abs/2107.12591",
          "publishedOn": "2021-07-28T02:02:33.232Z",
          "wordCount": 675,
          "title": "Combining Probabilistic Logic and Deep Learning for Self-Supervised Learning. (arXiv:2107.12591v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grassucci_E/0/1/0/all/0/1\">Eleonora Grassucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicero_E/0/1/0/all/0/1\">Edoardo Cicero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1\">Danilo Comminiello</a>",
          "description": "Latest Generative Adversarial Networks (GANs) are gathering outstanding\nresults through a large-scale training, thus employing models composed of\nmillions of parameters requiring extensive computational capabilities. Building\nsuch huge models undermines their replicability and increases the training\ninstability. Moreover, multi-channel data, such as images or audio, are usually\nprocessed by realvalued convolutional networks that flatten and concatenate the\ninput, often losing intra-channel spatial relations. To address these issues\nrelated to complexity and information loss, we propose a family of\nquaternion-valued generative adversarial networks (QGANs). QGANs exploit the\nproperties of quaternion algebra, e.g., the Hamilton product, that allows to\nprocess channels as a single entity and capture internal latent relations,\nwhile reducing by a factor of 4 the overall number of parameters. We show how\nto design QGANs and to extend the proposed approach even to advanced models.We\ncompare the proposed QGANs with real-valued counterparts on several image\ngeneration benchmarks. Results show that QGANs are able to obtain better FID\nscores than real-valued GANs and to generate visually pleasing images.\nFurthermore, QGANs save up to 75% of the training parameters. We believe these\nresults may pave the way to novel, more accessible, GANs capable of improving\nperformance and saving computational resources.",
          "link": "http://arxiv.org/abs/2104.09630",
          "publishedOn": "2021-07-28T02:02:33.225Z",
          "wordCount": 678,
          "title": "Quaternion Generative Adversarial Networks. (arXiv:2104.09630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sorochan_K/0/1/0/all/0/1\">Kynan Sorochan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yakun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Machine learning has been a popular tool in many different fields, including\nprocedural content generation. However, procedural content generation via\nmachine learning (PCGML) approaches can struggle with controllability and\ncoherence. In this paper, we attempt to address these problems by learning to\ngenerate human-like paths, and then generating levels based on these paths. We\nextract player path data from gameplay video, train an LSTM to generate new\npaths based on this data, and then generate game levels based on this path\ndata. We demonstrate that our approach leads to more coherent levels for the\ngame Lode Runner in comparison to an existing PCGML approach.",
          "link": "http://arxiv.org/abs/2107.12532",
          "publishedOn": "2021-07-28T02:02:33.218Z",
          "wordCount": 562,
          "title": "Generating Lode Runner Levels by Learning Player Paths with LSTMs. (arXiv:2107.12532v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadasivan_V/0/1/0/all/0/1\">Vinu Sankar Sadasivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1\">Anirban Dasgupta</a>",
          "description": "Curriculum learning is a training strategy that sorts the training examples\nby some measure of their difficulty and gradually exposes them to the learner\nto improve the network performance. Motivated by our insights from implicit\ncurriculum ordering, we first introduce a simple curriculum learning strategy\nthat uses statistical measures such as standard deviation and entropy values to\nscore the difficulty of data points for real image classification tasks. We\nempirically show its improvements in performance with convolutional and\nfully-connected neural networks on multiple real image datasets. We also\npropose and study the performance of a dynamic curriculum learning algorithm.\nOur dynamic curriculum algorithm tries to reduce the distance between the\nnetwork weight and an optimal weight at any training step by greedily sampling\nexamples with gradients that are directed towards the optimal weight. Further,\nwe use our algorithms to discuss why curriculum learning is helpful.",
          "link": "http://arxiv.org/abs/2103.00147",
          "publishedOn": "2021-07-28T02:02:33.211Z",
          "wordCount": 614,
          "title": "Statistical Measures For Defining Curriculum Scoring Function. (arXiv:2103.00147v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wenlong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhiling Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>",
          "description": "Generative adversarial networks (GANs) have promoted remarkable advances in\nsingle-image super-resolution (SR) by recovering photo-realistic images.\nHowever, high memory consumption of GAN-based SR (usually generators) causes\nperformance degradation and more energy consumption, hindering the deployment\nof GAN-based SR into resource-constricted mobile devices. In this paper, we\npropose a novel compression framework \\textbf{M}ulti-scale \\textbf{F}eature\n\\textbf{A}ggregation Net based \\textbf{GAN} (MFAGAN) for reducing the memory\naccess cost of the generator. First, to overcome the memory explosion of dense\nconnections, we utilize a memory-efficient multi-scale feature aggregation net\nas the generator. Second, for faster and more stable training, our method\nintroduces the PatchGAN discriminator. Third, to balance the student\ndiscriminator and the compressed generator, we distill both the generator and\nthe discriminator. Finally, we perform a hardware-aware neural architecture\nsearch (NAS) to find a specialized SubGenerator for the target mobile phone.\nBenefiting from these improvements, the proposed MFAGAN achieves up to\n\\textbf{8.3}$\\times$ memory saving and \\textbf{42.9}$\\times$ computation\nreduction, with only minor visual quality degradation, compared with ESRGAN.\nEmpirical studies also show $\\sim$\\textbf{70} milliseconds latency on Qualcomm\nSnapdragon 865 chipset.",
          "link": "http://arxiv.org/abs/2107.12679",
          "publishedOn": "2021-07-28T02:02:33.193Z",
          "wordCount": 610,
          "title": "MFAGAN: A Compression Framework for Memory-Efficient On-Device Super-Resolution GAN. (arXiv:2107.12679v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_B/0/1/0/all/0/1\">Byungjin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yu Xiao</a>",
          "description": "Vehicular fog computing (VFC) pushes the cloud computing capability to the\ndistributed fog nodes at the edge of the Internet, enabling compute-intensive\nand latency-sensitive computing services for vehicles through task offloading.\nHowever, a heterogeneous mobility environment introduces uncertainties in terms\nof resource supply and demand, which are inevitable bottlenecks for the optimal\noffloading decision. Also, these uncertainties bring extra challenges to task\noffloading under the oblivious adversary attack and data privacy risks. In this\narticle, we develop a new adversarial online learning algorithm with bandit\nfeedback based on the adversarial multi-armed bandit theory, to enable scalable\nand low-complexity offloading decision making. Specifically, we focus on\noptimizing fog node selection with the aim of minimizing the offloading service\ncosts in terms of delay and energy. The key is to implicitly tune the\nexploration bonus in the selection process and the assessment rules of the\ndesigned algorithm, taking into account volatile resource supply and demand. We\ntheoretically prove that the input-size dependent selection rule allows to\nchoose a suitable fog node without exploring the sub-optimal actions, and also\nan appropriate score patching rule allows to quickly adapt to evolving\ncircumstances, which reduce variance and bias simultaneously, thereby achieving\na better exploitation-exploration balance. Simulation results verify the\neffectiveness and robustness of the proposed algorithm.",
          "link": "http://arxiv.org/abs/2104.12827",
          "publishedOn": "2021-07-28T02:02:33.186Z",
          "wordCount": 700,
          "title": "Learning-based decentralized offloading decision making in an adversarial environment. (arXiv:2104.12827v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benaroya_L/0/1/0/all/0/1\">Laurent Benaroya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obin_N/0/1/0/all/0/1\">Nicolas Obin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roebel_A/0/1/0/all/0/1\">Axel Roebel</a>",
          "description": "Voice conversion (VC) consists of digitally altering the voice of an\nindividual to manipulate part of its content, primarily its identity, while\nmaintaining the rest unchanged. Research in neural VC has accomplished\nconsiderable breakthroughs with the capacity to falsify a voice identity using\na small amount of data with a highly realistic rendering. This paper goes\nbeyond voice identity and presents a neural architecture that allows the\nmanipulation of voice attributes (e.g., gender and age). Leveraging the latest\nadvances on adversarial learning of structured speech representation, a novel\nstructured neural network is proposed in which multiple auto-encoders are used\nto encode speech as a set of idealistically independent linguistic and\nextra-linguistic representations, which are learned adversariarly and can be\nmanipulated during VC. Moreover, the proposed architecture is time-synchronized\nso that the original voice timing is preserved during conversion which allows\nlip-sync applications. Applied to voice gender conversion on the real-world\nVCTK dataset, our proposed architecture can learn successfully\ngender-independent representation and convert the voice gender with a very high\nefficiency and naturalness.",
          "link": "http://arxiv.org/abs/2107.12346",
          "publishedOn": "2021-07-28T02:02:33.178Z",
          "wordCount": 637,
          "title": "Beyond Voice Identity Conversion: Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations. (arXiv:2107.12346v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Team_Open_Ended_Learning/0/1/0/all/0/1\">Open-Ended Learning Team</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stooke_A/0/1/0/all/0/1\">Adam Stooke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1\">Anuj Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_C/0/1/0/all/0/1\">Catarina Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deck_C/0/1/0/all/0/1\">Charlie Deck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_J/0/1/0/all/0/1\">Jakob Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1\">Jakub Sygnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trebacz_M/0/1/0/all/0/1\">Maja Trebacz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaderberg_M/0/1/0/all/0/1\">Max Jaderberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_M/0/1/0/all/0/1\">Michael Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1\">Nat McAleese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_Schmieg_N/0/1/0/all/0/1\">Nathalie Bradley-Schmieg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Nathaniel Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porcel_N/0/1/0/all/0/1\">Nicolas Porcel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_Fitt_S/0/1/0/all/0/1\">Steph Hughes-Fitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalibard_V/0/1/0/all/0/1\">Valentin Dalibard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_W/0/1/0/all/0/1\">Wojciech Marian Czarnecki</a>",
          "description": "In this work we create agents that can perform well beyond a single,\nindividual task, that exhibit much wider generalisation of behaviour to a\nmassive, rich space of challenges. We define a universe of tasks within an\nenvironment domain and demonstrate the ability to train agents that are\ngenerally capable across this vast space and beyond. The environment is\nnatively multi-agent, spanning the continuum of competitive, cooperative, and\nindependent games, which are situated within procedurally generated physical 3D\nworlds. The resulting space is exceptionally diverse in terms of the challenges\nposed to agents, and as such, even measuring the learning progress of an agent\nis an open research problem. We propose an iterative notion of improvement\nbetween successive generations of agents, rather than seeking to maximise a\nsingular objective, allowing us to quantify progress despite tasks being\nincomparable in terms of achievable rewards. We show that through constructing\nan open-ended learning process, which dynamically changes the training task\ndistributions and training objectives such that the agent never stops learning,\nwe achieve consistent learning of new behaviours. The resulting agent is able\nto score reward in every one of our humanly solvable evaluation levels, with\nbehaviour generalising to many held-out points in the universe of tasks.\nExamples of this zero-shot generalisation include good performance on Hide and\nSeek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks\nwe characterise the behaviour of our agent, and find interesting emergent\nheuristic behaviours such as trial-and-error experimentation, simple tool use,\noption switching, and cooperation. Finally, we demonstrate that the general\ncapabilities of this agent could unlock larger scale transfer of behaviour\nthrough cheap finetuning.",
          "link": "http://arxiv.org/abs/2107.12808",
          "publishedOn": "2021-07-28T02:02:33.166Z",
          "wordCount": 738,
          "title": "Open-Ended Learning Leads to Generally Capable Agents. (arXiv:2107.12808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1\">Denis Gudovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishizaka_S/0/1/0/all/0/1\">Shun Ishizaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1\">Kazuki Kozuka</a>",
          "description": "Unsupervised anomaly detection with localization has many practical\napplications when labeling is infeasible and, moreover, when anomaly examples\nare completely missing in the train data. While recently proposed models for\nsuch data setup achieve high accuracy metrics, their complexity is a limiting\nfactor for real-time processing. In this paper, we propose a real-time model\nand analytically derive its relationship to prior methods. Our CFLOW-AD model\nis based on a conditional normalizing flow framework adopted for anomaly\ndetection with localization. In particular, CFLOW-AD consists of a\ndiscriminatively pretrained encoder followed by a multi-scale generative\ndecoders where the latter explicitly estimate likelihood of the encoded\nfeatures. Our approach results in a computationally and memory-efficient model:\nCFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art\nwith the same input setting. Our experiments on the MVTec dataset show that\nCFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by\n1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source\nour code with fully reproducible experiments.",
          "link": "http://arxiv.org/abs/2107.12571",
          "publishedOn": "2021-07-28T02:02:33.147Z",
          "wordCount": 620,
          "title": "CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows. (arXiv:2107.12571v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zisen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Co-creative Procedural Content Generation via Machine Learning (PCGML) refers\nto systems where a PCGML agent and a human work together to produce output\ncontent. One of the limitations of co-creative PCGML is that it requires\nco-creative training data for a PCGML agent to learn to interact with humans.\nHowever, acquiring this data is a difficult and time-consuming process. In this\nwork, we propose approximating human-AI interaction data and employing transfer\nlearning to adapt learned co-creative knowledge from one game to a different\ngame. We explore this approach for co-creative Zelda dungeon room generation.",
          "link": "http://arxiv.org/abs/2107.12533",
          "publishedOn": "2021-07-28T02:02:33.138Z",
          "wordCount": 545,
          "title": "Toward Co-creative Dungeon Generation via Transfer Learning. (arXiv:2107.12533v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianxin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bing Yao</a>",
          "description": "The rapid developments in advanced sensing and imaging bring about a\ndata-rich environment, facilitating the effective modeling, monitoring, and\ncontrol of complex systems. For example, the body-sensor network captures\nmulti-channel information pertinent to the electrical activity of the heart\n(i.e., electrocardiograms (ECG)), which enables medical scientists to monitor\nand detect abnormal cardiac conditions. However, the high-dimensional sensing\ndata are generally complexly structured and realizing the full data potential\ndepends to a great extent on advanced analytical and predictive methods. This\npaper presents a physics-constrained deep learning (P-DL) framework for\nhigh-dimensional inverse ECG modeling. This method integrates the physical laws\nof the complex system with the advanced deep learning infrastructure for\neffective prediction of the system dynamics. The proposed P-DL approach is\nimplemented to solve the inverse ECG model and predict the time-varying\ndistribution of electric potentials in the heart from the ECG data measured by\nthe body-surface sensor network. Experimental results show that the proposed\nP-DL method significantly outperforms existing methods that are commonly used\nin current practice.",
          "link": "http://arxiv.org/abs/2107.12780",
          "publishedOn": "2021-07-28T02:02:33.131Z",
          "wordCount": 594,
          "title": "Physics-constrained Deep Learning for Robust Inverse ECG Modeling. (arXiv:2107.12780v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cromp_S/0/1/0/all/0/1\">Sonia Cromp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samadian_A/0/1/0/all/0/1\">Alireza Samadian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruhs_K/0/1/0/all/0/1\">Kirk Pruhs</a>",
          "description": "Many tasks use data housed in relational databases to train boosted\nregression tree models. In this paper, we give a relational adaptation of the\ngreedy algorithm for training boosted regression trees. For the subproblem of\ncalculating the sum of squared residuals of the dataset, which dominates the\nruntime of the boosting algorithm, we provide a $(1 + \\epsilon)$-approximation\nusing the tensor sketch technique. Employing this approximation within the\nrelational boosted regression trees algorithm leads to learning similar model\nparameters, but with asymptotically better runtime.",
          "link": "http://arxiv.org/abs/2107.12373",
          "publishedOn": "2021-07-28T02:02:33.124Z",
          "wordCount": 513,
          "title": "Relational Boosted Regression Trees. (arXiv:2107.12373v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-07-28T02:02:33.117Z",
          "wordCount": 641,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halina_E/0/1/0/all/0/1\">Emily Halina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Generating rhythm game charts from songs via machine learning has been a\nproblem of increasing interest in recent years. However, all existing systems\nstruggle to replicate human-like patterning: the placement of game objects in\nrelation to each other to form congruent patterns based on events in the song.\nPatterning is a key identifier of high quality rhythm game content, seen as a\nnecessary component in human rankings. We establish a new approach for chart\ngeneration that produces charts with more congruent, human-like patterning than\nseen in prior work.",
          "link": "http://arxiv.org/abs/2107.12506",
          "publishedOn": "2021-07-28T02:02:33.105Z",
          "wordCount": 547,
          "title": "TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games. (arXiv:2107.12506v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aouali_I/0/1/0/all/0/1\">Imad Aouali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_S/0/1/0/all/0/1\">Sergey Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gartrell_M/0/1/0/all/0/1\">Mike Gartrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_D/0/1/0/all/0/1\">David Rohde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1\">Flavian Vasile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaytsev_V/0/1/0/all/0/1\">Victor Zaytsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legrand_D/0/1/0/all/0/1\">Diego Legrand</a>",
          "description": "We consider the problem of slate recommendation, where the recommender system\npresents a user with a collection or slate composed of K recommended items at\nonce. If the user finds the recommended items appealing then the user may click\nand the recommender system receives some feedback. Two pieces of information\nare available to the recommender system: was the slate clicked? (the reward),\nand if the slate was clicked, which item was clicked? (rank). In this paper, we\nformulate several Bayesian models that incorporate the reward signal (Reward\nmodel), the rank signal (Rank model), or both (Full model), for\nnon-personalized slate recommendation. In our experiments, we analyze\nperformance gains of the Full model and show that it achieves significantly\nlower error as the number of products in the catalog grows or as the slate size\nincreases.",
          "link": "http://arxiv.org/abs/2107.12455",
          "publishedOn": "2021-07-28T02:02:33.086Z",
          "wordCount": 603,
          "title": "Combining Reward and Rank Signals for Slate Recommendation. (arXiv:2107.12455v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peizhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1\">Gao Cong</a>",
          "description": "Cardinality estimation is a fundamental problem in database systems. To\ncapture the rich joint data distributions of a relational table, most of the\nexisting work either uses data as unsupervised information or uses query\nworkload as supervised information. Very little work has been done to use both\ntypes of information, and cannot fully make use of both types of information to\nlearn the joint data distribution. In this work, we aim to close the gap\nbetween data-driven and query-driven methods by proposing a new unified deep\nautoregressive model, UAE, that learns the joint data distribution from both\nthe data and query workload. First, to enable using the supervised query\ninformation in the deep autoregressive model, we develop differentiable\nprogressive sampling using the Gumbel-Softmax trick. Second, UAE is able to\nutilize both types of information to learn the joint data distribution in a\nsingle model. Comprehensive experimental results demonstrate that UAE achieves\nsingle-digit multiplicative error at tail, better accuracies over\nstate-of-the-art methods, and is both space and time efficient.",
          "link": "http://arxiv.org/abs/2107.12295",
          "publishedOn": "2021-07-28T02:02:33.078Z",
          "wordCount": 615,
          "title": "A Unified Deep Model of Learning from both Data and Queries for Cardinality Estimation. (arXiv:2107.12295v1 [cs.DB] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Varela_J/0/1/0/all/0/1\">Jos&#xe9; Su&#xe1;rez-Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferriol_Galmes_M/0/1/0/all/0/1\">Miquel Ferriol-Galm&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Albert L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almasan_P/0/1/0/all/0/1\">Paul Almasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardez_G/0/1/0/all/0/1\">Guillermo Bern&#xe1;rdez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujol_Perich_D/0/1/0/all/0/1\">David Pujol-Perich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusek_K/0/1/0/all/0/1\">Krzysztof Rusek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonniot_L/0/1/0/all/0/1\">Lo&#xef;ck Bonniot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_C/0/1/0/all/0/1\">Christoph Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnitzler_F/0/1/0/all/0/1\">Fran&#xe7;ois Schnitzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taiani_F/0/1/0/all/0/1\">Fran&#xe7;ois Ta&#xef;ani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Happ_M/0/1/0/all/0/1\">Martin Happ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_C/0/1/0/all/0/1\">Christian Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jia Lei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herlich_M/0/1/0/all/0/1\">Matthias Herlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorfinger_P/0/1/0/all/0/1\">Peter Dorfinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hainke_N/0/1/0/all/0/1\">Nick Vincent Hainke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venz_S/0/1/0/all/0/1\">Stefan Venz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegener_J/0/1/0/all/0/1\">Johannes Wegener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wissing_H/0/1/0/all/0/1\">Henrike Wissing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shihan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlet_Ros_P/0/1/0/all/0/1\">Pere Barlet-Ros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabellos_Aparicio_A/0/1/0/all/0/1\">Albert Cabellos-Aparicio</a>",
          "description": "During the last decade, Machine Learning (ML) has increasingly become a hot\ntopic in the field of Computer Networks and is expected to be gradually adopted\nfor a plethora of control, monitoring and management tasks in real-world\ndeployments. This poses the need to count on new generations of students,\nresearchers and practitioners with a solid background in ML applied to\nnetworks. During 2020, the International Telecommunication Union (ITU) has\norganized the \"ITU AI/ML in 5G challenge'', an open global competition that has\nintroduced to a broad audience some of the current main challenges in ML for\nnetworks. This large-scale initiative has gathered 23 different challenges\nproposed by network operators, equipment manufacturers and academia, and has\nattracted a total of 1300+ participants from 60+ countries. This paper narrates\nour experience organizing one of the proposed challenges: the \"Graph Neural\nNetworking Challenge 2020''. We describe the problem presented to participants,\nthe tools and resources provided, some organization aspects and participation\nstatistics, an outline of the top-3 awarded solutions, and a summary with some\nlessons learned during all this journey. As a result, this challenge leaves a\ncurated set of educational resources openly available to anyone interested in\nthe topic.",
          "link": "http://arxiv.org/abs/2107.12433",
          "publishedOn": "2021-07-28T02:02:33.071Z",
          "wordCount": 712,
          "title": "The Graph Neural Networking Challenge: A Worldwide Competition for Education in AI/ML for Networks. (arXiv:2107.12433v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponomarev_E/0/1/0/all/0/1\">Evgeny Ponomarev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matveev_S/0/1/0/all/0/1\">Sergey Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan Oseledets</a>",
          "description": "A lot of deep learning applications are desired to be run on mobile devices.\nBoth accuracy and inference time are meaningful for a lot of them. While the\nnumber of FLOPs is usually used as a proxy for neural network latency, it may\nbe not the best choice. In order to obtain a better approximation of latency,\nresearch community uses look-up tables of all possible layers for latency\ncalculation for the final prediction of the inference on mobile CPU. It\nrequires only a small number of experiments. Unfortunately, on mobile GPU this\nmethod is not applicable in a straight-forward way and shows low precision. In\nthis work, we consider latency approximation on mobile GPU as a data and\nhardware-specific problem. Our main goal is to construct a convenient latency\nestimation tool for investigation(LETI) of neural network inference and\nbuilding robust and accurate latency prediction models for each specific task.\nTo achieve this goal, we build open-source tools which provide a convenient way\nto conduct massive experiments on different target devices focusing on mobile\nGPU. After evaluation of the dataset, we learn the regression model on\nexperimental data and use it for future latency prediction and analysis. We\nexperimentally demonstrate the applicability of such an approach on a subset of\npopular NAS-Benchmark 101 dataset and also evaluate the most popular neural\nnetwork architectures for two mobile GPUs. As a result, we construct latency\nprediction model with good precision on the target evaluation subset. We\nconsider LETI as a useful tool for neural architecture search or massive\nlatency evaluation. The project is available at https://github.com/leti-ai",
          "link": "http://arxiv.org/abs/2010.02871",
          "publishedOn": "2021-07-28T02:02:33.062Z",
          "wordCount": 758,
          "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU. (arXiv:2010.02871v2 [cs.PF] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laousy_O/0/1/0/all/0/1\">Othmane Laousy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with limited\namount of data and annotations.",
          "link": "http://arxiv.org/abs/2107.12800",
          "publishedOn": "2021-07-28T02:02:33.031Z",
          "wordCount": 577,
          "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment. (arXiv:2107.12800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12040",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenxi Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chang_Y/0/1/0/all/0/1\">Yi-Wei Chang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that\nvisualizes the structural and spatial organization of macromolecules at a\nnear-native state in single cells, which has broad applications in life\nscience. However, the systematic structural recognition and recovery of\nmacromolecules captured by cryo-ET are difficult due to high structural\ncomplexity and imaging limits. Deep learning based subtomogram classification\nhave played critical roles for such tasks. As supervised approaches, however,\ntheir performance relies on sufficient and laborious annotation on a large\ntraining dataset.\n\nResults: To alleviate this major labeling burden, we proposed a Hybrid Active\nLearning (HAL) framework for querying subtomograms for labelling from a large\nunlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select\nthe subtomograms that have the most uncertain predictions. Moreover, to\nmitigate the sampling bias caused by such strategy, a discriminator is\nintroduced to judge if a certain subtomogram is labeled or unlabeled and\nsubsequently the model queries the subtomogram that have higher probabilities\nto be unlabeled. Additionally, HAL introduces a subset sampling strategy to\nimprove the diversity of the query set, so that the information overlap is\ndecreased between the queried batches and the algorithmic efficiency is\nimproved. Our experiments on subtomogram classification tasks using both\nsimulated and real data demonstrate that we can achieve comparable testing\nperformance (on average only 3% accuracy drop) by using less than 30% of the\nlabeled subtomograms, which shows a very promising result for subtomogram\nclassification task with limited labeling resources.",
          "link": "http://arxiv.org/abs/2102.12040",
          "publishedOn": "2021-07-28T02:02:33.023Z",
          "wordCount": 783,
          "title": "Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography. (arXiv:2102.12040v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>",
          "description": "Federated learning (FL) is an emerging practical framework for effective and\nscalable machine learning among multiple participants, such as end users,\norganizations and companies. However, most existing FL or distributed learning\nframeworks have not well addressed two important issues together: collaborative\nfairness and adversarial robustness (e.g. free-riders and malicious\nparticipants). In conventional FL, all participants receive the global model\n(equal rewards), which might be unfair to the high-contributing participants.\nFurthermore, due to the lack of a safeguard mechanism, free-riders or malicious\nadversaries could game the system to access the global model for free or to\nsabotage it. In this paper, we propose a novel Robust and Fair Federated\nLearning (RFFL) framework to achieve collaborative fairness and adversarial\nrobustness simultaneously via a reputation mechanism. RFFL maintains a\nreputation for each participant by examining their contributions via their\nuploaded gradients (using vector similarity) and thus identifies\nnon-contributing or malicious participants to be removed. Our approach\ndifferentiates itself by not requiring any auxiliary/validation dataset.\nExtensive experiments on benchmark datasets show that RFFL can achieve high\nfairness and is very robust to different types of adversaries while achieving\ncompetitive predictive accuracy.",
          "link": "http://arxiv.org/abs/2011.10464",
          "publishedOn": "2021-07-28T02:02:33.007Z",
          "wordCount": 683,
          "title": "A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning. (arXiv:2011.10464v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azari_B/0/1/0/all/0/1\">Bahar Azari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>",
          "description": "Despite the vast success of standard planar convolutional neural networks,\nthey are not the most efficient choice for analyzing signals that lie on an\narbitrarily curved manifold, such as a cylinder. The problem arises when one\nperforms a planar projection of these signals and inevitably causes them to be\ndistorted or broken where there is valuable information. We propose a\nCircular-symmetric Correlation Layer (CCL) based on the formalism of\nroto-translation equivariant correlation on the continuous group $S^1 \\times\n\\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier\nTransform (FFT) algorithm. We showcase the performance analysis of a general\nnetwork equipped with CCL on various recognition and classification tasks and\ndatasets. The PyTorch package implementation of CCL is provided online.",
          "link": "http://arxiv.org/abs/2107.12480",
          "publishedOn": "2021-07-28T02:02:33.000Z",
          "wordCount": 552,
          "title": "Circular-Symmetric Correlation Layer based on FFT. (arXiv:2107.12480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12525",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kang_D/0/1/0/all/0/1\">Daniel Kang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Guibas_J/0/1/0/all/0/1\">John Guibas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bailis_P/0/1/0/all/0/1\">Peter Bailis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Given a dataset $\\mathcal{D}$, we are interested in computing the mean of a\nsubset of $\\mathcal{D}$ which matches a predicate. \\algname leverages\nstratified sampling and proxy models to efficiently compute this statistic\ngiven a sampling budget $N$. In this document, we theoretically analyze\n\\algname and show that the MSE of the estimate decays at rate $O(N_1^{-1} +\nN_2^{-1} + N_1^{1/2}N_2^{-3/2})$, where $N=K \\cdot N_1+N_2$ for some integer\nconstant $K$ and $K \\cdot N_1$ and $N_2$ represent the number of samples used\nin Stage 1 and Stage 2 of \\algname respectively. Hence, if a constant fraction\nof the total sample budget $N$ is allocated to each stage, we will achieve a\nmean squared error of $O(N^{-1})$ which matches the rate of mean squared error\nof the optimal stratified sampling algorithm given a priori knowledge of the\npredicate positive rate and standard deviation per stratum.",
          "link": "http://arxiv.org/abs/2107.12525",
          "publishedOn": "2021-07-28T02:02:32.982Z",
          "wordCount": 591,
          "title": "Proof: Accelerating Approximate Aggregation Queries with Expensive Predicates. (arXiv:2107.12525v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10205",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1\">Yongqian Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Q/0/1/0/all/0/1\">QianLi Lin</a>",
          "description": "With the development of end-to-end control based on deep learning, it is\nimportant to study new system modeling techniques to realize dynamics modeling\nwith high-dimensional inputs. In this paper, a novel Koopman-based deep\nconvolutional network, called CKNet, is proposed to identify latent dynamics\nfrom raw pixels. CKNet learns an encoder and decoder to play the role of the\nKoopman eigenfunctions and modes, respectively. The Koopman eigenvalues can be\napproximated by eigenvalues of the learned state transition matrix. The\ndeterministic convolutional Koopman network (DCKNet) and the variational\nconvolutional Koopman network (VCKNet) are proposed to span some subspace for\napproximating the Koopman operator respectively. Because CKNet is trained under\nthe constraints of the Koopman theory, the identified latent dynamics is in a\nlinear form and has good interpretability. Besides, the state transition and\ncontrol matrices are trained as trainable tensors so that the identified\ndynamics is also time-invariant. We also design an auxiliary weight term for\nreducing multi-step linearity and prediction losses. Experiments were conducted\non two offline trained and four online trained nonlinear forced dynamical\nsystems with continuous action spaces in Gym and Mujoco environment\nrespectively, and the results show that identified dynamics are adequate for\napproximating the latent dynamics and generating clear images. Especially for\noffline trained cases, this work confirms CKNet from a novel perspective that\nwe visualize the evolutionary processes of the latent states and the Koopman\neigenfunctions with DCKNet and VCKNet separately to each task based on the same\nepisode and results demonstrate that different approaches learn similar\nfeatures in shapes.",
          "link": "http://arxiv.org/abs/2102.10205",
          "publishedOn": "2021-07-28T02:02:32.974Z",
          "wordCount": 729,
          "title": "CKNet: A Convolutional Neural Network Based on Koopman Operator for Modeling Latent Dynamics from Pixels. (arXiv:2102.10205v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabo_A/0/1/0/all/0/1\">Andrea Sabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdizadeh_S/0/1/0/all/0/1\">Sina Mehdizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iaboni_A/0/1/0/all/0/1\">Andrea Iaboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1\">Babak Taati</a>",
          "description": "Drug-induced parkinsonism affects many older adults with dementia, often\ncausing gait disturbances. New advances in vision-based human pose-estimation\nhave opened possibilities for frequent and unobtrusive analysis of gait in\nresidential settings. This work proposes novel spatial-temporal graph\nconvolutional network (ST-GCN) architectures and training procedures to predict\nclinical scores of parkinsonism in gait from video of individuals with\ndementia. We propose a two-stage training approach consisting of a\nself-supervised pretraining stage that encourages the ST-GCN model to learn\nabout gait patterns before predicting clinical scores in the finetuning stage.\nThe proposed ST-GCN models are evaluated on joint trajectories extracted from\nvideo and are compared against traditional (ordinal, linear, random forest)\nregression models and temporal convolutional network baselines. Three 2D human\npose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft\nKinect (2D and 3D) are used to extract joint trajectories of 4787 natural\nwalking bouts from 53 older adults with dementia. A subset of 399 walks from 14\nparticipants is annotated with scores of parkinsonism severity on the gait\ncriteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the\nSimpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating\non 3D joint trajectories extracted from the Kinect consistently outperform all\nother models and feature sets. Prediction of parkinsonism scores in natural\nwalking bouts of unseen participants remains a challenging task, with the best\nmodels achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02\nfor UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for\nthis work is available:\nhttps://github.com/TaatiTeam/stgcn_parkinsonism_prediction.",
          "link": "http://arxiv.org/abs/2105.03464",
          "publishedOn": "2021-07-28T02:02:32.967Z",
          "wordCount": 723,
          "title": "Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia. (arXiv:2105.03464v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>",
          "description": "This paper presents the evaluation of effects of image size on deep learning\nperformance via semantic segmentation of magnetic resonance heart images with\nU-net for fully automated quantification of myocardial infarction. Both\nnon-extra pixel and extra pixel interpolation algorithms are used to change the\nsize of images in datasets of interest. Extra class labels, in interpolated\nground truth segmentation images, are removed using thresholding, median\nfiltering, and subtraction strategies. Common class metrics are used to\nevaluate the quality of semantic segmentation with U-net against the ground\ntruth segmentation while arbitrary threshold, comparison of the sums, and sums\nof differences between medical experts and fully automated results are options\nused to estimate the relationship between medical experts-based quantification\nand fully automated quantification results.",
          "link": "http://arxiv.org/abs/2101.11508",
          "publishedOn": "2021-07-28T02:02:32.957Z",
          "wordCount": 592,
          "title": "Effects of Image Size on Deep Learning. (arXiv:2101.11508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12438",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Gupta_V/0/1/0/all/0/1\">Vishal Gupta</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_M/0/1/0/all/0/1\">Michael Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rusmevichientong_P/0/1/0/all/0/1\">Paat Rusmevichientong</a>",
          "description": "Motivated by the poor performance of cross-validation in settings where data\nare scarce, we propose a novel estimator of the out-of-sample performance of a\npolicy in data-driven optimization.Our approach exploits the optimization\nproblem's sensitivity analysis to estimate the gradient of the optimal\nobjective value with respect to the amount of noise in the data and uses the\nestimated gradient to debias the policy's in-sample performance. Unlike\ncross-validation techniques, our approach avoids sacrificing data for a test\nset, utilizes all data when training and, hence, is well-suited to settings\nwhere data are scarce. We prove bounds on the bias and variance of our\nestimator for optimization problems with uncertain linear objectives but known,\npotentially non-convex, feasible regions. For more specialized optimization\nproblems where the feasible region is ``weakly-coupled\" in a certain sense, we\nprove stronger results. Specifically, we provide explicit high-probability\nbounds on the error of our estimator that hold uniformly over a policy class\nand depends on the problem's dimension and policy class's complexity. Our\nbounds show that under mild conditions, the error of our estimator vanishes as\nthe dimension of the optimization problem grows, even if the amount of\navailable data remains small and constant. Said differently, we prove our\nestimator performs well in the small-data, large-scale regime. Finally, we\nnumerically compare our proposed method to state-of-the-art approaches through\na case-study on dispatching emergency medical response services using real\ndata. Our method provides more accurate estimates of out-of-sample performance\nand learns better-performing policies.",
          "link": "http://arxiv.org/abs/2107.12438",
          "publishedOn": "2021-07-28T02:02:32.948Z",
          "wordCount": 683,
          "title": "Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization. (arXiv:2107.12438v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_R/0/1/0/all/0/1\">Raz Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_Y/0/1/0/all/0/1\">Yuval Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_K/0/1/0/all/0/1\">Kobi Cohen</a>",
          "description": "We consider a distributed learning problem in a wireless network, consisting\nof N distributed edge devices and a parameter server (PS). The objective\nfunction is a sum of the edge devices' local loss functions, who aim to train a\nshared model by communicating with the PS over multiple access channels (MAC).\nThis problem has attracted a growing interest in distributed sensing systems,\nand more recently in federated learning, known as over-the-air computation. In\nthis paper, we develop a novel Accelerated Gradient-descent Multiple Access\n(AGMA) algorithm that uses momentum-based gradient signals over noisy fading\nMAC to improve the convergence rate as compared to existing methods.\nFurthermore, AGMA does not require power control or beamforming to cancel the\nfading effect, which simplifies the implementation complexity. We analyze AGMA\ntheoretically, and establish a finite-sample bound of the error for both convex\nand strongly convex loss functions with Lipschitz gradient. For the strongly\nconvex case, we show that AGMA approaches the best-known linear convergence\nrate as the network increases. For the convex case, we show that AGMA\nsignificantly improves the sub-linear convergence rate as compared to existing\nmethods. Finally, we present simulation results using real datasets that\ndemonstrate better performance by AGMA.",
          "link": "http://arxiv.org/abs/2107.12452",
          "publishedOn": "2021-07-28T02:02:32.929Z",
          "wordCount": 638,
          "title": "Accelerated Gradient Descent Learning over Multiple Access Fading Channels. (arXiv:2107.12452v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12395",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Kahlhoefer_F/0/1/0/all/0/1\">Felix Kahlhoefer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Korsmeier_M/0/1/0/all/0/1\">Michael Korsmeier</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kramer_M/0/1/0/all/0/1\">Michael Kr&#xe4;mer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Manconi_S/0/1/0/all/0/1\">Silvia Manconi</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nippel_K/0/1/0/all/0/1\">Kathrin Nippel</a>",
          "description": "The interpretation of data from indirect detection experiments searching for\ndark matter annihilations requires computationally expensive simulations of\ncosmic-ray propagation. In this work we present a new method based on Recurrent\nNeural Networks that significantly accelerates simulations of secondary and\ndark matter Galactic cosmic ray antiprotons while achieving excellent accuracy.\nThis approach allows for an efficient profiling or marginalisation over the\nnuisance parameters of a cosmic ray propagation model in order to perform\nparameter scans for a wide range of dark matter models. We identify importance\nsampling as particularly suitable for ensuring that the network is only\nevaluated in well-trained parameter regions. We present resulting constraints\nusing the most recent AMS-02 antiproton data on several models of Weakly\nInteracting Massive Particles. The fully trained networks are released as\nDarkRayNet together with this work and achieve a speed-up of the runtime by at\nleast two orders of magnitude compared to conventional approaches.",
          "link": "http://arxiv.org/abs/2107.12395",
          "publishedOn": "2021-07-28T02:02:32.922Z",
          "wordCount": 608,
          "title": "Constraining dark matter annihilation with cosmic ray antiprotons using neural networks. (arXiv:2107.12395v1 [astro-ph.HE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12698",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Moreno_E/0/1/0/all/0/1\">Eric A. Moreno</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Vlimant_J/0/1/0/all/0/1\">Jean-Roch Vlimant</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Spiropulu_M/0/1/0/all/0/1\">Maria Spiropulu</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Borzyszkowski_B/0/1/0/all/0/1\">Bartlomiej Borzyszkowski</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>",
          "description": "We present an application of anomaly detection techniques based on deep\nrecurrent autoencoders to the problem of detecting gravitational wave signals\nin laser interferometers. Trained on noise data, this class of algorithms could\ndetect signals using an unsupervised strategy, i.e., without targeting a\nspecific kind of source. We develop a custom architecture to analyze the data\nfrom two interferometers. We compare the obtained performance to that obtained\nwith other autoencoder architectures and with a convolutional classifier. The\nunsupervised nature of the proposed strategy comes with a cost in terms of\naccuracy, when compared to more traditional supervised techniques. On the other\nhand, there is a qualitative gain in generalizing the experimental sensitivity\nbeyond the ensemble of pre-computed signal templates. The recurrent autoencoder\noutperforms other autoencoders based on different architectures. The class of\nrecurrent autoencoders presented in this paper could complement the search\nstrategy employed for gravitational wave detection and extend the reach of the\nongoing detection campaigns.",
          "link": "http://arxiv.org/abs/2107.12698",
          "publishedOn": "2021-07-28T02:02:32.904Z",
          "wordCount": 621,
          "title": "Source-Agnostic Gravitational-Wave Detection with Recurrent Autoencoders. (arXiv:2107.12698v1 [gr-qc])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12375",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Atz_K/0/1/0/all/0/1\">Kenneth Atz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Grisoni_F/0/1/0/all/0/1\">Francesca Grisoni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schneider_G/0/1/0/all/0/1\">Gisbert Schneider</a>",
          "description": "Geometric deep learning (GDL), which is based on neural network architectures\nthat incorporate and process symmetry information, has emerged as a recent\nparadigm in artificial intelligence. GDL bears particular promise in molecular\nmodeling applications, in which various molecular representations with\ndifferent symmetry properties and levels of abstraction exist. This review\nprovides a structured and harmonized overview of molecular GDL, highlighting\nits applications in drug discovery, chemical synthesis prediction, and quantum\nchemistry. Emphasis is placed on the relevance of the learned molecular\nfeatures and their complementarity to well-established molecular descriptors.\nThis review provides an overview of current challenges and opportunities, and\npresents a forecast of the future of GDL for molecular sciences.",
          "link": "http://arxiv.org/abs/2107.12375",
          "publishedOn": "2021-07-28T02:02:32.889Z",
          "wordCount": 546,
          "title": "Geometric Deep Learning on Molecular Representations. (arXiv:2107.12375v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12915",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Cho_I/0/1/0/all/0/1\">In Ho Cho</a>",
          "description": "Although researchers accumulated knowledge about seismogenesis and\ndecades-long earthquake data, predicting imminent individual earthquakes at a\nspecific time and location remains a long-standing enigma. This study\nhypothesizes that the observed data conceal the hidden rules which may be\nunraveled by a novel glass-box (as opposed to black-box) physics rule learner\n(GPRL) framework. Without any predefined earthquake-related mechanisms or\nstatistical laws, GPRL's two essentials, convolved information index and\ntransparent link function, seek generic expressions of rules directly from\ndata. GPRL's training with 10-years data appears to identify plausible rules,\nsuggesting a combination of the pseudo power and the pseudo vorticity of\nreleased energy in the lithosphere. Independent feasibility test supports the\npromising role of the unraveled rules in predicting earthquakes' magnitudes and\ntheir specific locations. The identified rules and GPRL are in their infancy\nrequiring substantial improvement. Still, this study hints at the existence of\nthe data-guided hidden pathway to imminent individual earthquake prediction.",
          "link": "http://arxiv.org/abs/2107.12915",
          "publishedOn": "2021-07-28T02:02:32.864Z",
          "wordCount": 600,
          "title": "Initial Foundation for Predicting Individual Earthquake's Location and Magnitude by Using Glass-Box Physics Rule Learner. (arXiv:2107.12915v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12723",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Richards_D/0/1/0/all/0/1\">Dominic Richards</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kuzborskij_I/0/1/0/all/0/1\">Ilja Kuzborskij</a>",
          "description": "We revisit on-average algorithmic stability of Gradient Descent (GD) for\ntraining overparameterised shallow neural networks and prove new generalisation\nand excess risk bounds without the Neural Tangent Kernel (NTK) or\nPolyak-{\\L}ojasiewicz (PL) assumptions. In particular, we show oracle type\nbounds which reveal that the generalisation and excess risk of GD is controlled\nby an interpolating network with the shortest GD path from initialisation (in a\nsense, an interpolating network with the smallest relative norm). While this\nwas known for kernelised interpolants, our proof applies directly to networks\ntrained by GD without intermediate kernelisation. At the same time, by relaxing\noracle inequalities developed here we recover existing NTK-based risk bounds in\na straightforward way, which demonstrates that our analysis is tighter.\nFinally, unlike most of the NTK-based analyses we focus on regression with\nlabel noise and show that GD with early stopping is consistent.",
          "link": "http://arxiv.org/abs/2107.12723",
          "publishedOn": "2021-07-28T02:02:32.857Z",
          "wordCount": 593,
          "title": "Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel. (arXiv:2107.12723v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bobadilla_J/0/1/0/all/0/1\">Jes&#xfa;s Bobadilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1\">Fernando Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_A/0/1/0/all/0/1\">Abraham Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>",
          "description": "Deep learning provides accurate collaborative filtering models to improve\nrecommender system results. Deep matrix factorization and their related\ncollaborative neural networks are the state-of-art in the field; nevertheless,\nboth models lack the necessary stochasticity to create the robust, continuous,\nand structured latent spaces that variational autoencoders exhibit. On the\nother hand, data augmentation through variational autoencoder does not provide\naccurate results in the collaborative filtering field due to the high sparsity\nof recommender systems. Our proposed models apply the variational concept to\ninject stochasticity in the latent space of the deep architecture, introducing\nthe variational technique in the neural collaborative filtering field. This\nmethod does not depend on the particular model used to generate the latent\nrepresentation. In this way, this approach can be applied as a plugin to any\ncurrent and future specific models. The proposed models have been tested using\nfour representative open datasets, three different quality measures, and\nstate-of-art baselines. The results show the superiority of the proposed\napproach in scenarios where the variational enrichment exceeds the injected\nnoise effect. Additionally, a framework is provided to enable the\nreproducibility of the conducted experiments.",
          "link": "http://arxiv.org/abs/2107.12677",
          "publishedOn": "2021-07-28T02:02:32.794Z",
          "wordCount": 637,
          "title": "Deep Variational Models for Collaborative Filtering-based Recommender Systems. (arXiv:2107.12677v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2001.07620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1\">Elvin Isufi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1\">Fernando Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "Driven by the outstanding performance of neural networks in the structured\nEuclidean domain, recent years have seen a surge of interest in developing\nneural networks for graphs and data supported on graphs. The graph is leveraged\nat each layer of the neural network as a parameterization to capture detail at\nthe node level with a reduced number of parameters and computational\ncomplexity. Following this rationale, this paper puts forth a general framework\nthat unifies state-of-the-art graph neural networks (GNNs) through the concept\nof EdgeNet. An EdgeNet is a GNN architecture that allows different nodes to use\ndifferent parameters to weigh the information of different neighbors. By\nextrapolating this strategy to more iterations between neighboring nodes, the\nEdgeNet learns edge- and neighbor-dependent weights to capture local detail.\nThis is a general linear and local operation that a node can perform and\nencompasses under one formulation all existing graph convolutional neural\nnetworks (GCNNs) as well as graph attention networks (GATs). In writing\ndifferent GNN architectures with a common language, EdgeNets highlight specific\narchitecture advantages and limitations, while providing guidelines to improve\ntheir capacity without compromising their local implementation. An interesting\nconclusion is the unification of GCNNs and GATs -- approaches that have been so\nfar perceived as separate. In particular, we show that GATs are GCNNs on a\ngraph that is learned from the features. This particularization opens the doors\nto develop alternative attention mechanisms for improving discriminatory power.",
          "link": "http://arxiv.org/abs/2001.07620",
          "publishedOn": "2021-07-28T02:02:32.769Z",
          "wordCount": 712,
          "title": "EdgeNets:Edge Varying Graph Neural Networks. (arXiv:2001.07620v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.",
          "link": "http://arxiv.org/abs/2107.12514",
          "publishedOn": "2021-07-28T02:02:32.743Z",
          "wordCount": 709,
          "title": "Language Grounding with 3D Objects. (arXiv:2107.12514v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stier_J/0/1/0/all/0/1\">Julian Stier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darji_H/0/1/0/all/0/1\">Harshil Darji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1\">Michael Granitzer</a>",
          "description": "Sparsity in the structure of Neural Networks can lead to less energy\nconsumption, less memory usage, faster computation times on convenient\nhardware, and automated machine learning. If sparsity gives rise to certain\nkinds of structure, it can explain automatically obtained features during\nlearning.\n\nWe provide insights into experiments in which we show how sparsity can be\nachieved through prior initialization, pruning, and during learning, and answer\nquestions on the relationship between the structure of Neural Networks and\ntheir performance. This includes the first work of inducing priors from network\ntheory into Recurrent Neural Networks and an architectural performance\nprediction during a Neural Architecture Search. Within our experiments, we show\nhow magnitude class blinded pruning achieves 97.5% on MNIST with 80%\ncompression and re-training, which is 0.5 points more than without compression,\nthat magnitude class uniform pruning is significantly inferior to it and how a\ngenetic search enhanced with performance prediction achieves 82.4% on CIFAR10.\nFurther, performance prediction for Recurrent Networks learning the Reber\ngrammar shows an $R^2$ of up to 0.81 given only structural information.",
          "link": "http://arxiv.org/abs/2107.12917",
          "publishedOn": "2021-07-28T02:02:32.718Z",
          "wordCount": 617,
          "title": "Experiments on Properties of Hidden Structures of Sparse Neural Networks. (arXiv:2107.12917v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1905.05976",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Matsuda_T/0/1/0/all/0/1\">Takeru Matsuda</a>, <a href=\"http://arxiv.org/find/math/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hyvarinen_A/0/1/0/all/0/1\">Aapo Hyvarinen</a>",
          "description": "Many statistical models are given in the form of non-normalized densities\nwith an intractable normalization constant. Since maximum likelihood estimation\nis computationally intensive for these models, several estimation methods have\nbeen developed which do not require explicit computation of the normalization\nconstant, such as noise contrastive estimation (NCE) and score matching.\nHowever, model selection methods for general non-normalized models have not\nbeen proposed so far. In this study, we develop information criteria for\nnon-normalized models estimated by NCE or score matching. They are\napproximately unbiased estimators of discrepancy measures for non-normalized\nmodels. Simulation results and applications to real data demonstrate that the\nproposed criteria enable selection of the appropriate non-normalized model in a\ndata-driven manner.",
          "link": "http://arxiv.org/abs/1905.05976",
          "publishedOn": "2021-07-28T02:02:32.710Z",
          "wordCount": 592,
          "title": "Information criteria for non-normalized models. (arXiv:1905.05976v5 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhenfei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haitao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_G/0/1/0/all/0/1\">Guangchun Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Haiwang Zhong</a>",
          "description": "In electricity markets, locational marginal price (LMP) forecasting is\nparticularly important for market participants in making reasonable bidding\nstrategies, managing potential trading risks, and supporting efficient system\nplanning and operation. Unlike existing methods that only consider LMPs'\ntemporal features, this paper tailors a spectral graph convolutional network\n(GCN) to greatly improve the accuracy of short-term LMP forecasting. A\nthree-branch network structure is then designed to match the structure of LMPs'\ncompositions. Such kind of network can extract the spatial-temporal features of\nLMPs, and provide fast and high-quality predictions for all nodes\nsimultaneously. The attention mechanism is also implemented to assign varying\nimportance weights between different nodes and time slots. Case studies based\non the IEEE-118 test system and real-world data from the PJM validate that the\nproposed model outperforms existing forecasting models in accuracy, and\nmaintains a robust performance by avoiding extreme errors.",
          "link": "http://arxiv.org/abs/2107.12794",
          "publishedOn": "2021-07-28T02:02:32.702Z",
          "wordCount": 601,
          "title": "Short-Term Electricity Price Forecasting based on Graph Convolution Network and Attention Mechanism. (arXiv:2107.12794v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>",
          "description": "We consider the problem of estimating frame-level full human body meshes\ngiven a video of a person with natural motion dynamics. While much progress in\nthis field has been in single image-based mesh estimation, there has been a\nrecent uptick in efforts to infer mesh dynamics from video given its role in\nalleviating issues such as depth ambiguity and occlusions. However, a key\nlimitation of existing work is the assumption that all the observed motion\ndynamics can be modeled using one dynamical/recurrent model. While this may\nwork well in cases with relatively simplistic dynamics, inference with\nin-the-wild videos presents many challenges. In particular, it is typically the\ncase that different body parts of a person undergo different dynamics in the\nvideo, e.g., legs may move in a way that may be dynamically different from\nhands (e.g., a person dancing). To address these issues, we present a new\nmethod for video mesh recovery that divides the human mesh into several local\nparts following the standard skeletal model. We then model the dynamics of each\nlocal part with separate recurrent models, with each model conditioned\nappropriately based on the known kinematic structure of the human body. This\nresults in a structure-informed local recurrent learning architecture that can\nbe trained in an end-to-end fashion with available annotations. We conduct a\nvariety of experiments on standard video mesh recovery benchmark datasets such\nas Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design\nof modeling local dynamics as well as establishing state-of-the-art results\nbased on standard evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.12847",
          "publishedOn": "2021-07-28T02:02:32.695Z",
          "wordCount": 708,
          "title": "Learning Local Recurrent Models for Human Mesh Recovery. (arXiv:2107.12847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Constantinou_A/0/1/0/all/0/1\">Anthony C. Constantinou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhigao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitson_N/0/1/0/all/0/1\">Neville K. Kitson</a>",
          "description": "Bayesian Networks (BNs) have become a powerful technology for reasoning under\nuncertainty, particularly in areas that require causal assumptions that enable\nus to simulate the effect of intervention. The graphical structure of these\nmodels can be determined by causal knowledge, learnt from data, or a\ncombination of both. While it seems plausible that the best approach in\nconstructing a causal graph involves combining knowledge with machine learning,\nthis approach remains underused in practice. We implement and evaluate 10\nknowledge approaches with application to different case studies and BN\nstructure learning algorithms available in the open-source Bayesys structure\nlearning system. The approaches enable us to specify pre-existing knowledge\nthat can be obtained from heterogeneous sources, to constrain or guide\nstructure learning. Each approach is assessed in terms of structure learning\neffectiveness and efficiency, including graphical accuracy, model fitting,\ncomplexity, and runtime; making this the first paper that provides a\ncomparative evaluation of a wide range of knowledge approaches for BN structure\nlearning. Because the value of knowledge depends on what data are available, we\nillustrate the results both with limited and big data. While the overall\nresults show that knowledge becomes less important with big data due to higher\nlearning accuracy rendering knowledge less important, some of the knowledge\napproaches are actually found to be more important with big data. Amongst the\nmain conclusions is the observation that reduced search space obtained from\nknowledge does not always imply reduced computational complexity, perhaps\nbecause the relationships implied by the data and knowledge are in tension.",
          "link": "http://arxiv.org/abs/2102.00473",
          "publishedOn": "2021-07-28T02:02:32.688Z",
          "wordCount": 719,
          "title": "Information fusion between knowledge and data in Bayesian network structure learning. (arXiv:2102.00473v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Squires_E/0/1/0/all/0/1\">Eric Squires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konda_R/0/1/0/all/0/1\">Rohit Konda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coogan_S/0/1/0/all/0/1\">Samuel Coogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egerstedt_M/0/1/0/all/0/1\">Magnus Egerstedt</a>",
          "description": "This paper demonstrates that in some cases the safety override arising from\nthe use of a barrier function can be needlessly restrictive. In particular, we\nexamine the case of fixed wing collision avoidance and show that when using a\nbarrier function, there are cases where two fixed wing aircraft can come closer\nto colliding than if there were no barrier function at all. In addition, we\nconstruct cases where the barrier function labels the system as unsafe even\nwhen the vehicles start arbitrarily far apart. In other words, the barrier\nfunction ensures safety but with unnecessary costs to performance. We therefore\nintroduce model free barrier functions which take a data driven approach to\ncreating a barrier function. We demonstrate the effectiveness of model free\nbarrier functions in a collision avoidance simulation of two fixed-wing\naircraft.",
          "link": "http://arxiv.org/abs/2107.12871",
          "publishedOn": "2021-07-28T02:02:32.679Z",
          "wordCount": 565,
          "title": "Model Free Barrier Functions via Implicit Evading Maneuvers. (arXiv:2107.12871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_P/0/1/0/all/0/1\">Paul Wimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnert_J/0/1/0/all/0/1\">Jens Mehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Condurache</a>",
          "description": "State-of-the-art deep neural network (DNN) pruning techniques, applied\none-shot before training starts, evaluate sparse architectures with the help of\na single criterion -- called pruning score. Pruning weights based on a solitary\nscore works well for some architectures and pruning rates but may also fail for\nother ones. As a common baseline for pruning scores, we introduce the notion of\na generalized synaptic score (GSS). In this work we do not concentrate on a\nsingle pruning criterion, but provide a framework for combining arbitrary GSSs\nto create more powerful pruning strategies. These COmbined Pruning Scores\n(COPS) are obtained by solving a constrained optimization problem. Optimizing\nfor more than one score prevents the sparse network to overly specialize on an\nindividual task, thus COntrols Pruning before training Starts. The\ncombinatorial optimization problem given by COPS is relaxed on a linear program\n(LP). This LP is solved analytically and determines a solution for COPS.\nFurthermore, an algorithm to compute it for two scores numerically is proposed\nand evaluated. Solving COPS in such a way has lower complexity than the best\ngeneral LP solver. In our experiments we compared pruning with COPS against\nstate-of-the-art methods for different network architectures and image\nclassification tasks and obtained improved results.",
          "link": "http://arxiv.org/abs/2107.12673",
          "publishedOn": "2021-07-28T02:02:32.672Z",
          "wordCount": 647,
          "title": "COPS: Controlled Pruning Before Training Starts. (arXiv:2107.12673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12033",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai V. Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bui_Thanh_T/0/1/0/all/0/1\">Tan Bui-Thanh</a>",
          "description": "Deep Learning (DL), in particular deep neural networks (DNN), by design is\npurely data-driven and in general does not require physics. This is the\nstrength of DL but also one of its key limitations when applied to science and\nengineering problems in which underlying physical properties (such as\nstability, conservation, and positivity) and desired accuracy need to be\nachieved. DL methods in their original forms are not capable of respecting the\nunderlying mathematical models or achieving desired accuracy even in big-data\nregimes. On the other hand, many data-driven science and engineering problems,\nsuch as inverse problems, typically have limited experimental or observational\ndata, and DL would overfit the data in this case. Leveraging information\nencoded in the underlying mathematical models, we argue, not only compensates\nmissing information in low data regimes but also provides opportunities to\nequip DL methods with the underlying physics and hence obtaining higher\naccuracy. This short communication introduces several model-constrained DL\napproaches (including both feed-forward DNN and autoencoders) that are capable\nof learning not only information hidden in the training data but also in the\nunderlying mathematical models to solve inverse problems. We present and\nprovide intuitions for our formulations for general nonlinear problems. For\nlinear inverse problems and linear networks, the first order optimality\nconditions show that our model-constrained DL approaches can learn information\nencoded in the underlying mathematical models, and thus can produce consistent\nor equivalent inverse solutions, while naive purely data-based counterparts\ncannot.",
          "link": "http://arxiv.org/abs/2105.12033",
          "publishedOn": "2021-07-28T02:02:32.631Z",
          "wordCount": 694,
          "title": "Model-Constrained Deep Learning Approaches for Inverse Problems. (arXiv:2105.12033v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varma_K/0/1/0/all/0/1\">Kamala Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1\">Nathalie Baracaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1\">Ali Anwar</a>",
          "description": "Federated learning has arisen as a mechanism to allow multiple participants\nto collaboratively train a model without sharing their data. In these settings,\nparticipants (workers) may not trust each other fully; for instance, a set of\ncompetitors may collaboratively train a machine learning model to detect fraud.\nThe workers provide local gradients that a central server uses to update a\nglobal model. This global model can be corrupted when Byzantine workers send\nmalicious gradients, which necessitates robust methods for aggregating\ngradients that mitigate the adverse effects of Byzantine inputs. Existing\nrobust aggregation algorithms are often computationally expensive and only\neffective under strict assumptions. In this paper, we introduce LayerwisE\nGradient AggregatTiOn (LEGATO), an aggregation algorithm that is, by contrast,\nscalable and generalizable. Informed by a study of layer-specific responses of\ngradients to Byzantine attacks, LEGATO employs a dynamic gradient reweighing\nscheme that is novel in its treatment of gradients based on layer-specific\nrobustness. We show that LEGATO is more computationally efficient than multiple\nstate-of-the-art techniques and more generally robust across a variety of\nattack settings in practice. We also demonstrate LEGATO's benefits for gradient\ndescent convergence in the absence of an attack.",
          "link": "http://arxiv.org/abs/2107.12490",
          "publishedOn": "2021-07-28T02:02:32.510Z",
          "wordCount": 644,
          "title": "LEGATO: A LayerwisE Gradient AggregaTiOn Algorithm for Mitigating Byzantine Attacks in Federated Learning. (arXiv:2107.12490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maurer_T/0/1/0/all/0/1\">Thomas Maurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>",
          "description": "Autonomous game design, generating games algorithmically, has been a longtime\ngoal within the technical games research field. However, existing autonomous\ngame design systems have relied in large part on human-authoring for game\ndesign knowledge, such as fitness functions in search-based methods. In this\npaper, we describe an experiment to attempt to learn a human-like fitness\nfunction for autonomous game design in an adversarial manner. While our\nexperimental work did not meet our expectations, we present an analysis of our\nsystem and results that we hope will be informative to future autonomous game\ndesign research.",
          "link": "http://arxiv.org/abs/2107.12501",
          "publishedOn": "2021-07-28T02:02:32.394Z",
          "wordCount": 558,
          "title": "Adversarial Random Forest Classifier for Automated Game Design. (arXiv:2107.12501v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rothermel_D/0/1/0/all/0/1\">Danielle Rothermel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Margaret Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "Self-supervised pre-training of large-scale transformer models on text\ncorpora followed by finetuning has achieved state-of-the-art on a number of\nnatural language processing tasks. Recently, Lu et al. (2021, arXiv:2103.05247)\nclaimed that frozen pretrained transformers (FPTs) match or outperform training\nfrom scratch as well as unfrozen (fine-tuned) pretrained transformers in a set\nof transfer tasks to other modalities. In our work, we find that this result\nis, in fact, an artifact of not tuning the learning rates. After carefully\nredesigning the empirical setup, we find that when tuning learning rates\nproperly, pretrained transformers do outperform or match training from scratch\nin all of our tasks, but only as long as the entire model is finetuned. Thus,\nwhile transfer from pretrained language models to other modalities does indeed\nprovide gains and hints at exciting possibilities for future work, properly\ntuning hyperparameters is important for arriving at robust findings.",
          "link": "http://arxiv.org/abs/2107.12460",
          "publishedOn": "2021-07-28T02:02:32.353Z",
          "wordCount": 609,
          "title": "Don't Sweep your Learning Rate under the Rug: A Closer Look at Cross-modal Transfer of Pretrained Transformers. (arXiv:2107.12460v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-27T02:03:39.955Z",
          "wordCount": null,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-27T02:03:39.753Z",
          "wordCount": null,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11419",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Komiyama_J/0/1/0/all/0/1\">Junpei Komiyama</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fouche_E/0/1/0/all/0/1\">Edouard Fouch&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1\">Junya Honda</a>",
          "description": "We consider nonstationary multi-armed bandit problems where the model\nparameters of the arms change over time. We introduce the adaptive resetting\nbandit (ADR-bandit), which is a class of bandit algorithms that leverages\nadaptive windowing techniques from the data stream community. We first provide\nnew guarantees on the quality of estimators resulting from adaptive windowing\ntechniques, which are of independent interest in the data mining community.\nFurthermore, we conduct a finite-time analysis of ADR-bandit in two typical\nenvironments: an abrupt environment where changes occur instantaneously and a\ngradual environment where changes occur progressively. We demonstrate that\nADR-bandit has nearly optimal performance when the abrupt or global changes\noccur in a coordinated manner that we call global changes. We demonstrate that\nforced exploration is unnecessary when we restrict the interest to the global\nchanges. Unlike the existing nonstationary bandit algorithms, ADR-bandit has\noptimal performance in stationary environments as well as nonstationary\nenvironments with global changes. Our experiments show that the proposed\nalgorithms outperform the existing approaches in synthetic and real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2107.11419",
          "publishedOn": "2021-07-27T02:03:39.735Z",
          "wordCount": null,
          "title": "Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits. (arXiv:2107.11419v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Triet H. M. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">M. Ali Babar</a>",
          "description": "Software Vulnerabilities (SVs) are increasing in complexity and scale, posing\ngreat security risks to many software systems. Given the limited resources in\npractice, SV assessment and prioritization help practitioners devise optimal SV\nmitigation plans based on various SV characteristics. The surge in SV data\nsources and data-driven techniques such as Machine Learning and Deep Learning\nhave taken SV assessment and prioritization to the next level. Our survey\nprovides a taxonomy of the past research efforts and highlights the best\npractices for data-driven SV assessment and prioritization. We also discuss the\ncurrent limitations and propose potential solutions to address such issues.",
          "link": "http://arxiv.org/abs/2107.08364",
          "publishedOn": "2021-07-27T02:03:39.406Z",
          "wordCount": null,
          "title": "A Survey on Data-driven Software Vulnerability Assessment and Prioritization. (arXiv:2107.08364v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajrin_J/0/1/0/all/0/1\">Janntatul Tajrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naira Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>",
          "description": "Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.",
          "link": "http://arxiv.org/abs/2107.03844",
          "publishedOn": "2021-07-27T02:03:39.382Z",
          "wordCount": null,
          "title": "A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models. (arXiv:2107.03844v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-27T02:03:39.376Z",
          "wordCount": null,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-27T02:03:39.375Z",
          "wordCount": null,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1\">Dirk Tasche</a>",
          "description": "For the binary prevalence quantification problem under prior probability\nshift, we determine the asymptotic variance of the maximum likelihood\nestimator. We find that it is a function of the Brier score for the regression\nof the class label against the features under the test data set distribution.\nThis observation suggests that optimising the accuracy of a base classifier on\nthe training data set helps to reduce the variance of the related quantifier on\nthe test data set. Therefore, we also point out training criteria for the base\nclassifier that imply optimisation of both of the Brier scores on the training\nand the test data sets.",
          "link": "http://arxiv.org/abs/2107.08209",
          "publishedOn": "2021-07-27T02:03:39.373Z",
          "wordCount": null,
          "title": "Minimising quantifier variance under prior probability shift. (arXiv:2107.08209v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezghani_A/0/1/0/all/0/1\">Amine Mezghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "We consider an Intelligent Reflecting Surface (IRS)-aided multiple-input\nsingle-output (MISO) system for downlink transmission. We compare the\nperformance of Deep Reinforcement Learning (DRL) and conventional optimization\nmethods in finding optimal phase shifts of the IRS elements to maximize the\nuser signal-to-noise (SNR) ratio. Furthermore, we evaluate the robustness of\nthese methods to channel impairments and changes in the system. We demonstrate\nnumerically that DRL solutions show more robustness to noisy channels and user\nmobility.",
          "link": "http://arxiv.org/abs/2107.08293",
          "publishedOn": "2021-07-27T02:03:39.372Z",
          "wordCount": null,
          "title": "On the Robustness of Deep Reinforcement Learning in IRS-Aided Wireless Communications Systems. (arXiv:2107.08293v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mrabah_N/0/1/0/all/0/1\">Nairouz Mrabah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1\">Mohamed Bouguessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touati_M/0/1/0/all/0/1\">Mohamed Fawzi Touati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ksantini_R/0/1/0/all/0/1\">Riadh Ksantini</a>",
          "description": "Most recent graph clustering methods have resorted to Graph Auto-Encoders\n(GAEs) to perform joint clustering and embedding learning. However, two\ncritical issues have been overlooked. First, the accumulative error, inflicted\nby learning with noisy clustering assignments, degrades the effectiveness and\nrobustness of the clustering model. This problem is called Feature Randomness.\nSecond, reconstructing the adjacency matrix sets the model to learn irrelevant\nsimilarities for the clustering task. This problem is called Feature Drift.\nInterestingly, the theoretical relation between the aforementioned problems has\nnot yet been investigated. We study these issues from two aspects: (1) there is\na trade-off between Feature Randomness and Feature Drift when clustering and\nreconstruction are performed at the same level, and (2) the problem of Feature\nDrift is more pronounced for GAE models, compared with vanilla auto-encoder\nmodels, due to the graph convolutional operation and the graph decoding design.\nMotivated by these findings, we reformulate the GAE-based clustering\nmethodology. Our solution is two-fold. First, we propose a sampling operator\n$\\Xi$ that triggers a protection mechanism against the noisy clustering\nassignments. Second, we propose an operator $\\Upsilon$ that triggers a\ncorrection mechanism against Feature Drift by gradually transforming the\nreconstructed graph into a clustering-oriented one. As principal advantages,\nour solution grants a considerable improvement in clustering effectiveness and\nrobustness and can be easily tailored to existing GAE models.",
          "link": "http://arxiv.org/abs/2107.08562",
          "publishedOn": "2021-07-27T02:03:39.370Z",
          "wordCount": null,
          "title": "Rethinking Graph Auto-Encoder Models for Attributed Graph Clustering. (arXiv:2107.08562v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiajun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Changnan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>",
          "description": "Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning\n(DRL) via combining deep learning (DL) with reinforcement learning (RL), which\nhas noticed that the distribution of the acquired data would change during the\ntraining process. DQN found this property might cause instability for training,\nso it proposed effective methods to handle the downside of the property.\nInstead of focusing on the unfavourable aspects, we find it critical for RL to\nease the gap between the estimated data distribution and the ground truth data\ndistribution while supervised learning (SL) fails to do so. From this new\nperspective, we extend the basic paradigm of RL called the Generalized Policy\nIteration (GPI) into a more generalized version, which is called the\nGeneralized Data Distribution Iteration (GDI). We see massive RL algorithms and\ntechniques can be unified into the GDI paradigm, which can be considered as one\nof the special cases of GDI. We provide theoretical proof of why GDI is better\nthan GPI and how it works. Several practical algorithms based on GDI have been\nproposed to verify the effectiveness and extensiveness of it. Empirical\nexperiments prove our state-of-the-art (SOTA) performance on Arcade Learning\nEnvironment (ALE), wherein our algorithm has achieved 9620.98% mean human\nnormalized score (HNS), 1146.39% median HNS and 22 human world record\nbreakthroughs (HWRB) using only 200M training frames. Our work aims to lead the\nRL research to step into the journey of conquering the human world records and\nseek real superhuman agents on both performance and efficiency.",
          "link": "http://arxiv.org/abs/2106.06232",
          "publishedOn": "2021-07-27T02:03:39.369Z",
          "wordCount": null,
          "title": "GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03383",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Prosperi_M/0/1/0/all/0/1\">Mattia Prosperi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marini_S/0/1/0/all/0/1\">Simone Marini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Boucher_C/0/1/0/all/0/1\">Christina Boucher</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "Whole genome sequencing (WGS) is quickly becoming the customary means for\nidentification of antimicrobial resistance (AMR) due to its ability to obtain\nhigh resolution information about the genes and mechanisms that are causing\nresistance and driving pathogen mobility. By contrast, traditional phenotypic\n(antibiogram) testing cannot easily elucidate such information. Yet development\nof AMR prediction tools from genotype-phenotype data can be biased, since\nsampling is non-randomized. Sample provenience, period of collection, and\nspecies representation can confound the association of genetic traits with AMR.\nThus, prediction models can perform poorly on new data with sampling\ndistribution shifts. In this work -- under an explicit set of causal\nassumptions -- we evaluate the effectiveness of propensity-based rebalancing\nand confounding adjustment on AMR prediction using genotype-phenotype AMR data\nfrom the Pathosystems Resource Integration Center (PATRIC). We select bacterial\ngenotypes (encoded as k-mer signatures, i.e. DNA fragments of length k),\ncountry, year, species, and AMR phenotypes for the tetracycline drug class,\npreparing test data with recent genomes coming from a single country. We test\nboosted logistic regression (BLR) and random forests (RF) with/without\nbias-handling. On 10,936 instances, we find evidence of species, location and\nyear imbalance with respect to the AMR phenotype. The crude versus\nbias-adjusted change in effect of genetic signatures on AMR varies but only\nmoderately (selecting the top 20,000 out of 40+ million k-mers). The area under\nthe receiver operating characteristic (AUROC) of the RF (0.95) is comparable to\nthat of BLR (0.94) on both out-of-bag samples from bootstrap and the external\ntest (n=1,085), where AUROCs do not decrease. We observe a 1%-5% gain in AUROC\nwith bias-handling compared to the sole use of genetic signatures. ...",
          "link": "http://arxiv.org/abs/2107.03383",
          "publishedOn": "2021-07-27T02:03:39.361Z",
          "wordCount": null,
          "title": "Assessing putative bias in prediction of anti-microbial resistance from real-world genotyping data under explicit causal assumptions. (arXiv:2107.03383v2 [q-bio.GN] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1\">Timour Igamberdiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>",
          "description": "Graph convolutional networks (GCNs) are a powerful architecture for\nrepresentation learning on documents that naturally occur as graphs, e.g.,\ncitation or social networks. However, sensitive personal information, such as\ndocuments with people's profiles or relationships as edges, are prone to\nprivacy leaks, as the trained model might reveal the original input. Although\ndifferential privacy (DP) offers a well-founded privacy-preserving framework,\nGCNs pose theoretical and practical challenges due to their training specifics.\nWe address these challenges by adapting differentially-private gradient-based\ntraining to GCNs and conduct experiments using two optimizers on five NLP\ndatasets in two languages. We propose a simple yet efficient method based on\nrandom graph splits that not only improves the baseline privacy bounds by a\nfactor of 2.7 while retaining competitive F1 scores, but also provides strong\nprivacy guarantees of epsilon = 1.0. We show that, under certain modeling\nchoices, privacy-preserving GCNs perform up to 90% of their non-private\nvariants, while formally guaranteeing strong privacy measures.",
          "link": "http://arxiv.org/abs/2102.09604",
          "publishedOn": "2021-07-27T02:03:39.357Z",
          "wordCount": null,
          "title": "Privacy-Preserving Graph Convolutional Networks for Text Classification. (arXiv:2102.09604v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03308",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Natarovskii_V/0/1/0/all/0/1\">Viacheslav Natarovskii</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rudolf_D/0/1/0/all/0/1\">Daniel Rudolf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sprungk_B/0/1/0/all/0/1\">Bj&#xf6;rn Sprungk</a>",
          "description": "For Bayesian learning, given likelihood function and Gaussian prior, the\nelliptical slice sampler, introduced by Murray, Adams and MacKay 2010, provides\na tool for the construction of a Markov chain for approximate sampling of the\nunderlying posterior distribution. Besides of its wide applicability and\nsimplicity its main feature is that no tuning is necessary. Under weak\nregularity assumptions on the posterior density we show that the corresponding\nMarkov chain is geometrically ergodic and therefore yield qualitative\nconvergence guarantees. We illustrate our result for Gaussian posteriors as\nthey appear in Gaussian process regression, as well as in a setting of a\nmulti-modal distribution. Remarkably, our numerical experiments indicate a\ndimension-independent performance of elliptical slice sampling even in\nsituations where our ergodicity result does not apply.",
          "link": "http://arxiv.org/abs/2105.03308",
          "publishedOn": "2021-07-27T02:03:39.356Z",
          "wordCount": null,
          "title": "Geometric convergence of elliptical slice sampling. (arXiv:2105.03308v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yinjun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1\">James Weimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1\">Susan B. Davidson</a>",
          "description": "High-quality labels are expensive to obtain for many machine learning tasks,\nsuch as medical image classification tasks. Therefore, probabilistic (weak)\nlabels produced by weak supervision tools are used to seed a process in which\ninfluential samples with weak labels are identified and cleaned by several\nhuman annotators to improve the model performance. To lower the overall cost\nand computational overhead of this process, we propose a solution called CHEF\n(CHEap and Fast label cleaning), which consists of the following three\ncomponents. First, to reduce the cost of human annotators, we use Infl, which\nprioritizes the most influential training samples for cleaning and provides\ncleaned labels to save the cost of one human annotator. Second, to accelerate\nthe sample selector phase and the model constructor phase, we use Increm-Infl\nto incrementally produce influential samples, and DeltaGrad-L to incrementally\nupdate the model. Third, we redesign the typical label cleaning pipeline so\nthat human annotators iteratively clean smaller batch of samples rather than\none big batch of samples. This yields better over all model performance and\nenables possible early termination when the expected model performance has been\nachieved. Extensive experiments show that our approach gives good model\nprediction performance while achieving significant speed-ups.",
          "link": "http://arxiv.org/abs/2107.08588",
          "publishedOn": "2021-07-27T02:03:39.355Z",
          "wordCount": null,
          "title": "CHEF: A Cheap and Fast Pipeline for Iteratively Cleaning Label Uncertainties (Technical Report). (arXiv:2107.08588v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Renzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakala_P/0/1/0/all/0/1\">Prem Sakala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xu Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yeye He</a>",
          "description": "Entity matching (EM) refers to the problem of identifying tuple pairs in one\nor more relations that refer to the same real world entities. Supervised\nmachine learning (ML) approaches, and deep learning based approaches in\nparticular, typically achieve state-of-the-art matching results. However, these\napproaches require many labeled examples, in the form of matching and\nnon-matching pairs, which are expensive and time-consuming to label. In this\npaper, we introduce Panda, a weakly supervised system specifically designed for\nEM. Panda uses the same labeling function abstraction as Snorkel, where\nlabeling functions (LF) are user-provided programs that can generate large\namounts of (somewhat noisy) labels quickly and cheaply, which can then be\ncombined via a labeling model to generate accurate final predictions. To\nsupport users developing LFs for EM, Panda provides an integrated development\nenvironment (IDE) that lives in a modern browser architecture. Panda's IDE\nfacilitates the development, debugging, and life-cycle management of LFs in the\ncontext of EM tasks, similar to how IDEs such as Visual Studio or Eclipse excel\nin general-purpose programming. Panda's IDE includes many novel features\npurpose-built for EM, such as smart data sampling, a builtin library of EM\nutility functions, automatically generated LFs, visual debugging of LFs, and\nfinally, an EM-specific labeling model. We show in this demo that Panda IDE can\ngreatly accelerate the development of high-quality EM solutions using weak\nsupervision.",
          "link": "http://arxiv.org/abs/2106.10821",
          "publishedOn": "2021-07-27T02:03:39.354Z",
          "wordCount": null,
          "title": "Demonstration of Panda: A Weakly Supervised Entity Matching System. (arXiv:2106.10821v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01863",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Mesuga_R/0/1/0/all/0/1\">Reymond Mesuga</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Bayanay_B/0/1/0/all/0/1\">Brian James Bayanay</a>",
          "description": "LIGO is considered the most sensitive and complicated gravitational\nexperiment ever built. Its main objective is to detect the gravitational wave\nfrom the strongest events in the universe by observing if the length of its\n4-kilometer arms change by a distance 10,000 times smaller than the diameter of\na proton. Due to its sensitivity, LIGO is prone to the disturbance of external\nnoises which affects the data being collected to detect the gravitational wave.\nThese noises are commonly called by the LIGO community as glitches. The\nobjective of this study is to evaluate the effeciency of various deep trasnfer\nlearning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch\nwaveform in gravitational wave data. The accuracy achieved by the said models\nare 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models\nachieved fairly high accuracy, it is observed that all of the model suffered\nfrom the lack of data for certain classes which is the main concern found in\nthe experiment.",
          "link": "http://arxiv.org/abs/2107.01863",
          "publishedOn": "2021-07-27T02:03:39.352Z",
          "wordCount": null,
          "title": "On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data. (arXiv:2107.01863v3 [gr-qc] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07283",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1\">Dirk Tasche</a>",
          "description": "When probabilistic classifiers are trained and calibrated, the so-called\ngrouping loss component of the calibration loss can easily be overlooked.\nGrouping loss refers to the gap between observable information and information\nactually exploited in the calibration exercise. We investigate the relation\nbetween grouping loss and the concept of sufficiency, identifying\ncomonotonicity as a useful criterion for sufficiency. We revisit the probing\nreduction approach of Langford & Zadrozny (2005) and find that it produces an\nestimator of probabilistic classifiers that reduces grouping loss. Finally, we\ndiscuss Brier curves as tools to support training and 'sufficient' calibration\nof probabilistic classifiers.",
          "link": "http://arxiv.org/abs/2105.07283",
          "publishedOn": "2021-07-27T02:03:39.349Z",
          "wordCount": null,
          "title": "Calibrating sufficiently. (arXiv:2105.07283v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14659",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Andrews_M/0/1/0/all/0/1\">Michael Andrews</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Burkle_B/0/1/0/all/0/1\">Bjorn Burkle</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-fan Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+DiCroce_D/0/1/0/all/0/1\">Davide DiCroce</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gleyzer_S/0/1/0/all/0/1\">Sergei Gleyzer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heintz_U/0/1/0/all/0/1\">Ulrich Heintz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Narain_M/0/1/0/all/0/1\">Meenakshi Narain</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Paulini_M/0/1/0/all/0/1\">Manfred Paulini</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pervan_N/0/1/0/all/0/1\">Nikolas Pervan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shafi_Y/0/1/0/all/0/1\">Yusef Shafi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Usai_E/0/1/0/all/0/1\">Emanuele Usai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_K/0/1/0/all/0/1\">Kun Yang</a>",
          "description": "We describe a novel application of the end-to-end deep learning technique to\nthe task of discriminating top quark-initiated jets from those originating from\nthe hadronization of a light quark or a gluon. The end-to-end deep learning\ntechnique combines deep learning algorithms and low-level detector\nrepresentation of the high-energy collision event. In this study, we use\nlow-level detector information from the simulated CMS Open Data samples to\nconstruct the top jet classifiers. To optimize classifier performance we\nprogressively add low-level information from the CMS tracking detector,\nincluding pixel detector reconstructed hits and impact parameters, and\ndemonstrate the value of additional tracking information even when no new\nspatial structures are added. Relying only on calorimeter energy deposits and\nreconstructed pixel detector hits, the end-to-end classifier achieves an AUC\nscore of 0.975$\\pm$0.002 for the task of classifying boosted top quark jets.\nAfter adding derived track quantities, the classifier AUC score increases to\n0.9824$\\pm$0.0013, serving as the first performance benchmark for these CMS\nOpen Data samples. We additionally provide a timing performance comparison of\ndifferent processor unit architectures for training the network.",
          "link": "http://arxiv.org/abs/2104.14659",
          "publishedOn": "2021-07-27T02:03:39.346Z",
          "wordCount": null,
          "title": "End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data. (arXiv:2104.14659v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04362",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Morehead_A/0/1/0/all/0/1\">Alex Morehead</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sedova_A/0/1/0/all/0/1\">Ada Sedova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_J/0/1/0/all/0/1\">Jianlin Cheng</a>",
          "description": "How and where proteins interface with one another can ultimately impact the\nproteins' functions along with a range of other biological processes. As such,\nprecise computational methods for protein interface prediction (PIP) come\nhighly sought after as they could yield significant advances in drug discovery\nand design as well as protein function analysis. However, the traditional\nbenchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a\nmodest 230 complexes for training, validating, and testing different machine\nlearning algorithms. In this work, we expand on a dataset recently introduced\nfor this task, the Database of Interacting Protein Structures (DIPS), to\npresent DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for\ngeometric deep learning of protein interfaces. The previous version of DIPS\ncontains only the Cartesian coordinates and types of the atoms comprising a\ngiven protein complex, whereas DIPS-Plus now includes a plethora of new\nresidue-level features including protrusion indices, half-sphere amino acid\ncompositions, and new profile hidden Markov model (HMM)-based sequence features\nfor each amino acid, giving researchers a large, well-curated feature bank for\ntraining protein interface prediction methods. We demonstrate through rigorous\nbenchmarks that training an existing state-of-the-art (SOTA) model for PIP on\nDIPS-Plus yields SOTA results, surpassing the performance of all other models\ntrained on residue-level and atom-level encodings of protein complexes to date.",
          "link": "http://arxiv.org/abs/2106.04362",
          "publishedOn": "2021-07-27T02:03:39.343Z",
          "wordCount": null,
          "title": "DIPS-Plus: The Enhanced Database of Interacting Protein Structures for Interface Prediction. (arXiv:2106.04362v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11589",
          "author": "<a href=\"http://arxiv.org/find/hep-ex/1/au:+Abbasi_R/0/1/0/all/0/1\">R. Abbasi</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Ackermann_M/0/1/0/all/0/1\">M. Ackermann</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Adams_J/0/1/0/all/0/1\">J. Adams</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Aguilar_J/0/1/0/all/0/1\">J. A. Aguilar</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Ahlers_M/0/1/0/all/0/1\">M. Ahlers</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Ahrens_M/0/1/0/all/0/1\">M. Ahrens</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Alispach_C/0/1/0/all/0/1\">C. Alispach</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Alves_A/0/1/0/all/0/1\">A. A. Alves Jr.</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Amin_N/0/1/0/all/0/1\">N. M. Amin</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+An_R/0/1/0/all/0/1\">R. An</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Andeen_K/0/1/0/all/0/1\">K. Andeen</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Anderson_T/0/1/0/all/0/1\">T. Anderson</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Ansseau_I/0/1/0/all/0/1\">I. Ansseau</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Anton_G/0/1/0/all/0/1\">G. Anton</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Arguelles_C/0/1/0/all/0/1\">C. Arg&#xfc;elles</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Axani_S/0/1/0/all/0/1\">S. Axani</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bai_X/0/1/0/all/0/1\">X. Bai</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+V%2E_A/0/1/0/all/0/1\">A. Balagopal V.</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Barbano_A/0/1/0/all/0/1\">A. Barbano</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Barwick_S/0/1/0/all/0/1\">S. W. Barwick</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bastian_B/0/1/0/all/0/1\">B. Bastian</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Basu_V/0/1/0/all/0/1\">V. Basu</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Baum_V/0/1/0/all/0/1\">V. Baum</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Baur_S/0/1/0/all/0/1\">S. Baur</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bay_R/0/1/0/all/0/1\">R. Bay</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Beatty_J/0/1/0/all/0/1\">J. J. Beatty</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Becker_K/0/1/0/all/0/1\">K.-H. Becker</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Tjus_J/0/1/0/all/0/1\">J. Becker Tjus</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bellenghi_C/0/1/0/all/0/1\">C. Bellenghi</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+BenZvi_S/0/1/0/all/0/1\">S. BenZvi</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Berley_D/0/1/0/all/0/1\">D. Berley</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bernardini_E/0/1/0/all/0/1\">E. Bernardini</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Besson_D/0/1/0/all/0/1\">D. Z. Besson</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Binder_G/0/1/0/all/0/1\">G. Binder</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bindig_D/0/1/0/all/0/1\">D. Bindig</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Blaufuss_E/0/1/0/all/0/1\">E. Blaufuss</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Blot_S/0/1/0/all/0/1\">S. Blot</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Boser_S/0/1/0/all/0/1\">S. B&#xf6;ser</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Botner_O/0/1/0/all/0/1\">O. Botner</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bottcher_J/0/1/0/all/0/1\">J. B&#xf6;ttcher</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bourbeau_E/0/1/0/all/0/1\">E. Bourbeau</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bourbeau_J/0/1/0/all/0/1\">J. Bourbeau</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bradascio_F/0/1/0/all/0/1\">F. Bradascio</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Braun_J/0/1/0/all/0/1\">J. Braun</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Bron_S/0/1/0/all/0/1\">S. Bron</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Brostean_Kaiser_J/0/1/0/all/0/1\">J. Brostean-Kaiser</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Burgman_A/0/1/0/all/0/1\">A. Burgman</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Busse_R/0/1/0/all/0/1\">R. S. Busse</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Campana_M/0/1/0/all/0/1\">M. A. Campana</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Chen_C/0/1/0/all/0/1\">C. Chen</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Chirkin_D/0/1/0/all/0/1\">D. Chirkin</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Choi_S/0/1/0/all/0/1\">S. Choi</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Clark_B/0/1/0/all/0/1\">B. A. Clark</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Clark_K/0/1/0/all/0/1\">K. Clark</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Classen_L/0/1/0/all/0/1\">L. Classen</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Coleman_A/0/1/0/all/0/1\">A. Coleman</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Collin_G/0/1/0/all/0/1\">G. H. Collin</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Conrad_J/0/1/0/all/0/1\">J. M. Conrad</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Coppin_P/0/1/0/all/0/1\">P. Coppin</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Correa_P/0/1/0/all/0/1\">P. Correa</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Cowen_D/0/1/0/all/0/1\">D. F. Cowen</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Cross_R/0/1/0/all/0/1\">R. Cross</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Dave_P/0/1/0/all/0/1\">P. Dave</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Clercq_C/0/1/0/all/0/1\">C. De Clercq</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+DeLaunay_J/0/1/0/all/0/1\">J. J. DeLaunay</a>, et al. (303 additional authors not shown)",
          "description": "Continued improvements on existing reconstruction methods are vital to the\nsuccess of high-energy physics experiments, such as the IceCube Neutrino\nObservatory. In IceCube, further challenges arise as the detector is situated\nat the geographic South Pole where computational resources are limited.\nHowever, to perform real-time analyses and to issue alerts to telescopes around\nthe world, powerful and fast reconstruction methods are desired. Deep neural\nnetworks can be extremely powerful, and their usage is computationally\ninexpensive once the networks are trained. These characteristics make a deep\nlearning-based approach an excellent candidate for the application in IceCube.\nA reconstruction method based on convolutional architectures and hexagonally\nshaped kernels is presented. The presented method is robust towards systematic\nuncertainties in the simulation and has been tested on experimental data. In\ncomparison to standard reconstruction methods in IceCube, it can improve upon\nthe reconstruction accuracy, while reducing the time necessary to run the\nreconstruction by two to three orders of magnitude.",
          "link": "http://arxiv.org/abs/2101.11589",
          "publishedOn": "2021-07-27T02:03:37.189Z",
          "wordCount": 1474,
          "title": "A Convolutional Neural Network based Cascade Reconstruction for the IceCube Neutrino Observatory. (arXiv:2101.11589v2 [hep-ex] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13471",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Barmparis_G/0/1/0/all/0/1\">G. D. Barmparis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tsironis_G/0/1/0/all/0/1\">G. P. Tsironis</a>",
          "description": "For an ensemble of nonlinear systems that model, for instance, molecules or\nphotonic systems, we propose a method that finds efficiently the configuration\nthat has prescribed transfer properties. Specifically, we use physics-informed\nmachine-learning (PIML) techniques to find the parameters for the efficient\ntransfer of an electron (or photon) to a targeted state in a non-linear dimer.\nWe create a machine learning model containing two variables, $\\chi_D$, and\n$\\chi_A$, representing the non-linear terms in the donor and acceptor target\nsystem states. We then introduce a data-free physics-informed loss function as\n$1.0 - P_j$, where $P_j$ is the probability, the electron being in the targeted\nstate, $j$. By minimizing the loss function, we maximize the occupation\nprobability to the targeted state. The method recovers known results in the\nTargeted Energy Transfer (TET) model, and it is then applied to a more complex\nsystem with an additional intermediate state. In this trimer configuration, the\nPIML approach discovers desired resonant paths from the donor to acceptor\nunits. The proposed PIML method is general and may be used in the chemical\ndesign of molecular complexes or engineering design of quantum or photonic\nsystems.",
          "link": "http://arxiv.org/abs/2104.13471",
          "publishedOn": "2021-07-27T02:03:37.141Z",
          "wordCount": 636,
          "title": "Discovering nonlinear resonances through physics-informed machine learning. (arXiv:2104.13471v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panerati_J/0/1/0/all/0/1\">Jacopo Panerati</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hehui Zheng</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">SiQi Zhou</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">James Xu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1\">Amanda Prorok</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Schoellig_A/0/1/0/all/0/1\">Angela P. Schoellig</a> (1 and 2) ((1) University of Toronto Institute for Aerospace Studies, (2) Vector Institute for Artificial Intelligence, (3) University of Cambridge)",
          "description": "Robotic simulators are crucial for academic research and education as well as\nthe development of safety-critical applications. Reinforcement learning\nenvironments -- simple simulations coupled with a problem specification in the\nform of a reward function -- are also important to standardize the development\n(and benchmarking) of learning algorithms. Yet, full-scale simulators typically\nlack portability and parallelizability. Vice versa, many reinforcement learning\nenvironments trade-off realism for high sample throughputs in toy-like\nproblems. While public data sets have greatly benefited deep learning and\ncomputer vision, we still lack the software tools to simultaneously develop --\nand fairly compare -- control theory and reinforcement learning approaches. In\nthis paper, we propose an open-source OpenAI Gym-like environment for multiple\nquadcopters based on the Bullet physics engine. Its multi-agent and vision\nbased reinforcement learning interfaces, as well as the support of realistic\ncollisions and aerodynamic effects, make it, to the best of our knowledge, a\nfirst of its kind. We demonstrate its use through several examples, either for\ncontrol (trajectory tracking with PID control, multi-robot flight with\ndownwash, etc.) or reinforcement learning (single and multi-agent stabilization\ntasks), hoping to inspire future research that combines control theory and\nmachine learning.",
          "link": "http://arxiv.org/abs/2103.02142",
          "publishedOn": "2021-07-27T02:03:37.081Z",
          "wordCount": 734,
          "title": "Learning to Fly -- a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control. (arXiv:2103.02142v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Hanbaek Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kureh_Y/0/1/0/all/0/1\">Yacoub H. Kureh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vendrow_J/0/1/0/all/0/1\">Joshua Vendrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porter_M/0/1/0/all/0/1\">Mason A. Porter</a>",
          "description": "It is common to use networks to encode the architecture of interactions\nbetween entities in complex systems in the physical, biological, social, and\ninformation sciences. Moreover, to study the large-scale behavior of complex\nsystems, it is important to study mesoscale structures in networks as building\nblocks that influence such behavior. In this paper, we present a new approach\nfor describing low-rank mesoscale structure in networks, and we illustrate our\napproach using several synthetic network models and empirical friendship,\ncollaboration, and protein--protein interaction (PPI) networks. We find that\nthese networks possess a relatively small number of `latent motifs' that\ntogether can successfully approximate most subnetworks at a fixed mesoscale. We\nuse an algorithm that we call \"network dictionary learning\" (NDL), which\ncombines a network sampling method and nonnegative matrix factorization, to\nlearn the latent motifs of a given network. The ability to encode a network\nusing a set of latent motifs has a wide range of applications to\nnetwork-analysis tasks, such as comparison, denoising, and edge inference.\nAdditionally, using our new network denoising and reconstruction (NDR)\nalgorithm, we demonstrate how to denoise a corrupted network by using only the\nlatent motifs that one learns directly from the corrupted networks.",
          "link": "http://arxiv.org/abs/2102.06984",
          "publishedOn": "2021-07-27T02:03:37.074Z",
          "wordCount": 689,
          "title": "Learning low-rank latent mesoscale structures in networks. (arXiv:2102.06984v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cousins_C/0/1/0/all/0/1\">Cyrus Cousins</a>",
          "description": "We address an inherent difficulty in welfare-theoretic fair machine learning\nby proposing an equivalently axiomatically-justified alternative and studying\nthe resulting computational and statistical learning questions. Welfare metrics\nquantify overall wellbeing across a population of one or more groups, and\nwelfare-based objectives and constraints have recently been proposed to\nincentivize fair machine learning methods to produce satisfactory solutions\nthat consider the diverse needs of multiple groups. Unfortunately, many\nmachine-learning problems are more naturally cast as loss minimization tasks,\nrather than utility maximization, which complicates direct application of\nwelfare-centric methods to fair machine learning. In this work, we define a\ncomplementary measure, termed malfare, measuring overall societal harm (rather\nthan wellbeing), with axiomatic justification via the standard axioms of\ncardinal welfare. We then cast fair machine learning as malfare minimization\nover the risk values (expected losses) of each group. Surprisingly, the axioms\nof cardinal welfare (malfare) dictate that this is not equivalent to simply\ndefining utility as negative loss. Building upon these concepts, we define\nfair-PAC (FPAC) learning, where an FPAC learner is an algorithm that learns an\n$\\varepsilon$-$\\delta$ malfare-optimal model with bounded sample complexity,\nfor any data distribution, and for any (axiomatically justified) malfare\nconcept. Finally, we show broad conditions under which, with appropriate\nmodifications, standard PAC-learners may be converted to FPAC learners. This\nplaces FPAC learning on firm theoretical ground, as it yields statistical and\ncomputational efficiency guarantees for many well-studied machine-learning\nmodels, and is also practically relevant, as it democratizes fair ML by\nproviding concrete training algorithms and rigorous generalization guarantees\nfor these models",
          "link": "http://arxiv.org/abs/2104.14504",
          "publishedOn": "2021-07-27T02:03:37.068Z",
          "wordCount": 705,
          "title": "An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning. (arXiv:2104.14504v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yong-Min Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Cong Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Won-Yong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>",
          "description": "We study the problem of embedding edgeless nodes such as users who newly\nenter the underlying network, while using graph neural networks (GNNs) widely\nstudied for effective representation learning of graphs thanks to its highly\nexpressive capability via message passing. Our study is motivated by the fact\nthat existing GNNs cannot be adopted for our problem since message passing to\nsuch edgeless nodes having no connections is impossible. To tackle this\nchallenge, we propose Edgeless-GNN, a new framework that enables GNNs to\ngenerate node embeddings even for edgeless nodes through unsupervised inductive\nlearning. Specifically, we start by constructing a $k$-nearest neighbor graph\n($k$NNG) based on the similarity of node attributes to replace the GNN's\ncomputation graph defined by the neighborhood-based aggregation of each node.\nAs our main contributions, the known network structure is used to train model\nparameters, while a new loss function is established using energy-based\nlearning in such a way that our model learns the network structure. For the\nedgeless nodes, we inductively infer embeddings for the edgeless nodes by using\nedges via $k$NNG construction as a computation graph. By evaluating the\nperformance of various downstream machine learning (ML) tasks, we empirically\ndemonstrate that Edgeless-GNN consistently outperforms state-of-the-art methods\nof inductive network embedding. Moreover, our findings corroborate the\neffectiveness of Edgeless-GNN in judiciously combining the replaced computation\ngraph with our newly designed loss. Our framework is GNN-model-agnostic; thus,\nGNN models can be appropriately chosen according to ones' needs and ML tasks.",
          "link": "http://arxiv.org/abs/2104.05225",
          "publishedOn": "2021-07-27T02:03:37.061Z",
          "wordCount": 724,
          "title": "Edgeless-GNN: Unsupervised Inductive Edgeless Network Embedding. (arXiv:2104.05225v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "Recent works in self-supervised learning have advanced the state-of-the-art\nby relying on the contrastive learning paradigm, which learns representations\nby pushing positive pairs, or similar examples from the same class, closer\ntogether while keeping negative pairs far apart. Despite the empirical\nsuccesses, theoretical foundations are limited -- prior analyses assume\nconditional independence of the positive pairs given the same class label, but\nrecent empirical applications use heavily correlated positive pairs (i.e., data\naugmentations of the same image). Our work analyzes contrastive learning\nwithout assuming conditional independence of positive pairs using a novel\nconcept of the augmentation graph on data. Edges in this graph connect\naugmentations of the same data, and ground-truth classes naturally form\nconnected sub-graphs. We propose a loss that performs spectral decomposition on\nthe population augmentation graph and can be succinctly written as a\ncontrastive learning objective on neural net representations. Minimizing this\nobjective leads to features with provable accuracy guarantees under linear\nprobe evaluation. By standard generalization bounds, these accuracy guarantees\nalso hold when minimizing the training contrastive loss. Empirically, the\nfeatures learned by our objective can match or outperform several strong\nbaselines on benchmark vision datasets. In all, this work provides the first\nprovable analysis for contrastive learning where guarantees for linear probe\nevaluation can apply to realistic empirical settings.",
          "link": "http://arxiv.org/abs/2106.04156",
          "publishedOn": "2021-07-27T02:03:37.054Z",
          "wordCount": 692,
          "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yining Wang</a>",
          "description": "The prevalence of e-commerce has made detailed customers' personal\ninformation readily accessible to retailers, and this information has been\nwidely used in pricing decisions. When involving personalized information, how\nto protect the privacy of such information becomes a critical issue in\npractice. In this paper, we consider a dynamic pricing problem over $T$ time\nperiods with an \\emph{unknown} demand function of posted price and personalized\ninformation. At each time $t$, the retailer observes an arriving customer's\npersonal information and offers a price. The customer then makes the purchase\ndecision, which will be utilized by the retailer to learn the underlying demand\nfunction. There is potentially a serious privacy concern during this process: a\nthird party agent might infer the personalized information and purchase\ndecisions from price changes from the pricing system. Using the fundamental\nframework of differential privacy from computer science, we develop a\nprivacy-preserving dynamic pricing policy, which tries to maximize the retailer\nrevenue while avoiding information leakage of individual customer's information\nand purchasing decisions. To this end, we first introduce a notion of\n\\emph{anticipating} $(\\varepsilon, \\delta)$-differential privacy that is\ntailored to dynamic pricing problem. Our policy achieves both the privacy\nguarantee and the performance guarantee in terms of regret. Roughly speaking,\nfor $d$-dimensional personalized information, our algorithm achieves the\nexpected regret at the order of $\\tilde{O}(\\varepsilon^{-1} \\sqrt{d^3 T})$,\nwhen the customers' information is adversarially chosen. For stochastic\npersonalized information, the regret bound can be further improved to\n$\\tilde{O}(\\sqrt{d^2T} + \\varepsilon^{-2} d^2)$",
          "link": "http://arxiv.org/abs/2009.12920",
          "publishedOn": "2021-07-27T02:03:37.028Z",
          "wordCount": 722,
          "title": "Privacy-Preserving Dynamic Personalized Pricing with Demand Learning. (arXiv:2009.12920v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00757",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Vecchio_A/0/1/0/all/0/1\">Alice Del Vecchio</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deac_A/0/1/0/all/0/1\">Andreea Deac</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>",
          "description": "Antibodies are proteins in the immune system which bind to antigens to detect\nand neutralise them. The binding sites in an antibody-antigen interaction are\nknown as the paratope and epitope, respectively, and the prediction of these\nregions is key to vaccine and synthetic antibody development. Contrary to prior\nart, we argue that paratope and epitope predictors require asymmetric\ntreatment, and propose distinct neural message passing architectures that are\ngeared towards the specific aspects of paratope and epitope prediction,\nrespectively. We obtain significant improvements on both tasks, setting the new\nstate-of-the-art and recovering favourable qualitative predictions on antigens\nof relevance to COVID-19.",
          "link": "http://arxiv.org/abs/2106.00757",
          "publishedOn": "2021-07-27T02:03:37.020Z",
          "wordCount": 613,
          "title": "Neural message passing for joint paratope-epitope prediction. (arXiv:2106.00757v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panknin_D/0/1/0/all/0/1\">Danny Panknin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shinichi Nakajima</a>",
          "description": "We propose a novel active learning strategy for regression, which is\nmodel-agnostic, robust against model mismatch, and interpretable. Assuming that\na small number of initial samples are available, we derive the optimal training\ndensity that minimizes the generalization error of local polynomial smoothing\n(LPS) with its kernel bandwidth tuned locally: We adopt the mean integrated\nsquared error (MISE) as a generalization criterion, and use the asymptotic\nbehavior of the MISE as well as the locally optimal bandwidths (LOB) - the\nbandwidth function that minimizes MISE in the asymptotic limit. The asymptotic\nexpression of our objective then reveals the dependence of the MISE on the\ntraining density, enabling analytic minimization. As a result,we obtain the\noptimal training density in a closed-form. The almost model-free nature of our\napproach thus helps to encode the essential properties of the target problem,\nproviding a robust and model-agnostic active learning strategy. Furthermore,\nthe obtained training density factorizes the influence of local function\ncomplexity, noise level and test density in a transparent and interpretable\nway. We validate our theory in numerical simulations, and show that the\nproposed active learning method outperforms the existing state-of-the-art\nmodel-agnostic approaches.",
          "link": "http://arxiv.org/abs/2105.11990",
          "publishedOn": "2021-07-27T02:03:36.950Z",
          "wordCount": 650,
          "title": "Optimal Sampling Density for Nonparametric Regression. (arXiv:2105.11990v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meltzer_P/0/1/0/all/0/1\">Peter Meltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1\">Hooman Shayani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1\">Amir Khasahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1\">Pradeep Kumar Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph Lambourne</a>",
          "description": "Boundary Representations (B-Reps) are the industry standard in 3D Computer\nAided Design/Manufacturing (CAD/CAM) and industrial design due to their\nfidelity in representing stylistic details. However, they have been ignored in\nthe 3D style research. Existing 3D style metrics typically operate on meshes or\npointclouds, and fail to account for end-user subjectivity by adopting fixed\ndefinitions of style, either through crowd-sourcing for style labels or\nhand-crafted features. We propose UVStyle-Net, a style similarity measure for\nB-Reps that leverages the style signals in the second order statistics of the\nactivations in a pre-trained (unsupervised) 3D encoder, and learns their\nrelative importance to a subjective end-user through few-shot learning. Our\napproach differs from all existing data-driven 3D style methods since it may be\nused in completely unsupervised settings, which is desirable given the lack of\npublicly available labelled B-Rep datasets. More importantly, the few-shot\nlearning accounts for the inherent subjectivity associated with style. We show\nquantitatively that our proposed method with B-Reps is able to capture stronger\nstyle signals than alternative methods on meshes and pointclouds despite its\nsignificantly greater computational efficiency. We also show it is able to\ngenerate meaningful style gradients with respect to the input shape, and that\nfew-shot learning with as few as two positive examples selected by an end-user\nis sufficient to significantly improve the style measure. Finally, we\ndemonstrate its efficacy on a large unlabeled public dataset of CAD models.\nSource code and data will be released in the future.",
          "link": "http://arxiv.org/abs/2105.02961",
          "publishedOn": "2021-07-27T02:03:36.928Z",
          "wordCount": 734,
          "title": "UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps. (arXiv:2105.02961v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1\">Abhishek Bhandwaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shari Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1\">Elizabeth Spelke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer D. Ullman</a>",
          "description": "For machine agents to successfully interact with humans in real-world\nsettings, they will need to develop an understanding of human mental life.\nIntuitive psychology, the ability to reason about hidden mental variables that\ndrive observable actions, comes naturally to people: even pre-verbal infants\ncan tell agents from objects, expecting agents to act efficiently to achieve\ngoals given constraints. Despite recent interest in machine agents that reason\nabout other agents, it is not clear if such agents learn or hold the core\npsychology principles that drive human reasoning. Inspired by cognitive\ndevelopment studies on intuitive psychology, we present a benchmark consisting\nof a large dataset of procedurally generated 3D animations, AGENT (Action,\nGoal, Efficiency, coNstraint, uTility), structured around four scenarios (goal\npreferences, action efficiency, unobserved constraints, and cost-reward\ntrade-offs) that probe key concepts of core intuitive psychology. We validate\nAGENT with human-ratings, propose an evaluation protocol emphasizing\ngeneralization, and compare two strong baselines built on Bayesian inverse\nplanning and a Theory of Mind neural network. Our results suggest that to pass\nthe designed tests of core intuitive psychology at human levels, a model must\nacquire or have built-in representations of how agents plan, combining utility\ncomputations and core knowledge of objects and physics.",
          "link": "http://arxiv.org/abs/2102.12321",
          "publishedOn": "2021-07-27T02:03:36.917Z",
          "wordCount": 712,
          "title": "AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_R/0/1/0/all/0/1\">Ryuji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishibashi_H/0/1/0/all/0/1\">Hideaki Ishibashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_T/0/1/0/all/0/1\">Tetsuo Furukawa</a>",
          "description": "Visual analytics (VA) is a visually assisted exploratory analysis approach in\nwhich knowledge discovery is executed interactively between the user and system\nin a human-centered manner. The purpose of this study is to develop a method\nfor the VA of set data aimed at supporting knowledge discovery and member\nselection. A typical target application is a visual support system for team\nanalysis and member selection, by which users can analyze past teams and\nexamine candidate lineups for new teams. Because there are several\ndifficulties, such as the combinatorial explosion problem, developing a VA\nsystem of set data is challenging. In this study, we first define the\nrequirements that the target system should satisfy and clarify the accompanying\nchallenges. Then we propose a method for the VA of set data, which satisfies\nthe requirements. The key idea is to model the generation process of sets and\ntheir outputs using a manifold network model. The proposed method visualizes\nthe relevant factors as a set of topographic maps on which various information\nis visualized. Furthermore, using the topographic maps as a bidirectional\ninterface, users can indicate their targets of interest in the system on these\nmaps. We demonstrate the proposed method by applying it to basketball teams,\nand compare with a benchmark system for outcome prediction and lineup\nreconstruction tasks. Because the method can be adapted to individual\napplication cases by extending the network structure, it can be a general\nmethod by which practical systems can be built.",
          "link": "http://arxiv.org/abs/2104.09231",
          "publishedOn": "2021-07-27T02:03:36.876Z",
          "wordCount": 731,
          "title": "Visual analytics of set data for knowledge discovery and member selection support. (arXiv:2104.09231v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1\">Geeticka Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1\">Brian Tse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "Recent years have seen many breakthroughs in natural language processing\n(NLP), transitioning it from a mostly theoretical field to one with many\nreal-world applications. Noting the rising number of applications of other\nmachine learning and AI techniques with pervasive societal impact, we\nanticipate the rising importance of developing NLP technologies for social\ngood. Inspired by theories in moral philosophy and global priorities research,\nwe aim to promote a guideline for social good in the context of NLP. We lay the\nfoundations via the moral philosophy definition of social good, propose a\nframework to evaluate the direct and indirect real-world impact of NLP tasks,\nand adopt the methodology of global priorities research to identify priority\ncauses for NLP research. Finally, we use our theoretical framework to provide\nsome practical guidelines for future NLP research for social good. Our data and\ncode are available at this http URL In\naddition, we curate a list of papers and resources on NLP for social good at\nhttps://github.com/zhijing-jin/NLP4SocialGood_Papers.",
          "link": "http://arxiv.org/abs/2106.02359",
          "publishedOn": "2021-07-27T02:03:36.838Z",
          "wordCount": 671,
          "title": "How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lowy_A/0/1/0/all/0/1\">Andrew Lowy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavan_R/0/1/0/all/0/1\">Rakesh Pavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baharlouei_S/0/1/0/all/0/1\">Sina Baharlouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1\">Meisam Razaviyayn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>",
          "description": "Despite the success of large-scale empirical risk minimization (ERM) at\nachieving high accuracy across a variety of machine learning tasks, fair ERM is\nhindered by the incompatibility of fairness constraints with stochastic\noptimization. In this paper, we propose the fair empirical risk minimization\nvia exponential R\\'enyi mutual information (FERMI) framework. FERMI is built on\na stochastic estimator for exponential R\\'enyi mutual information (ERMI), an\ninformation divergence measuring the degree of the dependence of predictions on\nsensitive attributes. Theoretically, we show that ERMI upper bounds existing\npopular fairness violation metrics, thus controlling ERMI provides guarantees\non other commonly used violations, such as $L_\\infty$. We derive an unbiased\nestimator for ERMI, which we use to derive the FERMI algorithm. We prove that\nFERMI converges for demographic parity, equalized odds, and equal opportunity\nnotions of fairness in stochastic optimization. Empirically, we show that FERMI\nis amenable to large-scale problems with multiple (non-binary) sensitive\nattributes and non-binary targets. Extensive experiments show that FERMI\nachieves the most favorable tradeoffs between fairness violation and test\naccuracy across all tested setups compared with state-of-the-art baselines for\ndemographic parity, equalized odds, equal opportunity. These benefits are\nespecially significant for non-binary classification with large sensitive sets\nand small batch sizes, showcasing the effectiveness of the FERMI objective and\nthe developed stochastic algorithm for solving it.",
          "link": "http://arxiv.org/abs/2102.12586",
          "publishedOn": "2021-07-27T02:03:36.819Z",
          "wordCount": 688,
          "title": "FERMI: Fair Empirical Risk Minimization via Exponential R\\'enyi Mutual Information. (arXiv:2102.12586v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oskarsdottir_M/0/1/0/all/0/1\">Mar&#xed;a &#xd3;skarsd&#xf3;ttir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1\">Cristi&#xe1;n Bravo</a>",
          "description": "We present a multilayer network model for credit risk assessment. Our model\naccounts for multiple connections between borrowers (such as their geographic\nlocation and their economic activity) and allows for explicitly modelling the\ninteraction between connected borrowers. We develop a multilayer personalized\nPageRank algorithm that allows quantifying the strength of the default exposure\nof any borrower in the network. We test our methodology in an agricultural\nlending framework, where it has been suspected for a long time default\ncorrelates between borrowers when they are subject to the same structural\nrisks. Our results show there are significant predictive gains just by\nincluding centrality multilayer network information in the model, and these\ngains are increased by more complex information such as the multilayer PageRank\nvariables. The results suggest default risk is highest when an individual is\nconnected to many defaulters, but this risk is mitigated by the size of the\nneighbourhood of the individual, showing both default risk and financial\nstability propagate throughout the network.",
          "link": "http://arxiv.org/abs/2010.09559",
          "publishedOn": "2021-07-27T02:03:36.812Z",
          "wordCount": 654,
          "title": "Multilayer Network Analysis for Improved Credit Risk Prediction. (arXiv:2010.09559v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zejin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>",
          "description": "The increasing concerns about data privacy and security drive an emerging\nfield of studying privacy-preserving machine learning from isolated data\nsources, i.e., federated learning. A class of federated learning, vertical\nfederated learning, where different parties hold different features for common\nusers, has a great potential of driving a more variety of business cooperation\namong enterprises in many fields. In machine learning, decision tree ensembles\nsuch as gradient boosting decision tree (GBDT) and random forest are widely\napplied powerful models with high interpretability and modeling efficiency.\nHowever, the interpretability is compromised in state-of-the-art vertical\nfederated learning frameworks such as SecureBoost with anonymous features to\navoid possible data breaches. To address this issue in the inference process,\nin this paper, we propose Fed-EINI to protect data privacy and allow the\ndisclosure of feature meaning by concealing decision paths with a\ncommunication-efficient secure computation method for inference outputs. The\nadvantages of Fed-EINI will be demonstrated through both theoretical analysis\nand extensive numerical results.",
          "link": "http://arxiv.org/abs/2105.09540",
          "publishedOn": "2021-07-27T02:03:36.805Z",
          "wordCount": 709,
          "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10162",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sen_P/0/1/0/all/0/1\">Pinaki Sen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bhatia_A/0/1/0/all/0/1\">Amandeep Singh Bhatia</a>",
          "description": "In quantum computing, the variational quantum algorithms (VQAs) are well\nsuited for finding optimal combinations of things in specific applications\nranging from chemistry all the way to finance. The training of VQAs with\ngradient descent optimization algorithm has shown a good convergence. At an\nearly stage, the simulation of variational quantum circuits on noisy\nintermediate-scale quantum (NISQ) devices suffers from noisy outputs. Just like\nclassical deep learning, it also suffers from vanishing gradient problems. It\nis a realistic goal to study the topology of loss landscape, to visualize the\ncurvature information and trainability of these circuits in the existence of\nvanishing gradients. In this paper, we calculated the Hessian and visualized\nthe loss landscape of variational quantum classifiers at different points in\nparameter space. The curvature information of variational quantum classifiers\n(VQC) is interpreted and the loss function's convergence is shown. It helps us\nbetter understand the behavior of variational quantum circuits to tackle\noptimization problems efficiently. We investigated the variational quantum\nclassifiers via Hessian on quantum computers, started with a simple 4-bit\nparity problem to gain insight into the practical behavior of Hessian, then\nthoroughly analyzed the behavior of Hessian's eigenvalues on training the\nvariational quantum classifier for the Diabetes dataset. Finally, we show that\nhow the adaptive Hessian learning rate can influence the convergence while\ntraining the variational circuits.",
          "link": "http://arxiv.org/abs/2105.10162",
          "publishedOn": "2021-07-27T02:03:36.797Z",
          "wordCount": 680,
          "title": "Variational Quantum Classifiers Through the Lens of the Hessian. (arXiv:2105.10162v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14694",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Farrell_M/0/1/0/all/0/1\">Max H. Farrell</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Liang_T/0/1/0/all/0/1\">Tengyuan Liang</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Misra_S/0/1/0/all/0/1\">Sanjog Misra</a>",
          "description": "We develop methodology for estimation and inference using machine learning to\nenrich economic models. Our framework takes a standard economic model and\nrecasts the parameters as fully flexible nonparametric functions, to capture\nthe rich heterogeneity based on potentially high dimensional or complex\nobservable characteristics. These \"parameter functions\" retain the\ninterpretability, economic meaning, and discipline of classical parameters.\nDeep learning is particularly well-suited to structured modeling of\nheterogeneity in economics. We show how to design the network architecture to\nmatch the structure of the economic model, delivering novel methodology that\nmoves deep learning beyond prediction. We prove convergence rates for the\nestimated parameter functions. These functions are the key inputs into the\nfinite-dimensional parameter of inferential interest. We obtain inference based\non a novel influence function calculation that covers any second-stage\nparameter and any machine-learning-enriched model that uses a smooth\nper-observation loss function. No additional derivations are required. The\nscore can be taken directly to data, using automatic differentiation if needed.\nThe researcher need only define the original model and define the parameter of\ninterest. A key insight is that we need not write down the influence function\nin order to evaluate it on the data. Our framework gives new results for a host\nof contexts, covering such diverse examples as price elasticities,\nwillingness-to-pay, and surplus measures in binary or multinomial choice\nmodels, effects of continuous treatment variables, fractional outcome models,\ncount data, heterogeneous production functions, and more. We apply our\nmethodology to a large scale advertising experiment for short-term loans. We\nshow how economically meaningful estimates and inferences can be made that\nwould be unavailable without our results.",
          "link": "http://arxiv.org/abs/2010.14694",
          "publishedOn": "2021-07-27T02:03:36.768Z",
          "wordCount": 729,
          "title": "Deep Learning for Individual Heterogeneity: An Automatic Inference Framework. (arXiv:2010.14694v2 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Long Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Do state-of-the-art natural language understanding models care about word\norder - one of the most important characteristics of a sequence? Not always! We\nfound 75% to 90% of the correct predictions of BERT-based classifiers, trained\non many GLUE tasks, remain constant after input words are randomly shuffled.\nDespite BERT embeddings are famously contextual, the contribution of each\nindividual word to downstream tasks is almost unchanged even after the word's\ncontext is shuffled. BERT-based models are able to exploit superficial cues\n(e.g. the sentiment of keywords in sentiment analysis; or the word-wise\nsimilarity between sequence-pair inputs in natural language inference) to make\ncorrect decisions when tokens are arranged in random orders. Encouraging\nclassifiers to capture word order information improves the performance on most\nGLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE\ntasks are not challenging machines to understand the meaning of a sentence.",
          "link": "http://arxiv.org/abs/2012.15180",
          "publishedOn": "2021-07-27T02:03:36.756Z",
          "wordCount": 649,
          "title": "Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?. (arXiv:2012.15180v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardaoui_D/0/1/0/all/0/1\">Dina Mardaoui</a>",
          "description": "The performance of modern algorithms on certain computer vision tasks such as\nobject recognition is now close to that of humans. This success was achieved at\nthe price of complicated architectures depending on millions of parameters and\nit has become quite challenging to understand how particular predictions are\nmade. Interpretability methods propose to give us this understanding. In this\npaper, we study LIME, perhaps one of the most popular. On the theoretical side,\nwe show that when the number of generated examples is large, LIME explanations\nare concentrated around a limit explanation for which we give an explicit\nexpression. We further this study for elementary shape detectors and linear\nmodels. As a consequence of this analysis, we uncover a connection between LIME\nand integrated gradients, another explanation method. More precisely, the LIME\nexplanations are similar to the sum of integrated gradients over the\nsuperpixels used in the preprocessing step of LIME.",
          "link": "http://arxiv.org/abs/2102.06307",
          "publishedOn": "2021-07-27T02:03:36.746Z",
          "wordCount": 617,
          "title": "What does LIME really see in images?. (arXiv:2102.06307v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1\">Aur&#xe9;lien Bellet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kermarrec_A/0/1/0/all/0/1\">Anne-Marie Kermarrec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavoie_E/0/1/0/all/0/1\">Erick Lavoie</a>",
          "description": "The convergence speed of machine learning models trained with Federated\nLearning is significantly affected by non-independent and identically\ndistributed (non-IID) data partitions, even more so in a fully decentralized\nsetting without a central server. In this paper, we show that the impact of\nlocal class bias, an important type of data non-IIDness, can be significantly\nreduced by carefully designing the underlying communication topology. We\npresent D-Cliques, a novel topology that reduces gradient bias by grouping\nnodes in interconnected cliques such that the local joint distribution in a\nclique is representative of the global class distribution. We also show how to\nadapt the updates of decentralized SGD to obtain unbiased gradients and\nimplement an effective momentum with D-Cliques. Our empirical evaluation on\nMNIST and CIFAR10 demonstrates that our approach provides similar convergence\nspeed as a fully-connected topology with a significant reduction in the number\nof edges and messages. In a 1000-node topology, D-Cliques requires 98% less\nedges and 96% less total messages, with further possible gains using a\nsmall-world topology across cliques.",
          "link": "http://arxiv.org/abs/2104.07365",
          "publishedOn": "2021-07-27T02:03:36.736Z",
          "wordCount": 682,
          "title": "D-Cliques: Compensating NonIIDness in Decentralized Federated Learning with Topology. (arXiv:2104.07365v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebel_P/0/1/0/all/0/1\">Patrick Ebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Most change detection methods assume that pre-change and post-change images\nare acquired by the same sensor. However, in many real-life scenarios, e.g.,\nnatural disaster, it is more practical to use the latest available images\nbefore and after the occurrence of incidence, which may be acquired using\ndifferent sensors. In particular, we are interested in the combination of the\nimages acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR\nimages appear vastly different from the optical images even when capturing the\nsame scene. Adding to this, change detection methods are often constrained to\nuse only target image-pair, no labeled data, and no additional unlabeled data.\nSuch constraints limit the scope of traditional supervised machine learning and\nunsupervised generative approaches for multi-sensor change detection. Recent\nrapid development of self-supervised learning methods has shown that some of\nthem can even work with only few images. Motivated by this, in this work we\npropose a method for multi-sensor change detection using only the unlabeled\ntarget bi-temporal images that are used for training a network in\nself-supervised fashion by using deep clustering and contrastive learning. The\nproposed method is evaluated on four multi-modal bi-temporal scenes showing\nchange and the benefits of our self-supervised approach are demonstrated.",
          "link": "http://arxiv.org/abs/2103.05102",
          "publishedOn": "2021-07-27T02:03:36.711Z",
          "wordCount": 667,
          "title": "Self-supervised Multisensor Change Detection. (arXiv:2103.05102v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14224",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gundersen_G/0/1/0/all/0/1\">Gregory W. Gundersen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cai_D/0/1/0/all/0/1\">Diana Cai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_C/0/1/0/all/0/1\">Chuteng Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Engelhardt_B/0/1/0/all/0/1\">Barbara E. Engelhardt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Adams_R/0/1/0/all/0/1\">Ryan P. Adams</a>",
          "description": "Online algorithms for detecting changepoints, or abrupt shifts in the\nbehavior of a time series, are often deployed with limited resources, e.g., to\nedge computing settings such as mobile phones or industrial sensors. In these\nscenarios it may be beneficial to trade the cost of collecting an environmental\nmeasurement against the quality or \"fidelity\" of this measurement and how the\nmeasurement affects changepoint estimation. For instance, one might decide\nbetween inertial measurements or GPS to determine changepoints for motion. A\nBayesian approach to changepoint detection is particularly appealing because we\ncan represent our posterior uncertainty about changepoints and make active,\ncost-sensitive decisions about data fidelity to reduce this posterior\nuncertainty. Moreover, the total cost could be dramatically lowered through\nactive fidelity switching, while remaining robust to changes in data\ndistribution. We propose a multi-fidelity approach that makes cost-sensitive\ndecisions about which data fidelity to collect based on maximizing information\ngain with respect to changepoints. We evaluate this framework on synthetic,\nvideo, and audio data and show that this information-based approach results in\naccurate predictions while reducing total cost.",
          "link": "http://arxiv.org/abs/2103.14224",
          "publishedOn": "2021-07-27T02:03:36.697Z",
          "wordCount": 638,
          "title": "Active multi-fidelity Bayesian online changepoint detection. (arXiv:2103.14224v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08181",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Granziol_D/0/1/0/all/0/1\">Diego Granziol</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1\">Xingchen Wan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "We conjecture that the inherent difference in generalisation between adaptive\nand non-adaptive gradient methods stems from the increased estimation noise in\nthe flattest directions of the true loss surface. We demonstrate that typical\nschedules used for adaptive methods (with low numerical stability or damping\nconstants) serve to bias relative movement towards flat directions relative to\nsharp directions, effectively amplifying the noise-to-signal ratio and harming\ngeneralisation. We further demonstrate that the numerical stability/damping\nconstant used in these methods can be decomposed into a learning rate reduction\nand linear shrinkage of the estimated curvature matrix. We then demonstrate\nsignificant generalisation improvements by increasing the shrinkage\ncoefficient, closing the generalisation gap entirely in both Logistic\nRegression and Deep Neural Network experiments. Finally, we show that other\npopular modifications to adaptive methods, such as decoupled weight decay and\npartial adaptivity can be shown to calibrate parameter updates to make better\nuse of sharper, more reliable directions.",
          "link": "http://arxiv.org/abs/2011.08181",
          "publishedOn": "2021-07-27T02:03:36.679Z",
          "wordCount": 615,
          "title": "Explaining the Adaptive Generalisation Gap. (arXiv:2011.08181v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baram_N/0/1/0/all/0/1\">Nir Baram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1\">Guy Tennenholtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>",
          "description": "Maximum Entropy (MaxEnt) reinforcement learning is a powerful learning\nparadigm which seeks to maximize return under entropy regularization. However,\naction entropy does not necessarily coincide with state entropy, e.g., when\nmultiple actions produce the same transition. Instead, we propose to maximize\nthe transition entropy, i.e., the entropy of next states. We show that\ntransition entropy can be described by two terms; namely, model-dependent\ntransition entropy and action redundancy. Particularly, we explore the latter\nin both deterministic and stochastic settings and develop tractable\napproximation methods in a near model-free setup. We construct algorithms to\nminimize action redundancy and demonstrate their effectiveness on a synthetic\nenvironment with multiple redundant actions as well as contemporary benchmarks\nin Atari and Mujoco. Our results suggest that action redundancy is a\nfundamental problem in reinforcement learning.",
          "link": "http://arxiv.org/abs/2102.11329",
          "publishedOn": "2021-07-27T02:03:36.672Z",
          "wordCount": 591,
          "title": "Action Redundancy in Reinforcement Learning. (arXiv:2102.11329v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jiaxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>",
          "description": "The rapid evolution of Graph Neural Networks (GNNs) has led to a growing\nnumber of new architectures as well as novel applications. However, current\nresearch focuses on proposing and evaluating specific architectural designs of\nGNNs, as opposed to studying the more general design space of GNNs that\nconsists of a Cartesian product of different design dimensions, such as the\nnumber of layers or the type of the aggregation function. Additionally, GNN\ndesigns are often specialized to a single task, yet few efforts have been made\nto understand how to quickly find the best GNN design for a novel task or a\nnovel dataset. Here we define and systematically study the architectural design\nspace for GNNs which consists of 315,000 different designs over 32 different\npredictive tasks. Our approach features three key innovations: (1) A general\nGNN design space; (2) a GNN task space with a similarity metric, so that for a\ngiven novel task/dataset, we can quickly identify/transfer the best performing\narchitecture; (3) an efficient and effective design space evaluation method\nwhich allows insights to be distilled from a huge number of model-task\ncombinations. Our key results include: (1) A comprehensive set of guidelines\nfor designing well-performing GNNs; (2) while best GNN designs for different\ntasks vary significantly, the GNN task space allows for transferring the best\ndesigns across different tasks; (3) models discovered using our design space\nachieve state-of-the-art performance. Overall, our work offers a principled and\nscalable approach to transition from studying individual GNN designs for\nspecific tasks, to systematically studying the GNN design space and the task\nspace. Finally, we release GraphGym, a powerful platform for exploring\ndifferent GNN designs and tasks. GraphGym features modularized GNN\nimplementation, standardized GNN evaluation, and reproducible and scalable\nexperiment management.",
          "link": "http://arxiv.org/abs/2011.08843",
          "publishedOn": "2021-07-27T02:03:36.665Z",
          "wordCount": 757,
          "title": "Design Space for Graph Neural Networks. (arXiv:2011.08843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1\">Farzan Memarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goo_W/0/1/0/all/0/1\">Wonjoon Goo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1\">Rudolf Lioutikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "We introduce Self-supervised Online Reward Shaping (SORS), which aims to\nimprove the sample efficiency of any RL algorithm in sparse-reward environments\nby automatically densifying rewards. The proposed framework alternates between\nclassification-based reward inference and policy update steps -- the original\nsparse reward provides a self-supervisory signal for reward inference by\nranking trajectories that the agent observes, while the policy update is\nperformed with the newly inferred, typically dense reward function. We\nintroduce theory that shows that, under certain conditions, this alteration of\nthe reward function will not change the optimal policy of the original MDP,\nwhile potentially increasing learning speed significantly. Experimental results\non several sparse-reward environments demonstrate that, across multiple\ndomains, the proposed algorithm is not only significantly more sample efficient\nthan a standard RL baseline using sparse rewards, but, at times, also achieves\nsimilar sample efficiency compared to when hand-designed dense reward functions\nare used.",
          "link": "http://arxiv.org/abs/2103.04529",
          "publishedOn": "2021-07-27T02:03:36.643Z",
          "wordCount": 625,
          "title": "Self-Supervised Online Reward Shaping in Sparse-Reward Environments. (arXiv:2103.04529v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Meiyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stankovic_J/0/1/0/all/0/1\">John Stankovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartocci_E/0/1/0/all/0/1\">Ezio Bartocci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lu Feng</a>",
          "description": "Predictive monitoring -- making predictions about future states and\nmonitoring if the predicted states satisfy requirements -- offers a promising\nparadigm in supporting the decision making of Cyber-Physical Systems (CPS).\nExisting works of predictive monitoring mostly focus on monitoring individual\npredictions rather than sequential predictions. We develop a novel approach for\nmonitoring sequential predictions generated from Bayesian Recurrent Neural\nNetworks (RNNs) that can capture the inherent uncertainty in CPS, drawing on\ninsights from our study of real-world CPS datasets. We propose a new logic\nnamed \\emph{Signal Temporal Logic with Uncertainty} (STL-U) to monitor a\nflowpipe containing an infinite set of uncertain sequences predicted by\nBayesian RNNs. We define STL-U strong and weak satisfaction semantics based on\nif all or some sequences contained in a flowpipe satisfy the requirement. We\nalso develop methods to compute the range of confidence levels under which a\nflowpipe is guaranteed to strongly (weakly) satisfy an STL-U formula.\nFurthermore, we develop novel criteria that leverage STL-U monitoring results\nto calibrate the uncertainty estimation in Bayesian RNNs. Finally, we evaluate\nthe proposed approach via experiments with real-world datasets and a simulated\nsmart city case study, which show very encouraging results of STL-U based\npredictive monitoring approach outperforming baselines.",
          "link": "http://arxiv.org/abs/2011.00384",
          "publishedOn": "2021-07-27T02:03:36.635Z",
          "wordCount": 699,
          "title": "Predictive Monitoring with Logic-Calibrated Uncertainty for Cyber-Physical Systems. (arXiv:2011.00384v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takemori_S/0/1/0/all/0/1\">Sho Takemori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_M/0/1/0/all/0/1\">Masahiro Sato</a>",
          "description": "The RKHS bandit problem (also called kernelized multi-armed bandit problem)\nis an online optimization problem of non-linear functions with noisy feedback.\nAlthough the problem has been extensively studied, there are unsatisfactory\nresults for some problems compared to the well-studied linear bandit case.\nSpecifically, there is no general algorithm for the adversarial RKHS bandit\nproblem. In addition, high computational complexity of existing algorithms\nhinders practical application. We address these issues by considering a novel\namalgamation of approximation theory and the misspecified linear bandit\nproblem. Using an approximation method, we propose efficient algorithms for the\nstochastic RKHS bandit problem and the first general algorithm for the\nadversarial RKHS bandit problem. Furthermore, we empirically show that one of\nour proposed methods has comparable cumulative regret to IGP-UCB and its\nrunning time is much shorter.",
          "link": "http://arxiv.org/abs/2010.12167",
          "publishedOn": "2021-07-27T02:03:36.628Z",
          "wordCount": 720,
          "title": "Approximation Theory Based Methods for RKHS Bandits. (arXiv:2010.12167v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sodhani_S/0/1/0/all/0/1\">Shagun Sodhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delalleau_O/0/1/0/all/0/1\">Olivier Delalleau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "Codistillation has been proposed as a mechanism to share knowledge among\nconcurrently trained models by encouraging them to represent the same function\nthrough an auxiliary loss. This contrasts with the more commonly used\nfully-synchronous data-parallel stochastic gradient descent methods, where\ndifferent model replicas average their gradients (or parameters) at every\niteration and thus maintain identical parameters. We investigate codistillation\nin a distributed training setup, complementing previous work which focused on\nextremely large batch sizes. Surprisingly, we find that even at moderate batch\nsizes, models trained with codistillation can perform as well as models trained\nwith synchronous data-parallel methods, despite using a much weaker\nsynchronization mechanism. These findings hold across a range of batch sizes\nand learning rate schedules, as well as different kinds of models and datasets.\nObtaining this level of accuracy, however, requires properly accounting for the\nregularization effect of codistillation, which we highlight through several\nempirical observations. Overall, this work contributes to a better\nunderstanding of codistillation and how to best take advantage of it in a\ndistributed computing environment.",
          "link": "http://arxiv.org/abs/2010.02838",
          "publishedOn": "2021-07-27T02:03:36.618Z",
          "wordCount": 653,
          "title": "A Closer Look at Codistillation for Distributed Training. (arXiv:2010.02838v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Janisch_J/0/1/0/all/0/1\">Jarom&#xed;r Janisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Pevn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lisy_V/0/1/0/all/0/1\">Viliam Lis&#xfd;</a>",
          "description": "We extend the framework of Classification with Costly Features (CwCF) that\nworks with samples of fixed dimensions to trees of varying depth and breadth\n(similar to a JSON/XML file). In this setting, the sample is a tree - sets of\nsets of features. Individually for each sample, the task is to sequentially\nselect informative features that help the classification. Each feature has a\nreal-valued cost, and the objective is to maximize accuracy while minimizing\nthe total cost. The process is modeled as an MDP where the states represent the\nacquired features, and the actions select unknown features. We present a\nspecialized neural network architecture trained through deep reinforcement\nlearning that naturally fits the data and directly selects features in the\ntree. We demonstrate our method in seven datasets and compare it to two\nbaselines.",
          "link": "http://arxiv.org/abs/1911.08756",
          "publishedOn": "2021-07-27T02:03:36.595Z",
          "wordCount": 625,
          "title": "Hierarchical Multiple-Instance Data Classification with Costly Features. (arXiv:1911.08756v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>",
          "description": "Health literacy has emerged as a crucial factor in making appropriate health\ndecisions and ensuring treatment outcomes. However, medical jargon and the\ncomplex structure of professional language in this domain make health\ninformation especially hard to interpret. Thus, there is an urgent unmet need\nfor automated methods to enhance the accessibility of the biomedical literature\nto the general population. This problem can be framed as a type of translation\nproblem between the language of healthcare professionals, and that of the\ngeneral public. In this paper, we introduce the novel task of automated\ngeneration of lay language summaries of biomedical scientific reviews, and\nconstruct a dataset to support the development and evaluation of automated\nmethods through which to enhance the accessibility of the biomedical\nliterature. We conduct analyses of the various challenges in solving this task,\nincluding not only summarization of the key points but also explanation of\nbackground knowledge and simplification of professional language. We experiment\nwith state-of-the-art summarization models as well as several data augmentation\ntechniques, and evaluate their performance using both automated metrics and\nhuman assessment. Results indicate that automatically generated summaries\nproduced using contemporary neural architectures can achieve promising quality\nand readability as compared with reference summaries developed for the lay\npublic by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score\nof 13.30). We also discuss the limitations of the current attempt, providing\ninsights and directions for future work.",
          "link": "http://arxiv.org/abs/2012.12573",
          "publishedOn": "2021-07-27T02:03:36.587Z",
          "wordCount": 716,
          "title": "Automated Lay Language Summarization of Biomedical Scientific Reviews. (arXiv:2012.12573v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Holla_N/0/1/0/all/0/1\">Nithin Holla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pushkar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>",
          "description": "Lifelong learning requires models that can continuously learn from sequential\nstreams of data without suffering catastrophic forgetting due to shifts in data\ndistributions. Deep learning models have thrived in the non-sequential learning\nparadigm; however, when used to learn a sequence of tasks, they fail to retain\npast knowledge and learn incrementally. We propose a novel approach to lifelong\nlearning of language tasks based on meta-learning with sparse experience replay\nthat directly optimizes to prevent forgetting. We show that under the realistic\nsetting of performing a single pass on a stream of tasks and without any task\nidentifiers, our method obtains state-of-the-art results on lifelong text\nclassification and relation extraction. We analyze the effectiveness of our\napproach and further demonstrate its low computational and space complexity.",
          "link": "http://arxiv.org/abs/2009.04891",
          "publishedOn": "2021-07-27T02:03:36.580Z",
          "wordCount": 591,
          "title": "Meta-Learning with Sparse Experience Replay for Lifelong Language Learning. (arXiv:2009.04891v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.12618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Golts_A/0/1/0/all/0/1\">Alex Golts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schechner_Y/0/1/0/all/0/1\">Yoav Y. Schechner</a>",
          "description": "Computer vision tasks are often expected to be executed on compressed images.\nClassical image compression standards like JPEG 2000 are widely used. However,\nthey do not account for the specific end-task at hand. Motivated by works on\nrecurrent neural network (RNN)-based image compression and three-dimensional\n(3D) reconstruction, we propose unified network architectures to solve both\ntasks jointly. These joint models provide image compression tailored for the\nspecific task of 3D reconstruction. Images compressed by our proposed models,\nyield 3D reconstruction performance superior as compared to using JPEG 2000\ncompression. Our models significantly extend the range of compression rates for\nwhich 3D reconstruction is possible. We also show that this can be done highly\nefficiently at almost no additional cost to obtain compression on top of the\ncomputation already required for performing the 3D reconstruction task.",
          "link": "http://arxiv.org/abs/2003.12618",
          "publishedOn": "2021-07-27T02:03:36.574Z",
          "wordCount": 605,
          "title": "Image compression optimized for 3D reconstruction by utilizing deep neural networks. (arXiv:2003.12618v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Allen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1\">Ankur Moitra</a>",
          "description": "This work represents a natural coalescence of two important lines of work:\nlearning mixtures of Gaussians and algorithmic robust statistics. In particular\nwe give the first provably robust algorithm for learning mixtures of any\nconstant number of Gaussians. We require only mild assumptions on the mixing\nweights (bounded fractionality) and that the total variation distance between\ncomponents is bounded away from zero. At the heart of our algorithm is a new\nmethod for proving dimension-independent polynomial identifiability through\napplying a carefully chosen sequence of differential operations to certain\ngenerating functions that not only encode the parameters we would like to learn\nbut also the system of polynomial equations we would like to solve. We show how\nthe symbolic identities we derive can be directly used to analyze a natural\nsum-of-squares relaxation.",
          "link": "http://arxiv.org/abs/2011.03622",
          "publishedOn": "2021-07-27T02:03:36.567Z",
          "wordCount": 613,
          "title": "Settling the Robust Learnability of Mixtures of Gaussians. (arXiv:2011.03622v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1\">Deepak Mishra</a>",
          "description": "Building neural network classifiers with an ability to distinguish between in\nand out-of distribution inputs is an important step towards faithful deep\nlearning systems. Some of the successful approaches for this, resort to\narchitectural novelties, such as ensembles, with increased complexities in\nterms of the number of parameters and training procedures. Whereas some other\napproaches make use of surrogate samples, which are easy to create and work as\nproxies for actual out-of-distribution (OOD) samples, to train the networks for\nOOD detection. In this paper, we propose a very simple approach for enhancing\nthe ability of a pretrained network to detect OOD inputs without even altering\nthe original parameter values. We define a probabilistic trust interval for\neach weight parameter of the network and optimize its size according to the\nin-distribution (ID) inputs. It allows the network to sample additional weight\nvalues along with the original values at the time of inference and use the\nobserved disagreement among the corresponding outputs for OOD detection. In\norder to capture the disagreement effectively, we also propose a measure and\nestablish its suitability using empirical evidence. Our approach outperforms\nthe existing state-of-the-art methods on various OOD datasets by considerable\nmargins without using any real or surrogate OOD samples. We also analyze the\nperformance of our approach on adversarial and corrupted inputs such as\nCIFAR-10-C and demonstrate its ability to clearly distinguish such inputs as\nwell. By using fundamental theorem of calculus on neural networks, we explain\nwhy our technique doesn't need to observe OOD samples during training to\nachieve results better than the previous works.",
          "link": "http://arxiv.org/abs/2102.01336",
          "publishedOn": "2021-07-27T02:03:36.544Z",
          "wordCount": 717,
          "title": "Probabilistic Trust Intervals for Out of Distribution Detection. (arXiv:2102.01336v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12487",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mardaoui_D/0/1/0/all/0/1\">Dina Mardaoui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>",
          "description": "Text data are increasingly handled in an automated fashion by machine\nlearning algorithms. But the models handling these data are not always\nwell-understood due to their complexity and are more and more often referred to\nas \"black-boxes.\" Interpretability methods aim to explain how these models\noperate. Among them, LIME has become one of the most popular in recent years.\nHowever, it comes without theoretical guarantees: even for simple models, we\nare not sure that LIME behaves accurately. In this paper, we provide a first\ntheoretical analysis of LIME for text data. As a consequence of our theoretical\nfindings, we show that LIME indeed provides meaningful explanations for simple\nmodels, namely decision trees and linear models.",
          "link": "http://arxiv.org/abs/2010.12487",
          "publishedOn": "2021-07-27T02:03:36.538Z",
          "wordCount": 574,
          "title": "An Analysis of LIME for Text Data. (arXiv:2010.12487v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05245",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_S/0/1/0/all/0/1\">Shunhui Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zewen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xinyu Zhou</a>",
          "description": "The pandemic of COVID-19 has caused millions of infections, which has led to\na great loss all over the world, socially and economically. Due to the\nfalse-negative rate and the time-consuming of the conventional Reverse\nTranscription Polymerase Chain Reaction (RT-PCR) tests, diagnosing based on\nX-ray images and Computed Tomography (CT) images has been widely adopted.\nTherefore, researchers of the computer vision area have developed many\nautomatic diagnosing models based on machine learning or deep learning to\nassist the radiologists and improve the diagnosing accuracy. In this paper, we\npresent a review of these recently emerging automatic diagnosing models. 70\nmodels proposed from February 14, 2020, to July 21, 2020, are involved. We\nanalyzed the models from the perspective of preprocessing, feature extraction,\nclassification, and evaluation. Based on the limitation of existing models, we\npointed out that domain adaption in transfer learning and interpretability\npromotion would be the possible future directions.",
          "link": "http://arxiv.org/abs/2006.05245",
          "publishedOn": "2021-07-27T02:03:36.531Z",
          "wordCount": 691,
          "title": "A Review of Automated Diagnosis of COVID-19 Based on Scanning Images. (arXiv:2006.05245v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Ahmed Mohamed Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oligeri_G/0/1/0/all/0/1\">Gabriele Oligeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voigt_T/0/1/0/all/0/1\">Thiemo Voigt</a>",
          "description": "We present a new machine learning-based attack that exploits network patterns\nto detect the presence of smart IoT devices and running services in the WiFi\nradio spectrum. We perform an extensive measurement campaign of data\ncollection, and we build up a model describing the traffic patterns\ncharacterizing three popular IoT smart home devices, i.e., Google Nest Mini,\nAmazon Echo, and Amazon Echo Dot. We prove that it is possible to detect and\nidentify with overwhelming probability their presence and the services running\nby the aforementioned devices in a crowded WiFi scenario. This work proves that\nstandard encryption techniques alone are not sufficient to protect the privacy\nof the end-user, since the network traffic itself exposes the presence of both\nthe device and the associated service. While more work is required to prevent\nnon-trusted third parties to detect and identify the user's devices, we\nintroduce Eclipse, a technique to mitigate these types of attacks, which\nreshapes the traffic making the identification of the devices and the\nassociated services similar to the random classification baseline.",
          "link": "http://arxiv.org/abs/2009.07672",
          "publishedOn": "2021-07-27T02:03:36.525Z",
          "wordCount": 695,
          "title": "The Dark (and Bright) Side of IoT: Attacks and Countermeasures for Identifying Smart Home Devices and Services. (arXiv:2009.07672v4 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aanjaneya_M/0/1/0/all/0/1\">Mridul Aanjaneya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>",
          "description": "Learning policies in simulation is promising for reducing human effort when\ntraining robot controllers. This is especially true for soft robots that are\nmore adaptive and safe but also more difficult to accurately model and control.\nThe sim2real gap is the main barrier to successfully transfer policies from\nsimulation to a real robot. System identification can be applied to reduce this\ngap but traditional identification methods require a lot of manual tuning.\nData-driven alternatives can tune dynamical models directly from data but are\noften data hungry, which also incorporates human effort in collecting data.\nThis work proposes a data-driven, end-to-end differentiable simulator focused\non the exciting but challenging domain of tensegrity robots. To the best of the\nauthors' knowledge, this is the first differentiable physics engine for\ntensegrity robots that supports cable, contact, and actuation modeling. The aim\nis to develop a reasonably simplified, data-driven simulation, which can learn\napproximate dynamics with limited ground truth data. The dynamics must be\naccurate enough to generate policies that can be transferred back to the\nground-truth system. As a first step in this direction, the current work\ndemonstrates sim2sim transfer, where the unknown physical model of MuJoCo acts\nas a ground truth system. Two different tensegrity robots are used for\nevaluation and learning of locomotion policies, a 6-bar and a 3-bar tensegrity.\nThe results indicate that only 0.25\\% of ground truth data are needed to train\na policy that works on the ground truth system when the differentiable engine\nis used for training against training the policy directly on the ground truth\nsystem.",
          "link": "http://arxiv.org/abs/2011.04929",
          "publishedOn": "2021-07-27T02:03:36.518Z",
          "wordCount": 744,
          "title": "Sim2Sim Evaluation of a Novel Data-Efficient Differentiable Physics Engine for Tensegrity Robots. (arXiv:2011.04929v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Abdelrahman Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadhoud_M/0/1/0/all/0/1\">Mayada Hadhoud</a>",
          "description": "Instance segmentation has gained recently huge attention in various computer\nvision applications. It aims at providing different IDs to different objects of\nthe scene, even if they belong to the same class. Instance segmentation is\nusually performed as a two-stage pipeline. First, an object is detected, then\nsemantic segmentation within the detected box area is performed which involves\ncostly up-sampling. In this paper, we propose Insta-YOLO, a novel one-stage\nend-to-end deep learning model for real-time instance segmentation. Instead of\npixel-wise prediction, our model predicts instances as object contours\nrepresented by 2D points in Cartesian space. We evaluate our model on three\ndatasets, namely, Carvana,Cityscapes and Airbus. We compare our results to the\nstate-of-the-art models for instance segmentation. The results show our model\nachieves competitive accuracy in terms of mAP at twice the speed on GTX-1080\nGPU.",
          "link": "http://arxiv.org/abs/2102.06777",
          "publishedOn": "2021-07-27T02:03:36.494Z",
          "wordCount": 596,
          "title": "INSTA-YOLO: Real-Time Instance Segmentation. (arXiv:2102.06777v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>",
          "description": "We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n\nHaving all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n\nWe solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article. Just like for linear convolution a\nmorphological convolution is specified by a kernel that we train in our\nPDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling\nand ReLUs as they are already subsumed by morphological convolutions.\n\nWe present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.",
          "link": "http://arxiv.org/abs/2001.09046",
          "publishedOn": "2021-07-27T02:03:36.473Z",
          "wordCount": 767,
          "title": "PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milani_F/0/1/0/all/0/1\">Federico Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraternali_P/0/1/0/all/0/1\">Piero Fraternali</a>",
          "description": "Iconography in art is the discipline that studies the visual content of\nartworks to determine their motifs and themes andto characterize the way these\nare represented. It is a subject of active research for a variety of purposes,\nincluding the interpretation of meaning, the investigation of the origin and\ndiffusion in time and space of representations, and the study of influences\nacross artists and art works. With the proliferation of digital archives of art\nimages, the possibility arises of applying Computer Vision techniques to the\nanalysis of art images at an unprecedented scale, which may support iconography\nresearch and education. In this paper we introduce a novel paintings data set\nfor iconography classification and present the quantitativeand qualitative\nresults of applying a Convolutional Neural Network (CNN) classifier to the\nrecognition of the iconography of artworks. The proposed classifier achieves\ngood performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73%\nAverage Precision) in the task of identifying saints in Christian religious\npaintings, a task made difficult by the presence of classes with very similar\nvisual features. Qualitative analysis of the results shows that the CNN focuses\non the traditional iconic motifs that characterize the representation of each\nsaint and exploits such hints to attain correct identification. The ultimate\ngoal of our work is to enable the automatic extraction, decomposition, and\ncomparison of iconography elements to support iconographic studies and\nautomatic art work annotation.",
          "link": "http://arxiv.org/abs/2010.11697",
          "publishedOn": "2021-07-27T02:03:36.466Z",
          "wordCount": 739,
          "title": "A Data Set and a Convolutional Model for Iconography Classification in Paintings. (arXiv:2010.11697v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07356",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jin_H/0/1/0/all/0/1\">Hui Jin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Montufar_G/0/1/0/all/0/1\">Guido Mont&#xfa;far</a>",
          "description": "We investigate gradient descent training of wide neural networks and the\ncorresponding implicit bias in function space. For univariate regression, we\nshow that the solution of training a width-$n$ shallow ReLU network is within\n$n^{- 1/2}$ of the function which fits the training data and whose difference\nfrom the initial function has the smallest 2-norm of the second derivative\nweighted by a curvature penalty that depends on the probability distribution\nthat is used to initialize the network parameters. We compute the curvature\npenalty function explicitly for various common initialization procedures. For\ninstance, asymmetric initialization with a uniform distribution yields a\nconstant curvature penalty, and thence the solution function is the natural\ncubic spline interpolation of the training data. We obtain a similar result for\ndifferent activation functions. For multivariate regression we show an\nanalogous result, whereby the second derivative is replaced by the Radon\ntransform of a fractional Laplacian. For initialization schemes that yield a\nconstant penalty function, the solutions are polyharmonic splines. Moreover, we\nshow that the training trajectories are captured by trajectories of smoothing\nsplines with decreasing regularization strength.",
          "link": "http://arxiv.org/abs/2006.07356",
          "publishedOn": "2021-07-27T02:03:36.459Z",
          "wordCount": 659,
          "title": "Implicit bias of gradient descent for mean squared error regression with wide neural networks. (arXiv:2006.07356v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects using\npoint cloud data. We present anchor design along with custom loss functions\nused in this work. A combination of spatial and channel wise attention module\nis used in this work. We use the Kitti 3D Birds Eye View dataset for\nbenchmarking and validating our results. Our method surpasses previous state of\nthe art in this domain both in terms of average precision and speed running at\n> 30 FPS. Finally, we present the ablation study to demonstrate that the\nperformance of our network is generalizable. This makes it a feasible option to\nbe deployed in real time applications like self driving cars.",
          "link": "http://arxiv.org/abs/2107.12137",
          "publishedOn": "2021-07-27T02:03:36.453Z",
          "wordCount": 605,
          "title": "AA3DNet: Attention Augmented Real Time 3D Object Detection. (arXiv:2107.12137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1\">Liang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1\">Hui Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhonghao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dewei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Ling Wang</a>",
          "description": "Price movement forecasting aims at predicting the future trends of financial\nassets based on the current market conditions and other relevant information.\nRecently, machine learning(ML) methods have become increasingly popular and\nachieved promising results for price movement forecasting in both academia and\nindustry. Most existing ML solutions formulate the forecasting problem as a\nclassification(to predict the direction) or a regression(to predict the return)\nproblem in the entire set of training data. However, due to the extremely low\nsignal-to-noise ratio and stochastic nature of financial data, good trading\nopportunities are extremely scarce. As a result, without careful selection of\npotentially profitable samples, such ML methods are prone to capture the\npatterns of noises instead of real signals. To address the above issues, we\npropose a novel framework-LARA(Locality-Aware Attention and Adaptive Refined\nLabeling), which contains the following three components: 1)Locality-aware\nattention automatically extracts the potentially profitable samples by\nattending to their label information in order to construct a more accurate\nclassifier on these selected samples. 2)Adaptive refined labeling further\niteratively refines the labels, alleviating the noise of samples. 3)Equipped\nwith metric learning techniques, Locality-aware attention enjoys task-specific\ndistance metrics and distributes attention on potentially profitable samples in\na more effective way. To validate our method, we conduct comprehensive\nexperiments on three real-world financial markets: ETFs, the China's A-share\nstock market, and the cryptocurrency market. LARA achieves superior performance\ncompared with the time-series analysis methods and a set of machine learning\nbased competitors on the Qlib platform. Extensive ablation studies and\nexperiments demonstrate that LARA indeed captures more reliable trading\nopportunities.",
          "link": "http://arxiv.org/abs/2107.11972",
          "publishedOn": "2021-07-27T02:03:36.447Z",
          "wordCount": 724,
          "title": "Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Adaptive Refined Labeling. (arXiv:2107.11972v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1905.11488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balghiti_O/0/1/0/all/0/1\">Othman El Balghiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmachtoub_A/0/1/0/all/0/1\">Adam N. Elmachtoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigas_P/0/1/0/all/0/1\">Paul Grigas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ambuj Tewari</a>",
          "description": "The predict-then-optimize framework is fundamental in many practical\nsettings: predict the unknown parameters of an optimization problem, and then\nsolve the problem using the predicted values of the parameters. A natural loss\nfunction in this environment is to consider the cost of the decisions induced\nby the predicted parameters, in contrast to the prediction error of the\nparameters. This loss function was recently introduced in Elmachtoub and Grigas\n(2017) and referred to as the Smart Predict-then-Optimize (SPO) loss. In this\nwork, we seek to provide bounds on how well the performance of a prediction\nmodel fit on training data generalizes out-of-sample, in the context of the SPO\nloss. Since the SPO loss is non-convex and non-Lipschitz, standard results for\nderiving generalization bounds do not apply.\n\nWe first derive bounds based on the Natarajan dimension that, in the case of\na polyhedral feasible region, scale at most logarithmically in the number of\nextreme points, but, in the case of a general convex feasible region, have\nlinear dependence on the decision dimension. By exploiting the structure of the\nSPO loss function and a key property of the feasible region, which we denote as\nthe strength property, we can dramatically improve the dependence on the\ndecision and feature dimensions. Our approach and analysis rely on placing a\nmargin around problematic predictions that do not yield unique optimal\nsolutions, and then providing generalization bounds in the context of a\nmodified margin SPO loss function that is Lipschitz continuous. Finally, we\ncharacterize the strength property and show that the modified SPO loss can be\ncomputed efficiently for both strongly convex bodies and polytopes with an\nexplicit extreme point representation.",
          "link": "http://arxiv.org/abs/1905.11488",
          "publishedOn": "2021-07-27T02:03:36.422Z",
          "wordCount": 745,
          "title": "Generalization Bounds in the Predict-then-Optimize Framework. (arXiv:1905.11488v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11876",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yen-Ju Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "Diffusion probabilistic models have demonstrated an outstanding capability to\nmodel natural images and raw audio waveforms through a paired diffusion and\nreverse processes. The unique property of the reverse process (namely,\neliminating non-target signals from the Gaussian noise and noisy signals) could\nbe utilized to restore clean signals. Based on this property, we propose a\ndiffusion probabilistic model-based speech enhancement (DiffuSE) model that\naims to recover clean speech signals from noisy signals. The fundamental\narchitecture of the proposed DiffuSE model is similar to that of DiffWave--a\nhigh-quality audio waveform generation model that has a relatively low\ncomputational cost and footprint. To attain better enhancement performance, we\ndesigned an advanced reverse process, termed the supportive reverse process,\nwhich adds noisy speech in each time-step to the predicted speech. The\nexperimental results show that DiffuSE yields performance that is comparable to\nrelated audio generative models on the standardized Voice Bank corpus SE task.\nMoreover, relative to the generally suggested full sampling schedule, the\nproposed supportive reverse process especially improved the fast sampling,\ntaking few steps to yield better enhancement results over the conventional full\nstep inference process.",
          "link": "http://arxiv.org/abs/2107.11876",
          "publishedOn": "2021-07-27T02:03:36.411Z",
          "wordCount": 637,
          "title": "A Study on Speech Enhancement Based on Diffusion Probabilistic Model. (arXiv:2107.11876v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenyi Zhang</a>",
          "description": "Decentralized federated learning (DFL) is a powerful framework of distributed\nmachine learning and decentralized stochastic gradient descent (SGD) is a\ndriving engine for DFL. The performance of decentralized SGD is jointly\ninfluenced by communication-efficiency and convergence rate. In this paper, we\npropose a general decentralized federated learning framework to strike a\nbalance between communication-efficiency and convergence performance. The\nproposed framework performs both multiple local updates and multiple inter-node\ncommunications periodically, unifying traditional decentralized SGD methods. We\nestablish strong convergence guarantees for the proposed DFL algorithm without\nthe assumption of convex objective function. The balance of communication and\ncomputation rounds is essential to optimize decentralized federated learning\nunder constrained communication and computation resources. For further\nimproving communication-efficiency of DFL, compressed communication is applied\nto DFL, named DFL with compressed communication (C-DFL). The proposed C-DFL\nexhibits linear convergence for strongly convex objectives. Experiment results\nbased on MNIST and CIFAR-10 datasets illustrate the superiority of DFL over\ntraditional decentralized SGD methods and show that C-DFL further enhances\ncommunication-efficiency.",
          "link": "http://arxiv.org/abs/2107.12048",
          "publishedOn": "2021-07-27T02:03:36.404Z",
          "wordCount": 600,
          "title": "Decentralized Federated Learning: Balancing Communication and Computing Costs. (arXiv:2107.12048v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1\">Moming Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Duo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xinyuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Renping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yujuan Tan</a>",
          "description": "Federated Learning (FL) enables the multiple participating devices to\ncollaboratively contribute to a global neural network model while keeping the\ntraining data locally. Unlike the centralized training setting, the non-IID and\nimbalanced (statistical heterogeneity) training data of FL is distributed in\nthe federated network, which will increase the divergences between the local\nmodels and global model, further degrading performance. In this paper, we\npropose a novel clustered federated learning (CFL) framework FedGroup, in which\nwe 1) group the training of clients based on the similarities between the\nclients' optimization directions for high training performance; 2) construct a\nnew data-driven distance measure to improve the efficiency of the client\nclustering procedure. 3) implement a newcomer device cold start mechanism based\non the auxiliary global model for framework scalability and practicality.\n\nFedGroup can achieve improvements by dividing joint optimization into groups\nof sub-optimization and can be combined with FL optimizer FedProx. The\nconvergence and complexity are analyzed to demonstrate the efficiency of our\nproposed framework. We also evaluate FedGroup and FedGrouProx (combined with\nFedProx) on several open datasets and made comparisons with related CFL\nframeworks. The results show that FedGroup can significantly improve absolute\ntest accuracy by +14.1% on FEMNIST compared to FedAvg. +3.4% on Sentiment140\ncompared to FedProx, +6.9% on MNIST compared to FeSEM.",
          "link": "http://arxiv.org/abs/2010.06870",
          "publishedOn": "2021-07-27T02:03:36.390Z",
          "wordCount": 740,
          "title": "FedGroup: Efficient Clustered Federated Learning via Decomposed Data-Driven Measure. (arXiv:2010.06870v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>",
          "description": "Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation and state-of-the-art\nlanguage technologies. We illustrate and evaluate the system using a COVID-19\nvaccine information case study.",
          "link": "http://arxiv.org/abs/2107.12079",
          "publishedOn": "2021-07-27T02:03:36.382Z",
          "wordCount": 541,
          "title": "An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1\">Lucie Charlotte Magister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1\">Dmitry Kazhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vikash Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>",
          "description": "While graph neural networks (GNNs) have been shown to perform well on\ngraph-based data from a variety of fields, they suffer from a lack of\ntransparency and accountability, which hinders trust and consequently the\ndeployment of such models in high-stake and safety-critical scenarios. Even\nthough recent research has investigated methods for explaining GNNs, these\nmethods are limited to single-instance explanations, also known as local\nexplanations. Motivated by the aim of providing global explanations, we adapt\nthe well-known Automated Concept-based Explanation approach (Ghorbani et al.,\n2019) to GNN node and graph classification, and propose GCExplainer.\nGCExplainer is an unsupervised approach for post-hoc discovery and extraction\nof global concept-based explanations for GNNs, which puts the human in the\nloop. We demonstrate the success of our technique on five node classification\ndatasets and two graph classification datasets, showing that we are able to\ndiscover and extract high-quality concept representations by putting the human\nin the loop. We achieve a maximum completeness score of 1 and an average\ncompleteness score of 0.753 across the datasets. Finally, we show that the\nconcept-based explanations provide an improved insight into the datasets and\nGNN models compared to the state-of-the-art explanations produced by\nGNNExplainer (Ying et al., 2019).",
          "link": "http://arxiv.org/abs/2107.11889",
          "publishedOn": "2021-07-27T02:03:36.357Z",
          "wordCount": 645,
          "title": "GCExplainer: Human-in-the-Loop Concept-based Explanations for Graph Neural Networks. (arXiv:2107.11889v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aftab_A/0/1/0/all/0/1\">Abdul Rafey Aftab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beeck_M/0/1/0/all/0/1\">Michael von der Beeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrhirsch_S/0/1/0/all/0/1\">Steven Rohrhirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diotte_B/0/1/0/all/0/1\">Benoit Diotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feld_M/0/1/0/all/0/1\">Michael Feld</a>",
          "description": "There is a growing interest in more intelligent natural user interaction with\nthe car. Hand gestures and speech are already being applied for driver-car\ninteraction. Moreover, multimodal approaches are also showing promise in the\nautomotive industry. In this paper, we utilize deep learning for a multimodal\nfusion network for referencing objects outside the vehicle. We use features\nfrom gaze, head pose and finger pointing simultaneously to precisely predict\nthe referenced objects in different car poses. We demonstrate the practical\nlimitations of each modality when used for a natural form of referencing,\nspecifically inside the car. As evident from our results, we overcome the\nmodality specific limitations, to a large extent, by the addition of other\nmodalities. This work highlights the importance of multimodal sensing,\nespecially when moving towards natural user interaction. Furthermore, our user\nbased analysis shows noteworthy differences in recognition of user behavior\ndepending upon the vehicle pose.",
          "link": "http://arxiv.org/abs/2107.12167",
          "publishedOn": "2021-07-27T02:03:36.350Z",
          "wordCount": 603,
          "title": "Multimodal Fusion Using Deep Learning Applied to Driver's Referencing of Outside-Vehicle Objects. (arXiv:2107.12167v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1\">Florian Tambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laberge_G/0/1/0/all/0/1\">Gabriel Laberge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Le An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1\">Amin Nikanjam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mindom_P/0/1/0/all/0/1\">Paulina Stevia Nouwou Mindom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pequignot_Y/0/1/0/all/0/1\">Yann Pequignot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1\">Giulio Antoniol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1\">Ettore Merlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laviolette_F/0/1/0/all/0/1\">Fran&#xe7;ois Laviolette</a>",
          "description": "Context: Machine Learning (ML) has been at the heart of many innovations over\nthe past years. However, including it in so-called 'safety-critical' systems\nsuch as automotive or aeronautic has proven to be very challenging, since the\nshift in paradigm that ML brings completely changes traditional certification\napproaches.\n\nObjective: This paper aims to elucidate challenges related to the\ncertification of ML-based safety-critical systems, as well as the solutions\nthat are proposed in the literature to tackle them, answering the question 'How\nto Certify Machine Learning Based Safety-critical Systems?'.\n\nMethod: We conduct a Systematic Literature Review (SLR) of research papers\npublished between 2015 to 2020, covering topics related to the certification of\nML systems. In total, we identified 229 papers covering topics considered to be\nthe main pillars of ML certification: Robustness, Uncertainty, Explainability,\nVerification, Safe Reinforcement Learning, and Direct Certification. We\nanalyzed the main trends and problems of each sub-field and provided summaries\nof the papers extracted.\n\nResults: The SLR results highlighted the enthusiasm of the community for this\nsubject, as well as the lack of diversity in terms of datasets and type of\nmodels. It also emphasized the need to further develop connections between\nacademia and industries to deepen the domain study. Finally, it also\nillustrated the necessity to build connections between the above mention main\npillars that are for now mainly studied separately.\n\nConclusion: We highlighted current efforts deployed to enable the\ncertification of ML based software systems, and discuss some future research\ndirections.",
          "link": "http://arxiv.org/abs/2107.12045",
          "publishedOn": "2021-07-27T02:03:36.344Z",
          "wordCount": 708,
          "title": "How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review. (arXiv:2107.12045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1\">Sagor Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Mehadi Hasan Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Kabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Azam Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1\">Stefan Decker</a>",
          "description": "The exponential growths of social media and micro-blogging sites not only\nprovide platforms for empowering freedom of expressions and individual voices,\nbut also enables people to express anti-social behavior like online harassment,\ncyberbullying, and hate speech. Numerous works have been proposed to utilize\ntextual data for social and anti-social behavior analysis, by predicting the\ncontexts mostly for highly-resourced languages like English. However, some\nlanguages are under-resourced, e.g., South Asian languages like Bengali, that\nlack computational resources for accurate natural language processing (NLP). In\nthis paper, we propose an explainable approach for hate speech detection from\nthe under-resourced Bengali language, which we called DeepHateExplainer.\nBengali texts are first comprehensively preprocessed, before classifying them\ninto political, personal, geopolitical, and religious hates using a neural\nensemble method of transformer-based neural architectures (i.e., monolingual\nBangla BERT-base, multilingual BERT-cased/uncased, and XLM-RoBERTa).\nImportant~(most and least) terms are then identified using sensitivity analysis\nand layer-wise relevance propagation~(LRP), before providing\nhuman-interpretable explanations. Finally, we compute comprehensiveness and\nsufficiency scores to measure the quality of explanations w.r.t faithfulness.\nEvaluations against machine learning~(linear and tree-based models) and neural\nnetworks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word embeddings) baselines\nyield F1-scores of 78%, 91%, 89%, and 84%, for political, personal,\ngeopolitical, and religious hates, respectively, outperforming both ML and DNN\nbaselines.",
          "link": "http://arxiv.org/abs/2012.14353",
          "publishedOn": "2021-07-27T02:03:36.331Z",
          "wordCount": 699,
          "title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1\">Yalin E. Sagduyu</a>",
          "description": "An over-the-air membership inference attack (MIA) is presented to leak\nprivate information from a wireless signal classifier. Machine learning (ML)\nprovides powerful means to classify wireless signals, e.g., for PHY-layer\nauthentication. As an adversarial machine learning attack, the MIA infers\nwhether a signal of interest has been used in the training data of a target\nclassifier. This private information incorporates waveform, channel, and device\ncharacteristics, and if leaked, can be exploited by an adversary to identify\nvulnerabilities of the underlying ML model (e.g., to infiltrate the PHY-layer\nauthentication). One challenge for the over-the-air MIA is that the received\nsignals and consequently the RF fingerprints at the adversary and the intended\nreceiver differ due to the discrepancy in channel conditions. Therefore, the\nadversary first builds a surrogate classifier by observing the spectrum and\nthen launches the black-box MIA on this classifier. The MIA results show that\nthe adversary can reliably infer signals (and potentially the radio and channel\ninformation) used to build the target classifier. Therefore, a proactive\ndefense is developed against the MIA by building a shadow MIA model and fooling\nthe adversary. This defense can successfully reduce the MIA accuracy and\nprevent information leakage from the wireless signal classifier.",
          "link": "http://arxiv.org/abs/2107.12173",
          "publishedOn": "2021-07-27T02:03:36.323Z",
          "wordCount": 656,
          "title": "Membership Inference Attack and Defense for Wireless Signal Classifiers with Deep Learning. (arXiv:2107.12173v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.08601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lerman_S/0/1/0/all/0/1\">Samuel Lerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venuto_C/0/1/0/all/0/1\">Charles Venuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_H/0/1/0/all/0/1\">Henry Kautz</a>",
          "description": "We present a simple yet highly generalizable method for explaining\ninteracting parts within a neural network's reasoning process. First, we design\nan algorithm based on cross derivatives for computing statistical interaction\neffects between individual features, which is generalized to both 2-way and\nhigher-order (3-way or more) interactions. We present results side by side with\na weight-based attribution technique, corroborating that cross derivatives are\na superior metric for both 2-way and higher-order interaction detection.\nMoreover, we extend the use of cross derivatives as an explanatory device in\nneural networks to the computer vision setting by expanding Grad-CAM, a popular\ngradient-based explanatory tool for CNNs, to the higher order. While Grad-CAM\ncan only explain the importance of individual objects in images, our method,\nwhich we call Taylor-CAM, can explain a neural network's relational reasoning\nacross multiple objects. We show the success of our explanations both\nqualitatively and quantitatively, including with a user study. We will release\nall code as a tool package to facilitate explainable deep learning.",
          "link": "http://arxiv.org/abs/2006.08601",
          "publishedOn": "2021-07-27T02:03:36.297Z",
          "wordCount": 643,
          "title": "Explaining Local, Global, And Higher-Order Interactions In Deep Learning. (arXiv:2006.08601v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12110",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Hunnefeld_M/0/1/0/all/0/1\">Mirco H&#xfc;nnefeld</a> (for the IceCube Collaboration)",
          "description": "The field of deep learning has become increasingly important for particle\nphysics experiments, yielding a multitude of advances, predominantly in event\nclassification and reconstruction tasks. Many of these applications have been\nadopted from other domains. However, data in the field of physics are unique in\nthe context of machine learning, insofar as their generation process and the\nlaws and symmetries they abide by are usually well understood. Most commonly\nused deep learning architectures fail at utilizing this available information.\nIn contrast, more traditional likelihood-based methods are capable of\nexploiting domain knowledge, but they are often limited by computational\ncomplexity. In this contribution, a hybrid approach is presented that utilizes\ngenerative neural networks to approximate the likelihood, which may then be\nused in a traditional maximum-likelihood setting. Domain knowledge, such as\ninvariances and detector characteristics, can easily be incorporated in this\napproach. The hybrid approach is illustrated by the example of event\nreconstruction in IceCube.",
          "link": "http://arxiv.org/abs/2107.12110",
          "publishedOn": "2021-07-27T02:03:36.290Z",
          "wordCount": 618,
          "title": "Combining Maximum-Likelihood with Deep Learning for Event Reconstruction in IceCube. (arXiv:2107.12110v1 [astro-ph.HE])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial examples contain carefully crafted perturbations that can fool\ndeep neural networks (DNNs) into making wrong predictions. Enhancing the\nadversarial robustness of DNNs has gained considerable interest in recent\nyears. Although image transformation-based defenses were widely considered at\nan earlier time, most of them have been defeated by adaptive attacks. In this\npaper, we propose a new image transformation defense based on error diffusion\nhalftoning, and combine it with adversarial training to defend against\nadversarial examples. Error diffusion halftoning projects an image into a 1-bit\nspace and diffuses quantization error to neighboring pixels. This process can\nremove adversarial perturbations from a given image while maintaining\nacceptable image quality in the meantime in favor of recognition. Experimental\nresults demonstrate that the proposed method is able to improve adversarial\nrobustness even under advanced adaptive attacks, while most of the other image\ntransformation-based defenses do not. We show that a proper image\ntransformation can still be an effective defense approach. Code:\nhttps://github.com/shaoyuanlo/Halftoning-Defense",
          "link": "http://arxiv.org/abs/2101.09451",
          "publishedOn": "2021-07-27T02:03:36.283Z",
          "wordCount": 654,
          "title": "Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhechun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1\">Theodoros Rekatsinas</a>",
          "description": "Data corruption is an impediment to modern machine learning deployments.\nCorrupted data can severely bias the learned model and can also lead to invalid\ninferences. We present, Picket, a simple framework to safeguard against data\ncorruptions during both training and deployment of machine learning models over\ntabular data. For the training stage, Picket identifies and removes corrupted\ndata points from the training data to avoid obtaining a biased model. For the\ndeployment stage, Picket flags, in an online manner, corrupted query points to\na trained machine learning model that due to noise will result in incorrect\npredictions. To detect corrupted data, Picket uses a self-supervised deep\nlearning model for mixed-type tabular data, which we call PicketNet. To\nminimize the burden of deployment, learning a PicketNet model does not require\nany human-labeled data. Picket is designed as a plugin that can increase the\nrobustness of any machine learning pipeline. We evaluate Picket on a diverse\narray of real-world data considering different corruption models that include\nsystematic and adversarial noise during both training and testing. We show that\nPicket consistently safeguards against corrupted data during both training and\ndeployment of various models ranging from SVMs to neural networks, beating a\ndiverse array of competing methods that span from data quality validation\nmodels to robust outlier-detection models.",
          "link": "http://arxiv.org/abs/2006.04730",
          "publishedOn": "2021-07-27T02:03:36.269Z",
          "wordCount": 704,
          "title": "Picket: Guarding Against Corrupted Data in Tabular Data during Learning and Inference. (arXiv:2006.04730v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xi Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaomei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weiwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xiaobo Lai</a>",
          "description": "Background: Glioma is the most common brain malignant tumor, with a high\nmorbidity rate and a mortality rate of more than three percent, which seriously\nendangers human health. The main method of acquiring brain tumors in the clinic\nis MRI. Segmentation of brain tumor regions from multi-modal MRI scan images is\nhelpful for treatment inspection, post-diagnosis monitoring, and effect\nevaluation of patients. However, the common operation in clinical brain tumor\nsegmentation is still manual segmentation, lead to its time-consuming and large\nperformance difference between different operators, a consistent and accurate\nautomatic segmentation method is urgently needed. Methods: To meet the above\nchallenges, we propose an automatic brain tumor MRI data segmentation framework\nwhich is called AGSE-VNet. In our study, the Squeeze and Excite (SE) module is\nadded to each encoder, the Attention Guide Filter (AG) module is added to each\ndecoder, using the channel relationship to automatically enhance the useful\ninformation in the channel to suppress the useless information, and use the\nattention mechanism to guide the edge information and remove the influence of\nirrelevant information such as noise. Results: We used the BraTS2020 challenge\nonline verification tool to evaluate our approach. The focus of verification is\nthat the Dice scores of the whole tumor (WT), tumor core (TC) and enhanced\ntumor (ET) are 0.68, 0.85 and 0.70, respectively. Conclusion: Although MRI\nimages have different intensities, AGSE-VNet is not affected by the size of the\ntumor, and can more accurately extract the features of the three regions, it\nhas achieved impressive results and made outstanding contributions to the\nclinical diagnosis and treatment of brain tumor patients.",
          "link": "http://arxiv.org/abs/2107.12046",
          "publishedOn": "2021-07-27T02:03:36.259Z",
          "wordCount": 725,
          "title": "3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework. (arXiv:2107.12046v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2008.02275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Critch_A/0/1/0/all/0/1\">Andrew Critch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "We show how to assess a language model's knowledge of basic concepts of\nmorality. We introduce the ETHICS dataset, a new benchmark that spans concepts\nin justice, well-being, duties, virtues, and commonsense morality. Models\npredict widespread moral judgments about diverse text scenarios. This requires\nconnecting physical and social world knowledge to value judgements, a\ncapability that may enable us to steer chatbot outputs or eventually regularize\nopen-ended reinforcement learning agents. With the ETHICS dataset, we find that\ncurrent language models have a promising but incomplete ability to predict\nbasic human ethical judgements. Our work shows that progress can be made on\nmachine ethics today, and it provides a steppingstone toward AI that is aligned\nwith human values.",
          "link": "http://arxiv.org/abs/2008.02275",
          "publishedOn": "2021-07-27T02:03:36.231Z",
          "wordCount": 634,
          "title": "Aligning AI With Shared Human Values. (arXiv:2008.02275v5 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.10136",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zatarain_Vera_O/0/1/0/all/0/1\">Oscar Zatarain-Vera</a>",
          "description": "Andreas Maurer in the paper \"A vector-contraction inequality for Rademacher\ncomplexities\" extended the contraction inequality for Rademacher averages to\nLipschitz functions with vector-valued domains; He did it replacing the\nRademacher variables in the bounding expression by arbitrary idd symmetric and\nsub-gaussian variables. We will see how to extend this work when we replace\nsub-gaussian variables by $p$-stable variables for $1<p<2$.",
          "link": "http://arxiv.org/abs/1912.10136",
          "publishedOn": "2021-07-27T02:03:36.225Z",
          "wordCount": 510,
          "title": "A vector-contraction inequality for Rademacher complexities using $p$-stable variables. (arXiv:1912.10136v2 [math.PR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12070",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tastan_A/0/1/0/all/0/1\">Aylin Tastan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muma_M/0/1/0/all/0/1\">Michael Muma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zoubir_A/0/1/0/all/0/1\">Abdelhak M. Zoubir</a>",
          "description": "The Fiedler vector of a connected graph is the eigenvector associated with\nthe algebraic connectivity of the graph Laplacian and it provides substantial\ninformation to learn the latent structure of a graph. In real-world\napplications, however, the data may be subject to heavy-tailed noise and\noutliers which results in deteriorations in the structure of the Fiedler vector\nestimate. We design a Robust Regularized Locality Preserving Indexing (RRLPI)\nmethod for Fiedler vector estimation that aims to approximate the nonlinear\nmanifold structure of the Laplace Beltrami operator while minimizing the\nnegative impact of outliers. First, an analysis of the effects of two\nfundamental outlier types on the eigen-decomposition for block affinity\nmatrices which are essential in cluster analysis is conducted. Then, an error\nmodel is formulated and a robust Fiedler vector estimation algorithm is\ndeveloped. An unsupervised penalty parameter selection algorithm is proposed\nthat leverages the geometric structure of the projection space to perform\nrobust regularized Fiedler estimation. The performance of RRLPI is benchmarked\nagainst existing competitors in terms of detection probability, partitioning\nquality, image segmentation capability, robustness and computation time using a\nlarge variety of synthetic and real data experiments.",
          "link": "http://arxiv.org/abs/2107.12070",
          "publishedOn": "2021-07-27T02:03:36.218Z",
          "wordCount": 628,
          "title": "Robust Regularized Locality Preserving Indexing for Fiedler Vector Estimation. (arXiv:2107.12070v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Janisch_J/0/1/0/all/0/1\">Jarom&#xed;r Janisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Pevn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lisy_V/0/1/0/all/0/1\">Viliam Lis&#xfd;</a>",
          "description": "We focus on reinforcement learning (RL) in relational problems that are\nnaturally defined in terms of objects, their relations, and manipulations.\nThese problems are characterized by variable state and action spaces, and\nfinding a fixed-length representation, required by most existing RL methods, is\ndifficult, if not impossible. We present a deep RL framework based on graph\nneural networks and auto-regressive policy decomposition that naturally works\nwith these problems and is completely domain-independent. We demonstrate the\nframework in three very distinct domains and we report the method's competitive\nperformance and impressive zero-shot generalization over different problem\nsizes. In goal-oriented BlockWorld, we demonstrate multi-parameter actions with\npre-conditions. In SysAdmin, we show how to select multiple objects\nsimultaneously. In the classical planning domain of Sokoban, the method trained\nexclusively on 10x10 problems with three boxes solves 89% of 15x15 problems\nwith five boxes.",
          "link": "http://arxiv.org/abs/2009.12462",
          "publishedOn": "2021-07-27T02:03:36.212Z",
          "wordCount": 623,
          "title": "Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks. (arXiv:2009.12462v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1\">Norman Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Frank Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorundo_E/0/1/0/all/0/1\">Evan Dorundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1\">Rahul Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tyler Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1\">Samyak Parajuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mike Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1\">Justin Gilmer</a>",
          "description": "We introduce four new real-world distribution shift datasets consisting of\nchanges in image style, image blurriness, geographic location, camera\noperation, and more. With our new datasets, we take stock of previously\nproposed methods for improving out-of-distribution robustness and put them to\nthe test. We find that using larger models and artificial data augmentations\ncan improve robustness on real-world distribution shifts, contrary to claims in\nprior work. We find improvements in artificial robustness benchmarks can\ntransfer to real-world distribution shifts, contrary to claims in prior work.\nMotivated by our observation that data augmentations can help with real-world\ndistribution shifts, we also introduce a new data augmentation method which\nadvances the state-of-the-art and outperforms models pretrained with 1000 times\nmore labeled data. Overall we find that some methods consistently help with\ndistribution shifts in texture and local image statistics, but these methods do\nnot help with some other distribution shifts like geographic changes. Our\nresults show that future research must study multiple distribution shifts\nsimultaneously, as we demonstrate that no evaluated method consistently\nimproves robustness.",
          "link": "http://arxiv.org/abs/2006.16241",
          "publishedOn": "2021-07-27T02:03:36.205Z",
          "wordCount": 694,
          "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. (arXiv:2006.16241v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03983",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Avrachenkov_K/0/1/0/all/0/1\">Konstantin Avrachenkov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek S. Borkar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Moharir_S/0/1/0/all/0/1\">Sharayu Moharir</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shah_S/0/1/0/all/0/1\">Suhail M. Shah</a>",
          "description": "We introduce a model of graph-constrained dynamic choice with reinforcement\nmodeled by positively $\\alpha$-homogeneous rewards. We show that its empirical\nprocess, which can be written as a stochastic approximation recursion with\nMarkov noise, has the same probability law as a certain vertex reinforced\nrandom walk. We use this equivalence to show that for $\\alpha > 0$, the\nasymptotic outcome concentrates around the optimum in a certain limiting sense\nwhen `annealed' by letting $\\alpha\\uparrow\\infty$ slowly.",
          "link": "http://arxiv.org/abs/2007.03983",
          "publishedOn": "2021-07-27T02:03:36.198Z",
          "wordCount": 537,
          "title": "Dynamic social learning under graph constraints. (arXiv:2007.03983v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12078",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhemchuzhnikov_D/0/1/0/all/0/1\">Dmitrii Zhemchuzhnikov</a> (DAO), <a href=\"http://arxiv.org/find/q-bio/1/au:+Igashov_I/0/1/0/all/0/1\">Ilia Igashov</a> (DAO), <a href=\"http://arxiv.org/find/q-bio/1/au:+Grudinin_S/0/1/0/all/0/1\">Sergei Grudinin</a> (DAO)",
          "description": "In this work, we introduce 6D Convolutional Neural Network (6DCNN) designed\nto tackle the problem of detecting relative positions and orientations of local\npatterns when processing three-dimensional volumetric data. 6DCNN also includes\nSE(3)-equivariant message-passing and nonlinear activation operations\nconstructed in the Fourier space. Working in the Fourier space allows\nsignificantly reducing the computational complexity of our operations. We\ndemonstrate the properties of the 6D convolution and its efficiency in the\nrecognition of spatial patterns. We also assess the 6DCNN model on several\ndatasets from the recent CASP protein structure prediction challenges. Here,\n6DCNN improves over the baseline architecture and also outperforms the state of\nthe art.",
          "link": "http://arxiv.org/abs/2107.12078",
          "publishedOn": "2021-07-27T02:03:36.173Z",
          "wordCount": 552,
          "title": "6DCNN with roto-translational convolution filters for volumetric data processing. (arXiv:2107.12078v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaohui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qi Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zidong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>",
          "description": "Policy gradient methods are appealing in deep reinforcement learning but\nsuffer from high variance of gradient estimate. To reduce the variance, the\nstate value function is applied commonly. However, the effect of the state\nvalue function becomes limited in stochastic dynamic environments, where the\nunexpected state dynamics and rewards will increase the variance. In this\npaper, we propose to replace the state value function with a novel hindsight\nvalue function, which leverages the information from the future to reduce the\nvariance of the gradient estimate for stochastic dynamic environments.\n\nParticularly, to obtain an ideally unbiased gradient estimate, we propose an\ninformation-theoretic approach, which optimizes the embeddings of the future to\nbe independent of previous actions. In our experiments, we apply the proposed\nhindsight value function in stochastic dynamic environments, including\ndiscrete-action environments and continuous-action environments. Compared with\nthe standard state value function, the proposed hindsight value function\nconsistently reduces the variance, stabilizes the training, and improves the\neventual policy.",
          "link": "http://arxiv.org/abs/2107.12216",
          "publishedOn": "2021-07-27T02:03:36.167Z",
          "wordCount": 607,
          "title": "Hindsight Value Function for Variance Reduction in Stochastic Dynamic Environment. (arXiv:2107.12216v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fraboni_Y/0/1/0/all/0/1\">Yann Fraboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Richard Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kameni_L/0/1/0/all/0/1\">Laetitia Kameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>",
          "description": "While clients' sampling is a central operation of current state-of-the-art\nfederated learning (FL) approaches, the impact of this procedure on the\nconvergence and speed of FL remains to date under-investigated. In this work we\nintroduce a novel decomposition theorem for the convergence of FL, allowing to\nclearly quantify the impact of client sampling on the global model update.\nContrarily to previous convergence analyses, our theorem provides the exact\ndecomposition of a given convergence step, thus enabling accurate\nconsiderations about the role of client sampling and heterogeneity. First, we\nprovide a theoretical ground for previously reported results on the\nrelationship between FL convergence and the variance of the aggregation\nweights. Second, we prove for the first time that the quality of FL convergence\nis also impacted by the resulting covariance between aggregation weights.\nThird, we establish that the sum of the aggregation weights is another source\nof slow-down and should be equal to 1 to improve FL convergence speed. Our\ntheory is general, and is here applied to Multinomial Distribution (MD) and\nUniform sampling, the two default client sampling in FL, and demonstrated\nthrough a series of experiments in non-iid and unbalanced scenarios. Our\nresults suggest that MD sampling should be used as default sampling scheme, due\nto the resilience to the changes in data ratio during the learning process,\nwhile Uniform sampling is superior only in the special case when clients have\nthe same amount of data.",
          "link": "http://arxiv.org/abs/2107.12211",
          "publishedOn": "2021-07-27T02:03:36.161Z",
          "wordCount": 682,
          "title": "On The Impact of Client Sampling on Federated Learning Convergence. (arXiv:2107.12211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brabec_J/0/1/0/all/0/1\">Jan Brabec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machlica_L/0/1/0/all/0/1\">Lukas Machlica</a>",
          "description": "In this paper, Bayesian based aggregation of decision trees in an ensemble\n(decision forest) is investigated. The focus is laid on multi-class\nclassification with number of samples significantly skewed toward one of the\nclasses. The algorithm leverages out-of-bag datasets to estimate prediction\nerrors of individual trees, which are then used in accordance with the Bayes\nrule to refine the decision of the ensemble. The algorithm takes prevalence of\nindividual classes into account and does not require setting of any additional\nparameters related to class weights or decision-score thresholds. Evaluation is\nbased on publicly available datasets as well as on an proprietary dataset\ncomprising network traffic telemetry from hundreds of enterprise networks with\nover a million of users overall. The aim is to increase the detection\ncapabilities of an operating malware detection system. While we were able to\nkeep precision of the system higher than 94\\%, that is only 6 out of 100\ndetections shown to the network administrator are false alarms, we were able to\nachieve increase of approximately 7\\% in the number of detections. The\nalgorithm effectively handles large amounts of data, and can be used in\nconjunction with most of the state-of-the-art algorithms used to train decision\nforests.",
          "link": "http://arxiv.org/abs/2107.11862",
          "publishedOn": "2021-07-27T02:03:36.131Z",
          "wordCount": 710,
          "title": "Decision-forest voting scheme for classification of rare classes in network intrusion detection. (arXiv:2107.11862v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12156",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Mishra_A/0/1/0/all/0/1\">Akshansh Mishra</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Dixit_D/0/1/0/all/0/1\">Devarrishi Dixit</a>",
          "description": "Advent in machine learning is leaving a deep impact on various sectors\nincluding the material science domain. The present paper highlights the\napplication of various supervised machine learning regression algorithms such\nas polynomial regression, decision tree regression algorithm, random forest\nalgorithm, support vector regression algorithm, and artificial neural network\nalgorithm to determine the thin film thickness of Polystyrene on the glass\nsubstrates. The results showed that the polynomial regression machine learning\nalgorithm outperforms all other machine learning models by yielding the\ncoefficient of determination of 0.96 approximately and mean square error of\n0.04 respectively.",
          "link": "http://arxiv.org/abs/2107.12156",
          "publishedOn": "2021-07-27T02:03:36.125Z",
          "wordCount": 543,
          "title": "Brain Inspired Computing Approach for the Optimization of the Thin Film Thickness of Polystyrene on the Glass Substrates. (arXiv:2107.12156v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ohmichi_Y/0/1/0/all/0/1\">Yuya Ohmichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugioka_Y/0/1/0/all/0/1\">Yosuke Sugioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakakita_K/0/1/0/all/0/1\">Kazuyuki Nakakita</a>",
          "description": "In this study, we proposed the truncated total least squares dynamic mode\ndecomposition (T-TLS DMD) algorithm, which can perform DMD analysis of noisy\ndata. By adding truncation regularization to the conventional TLS DMD\nalgorithm, T-TLS DMD improves the stability of the computation while\nmaintaining the accuracy of TLS DMD. The effectiveness of the proposed method\nwas evaluated by the analysis of the wake behind a cylinder and\npressure-sensitive paint (PSP) data for the buffet cell phenomenon. The results\nshowed the importance of regularization in the DMD algorithm. With respect to\nthe eigenvalues, T-TLS DMD was less affected by noise, and accurate eigenvalues\ncould be obtained stably, whereas the eigenvalues of TLS and subspace DMD\nvaried greatly due to noise. It was also observed that the eigenvalues of the\nstandard and exact DMD had the problem of shifting to the damping side, as\nreported in previous studies. With respect to eigenvectors, T-TLS and exact DMD\ncaptured the characteristic flow patterns clearly even in the presence of\nnoise, whereas TLS and subspace DMD were not able to capture them clearly due\nto noise.",
          "link": "http://arxiv.org/abs/2107.11999",
          "publishedOn": "2021-07-27T02:03:36.118Z",
          "wordCount": 624,
          "title": "Stable Dynamic Mode Decomposition Algorithm for Noisy Pressure-Sensitive Paint Measurement Data. (arXiv:2107.11999v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jicong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Yiheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingbo Zhao</a>",
          "description": "The performance of spectral clustering heavily relies on the quality of\naffinity matrix. A variety of affinity-matrix-construction methods have been\nproposed but they have hyper-parameters to determine beforehand, which requires\nstrong experience and lead to difficulty in real applications especially when\nthe inter-cluster similarity is high or/and the dataset is large. On the other\nhand, we often have to determine to use a linear model or a nonlinear model,\nwhich still depends on experience. To solve these two problems, in this paper,\nwe present an eigen-gap guided search method for subspace clustering. The main\nidea is to find the most reliable affinity matrix among a set of candidates\nconstructed by linear and kernel regressions, where the reliability is\nquantified by the \\textit{relative-eigen-gap} of graph Laplacian defined in\nthis paper. We show, theoretically and numerically, that the Laplacian matrix\nwith a larger relative-eigen-gap often yields a higher clustering accuracy and\nstability. Our method is able to automatically search the best model and\nhyper-parameters in a pre-defined space. The search space is very easy to\ndetermine and can be arbitrarily large, though a relatively compact search\nspace can reduce the highly unnecessary computation. Our method has high\nflexibility and convenience in real applications, and also has low\ncomputational cost because the affinity matrix is not computed by iterative\noptimization. We extend the method to large-scale datasets such as MNIST, on\nwhich the time cost is less than 90s and the clustering accuracy is\nstate-of-the-art. Extensive experiments of natural image clustering show that\nour method is more stable, accurate, and efficient than baseline methods.",
          "link": "http://arxiv.org/abs/2107.12183",
          "publishedOn": "2021-07-27T02:03:36.111Z",
          "wordCount": 696,
          "title": "EGGS: Eigen-Gap Guided Search\\\\ Making Subspace Clustering Easy. (arXiv:2107.12183v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molitor_D/0/1/0/all/0/1\">Dirk Alexander Molitor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubik_C/0/1/0/all/0/1\">Christian Kubik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hetfleisch_R/0/1/0/all/0/1\">Ruben Helmut Hetfleisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groche_P/0/1/0/all/0/1\">Peter Groche</a>",
          "description": "Blanking processes belong to the most widely used manufacturing techniques\ndue to their economic efficiency. Their economic viability depends to a large\nextent on the resulting product quality and the associated customer\nsatisfaction as well as on possible downtimes. In particular, the occurrence of\nincreased tool wear reduces the product quality and leads to downtimes, which\nis why considerable research has been carried out in recent years with regard\nto wear detection. While processes have widely been monitored based on force\nand acceleration signals, a new approach is pursued in this paper. Blanked\nworkpieces manufactured by punches with 16 different wear states are\nphotographed and then used as inputs for Deep Convolutional Neural Networks to\nclassify wear states. The results show that wear states can be predicted with\nsurprisingly high accuracy, opening up new possibilities and research\nopportunities for tool wear monitoring of blanking processes.",
          "link": "http://arxiv.org/abs/2107.12034",
          "publishedOn": "2021-07-27T02:03:36.096Z",
          "wordCount": 600,
          "title": "Workpiece Image-based Tool Wear Classification in Blanking Processes Using Deep Convolutional Neural Networks. (arXiv:2107.12034v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gote_C/0/1/0/all/0/1\">Christoph Gote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perri_V/0/1/0/all/0/1\">Vincenzo Perri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholtes_I/0/1/0/all/0/1\">Ingo Scholtes</a>",
          "description": "Networks are frequently used to model complex systems comprised of\ninteracting elements. While links capture the topology of direct interactions,\nthe true complexity of many systems originates from higher-order patterns in\npaths by which nodes can indirectly influence each other. Path data,\nrepresenting ordered sequences of consecutive direct interactions, can be used\nto model these patterns. However, to avoid overfitting, such models should only\nconsider those higher-order patterns for which the data provide sufficient\nstatistical evidence. On the other hand, we hypothesise that network models,\nwhich capture only direct interactions, underfit higher-order patterns present\nin data. Consequently, both approaches are likely to misidentify influential\nnodes in complex networks. We contribute to this issue by proposing eight\ncentrality measures based on MOGen, a multi-order generative model that\naccounts for all paths up to a maximum distance but disregards paths at higher\ndistances. We compare MOGen-based centralities to equivalent measures for\nnetwork models and path data in a prediction experiment where we aim to\nidentify influential nodes in out-of-sample data. Our results show strong\nevidence supporting our hypothesis. MOGen consistently outperforms both the\nnetwork model and path-based prediction. We further show that the performance\ndifference between MOGen and the path-based approach disappears if we have\nsufficient observations, confirming that the error is due to overfitting.",
          "link": "http://arxiv.org/abs/2107.12100",
          "publishedOn": "2021-07-27T02:03:36.090Z",
          "wordCount": 677,
          "title": "Predicting Influential Higher-Order Patterns in Temporal Network Data. (arXiv:2107.12100v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1\">Rujing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_O/0/1/0/all/0/1\">Ou Wu</a>",
          "description": "Weighting strategy prevails in machine learning. For example, a common\napproach in robust machine learning is to exert lower weights on samples which\nare likely to be noisy or hard. This study reveals another undiscovered\nstrategy, namely, compensating, that has also been widely used in machine\nlearning. Learning with compensating is called compensation learning and a\nsystematic taxonomy is constructed for it in this study. In our taxonomy,\ncompensation learning is divided on the basis of the compensation targets,\ninference manners, and granularity levels. Many existing learning algorithms\nincluding some classical ones can be seen as a special case of compensation\nlearning or partially leveraging compensating. Furthermore, a family of new\nlearning algorithms can be obtained by plugging the compensation learning into\nexisting learning algorithms. Specifically, three concrete new learning\nalgorithms are proposed for robust machine learning. Extensive experiments on\ntext sentiment analysis, image classification, and graph classification verify\nthe effectiveness of the three new algorithms. Compensation learning can also\nbe used in various learning scenarios, such as imbalance learning, clustering,\nregression, and so on.",
          "link": "http://arxiv.org/abs/2107.11921",
          "publishedOn": "2021-07-27T02:03:36.083Z",
          "wordCount": 591,
          "title": "Compensation Learning. (arXiv:2107.11921v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenhai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>",
          "description": "We describe an efficient hierarchical method to compute attention in the\nTransformer architecture. The proposed attention mechanism exploits a matrix\nstructure similar to the Hierarchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear run time and memory complexity. We\nperform extensive experiments to show that the inductive bias embodied by our\nhierarchical attention is effective in capturing the hierarchical structure in\nthe sequences typical for natural language and vision tasks. Our method is\nsuperior to alternative sub-quadratic proposals by over +6 points on average on\nthe Long Range Arena benchmark. It also sets a new SOTA test perplexity on\nOne-Billion Word dataset with 5x fewer model parameters than that of the\nprevious-best Transformer-based models.",
          "link": "http://arxiv.org/abs/2107.11906",
          "publishedOn": "2021-07-27T02:03:35.956Z",
          "wordCount": 552,
          "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. (arXiv:2107.11906v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12013",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lai_M/0/1/0/all/0/1\">Ming-Chih Lai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chang_C/0/1/0/all/0/1\">Che-Chia Chang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lin_W/0/1/0/all/0/1\">Wei-Syuan Lin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hu_W/0/1/0/all/0/1\">Wei-Fan Hu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1\">Te-Sheng Lin</a>",
          "description": "In this paper, a shallow Ritz-type neural network for solving elliptic\nproblems with delta function singular sources on an interface is developed.\nThere are three novel features in the present work; namely, (i) the delta\nfunction singularity is naturally removed, (ii) level set function is\nintroduced as a feather input, (iii) it is completely shallow consisting of\nonly one hidden layer. We first introduce the energy functional of the problem\nand then transform the contribution of singular sources to a regular surface\nintegral along the interface. In such a way the delta function singularity can\nbe naturally removed without the introduction of discrete delta function that\nis commonly used in traditional regularization methods such as the well-known\nimmersed boundary method. The original problem is then reformulated as a\nminimization problem. We propose a shallow Ritz-type neural network with one\nhidden layer to approximate the global minimizer of the energy functional. As a\nresult, the network is trained by minimizing the loss function that is a\ndiscrete version of the energy. In addition, we include the level set function\nof the interface as a feature input and find that it significantly improves the\ntraining efficiency and accuracy. We perform a series of numerical tests to\ndemonstrate the accuracy of the present network as well as its capability for\nproblems in irregular domains and in higher dimensions.",
          "link": "http://arxiv.org/abs/2107.12013",
          "publishedOn": "2021-07-27T02:03:35.948Z",
          "wordCount": 666,
          "title": "A Shallow Ritz Method for elliptic problems with Singular Sources. (arXiv:2107.12013v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mardia_J/0/1/0/all/0/1\">Jay Mardia</a>",
          "description": "The planted clique problem is well-studied in the context of observing,\nexplaining, and predicting interesting computational phenomena associated with\nstatistical problems. When equating computational efficiency with the existence\nof polynomial time algorithms, the computational hardness of (some variant of)\nthe planted clique problem can be used to infer the computational hardness of a\nhost of other statistical problems.\n\nIs this ability to transfer computational hardness from (some variant of) the\nplanted clique problem to other statistical problems robust to changing our\nnotion of computational efficiency to space efficiency?\n\nWe answer this question affirmatively for three different statistical\nproblems, namely Sparse PCA, submatrix detection, and testing almost k-wise\nindependence. The key challenge is that space efficient randomized reductions\nneed to repeatedly access the randomness they use. Known reductions to these\nproblems are all randomized and need polynomially many random bits to\nimplement. Since we can not store polynomially many random bits in memory, it\nis unclear how to implement these existing reductions space efficiently. There\nare two ideas involved in circumventing this issue and implementing known\nreductions to these problems space efficiently.\n\n1. When solving statistical problems, we can use parts of the input itself as\nrandomness.\n\n2. Secret leakage variants of the planted clique problem with appropriate\nsecret leakage can be more useful than the standard planted clique problem when\nwe want to use parts of the input as randomness.\n\n(abstract shortened due to arxiv constraints)",
          "link": "http://arxiv.org/abs/2107.11886",
          "publishedOn": "2021-07-27T02:03:35.926Z",
          "wordCount": 675,
          "title": "Logspace Reducibility From Secret Leakage Planted Clique. (arXiv:2107.11886v1 [cs.CC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12009",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cahan_N/0/1/0/all/0/1\">Noa Cahan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marom_E/0/1/0/all/0/1\">Edith M. Marom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soffer_S/0/1/0/all/0/1\">Shelly Soffer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barash_Y/0/1/0/all/0/1\">Yiftach Barash</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konen_E/0/1/0/all/0/1\">Eli Konen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klang_E/0/1/0/all/0/1\">Eyal Klang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greenspan_H/0/1/0/all/0/1\">Hayit Greenspan</a>",
          "description": "Pulmonary embolus (PE) refers to obstruction of pulmonary arteries by blood\nclots. PE accounts for approximately 100,000 deaths per year in the United\nStates alone. The clinical presentation of PE is often nonspecific, making the\ndiagnosis challenging. Thus, rapid and accurate risk stratification is of\nparamount importance. High-risk PE is caused by right ventricular (RV)\ndysfunction from acute pressure overload, which in return can help identify\nwhich patients require more aggressive therapy. Reconstructed four-chamber\nviews of the heart on chest CT can detect right ventricular enlargement. CT\npulmonary angiography (CTPA) is the golden standard in the diagnostic workup of\nsuspected PE. Therefore, it can link between diagnosis and risk stratification\nstrategies. We developed a weakly supervised deep learning algorithm, with an\nemphasis on a novel attention mechanism, to automatically classify RV strain on\nCTPA. Our method is a 3D DenseNet model with integrated 3D residual attention\nblocks. We evaluated our model on a dataset of CTPAs of emergency department\n(ED) PE patients. This model achieved an area under the receiver operating\ncharacteristic curve (AUC) of 0.88 for classifying RV strain. The model showed\na sensitivity of 87% and specificity of 83.7%. Our solution outperforms\nstate-of-the-art 3D CNN networks. The proposed design allows for a fully\nautomated network that can be trained easily in an end-to-end manner without\nrequiring computationally intensive and time-consuming preprocessing or\nstrenuous labeling of the data.We infer that unmarked CTPAs can be used for\neffective RV strain classification. This could be used as a second reader,\nalerting for high-risk PE patients. To the best of our knowledge, there are no\nprevious deep learning-based studies that attempted to solve this problem.",
          "link": "http://arxiv.org/abs/2107.12009",
          "publishedOn": "2021-07-27T02:03:35.918Z",
          "wordCount": 751,
          "title": "Weakly Supervised Attention Model for RV StrainClassification from volumetric CTPA Scans. (arXiv:2107.12009v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11856",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Amor_A/0/1/0/all/0/1\">Amine Amor</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Lio&#x27;</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Singh_V/0/1/0/all/0/1\">Vikash Singh</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Torne_R/0/1/0/all/0/1\">Ramon Vi&#xf1;as Torn&#xe9;</a> (1), <a href=\"http://arxiv.org/find/q-bio/1/au:+Terre_H/0/1/0/all/0/1\">Helena Andres Terre</a> (1)",
          "description": "Combining different modalities of data from human tissues has been critical\nin advancing biomedical research and personalised medical care. In this study,\nwe leverage a graph embedding model (i.e VGAE) to perform link prediction on\ntissue-specific Gene-Gene Interaction (GGI) networks. Through ablation\nexperiments, we prove that the combination of multiple biological modalities\n(i.e multi-omics) leads to powerful embeddings and better link prediction\nperformances. Our evaluation shows that the integration of gene methylation\nprofiles and RNA-sequencing data significantly improves the link prediction\nperformance. Overall, the combination of RNA-sequencing and gene methylation\ndata leads to a link prediction accuracy of 71% on GGI networks. By harnessing\ngraph representation learning on multi-omics data, our work brings novel\ninsights to the current literature on multi-omics integration in\nbioinformatics.",
          "link": "http://arxiv.org/abs/2107.11856",
          "publishedOn": "2021-07-27T02:03:35.911Z",
          "wordCount": 578,
          "title": "Graph Representation Learning on Tissue-Specific Multi-Omics. (arXiv:2107.11856v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">David D. Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agha_mohammadi_A/0/1/0/all/0/1\">Ali-akbar Agha-mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1\">Evangelos A. Theodorou</a>",
          "description": "One of the main challenges in autonomous robotic exploration and navigation\nin unknown and unstructured environments is determining where the robot can or\ncannot safely move. A significant source of difficulty in this determination\narises from stochasticity and uncertainty, coming from localization error,\nsensor sparsity and noise, difficult-to-model robot-ground interactions, and\ndisturbances to the motion of the vehicle. Classical approaches to this problem\nrely on geometric analysis of the surrounding terrain, which can be prone to\nmodeling errors and can be computationally expensive. Moreover, modeling the\ndistribution of uncertain traversability costs is a difficult task, compounded\nby the various error sources mentioned above. In this work, we take a\nprincipled learning approach to this problem. We introduce a neural network\narchitecture for robustly learning the distribution of traversability costs.\nBecause we are motivated by preserving the life of the robot, we tackle this\nlearning problem from the perspective of learning tail-risks, i.e. the\nConditional Value-at-Risk (CVaR). We show that this approach reliably learns\nthe expected tail risk given a desired probability risk threshold between 0 and\n1, producing a traversability costmap which is more robust to outliers, more\naccurately captures tail risks, and is more computationally efficient, when\ncompared against baselines. We validate our method on data collected a legged\nrobot navigating challenging, unstructured environments including an abandoned\nsubway, limestone caves, and lava tube caves.",
          "link": "http://arxiv.org/abs/2107.11722",
          "publishedOn": "2021-07-27T02:03:35.905Z",
          "wordCount": 662,
          "title": "Learning Risk-aware Costmaps for Traversability in Challenging Environments. (arXiv:2107.11722v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_F/0/1/0/all/0/1\">Frederik Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_C/0/1/0/all/0/1\">Christopher Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabe_M/0/1/0/all/0/1\">Markus N. Rabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkbeiner_B/0/1/0/all/0/1\">Bernd Finkbeiner</a>",
          "description": "We train hierarchical Transformers on the task of synthesizing hardware\ncircuits directly out of high-level logical specifications in linear-time\ntemporal logic (LTL). The LTL synthesis problem is a well-known algorithmic\nchallenge with a long history and an annual competition is organized to track\nthe improvement of algorithms and tooling over time. New approaches using\nmachine learning might open a lot of possibilities in this area, but suffer\nfrom the lack of sufficient amounts of training data. In this paper, we\nconsider a method to generate large amounts of additional training data, i.e.,\npairs of specifications and circuits implementing them. We ensure that this\nsynthetic data is sufficiently close to human-written specifications by mining\ncommon patterns from the specifications used in the synthesis competitions. We\nshow that hierarchical Transformers trained on this synthetic data solve a\nsignificant portion of problems from the synthesis competitions, and even\nout-of-distribution examples from a recent case study.",
          "link": "http://arxiv.org/abs/2107.11864",
          "publishedOn": "2021-07-27T02:03:35.870Z",
          "wordCount": 585,
          "title": "Neural Circuit Synthesis from Specification Patterns. (arXiv:2107.11864v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toivonen_T/0/1/0/all/0/1\">Tapani Toivonen</a>",
          "description": "Many combinatorial optimization problems are often considered intractable to\nsolve exactly or by approximation. An example of such problem is maximum clique\nwhich -- under standard assumptions in complexity theory -- cannot be solved in\nsub-exponential time or be approximated within polynomial factor efficiently.\nWe show that if a polynomial time algorithm can query informative Gaussian\npriors from an expert $poly(n)$ times, then a class of combinatorial\noptimization problems can be solved efficiently in expectation up to a\nmultiplicative factor $\\epsilon$ where $\\epsilon$ is arbitrary constant. While\nour proposed methods are merely theoretical, they cast new light on how to\napproach solving these problems that have been usually considered intractable.",
          "link": "http://arxiv.org/abs/2107.11784",
          "publishedOn": "2021-07-27T02:03:35.863Z",
          "wordCount": 551,
          "title": "Power of human-algorithm collaboration in solving combinatorial optimization problems. (arXiv:2107.11784v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Arnab Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1\">Sutanu Gayen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandasamy_S/0/1/0/all/0/1\">Saravanan Kandasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raval_V/0/1/0/all/0/1\">Vedant Raval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinodchandran_N/0/1/0/all/0/1\">N. V. Vinodchandran</a>",
          "description": "We consider the problem of efficiently inferring interventional distributions\nin a causal Bayesian network from a finite number of observations. Let\n$\\mathcal{P}$ be a causal model on a set $\\mathbf{V}$ of observable variables\non a given causal graph $G$. For sets $\\mathbf{X},\\mathbf{Y}\\subseteq\n\\mathbf{V}$, and setting ${\\bf x}$ to $\\mathbf{X}$, let $P_{\\bf x}(\\mathbf{Y})$\ndenote the interventional distribution on $\\mathbf{Y}$ with respect to an\nintervention ${\\bf x}$ to variables ${\\bf x}$. Shpitser and Pearl (AAAI 2006),\nbuilding on the work of Tian and Pearl (AAAI 2001), gave an exact\ncharacterization of the class of causal graphs for which the interventional\ndistribution $P_{\\bf x}({\\mathbf{Y}})$ can be uniquely determined. We give the\nfirst efficient version of the Shpitser-Pearl algorithm. In particular, under\nnatural assumptions, we give a polynomial-time algorithm that on input a causal\ngraph $G$ on observable variables $\\mathbf{V}$, a setting ${\\bf x}$ of a set\n$\\mathbf{X} \\subseteq \\mathbf{V}$ of bounded size, outputs succinct\ndescriptions of both an evaluator and a generator for a distribution $\\hat{P}$\nthat is $\\varepsilon$-close (in total variation distance) to $P_{\\bf\nx}({\\mathbf{Y}})$ where $Y=\\mathbf{V}\\setminus \\mathbf{X}$, if $P_{\\bf\nx}(\\mathbf{Y})$ is identifiable. We also show that when $\\mathbf{Y}$ is an\narbitrary set, there is no efficient algorithm that outputs an evaluator of a\ndistribution that is $\\varepsilon$-close to $P_{\\bf x}({\\mathbf{Y}})$ unless\nall problems that have statistical zero-knowledge proofs, including the Graph\nIsomorphism problem, have efficient randomized algorithms.",
          "link": "http://arxiv.org/abs/2107.11712",
          "publishedOn": "2021-07-27T02:03:35.856Z",
          "wordCount": 671,
          "title": "Efficient inference of interventional distributions. (arXiv:2107.11712v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1\">P Preethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1\">Hrishikesh Viswanath</a>",
          "description": "This work presents a comparison of machine learning algorithms that are\nimplemented to segment the characters of text presented as an image. The\nalgorithms are designed to work on degraded documents with text that is not\naligned in an organized fashion. The paper investigates the use of Support\nVector Machines, K-Nearest Neighbor algorithm and an Encoder Network to perform\nthe operation of character spotting. Character Spotting involves extracting\npotential characters from a stream of text by selecting regions bound by white\nspace.",
          "link": "http://arxiv.org/abs/2107.11795",
          "publishedOn": "2021-07-27T02:03:35.850Z",
          "wordCount": 518,
          "title": "Character Spotting Using Machine Learning Techniques. (arXiv:2107.11795v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11911",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Frazier_P/0/1/0/all/0/1\">Peter I. Frazier</a>",
          "description": "We consider finite-horizon restless bandits with multiple pulls per period,\nwhich play an important role in recommender systems, active learning, revenue\nmanagement, and many other areas. While an optimal policy can be computed, in\nprinciple, using dynamic programming, the computation required scales\nexponentially in the number of arms $N$. Thus, there is substantial value in\nunderstanding the performance of index policies and other policies that can be\ncomputed efficiently for large $N$. We study the growth of the optimality gap,\ni.e., the loss in expected performance compared to an optimal policy, for such\npolicies in a classical asymptotic regime proposed by Whittle in which $N$\ngrows while holding constant the fraction of arms that can be pulled per\nperiod. Intuition from the Central Limit Theorem and the tightest previous\ntheoretical bounds suggest that this optimality gap should grow like\n$O(\\sqrt{N})$. Surprisingly, we show that it is possible to outperform this\nbound. We characterize a non-degeneracy condition and a wide class of novel\npractically-computable policies, called fluid-priority policies, in which the\noptimality gap is $O(1)$. These include most widely-used index policies. When\nthis non-degeneracy condition does not hold, we show that fluid-priority\npolicies nevertheless have an optimality gap that is $O(\\sqrt{N})$,\nsignificantly generalizing the class of policies for which convergence rates\nare known. We demonstrate that fluid-priority policies offer state-of-the-art\nperformance on a collection of restless bandit problems in numerical\nexperiments.",
          "link": "http://arxiv.org/abs/2107.11911",
          "publishedOn": "2021-07-27T02:03:35.843Z",
          "wordCount": 669,
          "title": "Restless Bandits with Many Arms: Beating the Central Limit Theorem. (arXiv:2107.11911v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ogishima_R/0/1/0/all/0/1\">Ryoya Ogishima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karino_I/0/1/0/all/0/1\">Izumi Karino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1\">Yasuo Kuniyoshi</a>",
          "description": "Reinforcement Learning (RL) requires a large amount of exploration especially\nin sparse-reward settings. Imitation Learning (IL) can learn from expert\ndemonstrations without exploration, but it never exceeds the expert's\nperformance and is also vulnerable to distributional shift between\ndemonstration and execution. In this paper, we radically unify RL and IL based\non Free Energy Principle (FEP). FEP is a unified Bayesian theory of the brain\nthat explains perception, action and model learning by a common fundamental\nprinciple. We present a theoretical extension of FEP and derive an algorithm in\nwhich an agent learns the world model that internalizes expert demonstrations\nand at the same time uses the model to infer the current and future states and\nactions that maximize rewards. The algorithm thus reduces exploration costs by\npartially imitating experts as well as maximizing its return in a seamless way,\nresulting in a higher performance than the suboptimal expert. Our experimental\nresults show that this approach is promising in visual control tasks especially\nin sparse-reward environments.",
          "link": "http://arxiv.org/abs/2107.11811",
          "publishedOn": "2021-07-27T02:03:35.815Z",
          "wordCount": 595,
          "title": "Reinforced Imitation Learning by Free Energy Principle. (arXiv:2107.11811v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11882",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kaiwen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Ho Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deppen_S/0/1/0/all/0/1\">Steve Deppen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandler_K/0/1/0/all/0/1\">Kim Sandler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Massion_P/0/1/0/all/0/1\">Pierre Massion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lasko_T/0/1/0/all/0/1\">Thomas A. Lasko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>",
          "description": "Data from multi-modality provide complementary information in clinical\nprediction, but missing data in clinical cohorts limits the number of subjects\nin multi-modal learning context. Multi-modal missing imputation is challenging\nwith existing methods when 1) the missing data span across heterogeneous\nmodalities (e.g., image vs. non-image); or 2) one modality is largely missing.\nIn this paper, we address imputation of missing data by modeling the joint\ndistribution of multi-modal data. Motivated by partial bidirectional generative\nadversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method\nthat imputes one modality combining the conditional knowledge from another\nmodality. Specifically, C-PBiGAN introduces a conditional latent space in a\nmissing imputation framework that jointly encodes the available multi-modal\ndata, along with a class regularization loss on imputed data to recover\ndiscriminative information. To our knowledge, it is the first generative\nadversarial model that addresses multi-modal missing imputation by modeling the\njoint distribution of image and non-image data. We validate our model with both\nthe national lung screening trial (NLST) dataset and an external clinical\nvalidation cohort. The proposed C-PBiGAN achieves significant improvements in\nlung cancer risk estimation compared with representative imputation methods\n(e.g., AUC values increase in both NLST (+2.9\\%) and in-house dataset (+4.3\\%)\ncompared with PBiGAN, p$<$0.05).",
          "link": "http://arxiv.org/abs/2107.11882",
          "publishedOn": "2021-07-27T02:03:35.807Z",
          "wordCount": 683,
          "title": "Lung Cancer Risk Estimation with Incomplete Data: A Joint Missing Imputation Perspective. (arXiv:2107.11882v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mengwu Guo</a>",
          "description": "As a generalization of the work in [Lee et al., 2017], this note briefly\ndiscusses when the prior of a neural network output follows a Gaussian process,\nand how a neural-network-induced Gaussian process is formulated. The posterior\nmean functions of such a Gaussian process regression lie in the reproducing\nkernel Hilbert space defined by the neural-network-induced kernel. In the case\nof two-layer neural networks, the induced Gaussian processes provide an\ninterpretation of the reproducing kernel Hilbert spaces whose union forms a\nBarron space.",
          "link": "http://arxiv.org/abs/2107.11892",
          "publishedOn": "2021-07-27T02:03:35.785Z",
          "wordCount": 523,
          "title": "A brief note on understanding neural networks as Gaussian processes. (arXiv:2107.11892v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asperti_A/0/1/0/all/0/1\">Andrea Asperti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evangelista_D/0/1/0/all/0/1\">Davide Evangelista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzolla_M/0/1/0/all/0/1\">Moreno Marzolla</a>",
          "description": "The term GreenAI refers to a novel approach to Deep Learning, that is more\naware of the ecological impact and the computational efficiency of its methods.\nThe promoters of GreenAI suggested the use of Floating Point Operations (FLOPs)\nas a measure of the computational cost of Neural Networks; however, that\nmeasure does not correlate well with the energy consumption of hardware\nequipped with massively parallel processing units like GPUs or TPUs. In this\narticle, we propose a simple refinement of the formula used to compute floating\npoint operations for convolutional layers, called {\\alpha}-FLOPs, explaining\nand correcting the traditional discrepancy with respect to different layers,\nand closer to reality. The notion of {\\alpha}-FLOPs relies on the crucial\ninsight that, in case of inputs with multiple dimensions, there is no reason to\nbelieve that the speedup offered by parallelism will be uniform along all\ndifferent axes.",
          "link": "http://arxiv.org/abs/2107.11949",
          "publishedOn": "2021-07-27T02:03:35.778Z",
          "wordCount": 602,
          "title": "Dissecting FLOPs along input dimensions for GreenAI cost estimations. (arXiv:2107.11949v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yeli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1\">Daniel Jun Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1\">Arvind Easwaran</a>",
          "description": "Uncertainties in machine learning are a significant roadblock for its\napplication in safety-critical cyber-physical systems (CPS). One source of\nuncertainty arises from distribution shifts in the input data between training\nand test scenarios. Detecting such distribution shifts in real-time is an\nemerging approach to address the challenge. The high dimensional input space in\nCPS applications involving imaging adds extra difficulty to the task.\nGenerative learning models are widely adopted for the task, namely\nout-of-distribution (OoD) detection. To improve the state-of-the-art, we\nstudied existing proposals from both machine learning and CPS fields. In the\nlatter, safety monitoring in real-time for autonomous driving agents has been a\nfocus. Exploiting the spatiotemporal correlation of motion in videos, we can\nrobustly detect hazardous motion around autonomous driving agents. Inspired by\nthe latest advances in the Variational Autoencoder (VAE) theory and practice,\nwe tapped into the prior knowledge in data to further boost OoD detection's\nrobustness. Comparison studies over nuScenes and Synthia data sets show our\nmethods significantly improve detection capabilities of OoD factors unique to\ndriving scenarios, 42% better than state-of-the-art approaches. Our model also\ngeneralized near-perfectly, 97% better than the state-of-the-art across the\nreal-world and simulation driving data sets experimented. Finally, we\ncustomized one proposed method into a twin-encoder model that can be deployed\nto resource limited embedded devices for real-time OoD detection. Its execution\ntime was reduced over four times in low-precision 8-bit integer inference,\nwhile detection capability is comparable to its corresponding floating-point\nmodel.",
          "link": "http://arxiv.org/abs/2107.11750",
          "publishedOn": "2021-07-27T02:03:35.751Z",
          "wordCount": 686,
          "title": "Improving Variational Autoencoder based Out-of-Distribution Detection for Embedded Real-time Applications. (arXiv:2107.11750v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11843",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Drgona_J/0/1/0/all/0/1\">Jan Drgona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tuor_A/0/1/0/all/0/1\">Aaron Tuor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vasisht_S/0/1/0/all/0/1\">Soumya Vasisht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Skomski_E/0/1/0/all/0/1\">Elliott Skomski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vrabie_D/0/1/0/all/0/1\">Draguna Vrabie</a>",
          "description": "We present a differentiable predictive control (DPC) methodology for learning\nconstrained control laws for unknown nonlinear systems. DPC poses an\napproximate solution to multiparametric programming problems emerging from\nexplicit nonlinear model predictive control (MPC). Contrary to approximate MPC,\nDPC does not require supervision by an expert controller. Instead, a system\ndynamics model is learned from the observed system's dynamics, and the neural\ncontrol law is optimized offline by leveraging the differentiable closed-loop\nsystem model. The combination of a differentiable closed-loop system and\npenalty methods for constraint handling of system outputs and inputs allows us\nto optimize the control law's parameters directly by backpropagating economic\nMPC loss through the learned system model. The control performance of the\nproposed DPC method is demonstrated in simulation using learned model of\nmulti-zone building thermal dynamics.",
          "link": "http://arxiv.org/abs/2107.11843",
          "publishedOn": "2021-07-27T02:03:35.744Z",
          "wordCount": 582,
          "title": "Deep Learning Explicit Differentiable Predictive Control Laws for Buildings. (arXiv:2107.11843v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping-Chia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively.",
          "link": "http://arxiv.org/abs/2107.11769",
          "publishedOn": "2021-07-27T02:03:35.737Z",
          "wordCount": 608,
          "title": "ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1\">Pedro H.C. Avelar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audibert_R/0/1/0/all/0/1\">Rafael B. Audibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavares_A/0/1/0/all/0/1\">Anderson R. Tavares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1\">Lu&#xed;s C. Lamb</a>",
          "description": "Recently, the use of sound measures and metrics in Artificial Intelligence\nhas become the subject of interest of academia, government, and industry.\nEfforts towards measuring different phenomena have gained traction in the AI\ncommunity, as illustrated by the publication of several influential field\nreports and policy documents. These metrics are designed to help decision\ntakers to inform themselves about the fast-moving and impacting influences of\nkey advances in Artificial Intelligence in general and Machine Learning in\nparticular. In this paper we propose to use such newfound capabilities of AI\ntechnologies to augment our AI measuring capabilities. We do so by training a\nmodel to classify publications related to ethical issues and concerns. In our\nmethodology we use an expert, manually curated dataset as the training set and\nthen evaluate a large set of research papers. Finally, we highlight the\nimplications of AI metrics, in particular their contribution towards developing\ntrustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI\nFairness; AI Measurement. Ethics in Computer Science.",
          "link": "http://arxiv.org/abs/2107.11913",
          "publishedOn": "2021-07-27T02:03:35.731Z",
          "wordCount": 619,
          "title": "Measuring Ethics in AI with AI: A Methodology and Dataset Construction. (arXiv:2107.11913v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1\">Nasibullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>",
          "description": "Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning-based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. This paper addresses the\ndrawback by introducing a dynamic loss network (DLN), which provides an\nadditional feedback signal that directly reflects the evaluation metrics. Our\nresults on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to\nText (MSRVTT) datasets outperform previous methods.",
          "link": "http://arxiv.org/abs/2107.11707",
          "publishedOn": "2021-07-27T02:03:35.716Z",
          "wordCount": 585,
          "title": "Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Ziji Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "The transformer has recently achieved impressive results on various tasks. To\nfurther improve the effectiveness and efficiency of the transformer, there are\ntwo trains of thought among existing works: (1) going wider by scaling to more\ntrainable parameters; (2) going shallower by parameter sharing or model\ncompressing along with the depth. However, larger models usually do not scale\nwell when fewer tokens are available to train, and advanced parallelisms are\nrequired when the model is extremely large. Smaller models usually achieve\ninferior performance compared to the original transformer model due to the loss\nof representation power. In this paper, to achieve better performance with\nfewer trainable parameters, we propose a framework to deploy trainable\nparameters efficiently, by going wider instead of deeper. Specially, we scale\nalong model width by replacing feed-forward network (FFN) with\nmixture-of-experts (MoE). We then share the MoE layers across transformer\nblocks using individual layer normalization. Such deployment plays the role to\ntransform various semantic representations, which makes the model more\nparameter-efficient and effective. To evaluate our framework, we design WideNet\nand evaluate it on ImageNet-1K. Our best model outperforms Vision Transformer\n(ViT) by $1.46\\%$ with $0.72 \\times$ trainable parameters. Using $0.46 \\times$\nand $0.13 \\times$ parameters, our WideNet can still surpass ViT and ViT-MoE by\n$0.83\\%$ and $2.08\\%$, respectively.",
          "link": "http://arxiv.org/abs/2107.11817",
          "publishedOn": "2021-07-27T02:03:35.681Z",
          "wordCount": 650,
          "title": "Go Wider Instead of Deeper. (arXiv:2107.11817v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sleeman_W/0/1/0/all/0/1\">William C. Sleeman IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1\">Bartosz Krawczyk</a>",
          "description": "Learning from imbalanced data is among the most challenging areas in\ncontemporary machine learning. This becomes even more difficult when considered\nthe context of big data that calls for dedicated architectures capable of\nhigh-performance processing. Apache Spark is a highly efficient and popular\narchitecture, but it poses specific challenges for algorithms to be implemented\nfor it. While oversampling algorithms are an effective way for handling class\nimbalance, they have not been designed for distributed environments. In this\npaper, we propose a holistic look on oversampling algorithms for imbalanced big\ndata. We discuss the taxonomy of oversampling algorithms and their mechanisms\nused to handle skewed class distributions. We introduce a Spark library with 14\nstate-of-the-art oversampling algorithms implemented and evaluate their\nefficacy via extensive experimental study. Using binary and multi-class massive\ndata sets, we analyze the effectiveness of oversampling algorithms and their\nrelationships with different types of classifiers. We evaluate the trade-off\nbetween accuracy and time complexity of oversampling algorithms, as well as\ntheir scalability when increasing the size of data. This allows us to gain\ninsight into the usefulness of specific components of oversampling algorithms\nfor big data, as well as formulate guidelines and recommendations for designing\nfuture resampling approaches for massive imbalanced data. Our library can be\ndownloaded from https://github.com/fsleeman/spark-class-balancing.git.",
          "link": "http://arxiv.org/abs/2107.11508",
          "publishedOn": "2021-07-27T02:03:35.666Z",
          "wordCount": 665,
          "title": "Imbalanced Big Data Oversampling: Taxonomy, Algorithms, Software, Guidelines and Future Directions. (arXiv:2107.11508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1\">Ali Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1\">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Huaiyu Dai</a>",
          "description": "Adversarial training has been shown as an effective approach to improve the\nrobustness of image classifiers against white-box attacks. However, its\neffectiveness against black-box attacks is more nuanced. In this work, we\ndemonstrate that some geometric consequences of adversarial training on the\ndecision boundary of deep networks give an edge to certain types of black-box\nattacks. In particular, we define a metric called robustness gain to show that\nwhile adversarial training is an effective method to dramatically improve the\nrobustness in white-box scenarios, it may not provide such a good robustness\ngain against the more realistic decision-based black-box attacks. Moreover, we\nshow that even the minimal perturbation white-box attacks can converge faster\nagainst adversarially-trained neural networks compared to the regular ones.",
          "link": "http://arxiv.org/abs/2107.11671",
          "publishedOn": "2021-07-27T02:03:35.576Z",
          "wordCount": 569,
          "title": "Adversarial training may be a double-edged sword. (arXiv:2107.11671v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11533",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1\">Hung Tran-The</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1\">Thanh Nguyen-Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "We address policy learning with logged data in contextual bandits. Current\noffline-policy learning algorithms are mostly based on inverse propensity score\n(IPS) weighting requiring the logging policy to have \\emph{full support} i.e. a\nnon-zero probability for any context/action of the evaluation policy. However,\nmany real-world systems do not guarantee such logging policies, especially when\nthe action space is large and many actions have poor or missing rewards. With\nsuch \\emph{support deficiency}, the offline learning fails to find optimal\npolicies. We propose a novel approach that uses a hybrid of offline learning\nwith online exploration. The online exploration is used to explore unsupported\nactions in the logged data whilst offline learning is used to exploit supported\nactions from the logged data avoiding unnecessary explorations. Our approach\ndetermines an optimal policy with theoretical guarantees using the minimal\nnumber of online explorations. We demonstrate our algorithms' effectiveness\nempirically on a diverse collection of datasets.",
          "link": "http://arxiv.org/abs/2107.11533",
          "publishedOn": "2021-07-27T02:03:35.542Z",
          "wordCount": 594,
          "title": "Combining Online Learning and Offline Learning for Contextual Bandits with Deficient Support. (arXiv:2107.11533v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bose_R/0/1/0/all/0/1\">Rupak Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_S/0/1/0/all/0/1\">Shivam Pande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>",
          "description": "As the field of remote sensing is evolving, we witness the accumulation of\ninformation from several modalities, such as multispectral (MS), hyperspectral\n(HSI), LiDAR etc. Each of these modalities possess its own distinct\ncharacteristics and when combined synergistically, perform very well in the\nrecognition and classification tasks. However, fusing multiple modalities in\nremote sensing is cumbersome due to highly disparate domains. Furthermore, the\nexisting methods do not facilitate cross-modal interactions. To this end, we\npropose a novel transformer based fusion method for HSI and LiDAR modalities.\nThe model is composed of stacked auto encoders that harness the cross key-value\npairs for HSI and LiDAR, thus establishing a communication between the two\nmodalities, while simultaneously using the CNNs to extract the spectral and\nspatial information from HSI and LiDAR. We test our model on Houston (Data\nFusion Contest - 2013) and MUUFL Gulfport datasets and achieve competitive\nresults.",
          "link": "http://arxiv.org/abs/2107.11585",
          "publishedOn": "2021-07-27T02:03:35.534Z",
          "wordCount": 605,
          "title": "Two Headed Dragons: Multimodal Fusion and Cross Modal Transactions. (arXiv:2107.11585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Lin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>",
          "description": "Smart contract vulnerability detection draws extensive attention in recent\nyears due to the substantial losses caused by hacker attacks. Existing efforts\nfor contract security analysis heavily rely on rigid rules defined by experts,\nwhich are labor-intensive and non-scalable. More importantly, expert-defined\nrules tend to be error-prone and suffer the inherent risk of being cheated by\ncrafty attackers. Recent researches focus on the symbolic execution and formal\nanalysis of smart contracts for vulnerability detection, yet to achieve a\nprecise and scalable solution. Although several methods have been proposed to\ndetect vulnerabilities in smart contracts, there is still a lack of effort that\nconsiders combining expert-defined security patterns with deep neural networks.\nIn this paper, we explore using graph neural networks and expert knowledge for\nsmart contract vulnerability detection. Specifically, we cast the rich control-\nand data- flow semantics of the source code into a contract graph. To highlight\nthe critical nodes in the graph, we further design a node elimination phase to\nnormalize the graph. Then, we propose a novel temporal message propagation\nnetwork to extract the graph feature from the normalized graph, and combine the\ngraph feature with designed expert patterns to yield a final detection system.\nExtensive experiments are conducted on all the smart contracts that have source\ncode in Ethereum and VNT Chain platforms. Empirical results show significant\naccuracy improvements over the state-of-the-art methods on three types of\nvulnerabilities, where the detection accuracy of our method reaches 89.15%,\n89.02%, and 83.21% for reentrancy, timestamp dependence, and infinite loop\nvulnerabilities, respectively.",
          "link": "http://arxiv.org/abs/2107.11598",
          "publishedOn": "2021-07-27T02:03:35.527Z",
          "wordCount": 715,
          "title": "Combining Graph Neural Networks with Expert Knowledge for Smart Contract Vulnerability Detection. (arXiv:2107.11598v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leino_K/0/1/0/all/0/1\">Klas Leino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fromherz_A/0/1/0/all/0/1\">Aymeric Fromherz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangal_R/0/1/0/all/0/1\">Ravi Mangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1\">Matt Fredrikson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parno_B/0/1/0/all/0/1\">Bryan Parno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1\">Corina P&#x103;s&#x103;reanu</a>",
          "description": "Neural networks are increasingly being deployed in contexts where safety is a\ncritical concern. In this work, we propose a way to construct neural network\nclassifiers that dynamically repair violations of non-relational safety\nconstraints called safe ordering properties. Safe ordering properties relate\nrequirements on the ordering of a network's output indices to conditions on\ntheir input, and are sufficient to express most useful notions of\nnon-relational safety for classifiers. Our approach is based on a novel\nself-repairing layer, which provably yields safe outputs regardless of the\ncharacteristics of its input. We compose this layer with an existing network to\nconstruct a self-repairing network (SR-Net), and show that in addition to\nproviding safe outputs, the SR-Net is guaranteed to preserve the accuracy of\nthe original network. Notably, our approach is independent of the size and\narchitecture of the network being repaired, depending only on the specified\nproperty and the dimension of the network's output; thus it is scalable to\nlarge state-of-the-art networks. We show that our approach can be implemented\nusing vectorized computations that execute efficiently on a GPU, introducing\nrun-time overhead of less than one millisecond on current hardware -- even on\nlarge, widely-used networks containing hundreds of thousands of neurons and\nmillions of parameters.",
          "link": "http://arxiv.org/abs/2107.11445",
          "publishedOn": "2021-07-27T02:03:35.520Z",
          "wordCount": 651,
          "title": "Self-Repairing Neural Networks: Provable Safety for Deep Networks via Dynamic Repair. (arXiv:2107.11445v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Siqi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangjing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Federated learning (FL) is a solution for privacy challenge, which allows\nmultiparty to train a shared model without violating privacy protection\nregulations. Many excellent works of FL have been proposed in recent years. To\nhelp researchers verify their ideas in FL, we designed and developed FedLab, a\nflexible and modular FL framework based on PyTorch. In this paper, we will\nintroduce architecture and features of FedLab. For current popular research\npoints: optimization and communication compression, FedLab provides functional\ninterfaces and a series of baseline implementation are available, making\nresearchers quickly implement ideas. In addition, FedLab is scale-able in both\nclient simulation and distributed communication.",
          "link": "http://arxiv.org/abs/2107.11621",
          "publishedOn": "2021-07-27T02:03:35.485Z",
          "wordCount": 533,
          "title": "FedLab: A Flexible Federated Learning Framework. (arXiv:2107.11621v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kegl_B/0/1/0/all/0/1\">Bal&#xe1;zs K&#xe9;gl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_G/0/1/0/all/0/1\">Gabriel Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1\">Albert Thomas</a>",
          "description": "We contribute to micro-data model-based reinforcement learning (MBRL) by\nrigorously comparing popular generative models using a fixed (random shooting)\ncontrol agent. We find that on an environment that requires multimodal\nposterior predictives, mixture density nets outperform all other models by a\nlarge margin. When multimodality is not required, our surprising finding is\nthat we do not need probabilistic posterior predictives: deterministic models\nare on par, in fact they consistently (although non-significantly) outperform\ntheir probabilistic counterparts. We also found that heteroscedasticity at\ntraining time, perhaps acting as a regularizer, improves predictions at longer\nhorizons. At the methodological side, we design metrics and an experimental\nprotocol which can be used to evaluate the various models, predicting their\nasymptotic performance when using them on the control problem. Using this\nframework, we improve the state-of-the-art sample complexity of MBRL on Acrobot\nby two to four folds, using an aggressive training schedule which is outside of\nthe hyperparameter interval usually considered",
          "link": "http://arxiv.org/abs/2107.11587",
          "publishedOn": "2021-07-27T02:03:35.479Z",
          "wordCount": 609,
          "title": "Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?. (arXiv:2107.11587v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lindt_A/0/1/0/all/0/1\">Alexandra Lindt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1\">Emiel Hoogeboom</a>",
          "description": "Discrete flow-based models are a recently proposed class of generative models\nthat learn invertible transformations for discrete random variables. Since they\ndo not require data dequantization and maximize an exact likelihood objective,\nthey can be used in a straight-forward manner for lossless compression. In this\npaper, we introduce a new discrete flow-based model for categorical random\nvariables: Discrete Denoising Flows (DDFs). In contrast with other discrete\nflow-based models, our model can be locally trained without introducing\ngradient bias. We show that DDFs outperform Discrete Flows on modeling a toy\nexample, binary MNIST and Cityscapes segmentation maps, measured in\nlog-likelihood.",
          "link": "http://arxiv.org/abs/2107.11625",
          "publishedOn": "2021-07-27T02:03:35.472Z",
          "wordCount": 533,
          "title": "Discrete Denoising Flows. (arXiv:2107.11625v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Susu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berges_M/0/1/0/all/0/1\">Mario Berg&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1\">Hae Young Noh</a>",
          "description": "Monitoring bridge health using vibrations of drive-by vehicles has various\nbenefits, such as no need for directly installing and maintaining sensors on\nthe bridge. However, many of the existing drive-by monitoring approaches are\nbased on supervised learning models that require labeled data from every bridge\nof interest, which is expensive and time-consuming, if not impossible, to\nobtain. To this end, we introduce a new framework that transfers the model\nlearned from one bridge to diagnose damage in another bridge without any labels\nfrom the target bridge. Our framework trains a hierarchical neural network\nmodel in an adversarial way to extract task-shared and task-specific features\nthat are informative to multiple diagnostic tasks and invariant across multiple\nbridges. We evaluate our framework on experimental data collected from 2\nbridges and 3 vehicles. We achieve accuracies of 95% for damage detection, 93%\nfor localization, and up to 72% for quantification, which are ~2 times\nimprovements from baseline methods.",
          "link": "http://arxiv.org/abs/2107.11435",
          "publishedOn": "2021-07-27T02:03:35.466Z",
          "wordCount": 597,
          "title": "HierMUD: Hierarchical Multi-task Unsupervised Domain Adaptation between Bridges for Drive-by Damage Diagnosis. (arXiv:2107.11435v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>",
          "description": "Non-Euclidean geometry with constant negative curvature, i.e., hyperbolic\nspace, has attracted sustained attention in the community of machine learning.\nHyperbolic space, owing to its ability to embed hierarchical structures\ncontinuously with low distortion, has been applied for learning data with\ntree-like structures. Hyperbolic Neural Networks (HNNs) that operate directly\nin hyperbolic space have also been proposed recently to further exploit the\npotential of hyperbolic representations. While HNNs have achieved better\nperformance than Euclidean neural networks (ENNs) on datasets with implicit\nhierarchical structure, they still perform poorly on standard classification\nbenchmarks such as CIFAR and ImageNet. The traditional wisdom is that it is\ncritical for the data to respect the hyperbolic geometry when applying HNNs. In\nthis paper, we first conduct an empirical study showing that the inferior\nperformance of HNNs on standard recognition datasets can be attributed to the\nnotorious vanishing gradient problem. We further discovered that this problem\nstems from the hybrid architecture of HNNs. Our analysis leads to a simple yet\neffective solution called Feature Clipping, which regularizes the hyperbolic\nembedding whenever its norm exceeding a given threshold. Our thorough\nexperiments show that the proposed method can successfully avoid the vanishing\ngradient problem when training HNNs with backpropagation. The improved HNNs are\nable to achieve comparable performance with ENNs on standard image recognition\ndatasets including MNIST, CIFAR10, CIFAR100 and ImageNet, while demonstrating\nmore adversarial robustness and stronger out-of-distribution detection\ncapability.",
          "link": "http://arxiv.org/abs/2107.11472",
          "publishedOn": "2021-07-27T02:03:35.460Z",
          "wordCount": 670,
          "title": "Free Hyperbolic Neural Networks with Limited Radii. (arXiv:2107.11472v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maalouf_A/0/1/0/all/0/1\">Alaa Maalouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_O/0/1/0/all/0/1\">Oren Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1\">Dan Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "We present a novel global compression framework for deep neural networks that\nautomatically analyzes each layer to identify the optimal per-layer compression\nratio, while simultaneously achieving the desired overall compression. Our\nalgorithm hinges on the idea of compressing each convolutional (or\nfully-connected) layer by slicing its channels into multiple groups and\ndecomposing each group via low-rank decomposition. At the core of our algorithm\nis the derivation of layer-wise error bounds from the Eckart Young Mirsky\ntheorem. We then leverage these bounds to frame the compression problem as an\noptimization problem where we wish to minimize the maximum compression error\nacross layers and propose an efficient algorithm towards a solution. Our\nexperiments indicate that our method outperforms existing low-rank compression\napproaches across a wide range of networks and data sets. We believe that our\nresults open up new avenues for future research into the global\nperformance-size trade-offs of modern neural networks. Our code is available at\nhttps://github.com/lucaslie/torchprune.",
          "link": "http://arxiv.org/abs/2107.11442",
          "publishedOn": "2021-07-27T02:03:35.449Z",
          "wordCount": 602,
          "title": "Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition. (arXiv:2107.11442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>",
          "description": "Making classifiers robust to adversarial examples is hard. Thus, many\ndefenses tackle the seemingly easier task of detecting perturbed inputs. We\nshow a barrier towards this goal. We prove a general hardness reduction between\ndetection and classification of adversarial examples: given a robust detector\nfor attacks at distance {\\epsilon} (in some metric), we can build a similarly\nrobust (but inefficient) classifier for attacks at distance {\\epsilon}/2. Our\nreduction is computationally inefficient, and thus cannot be used to build\npractical classifiers. Instead, it is a useful sanity check to test whether\nempirical detection results imply something much stronger than the authors\npresumably anticipated. To illustrate, we revisit 13 detector defenses. For\n11/13 cases, we show that the claimed detection results would imply an\ninefficient classifier with robustness far beyond the state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.11630",
          "publishedOn": "2021-07-27T02:03:35.415Z",
          "wordCount": 581,
          "title": "Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them. (arXiv:2107.11630v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_A/0/1/0/all/0/1\">Abdallah Moubayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shami_A/0/1/0/all/0/1\">Abdallah Shami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_P/0/1/0/all/0/1\">Parisa Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhtouta_A/0/1/0/all/0/1\">Amine Boukhtouta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larabi_A/0/1/0/all/0/1\">Adel Larabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunner_R/0/1/0/all/0/1\">Richard Brunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preda_S/0/1/0/all/0/1\">Stere Preda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migault_D/0/1/0/all/0/1\">Daniel Migault</a>",
          "description": "Content delivery networks (CDNs) provide efficient content distribution over\nthe Internet. CDNs improve the connectivity and efficiency of global\ncommunications, but their caching mechanisms may be breached by\ncyber-attackers. Among the security mechanisms, effective anomaly detection\nforms an important part of CDN security enhancement. In this work, we propose a\nmulti-perspective unsupervised learning framework for anomaly detection in\nCDNs. In the proposed framework, a multi-perspective feature engineering\napproach, an optimized unsupervised anomaly detection model that utilizes an\nisolation forest and a Gaussian mixture model, and a multi-perspective\nvalidation method, are developed to detect abnormal behaviors in CDNs mainly\nfrom the client Internet Protocol (IP) and node perspectives, therefore to\nidentify the denial of service (DoS) and cache pollution attack (CPA) patterns.\nExperimental results are presented based on the analytics of eight days of\nreal-world CDN log data provided by a major CDN operator. Through experiments,\nthe abnormal contents, compromised nodes, malicious IPs, as well as their\ncorresponding attack types, are identified effectively by the proposed\nframework and validated by multiple cybersecurity experts. This shows the\neffectiveness of the proposed method when applied to real-world CDN data.",
          "link": "http://arxiv.org/abs/2107.11514",
          "publishedOn": "2021-07-27T02:03:35.407Z",
          "wordCount": 670,
          "title": "Multi-Perspective Content Delivery Networks Security Framework Using Optimized Unsupervised Anomaly Detection. (arXiv:2107.11514v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadigurschi_M/0/1/0/all/0/1\">Menachem Sadigurschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1\">Uri Stemmer</a>",
          "description": "We revisit the fundamental problem of learning Axis-Aligned-Rectangles over a\nfinite grid $X^d\\subseteq{\\mathbb{R}}^d$ with differential privacy. Existing\nresults show that the sample complexity of this problem is at most $\\min\\left\\{\nd{\\cdot}\\log|X| \\;,\\; d^{1.5}{\\cdot}\\left(\\log^*|X| \\right)^{1.5}\\right\\}$.\nThat is, existing constructions either require sample complexity that grows\nlinearly with $\\log|X|$, or else it grows super linearly with the dimension\n$d$. We present a novel algorithm that reduces the sample complexity to only\n$\\tilde{O}\\left\\{d{\\cdot}\\left(\\log^*|X|\\right)^{1.5}\\right\\}$, attaining a\ndimensionality optimal dependency without requiring the sample complexity to\ngrow with $\\log|X|$.The technique used in order to attain this improvement\ninvolves the deletion of \"exposed\" data-points on the go, in a fashion designed\nto avoid the cost of the adaptive composition theorems. The core of this\ntechnique may be of individual interest, introducing a new method for\nconstructing statistically-efficient private algorithms.",
          "link": "http://arxiv.org/abs/2107.11526",
          "publishedOn": "2021-07-27T02:03:35.400Z",
          "wordCount": 573,
          "title": "On the Sample Complexity of Privately Learning Axis-Aligned Rectangles. (arXiv:2107.11526v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1\">Biswadeep Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1\">Saibal Mukhopadhyay</a>",
          "description": "We present a Model Uncertainty-aware Differentiable ARchiTecture Search\n($\\mu$DARTS) that optimizes neural networks to simultaneously achieve high\naccuracy and low uncertainty. We introduce concrete dropout within DARTS cells\nand include a Monte-Carlo regularizer within the training loss to optimize the\nconcrete dropout probabilities. A predictive variance term is introduced in the\nvalidation loss to enable searching for architecture with minimal model\nuncertainty. The experiments on CIFAR10, CIFAR100, SVHN, and ImageNet verify\nthe effectiveness of $\\mu$DARTS in improving accuracy and reducing uncertainty\ncompared to existing DARTS methods. Moreover, the final architecture obtained\nfrom $\\mu$DARTS shows higher robustness to noise at the input image and model\nparameters compared to the architecture obtained from existing DARTS methods.",
          "link": "http://arxiv.org/abs/2107.11500",
          "publishedOn": "2021-07-27T02:03:35.280Z",
          "wordCount": 549,
          "title": "$\\mu$DARTS: Model Uncertainty-Aware Differentiable Architecture Search. (arXiv:2107.11500v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_B/0/1/0/all/0/1\">Bahador Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">WaiChing Sun</a>",
          "description": "This paper presents a PINN training framework that employs (1) pre-training\nsteps that accelerates and improve the robustness of the training of\nphysics-informed neural network with auxiliary data stored in point clouds, (2)\na net-to-net knowledge transfer algorithm that improves the weight\ninitialization of the neural network and (3) a multi-objective optimization\nalgorithm that may improve the performance of a physical-informed neural\nnetwork with competing constraints. We consider the training and transfer and\nmulti-task learning of physics-informed neural network (PINN) as\nmulti-objective problems where the physics constraints such as the governing\nequation, boundary conditions, thermodynamic inequality, symmetry, and\ninvariant properties, as well as point cloud used for pre-training can\nsometimes lead to conflicts and necessitating the seek of the Pareto optimal\nsolution. In these situations, weighted norms commonly used to handle multiple\nconstraints may lead to poor performance, while other multi-objective\nalgorithms may scale poorly with increasing dimensionality. To overcome this\ntechnical barrier, we adopt the concept of vectorized objective function and\nmodify a gradient descent approach to handle the issue of conflicting\ngradients. Numerical experiments are compared the benchmark boundary value\nproblems solved via PINN. The performance of the proposed paradigm is compared\nagainst the classical equal-weighted norm approach. Our numerical experiments\nindicate that the brittleness and lack of robustness demonstrated in some PINN\nimplementations can be overcome with the proposed strategy.",
          "link": "http://arxiv.org/abs/2107.11496",
          "publishedOn": "2021-07-27T02:03:35.261Z",
          "wordCount": 652,
          "title": "Training multi-objective/multi-task collocation physics-informed neural network with student/teachers transfer learnings. (arXiv:2107.11496v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Rui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1\">Robert M. Gower</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>",
          "description": "The policy gradient (PG) is one of the most popular methods for solving\nreinforcement learning (RL) problems. However, a solid theoretical\nunderstanding of even the \"vanilla\" PG has remained elusive for long time. In\nthis paper, we apply recent tools developed for the analysis of SGD in\nnon-convex optimization to obtain convergence guarantees for both REINFORCE and\nGPOMDP under smoothness assumption on the objective function and weak\nconditions on the second moment of the norm of the estimated gradient. When\ninstantiated under common assumptions on the policy space, our general result\nimmediately recovers existing $\\widetilde{\\mathcal{O}}(\\epsilon^{-4})$ sample\ncomplexity guarantees, but for wider ranges of parameters (e.g., step size and\nbatch size $m$) with respect to previous literature. Notably, our result\nincludes the single trajectory case (i.e., $m=1$) and it provides a more\naccurate analysis of the dependency on problem-specific parameters by fixing\nprevious results available in the literature. We believe that the integration\nof state-of-the-art tools from non-convex optimization may lead to identify a\nmuch broader range of problems where PG methods enjoy strong theoretical\nguarantees.",
          "link": "http://arxiv.org/abs/2107.11433",
          "publishedOn": "2021-07-27T02:03:35.252Z",
          "wordCount": 624,
          "title": "A general sample complexity analysis of vanilla policy gradient. (arXiv:2107.11433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blumer_K/0/1/0/all/0/1\">Katy Blumer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1\">Jon Kleinberg</a>",
          "description": "We analyze a dataset of retinal images using linear probes: linear regression\nmodels trained on some \"target\" task, using embeddings from a deep\nconvolutional (CNN) model trained on some \"source\" task as input. We use this\nmethod across all possible pairings of 93 tasks in the UK Biobank dataset of\nretinal images, leading to ~164k different models. We analyze the performance\nof these linear probes by source and target task and by layer depth. We observe\nthat representations from the middle layers of the network are more\ngeneralizable. We find that some target tasks are easily predicted irrespective\nof the source task, and that some other target tasks are more accurately\npredicted from correlated source tasks than from embeddings trained on the same\ntask.",
          "link": "http://arxiv.org/abs/2107.11468",
          "publishedOn": "2021-07-27T02:03:35.239Z",
          "wordCount": 596,
          "title": "Using a Cross-Task Grid of Linear Probes to Interpret CNN Model Predictions On Retinal Images. (arXiv:2107.11468v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weichao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ruihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basar_T/0/1/0/all/0/1\">Tamer Ba&#x15f;ar</a>",
          "description": "We consider model-free reinforcement learning (RL) in non-stationary Markov\ndecision processes. Both the reward functions and the state transition\nfunctions are allowed to vary arbitrarily over time as long as their cumulative\nvariations do not exceed certain variation budgets. We propose Restarted\nQ-Learning with Upper Confidence Bounds (RestartQ-UCB), the first model-free\nalgorithm for non-stationary RL, and show that it outperforms existing\nsolutions in terms of dynamic regret. Specifically, RestartQ-UCB with\nFreedman-type bonus terms achieves a dynamic regret bound of\n$\\widetilde{O}(S^{\\frac{1}{3}} A^{\\frac{1}{3}} \\Delta^{\\frac{1}{3}} H\nT^{\\frac{2}{3}})$, where $S$ and $A$ are the numbers of states and actions,\nrespectively, $\\Delta>0$ is the variation budget, $H$ is the number of time\nsteps per episode, and $T$ is the total number of time steps. We further\npresent a parameter-free algorithm named Double-Restart Q-UCB that does not\nrequire prior knowledge of the variation budget. We show that our algorithms\nare \\emph{nearly optimal} by establishing an information-theoretical lower\nbound of $\\Omega(S^{\\frac{1}{3}} A^{\\frac{1}{3}} \\Delta^{\\frac{1}{3}}\nH^{\\frac{2}{3}} T^{\\frac{2}{3}})$, the first lower bound in non-stationary RL.\nNumerical experiments validate the advantages of RestartQ-UCB in terms of both\ncumulative rewards and computational efficiency. We demonstrate the power of\nour results in examples of multi-agent RL and inventory control across related\nproducts.",
          "link": "http://arxiv.org/abs/2010.03161",
          "publishedOn": "2021-07-27T02:03:34.879Z",
          "wordCount": 700,
          "title": "Model-Free Non-Stationary RL: Near-Optimal Regret and Applications in Multi-Agent RL and Inventory Control. (arXiv:2010.03161v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uhrenholt_A/0/1/0/all/0/1\">Anders Kirk Uhrenholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charvet_V/0/1/0/all/0/1\">Valentin Charvet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_B/0/1/0/all/0/1\">Bj&#xf8;rn Sand Jensen</a>",
          "description": "Sparse Gaussian processes and various extensions thereof are enabled through\ninducing points, that simultaneously bottleneck the predictive capacity and act\nas the main contributor towards model complexity. However, the number of\ninducing points is generally not associated with uncertainty which prevents us\nfrom applying the apparatus of Bayesian reasoning for identifying an\nappropriate trade-off. In this work we place a point process prior on the\ninducing points and approximate the associated posterior through stochastic\nvariational inference. By letting the prior encourage a moderate number of\ninducing points, we enable the model to learn which and how many points to\nutilise. We experimentally show that fewer inducing points are preferred by the\nmodel as the points become less informative, and further demonstrate how the\nmethod can be employed in deep Gaussian processes and latent variable\nmodelling.",
          "link": "http://arxiv.org/abs/2010.09370",
          "publishedOn": "2021-07-27T02:03:34.843Z",
          "wordCount": 628,
          "title": "Probabilistic selection of inducing points in sparse Gaussian processes. (arXiv:2010.09370v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.02417",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael. I. Jordan</a>",
          "description": "This paper resolves a longstanding open question pertaining to the design of\nnear-optimal first-order algorithms for smooth and\nstrongly-convex-strongly-concave minimax problems. Current state-of-the-art\nfirst-order algorithms find an approximate Nash equilibrium using\n$\\tilde{O}(\\kappa_{\\mathbf x}+\\kappa_{\\mathbf y})$ or\n$\\tilde{O}(\\min\\{\\kappa_{\\mathbf x}\\sqrt{\\kappa_{\\mathbf y}},\n\\sqrt{\\kappa_{\\mathbf x}}\\kappa_{\\mathbf y}\\})$ gradient evaluations, where\n$\\kappa_{\\mathbf x}$ and $\\kappa_{\\mathbf y}$ are the condition numbers for the\nstrong-convexity and strong-concavity assumptions. A gap still remains between\nthese results and the best existing lower bound\n$\\tilde{\\Omega}(\\sqrt{\\kappa_{\\mathbf x}\\kappa_{\\mathbf y}})$. This paper\npresents the first algorithm with $\\tilde{O}(\\sqrt{\\kappa_{\\mathbf\nx}\\kappa_{\\mathbf y}})$ gradient complexity, matching the lower bound up to\nlogarithmic factors. Our algorithm is designed based on an accelerated proximal\npoint method and an accelerated solver for minimax proximal steps. It can be\neasily extended to the settings of strongly-convex-concave, convex-concave,\nnonconvex-strongly-concave, and nonconvex-concave functions. This paper also\npresents algorithms that match or outperform all existing methods in these\nsettings in terms of gradient complexity, up to logarithmic factors.",
          "link": "http://arxiv.org/abs/2002.02417",
          "publishedOn": "2021-07-27T02:03:34.817Z",
          "wordCount": 657,
          "title": "Near-Optimal Algorithms for Minimax Optimization. (arXiv:2002.02417v6 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1806.03884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thomas George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_C/0/1/0/all/0/1\">C&#xe9;sar Laurent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouthillier_X/0/1/0/all/0/1\">Xavier Bouthillier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1\">Pascal Vincent</a>",
          "description": "Optimization algorithms that leverage gradient covariance information, such\nas variants of natural gradient descent (Amari, 1998), offer the prospect of\nyielding more effective descent directions. For models with many parameters,\nthe covariance matrix they are based on becomes gigantic, making them\ninapplicable in their original form. This has motivated research into both\nsimple diagonal approximations and more sophisticated factored approximations\nsuch as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In\nthe present work we draw inspiration from both to propose a novel approximation\nthat is provably better than KFAC and amendable to cheap partial updates. It\nconsists in tracking a diagonal variance, not in parameter coordinates, but in\na Kronecker-factored eigenbasis, in which the diagonal approximation is likely\nto be more effective. Experiments show improvements over KFAC in optimization\nspeed for several deep network architectures.",
          "link": "http://arxiv.org/abs/1806.03884",
          "publishedOn": "2021-07-27T02:03:34.810Z",
          "wordCount": 617,
          "title": "Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis. (arXiv:1806.03884v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeub_L/0/1/0/all/0/1\">Lucas G. S. Jeub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colavizza_G/0/1/0/all/0/1\">Giovanni Colavizza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaowen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1\">Marya Bazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1\">Mihai Cucuringu</a>",
          "description": "We propose a decentralised \"local2global\" approach to graph representation\nlearning, that one can a-priori use to scale any embedding technique. Our\nlocal2global approach proceeds by first dividing the input graph into\noverlapping subgraphs (or \"patches\") and training local representations for\neach patch independently. In a second step, we combine the local\nrepresentations into a globally consistent representation by estimating the set\nof rigid motions that best align the local representations using information\nfrom the patch overlaps, via group synchronization. A key distinguishing\nfeature of local2global relative to existing work is that patches are trained\nindependently without the need for the often costly parameter synchronisation\nduring distributed training. This allows local2global to scale to large-scale\nindustrial applications, where the input graph may not even fit into memory and\nmay be stored in a distributed manner. Preliminary results on medium-scale data\nsets (up to $\\sim$7K nodes and $\\sim$200K edges) are promising, with a graph\nreconstruction performance for local2global that is comparable to that of\nglobally trained embeddings. A thorough evaluation of local2global on large\nscale data and applications to downstream tasks, such as node classification\nand link prediction, constitutes ongoing work.",
          "link": "http://arxiv.org/abs/2107.12224",
          "publishedOn": "2021-07-27T02:03:34.804Z",
          "wordCount": 638,
          "title": "Local2Global: Scaling global representation learning on graphs via local training. (arXiv:2107.12224v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.06557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilinski_M/0/1/0/all/0/1\">Mateusz Wilinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lokhov_A/0/1/0/all/0/1\">Andrey Y. Lokhov</a>",
          "description": "Spreading processes play an increasingly important role in modeling for\ndiffusion networks, information propagation, marketing and opinion setting. We\naddress the problem of learning of a spreading model such that the predictions\ngenerated from this model are accurate and could be subsequently used for the\noptimization, and control of diffusion dynamics. We focus on a challenging\nsetting where full observations of the dynamics are not available, and standard\napproaches such as maximum likelihood quickly become intractable for large\nnetwork instances. We introduce a computationally efficient algorithm, based on\na scalable dynamic message-passing approach, which is able to learn parameters\nof the effective spreading model given only limited information on the\nactivation times of nodes in the network. The popular Independent Cascade model\nis used to illustrate our approach. We show that tractable inference from the\nlearned model generates a better prediction of marginal probabilities compared\nto the original model. We develop a systematic procedure for learning a mixture\nof models which further improves the prediction quality.",
          "link": "http://arxiv.org/abs/2007.06557",
          "publishedOn": "2021-07-27T02:03:34.797Z",
          "wordCount": 659,
          "title": "Prediction-Centric Learning of Independent Cascade Dynamics from Partial Observations. (arXiv:2007.06557v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12243",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Wei_J/0/1/0/all/0/1\">Junkang Wei</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1\">Siyuan Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zong_L/0/1/0/all/0/1\">Licheng Zong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>",
          "description": "Protein-RNA interactions are of vital importance to a variety of cellular\nactivities. Both experimental and computational techniques have been developed\nto study the interactions. Due to the limitation of the previous database,\nespecially the lack of protein structure data, most of the existing\ncomputational methods rely heavily on the sequence data, with only a small\nportion of the methods utilizing the structural information. Recently,\nAlphaFold has revolutionized the entire protein and biology field. Foreseeably,\nthe protein-RNA interaction prediction will also be promoted significantly in\nthe upcoming years. In this work, we give a thorough review of this field,\nsurveying both the binding site and binding preference prediction problems and\ncovering the commonly used datasets, features, and models. We also point out\nthe potential challenges and opportunities in this field. This survey\nsummarizes the development of the RBP-RNA interaction field in the past and\nforesees its future development in the post-AlphaFold era.",
          "link": "http://arxiv.org/abs/2107.12243",
          "publishedOn": "2021-07-27T02:03:34.772Z",
          "wordCount": 591,
          "title": "Protein-RNA interaction prediction with deep learning: Structure matters. (arXiv:2107.12243v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Preethi_P/0/1/0/all/0/1\">P Preethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1\">Hrishikesh Viswanath</a>",
          "description": "This paper is a presentation of a new method for denoising images using\nHaralick features and further segmenting the characters using artificial neural\nnetworks. The image is divided into kernels, each of which is converted to a\nGLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation\nfunction is called, the result of which is an array with fourteen elements\ncorresponding to fourteen features The Haralick values and the corresponding\nnoise/text classification form a dictionary, which is then used to de-noise the\nimage through kernel comparison. Segmentation is the process of extracting\ncharacters from a document and can be used when letters are separated by white\nspace, which is an explicit boundary marker. Segmentation is the first step in\nmany Natural Language Processing problems. This paper explores the process of\nsegmentation using Neural Networks. While there have been numerous methods to\nsegment characters of a document, this paper is only concerned with the\naccuracy of doing so using neural networks. It is imperative that the\ncharacters be segmented correctly, for failing to do so will lead to incorrect\nrecognition by Natural language processing tools. Artificial Neural Networks\nwas used to attain accuracy of upto 89%. This method is suitable for languages\nwhere the characters are delimited by white space. However, this method will\nfail to provide acceptable results when the language heavily uses connected\nletters. An example would be the Devanagari script, which is predominantly used\nin northern India.",
          "link": "http://arxiv.org/abs/2107.11801",
          "publishedOn": "2021-07-27T02:03:34.764Z",
          "wordCount": 677,
          "title": "Denoising and Segmentation of Epigraphical Scripts. (arXiv:2107.11801v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Susheel Kumar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_J/0/1/0/all/0/1\">Jagdish Chand Bansal</a>",
          "description": "In the binary search space, GSA framework encounters the shortcomings of\nstagnation, diversity loss, premature convergence and high time complexity. To\naddress these issues, a novel binary variant of GSA called `A novel\nneighbourhood archives embedded gravitational constant in GSA for binary search\nspace (BNAGGSA)' is proposed in this paper. In BNAGGSA, the novel\nfitness-distance based social interaction strategy produces a self-adaptive\nstep size mechanism through which the agent moves towards the optimal direction\nwith the optimal step size, as per its current search requirement. The\nperformance of the proposed algorithm is compared with the two binary variants\nof GSA over 23 well-known benchmark test problems. The experimental results and\nstatistical analyses prove the supremacy of BNAGGSA over the compared\nalgorithms. Furthermore, to check the applicability of the proposed algorithm\nin solving real-world applications, a windfarm layout optimization problem is\nconsidered. Two case studies with two different wind data sets of two different\nwind sites is considered for experiments.",
          "link": "http://arxiv.org/abs/2107.11844",
          "publishedOn": "2021-07-27T02:03:34.758Z",
          "wordCount": 621,
          "title": "A binary variant of gravitational search algorithm and its application to windfarm layout optimization problem. (arXiv:2107.11844v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1\">Jay Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>",
          "description": "Deep learning-based models are developed to automatically detect if a retina\nimage is `referable' in diabetic retinopathy (DR) screening. However, their\nclassification accuracy degrades as the input images distributionally shift\nfrom their training distribution. Further, even if the input is not a retina\nimage, a standard DR classifier produces a high confident prediction that the\nimage is `referable'. Our paper presents a Dirichlet Prior Network-based\nframework to address this issue. It utilizes an out-of-distribution (OOD)\ndetector model and a DR classification model to improve generalizability by\nidentifying OOD images. Experiments on real-world datasets indicate that the\nproposed framework can eliminate the unknown non-retina images and identify the\ndistributionally shifted retina images for human intervention.",
          "link": "http://arxiv.org/abs/2107.11822",
          "publishedOn": "2021-07-27T02:03:34.751Z",
          "wordCount": 560,
          "title": "Distributional Shifts in Automated Diabetic Retinopathy Screening. (arXiv:2107.11822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nordby_J/0/1/0/all/0/1\">Jon Nordby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemazi_F/0/1/0/all/0/1\">Fabian Nemazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieber_D/0/1/0/all/0/1\">Dag Rieber</a>",
          "description": "Outdoor shooting ranges are subject to noise regulations from local and\nnational authorities. Restrictions found in these regulations may include\nlimits on times of activities, the overall number of noise events, as well as\nlimits on number of events depending on the class of noise or activity. A noise\nmonitoring system may be used to track overall sound levels, but rarely provide\nthe ability to detect activity or count the number of events, required to\ncompare directly with such regulations. This work investigates the feasibility\nand performance of an automatic detection system to count noise events. An\nempirical evaluation was done by collecting data at a newly constructed\nshooting range and training facility. The data includes tests of multiple\nweapon configurations from small firearms to high caliber rifles and\nexplosives, at multiple source positions, and collected on multiple different\ndays. Several alternative machine learning models are tested, using as inputs\ntime-series of standard acoustic indicators such as A-weighted sound levels and\n1/3 octave spectrogram, and classifiers such as Logistic Regression and\nConvolutional Neural Networks. Performance for the various alternatives are\nreported in terms of the False Positive Rate and False Negative Rate. The\ndetection performance was found to be satisfactory for use in automatic logging\nof time-periods with training activity.",
          "link": "http://arxiv.org/abs/2107.11453",
          "publishedOn": "2021-07-27T02:03:34.745Z",
          "wordCount": 663,
          "title": "Automatic Detection Of Noise Events at Shooting Range Using Machine Learning. (arXiv:2107.11453v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Chun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Le Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shaoming Song</a>",
          "description": "Although federated learning (FL) has recently been proposed for efficient\ndistributed training and data privacy protection, it still encounters many\nobstacles. One of these is the naturally existing statistical heterogeneity\namong clients, making local data distributions non independently and\nidentically distributed (i.e., non-iid), which poses challenges for model\naggregation and personalization. For FL with a deep neural network (DNN),\nprivatizing some layers is a simple yet effective solution for non-iid\nproblems. However, which layers should we privatize to facilitate the learning\nprocess? Do different categories of non-iid scenes have preferred privatization\nways? Can we automatically learn the most appropriate privatization way during\nFL? In this paper, we answer these questions via abundant experimental studies\non several FL benchmarks. First, we present the detailed statistics of these\nbenchmarks and categorize them into covariate and label shift non-iid scenes.\nThen, we investigate both coarse-grained and fine-grained network splits and\nexplore whether the preferred privatization ways have any potential relations\nto the specific category of a non-iid scene. Our findings are exciting, e.g.,\nprivatizing the base layers could boost the performances even in label shift\nnon-iid scenes, which are inconsistent with some natural conjectures. We also\nfind that none of these privatization ways could improve the performances on\nthe Shakespeare benchmark, and we guess that Shakespeare may not be a seriously\nnon-iid scene. Finally, we propose several approaches to automatically learn\nwhere to aggregate via cross-stitch, soft attention, and hard selection. We\nadvocate the proposed methods could serve as a preliminary try to explore where\nto privatize for a novel non-iid scene.",
          "link": "http://arxiv.org/abs/2107.11954",
          "publishedOn": "2021-07-27T02:03:34.722Z",
          "wordCount": 709,
          "title": "Aggregate or Not? Exploring Where to Privatize in DNN Based Federated Learning Under Different Non-IID Scenes. (arXiv:2107.11954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Um_S/0/1/0/all/0/1\">Se-Yun Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sangshin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_K/0/1/0/all/0/1\">Kyungguen Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hong-Goo Kang</a>",
          "description": "In this paper, we propose an effective method to synthesize speaker-specific\nspeech waveforms by conditioning on videos of an individual's face. Using a\ngenerative adversarial network (GAN) with linguistic and speaker characteristic\nfeatures as auxiliary conditions, our method directly converts face images into\nspeech waveforms under an end-to-end training framework. The linguistic\nfeatures are extracted from lip movements using a lip-reading model, and the\nspeaker characteristic features are predicted from face images using\ncross-modal learning with a pre-trained acoustic model. Since these two\nfeatures are uncorrelated and controlled independently, we can flexibly\nsynthesize speech waveforms whose speaker characteristics vary depending on the\ninput face images. Therefore, our method can be regarded as a multi-speaker\nface-to-speech waveform model. We show the superiority of our proposed model\nover conventional methods in terms of both objective and subjective evaluation\nresults. Specifically, we evaluate the performances of the linguistic feature\nand the speaker characteristic generation modules by measuring the accuracy of\nautomatic speech recognition and automatic speaker/gender recognition tasks,\nrespectively. We also evaluate the naturalness of the synthesized speech\nwaveforms using a mean opinion score (MOS) test.",
          "link": "http://arxiv.org/abs/2107.12003",
          "publishedOn": "2021-07-27T02:03:34.706Z",
          "wordCount": 646,
          "title": "Facetron: Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11678",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shang_R/0/1/0/all/0/1\">Ruibo Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OBrien_M/0/1/0/all/0/1\">Mikaela A. O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luke_G/0/1/0/all/0/1\">Geoffrey P. Luke</a>",
          "description": "Single-pixel imaging (SPI) has the advantages of high-speed acquisition over\na broad wavelength range and system compactness, which are difficult to achieve\nby conventional imaging sensors. However, a common challenge is low image\nquality arising from undersampling. Deep learning (DL) is an emerging and\npowerful tool in computational imaging for many applications and researchers\nhave applied DL in SPI to achieve higher image quality than conventional\nreconstruction approaches. One outstanding challenge, however, is that the\naccuracy of DL predictions in SPI cannot be assessed in practical applications\nwhere the ground truths are unknown. Here, we propose the use of the Bayesian\nconvolutional neural network (BCNN) to approximate the uncertainty (coming from\nfinite training data and network model) of the DL predictions in SPI. Each\npixel in the predicted result from BCNN represents the parameter of a\nprobability distribution rather than the image intensity value. Then, the\nuncertainty can be approximated with BCNN by minimizing a negative\nlog-likelihood loss function in the training stage and Monte Carlo dropout in\nthe prediction stage. The results show that the BCNN can reliably approximate\nthe uncertainty of the DL predictions in SPI with varying compression ratios\nand noise levels. The predicted uncertainty from BCNN in SPI reveals that most\nof the reconstruction errors in deep-learning-based SPI come from the edges of\nthe image features. The results show that the proposed BCNN can provide a\nreliable tool to approximate the uncertainty of DL predictions in SPI and can\nbe widely used in many applications of SPI.",
          "link": "http://arxiv.org/abs/2107.11678",
          "publishedOn": "2021-07-27T02:03:34.669Z",
          "wordCount": 695,
          "title": "Deep-learning-driven Reliable Single-pixel Imaging with Uncertainty Approximation. (arXiv:2107.11678v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>",
          "description": "One-class novelty detection is to identify anomalous instances that do not\nconform to the expected normal instances. In this paper, the Generative\nAdversarial Networks (GANs) based on encoder-decoder-encoder pipeline are used\nfor detection and achieve state-of-the-art performance. However, deep neural\nnetworks are too over-parameterized to deploy on resource-limited devices.\nTherefore, Progressive Knowledge Distillation with GANs (PKDGAN) is proposed to\nlearn compact and fast novelty detection networks. The P-KDGAN is a novel\nattempt to connect two standard GANs by the designed distillation loss for\ntransferring knowledge from the teacher to the student. The progressive\nlearning of knowledge distillation is a two-step approach that continuously\nimproves the performance of the student GAN and achieves better performance\nthan single step methods. In the first step, the student GAN learns the basic\nknowledge totally from the teacher via guiding of the pretrained teacher GAN\nwith fixed weights. In the second step, joint fine-training is adopted for the\nknowledgeable teacher and student GANs to further improve the performance and\nstability. The experimental results on CIFAR-10, MNIST, and FMNIST show that\nour method improves the performance of the student GAN by 2.44%, 1.77%, and\n1.73% when compressing the computation at ratios of 24.45:1, 311.11:1, and\n700:1, respectively.",
          "link": "http://arxiv.org/abs/2007.06963",
          "publishedOn": "2021-07-27T02:03:34.662Z",
          "wordCount": 674,
          "title": "P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection. (arXiv:2007.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Maojun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guangxu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiamo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Caijun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "The popular federated edge learning (FEEL) framework allows\nprivacy-preserving collaborative model training via frequent learning-updates\nexchange between edge devices and server. Due to the constrained bandwidth,\nonly a subset of devices can upload their updates at each communication round.\nThis has led to an active research area in FEEL studying the optimal device\nscheduling policy for minimizing communication time. However, owing to the\ndifficulty in quantifying the exact communication time, prior work in this area\ncan only tackle the problem partially by considering either the communication\nrounds or per-round latency, while the total communication time is determined\nby both metrics. To close this gap, we make the first attempt in this paper to\nformulate and solve the communication time minimization problem. We first\nderive a tight bound to approximate the communication time through\ncross-disciplinary effort involving both learning theory for convergence\nanalysis and communication theory for per-round latency analysis. Building on\nthe analytical result, an optimized probabilistic scheduling policy is derived\nin closed-form by solving the approximate communication time minimization\nproblem. It is found that the optimized policy gradually turns its priority\nfrom suppressing the remaining communication rounds to reducing per-round\nlatency as the training process evolves. The effectiveness of the proposed\nscheme is demonstrated via a use case on collaborative 3D objective detection\nin autonomous driving.",
          "link": "http://arxiv.org/abs/2107.11588",
          "publishedOn": "2021-07-27T02:03:34.640Z",
          "wordCount": 684,
          "title": "Accelerating Federated Edge Learning via Optimized Probabilistic Device Scheduling. (arXiv:2107.11588v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuezihan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Zeang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xupeng Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "Graph neural networks (GNNs) have been widely used in many graph-based tasks\nsuch as node classification, link prediction, and node clustering. However,\nGNNs gain their performance benefits mainly from performing the feature\npropagation and smoothing across the edges of the graph, thus requiring\nsufficient connectivity and label information for effective propagation.\nUnfortunately, many real-world networks are sparse in terms of both edges and\nlabels, leading to sub-optimal performance of GNNs. Recent interest in this\nsparse problem has focused on the self-training approach, which expands\nsupervised signals with pseudo labels. Nevertheless, the self-training approach\ninherently cannot realize the full potential of refining the learning\nperformance on sparse graphs due to the unsatisfactory quality and quantity of\npseudo labels.\n\nIn this paper, we propose ROD, a novel reception-aware online knowledge\ndistillation approach for sparse graph learning. We design three supervision\nsignals for ROD: multi-scale reception-aware graph knowledge, task-based\nsupervision, and rich distilled knowledge, allowing online knowledge transfer\nin a peer-teaching manner. To extract knowledge concealed in the multi-scale\nreception fields, ROD explicitly requires individual student models to preserve\ndifferent levels of locality information. For a given task, each student would\npredict based on its reception-scale knowledge, while simultaneously a strong\nteacher is established on-the-fly by combining multi-scale knowledge. Our\napproach has been extensively evaluated on 9 datasets and a variety of\ngraph-based tasks, including node classification, link prediction, and node\nclustering. The result demonstrates that ROD achieves state-of-art performance\nand is more robust for the graph sparsity.",
          "link": "http://arxiv.org/abs/2107.11789",
          "publishedOn": "2021-07-27T02:03:34.633Z",
          "wordCount": 687,
          "title": "ROD: Reception-aware Online Distillation for Sparse Graphs. (arXiv:2107.11789v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krause_D/0/1/0/all/0/1\">Daniel Aleksander Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Politis_A/0/1/0/all/0/1\">Archontis Politis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesaros_A/0/1/0/all/0/1\">Annamaria Mesaros</a>",
          "description": "Sound source proximity and distance estimation are of great interest in many\npractical applications, since they provide significant information for acoustic\nscene analysis. As both tasks share complementary qualities, ensuring efficient\ninteraction between these two is crucial for a complete picture of an aural\nenvironment. In this paper, we aim to investigate several ways of performing\njoint proximity and direction estimation from binaural recordings, both defined\nas coarse classification problems based on Deep Neural Networks (DNNs).\nConsidering the limitations of binaural audio, we propose two methods of\nsplitting the sphere into angular areas in order to obtain a set of directional\nclasses. For each method we study different model types to acquire information\nabout the direction-of-arrival (DoA). Finally, we propose various ways of\ncombining the proximity and direction estimation problems into a joint task\nproviding temporal information about the onsets and offsets of the appearing\nsources. Experiments are performed for a synthetic reverberant binaural dataset\nconsisting of up to two overlapping sound events.",
          "link": "http://arxiv.org/abs/2107.12033",
          "publishedOn": "2021-07-27T02:03:34.622Z",
          "wordCount": 611,
          "title": "Joint Direction and Proximity Classification of Overlapping Sound Events from Binaural Audio. (arXiv:2107.12033v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dionelis_N/0/1/0/all/0/1\">Nikolaos Dionelis</a>",
          "description": "Generative Adversarial Networks (GAN) are a powerful methodology and can be\nused for unsupervised anomaly detection, where current techniques have\nlimitations such as the accurate detection of anomalies near the tail of a\ndistribution. GANs generally do not guarantee the existence of a probability\ndensity and are susceptible to mode collapse, while few GANs use likelihood to\nreduce mode collapse. In this paper, we create a GAN-based tail formation model\nfor anomaly detection, the Tail of distribution GAN (TailGAN), to generate\nsamples on the tail of the data distribution and detect anomalies near the\nsupport boundary. Using TailGAN, we leverage GANs for anomaly detection and use\nmaximum entropy regularization. Using GANs that learn the probability of the\nunderlying distribution has advantages in improving the anomaly detection\nmethodology by allowing us to devise a generator for boundary samples, and use\nthis model to characterize anomalies. TailGAN addresses supports with disjoint\ncomponents and achieves competitive performance on images. We evaluate TailGAN\nfor identifying Out-of-Distribution (OoD) data and its performance evaluated on\nMNIST, CIFAR-10, Baggage X-Ray, and OoD data shows competitiveness compared to\nmethods from the literature.",
          "link": "http://arxiv.org/abs/2107.11658",
          "publishedOn": "2021-07-27T02:03:34.616Z",
          "wordCount": 635,
          "title": "Tail of Distribution GAN (TailGAN): Generative- Adversarial-Network-Based Boundary Formation. (arXiv:2107.11658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhuoqing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shi Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>",
          "description": "In this work, we consider the decentralized optimization problem in which a\nnetwork of $n$ agents, each possessing a smooth and convex objective function,\nwish to collaboratively minimize the average of all the objective functions\nthrough peer-to-peer communication in a directed graph. To solve the problem,\nwe propose two accelerated Push-DIGing methods termed APD and APD-SC for\nminimizing non-strongly convex objective functions and strongly convex ones,\nrespectively. We show that APD and APD-SC respectively converge at the rates\n$O\\left(\\frac{1}{k^2}\\right)$ and $O\\left(\\left(1 -\nC\\sqrt{\\frac{\\mu}{L}}\\right)^k\\right)$ up to constant factors depending only on\nthe mixing matrix. To the best of our knowledge, APD and APD-SC are the first\ndecentralized methods to achieve provable acceleration over unbalanced directed\ngraphs. Numerical experiments demonstrate the effectiveness of both methods.",
          "link": "http://arxiv.org/abs/2107.12065",
          "publishedOn": "2021-07-27T02:03:34.607Z",
          "wordCount": 583,
          "title": "Provably Accelerated Decentralized Gradient Method Over Unbalanced Directed Graphs. (arXiv:2107.12065v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michelucci_U/0/1/0/all/0/1\">Umberto Michelucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperti_M/0/1/0/all/0/1\">Michela Sperti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piga_D/0/1/0/all/0/1\">Dario Piga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venturini_F/0/1/0/all/0/1\">Francesca Venturini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deriu_M/0/1/0/all/0/1\">Marco A. Deriu</a>",
          "description": "This paper presents the intrinsic limit determination algorithm (ILD\nAlgorithm), a novel technique to determine the best possible performance,\nmeasured in terms of the AUC (area under the ROC curve) and accuracy, that can\nbe obtained from a specific dataset in a binary classification problem with\ncategorical features {\\sl regardless} of the model used. This limit, namely the\nBayes error, is completely independent of any model used and describes an\nintrinsic property of the dataset. The ILD algorithm thus provides important\ninformation regarding the prediction limits of any binary classification\nalgorithm when applied to the considered dataset. In this paper the algorithm\nis described in detail, its entire mathematical framework is presented and the\npseudocode is given to facilitate its implementation. Finally, an example with\na real dataset is given.",
          "link": "http://arxiv.org/abs/2107.11609",
          "publishedOn": "2021-07-27T02:03:34.590Z",
          "wordCount": 579,
          "title": "A Model-Agnostic Algorithm for Bayes Error Determination in Binary Classification. (arXiv:2107.11609v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kadeethum_T/0/1/0/all/0/1\">T. Kadeethum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballarin_F/0/1/0/all/0/1\">F. Ballarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Y. Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OMalley_D/0/1/0/all/0/1\">D. O&#x27;Malley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">H. Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouklas_N/0/1/0/all/0/1\">N. Bouklas</a>",
          "description": "Natural convection in porous media is a highly nonlinear multiphysical\nproblem relevant to many engineering applications (e.g., the process of\n$\\mathrm{CO_2}$ sequestration). Here, we present a non-intrusive reduced order\nmodel of natural convection in porous media employing deep convolutional\nautoencoders for the compression and reconstruction and either radial basis\nfunction (RBF) interpolation or artificial neural networks (ANNs) for mapping\nparameters of partial differential equations (PDEs) on the corresponding\nnonlinear manifolds. To benchmark our approach, we also describe linear\ncompression and reconstruction processes relying on proper orthogonal\ndecomposition (POD) and ANNs. We present comprehensive comparisons among\ndifferent models through three benchmark problems. The reduced order models,\nlinear and nonlinear approaches, are much faster than the finite element model,\nobtaining a maximum speed-up of $7 \\times 10^{6}$ because our framework is not\nbound by the Courant-Friedrichs-Lewy condition; hence, it could deliver\nquantities of interest at any given time contrary to the finite element model.\nOur model's accuracy still lies within a mean squared error of 0.07 (two-order\nof magnitude lower than the maximum value of the finite element results) in the\nworst-case scenario. We illustrate that, in specific settings, the nonlinear\napproach outperforms its linear counterpart and vice versa. We hypothesize that\na visual comparison between principal component analysis (PCA) or t-Distributed\nStochastic Neighbor Embedding (t-SNE) could indicate which method will perform\nbetter prior to employing any specific compression strategy.",
          "link": "http://arxiv.org/abs/2107.11460",
          "publishedOn": "2021-07-27T02:03:34.584Z",
          "wordCount": 697,
          "title": "Non-intrusive reduced order modeling of natural convection in porous media using convolutional autoencoders: comparison with linear subspace techniques. (arXiv:2107.11460v1 [cs.CE])"
        },
        {
          "id": "http://arxiv.org/abs/2005.02077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Angione_C/0/1/0/all/0/1\">Claudio Angione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silverman_E/0/1/0/all/0/1\">Eric Silverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaneske_E/0/1/0/all/0/1\">Elisabeth Yaneske</a>",
          "description": "In this proof-of-concept work, we evaluate the performance of multiple\nmachine-learning methods as statistical emulators for use in the analysis of\nagent-based models (ABMs). Analysing ABM outputs can be challenging, as the\nrelationships between input parameters can be non-linear or even chaotic even\nin relatively simple models, and each model run can require significant CPU\ntime. Statistical emulation, in which a statistical model of the ABM is\nconstructed to facilitate detailed model analyses, has been proposed as an\nalternative to computationally costly Monte Carlo methods. Here we compare\nmultiple machine-learning methods for ABM emulation in order to determine the\napproaches best suited to emulating the complex behaviour of ABMs. Our results\nsuggest that, in most scenarios, artificial neural networks (ANNs) and\ngradient-boosted trees outperform Gaussian process emulators, currently the\nmost commonly used method for the emulation of complex computational models.\nANNs produced the most accurate model replications in scenarios with high\nnumbers of model runs, although training times were longer than the other\nmethods. We propose that agent-based modelling would benefit from using\nmachine-learning methods for emulation, as this can facilitate more robust\nsensitivity analyses for the models while also reducing CPU time consumption\nwhen calibrating and analysing the simulation.",
          "link": "http://arxiv.org/abs/2005.02077",
          "publishedOn": "2021-07-27T02:03:34.563Z",
          "wordCount": 664,
          "title": "Using Machine Learning to Emulate Agent-Based Simulations. (arXiv:2005.02077v2 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11740",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1\">Chongcan Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cong_Y/0/1/0/all/0/1\">Yong Cong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_W/0/1/0/all/0/1\">Weihua Deng</a>",
          "description": "We preprocess the raw NMR spectrum and extract key characteristic features by\nusing two different methodologies, called equidistant sampling and peak\nsampling for subsequent substructure pattern recognition; meanwhile may provide\nthe alternative strategy to address the imbalance issue of the NMR dataset\nfrequently encountered in dataset collection of statistical modeling and\nestablish two conventional SVM and KNN models to assess the capability of two\nfeature selection, respectively. Our results in this study show that the models\nusing the selected features of peak sampling outperform the ones using the\nother. Then we build the Recurrent Neural Network (RNN) model trained by Data B\ncollected from peak sampling. Furthermore, we illustrate the easier\noptimization of hyper parameters and the better generalization ability of the\nRNN deep learning model by comparison with traditional machine learning SVM and\nKNN models in detail.",
          "link": "http://arxiv.org/abs/2107.11740",
          "publishedOn": "2021-07-27T02:03:34.491Z",
          "wordCount": 591,
          "title": "Identifying the fragment structure of the organic compounds by deeply learning the original NMR data. (arXiv:2107.11740v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bo Ji</a>",
          "description": "In this paper, we study the problem of fair worker selection in Federated\nLearning systems, where fairness serves as an incentive mechanism that\nencourages more workers to participate in the federation. Considering the\nachieved training accuracy of the global model as the utility of the selected\nworkers, which is typically a monotone submodular function, we formulate the\nworker selection problem as a new multi-round monotone submodular maximization\nproblem with cardinality and fairness constraints. The objective is to maximize\nthe time-average utility over multiple rounds subject to an additional fairness\nrequirement that each worker must be selected for a certain fraction of time.\nWhile the traditional submodular maximization with a cardinality constraint is\nalready a well-known NP-Hard problem, the fairness constraint in the\nmulti-round setting adds an extra layer of difficulty. To address this novel\nchallenge, we propose three algorithms: Fair Continuous Greedy (FairCG1 and\nFairCG2) and Fair Discrete Greedy (FairDG), all of which satisfy the fairness\nrequirement whenever feasible. Moreover, we prove nontrivial lower bounds on\nthe achieved time-average utility under FairCG1 and FairCG2. In addition, by\ngiving a higher priority to fairness, FairDG ensures a stronger short-term\nfairness guarantee, which holds in every round. Finally, we perform extensive\nsimulations to verify the effectiveness of the proposed algorithms in terms of\nthe time-average utility and fairness satisfaction.",
          "link": "http://arxiv.org/abs/2107.11728",
          "publishedOn": "2021-07-27T02:03:34.483Z",
          "wordCount": 672,
          "title": "Federated Learning with Fair Worker Selection: A Multi-Round Submodular Maximization Approach. (arXiv:2107.11728v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11662",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yongxin Chen</a>",
          "description": "We consider inference problems for a class of continuous state collective\nhidden Markov models, where the data is recorded in aggregate (collective) form\ngenerated by a large population of individuals following the same dynamics. We\npropose an aggregate inference algorithm called collective Gaussian\nforward-backward algorithm, extending recently proposed Sinkhorn belief\npropagation algorithm to models characterized by Gaussian densities. Our\nalgorithm enjoys convergence guarantee. In addition, it reduces to the standard\nKalman filter when the observations are generated by a single individual. The\nefficacy of the proposed algorithm is demonstrated through multiple\nexperiments.",
          "link": "http://arxiv.org/abs/2107.11662",
          "publishedOn": "2021-07-27T02:03:34.477Z",
          "wordCount": 531,
          "title": "Inference of collective Gaussian hidden Markov models. (arXiv:2107.11662v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1\">Chandrajit Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Avik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoran Zhang</a>",
          "description": "Variational Autoencoders (VAEs) have been shown to be remarkably effective in\nrecovering model latent spaces for several computer vision tasks. However,\ncurrently trained VAEs, for a number of reasons, seem to fall short in learning\ninvariant and equivariant clusters in latent space. Our work focuses on\nproviding solutions to this problem and presents an approach to disentangle\nequivariance feature maps in a Lie group manifold by enforcing deep,\ngroup-invariant learning. Simultaneously implementing a novel separation of\nsemantic and equivariant variables of the latent space representation, we\nformulate a modified Evidence Lower BOund (ELBO) by using a mixture model pdf\nlike Gaussian mixtures for invariant cluster embeddings that allows superior\nunsupervised variational clustering. Our experiments show that this model\neffectively learns to disentangle the invariant and equivariant representations\nwith significant improvements in the learning rate and an observably superior\nimage recognition and canonical state reconstruction compared to the currently\nbest deep learning models.",
          "link": "http://arxiv.org/abs/2107.11717",
          "publishedOn": "2021-07-27T02:03:34.470Z",
          "wordCount": 598,
          "title": "Invariance-based Multi-Clustering of Latent Space Embeddings for Equivariant Learning. (arXiv:2107.11717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yeli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Easwaran_A/0/1/0/all/0/1\">Arvind Easwaran</a>",
          "description": "Highly complex deep learning models are increasingly integrated into modern\ncyber-physical systems (CPS), many of which have strict safety requirements.\nOne problem arising from this is that deep learning lacks interpretability,\noperating as a black box. The reliability of deep learning is heavily impacted\nby how well the model training data represents runtime test data, especially\nwhen the input space dimension is high as natural images. In response, we\npropose a robust out-of-distribution (OOD) detection framework. Our approach\ndetects unusual movements from driving video in real-time by combining\nclassical optic flow operation with representation learning via variational\nautoencoder (VAE). We also design a method to locate OOD factors in images.\nEvaluation on a driving simulation data set shows that our approach is\nstatistically more robust than related works.",
          "link": "http://arxiv.org/abs/2107.11736",
          "publishedOn": "2021-07-27T02:03:34.463Z",
          "wordCount": 564,
          "title": "WiP Abstract : Robust Out-of-distribution Motion Detection and Localization in Autonomous CPS. (arXiv:2107.11736v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biza_O/0/1/0/all/0/1\">Ondrej Biza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pol_E/0/1/0/all/0/1\">Elise van der Pol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>",
          "description": "World models trained by contrastive learning are a compelling alternative to\nautoencoder-based world models, which learn by reconstructing pixel states. In\nthis paper, we describe three cases where small changes in how we sample\nnegative states in the contrastive loss lead to drastic changes in model\nperformance. In previously studied Atari datasets, we show that leveraging time\nstep correlations can double the performance of the Contrastive Structured\nWorld Model. We also collect a full version of the datasets to study\ncontrastive learning under a more diverse set of experiences.",
          "link": "http://arxiv.org/abs/2107.11676",
          "publishedOn": "2021-07-27T02:03:34.446Z",
          "wordCount": 538,
          "title": "The Impact of Negative Sampling on Contrastive Structured World Models. (arXiv:2107.11676v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abou_Kreisha_M/0/1/0/all/0/1\">Mohamed Taha Abou-Kreisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elnashar_H/0/1/0/all/0/1\">Hany Elnashar</a>",
          "description": "Automated Vehicle License Plate (VLP) detection and recognition have ended up\nbeing a significant research issue as of late. VLP localization and recognition\nare some of the most essential techniques for managing traffic using digital\ntechniques. In this paper, four smart systems are developed to recognize\nEgyptian vehicles license plates. Two systems are based on character\nrecognition, which are (System1, Characters Recognition with Classical Machine\nLearning) and (System2, Characters Recognition with Deep Machine Learning). The\nother two systems are based on the whole plate recognition which are (System3,\nWhole License Plate Recognition with Classical Machine Learning) and (System4,\nWhole License Plate Recognition with Deep Machine Learning). We use object\ndetection algorithms, and machine learning based object recognition algorithms.\nThe performance of the developed systems has been tested on real images, and\nthe experimental results demonstrate that the best detection accuracy rate for\nVLP is provided by using the deep learning method. Where the VLP detection\naccuracy rate is better than the classical system by 32%. However, the best\ndetection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is\nprovided by using the classical method. Where VLPAC detection accuracy rate is\nbetter than the deep learning-based system by 6%. Also, the results show that\ndeep learning is better than the classical technique used in VLP recognition\nprocesses. Where the recognition accuracy rate is better than the classical\nsystem by 8%. Finally, the paper output recommends a robust VLP recognition\nsystem based on both statistical and deep machine learning.",
          "link": "http://arxiv.org/abs/2107.11640",
          "publishedOn": "2021-07-27T02:03:34.436Z",
          "wordCount": 722,
          "title": "Deep Machine Learning Based Egyptian Vehicle License Plate Recognition Systems. (arXiv:2107.11640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1\">Liu Ziyin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Botao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masahito Ueda</a>",
          "description": "Stochastic gradient descent (SGD) has been deployed to solve highly\nnon-linear and non-convex machine learning problems such as the training of\ndeep neural networks. However, previous works on SGD often rely on highly\nrestrictive and unrealistic assumptions about the nature of noise in SGD. In\nthis work, we mathematically construct examples that defy previous\nunderstandings of SGD. For example, our constructions show that: (1) SGD may\nconverge to a local maximum; (2) SGD may escape a saddle point arbitrarily\nslowly; (3) SGD may prefer sharp minima over the flat ones; and (4) AMSGrad may\nconverge to a local maximum. Our result suggests that the noise structure of\nSGD might be more important than the loss landscape in neural network training\nand that future research should focus on deriving the actual noise structure in\ndeep learning.",
          "link": "http://arxiv.org/abs/2107.11774",
          "publishedOn": "2021-07-27T02:03:34.380Z",
          "wordCount": 570,
          "title": "SGD May Never Escape Saddle Points. (arXiv:2107.11774v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>",
          "description": "Although Graph Convolutional Networks (GCNs) have demonstrated their power in\nvarious applications, the graph convolutional layers, as the most important\ncomponent of GCN, are still using linear transformations and a simple pooling\nstep. In this paper, we propose a novel generalization of Factorized Bilinear\n(FB) layer to model the feature interactions in GCNs. FB performs two\nmatrix-vector multiplications, that is, the weight matrix is multiplied with\nthe outer product of the vector of hidden features from both sides. However,\nthe FB layer suffers from the quadratic number of coefficients, overfitting and\nthe spurious correlations due to correlations between channels of hidden\nrepresentations that violate the i.i.d. assumption. Thus, we propose a compact\nFB layer by defining a family of summarizing operators applied over the\nquadratic term. We analyze proposed pooling operators and motivate their use.\nOur experimental results on multiple datasets demonstrate that the GFB-GCN is\ncompetitive with other methods for text classification.",
          "link": "http://arxiv.org/abs/2107.11666",
          "publishedOn": "2021-07-27T02:03:34.352Z",
          "wordCount": 588,
          "title": "Graph Convolutional Network with Generalized Factorized Bilinear Aggregation. (arXiv:2107.11666v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruoxuan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1\">Allison Koenecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1\">Michael Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1\">Susan Athey</a>",
          "description": "Analyzing observational data from multiple sources can be useful for\nincreasing statistical power to detect a treatment effect; however, practical\nconstraints such as privacy considerations may restrict individual-level\ninformation sharing across data sets. This paper develops federated methods\nthat only utilize summary-level information from heterogeneous data sets. Our\nfederated methods provide doubly-robust point estimates of treatment effects as\nwell as variance estimates. We derive the asymptotic distributions of our\nfederated estimators, which are shown to be asymptotically equivalent to the\ncorresponding estimators from the combined, individual-level data. We show that\nto achieve these properties, federated methods should be adjusted based on\nconditions such as whether models are correctly specified and stable across\nheterogeneous data sets.",
          "link": "http://arxiv.org/abs/2107.11732",
          "publishedOn": "2021-07-27T02:03:34.332Z",
          "wordCount": 561,
          "title": "Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1\">Haoyi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zheyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>",
          "description": "How to explore corner cases as efficiently and thoroughly as possible has\nlong been one of the top concerns in the context of deep reinforcement learning\n(DeepRL) autonomous driving. Training with simulated data is less costly and\ndangerous than utilizing real-world data, but the inconsistency of parameter\ndistribution and the incorrect system modeling in simulators always lead to an\ninevitable Sim2real gap, which probably accounts for the underperformance in\nnovel, anomalous and risky cases that simulators can hardly generate. Domain\nRandomization(DR) is a methodology that can bridge this gap with little or no\nreal-world data. Consequently, in this research, an adversarial model is put\nforward to robustify DeepRL-based autonomous vehicles trained in simulation to\ngradually surfacing harder events, so that the models could readily transfer to\nthe real world.",
          "link": "http://arxiv.org/abs/2107.11762",
          "publishedOn": "2021-07-27T02:03:34.325Z",
          "wordCount": 579,
          "title": "DR2L: Surfacing Corner Cases to Robustify Autonomous Driving via Domain Randomization Reinforcement Learning. (arXiv:2107.11762v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07179",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Zhai_H/0/1/0/all/0/1\">Hanfeng Zhai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hu_G/0/1/0/all/0/1\">Guohui Hu</a>",
          "description": "Micro-bubbles and bubbly flows are widely observed and applied in chemical\nengineering, medicine, involves deformation, rupture, and collision of bubbles,\nphase mixture, etc. We study bubble dynamics by setting up two numerical\nsimulation cases: bubbly flow with a single bubble and multiple bubbles, both\nconfined in the microchannel, with parameters corresponding to their medical\nbackgrounds. Both the cases have their medical background applications.\nMultiphase flow simulation requires high computation accuracy due to possible\ncomponent losses that may be caused by sparse meshing during the computation.\nHence, data-driven methods can be adopted as an useful tool. Based on\nphysics-informed neural networks (PINNs), we propose a novel deep learning\nframework BubbleNet, which entails three main parts: deep neural networks (DNN)\nwith sub nets for predicting different physics fields; the\nsemi-physics-informed part, with only the fluid continuum condition and the\npressure Poisson equation $\\mathcal{P}$ encoded within; the time discretized\nnormalizer (TDN), an algorithm to normalize field data per time step before\ntraining. We apply the traditional DNN and our BubbleNet to train the coarsened\nsimulation data and predict the physics fields of both the two bubbly flow\ncases. The BubbleNets are trained for both with and without $\\mathcal{P}$, from\nwhich we conclude that the 'physics-informed' part can serve as inner\nsupervision. Results indicate our framework can predict the physics fields more\naccurately, estimating the prediction absolute errors. Our deep learning\npredictions outperform traditional numerical methods computed with similar data\ndensity meshing. The proposed network can potentially be applied to many other\nengineering fields.",
          "link": "http://arxiv.org/abs/2105.07179",
          "publishedOn": "2021-07-27T02:03:34.109Z",
          "wordCount": 714,
          "title": "BubbleNet: Inferring micro-bubble dynamics with semi-physics-informed deep learning. (arXiv:2105.07179v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shih-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "Transformers and variational autoencoders (VAE) have been extensively\nemployed for symbolic (e.g., MIDI) domain music generation. While the former\nboast an impressive capability in modeling long sequences, the latter allow\nusers to willingly exert control over different parts (e.g., bars) of the music\nto be generated. In this paper, we are interested in bringing the two together\nto construct a single model that exhibits both strengths. The task is split\ninto two steps. First, we equip Transformer decoders with the ability to accept\nsegment-level, time-varying conditions during sequence generation.\nSubsequently, we combine the developed and tested in-attention decoder with a\nTransformer encoder, and train the resulting MuseMorphose model with the VAE\nobjective to achieve style transfer of long musical pieces, in which users can\nspecify musical attributes including rhythmic intensity and polyphony (i.e.,\nharmonic fullness) they desire, down to the bar level. Experiments show that\nMuseMorphose outperforms recurrent neural network (RNN) based baselines on\nnumerous widely-used metrics for style transfer tasks.",
          "link": "http://arxiv.org/abs/2105.04090",
          "publishedOn": "2021-07-27T02:03:34.059Z",
          "wordCount": 639,
          "title": "MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with One Transformer VAE. (arXiv:2105.04090v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "Generalization performance of stochastic optimization stands a central place\nin learning theory. In this paper, we investigate the excess risk performance\nand towards improved learning rates for two popular approaches of stochastic\noptimization: empirical risk minimization (ERM) and stochastic gradient descent\n(SGD). Although there exists plentiful generalization analysis of ERM and SGD\nfor supervised learning, current theoretical understandings of ERM and SGD\neither have stronger assumptions in convex learning, e.g., strong convexity, or\nshow slow rates and less studied in nonconvex learning. Motivated by these\nproblems, we aim to provide improved rates under milder assumptions in convex\nlearning and derive faster rates in nonconvex learning. It is notable that our\nanalysis span two popular theoretical viewpoints: \\emph{stability} and\n\\emph{uniform convergence}. Specifically, in stability regime, we present high\nprobability learning rates of order $\\mathcal{O} (1/n)$ w.r.t. the sample size\n$n$ for ERM and SGD with milder assumptions in convex learning and similar high\nprobability rates of order $\\mathcal{O} (1/n)$ in nonconvex learning, rather\nthan in expectation. Furthermore, this type of learning rate is improved to\nfaster order $\\mathcal{O} (1/n^2)$ in uniform convergence regime. To our best\nknowledge, for ERM and SGD, the learning rates presented in this paper are all\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2107.08686",
          "publishedOn": "2021-07-27T02:03:34.046Z",
          "wordCount": 657,
          "title": "Improved Learning Rates for Stochastic Optimization: Two Theoretical Viewpoints. (arXiv:2107.08686v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>",
          "description": "We study the challenging task of neural network quantization without\nend-to-end retraining, called Post-training Quantization (PTQ). PTQ usually\nrequires a small subset of training data but produces less powerful quantized\nmodels than Quantization-Aware Training (QAT). In this work, we propose a novel\nPTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to\nINT2 for the first time. BRECQ leverages the basic building blocks in neural\nnetworks and reconstructs them one-by-one. In a comprehensive theoretical study\nof the second-order error, we show that BRECQ achieves a good balance between\ncross-layer dependency and generalization error. To further employ the power of\nquantization, the mixed precision technique is incorporated in our framework by\napproximating the inter-layer and intra-layer sensitivity. Extensive\nexperiments on various handcrafted and searched neural architectures are\nconducted for both image classification and object detection tasks. And for the\nfirst time we prove that, without bells and whistles, PTQ can attain 4-bit\nResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster\nproduction of quantized models. Codes are available at\nhttps://github.com/yhhhli/BRECQ.",
          "link": "http://arxiv.org/abs/2102.05426",
          "publishedOn": "2021-07-27T02:03:34.040Z",
          "wordCount": 658,
          "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction. (arXiv:2102.05426v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1\">Francesco D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>",
          "description": "Deep ensembles have recently gained popularity in the deep learning community\nfor their conceptual simplicity and efficiency. However, maintaining functional\ndiversity between ensemble members that are independently trained with gradient\ndescent is challenging. This can lead to pathologies when adding more ensemble\nmembers, such as a saturation of the ensemble performance, which converges to\nthe performance of a single model. Moreover, this does not only affect the\nquality of its predictions, but even more so the uncertainty estimates of the\nensemble, and thus its performance on out-of-distribution data. We hypothesize\nthat this limitation can be overcome by discouraging different ensemble members\nfrom collapsing to the same function. To this end, we introduce a kernelized\nrepulsive term in the update rule of the deep ensembles. We show that this\nsimple modification not only enforces and maintains diversity among the members\nbut, even more importantly, transforms the maximum a posteriori inference into\nproper Bayesian inference. Namely, we show that the training dynamics of our\nproposed repulsive ensembles follow a Wasserstein gradient flow of the KL\ndivergence with the true posterior. We study repulsive terms in weight and\nfunction space and empirically compare their performance to standard ensembles\nand Bayesian baselines on synthetic and real-world prediction tasks.",
          "link": "http://arxiv.org/abs/2106.11642",
          "publishedOn": "2021-07-27T02:03:34.022Z",
          "wordCount": 655,
          "title": "Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Bi-Level Optimization (BLO) is originated from the area of economic game\ntheory and then introduced into the optimization community. BLO is able to\nhandle problems with a hierarchical structure, involving two levels of\noptimization tasks, where one task is nested inside the other. In machine\nlearning and computer vision fields, despite the different motivations and\nmechanisms, a lot of complex problems, such as hyper-parameter optimization,\nmulti-task and meta-learning, neural architecture search, adversarial learning\nand deep reinforcement learning, actually all contain a series of closely\nrelated subproblms. In this paper, we first uniformly express these complex\nlearning and vision problems from the perspective of BLO. Then we construct a\nbest-response-based single-level reformulation and establish a unified\nalgorithmic framework to understand and formulate mainstream gradient-based BLO\nmethodologies, covering aspects ranging from fundamental automatic\ndifferentiation schemes to various accelerations, simplifications, extensions\nand their convergence and complexity properties. Last but not least, we discuss\nthe potentials of our unified BLO framework for designing new algorithms and\npoint out some promising directions for future research.",
          "link": "http://arxiv.org/abs/2101.11517",
          "publishedOn": "2021-07-27T02:03:34.015Z",
          "wordCount": 664,
          "title": "Investigating Bi-Level Optimization for Learning and Vision from a Unified Perspective: A Survey and Beyond. (arXiv:2101.11517v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Visca_M/0/1/0/all/0/1\">Marco Visca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1\">Sampo Kuutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1\">Roger Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>",
          "description": "Terrain traversability analysis plays a major role in ensuring safe robotic\nnavigation in unstructured environments. However, real-time constraints\nfrequently limit the accuracy of online tests especially in scenarios where\nrealistic robot-terrain interactions are complex to model. In this context, we\npropose a deep learning framework trained in an end-to-end fashion from\nelevation maps and trajectories to estimate the occurrence of failure events.\nThe network is first trained and tested in simulation over synthetic maps\ngenerated by the OpenSimplex algorithm. The prediction performance of the Deep\nLearning framework is illustrated by being able to retain over 94% recall of\nthe original simulator at 30% of the computational time. Finally, the network\nis transferred and tested on real elevation maps collected by the SEEKER\nconsortium during the Martian rover test trial in the Atacama desert in Chile.\nWe show that transferring and fine-tuning of an application-independent\npre-trained model retains better performance than training uniquely on scarcely\navailable real data.",
          "link": "http://arxiv.org/abs/2105.10937",
          "publishedOn": "2021-07-27T02:03:34.008Z",
          "wordCount": 639,
          "title": "Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments. (arXiv:2105.10937v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of\nReinforcement Learning due to the non-stationarity of the environments and the\nlarge dimensionality of the combined action space. Deep MARL algorithms have\nbeen applied to solve different task offloading problems. However, in\nreal-world applications, information required by the agents (i.e. rewards and\nstates) are subject to noise and alterations. The stability and the robustness\nof deep MARL to practical challenges is still an open research problem. In this\nwork, we apply state-of-the art MARL algorithms to solve task offloading with\nreward uncertainty. We show that perturbations in the reward signal can induce\ndecrease in the performance compared to learning with perfect rewards. We\nexpect this paper to stimulate more research in studying and addressing the\npractical challenges of deploying deep MARL solutions in wireless\ncommunications systems.",
          "link": "http://arxiv.org/abs/2107.08114",
          "publishedOn": "2021-07-27T02:03:34.002Z",
          "wordCount": 585,
          "title": "Decentralized Multi-Agent Reinforcement Learning for Task Offloading Under Uncertainty. (arXiv:2107.08114v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper concentrates on the approximation power of deep feed-forward\nneural networks in terms of width and depth. It is proved by construction that\nReLU networks with width $\\mathcal{O}\\big(\\max\\{d\\lfloor N^{1/d}\\rfloor,\\,\nN+2\\}\\big)$ and depth $\\mathcal{O}(L)$ can approximate a H\\\"older continuous\nfunction on $[0,1]^d$ with an approximation rate\n$\\mathcal{O}\\big(\\lambda\\sqrt{d} (N^2L^2\\ln N)^{-\\alpha/d}\\big)$, where\n$\\alpha\\in (0,1]$ and $\\lambda>0$ are H\\\"older order and constant,\nrespectively. Such a rate is optimal up to a constant in terms of width and\ndepth separately, while existing results are only nearly optimal without the\nlogarithmic factor in the approximation rate. More generally, for an arbitrary\ncontinuous function $f$ on $[0,1]^d$, the approximation rate becomes\n$\\mathcal{O}\\big(\\,\\sqrt{d}\\,\\omega_f\\big( (N^2L^2\\ln N)^{-1/d}\\big)\\,\\big)$,\nwhere $\\omega_f(\\cdot)$ is the modulus of continuity. We also extend our\nanalysis to any continuous function $f$ on a bounded set. Particularly, if ReLU\nnetworks with depth $31$ and width $\\mathcal{O}(N)$ are used to approximate\none-dimensional Lipschitz continuous functions on $[0,1]$ with a Lipschitz\nconstant $\\lambda>0$, the approximation rate in terms of the total number of\nparameters, $W=\\mathcal{O}(N^2)$, becomes $\\mathcal{O}(\\tfrac{\\lambda}{W\\ln\nW})$, which has not been discovered in the literature for fixed-depth ReLU\nnetworks.",
          "link": "http://arxiv.org/abs/2103.00502",
          "publishedOn": "2021-07-27T02:03:33.994Z",
          "wordCount": 677,
          "title": "Optimal Approximation Rate of ReLU Networks in terms of Width and Depth. (arXiv:2103.00502v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lixin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhanxing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation,\nCriticism, and Application Trend of Explainable AI. Deep neural networks (DNNs)\nhave undoubtedly brought great success to a wide range of applications in\ncomputer vision, computational linguistics, and AI. However, foundational\nprinciples underlying the DNNs' success and their resilience to adversarial\nattacks are still largely missing. Interpreting and theorizing the internal\nmechanisms of DNNs becomes a compelling yet controversial topic. This workshop\npays a special interest in theoretic foundations, limitations, and new\napplication trends in the scope of XAI. These issues reflect new bottlenecks in\nthe future development of XAI.",
          "link": "http://arxiv.org/abs/2107.08821",
          "publishedOn": "2021-07-27T02:03:33.974Z",
          "wordCount": 584,
          "title": "Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. (arXiv:2107.08821v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-07-27T02:03:33.967Z",
          "wordCount": 596,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1\">Larisa Markeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1\">Daniel Keysers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1\">Ilya Tolstikhin</a>",
          "description": "We introduce a generalization to the lottery ticket hypothesis in which the\nnotion of \"sparsity\" is relaxed by choosing an arbitrary basis in the space of\nparameters. We present evidence that the original results reported for the\ncanonical basis continue to hold in this broader setting. We describe how\nstructured pruning methods, including pruning units or factorizing\nfully-connected layers into products of low-rank matrices, can be cast as\nparticular instances of this \"generalized\" lottery ticket hypothesis. The\ninvestigations reported here are preliminary and are provided to encourage\nfurther research along this direction.",
          "link": "http://arxiv.org/abs/2107.06825",
          "publishedOn": "2021-07-27T02:03:33.960Z",
          "wordCount": 571,
          "title": "A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying-Jun Angela Zhang</a>",
          "description": "We study over-the-air model aggregation in federated edge learning (FEEL)\nsystems, where channel state information at the transmitters (CSIT) is assumed\nto be unavailable. We leverage the reconfigurable intelligent surface (RIS)\ntechnology to align the cascaded channel coefficients for CSIT-free model\naggregation. To this end, we jointly optimize the RIS and the receiver by\nminimizing the aggregation error under the channel alignment constraint. We\nthen develop a difference-of-convex algorithm for the resulting non-convex\noptimization. Numerical experiments on image classification show that the\nproposed method is able to achieve a similar learning accuracy as the\nstate-of-the-art CSIT-based solution, demonstrating the efficiency of our\napproach in combating the lack of CSIT.",
          "link": "http://arxiv.org/abs/2102.10749",
          "publishedOn": "2021-07-27T02:03:33.952Z",
          "wordCount": 631,
          "title": "CSIT-Free Model Aggregation for Federated Edge Learning via Reconfigurable Intelligent Surface. (arXiv:2102.10749v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schutz_N/0/1/0/all/0/1\">Narayan Sch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botros_A/0/1/0/all/0/1\">Angela Botros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Single_M/0/1/0/all/0/1\">Michael Single</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naef_A/0/1/0/all/0/1\">Aileen C. Naef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buluschek_P/0/1/0/all/0/1\">Philipp Buluschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nef_T/0/1/0/all/0/1\">Tobias Nef</a>",
          "description": "Sensor technologies are becoming increasingly prevalent in the biomedical\nfield, with applications ranging from telemonitoring of people at risk, to\nusing sensor derived information as objective endpoints in clinical trials. To\nfully utilize sensor information, signals from distinct sensors often have to\nbe temporally aligned. However, due to imperfect oscillators and significant\nnoise, commonly encountered with biomedical signals, temporal alignment of raw\nsignals is an all but trivial problem, with, to-date, no generally applicable\nsolution. In this work, we present Deep Canonical Correlation Alignment (DCCA),\na novel, generally applicable solution for the temporal alignment of raw\n(biomedical) sensor signals. DCCA allows practitioners to directly align raw\nsignals, from distinct sensors, without requiring deep domain knowledge. On a\nselection of artificial and real datasets, we demonstrate the performance and\nutility of DCCA under a variety of conditions. We compare the DCCA algorithm to\nother warping based methods, DCCA outperforms dynamic time warping and cross\ncorrelation based methods by an order of magnitude in terms of alignment error.\nDCCA performs especially well on almost periodic biomedical signals such as\nheart-beats and breathing patterns. In comparison to existing approaches, that\nare not tailored towards raw sensor data, DCCA is not only fast enough to work\non signals with billions of data points but also provides automatic filtering\nand transformation functionalities, allowing it to deal with very noisy and\neven morphologically distinct signals.",
          "link": "http://arxiv.org/abs/2106.03637",
          "publishedOn": "2021-07-27T02:03:33.945Z",
          "wordCount": 691,
          "title": "Deep Canonical Correlation Alignment for Sensor Signals. (arXiv:2106.03637v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We consider nonconvex-concave minimax problems, $\\min_{\\mathbf{x}}\n\\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\mathbf{x}, \\mathbf{y})$, where $f$ is\nnonconvex in $\\mathbf{x}$ but concave in $\\mathbf{y}$ and $\\mathcal{Y}$ is a\nconvex and bounded set. One of the most popular algorithms for solving this\nproblem is the celebrated gradient descent ascent (GDA) algorithm, which has\nbeen widely used in machine learning, control theory and economics. Despite the\nextensive convergence results for the convex-concave setting, GDA with equal\nstepsize can converge to limit cycles or even diverge in a general setting. In\nthis paper, we present the complexity results on two-time-scale GDA for solving\nnonconvex-concave minimax problems, showing that the algorithm can find a\nstationary point of the function $\\Phi(\\cdot) := \\max_{\\mathbf{y} \\in\n\\mathcal{Y}} f(\\cdot, \\mathbf{y})$ efficiently. To the best our knowledge, this\nis the first nonasymptotic analysis for two-time-scale GDA in this setting,\nshedding light on its superior practical performance in training generative\nadversarial networks (GANs) and other real applications.",
          "link": "http://arxiv.org/abs/1906.00331",
          "publishedOn": "2021-07-27T02:03:33.938Z",
          "wordCount": 684,
          "title": "On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guozhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Cheng Kevin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_P/0/1/0/all/0/1\">Pulin Gong</a>",
          "description": "Learning in deep neural networks (DNNs) is implemented through minimizing a\nhighly non-convex loss function, typically by a stochastic gradient descent\n(SGD) method. This learning process can effectively find good wide minima\nwithout being trapped in poor local ones. We present a novel account of how\nsuch effective deep learning emerges through the interactions of the SGD and\nthe geometrical structure of the loss landscape. Rather than being a normal\ndiffusion process (i.e. Brownian motion) as often assumed, we find that the SGD\nexhibits rich, complex dynamics when navigating through the loss landscape;\ninitially, the SGD exhibits anomalous superdiffusion, which attenuates\ngradually and changes to subdiffusion at long times when the solution is\nreached. Such learning dynamics happen ubiquitously in different DNNs such as\nResNet and VGG-like networks and are insensitive to batch size and learning\nrate. The anomalous superdiffusion process during the initial learning phase\nindicates that the motion of SGD along the loss landscape possesses\nintermittent, big jumps; this non-equilibrium property enables the SGD to\nescape from sharp local minima. By adapting the methods developed for studying\nenergy landscapes in complex physical systems, we find that such superdiffusive\nlearning dynamics are due to the interactions of the SGD and the fractal-like\nstructure of the loss landscape. We further develop a simple model to\ndemonstrate the mechanistic role of the fractal loss landscape in enabling the\nSGD to effectively find global minima. Our results thus reveal the\neffectiveness of deep learning from a novel perspective and have implications\nfor designing efficient deep neural networks.",
          "link": "http://arxiv.org/abs/2009.10588",
          "publishedOn": "2021-07-27T02:03:33.919Z",
          "wordCount": 732,
          "title": "Anomalous diffusion dynamics of learning in deep neural networks. (arXiv:2009.10588v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.08260",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zellinger_W/0/1/0/all/0/1\">Werner Zellinger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moser_B/0/1/0/all/0/1\">Bernhard A Moser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saminger_Platz_S/0/1/0/all/0/1\">Susanne Saminger-Platz</a>",
          "description": "Domain adaptation algorithms are designed to minimize the misclassification\nrisk of a discriminative model for a target domain with little training data by\nadapting a model from a source domain with a large amount of training data.\nStandard approaches measure the adaptation discrepancy based on distance\nmeasures between the empirical probability distributions in the source and\ntarget domain. In this setting, we address the problem of deriving\ngeneralization bounds under practice-oriented general conditions on the\nunderlying probability distributions. As a result, we obtain generalization\nbounds for domain adaptation based on finitely many moments and smoothness\nconditions.",
          "link": "http://arxiv.org/abs/2002.08260",
          "publishedOn": "2021-07-27T02:03:33.901Z",
          "wordCount": 569,
          "title": "On generalization in moment-based domain adaptation. (arXiv:2002.08260v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_I/0/1/0/all/0/1\">Icaro O. de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_K/0/1/0/all/0/1\">Keiko V. O. Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "This work addresses the problem of vehicle identification through\nnon-overlapping cameras. As our main contribution, we introduce a novel dataset\nfor vehicle identification, called Vehicle-Rear, that contains more than three\nhours of high-resolution videos, with accurate information about the make,\nmodel, color and year of nearly 3,000 vehicles, in addition to the position and\nidentification of their license plates. To explore our dataset we design a\ntwo-stream CNN that simultaneously uses two of the most distinctive and\npersistent features available: the vehicle's appearance and its license plate.\nThis is an attempt to tackle a major problem: false alarms caused by vehicles\nwith similar designs or by very close license plate identifiers. In the first\nnetwork stream, shape similarities are identified by a Siamese CNN that uses a\npair of low-resolution vehicle patches recorded by two different cameras. In\nthe second stream, we use a CNN for OCR to extract textual information,\nconfidence scores, and string similarities from a pair of high-resolution\nlicense plate patches. Then, features from both streams are merged by a\nsequence of fully connected layers for decision. In our experiments, we\ncompared the two-stream network against several well-known CNN architectures\nusing single or multiple vehicle features. The architectures, trained models,\nand dataset are publicly available at https://github.com/icarofua/vehicle-rear.",
          "link": "http://arxiv.org/abs/1911.05541",
          "publishedOn": "2021-07-27T02:03:33.894Z",
          "wordCount": 721,
          "title": "Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks. (arXiv:1911.05541v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>",
          "description": "Generative adversarial networks (GANs) nowadays are capable of producing\nimages of incredible realism. One concern raised is whether the\nstate-of-the-art GAN's learned distribution still suffers from mode collapse,\nand what to do if so. Existing diversity tests of samples from GANs are usually\nconducted qualitatively on a small scale, and/or depends on the access to\noriginal training data as well as the trained model parameters. This paper\nexplores to diagnose GAN intra-mode collapse and calibrate that, in a novel\nblack-box setting: no access to training data, nor the trained model\nparameters, is assumed. The new setting is practically demanded, yet rarely\nexplored and significantly more challenging. As a first stab, we devise a set\nof statistical tools based on sampling, that can visualize, quantify, and\nrectify intra-mode collapse. We demonstrate the effectiveness of our proposed\ndiagnosis and calibration techniques, via extensive simulations and\nexperiments, on unconditional GAN image generation (e.g., face and vehicle).\nOur study reveals that the intra-mode collapse is still a prevailing problem in\nstate-of-the-art GANs and the mode collapse is diagnosable and calibratable in\nblack-box settings. Our codes are available at:\nhttps://github.com/VITA-Group/BlackBoxGANCollapse.",
          "link": "http://arxiv.org/abs/2107.12202",
          "publishedOn": "2021-07-27T02:03:33.750Z",
          "wordCount": 646,
          "title": "Black-Box Diagnosis and Calibration on GAN Intra-Mode Collapse: A Pilot Study. (arXiv:2107.12202v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>",
          "description": "When humans solve complex problems, they rarely come up with a decision\nright-away. Instead, they start with an intuitive decision, reflect upon it,\nspot mistakes, resolve contradictions and jump between different hypotheses.\nThus, they create a sequence of ideas and follow a train of thought that\nultimately reaches a conclusive decision. Contrary to this, today's neural\nclassification models are mostly trained to map an input to one single and\nfixed output. In this paper, we investigate how we can give models the\nopportunity of a second, third and $k$-th thought. We take inspiration from\nHegel's dialectics and propose a method that turns an existing classifier's\nclass prediction (such as the image class forest) into a sequence of\npredictions (such as forest $\\rightarrow$ tree $\\rightarrow$ mushroom).\nConcretely, we propose a correction module that is trained to estimate the\nmodel's correctness as well as an iterative prediction update based on the\nprediction's gradient. Our approach results in a dynamic system over class\nprobability distributions $\\unicode{x2014}$ the thought flow. We evaluate our\nmethod on diverse datasets and tasks from computer vision and natural language\nprocessing. We observe surprisingly complex but intuitive behavior and\ndemonstrate that our method (i) can correct misclassifications, (ii)\nstrengthens model performance, (iii) is robust to high levels of adversarial\nattacks, (iv) can increase accuracy up to 4% in a label-distribution-shift\nsetting and (iv) provides a tool for model interpretability that uncovers model\nknowledge which otherwise remains invisible in a single distribution\nprediction.",
          "link": "http://arxiv.org/abs/2107.12220",
          "publishedOn": "2021-07-27T02:03:33.743Z",
          "wordCount": 694,
          "title": "Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Chun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shaoming Song</a>",
          "description": "Automatically mining sentiment tendency contained in natural language is a\nfundamental research to some artificial intelligent applications, where\nsolutions alternate with challenges. Transfer learning and multi-task learning\ntechniques have been leveraged to mitigate the supervision sparsity and\ncollaborate multiple heterogeneous domains correspondingly. Recent years, the\nsensitive nature of users' private data raises another challenge for sentiment\nclassification, i.e., data privacy protection. In this paper, we resort to\nfederated learning for multiple domain sentiment classification under the\nconstraint that the corpora must be stored on decentralized devices. In view of\nthe heterogeneous semantics across multiple parties and the peculiarities of\nword embedding, we pertinently provide corresponding solutions. First, we\npropose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for\nbetter model aggregation and personalization in federated sentiment\nclassification. Second, we propose KTEPS$^\\star$ with the consideration of the\nrich semantic and huge embedding size properties of word vectors, utilizing\nProjection-based Dimension Reduction (PDR) methods for privacy protection and\nefficient transmission simultaneously. We propose two federated sentiment\nclassification scenes based on public benchmarks, and verify the superiorities\nof our proposed methods with abundant experimental investigations.",
          "link": "http://arxiv.org/abs/2107.11956",
          "publishedOn": "2021-07-27T02:03:33.704Z",
          "wordCount": 615,
          "title": "Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Arun Kumar Singh</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Priyanka Singh</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Nathwani_K/0/1/0/all/0/1\">Karan Nathwani</a> (1) ((1) Indian Institute of Technology Jammu, (2) Dhirubhai Ambani Institute of Information and Communication Technology)",
          "description": "The recent developments in technology have re-warded us with amazing audio\nsynthesis models like TACOTRON and WAVENETS. On the other side, it poses\ngreater threats such as speech clones and deep fakes, that may go undetected.\nTo tackle these alarming situations, there is an urgent need to propose models\nthat can help discriminate a synthesized speech from an actual human speech and\nalso identify the source of such a synthesis. Here, we propose a model based on\nConvolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network\n(BiRNN) that helps to achieve both the aforementioned objectives. The temporal\ndependencies present in AI synthesized speech are exploited using Bidirectional\nRNN and CNN. The model outperforms the state-of-the-art approaches by\nclassifying the AI synthesized audio from real human speech with an error rate\nof 1.9% and detecting the underlying architecture with an accuracy of 97%.",
          "link": "http://arxiv.org/abs/2107.11412",
          "publishedOn": "2021-07-27T02:03:33.549Z",
          "wordCount": 629,
          "title": "Using Deep Learning Techniques and Inferential Speech Statistics for AI Synthesised Speech Recognition. (arXiv:2107.11412v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chung-Hsuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_E/0/1/0/all/0/1\">Erik G. Larsson</a>",
          "description": "Federated Learning (FL) is a newly emerged decentralized machine learning\n(ML) framework that combines on-device local training with server-based model\nsynchronization to train a centralized ML model over distributed nodes. In this\npaper, we propose an asynchronous FL framework with periodic aggregation to\neliminate the straggler issue in FL systems. For the proposed model, we\ninvestigate several device scheduling and update aggregation policies and\ncompare their performances when the devices have heterogeneous computation\ncapabilities and training data distributions. From the simulation results, we\nconclude that the scheduling and aggregation design for asynchronous FL can be\nrather different from the synchronous case. For example, a norm-based\nsignificance-aware scheduling policy might not be efficient in an asynchronous\nFL setting, and an appropriate \"age-aware\" weighting design for the model\naggregation can greatly improve the learning performance of such systems.",
          "link": "http://arxiv.org/abs/2107.11415",
          "publishedOn": "2021-07-27T02:03:33.455Z",
          "wordCount": 607,
          "title": "Device Scheduling and Update Aggregation Policies for Asynchronous Federated Learning. (arXiv:2107.11415v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_K/0/1/0/all/0/1\">Keren Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masotto_X/0/1/0/all/0/1\">Xander Masotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachani_V/0/1/0/all/0/1\">Vandana Bachani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikodem_J/0/1/0/all/0/1\">Jack Nikodem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dong Yin</a>",
          "description": "We propose a simulation framework for generating realistic instance-dependent\nnoisy labels via a pseudo-labeling paradigm. We show that this framework\ngenerates synthetic noisy labels that exhibit important characteristics of the\nlabel noise in practical settings via comparison with the CIFAR10-H dataset.\nEquipped with controllable label noise, we study the negative impact of noisy\nlabels across a few realistic settings to understand when label noise is more\nproblematic. We also benchmark several existing algorithms for learning with\nnoisy labels and compare their behavior on our synthetic datasets and on the\ndatasets with independent random label noise. Additionally, with the\navailability of annotator information from our simulation framework, we propose\na new technique, Label Quality Model (LQM), that leverages annotator features\nto predict and correct against noisy labels. We show that by adding LQM as a\nlabel correction step before applying existing noisy label techniques, we can\nfurther improve the models' performance.",
          "link": "http://arxiv.org/abs/2107.11413",
          "publishedOn": "2021-07-27T02:03:33.417Z",
          "wordCount": 596,
          "title": "A Realistic Simulation Framework for Learning with Label Noise. (arXiv:2107.11413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11381",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Min_S/0/1/0/all/0/1\">Seonwoo Min</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lee_B/0/1/0/all/0/1\">Byunghan Lee</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "MicroRNAs (miRNAs) play pivotal roles in gene expression regulation by\nbinding to target sites of messenger RNAs (mRNAs). While identifying functional\ntargets of miRNAs is of utmost importance, their prediction remains a great\nchallenge. Previous computational algorithms have major limitations. They use\nconservative candidate target site (CTS) selection criteria mainly focusing on\ncanonical site types, rely on laborious and time-consuming manual feature\nextraction, and do not fully capitalize on the information underlying miRNA-CTS\ninteractions. In this paper, we introduce TargetNet, a novel deep\nlearning-based algorithm for functional miRNA target prediction. To address the\nlimitations of previous approaches, TargetNet has three key components: (1)\nrelaxed CTS selection criteria accommodating irregularities in the seed region,\n(2) a novel miRNA-CTS sequence encoding scheme incorporating extended seed\nregion alignments, and (3) a deep residual network-based prediction model. The\nproposed model was trained with miRNA-CTS pair datasets and evaluated with\nmiRNA-mRNA pair datasets. TargetNet advances the previous state-of-the-art\nalgorithms used in functional miRNA target classification. Furthermore, it\ndemonstrates great potential for distinguishing high-functional miRNA targets.",
          "link": "http://arxiv.org/abs/2107.11381",
          "publishedOn": "2021-07-27T02:03:33.388Z",
          "wordCount": 612,
          "title": "TargetNet: Functional microRNA Target Prediction with Deep Neural Networks. (arXiv:2107.11381v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_I/0/1/0/all/0/1\">Ian E. Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dera_D/0/1/0/all/0/1\">Dimah Dera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1\">Ravi P. Ramachandran</a>",
          "description": "With the rise of deep neural networks, the challenge of explaining the\npredictions of these networks has become increasingly recognized. While many\nmethods for explaining the decisions of deep neural networks exist, there is\ncurrently no consensus on how to evaluate them. On the other hand, robustness\nis a popular topic for deep learning research; however, it is hardly talked\nabout in explainability until very recently. In this tutorial paper, we start\nby presenting gradient-based interpretability methods. These techniques use\ngradient signals to assign the burden of the decision on the input features.\nLater, we discuss how gradient-based methods can be evaluated for their\nrobustness and the role that adversarial robustness plays in having meaningful\nexplanations. We also discuss the limitations of gradient-based methods.\nFinally, we present the best practices and attributes that should be examined\nbefore choosing an explainability method. We conclude with the future\ndirections for research in the area at the convergence of robustness and\nexplainability.",
          "link": "http://arxiv.org/abs/2107.11400",
          "publishedOn": "2021-07-27T02:03:33.329Z",
          "wordCount": 617,
          "title": "Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks. (arXiv:2107.11400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Arpita Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gaurav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1\">Pradeep Varakantham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1\">Milind Tambe</a>",
          "description": "In many public health settings, it is important for patients to adhere to\nhealth programs, such as taking medications and periodic health checks.\nUnfortunately, beneficiaries may gradually disengage from such programs, which\nis detrimental to their health. A concrete example of gradual disengagement has\nbeen observed by an organization that carries out a free automated call-based\nprogram for spreading preventive care information among pregnant women. Many\nwomen stop picking up calls after being enrolled for a few months. To avoid\nsuch disengagements, it is important to provide timely interventions. Such\ninterventions are often expensive and can be provided to only a small fraction\nof the beneficiaries. We model this scenario as a restless multi-armed bandit\n(RMAB) problem, where each beneficiary is assumed to transition from one state\nto another depending on the intervention. Moreover, since the transition\nprobabilities are unknown a priori, we propose a Whittle index based Q-Learning\nmechanism and show that it converges to the optimal solution. Our method\nimproves over existing learning-based methods for RMABs on multiple benchmarks\nfrom literature and also on the maternal healthcare dataset.",
          "link": "http://arxiv.org/abs/2105.07965",
          "publishedOn": "2021-07-26T02:01:00.229Z",
          "wordCount": 670,
          "title": "Learn to Intervene: An Adaptive Learning Policy for Restless Bandits in Application to Preventive Healthcare. (arXiv:2105.07965v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1\">Neil Band</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganshin/0/1/0/all/0/1\">Ganshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander/0/1/0/all/0/1\">Alexander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesnokov_G/0/1/0/all/0/1\">German Chesnokov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noskov_A/0/1/0/all/0/1\">Alexey Noskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploskonosov_A/0/1/0/all/0/1\">Andrey Ploskonosov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1\">Liudmila Prokhorenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1\">Ivan Provilkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roginskiy/0/1/0/all/0/1\">Roginskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis/0/1/0/all/0/1\">Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatova_M/0/1/0/all/0/1\">Mariya Shmatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1\">Panos Tigas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1\">Boris Yangel</a>",
          "description": "There has been significant research done on developing methods for improving\nrobustness to distributional shift and uncertainty estimation. In contrast,\nonly limited work has examined developing standard datasets and benchmarks for\nassessing these approaches. Additionally, most work on uncertainty estimation\nand robustness has developed new techniques based on small-scale regression or\nimage classification tasks. However, many tasks of practical interest have\ndifferent modalities, such as tabular data, audio, text, or sensor data, which\noffer significant challenges involving regression and discrete or continuous\nstructured prediction. Thus, given the current state of the field, a\nstandardized large-scale dataset of tasks across a range of modalities affected\nby distributional shifts is necessary. This will enable researchers to\nmeaningfully evaluate the plethora of recently developed uncertainty\nquantification methods, as well as assessment criteria and state-of-the-art\nbaselines. In this work, we propose the \\emph{Shifts Dataset} for evaluation of\nuncertainty estimates and robustness to distributional shift. The dataset,\nwhich has been collected from industrial sources and services, is composed of\nthree tasks, with each corresponding to a particular data modality: tabular\nweather prediction, machine translation, and self-driving car (SDC) vehicle\nmotion prediction. All of these data modalities and tasks are affected by real,\n`in-the-wild' distributional shifts and pose interesting challenges with\nrespect to uncertainty estimation. In this work we provide a description of the\ndataset and baseline results for all tasks.",
          "link": "http://arxiv.org/abs/2107.07455",
          "publishedOn": "2021-07-26T02:01:00.222Z",
          "wordCount": 714,
          "title": "Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. (arXiv:2107.07455v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12342",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Gotoh_J/0/1/0/all/0/1\">Jun-ya Gotoh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kim_M/0/1/0/all/0/1\">Michael Jong Kim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lim_A/0/1/0/all/0/1\">Andrew E.B. Lim</a>",
          "description": "While solutions of Distributionally Robust Optimization (DRO) problems can\nsometimes have a higher out-of-sample expected reward than the Sample Average\nApproximation (SAA), there is no guarantee. In this paper, we introduce the\nclass of Distributionally Optimistic Optimization (DOO) models, and show that\nit is always possible to \"beat\" SAA out-of-sample if we consider not just\nworst-case (DRO) models but also best-case (DOO) ones. We also show, however,\nthat this comes at a cost: Optimistic solutions are more sensitive to model\nerror than either worst-case or SAA optimizers, and hence are less robust.",
          "link": "http://arxiv.org/abs/2105.12342",
          "publishedOn": "2021-07-26T02:01:00.214Z",
          "wordCount": 576,
          "title": "A data-driven approach to beating SAA out-of-sample. (arXiv:2105.12342v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1\">Dobrik Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Franscesco Giannini</a>",
          "description": "\"PyTorch, Explain!\" is a Python module integrating a variety of\nstate-of-the-art approaches to provide logic explanations from neural networks.\nThis package focuses on bringing these methods to non-specialists. It has\nminimal dependencies and it is distributed under the Apache 2.0 licence\nallowing both academic and commercial use. Source code and documentation can be\ndownloaded from the github repository:\nhttps://github.com/pietrobarbiero/pytorch_explain.",
          "link": "http://arxiv.org/abs/2105.11697",
          "publishedOn": "2021-07-26T02:01:00.205Z",
          "wordCount": 522,
          "title": "PyTorch, Explain! A Python library for Logic Explained Networks. (arXiv:2105.11697v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05852",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Adiga_D/0/1/0/all/0/1\">Devaraja Adiga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_R/0/1/0/all/0/1\">Rishabh Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishna_A/0/1/0/all/0/1\">Amrith Krishna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>",
          "description": "Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the\nvarious linguistic peculiarities present in the language. The Sanskrit language\nis lexically productive, undergoes euphonic assimilation of phones at the word\nboundaries and exhibits variations in spelling conventions and in\npronunciations. In this work, we propose the first large scale study of\nautomatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact\nof unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR\ndataset for Sanskrit, which faithfully captures several of the linguistic\ncharacteristics expressed by the language. We investigate the role of different\nacoustic model and language model units in ASR systems for Sanskrit. We also\npropose a new modelling unit, inspired by the syllable level unit selection,\nthat captures character sequences from one vowel in the word to the next vowel.\nWe also highlight the importance of choosing graphemic representations for\nSanskrit and show the impact of this choice on word error rates (WER). Finally,\nwe extend these insights from Sanskrit ASR for building ASR systems in two\nother Indic languages, Gujarati and Telugu. For both these languages, our\nexperimental results show that the use of phonetic based graphemic\nrepresentations in ASR results in performance improvements as compared to ASR\nsystems that use native scripts.",
          "link": "http://arxiv.org/abs/2106.05852",
          "publishedOn": "2021-07-26T02:01:00.198Z",
          "wordCount": 708,
          "title": "Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights. (arXiv:2106.05852v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-07-26T02:01:00.174Z",
          "wordCount": 633,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadghiani_N/0/1/0/all/0/1\">Nima Salehi Sadghiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhuo Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew Cohen</a>",
          "description": "From the ad network standpoint, a user's activity is a multi-type sequence of\ntemporal events consisting of event types and time intervals. Understanding\nuser patterns in ad networks has received increasing attention from the machine\nlearning community. Particularly, the problems of fraud detection, Conversion\nRate (CVR), and Click-Through Rate (CTR) prediction are of interest. However,\nthe class imbalance between major and minor classes in these tasks can bias a\nmachine learning model leading to poor performance. This study proposes using\ntwo multi-type (continuous and discrete) training approaches for GANs to deal\nwith the limitations of traditional GANs in passing the gradient updates for\ndiscrete tokens. First, we used the Reinforcement Learning (RL)-based training\napproach and then, an approximation of the multinomial distribution\nparameterized in terms of the softmax function (Gumble-Softmax). Our extensive\nexperiments based on synthetic data have shown the trained generator can\ngenerate sequences with desired properties measured by multiple criteria.",
          "link": "http://arxiv.org/abs/2104.03428",
          "publishedOn": "2021-07-26T02:01:00.167Z",
          "wordCount": 607,
          "title": "Generating Multi-type Temporal Sequences to Mitigate Class-imbalanced Problem. (arXiv:2104.03428v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1\">Marian-Andrei Rizoiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soen_A/0/1/0/all/0/1\">Alexander Soen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shidi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderon_P/0/1/0/all/0/1\">Pio Calderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Leanne Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lexing Xie</a>",
          "description": "This work builds a novel point process and tools to use the Hawkes process\nwith interval-censored data. Such data records the aggregated counts of events\nsolely during specific time intervals -- such as the number of patients\nadmitted to the hospital or the volume of vehicles passing traffic loop\ndetectors -- and not the exact occurrence time of the events. First, we\nestablish the Mean Behavior Poisson (MBP) process, a novel Poisson process with\na direct parameter correspondence to the popular self-exciting Hawkes process.\nThe event intensity function of the MBP is the expected intensity over all\npossible Hawkes realizations with the same parameter set. We fit MBP in the\ninterval-censored setting using an interval-censored Poisson log-likelihood\n(IC-LL). We use the parameter equivalence to uncover the parameters of the\nassociated Hawkes process. Second, we introduce two novel exogenous functions\nto distinguish the exogenous from the endogenous events. We propose the\nmulti-impulse exogenous function when the exogenous events are observed as\nevent time and the latent homogeneous Poisson process exogenous function when\nthe exogenous events are presented as interval-censored volumes. Third, we\nprovide several approximation methods to estimate the intensity and compensator\nfunction of MBP when no analytical solution exists. Fourth and finally, we\nconnect the interval-censored loss of MBP to a broader class of Bregman\ndivergence-based functions. Using the connection, we show that the current\nstate of the art in popularity estimation (Hawkes Intensity Process (HIP)\n(Rizoiu et al.,2017b)) is a particular case of the MBP process. We verify our\nmodels through empirical testing on synthetic data and real-world data. We find\nthat on real-world datasets that our MBP process outperforms HIP for the task\nof popularity prediction.",
          "link": "http://arxiv.org/abs/2104.07932",
          "publishedOn": "2021-07-26T02:01:00.159Z",
          "wordCount": 755,
          "title": "Interval-censored Hawkes processes. (arXiv:2104.07932v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Global covariance pooling (GCP) aims at exploiting the second-order\nstatistics of the convolutional feature. Its effectiveness has been\ndemonstrated in boosting the classification performance of Convolutional Neural\nNetworks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute\nthe matrix square root. However, the approximate matrix square root calculated\nusing Newton-Schulz iteration \\cite{li2018towards} outperforms the accurate one\ncomputed via SVD \\cite{li2017second}. We empirically analyze the reason behind\nthe performance gap from the perspectives of data precision and gradient\nsmoothness. Various remedies for computing smooth SVD gradients are\ninvestigated. Based on our observation and analyses, a hybrid training protocol\nis proposed for SVD-based GCP meta-layers such that competitive performances\ncan be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP\nmeta-layer that uses SVD in the forward pass, and Pad\\'e Approximants in the\nbackward propagation to compute the gradients. The proposed meta-layer has been\nintegrated into different CNN models and achieves state-of-the-art performances\non both large-scale and fine-grained datasets.",
          "link": "http://arxiv.org/abs/2105.02498",
          "publishedOn": "2021-07-26T02:01:00.152Z",
          "wordCount": 639,
          "title": "Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?. (arXiv:2105.02498v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dingshuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zixuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuzhen Cheng</a>",
          "description": "Many real-world IoT systems, which include a variety of internet-connected\nsensory devices, produce substantial amounts of multivariate time series data.\nMeanwhile, vital IoT infrastructures like smart power grids and water\ndistribution networks are frequently targeted by cyber-attacks, making anomaly\ndetection an important study topic. Modeling such relatedness is, nevertheless,\nunavoidable for any efficient and effective anomaly detection system, given the\nintricate topological and nonlinear connections that are originally unknown\namong sensors. Furthermore, detecting anomalies in multivariate time series is\ndifficult due to their temporal dependency and stochasticity. This paper\npresented GTA, a new framework for multivariate time series anomaly detection\nthat involves automatically learning a graph structure, graph convolution, and\nmodeling temporal dependency using a Transformer-based architecture. The\nconnection learning policy, which is based on the Gumbel-softmax sampling\napproach to learn bi-directed links among sensors directly, is at the heart of\nlearning graph structure. To describe the anomaly information flow between\nnetwork nodes, we introduced a new graph convolution called Influence\nPropagation convolution. In addition, to tackle the quadratic complexity\nbarrier, we suggested a multi-branch attention mechanism to replace the\noriginal multi-head self-attention method. Extensive experiments on four\npublicly available anomaly detection benchmarks further demonstrate the\nsuperiority of our approach over alternative state-of-the-arts.",
          "link": "http://arxiv.org/abs/2104.03466",
          "publishedOn": "2021-07-26T02:01:00.144Z",
          "wordCount": 696,
          "title": "Learning Graph Structures with Transformer for Multivariate Time Series Anomaly Detection in IoT. (arXiv:2104.03466v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monperrus_M/0/1/0/all/0/1\">Martin Monperrus</a>",
          "description": "Semantic code search is about finding semantically relevant code snippets for\na given natural language query. In the state-of-the-art approaches, the\nsemantic similarity between code and query is quantified as the distance of\ntheir representation in the shared vector space. In this paper, to improve the\nvector space, we introduce tree-serialization methods on a simplified form of\nAST and build the multimodal representation for the code data. We conduct\nextensive experiments using a single corpus that is large-scale and\nmulti-language: CodeSearchNet. Our results show that both our tree-serialized\nrepresentations and multimodal learning model improve the performance of code\nsearch. Last, we define intuitive quantification metrics oriented to the\ncompleteness of semantic and syntactic information of the code data, to help\nunderstand the experimental findings.",
          "link": "http://arxiv.org/abs/2107.00992",
          "publishedOn": "2021-07-26T02:01:00.137Z",
          "wordCount": 580,
          "title": "Multimodal Representation for Neural Code Search. (arXiv:2107.00992v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.03148",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Reisinger_C/0/1/0/all/0/1\">Christoph Reisinger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Yufei Zhang</a>",
          "description": "This paper proposes a relaxed control regularization with general exploration\nrewards to design robust feedback controls for multi-dimensional\ncontinuous-time stochastic exit time problems. We establish that the\nregularized control problem admits a H\\\"{o}lder continuous feedback control,\nand demonstrate that both the value function and the feedback control of the\nregularized control problem are Lipschitz stable with respect to parameter\nperturbations. Moreover, we show that a pre-computed feedback relaxed control\nhas a robust performance in a perturbed system, and derive a first-order\nsensitivity equation for both the value function and optimal feedback relaxed\ncontrol. These stability results provide a theoretical justification for recent\nreinforcement learning heuristics that including an exploration reward in the\noptimization objective leads to more robust decision making. We finally prove\nfirst-order monotone convergence of the value functions for relaxed control\nproblems with vanishing exploration parameters, which subsequently enables us\nto construct the pure exploitation strategy of the original control problem\nbased on the feedback relaxed controls.",
          "link": "http://arxiv.org/abs/2001.03148",
          "publishedOn": "2021-07-26T02:01:00.113Z",
          "wordCount": 642,
          "title": "Regularity and stability of feedback relaxed controls. (arXiv:2001.03148v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochgesang_J/0/1/0/all/0/1\">Julie Hochgesang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>",
          "description": "Signed languages are the primary means of communication for many deaf and\nhard of hearing individuals. Since signed languages exhibit all the fundamental\nlinguistic properties of natural language, we believe that tools and theories\nof Natural Language Processing (NLP) are crucial towards its modeling. However,\nexisting research in Sign Language Processing (SLP) seldom attempt to explore\nand leverage the linguistic organization of signed languages. This position\npaper calls on the NLP community to include signed languages as a research area\nwith high social and scientific impact. We first discuss the linguistic\nproperties of signed languages to consider during their modeling. Then, we\nreview the limitations of current SLP models and identify the open challenges\nto extend NLP to signed languages. Finally, we urge (1) the adoption of an\nefficient tokenization method; (2) the development of linguistically-informed\nmodels; (3) the collection of real-world signed language data; (4) the\ninclusion of local signed language communities as an active and leading voice\nin the direction of research.",
          "link": "http://arxiv.org/abs/2105.05222",
          "publishedOn": "2021-07-26T02:01:00.106Z",
          "wordCount": 638,
          "title": "Including Signed Languages in Natural Language Processing. (arXiv:2105.05222v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08659",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1\">Aleksandr Beknazaryan</a>",
          "description": "In this paper it is shown that $C_\\beta$-smooth functions can be approximated\nby deep neural networks with ReLU activation function and with parameters\n$\\{0,\\pm \\frac{1}{2}, \\pm 1, 2\\}$. The $l_0$ and $l_1$ parameter norms of\nconsidered networks are thus equivalent. The depth, width and the number of\nactive parameters of the constructed networks have, up to a logarithmic factor,\nthe same dependence on the approximation error as the networks with parameters\nin $[-1,1]$. In particular, this means that the nonparametric regression\nestimation with the constructed networks attains the same convergence rate as\nwith sparse networks with parameters in $[-1,1]$.",
          "link": "http://arxiv.org/abs/2103.08659",
          "publishedOn": "2021-07-26T02:01:00.098Z",
          "wordCount": 563,
          "title": "Function approximation by deep neural networks with parameters $\\{0,\\pm \\frac{1}{2}, \\pm 1, 2\\}$. (arXiv:2103.08659v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastianello_N/0/1/0/all/0/1\">Nicola Bastianello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonetto_A/0/1/0/all/0/1\">Andrea Simonetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DallAnese_E/0/1/0/all/0/1\">Emiliano Dall&#x27;Anese</a>",
          "description": "This paper presents a new regularization approach -- termed OpReg-Boost -- to\nboost the convergence and lessen the asymptotic error of online optimization\nand learning algorithms. In particular, the paper considers online algorithms\nfor optimization problems with a time-varying (weakly) convex composite cost.\nFor a given online algorithm, OpReg-Boost learns the closest algorithmic map\nthat yields linear convergence; to this end, the learning procedure hinges on\nthe concept of operator regression. We show how to formalize the operator\nregression problem and propose a computationally-efficient Peaceman-Rachford\nsolver that exploits a closed-form solution of simple quadratically-constrained\nquadratic programs (QCQPs). Simulation results showcase the superior properties\nof OpReg-Boost w.r.t. the more classical forward-backward algorithm, FISTA, and\nAnderson acceleration, and with respect to its close relative\nconvex-regression-boost (CvxReg-Boost) which is also novel but less performing.",
          "link": "http://arxiv.org/abs/2105.13271",
          "publishedOn": "2021-07-26T02:01:00.075Z",
          "wordCount": 601,
          "title": "OpReg-Boost: Learning to Accelerate Online Algorithms with Operator Regression. (arXiv:2105.13271v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1\">Daniel J. Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1\">Erik Bollt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1\">Aaron Griffith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1\">Wendson A.S. Barbosa</a>",
          "description": "Reservoir computing is a best-in-class machine learning algorithm for\nprocessing information generated by dynamical systems using observed\ntime-series data. Importantly, it requires very small training data sets, uses\nlinear optimization, and thus requires minimal computing resources. However,\nthe algorithm uses randomly sampled matrices to define the underlying recurrent\nneural network and has a multitude of metaparameters that must be optimized.\nRecent results demonstrate the equivalence of reservoir computing to nonlinear\nvector autoregression, which requires no random matrices, fewer metaparameters,\nand provides interpretable results. Here, we demonstrate that nonlinear vector\nautoregression excels at reservoir computing benchmark tasks and requires even\nshorter training data sets and training time, heralding the next generation of\nreservoir computing.",
          "link": "http://arxiv.org/abs/2106.07688",
          "publishedOn": "2021-07-26T02:01:00.067Z",
          "wordCount": 589,
          "title": "Next Generation Reservoir Computing. (arXiv:2106.07688v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaakoubi_Y/0/1/0/all/0/1\">Yassine Yaakoubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soumis_F/0/1/0/all/0/1\">Fran&#xe7;ois Soumis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1\">Simon Lacoste-Julien</a>",
          "description": "Motivated by the needs from an airline crew scheduling application, we\nintroduce structured convolutional kernel networks (Struct-CKN), which combine\nCKNs from Mairal et al. (2014) in a structured prediction framework that\nsupports constraints on the outputs. CKNs are a particular kind of\nconvolutional neural networks that approximate a kernel feature map on training\ndata, thus combining properties of deep learning with the non-parametric\nflexibility of kernel methods. Extending CKNs to structured outputs allows us\nto obtain useful initial solutions on a flight-connection dataset that can be\nfurther refined by an airline crew scheduling solver. More specifically, we use\na flight-based network modeled as a general conditional random field capable of\nincorporating local constraints in the learning process. Our experiments\ndemonstrate that this approach yields significant improvements for the\nlarge-scale crew pairing problem (50,000 flights per month) over standard\napproaches, reducing the solution cost by 17% (a gain of millions of dollars)\nand the cost of global constraints by 97%.",
          "link": "http://arxiv.org/abs/2105.11646",
          "publishedOn": "2021-07-26T02:01:00.059Z",
          "wordCount": 627,
          "title": "Structured Convolutional Kernel Networks for Airline Crew Scheduling. (arXiv:2105.11646v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1\">Lukas Sch&#xe4;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1\">Filippos Christianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1\">Josiah Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1\">Stefano V. Albrecht</a>",
          "description": "Intrinsic rewards are commonly applied to improve exploration in\nreinforcement learning. However, these approaches suffer from instability\ncaused by non-stationary reward shaping and strong dependency on\nhyperparameters. In this work, we propose Decoupled RL (DeRL) which trains\nseparate policies for exploration and exploitation. DeRL can be applied with\non-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two\nsparse-reward environments with multiple types of intrinsic rewards. We show\nthat DeRL is more robust to scaling and speed of decay of intrinsic rewards and\nconverges to the same evaluation returns than intrinsically motivated baselines\nin fewer interactions.",
          "link": "http://arxiv.org/abs/2107.08966",
          "publishedOn": "2021-07-26T02:01:00.051Z",
          "wordCount": 563,
          "title": "Decoupling Exploration and Exploitation in Reinforcement Learning. (arXiv:2107.08966v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Han Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Federated learning has emerged as an important paradigm for training machine\nlearning models in different domains. For graph-level tasks such as graph\nclassification, graphs can also be regarded as a special type of data samples,\nwhich can be collected and stored in separate local systems. Similar to other\ndomains, multiple local systems, each holding a small set of graphs, may\nbenefit from collaboratively training a powerful graph mining model, such as\nthe popular graph neural networks (GNNs). To provide more motivation towards\nsuch endeavors, we analyze real-world graphs from different domains to confirm\nthat they indeed share certain graph properties that are statistically\nsignificant compared with random graphs. However, we also find that different\nsets of graphs, even from the same domain or same dataset, are non-IID\nregarding both graph structures and node features. To handle this, we propose a\ngraph clustered federated learning (GCFL) framework that dynamically finds\nclusters of local systems based on the gradients of GNNs, and theoretically\njustify that such clusters can reduce the structure and feature heterogeneity\namong graphs owned by the local systems. Moreover, we observe the gradients of\nGNNs to be rather fluctuating in GCFL which impedes high-quality clustering,\nand design a gradient sequence-based clustering mechanism based on dynamic time\nwarping (GCFL+). Extensive experimental results and in-depth analysis\ndemonstrate the effectiveness of our proposed frameworks.",
          "link": "http://arxiv.org/abs/2106.13423",
          "publishedOn": "2021-07-26T02:01:00.036Z",
          "wordCount": 692,
          "title": "Federated Graph Classification over Non-IID Graphs. (arXiv:2106.13423v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.10673",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ding_L/0/1/0/all/0/1\">Lijun Ding</a>, <a href=\"http://arxiv.org/find/math/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>",
          "description": "Low rank matrix recovery problems appear widely in statistics, combinatorics,\nand imaging. One celebrated method for solving these problems is to formulate\nand solve a semidefinite program (SDP). It is often known that the exact\nsolution to the SDP with perfect data recovers the solution to the original low\nrank matrix recovery problem. It is more challenging to show that an\napproximate solution to the SDP formulated with noisy problem data acceptably\nsolves the original problem; arguments are usually ad hoc for each problem\nsetting, and can be complex.\n\nIn this note, we identify a set of conditions that we call simplicity that\nlimit the error due to noisy problem data or incomplete convergence. In this\nsense, simple SDPs are robust: simple SDPs can be (approximately) solved\nefficiently at scale; and the resulting approximate solutions, even with noisy\ndata, can be trusted. Moreover, we show that simplicity holds generically, and\nalso for many structured low rank matrix recovery problems, including the\nstochastic block model, $\\mathbb{Z}_2$ synchronization, and matrix completion.\nFormally, we call an SDP simple if it has a surjective constraint map, admits a\nunique primal and dual solution pair, and satisfies strong duality and strict\ncomplementarity.\n\nHowever, simplicity is not a panacea: we show the Burer-Monteiro formulation\nof the SDP may have spurious second-order critical points, even for a simple\nSDP with a rank 1 solution.",
          "link": "http://arxiv.org/abs/2002.10673",
          "publishedOn": "2021-07-26T02:01:00.018Z",
          "wordCount": 693,
          "title": "On the simplicity and conditioning of low rank semidefinite programs. (arXiv:2002.10673v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.13001",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mckenzie_D/0/1/0/all/0/1\">Daniel Mckenzie</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenliang Zhang</a>",
          "description": "We consider the problem of minimizing a high-dimensional objective function,\nwhich may include a regularization term, using (possibly noisy) evaluations of\nthe function. Such optimization is also called derivative-free, zeroth-order,\nor black-box optimization. We propose a new $\\textbf{Z}$eroth-$\\textbf{O}$rder\n$\\textbf{R}$egularized $\\textbf{O}$ptimization method, dubbed ZORO. When the\nunderlying gradient is approximately sparse at an iterate, ZORO needs very few\nobjective function evaluations to obtain a new iterate that decreases the\nobjective function. We achieve this with an adaptive, randomized gradient\nestimator, followed by an inexact proximal-gradient scheme. Under a novel\napproximately sparse gradient assumption and various different convex settings,\nwe show the (theoretical and empirical) convergence rate of ZORO is only\nlogarithmically dependent on the problem dimension. Numerical experiments show\nthat ZORO outperforms the existing methods with similar assumptions, on both\nsynthetic and real datasets.",
          "link": "http://arxiv.org/abs/2003.13001",
          "publishedOn": "2021-07-26T02:01:00.010Z",
          "wordCount": 617,
          "title": "Zeroth-Order Regularized Optimization (ZORO): Approximately Sparse Gradients and Adaptive Sampling. (arXiv:2003.13001v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lacroce_C/0/1/0/all/0/1\">Clara Lacroce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1\">Prakash Panangaden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1\">Guillaume Rabusseau</a>",
          "description": "In this paper we study the approximate minimization problem for language\nmodelling. We assume we are given some language model as a black box. The\nobjective is to obtain a weighted finite automaton (WFA) that fits within a\ngiven size constraint and which mimics the behaviour of the original model\nwhile minimizing some notion of distance between the black box and the\nextracted WFA. We provide an algorithm for the approximate minimization of\nblack boxes trained for language modelling of sequential data over a one-letter\nalphabet. By reformulating the problem in terms of Hankel matrices, we leverage\nclassical results on the approximation of Hankel operators, namely the\ncelebrated Adamyan-Arov-Krein (AAK) theory. This allows us to use the spectral\nnorm to measure the distance between the black box and the WFA. We provide\ntheoretical guarantees to study the potentially infinite-rank Hankel matrix of\nthe black box, without accessing the training data, and we prove that our\nmethod returns an asymptotically-optimal approximation.",
          "link": "http://arxiv.org/abs/2106.02965",
          "publishedOn": "2021-07-26T02:01:00.001Z",
          "wordCount": 638,
          "title": "Extracting Weighted Automata for Approximate Minimization in Language Modelling. (arXiv:2106.02965v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scrugli_M/0/1/0/all/0/1\">Matteo Antonio Scrugli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loi_D/0/1/0/all/0/1\">Daniela Loi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffo_L/0/1/0/all/0/1\">Luigi Raffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meloni_P/0/1/0/all/0/1\">Paolo Meloni</a>",
          "description": "The Internet of Medical Things (IoMT) paradigm is becoming mainstream in\nmultiple clinical trials and healthcare procedures. Cardiovascular diseases\nmonitoring, usually involving electrocardiogram (ECG) traces analysis, is one\nof the most promising and high-impact applications. Nevertheless, to fully\nexploit the potential of IoMT in this domain, some steps forward are needed.\nFirst, the edge-computing paradigm must be added to the picture. A certain\nlevel of near-sensor processing has to be enabled, to improve the scalability,\nportability, reliability, responsiveness of the IoMT nodes. Second, novel,\nincreasingly accurate, data analysis algorithms, such as those based on\nartificial intelligence and Deep Learning, must be exploited. To reach these\nobjectives, designers and programmers of IoMT nodes, have to face challenging\noptimization tasks, in order to execute fairly complex computing tasks on\nlow-power wearable and portable processing systems, with tight power and\nbattery lifetime budgets. In this work, we explore the implementation of a\ncognitive data analysis algorithm, based on a convolutional neural network\ntrained to classify ECG waveforms, on a resource-constrained\nmicrocontroller-based computing platform. To minimize power consumption, we add\nan adaptivity layer that dynamically manages the hardware and software\nconfiguration of the device to adapt it at runtime to the required operating\nmode. Our experimental results show that adapting the node setup to the\nworkload at runtime can save up to 50% power consumption. Our optimized and\nquantized neural network reaches an accuracy value higher than 97% for\narrhythmia disorders detection on MIT-BIH Arrhythmia dataset.",
          "link": "http://arxiv.org/abs/2106.06498",
          "publishedOn": "2021-07-26T02:00:59.969Z",
          "wordCount": 722,
          "title": "An adaptive cognitive sensor node for ECG monitoring in the Internet of Medical Things. (arXiv:2106.06498v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.01586",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Liang_T/0/1/0/all/0/1\">Tengyuan Liang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sur_P/0/1/0/all/0/1\">Pragya Sur</a>",
          "description": "This paper establishes a precise high-dimensional asymptotic theory for\nboosting on separable data, taking statistical and computational perspectives.\nWe consider a high-dimensional setting where the number of features (weak\nlearners) $p$ scales with the sample size $n$, in an overparametrized regime.\nUnder a class of statistical models, we provide an exact analysis of the\ngeneralization error of boosting when the algorithm interpolates the training\ndata and maximizes the empirical $\\ell_1$-margin. Further, we explicitly pin\ndown the relation between the boosting test error and the optimal Bayes error,\nas well as the proportion of active features at interpolation (with zero\ninitialization). In turn, these precise characterizations answer certain\nquestions raised in \\cite{breiman1999prediction, schapire1998boosting}\nsurrounding boosting, under assumed data generating processes. At the heart of\nour theory lies an in-depth study of the maximum-$\\ell_1$-margin, which can be\naccurately described by a new system of non-linear equations; to analyze this\nmargin, we rely on Gaussian comparison techniques and develop a novel uniform\ndeviation argument. Our statistical and computational arguments can handle (1)\nany finite-rank spiked covariance model for the feature distribution and (2)\nvariants of boosting corresponding to general $\\ell_q$-geometry, $q \\in [1,\n2]$. As a final component, via the Lindeberg principle, we establish a\nuniversality result showcasing that the scaled $\\ell_1$-margin (asymptotically)\nremains the same, whether the covariates used for boosting arise from a\nnon-linear random feature model or an appropriately linearized model with\nmatching moments.",
          "link": "http://arxiv.org/abs/2002.01586",
          "publishedOn": "2021-07-26T02:00:59.926Z",
          "wordCount": 712,
          "title": "A Precise High-Dimensional Asymptotic Theory for Boosting and Minimum-$\\ell_1$-Norm Interpolated Classifiers. (arXiv:2002.01586v3 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.00781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernest_D/0/1/0/all/0/1\">Dan Hernest</a>",
          "description": "Recent research in synthesis of programs written in some Domain Specific\nLanguage (DSL) by means of neural networks from a limited set of inputs-output\ncorrespondences such as DeepCoder and its PCCoder reimplementation/optimization\nproved the efficiency of this kind of approach to automatic program generation\nin a DSL language that although limited in scope is universal in the sense that\nprograms can be translated to basically any programming language. We experiment\nwith the extension of the DSL of DeepCoder/PCCoder with symbols IFI and IFL\nwhich denote functional expressions of the If ramification (test) instruction\nfor types Int and List. We notice an increase (doubling) of the size of the\ntraining set, the number of parameters of the trained neural network and of the\ntime spent looking for the program synthesized from limited sets of\ninputs-output correspondences. The result is positive in the sense of\npreserving the accuracy of applying synthesis on randomly generated test sets.",
          "link": "http://arxiv.org/abs/1912.00781",
          "publishedOn": "2021-07-26T02:00:59.908Z",
          "wordCount": 610,
          "title": "Experiments with a PCCoder extension. (arXiv:1912.00781v2 [cs.PL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11253",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Malartic_Q/0/1/0/all/0/1\">Quentin Malartic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Farchi_A/0/1/0/all/0/1\">Alban Farchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bocquet_M/0/1/0/all/0/1\">Marc Bocquet</a>",
          "description": "Recent studies have shown that it is possible to combine machine learning\nmethods with data assimilation to reconstruct a dynamical system using only\nsparse and noisy observations of that system. The same approach can be used to\ncorrect the error of a knowledge-based model. The resulting surrogate model is\nhybrid, with a statistical part supplementing a physical part. In practice, the\ncorrection can be added as an integrated term (\\textit{i.e.} in the model\nresolvent) or directly inside the tendencies of the physical model. The\nresolvent correction is easy to implement. The tendency correction is more\ntechnical, in particular it requires the adjoint of the physical model, but\nalso more flexible. We use the two-scale Lorenz model to compare the two\nmethods. The accuracy in long-range forecast experiments is somewhat similar\nbetween the surrogate models using the resolvent correction and the tendency\ncorrection. By contrast, the surrogate models using the tendency correction\nsignificantly outperform the surrogate models using the resolvent correction in\ndata assimilation experiments. Finally, we show that the tendency correction\nopens the possibility to make online model error correction, \\textit{i.e.}\nimproving the model progressively as new observations become available. The\nresulting algorithm can be seen as a new formulation of weak-constraint 4D-Var.\nWe compare online and offline learning using the same framework with the\ntwo-scale Lorenz system, and show that with online learning, it is possible to\nextract all the information from sparse and noisy observations.",
          "link": "http://arxiv.org/abs/2107.11253",
          "publishedOn": "2021-07-26T02:00:59.900Z",
          "wordCount": 712,
          "title": "State, global and local parameter estimation using local ensemble Kalman filters: applications to online machine learning of chaotic dynamics. (arXiv:2107.11253v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1\">Gyri Reiersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1\">David Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Forest carbon offsets are increasingly popular and can play a significant\nrole in financing climate mitigation, forest conservation, and reforestation.\nMeasuring how much carbon is stored in forests is, however, still largely done\nvia expensive, time-consuming, and sometimes unaccountable field measurements.\nTo overcome these limitations, many verification bodies are leveraging machine\nlearning (ML) algorithms to estimate forest carbon from satellite or aerial\nimagery. Aerial imagery allows for tree species or family classification, which\nimproves the satellite imagery-based forest type classification. However,\naerial imagery is significantly more expensive to collect and it is unclear by\nhow much the higher resolution improves the forest carbon estimation. This\nproposal paper describes the first systematic comparison of forest carbon\nestimation from aerial imagery, satellite imagery, and ground-truth field\nmeasurements via deep learning-based algorithms for a tropical reforestation\nproject. Our initial results show that forest carbon estimates from satellite\nimagery can overestimate above-ground biomass by more than 10-times for\ntropical reforestation projects. The significant difference between aerial and\nsatellite-derived forest carbon measurements shows the potential for aerial\nimagery-based ML algorithms and raises the importance to extend this study to a\nglobal benchmark between options for carbon measurements.",
          "link": "http://arxiv.org/abs/2107.11320",
          "publishedOn": "2021-07-26T02:00:59.887Z",
          "wordCount": 663,
          "title": "Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.11184",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Drgona_J/0/1/0/all/0/1\">Jan Drgona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tuor_A/0/1/0/all/0/1\">Aaron Tuor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vrabie_D/0/1/0/all/0/1\">Draguna Vrabie</a>",
          "description": "We present differentiable predictive control (DPC), a method for learning\nconstrained adaptive neural control policies and dynamical models of unknown\nlinear systems. DPC presents an approximate data-driven solution approach to\nthe explicit Model Predictive Control (MPC) problem as a scalable alternative\nto computationally expensive multiparametric programming solvers. DPC is\nformulated as a constrained deep learning problem whose architecture is\ninspired by the structure of classical MPC. The optimization of the neural\ncontrol policy is based on automatic differentiation of the MPC-inspired loss\nfunction through a differentiable closed-loop system model. This novel solution\napproach can optimize adaptive neural control policies for time-varying\nreferences while obeying state and input constraints without the prior need of\nan MPC controller. We show that DPC can learn to stabilize constrained neural\ncontrol policies for systems with unstable dynamics. Moreover, we provide\nsufficient conditions for asymptotic stability of generic closed-loop system\ndynamics with neural feedback policies. In simulation case studies, we assess\nthe performance of the proposed DPC method in terms of reference tracking,\nrobustness, and computational and memory footprints compared against classical\nmodel-based and data-driven control approaches. We demonstrate that DPC scales\nlinearly with problem size, compared to exponential scalability of classical\nexplicit MPC based on multiparametric programming.",
          "link": "http://arxiv.org/abs/2004.11184",
          "publishedOn": "2021-07-26T02:00:59.880Z",
          "wordCount": 712,
          "title": "Learning Stable Adaptive Explicit Differentiable Predictive Control for Unknown Linear Systems. (arXiv:2004.11184v5 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11371",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Sen_J/0/1/0/all/0/1\">Jaydip Sen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mehtab_S/0/1/0/all/0/1\">Sidra Mehtab</a>",
          "description": "Designing an optimum portfolio that allocates weights to its constituent\nstocks in a way that achieves the best trade-off between the return and the\nrisk is a challenging research problem. The classical mean-variance theory of\nportfolio proposed by Markowitz is found to perform sub-optimally on the\nreal-world stock market data since the error in estimation for the expected\nreturns adversely affects the performance of the portfolio. This paper presents\nthree approaches to portfolio design, viz, the minimum risk portfolio, the\noptimum risk portfolio, and the Eigen portfolio, for seven important sectors of\nthe Indian stock market. The daily historical prices of the stocks are scraped\nfrom Yahoo Finance website from January 1, 2016, to December 31, 2020. Three\nportfolios are built for each of the seven sectors chosen for this study, and\nthe portfolios are analyzed on the training data based on several metrics such\nas annualized return and risk, weights assigned to the constituent stocks, the\ncorrelation heatmaps, and the principal components of the Eigen portfolios.\nFinally, the optimum risk portfolios and the Eigen portfolios for all sectors\nare tested on their return over a period of a six-month period. The\nperformances of the portfolios are compared and the portfolio yielding the\nhigher return for each sector is identified.",
          "link": "http://arxiv.org/abs/2107.11371",
          "publishedOn": "2021-07-26T02:00:59.856Z",
          "wordCount": 701,
          "title": "Optimum Risk Portfolio and Eigen Portfolio: A Comparative Analysis Using Selected Stocks from the Indian Stock Market. (arXiv:2107.11371v1 [q-fin.PM])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1\">Kazuhisa Fujita</a>",
          "description": "Spectral clustering (SC) is one of the most popular clustering methods and\noften outperforms traditional clustering methods. SC uses the eigenvectors of a\nLaplacian matrix calculated from a similarity matrix of a dataset. SC has\nserious drawbacks: the significant increases in the time complexity derived\nfrom the computation of eigenvectors and the memory space complexity to store\nthe similarity matrix. To address the issues, I develop a new approximate\nspectral clustering using the network generated by growing neural gas (GNG),\ncalled ASC with GNG in this study. ASC with GNG uses not only reference vectors\nfor vector quantization but also the topology of the network for extraction of\nthe topological relationship between data points in a dataset. ASC with GNG\ncalculates the similarity matrix from both the reference vectors and the\ntopology of the network generated by GNG. Using the network generated from a\ndataset by GNG, ASC with GNG achieves to reduce the computational and space\ncomplexities and improve clustering quality. In this study, I demonstrate that\nASC with GNG effectively reduces the computational time. Moreover, this study\nshows that ASC with GNG provides equal to or better clustering performance than\nSC.",
          "link": "http://arxiv.org/abs/2009.07101",
          "publishedOn": "2021-07-26T02:00:59.845Z",
          "wordCount": 683,
          "title": "Approximate spectral clustering using both reference vectors and topology of the network generated by growing neural gas. (arXiv:2009.07101v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13349",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nair_V/0/1/0/all/0/1\">Vinod Nair</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bartunov_S/0/1/0/all/0/1\">Sergey Bartunov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gimeno_F/0/1/0/all/0/1\">Felix Gimeno</a>, <a href=\"http://arxiv.org/find/math/1/au:+Glehn_I/0/1/0/all/0/1\">Ingrid von Glehn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lichocki_P/0/1/0/all/0/1\">Pawel Lichocki</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lobov_I/0/1/0/all/0/1\">Ivan Lobov</a>, <a href=\"http://arxiv.org/find/math/1/au:+ODonoghue_B/0/1/0/all/0/1\">Brendan O&#x27;Donoghue</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sonnerat_N/0/1/0/all/0/1\">Nicolas Sonnerat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tjandraatmadja_C/0/1/0/all/0/1\">Christian Tjandraatmadja</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_P/0/1/0/all/0/1\">Pengming Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Addanki_R/0/1/0/all/0/1\">Ravichandra Addanki</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hapuarachchi_T/0/1/0/all/0/1\">Tharindi Hapuarachchi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Keck_T/0/1/0/all/0/1\">Thomas Keck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Keeling_J/0/1/0/all/0/1\">James Keeling</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kohli_P/0/1/0/all/0/1\">Pushmeet Kohli</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ktena_I/0/1/0/all/0/1\">Ira Ktena</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">Yujia Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zwols_Y/0/1/0/all/0/1\">Yori Zwols</a>",
          "description": "Mixed Integer Programming (MIP) solvers rely on an array of sophisticated\nheuristics developed with decades of research to solve large-scale MIP\ninstances encountered in practice. Machine learning offers to automatically\nconstruct better heuristics from data by exploiting shared structure among\ninstances in the data. This paper applies learning to the two key sub-tasks of\na MIP solver, generating a high-quality joint variable assignment, and bounding\nthe gap in objective value between that assignment and an optimal one. Our\napproach constructs two corresponding neural network-based components, Neural\nDiving and Neural Branching, to use in a base MIP solver such as SCIP. Neural\nDiving learns a deep neural network to generate multiple partial assignments\nfor its integer variables, and the resulting smaller MIPs for un-assigned\nvariables are solved with SCIP to construct high quality joint assignments.\nNeural Branching learns a deep neural network to make variable selection\ndecisions in branch-and-bound to bound the objective value gap with a small\ntree. This is done by imitating a new variant of Full Strong Branching we\npropose that scales to large instances using GPUs. We evaluate our approach on\nsix diverse real-world datasets, including two Google production datasets and\nMIPLIB, by training separate neural networks on each. Most instances in all the\ndatasets combined have $10^3-10^6$ variables and constraints after presolve,\nwhich is significantly larger than previous learning approaches. Comparing\nsolvers with respect to primal-dual gap averaged over a held-out set of\ninstances, the learning-augmented SCIP is 2x to 10x better on all datasets\nexcept one on which it is $10^5$x better, at large time limits. To the best of\nour knowledge, ours is the first learning approach to demonstrate such large\nimprovements over SCIP on both large-scale real-world application datasets and\nMIPLIB.",
          "link": "http://arxiv.org/abs/2012.13349",
          "publishedOn": "2021-07-26T02:00:59.837Z",
          "wordCount": 784,
          "title": "Solving Mixed Integer Programs Using Neural Networks. (arXiv:2012.13349v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takezawa_Y/0/1/0/all/0/1\">Yuki Takezawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1\">Makoto Yamada</a>",
          "description": "To measure the similarity of documents, the Wasserstein distance is a\npowerful tool, but it requires a high computational cost. Recently, for fast\ncomputation of the Wasserstein distance, methods for approximating the\nWasserstein distance using a tree metric have been proposed. These tree-based\nmethods allow fast comparisons of a large number of documents; however, they\nare unsupervised and do not learn task-specific distances. In this work, we\npropose the Supervised Tree-Wasserstein (STW) distance, a fast, supervised\nmetric learning method based on the tree metric. Specifically, we rewrite the\nWasserstein distance on the tree metric by the parent-child relationships of a\ntree and formulate it as a continuous optimization problem using a contrastive\nloss. Experimentally, we show that the STW distance can be computed fast, and\nimproves the accuracy of document classification tasks. Furthermore, the STW\ndistance is formulated by matrix multiplications, runs on a GPU, and is\nsuitable for batch processing. Therefore, we show that the STW distance is\nextremely efficient when comparing a large number of documents.",
          "link": "http://arxiv.org/abs/2101.11520",
          "publishedOn": "2021-07-26T02:00:59.829Z",
          "wordCount": 618,
          "title": "Supervised Tree-Wasserstein Distance. (arXiv:2101.11520v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08074",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yan_C/0/1/0/all/0/1\">Chao Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_X/0/1/0/all/0/1\">Xiaojia Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lan_Z/0/1/0/all/0/1\">Zhen Lan</a>",
          "description": "Developing the flocking behavior for a dynamic squad of fixed-wing UAVs is\nstill a challenge due to kinematic complexity and environmental uncertainty. In\nthis paper, we deal with the decentralized flocking and collision avoidance\nproblem through deep reinforcement learning (DRL). Specifically, we formulate a\ndecentralized DRL-based decision making framework from the perspective of every\nfollower, where a collision avoidance mechanism is integrated into the flocking\ncontroller. Then, we propose a novel reinforcement learning algorithm PS-CACER\nfor training a shared control policy for all the followers. Besides, we design\na plug-n-play embedding module based on convolutional neural networks and the\nattention mechanism. As a result, the variable-length system state can be\nencoded into a fixed-length embedding vector, which makes the learned DRL\npolicy independent with the number and the order of followers. Finally,\nnumerical simulation results demonstrate the effectiveness of the proposed\nmethod, and the learned policies can be directly transferred to semi-physical\nsimulation without any parameter finetuning.",
          "link": "http://arxiv.org/abs/2101.08074",
          "publishedOn": "2021-07-26T02:00:59.821Z",
          "wordCount": 656,
          "title": "Flocking and Collision Avoidance for a Dynamic Squad of Fixed-Wing UAVs Using Deep Reinforcement Learning. (arXiv:2101.08074v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_H/0/1/0/all/0/1\">Hussain Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duricic_T/0/1/0/all/0/1\">Tomislav Duricic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1\">Elisabeth Lex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helic_D/0/1/0/all/0/1\">Denis Helic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohmaier_M/0/1/0/all/0/1\">Markus Strohmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kern_R/0/1/0/all/0/1\">Roman Kern</a>",
          "description": "Recent work has shown that graph neural networks (GNNs) are vulnerable to\nadversarial attacks on graph data. Common attack approaches are typically\ninformed, i.e. they have access to information about node attributes such as\nlabels and feature vectors. In this work, we study adversarial attacks that are\nuninformed, where an attacker only has access to the graph structure, but no\ninformation about node attributes. Here the attacker aims to exploit structural\nknowledge and assumptions, which GNN models make about graph data. In\nparticular, literature has shown that structural node centrality and similarity\nhave a strong influence on learning with GNNs. Therefore, we study the impact\nof centrality and similarity on adversarial attacks on GNNs. We demonstrate\nthat attackers can exploit this information to decrease the performance of GNNs\nby focusing on injecting links between nodes of low similarity and,\nsurprisingly, low centrality. We show that structure-based uninformed attacks\ncan approach the performance of informed attacks, while being computationally\nmore efficient. With our paper, we present a new attack strategy on GNNs that\nwe refer to as Structack. Structack can successfully manipulate the performance\nof GNNs with very limited information while operating under tight computational\nconstraints. Our work contributes towards building more robust machine learning\napproaches on graphs.",
          "link": "http://arxiv.org/abs/2107.11327",
          "publishedOn": "2021-07-26T02:00:59.795Z",
          "wordCount": 662,
          "title": "Structack: Structure-based Adversarial Attacks on Graph Neural Networks. (arXiv:2107.11327v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01512",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/math/1/au:+To_T/0/1/0/all/0/1\">Tat Dat To</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Hang Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_T/0/1/0/all/0/1\">Thu Hang Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang Phuong Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>",
          "description": "We propose in this paper New Q-Newton's method. The update rule for the\nsimplest version is $x_{n+1}=x_n-w_n$ where\n$w_n=pr_{A_n,+}(v_n)-pr_{A_n,-}(v_n)$, with $A_n=\\nabla\n^2f(x_n)+\\delta_n||\\nabla f(x_n)||^2.Id$ and $v_n=A_n^{-1}.\\nabla f(x_n)$. Here\n$\\delta_n$ is an appropriate real number so that $A_n$ is invertible, and\n$pr_{A_n,\\pm}$ are projections to the vector subspaces generated by\neigenvectors of positive (correspondingly negative) eigenvalues of $A_n$.\n\nThe main result of this paper roughly says that if $f$ is $C^3$ and a\nsequence $\\{x_n\\}$, constructed by the New Q-Newton's method from a random\ninitial point $x_0$, {\\bf converges}, then the limit point is a critical point\nand is not a saddle point, and the convergence rate is the same as that of\nNewton's method. At the end of the paper, we present some issues (saddle points\nand convergence) one faces when implementing Newton's method and modifications\ninto Deep Neural Networks. We test the good performance of New Q-Newton's\nmethod against algorithms such as Newton's method, BFGS, Adaptive Cubic\nRegularization, Random damping Newton's method and Inertial Newton's method, as\nwell as Unbounded Two-way Backtracking Gradient Descent. The experiments cover\nboth realistic settings (such as a toy model of protein folding and stochastic\noptimization) as well as various benchmark test functions.",
          "link": "http://arxiv.org/abs/2006.01512",
          "publishedOn": "2021-07-26T02:00:59.788Z",
          "wordCount": 730,
          "title": "A fast and simple modification of Newton's method helping to avoid saddle points. (arXiv:2006.01512v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.04919",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Willard_J/0/1/0/all/0/1\">Jared Willard</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1\">Shaoming Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Steinbach_M/0/1/0/all/0/1\">Michael Steinbach</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "There is a growing consensus that solutions to complex science and\nengineering problems require novel methodologies that are able to integrate\ntraditional physics-based modeling approaches with state-of-the-art machine\nlearning (ML) techniques. This paper provides a structured overview of such\ntechniques. Application-centric objective areas for which these approaches have\nbeen applied are summarized, and then classes of methodologies used to\nconstruct physics-guided ML models and hybrid physics-ML frameworks are\ndescribed. We then provide a taxonomy of these existing techniques, which\nuncovers knowledge gaps and potential crossovers of methods between disciplines\nthat can serve as ideas for future research.",
          "link": "http://arxiv.org/abs/2003.04919",
          "publishedOn": "2021-07-26T02:00:59.781Z",
          "wordCount": 601,
          "title": "Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems. (arXiv:2003.04919v5 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Babadi_A/0/1/0/all/0/1\">Amin Babadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panne_M/0/1/0/all/0/1\">Michiel van de Panne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_P/0/1/0/all/0/1\">Perttu H&#xe4;m&#xe4;l&#xe4;inen</a>",
          "description": "We propose a novel method for exploring the dynamics of physically based\nanimated characters, and learning a task-agnostic action space that makes\nmovement optimization easier. Like several previous papers, we parameterize\nactions as target states, and learn a short-horizon goal-conditioned low-level\ncontrol policy that drives the agent's state towards the targets. Our novel\ncontribution is that with our exploration data, we are able to learn the\nlow-level policy in a generic manner and without any reference movement data.\nTrained once for each agent or simulation environment, the policy improves the\nefficiency of optimizing both trajectories and high-level policies across\nmultiple tasks and optimization algorithms. We also contribute novel\nvisualizations that show how using target states as actions makes optimized\ntrajectories more robust to disturbances; this manifests as wider optima that\nare easy to find. Due to its simplicity and generality, our proposed approach\nshould provide a building block that can improve a large variety of movement\noptimization methods and applications.",
          "link": "http://arxiv.org/abs/2009.10337",
          "publishedOn": "2021-07-26T02:00:59.773Z",
          "wordCount": 656,
          "title": "Learning Task-Agnostic Action Spaces for Movement Optimization. (arXiv:2009.10337v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10686",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mucke_N/0/1/0/all/0/1\">Nicole M&#xfc;cke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Steinwart_I/0/1/0/all/0/1\">Ingo Steinwart</a>",
          "description": "A common strategy to train deep neural networks (DNNs) is to use very large\narchitectures and to train them until they (almost) achieve zero training\nerror. Empirically observed good generalization performance on test data, even\nin the presence of lots of label noise, corroborate such a procedure. On the\nother hand, in statistical learning theory it is known that over-fitting models\nmay lead to poor generalization properties, occurring in e.g. empirical risk\nminimization (ERM) over too large hypotheses classes. Inspired by this\ncontradictory behavior, so-called interpolation methods have recently received\nmuch attention, leading to consistent and optimally learning methods for some\nlocal averaging schemes with zero training error. However, there is no\ntheoretical analysis of interpolating ERM-like methods so far. We take a step\nin this direction by showing that for certain, large hypotheses classes, some\ninterpolating ERMs enjoy very good statistical guarantees while others fail in\nthe worst sense. Moreover, we show that the same phenomenon occurs for DNNs\nwith zero training error and sufficiently large architectures.",
          "link": "http://arxiv.org/abs/1905.10686",
          "publishedOn": "2021-07-26T02:00:59.766Z",
          "wordCount": 625,
          "title": "Empirical Risk Minimization in the Interpolating Regime with Application to Neural Network Learning. (arXiv:1905.10686v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1\">Diego Elias Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujahid_S/0/1/0/all/0/1\">Suhaib Mujahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdalkareem_R/0/1/0/all/0/1\">Rabe Abdalkareem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shihab_E/0/1/0/all/0/1\">Emad Shihab</a>",
          "description": "A decade after its first release, the Go programming language has become a\nmajor programming language in the development landscape. While praised for its\nclean syntax and C-like performance, Go also contains a strong static\ntype-system that prevents arbitrary type casting and arbitrary memory access,\nmaking the language type-safe by design. However, to give developers the\npossibility of implementing low-level code, Go ships with a special package\ncalled unsafe that offers developers a way around the type-safety of Go\nprograms. The package gives greater flexibility to developers but comes at a\nhigher risk of runtime errors, chances of non-portability, and the loss of\ncompatibility guarantees for future versions of Go.\n\nIn this paper, we present the first large-scale study on the usage of the\nunsafe package in 2,438 popular Go projects. Our investigation shows that\nunsafe is used in 24% of Go projects, motivated primarily by communicating with\noperating systems and C code, but is also commonly used as a source of\nperformance optimization. Developers are willing to use unsafe to break\nlanguage specifications (e.g., string immutability) for better performance and\n6% of analyzed projects that use unsafe perform risky pointer conversions that\ncan lead to program crashes and unexpected behavior. Furthermore, we report a\nseries of real issues faced by projects that use unsafe, from crashing errors\nand non-deterministic behavior to having their deployment restricted from\ncertain popular environments. Our findings can be used to understand how and\nwhy developers break type-safety in Go, and help motivate further tools and\nlanguage development that could make the usage of unsafe in Go even safer.",
          "link": "http://arxiv.org/abs/2006.09973",
          "publishedOn": "2021-07-26T02:00:59.740Z",
          "wordCount": 763,
          "title": "Breaking Type Safety in Go: An Empirical Study on the Usage of the unsafe Package. (arXiv:2006.09973v4 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qizheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1\">Hui Guan</a>",
          "description": "Hard parameter sharing in multi-task learning (MTL) allows tasks to share\nsome of model parameters, reducing storage cost and improving prediction\naccuracy. The common sharing practice is to share bottom layers of a deep\nneural network among tasks while using separate top layers for each task. In\nthis work, we revisit this common practice via an empirical study on\nfine-grained image classification tasks and make two surprising observations.\n(1) Using separate bottom-layer parameters could achieve significantly better\nperformance than the common practice and this phenomenon holds for different\nnumber of tasks jointly trained on different backbone architectures with\ndifferent quantity of task-specific parameters. (2) A multi-task model with a\nsmall proportion of task-specific parameters from bottom layers can achieve\ncompetitive performance with independent models trained on each task separately\nand outperform a state-of-the-art MTL framework. Our observations suggest that\npeople rethink the current sharing paradigm and adopt the new strategy of using\nseparate bottom-layer parameters as a stronger baseline for model design in\nMTL.",
          "link": "http://arxiv.org/abs/2107.11359",
          "publishedOn": "2021-07-26T02:00:59.731Z",
          "wordCount": 595,
          "title": "Rethinking Hard-Parameter Sharing in Multi-Task Learning. (arXiv:2107.11359v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saksham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>",
          "description": "Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.",
          "link": "http://arxiv.org/abs/2101.06069",
          "publishedOn": "2021-07-26T02:00:59.723Z",
          "wordCount": 700,
          "title": "Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Akshay Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Alnur Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_S/0/1/0/all/0/1\">Stephen Boyd</a>",
          "description": "We consider the vector embedding problem. We are given a finite set of items,\nwith the goal of assigning a representative vector to each one, possibly under\nsome constraints (such as the collection of vectors being standardized, i.e.,\nhave zero mean and unit covariance). We are given data indicating that some\npairs of items are similar, and optionally, some other pairs are dissimilar.\nFor pairs of similar items, we want the corresponding vectors to be near each\nother, and for dissimilar pairs, we want the corresponding vectors to not be\nnear each other, measured in Euclidean distance. We formalize this by\nintroducing distortion functions, defined for some pairs of the items. Our goal\nis to choose an embedding that minimizes the total distortion, subject to the\nconstraints. We call this the minimum-distortion embedding (MDE) problem.\n\nThe MDE framework is simple but general. It includes a wide variety of\nembedding methods, such as spectral embedding, principal component analysis,\nmultidimensional scaling, dimensionality reduction methods (like Isomap and\nUMAP), force-directed layout, and others. It also includes new embeddings, and\nprovides principled ways of validating historical and new embeddings alike.\n\nWe develop a projected quasi-Newton method that approximately solves MDE\nproblems and scales to large data sets. We implement this method in PyMDE, an\nopen-source Python package. In PyMDE, users can select from a library of\ndistortion functions and constraints or specify custom ones, making it easy to\nrapidly experiment with different embeddings. Our software scales to data sets\nwith millions of items and tens of millions of distortion functions. To\ndemonstrate our method, we compute embeddings for several real-world data sets,\nincluding images, an academic co-author network, US county demographic data,\nand single-cell mRNA transcriptomes.",
          "link": "http://arxiv.org/abs/2103.02559",
          "publishedOn": "2021-07-26T02:00:59.716Z",
          "wordCount": 747,
          "title": "Minimum-Distortion Embedding. (arXiv:2103.02559v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Anand Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zylich_B/0/1/0/all/0/1\">Brian Zylich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottmar_E/0/1/0/all/0/1\">Erin Ottmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LoCasale_Crouch_J/0/1/0/all/0/1\">Jennifer LoCasale-Crouch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob Whitehill</a>",
          "description": "In this work we present a multi-modal machine learning-based system, which we\ncall ACORN, to analyze videos of school classrooms for the Positive Climate\n(PC) and Negative Climate (NC) dimensions of the CLASS observation protocol\nthat is widely used in educational research. ACORN uses convolutional neural\nnetworks to analyze spectral audio features, the faces of teachers and\nstudents, and the pixels of each image frame, and then integrates this\ninformation over time using Temporal Convolutional Networks. The audiovisual\nACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$\nwith ground-truth scores provided by expert CLASS coders on the UVA Toddler\ndataset (cross-validation on $n=300$ 15-min video segments), and a purely\nauditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the\nMET dataset (test set of $n=2000$ videos segments). These numbers are similar\nto inter-coder reliability of human coders. Finally, using Graph Convolutional\nNetworks we make early strides (AUC=$0.70$) toward predicting the specific\nmoments (45-90sec clips) when the PC is particularly weak/strong. Our findings\ninform the design of automatic classroom observation and also more general\nvideo activity recognition and summary recognition systems.",
          "link": "http://arxiv.org/abs/2005.09525",
          "publishedOn": "2021-07-26T02:00:59.707Z",
          "wordCount": 712,
          "title": "Toward Automated Classroom Observation: Multimodal Machine Learning to Estimate CLASS Positive Climate and Negative Climate. (arXiv:2005.09525v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Schonlieb</a>",
          "description": "Alzheimer's disease (AD) is the most common age-related dementia. It remains\na challenge to identify the individuals at risk of dementia for precise\nmanagement. Brain MRI offers a noninvasive biomarker to detect brain aging.\nPrevious evidence shows that the brain structural change detected by diffusion\nMRI is associated with dementia. Mounting studies has conceptualised the brain\nas a complex network, which has shown the utility of this approach in\ncharacterising various neurological and psychiatric disorders. Therefore, the\nstructural connectivity shows promise in dementia classification. The proposed\nBrainNetGAN is a generative adversarial network variant to augment the brain\nstructural connectivity matrices for binary dementia classification tasks.\nStructural connectivity matrices between separated brain regions are\nconstructed using tractography on diffusion MRI data. The BrainNetGAN model is\ntrained to generate fake brain connectivity matrices, which are expected to\nreflect latent distribution of the real brain network data. Finally, a\nconvolutional neural network classifier is proposed for binary dementia\nclassification. Numerical results show that the binary classification\nperformance in the testing set was improved using the BrainNetGAN augmented\ndataset. The proposed methodology allows quick synthesis of an arbitrary number\nof augmented connectivity matrices and can be easily transferred to similar\nclassification tasks.",
          "link": "http://arxiv.org/abs/2103.08494",
          "publishedOn": "2021-07-26T02:00:59.681Z",
          "wordCount": 690,
          "title": "BrainNetGAN: Data augmentation of brain connectivity using generative adversarial network for dementia classification. (arXiv:2103.08494v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fursov_I/0/1/0/all/0/1\">Ivan Fursov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1\">Alexey Zaytsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnyshev_P/0/1/0/all/0/1\">Pavel Burnyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dmitrieva_E/0/1/0/all/0/1\">Ekaterina Dmitrieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klyuchnikov_N/0/1/0/all/0/1\">Nikita Klyuchnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravchenko_A/0/1/0/all/0/1\">Andrey Kravchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "Robustness of huge Transformer-based models for natural language processing\nis an important issue due to their capabilities and wide adoption. One way to\nunderstand and improve robustness of these models is an exploration of an\nadversarial attack scenario: check if a small perturbation of an input can fool\na model.\n\nDue to the discrete nature of textual data, gradient-based adversarial\nmethods, widely used in computer vision, are not applicable per~se. The\nstandard strategy to overcome this issue is to develop token-level\ntransformations, which do not take the whole sentence into account.\n\nIn this paper, we propose a new black-box sentence-level attack. Our method\nfine-tunes a pre-trained language model to generate adversarial examples. A\nproposed differentiable loss function depends on a substitute classifier score\nand an approximate edit distance computed via a deep learning model.\n\nWe show that the proposed attack outperforms competitors on a diverse set of\nNLP problems for both computed metrics and human evaluation. Moreover, due to\nthe usage of the fine-tuned language model, the generated adversarial examples\nare hard to detect, thus current models are not robust. Hence, it is difficult\nto defend from the proposed attack, which is not the case for other attacks.",
          "link": "http://arxiv.org/abs/2107.11275",
          "publishedOn": "2021-07-26T02:00:59.673Z",
          "wordCount": 652,
          "title": "A Differentiable Language Model Adversarial Attack on Text Classifiers. (arXiv:2107.11275v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.11134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1\">Utku Evci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1\">Trevor Gale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>",
          "description": "Many applications require sparse neural networks due to space or inference\ntime restrictions. There is a large body of work on training dense networks to\nyield sparse networks for inference, but this limits the size of the largest\ntrainable sparse model to that of the largest trainable dense model. In this\npaper we introduce a method to train sparse neural networks with a fixed\nparameter count and a fixed computational cost throughout training, without\nsacrificing accuracy relative to existing dense-to-sparse training methods. Our\nmethod updates the topology of the sparse network during training by using\nparameter magnitudes and infrequent gradient calculations. We show that this\napproach requires fewer floating-point operations (FLOPs) to achieve a given\nlevel of accuracy compared to prior techniques. We demonstrate state-of-the-art\nsparse training results on a variety of networks and datasets, including\nResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we\nprovide some insights into why allowing the topology to change during the\noptimization can overcome local minima encountered when the topology remains\nstatic. Code used in our work can be found in github.com/google-research/rigl.",
          "link": "http://arxiv.org/abs/1911.11134",
          "publishedOn": "2021-07-26T02:00:59.666Z",
          "wordCount": 696,
          "title": "Rigging the Lottery: Making All Tickets Winners. (arXiv:1911.11134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11191",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Duff_M/0/1/0/all/0/1\">Margaret Duff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_N/0/1/0/all/0/1\">Neill D. F. Campbell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehrhardt_M/0/1/0/all/0/1\">Matthias J. Ehrhardt</a>",
          "description": "Deep neural network approaches to inverse imaging problems have produced\nimpressive results in the last few years. In this paper, we consider the use of\ngenerative models in a variational regularisation approach to inverse problems.\nThe considered regularisers penalise images that are far from the range of a\ngenerative model that has learned to produce images similar to a training\ndataset. We name this family \\textit{generative regularisers}. The success of\ngenerative regularisers depends on the quality of the generative model and so\nwe propose a set of desired criteria to assess models and guide future\nresearch. In our numerical experiments, we evaluate three common generative\nmodels, autoencoders, variational autoencoders and generative adversarial\nnetworks, against our desired criteria. We also test three different generative\nregularisers on the inverse problems of deblurring, deconvolution, and\ntomography. We show that the success of solutions restricted to lie exactly in\nthe range of the generator is highly dependent on the ability of the generative\nmodel but that allowing small deviations from the range of the generator\nproduces more consistent results.",
          "link": "http://arxiv.org/abs/2107.11191",
          "publishedOn": "2021-07-26T02:00:59.658Z",
          "wordCount": 631,
          "title": "Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shaojie Tang</a>",
          "description": "Most of existing studies on adaptive submodular optimization focus on the\naverage-case, i.e., their objective is to find a policy that maximizes the\nexpected utility over a known distribution of realizations. However, a policy\nthat has a good average-case performance may have very poor performance under\nthe worst-case realization. In this study, we propose to study two variants of\nadaptive submodular optimization problems, namely, worst-case adaptive\nsubmodular maximization and robust submodular maximization. The first problem\naims to find a policy that maximizes the worst-case utility and the latter one\naims to find a policy, if any, that achieves both near optimal average-case\nutility and worst-case utility simultaneously. We introduce a new class of\nstochastic functions, called \\emph{worst-case submodular function}. For the\nworst-case adaptive submodular maximization problem subject to a $p$-system\nconstraint, we develop an adaptive worst-case greedy policy that achieves a\n$\\frac{1}{p+1}$ approximation ratio against the optimal worst-case utility if\nthe utility function is worst-case submodular. For the robust adaptive\nsubmodular maximization problem subject to a cardinality constraint, if the\nutility function is both worst-case submodular and adaptive submodular, we\ndevelop a hybrid adaptive policy that achieves an approximation close to\n$1-e^{-\\frac{1}{2}}$ under both worst case setting and average case setting\nsimultaneously. We also describe several applications of our theoretical\nresults, including pool-base active learning, stochastic submodular set cover\nand adaptive viral marketing.",
          "link": "http://arxiv.org/abs/2107.11333",
          "publishedOn": "2021-07-26T02:00:59.650Z",
          "wordCount": 653,
          "title": "Robust Adaptive Submodular Maximization. (arXiv:2107.11333v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cen_Y/0/1/0/all/0/1\">Yukuo Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhenyu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qibin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yizhen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Aohan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shiguang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1\">Guohao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Deep learning on graphs has attracted tremendous attention from the graph\nlearning community in recent years. It has been widely used in several\nreal-world applications such as social network analysis and recommender\nsystems. In this paper, we introduce CogDL, an extensive toolkit for deep\nlearning on graphs that allows researchers and developers to easily conduct\nexperiments and build applications. It provides standard training and\nevaluation for the most important tasks in the graph domain, including node\nclassification, graph classification, etc. For each task, it provides\nimplementations of state-of-the-art models. The models in our toolkit are\ndivided into two major parts, graph embedding methods and graph neural\nnetworks. Most of the graph embedding methods learn node-level or graph-level\nrepresentations in an unsupervised way and preserves the graph properties such\nas structural information, while graph neural networks capture node features\nand work in semi-supervised or self-supervised settings. All models implemented\nin our toolkit can be easily reproducible for leaderboard results. Most models\nin CogDL are developed on top of PyTorch, and users can leverage the advantages\nof PyTorch to implement their own models. Furthermore, we demonstrate the\neffectiveness of CogDL for real-world applications in AMiner, a large academic\nmining system.",
          "link": "http://arxiv.org/abs/2103.00959",
          "publishedOn": "2021-07-26T02:00:59.613Z",
          "wordCount": 692,
          "title": "CogDL: Toolkit for Deep Learning on Graphs. (arXiv:2103.00959v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Estienne_T/0/1/0/all/0/1\">Th&#xe9;o Estienne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulidis_S/0/1/0/all/0/1\">Stergios Christodoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battistella_E/0/1/0/all/0/1\">Enzo Battistella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1\">Th&#xe9;ophraste Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerousseau_M/0/1/0/all/0/1\">Marvin Lerousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_A/0/1/0/all/0/1\">Amaury Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chassagnon_G/0/1/0/all/0/1\">Guillaume Chassagnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revel_M/0/1/0/all/0/1\">Marie-Pierre Revel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_E/0/1/0/all/0/1\">Eric Deutsch</a>",
          "description": "Explainability of deep neural networks is one of the most challenging and\ninteresting problems in the field. In this study, we investigate the topic\nfocusing on the interpretability of deep learning-based registration methods.\nIn particular, with the appropriate model architecture and using a simple\nlinear projection, we decompose the encoding space, generating a new basis, and\nwe empirically show that this basis captures various decomposed anatomically\naware geometrical transformations. We perform experiments using two different\ndatasets focusing on lungs and hippocampus MRI. We show that such an approach\ncan decompose the highly convoluted latent spaces of registration pipelines in\nan orthogonal space with several interesting properties. We hope that this work\ncould shed some light on a better understanding of deep learning-based\nregistration methods.",
          "link": "http://arxiv.org/abs/2107.11238",
          "publishedOn": "2021-07-26T02:00:59.603Z",
          "wordCount": 591,
          "title": "Exploring Deep Registration Latent Spaces. (arXiv:2107.11238v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00083",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kim_T/0/1/0/all/0/1\">Taesup Kim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fakoor_R/0/1/0/all/0/1\">Rasool Fakoor</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1\">Ryan J. Tibshirani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "Conditional quantile estimation is a key statistical learning challenge\nmotivated by the need to quantify uncertainty in predictions or to model a\ndiverse population without being overly reductive. As such, many models have\nbeen developed for this problem. Adopting a meta viewpoint, we propose a\ngeneral framework (inspired by neural network optimization) for aggregating any\nnumber of conditional quantile models in order to boost predictive accuracy. We\nconsider weighted ensembling strategies of increasing flexibility where the\nweights may vary over individual models, quantile levels, and feature values.\nAn appeal of our approach is its portability: we ensure that estimated\nquantiles at adjacent levels do not cross by applying simple transformations\nthrough which gradients can be backpropagated, and this allows us to leverage\nthe modern deep learning toolkit for building quantile ensembles. Our\nexperiments confirm that ensembling can lead to big gains in accuracy, even\nwhen the constituent models are themselves powerful and flexible.",
          "link": "http://arxiv.org/abs/2103.00083",
          "publishedOn": "2021-07-26T02:00:59.593Z",
          "wordCount": 606,
          "title": "Deep Quantile Aggregation. (arXiv:2103.00083v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendrickx_K/0/1/0/all/0/1\">Kilian Hendrickx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perini_L/0/1/0/all/0/1\">Lorenzo Perini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plas_D/0/1/0/all/0/1\">Dries Van der Plas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meert_W/0/1/0/all/0/1\">Wannes Meert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jesse Davis</a>",
          "description": "Machine learning models always make a prediction, even when it is likely to\nbe inaccurate. This behavior should be avoided in many decision support\napplications, where mistakes can have severe consequences. Albeit already\nstudied in 1970, machine learning with a reject option recently gained\ninterest. This machine learning subfield enables machine learning models to\nabstain from making a prediction when likely to make a mistake.\n\nThis survey aims to provide an overview on machine learning with a reject\noption. We introduce the conditions leading to two types of rejection,\nambiguity and novelty rejection. Moreover, we define the existing architectures\nfor models with a reject option, describe the standard learning strategies to\ntrain such models and relate traditional machine learning techniques to\nrejection. Additionally, we review strategies to evaluate a model's predictive\nand rejective quality. Finally, we provide examples of relevant application\ndomains and show how machine learning with rejection relates to other machine\nlearning research areas.",
          "link": "http://arxiv.org/abs/2107.11277",
          "publishedOn": "2021-07-26T02:00:59.585Z",
          "wordCount": 599,
          "title": "Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11304",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lee_C/0/1/0/all/0/1\">Chang-Shen Lee</a>, <a href=\"http://arxiv.org/find/math/1/au:+Michelusi_N/0/1/0/all/0/1\">Nicol&#xf2; Michelusi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Scutari_G/0/1/0/all/0/1\">Gesualdo Scutari</a>",
          "description": "This paper studies distributed algorithms for (strongly convex) composite\noptimization problems over mesh networks, subject to quantized communications.\nInstead of focusing on a specific algorithmic design, we propose a black-box\nmodel casting distributed algorithms in the form of fixed-point iterates,\nconverging at linear rate. The algorithmic model is coupled with a novel\n(random) Biased Compression (BC-)rule on the quantizer design, which preserves\nlinear convergence. A new quantizer coupled with a communication-efficient\nencoding scheme is also proposed, which efficiently implements the BC-rule\nusing a finite number of bits. This contrasts with most of existing\nquantization rules, whose implementation calls for an infinite number of bits.\nA unified communication complexity analysis is developed for the black-box\nmodel, determining the average number of bit required to reach a solution of\nthe optimization problem within the required accuracy. Numerical results\nvalidate our theoretical findings and show that distributed algorithms equipped\nwith the proposed quantizer have more favorable communication complexity than\nalgorithms using existing quantization rules.",
          "link": "http://arxiv.org/abs/2107.11304",
          "publishedOn": "2021-07-26T02:00:59.577Z",
          "wordCount": 608,
          "title": "Finite-Bit Quantization For Distributed Algorithms With Linear Convergence. (arXiv:2107.11304v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jawali_D/0/1/0/all/0/1\">Dhruv Jawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seelamantula_C/0/1/0/all/0/1\">Chandra Sekhar Seelamantula</a>",
          "description": "Wavelets have proven to be highly successful in several signal and image\nprocessing applications. Wavelet design has been an active field of research\nfor over two decades, with the problem often being approached from an\nanalytical perspective. In this paper, we introduce a learning based approach\nto wavelet design. We draw a parallel between convolutional autoencoders and\nwavelet multiresolution approximation, and show how the learning angle provides\na coherent computational framework for addressing the design problem. We aim at\ndesigning data-independent wavelets by training filterbank autoencoders, which\nprecludes the need for customized datasets. In fact, we use high-dimensional\nGaussian vectors for training filterbank autoencoders, and show that a\nnear-zero training loss implies that the learnt filters satisfy the perfect\nreconstruction property with very high probability. Properties of a wavelet\nsuch as orthogonality, compact support, smoothness, symmetry, and vanishing\nmoments can be incorporated by designing the autoencoder architecture\nappropriately and with a suitable regularization term added to the mean-squared\nerror cost used in the learning process. Our approach not only recovers the\nwell known Daubechies family of orthogonal wavelets and the\nCohen-Daubechies-Feauveau family of symmetric biorthogonal wavelets, but also\nlearns wavelets outside these families.",
          "link": "http://arxiv.org/abs/2107.11225",
          "publishedOn": "2021-07-26T02:00:59.550Z",
          "wordCount": 636,
          "title": "Wavelet Design in a Learning Framework. (arXiv:2107.11225v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>",
          "description": "We propose the adjacency adaptive graph convolutional long-short term memory\nnetwork (AAGC-LSTM) for human pose estimation from sparse inertial\nmeasurements, obtained from only 6 measurement units. The AAGC-LSTM combines\nboth spatial and temporal dependency in a single network operation. This is\nmade possible by equipping graph convolutions with adjacency adaptivity, which\nalso allows for learning unknown dependencies of the human body joints. To\nfurther boost accuracy, we propose longitudinal loss weighting to consider\nnatural movement patterns, as well as body-aware contralateral data\naugmentation. By combining these contributions, we are able to utilize the\ninherent graph nature of the human body, and can thus outperform the state of\nthe art for human pose estimation from sparse inertial measurements.",
          "link": "http://arxiv.org/abs/2107.11214",
          "publishedOn": "2021-07-26T02:00:59.543Z",
          "wordCount": 558,
          "title": "Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_C/0/1/0/all/0/1\">Chen Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1\">Bryon Aragam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi S. Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1\">Geoffrey J. Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>",
          "description": "Many machine learning applications, e.g., privacy-preserving learning,\nalgorithmic fairness and domain adaptation/generalization, involve learning the\nso-called invariant representations that achieve two competing goals: To\nmaximize information or accuracy with respect to a target while simultaneously\nmaximizing invariance or independence with respect to a set of protected\nfeatures (e.g.\\ for fairness, privacy, etc). Despite its abundant applications\nin the aforementioned domains, theoretical understanding on the limits and\ntradeoffs of invariant representations is still severely lacking. In this\npaper, we provide an information theoretic analysis of this general and\nimportant problem under both classification and regression settings. In both\ncases, we analyze the inherent tradeoffs between accuracy and invariance by\nproviding a geometric characterization of the feasible region in the\ninformation plane, where we connect the geometric properties of this feasible\nregion to the fundamental limitations of the tradeoff problem. In the\nregression setting, we further give a complete and exact characterization of\nthe frontier between accuracy and invariance. Although our contributions are\nmainly theoretical, we also demonstrate the practical applications of our\nresults in certifying the suboptimality of certain representation learning\nalgorithms in both classification and regression tasks. Our results shed new\nlight on this fundamental problem by providing insights on the interplay\nbetween accuracy and invariance. These results deepen our understanding of this\nfundamental problem and may be useful in guiding the design of future\nrepresentation learning algorithms.",
          "link": "http://arxiv.org/abs/2012.10713",
          "publishedOn": "2021-07-26T02:00:59.536Z",
          "wordCount": 721,
          "title": "Fundamental Limits and Tradeoffs in Invariant Representation Learning. (arXiv:2012.10713v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1\">Yotam Nitzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1\">Rinon Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_O/0/1/0/all/0/1\">Ofir Brenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>",
          "description": "We propose a novel method for solving regression tasks using few-shot or weak\nsupervision. At the core of our method is the fundamental observation that GANs\nare incredibly successful at encoding semantic information within their latent\nspace, even in a completely unsupervised setting. For modern generative\nframeworks, this semantic encoding manifests as smooth, linear directions which\naffect image attributes in a disentangled manner. These directions have been\nwidely used in GAN-based image editing. We show that such directions are not\nonly linear, but that the magnitude of change induced on the respective\nattribute is approximately linear with respect to the distance traveled along\nthem. By leveraging this observation, our method turns a pre-trained GAN into a\nregression model, using as few as two labeled samples. This enables solving\nregression tasks on datasets and attributes which are difficult to produce\nquality supervision for. Additionally, we show that the same latent-distances\ncan be used to sort collections of images by the strength of given attributes,\neven in the absence of explicit supervision. Extensive experimental evaluations\ndemonstrate that our method can be applied across a wide range of domains,\nleverage multiple latent direction discovery frameworks, and achieve\nstate-of-the-art results in few-shot and low-supervision settings, even when\ncompared to methods designed to tackle a single task.",
          "link": "http://arxiv.org/abs/2107.11186",
          "publishedOn": "2021-07-26T02:00:59.528Z",
          "wordCount": 657,
          "title": "LARGE: Latent-Based Regression through GAN Semantics. (arXiv:2107.11186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11357",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Harris_C/0/1/0/all/0/1\">Chris Harris</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pymar_R/0/1/0/all/0/1\">Richard Pymar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rowat_C/0/1/0/all/0/1\">Colin Rowat</a>",
          "description": "The Shapley value is one of the most widely used model-agnostic measures of\nfeature importance in explainable AI: it has clear axiomatic foundations, is\nguaranteed to uniquely exist, and has a clear interpretation as a feature's\naverage effect on a model's prediction. We introduce joint Shapley values,\nwhich directly extend the Shapley axioms. This preserves the classic Shapley\nvalue's intuitions: joint Shapley values measure a set of features' average\neffect on a model's prediction. We prove the uniqueness of joint Shapley\nvalues, for any order of explanation. Results for games show that joint Shapley\nvalues present different insights from existing interaction indices, which\nassess the effect of a feature within a set of features. Deriving joint Shapley\nvalues in ML attribution problems thus gives us the first measure of the joint\neffect of sets of features on model predictions. In a dataset with binary\nfeatures, we present a presence-adjusted method for calculating global values\nthat retains the efficiency property.",
          "link": "http://arxiv.org/abs/2107.11357",
          "publishedOn": "2021-07-26T02:00:59.521Z",
          "wordCount": 605,
          "title": "Joint Shapley values: a measure of joint feature importance. (arXiv:2107.11357v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Axel_M/0/1/0/all/0/1\">Marmoret Axel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nancy_B/0/1/0/all/0/1\">Bertin Nancy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeremy_C/0/1/0/all/0/1\">Cohen Jeremy</a>",
          "description": "Music is an art, perceived in unique ways by every listener, coming from\nacoustic signals. In the meantime, standards as musical scores exist to\ndescribe it. Even if humans can make this transcription, it is costly in terms\nof time and efforts, even more with the explosion of information consecutively\nto the rise of the Internet. In that sense, researches are driven in the\ndirection of Automatic Music Transcription. While this task is considered\nsolved in the case of single notes, it is still open when notes superpose\nthemselves, forming chords. This report aims at developing some of the existing\ntechniques towards Music Transcription, particularly matrix factorization, and\nintroducing the concept of multi-channel automatic music transcription. This\nconcept will be explored with mathematical objects called tensors.",
          "link": "http://arxiv.org/abs/2107.11250",
          "publishedOn": "2021-07-26T02:00:59.512Z",
          "wordCount": 581,
          "title": "Multi-Channel Automatic Music Transcription Using Tensor Algebra. (arXiv:2107.11250v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1\">Xuan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Ying Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Recent studies in neuroscience show great potential of functional brain\nnetworks constructed from fMRI data for popularity modeling and clinical\npredictions. However, existing functional brain networks are noisy and unaware\nof downstream prediction tasks, while also incompatible with recent powerful\nmachine learning models of GNNs. In this work, we develop an end-to-end\ntrainable pipeline to extract prominent fMRI features, generate brain networks,\nand make predictions with GNNs, all under the guidance of downstream prediction\ntasks. Preliminary experiments on the PNC fMRI data show the superior\neffectiveness and unique interpretability of our framework.",
          "link": "http://arxiv.org/abs/2107.11247",
          "publishedOn": "2021-07-26T02:00:59.477Z",
          "wordCount": 552,
          "title": "Effective and Interpretable fMRI Analysis via Functional Brain Network Generation. (arXiv:2107.11247v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaoqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theisen_R/0/1/0/all/0/1\">Ryan Theisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Joe Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "Viewing neural network models in terms of their loss landscapes has a long\nhistory in the statistical mechanics approach to learning, and in recent years\nit has received attention within machine learning proper. Among other things,\nlocal metrics (such as the smoothness of the loss landscape) have been shown to\ncorrelate with global properties of the model (such as good generalization).\nHere, we perform a detailed empirical analysis of the loss landscape structure\nof thousands of neural network models, systematically varying learning tasks,\nmodel architectures, and/or quantity/quality of data. By considering a range of\nmetrics that attempt to capture different aspects of the loss landscape, we\ndemonstrate that the best test accuracy is obtained when: the loss landscape is\nglobally well-connected; ensembles of trained models are more similar to each\nother; and models converge to locally smooth regions. We also show that\nglobally poorly-connected landscapes can arise when models are small or when\nthey are trained to lower quality data; and that, if the loss landscape is\nglobally poorly-connected, then training to zero loss can actually lead to\nworse test accuracy. Based on these results, we develop a simple\none-dimensional model with load-like and temperature-like parameters, we\nintroduce the notion of an \\emph{effective loss landscape} depending on these\nparameters, and we interpret our results in terms of a \\emph{rugged convexity}\nof the loss landscape. When viewed through this lens, our detailed empirical\nresults shed light on phases of learning (and consequent double descent\nbehavior), fundamental versus incidental determinants of good generalization,\nthe role of load-like and temperature-like parameters in the learning process,\ndifferent influences on the loss landscape from model and data, and the\nrelationships between local and global metrics, all topics of recent interest.",
          "link": "http://arxiv.org/abs/2107.11228",
          "publishedOn": "2021-07-26T02:00:59.469Z",
          "wordCount": 727,
          "title": "Taxonomizing local versus global structure in neural network loss landscapes. (arXiv:2107.11228v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Heatmap-based methods dominate in the field of human pose estimation by\nmodelling the output distribution through likelihood heatmaps. In contrast,\nregression-based methods are more efficient but suffer from inferior\nperformance. In this work, we explore maximum likelihood estimation (MLE) to\ndevelop an efficient and effective regression-based methods. From the\nperspective of MLE, adopting different regression losses is making different\nassumptions about the output density function. A density function closer to the\ntrue distribution leads to a better regression performance. In light of this,\nwe propose a novel regression paradigm with Residual Log-likelihood Estimation\n(RLE) to capture the underlying output distribution. Concretely, RLE learns the\nchange of the distribution instead of the unreferenced underlying distribution\nto facilitate the training process. With the proposed reparameterization\ndesign, our method is compatible with off-the-shelf flow models. The proposed\nmethod is effective, efficient and flexible. We show its potential in various\nhuman pose estimation tasks with comprehensive experiments. Compared to the\nconventional regression paradigm, regression with RLE bring 12.4 mAP\nimprovement on MSCOCO without any test-time overhead. Moreover, for the first\ntime, especially on multi-person pose estimation, our regression method is\nsuperior to the heatmap-based methods. Our code is available at\nhttps://github.com/Jeff-sjtu/res-loglikelihood-regression",
          "link": "http://arxiv.org/abs/2107.11291",
          "publishedOn": "2021-07-26T02:00:59.462Z",
          "wordCount": 647,
          "title": "Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenxuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1\">Yiming Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qingyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Liming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>",
          "description": "This paper introduces the sixth Oriental Language Recognition (OLR) 2021\nChallenge, which intends to improve the performance of language recognition\nsystems and speech recognition systems within multilingual scenarios. The data\nprofile, four tasks, two baselines, and the evaluation principles are\nintroduced in this paper. In addition to the Language Identification (LID)\ntasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to\nOLR 2021 Challenge for the first time. The challenge this year focuses on more\npractical and challenging problems, with four tasks: (1) constrained LID, (2)\nunconstrained LID, (3) constrained multilingual ASR, (4) unconstrained\nmultilingual ASR. Baselines for LID tasks and multilingual ASR tasks are\nprovided, respectively. The LID baseline system is an extended TDNN x-vector\nmodel constructed with Pytorch. A transformer-based end-to-end model is\nprovided as the multilingual ASR baseline system. These recipes will be online\npublished, and available for participants to construct their own LID or ASR\nsystems. The baseline results demonstrate that those tasks are rather\nchallenging and deserve more effort to achieve better performance.",
          "link": "http://arxiv.org/abs/2107.11113",
          "publishedOn": "2021-07-26T02:00:59.454Z",
          "wordCount": 636,
          "title": "OLR 2021 Challenge: Datasets, Rules and Baselines. (arXiv:2107.11113v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11284",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Yip_K/0/1/0/all/0/1\">Kai Hou Yip</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Changeat_Q/0/1/0/all/0/1\">Quentin Changeat</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nikolaou_N/0/1/0/all/0/1\">Nikolaos Nikolaou</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Morvan_M/0/1/0/all/0/1\">Mario Morvan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Edwards_B/0/1/0/all/0/1\">Billy Edwards</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Waldmann_I/0/1/0/all/0/1\">Ingo P. Waldmann</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tinetti_G/0/1/0/all/0/1\">Giovanna Tinetti</a>",
          "description": "Deep learning algorithms are growing in popularity in the field of\nexoplanetary science due to their ability to model highly non-linear relations\nand solve interesting problems in a data-driven manner. Several works have\nattempted to perform fast retrievals of atmospheric parameters with the use of\nmachine learning algorithms like deep neural networks (DNNs). Yet, despite\ntheir high predictive power, DNNs are also infamous for being 'black boxes'. It\nis their apparent lack of explainability that makes the astrophysics community\nreluctant to adopt them. What are their predictions based on? How confident\nshould we be in them? When are they wrong and how wrong can they be? In this\nwork, we present a number of general evaluation methodologies that can be\napplied to any trained model and answer questions like these. In particular, we\ntrain three different popular DNN architectures to retrieve atmospheric\nparameters from exoplanet spectra and show that all three achieve good\npredictive performance. We then present an extensive analysis of the\npredictions of DNNs, which can inform us - among other things - of the\ncredibility limits for atmospheric parameters for a given instrument and model.\nFinally, we perform a perturbation-based sensitivity analysis to identify to\nwhich features of the spectrum the outcome of the retrieval is most sensitive.\nWe conclude that for different molecules, the wavelength ranges to which the\nDNN's predictions are most sensitive, indeed coincide with their characteristic\nabsorption regions. The methodologies presented in this work help to improve\nthe evaluation of DNNs and to grant interpretability to their predictions.",
          "link": "http://arxiv.org/abs/2011.11284",
          "publishedOn": "2021-07-26T02:00:59.447Z",
          "wordCount": 747,
          "title": "Peeking inside the Black Box: Interpreting Deep Learning Models for Exoplanet Atmospheric Retrievals. (arXiv:2011.11284v2 [astro-ph.EP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11114",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Farchi_A/0/1/0/all/0/1\">Alban Farchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bocquet_M/0/1/0/all/0/1\">Marc Bocquet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Laloyaux_P/0/1/0/all/0/1\">Patrick Laloyaux</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bonavita_M/0/1/0/all/0/1\">Massimo Bonavita</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Malartic_Q/0/1/0/all/0/1\">Quentin Malartic</a>",
          "description": "Recent studies have shown that it is possible to combine machine learning\nmethods with data assimilation to reconstruct a dynamical system using only\nsparse and noisy observations of that system. The same approach can be used to\ncorrect the error of a knowledge-based model. The resulting surrogate model is\nhybrid, with a statistical part supplementing a physical part. In practice, the\ncorrection can be added as an integrated term (i.e. in the model resolvent) or\ndirectly inside the tendencies of the physical model. The resolvent correction\nis easy to implement. The tendency correction is more technical, in particular\nit requires the adjoint of the physical model, but also more flexible. We use\nthe two-scale Lorenz model to compare the two methods. The accuracy in\nlong-range forecast experiments is somewhat similar between the surrogate\nmodels using the resolvent correction and the tendency correction. By contrast,\nthe surrogate models using the tendency correction significantly outperform the\nsurrogate models using the resolvent correction in data assimilation\nexperiments. Finally, we show that the tendency correction opens the\npossibility to make online model error correction, i.e. improving the model\nprogressively as new observations become available. The resulting algorithm can\nbe seen as a new formulation of weak-constraint 4D-Var. We compare online and\noffline learning using the same framework with the two-scale Lorenz system, and\nshow that with online learning, it is possible to extract all the information\nfrom sparse and noisy observations.",
          "link": "http://arxiv.org/abs/2107.11114",
          "publishedOn": "2021-07-26T02:00:59.420Z",
          "wordCount": 691,
          "title": "A comparison of combined data assimilation and machine learning methods for offline and online model error correction. (arXiv:2107.11114v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_R/0/1/0/all/0/1\">Reena Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shubham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Anand Raghunathan</a>",
          "description": "Precision scaling has emerged as a popular technique to optimize the compute\nand storage requirements of Deep Neural Networks (DNNs). Efforts toward\ncreating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum\nprecision required to achieve a given network-level accuracy varies\nconsiderably across networks, and even across layers within a network,\nrequiring support for variable precision in DNN hardware. Previous proposals\nsuch as bit-serial hardware incur high overheads, significantly diminishing the\nbenefits of lower precision. To efficiently support precision\nre-configurability in DNN accelerators, we introduce an approximate computing\nmethod wherein DNN computations are performed block-wise (a block is a group of\nbits) and re-configurability is supported at the granularity of blocks. Results\nof block-wise computations are composed in an approximate manner to enable\nefficient re-configurability. We design a DNN accelerator that embodies\napproximate blocked computation and propose a method to determine a suitable\napproximation configuration for a given DNN. By varying the approximation\nconfigurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement\nin system energy and performance respectively, over an 8-bit fixed-point (FxP8)\nbaseline, with negligible loss in classification accuracy. Further, by varying\nthe approximation configurations across layers and data-structures within DNNs,\nwe achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and\nperformance respectively, with negligible accuracy loss.",
          "link": "http://arxiv.org/abs/2011.13000",
          "publishedOn": "2021-07-26T02:00:59.410Z",
          "wordCount": 676,
          "title": "Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration. (arXiv:2011.13000v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navratil_J/0/1/0/all/0/1\">Jiri Navratil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Matthew Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elder_B/0/1/0/all/0/1\">Benjamin Elder</a>",
          "description": "Generating high quality uncertainty estimates for sequential regression,\nparticularly deep recurrent networks, remains a challenging and open problem.\nExisting approaches often make restrictive assumptions (such as stationarity)\nyet still perform poorly in practice, particularly in presence of real world\nnon-stationary signals and drift. This paper describes a flexible method that\ncan generate symmetric and asymmetric uncertainty estimates, makes no\nassumptions about stationarity, and outperforms competitive baselines on both\ndrift and non drift scenarios. This work helps make sequential regression more\neffective and practical for use in real-world applications, and is a powerful\nnew addition to the modeling toolbox for sequential uncertainty quantification\nin general.",
          "link": "http://arxiv.org/abs/2007.01350",
          "publishedOn": "2021-07-26T02:00:59.398Z",
          "wordCount": 566,
          "title": "Uncertainty Prediction for Deep Sequential Regression Using Meta Models. (arXiv:2007.01350v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Satya Narayan Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlin_B/0/1/0/all/0/1\">Benjamin M. Marlin</a>",
          "description": "Irregularly sampled time series commonly occur in several domains where they\npresent a significant challenge to standard deep learning models. In this\npaper, we propose a new deep learning framework for probabilistic interpolation\nof irregularly sampled time series that we call the Heteroscedastic Temporal\nVariational Autoencoder (HeTVAE). HeTVAE includes a novel input layer to encode\ninformation about input observation sparsity, a temporal VAE architecture to\npropagate uncertainty due to input sparsity, and a heteroscedastic output layer\nto enable variable uncertainty in output interpolations. Our results show that\nthe proposed architecture is better able to reflect variable uncertainty\nthrough time due to sparse and irregular sampling than a range of baseline and\ntraditional models, as well as recently proposed deep latent variable models\nthat use homoscedastic output layers.",
          "link": "http://arxiv.org/abs/2107.11350",
          "publishedOn": "2021-07-26T02:00:59.389Z",
          "wordCount": 562,
          "title": "Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series. (arXiv:2107.11350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huyen N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Jake Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan V.T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tommy Dang</a>",
          "description": "This paper presents VisMCA, an interactive visual analytics system that\nsupports deepening understanding in ML results, augmenting users' capabilities\nin correcting misclassification, and providing an analysis of underlying\npatterns, in response to the VAST Challenge 2020 Mini-Challenge 2. VisMCA\nfacilitates tracking provenance and provides a comprehensive view of object\ndetection results, easing re-labeling, and producing reliable, corrected data\nfor future training. Our solution implements multiple analytical views on\nvisual analysis to offer a deep insight for underlying pattern discovery.",
          "link": "http://arxiv.org/abs/2107.11181",
          "publishedOn": "2021-07-26T02:00:59.381Z",
          "wordCount": 563,
          "title": "VisMCA: A Visual Analytics System for Misclassification Correction and Analysis. VAST Challenge 2020, Mini-Challenge 2 Award: Honorable Mention for Detailed Analysis of Patterns of Misclassification. (arXiv:2107.11181v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Opala_A/0/1/0/all/0/1\">Andrzej Opala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panico_R/0/1/0/all/0/1\">Riccardo Panico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardizzone_V/0/1/0/all/0/1\">Vincenzo Ardizzone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietka_B/0/1/0/all/0/1\">Barbara Pietka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szczytko_J/0/1/0/all/0/1\">Jacek Szczytko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanvitto_D/0/1/0/all/0/1\">Daniele Sanvitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matuszewski_M/0/1/0/all/0/1\">Micha&#x142; Matuszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballarini_D/0/1/0/all/0/1\">Dario Ballarini</a>",
          "description": "In contrast to software simulations of neural networks, hardware or\nneuromorphic implementations have often limited or no tunability. While such\nnetworks promise great improvements in terms of speed and energy efficiency,\ntheir performance is limited by the difficulty to apply efficient teaching. We\npropose a system of non-tunable exciton-polariton nodes and an efficient\nteaching method that relies on the precise measurement of the nonlinear node\nresponse and the subsequent use of the backpropagation algorithm. We\ndemonstrate experimentally that the classification accuracy in the MNIST\nhandwritten digit benchmark is greatly improved compared to the case where\nbackpropagation is not used.",
          "link": "http://arxiv.org/abs/2107.11156",
          "publishedOn": "2021-07-26T02:00:59.362Z",
          "wordCount": 549,
          "title": "Teaching a neural network with non-tunable exciton-polariton nodes. (arXiv:2107.11156v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>",
          "description": "Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.",
          "link": "http://arxiv.org/abs/2107.11170",
          "publishedOn": "2021-07-26T02:00:59.295Z",
          "wordCount": 709,
          "title": "Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wit_J/0/1/0/all/0/1\">J.S. Panman de Wit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_J/0/1/0/all/0/1\">J. van der Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1\">D. Bucur</a>",
          "description": "Mobile malware are malicious programs that target mobile devices. They are an\nincreasing problem, as seen in the rise of detected mobile malware samples per\nyear. The number of active smartphone users is expected to grow, stressing the\nimportance of research on the detection of mobile malware. Detection methods\nfor mobile malware exist but are still limited.\n\nIn this paper, we provide an overview of the performance of machine learning\n(ML) techniques to detect malware on Android, without using privileged access.\nThe ML-classifiers use device information such as the CPU usage, battery usage,\nand memory usage for the detection of 10 subtypes of Mobile Trojans on the\nAndroid Operating System (OS).\n\nWe use a real-life dataset containing device and malware data from 47 users\nfor a year (2016). We examine which features, i.e. aspects, of a device, are\nmost important to monitor to detect (subtypes of) Mobile Trojans. The focus of\nthis paper is on dynamic hardware features. Using these dynamic features we\napply state-of-the-art machine learning classifiers: Random Forest, K-Nearest\nNeighbour, and AdaBoost. We show classification results on different feature\nsets, making a distinction between global device features, and specific app\nfeatures. None of the measured feature sets require privileged access.\n\nOur results show that the Random Forest classifier performs best as a general\nmalware classifier: across 10 subtypes of Mobile Trojans, it achieves an F1\nscore of 0.73 with a False Positive Rate (FPR) of 0.009 and a False Negative\nRate (FNR) of 0.380. The Random Forest, K-Nearest Neighbours, and AdaBoost\nclassifiers achieve F1 scores above 0.72, an FPR below 0.02 and, an FNR below\n0.33, when trained separately to detect each subtype of Mobile Trojans.",
          "link": "http://arxiv.org/abs/2107.11167",
          "publishedOn": "2021-07-26T02:00:59.288Z",
          "wordCount": 745,
          "title": "Dynamic detection of mobile malware using smartphone data and machine learning. (arXiv:2107.11167v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whittington_J/0/1/0/all/0/1\">James C.R. Whittington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabra_R/0/1/0/all/0/1\">Rishabh Kabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthey_L/0/1/0/all/0/1\">Loic Matthey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgess_C/0/1/0/all/0/1\">Christopher P. Burgess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerchner_A/0/1/0/all/0/1\">Alexander Lerchner</a>",
          "description": "Learning structured representations of visual scenes is currently a major\nbottleneck to bridging perception with reasoning. While there has been exciting\nprogress with slot-based models, which learn to segment scenes into sets of\nobjects, learning configurational properties of entire groups of objects is\nstill under-explored. To address this problem, we introduce Constellation, a\nnetwork that learns relational abstractions of static visual scenes, and\ngeneralises these abstractions over sensory particularities, thus offering a\npotential basis for abstract relational reasoning. We further show that this\nbasis, along with language association, provides a means to imagine sensory\ncontent in new ways. This work is a first step in the explicit representation\nof visual relationships and using them for complex cognitive procedures.",
          "link": "http://arxiv.org/abs/2107.11153",
          "publishedOn": "2021-07-26T02:00:59.281Z",
          "wordCount": 563,
          "title": "Constellation: Learning relational abstractions over objects for compositional imagination. (arXiv:2107.11153v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shengpu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1\">Jenna Wiens</a>",
          "description": "Reinforcement learning (RL) can be used to learn treatment policies and aid\ndecision making in healthcare. However, given the need for generalization over\ncomplex state/action spaces, the incorporation of function approximators (e.g.,\ndeep neural networks) requires model selection to reduce overfitting and\nimprove policy performance at deployment. Yet a standard validation pipeline\nfor model selection requires running a learned policy in the actual\nenvironment, which is often infeasible in a healthcare setting. In this work,\nwe investigate a model selection pipeline for offline RL that relies on\noff-policy evaluation (OPE) as a proxy for validation performance. We present\nan in-depth analysis of popular OPE methods, highlighting the additional\nhyperparameters and computational requirements (fitting/inference of auxiliary\nmodels) when used to rank a set of candidate policies. We compare the utility\nof different OPE methods as part of the model selection pipeline in the context\nof learning to treat patients with sepsis. Among all the OPE methods we\nconsidered, fitted Q evaluation (FQE) consistently leads to the best validation\nranking, but at a high computational cost. To balance this trade-off between\naccuracy of ranking and computational efficiency, we propose a simple two-stage\napproach to accelerate model selection by avoiding potentially unnecessary\ncomputation. Our work serves as a practical guide for offline RL model\nselection and can help RL practitioners select policies using real-world\ndatasets. To facilitate reproducibility and future extensions, the code\naccompanying this paper is available online at\nhttps://github.com/MLD3/OfflineRL_ModelSelection.",
          "link": "http://arxiv.org/abs/2107.11003",
          "publishedOn": "2021-07-26T02:00:59.232Z",
          "wordCount": 684,
          "title": "Model Selection for Offline Reinforcement Learning: Practical Considerations for Healthcare Settings. (arXiv:2107.11003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brophy_E/0/1/0/all/0/1\">Eoin Brophy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_T/0/1/0/all/0/1\">Tomas Ward</a>",
          "description": "Generative adversarial networks (GANs) studies have grown exponentially in\nthe past few years. Their impact has been seen mainly in the computer vision\nfield with realistic image and video manipulation, especially generation,\nmaking significant advancements. While these computer vision advances have\ngarnered much attention, GAN applications have diversified across disciplines\nsuch as time series and sequence generation. As a relatively new niche for\nGANs, fieldwork is ongoing to develop high quality, diverse and private time\nseries data. In this paper, we review GAN variants designed for time series\nrelated applications. We propose a taxonomy of discrete-variant GANs and\ncontinuous-variant GANs, in which GANs deal with discrete time series and\ncontinuous time series data. Here we showcase the latest and most popular\nliterature in this field; their architectures, results, and applications. We\nalso provide a list of the most popular evaluation metrics and their\nsuitability across applications. Also presented is a discussion of privacy\nmeasures for these GANs and further protections and directions for dealing with\nsensitive data. We aim to frame clearly and concisely the latest and\nstate-of-the-art research in this area and their applications to real-world\ntechnologies.",
          "link": "http://arxiv.org/abs/2107.11098",
          "publishedOn": "2021-07-26T02:00:59.225Z",
          "wordCount": 625,
          "title": "Generative adversarial networks in time series: A survey and taxonomy. (arXiv:2107.11098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weien Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Physics-informed neural networks (PINNs) have been widely used to solve\nvarious scientific computing problems. However, large training costs limit\nPINNs for some real-time applications. Although some works have been proposed\nto improve the training efficiency of PINNs, few consider the influence of\ninitialization. To this end, we propose a New Reptile initialization based\nPhysics-Informed Neural Network (NRPINN). The original Reptile algorithm is a\nmeta-learning initialization method based on labeled data. PINNs can be trained\nwith less labeled data or even without any labeled data by adding partial\ndifferential equations (PDEs) as a penalty term into the loss function.\nInspired by this idea, we propose the new Reptile initialization to sample more\ntasks from the parameterized PDEs and adapt the penalty term of the loss. The\nnew Reptile initialization can acquire initialization parameters from related\ntasks by supervised, unsupervised, and semi-supervised learning. Then, PINNs\nwith initialization parameters can efficiently solve PDEs. Besides, the new\nReptile initialization can also be used for the variants of PINNs. Finally, we\ndemonstrate and verify the NRPINN considering both forward problems, including\nsolving Poisson, Burgers, and Schr\\\"odinger equations, as well as inverse\nproblems, where unknown parameters in the PDEs are estimated. Experimental\nresults show that the NRPINN training is much faster and achieves higher\naccuracy than PINNs with other initialization methods.",
          "link": "http://arxiv.org/abs/2107.10991",
          "publishedOn": "2021-07-26T02:00:59.218Z",
          "wordCount": 649,
          "title": "A novel meta-learning initialization method for physics-informed neural networks. (arXiv:2107.10991v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11099",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Jing_Y/0/1/0/all/0/1\">Yu Jing</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wu_C/0/1/0/all/0/1\">Chonghang Wu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Fu_W/0/1/0/all/0/1\">Wenbing Fu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1\">Xiaogang Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>",
          "description": "With the rapid growth of qubit numbers and coherence times in quantum\nhardware technology, implementing shallow neural networks on the so-called\nNoisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of\ninterest. Many quantum (convolutional) circuit ansaetze are proposed for\ngrayscale images classification tasks with promising empirical results.\nHowever, when applying these ansaetze on RGB images, the intra-channel\ninformation that is useful for vision tasks is not extracted effectively. In\nthis paper, we propose two types of quantum circuit ansaetze to simulate\nconvolution operations on RGB images, which differ in the way how inter-channel\nand intra-channel information are extracted. To the best of our knowledge, this\nis the first work of a quantum convolutional circuit to deal with RGB images\neffectively, with a higher test accuracy compared to the purely classical CNNs.\nWe also investigate the relationship between the size of quantum circuit ansatz\nand the learnability of the hybrid quantum-classical convolutional neural\nnetwork. Through experiments based on CIFAR-10 and MNIST datasets, we\ndemonstrate that a larger size of the quantum circuit ansatz improves\npredictive performance in multiclass classification tasks, providing useful\ninsights for near term quantum algorithm developments.",
          "link": "http://arxiv.org/abs/2107.11099",
          "publishedOn": "2021-07-26T02:00:59.211Z",
          "wordCount": 633,
          "title": "RGB Image Classification with Quantum Convolutional Ansaetze. (arXiv:2107.11099v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jie Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jude_C/0/1/0/all/0/1\">Curran Jude</a>",
          "description": "We consider unsupervised cell nuclei segmentation in this paper. Exploiting\nthe recently-proposed unpaired image-to-image translation between cell nuclei\nimages and randomly synthetic masks, existing approaches, e.g., CycleGAN, have\nachieved encouraging results. However, these methods usually take a two-stage\npipeline and fail to learn end-to-end in cell nuclei images. More seriously,\nthey could lead to the lossy transformation problem, i.e., the content\ninconsistency between the original images and the corresponding segmentation\noutput. To address these limitations, we propose a novel end-to-end\nunsupervised framework called Aligned Disentangling Generative Adversarial\nNetwork (AD-GAN). Distinctively, AD-GAN introduces representation\ndisentanglement to separate content representation (the underling spatial\nstructure) from style representation (the rendering of the structure). With\nthis framework, spatial structure can be preserved explicitly, enabling a\nsignificant reduction of macro-level lossy transformation. We also propose a\nnovel training algorithm able to align the disentangled content in the latent\nspace to reduce micro-level lossy transformation. Evaluations on real-world 2D\nand 3D datasets show that AD-GAN substantially outperforms the other comparison\nmethods and the professional software both quantitatively and qualitatively.\nSpecifically, the proposed AD-GAN leads to significant improvement over the\ncurrent best unsupervised methods by an average 17.8% relatively (w.r.t. the\nmetric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN\neven performs competitive with the best supervised models, taking a further\nleap towards end-to-end unsupervised nuclei segmentation.",
          "link": "http://arxiv.org/abs/2107.11022",
          "publishedOn": "2021-07-26T02:00:59.204Z",
          "wordCount": 674,
          "title": "AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1\">Patrik Puchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermosilla_P/0/1/0/all/0/1\">Pedro Hermosilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>",
          "description": "Density estimation plays a crucial role in many data analysis tasks, as it\ninfers a continuous probability density function (PDF) from discrete samples.\nThus, it is used in tasks as diverse as analyzing population data, spatial\nlocations in 2D sensor readings, or reconstructing scenes from 3D scans. In\nthis paper, we introduce a learned, data-driven deep density estimation (DDE)\nto infer PDFs in an accurate and efficient manner, while being independent of\ndomain dimensionality or sample size. Furthermore, we do not require access to\nthe original PDF during estimation, neither in parametric form, nor as priors,\nor in the form of many samples. This is enabled by training an unstructured\nconvolutional neural network on an infinite stream of synthetic PDFs, as\nunbound amounts of synthetic training data generalize better across a deck of\nnatural PDFs than any natural finite training data will do. Thus, we hope that\nour publicly available DDE method will be beneficial in many areas of data\nanalysis, where continuous models are to be estimated from discrete\nobservations.",
          "link": "http://arxiv.org/abs/2107.11085",
          "publishedOn": "2021-07-26T02:00:59.184Z",
          "wordCount": 629,
          "title": "Data-driven deep density estimation. (arXiv:2107.11085v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anastasiu_C/0/1/0/all/0/1\">Cristian Anastasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_H/0/1/0/all/0/1\">Hanna Behnke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luck_S/0/1/0/all/0/1\">Sarah L&#xfc;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malesevic_V/0/1/0/all/0/1\">Viktor Malesevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najmi_A/0/1/0/all/0/1\">Aamna Najmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poveda_Panter_J/0/1/0/all/0/1\">Javier Poveda-Panter</a>",
          "description": "Automated headline generation for online news articles is not a trivial task\n- machine generated titles need to be grammatically correct, informative,\ncapture attention and generate search traffic without being \"click baits\" or\n\"fake news\". In this paper we showcase how a pre-trained language model can be\nleveraged to create an abstractive news headline generator for German language.\nWe incorporate state of the art fine-tuning techniques for abstractive text\nsummarization, i.e. we use different optimizers for the encoder and decoder\nwhere the former is pre-trained and the latter is trained from scratch. We\nmodify the headline generation to incorporate frequently sought keywords\nrelevant for search engine optimization. We conduct experiments on a German\nnews data set and achieve a ROUGE-L-gram F-score of 40.02. Furthermore, we\naddress the limitations of ROUGE for measuring the quality of text\nsummarization by introducing a sentence similarity metric and human evaluation.",
          "link": "http://arxiv.org/abs/2107.10935",
          "publishedOn": "2021-07-26T02:00:59.176Z",
          "wordCount": 591,
          "title": "DeepTitle -- Leveraging BERT to generate Search Engine Optimized Headlines. (arXiv:2107.10935v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10980",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Xia_S/0/1/0/all/0/1\">Steve Q. Xia</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>",
          "description": "We investigate the effectiveness of different machine learning methodologies\nin predicting economic cycles. We identify the deep learning methodology of\nBi-LSTM with Autoencoder as the most accurate model to forecast the beginning\nand end of economic recessions in the U.S. We adopt commonly-available macro\nand market-condition features to compare the ability of different machine\nlearning models to generate good predictions both in-sample and out-of-sample.\nThe proposed model is flexible and dynamic when both predictive variables and\nmodel coefficients vary over time. It provided good out-of-sample predictions\nfor the past two recessions and early warning about the COVID-19 recession.",
          "link": "http://arxiv.org/abs/2107.10980",
          "publishedOn": "2021-07-26T02:00:59.169Z",
          "wordCount": 577,
          "title": "Economic Recession Prediction Using Deep Neural Network. (arXiv:2107.10980v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yunjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "Recent state-of-the-art active learning methods have mostly leveraged\nGenerative Adversarial Networks (GAN) for sample acquisition; however, GAN is\nusually known to suffer from instability and sensitivity to hyper-parameters.\nIn contrast to these methods, we propose in this paper a novel active learning\nframework that we call Maximum Classifier Discrepancy for Active Learning\n(MCDAL) which takes the prediction discrepancies between multiple classifiers.\nIn particular, we utilize two auxiliary classification layers that learn\ntighter decision boundaries by maximizing the discrepancies among them.\nIntuitively, the discrepancies in the auxiliary classification layers'\npredictions indicate the uncertainty in the prediction. In this regard, we\npropose a novel method to leverage the classifier discrepancies for the\nacquisition function for active learning. We also provide an interpretation of\nour idea in relation to existing GAN based active learning methods and domain\nadaptation frameworks. Moreover, we empirically demonstrate the utility of our\napproach where the performance of our approach exceeds the state-of-the-art\nmethods on several image classification and semantic segmentation datasets in\nactive learning setups.",
          "link": "http://arxiv.org/abs/2107.11049",
          "publishedOn": "2021-07-26T02:00:59.162Z",
          "wordCount": 614,
          "title": "MCDAL: Maximum Classifier Discrepancy for Active Learning. (arXiv:2107.11049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11042",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Abduallah_Y/0/1/0/all/0/1\">Yasser Abduallah</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1\">Jason T. L. Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Shen_Y/0/1/0/all/0/1\">Yucong Shen</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Alobaid_K/0/1/0/all/0/1\">Khalid A. Alobaid</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Criscuoli_S/0/1/0/all/0/1\">Serena Criscuoli</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_H/0/1/0/all/0/1\">Haimin Wang</a>",
          "description": "The Earth's primary source of energy is the radiant energy generated by the\nSun, which is referred to as solar irradiance, or total solar irradiance (TSI)\nwhen all of the radiation is measured. A minor change in the solar irradiance\ncan have a significant impact on the Earth's climate and atmosphere. As a\nresult, studying and measuring solar irradiance is crucial in understanding\nclimate changes and solar variability. Several methods have been developed to\nreconstruct total solar irradiance for long and short periods of time; however,\nthey are physics-based and rely on the availability of data, which does not go\nbeyond 9,000 years. In this paper we propose a new method, called TSInet, to\nreconstruct total solar irradiance by deep learning for short and long periods\nof time that span beyond the physical models' data availability. On the data\nthat are available, our method agrees well with the state-of-the-art\nphysics-based reconstruction models. To our knowledge, this is the first time\nthat deep learning has been used to reconstruct total solar irradiance for more\nthan 9,000 years.",
          "link": "http://arxiv.org/abs/2107.11042",
          "publishedOn": "2021-07-26T02:00:59.143Z",
          "wordCount": 628,
          "title": "Deep Learning Based Reconstruction of Total Solar Irradiance. (arXiv:2107.11042v1 [astro-ph.SR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Richman_R/0/1/0/all/0/1\">Ronald Richman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1\">Mario V. W&#xfc;thrich</a>",
          "description": "Deep learning models have gained great popularity in statistical modeling\nbecause they lead to very competitive regression models, often outperforming\nclassical statistical models such as generalized linear models. The\ndisadvantage of deep learning models is that their solutions are difficult to\ninterpret and explain, and variable selection is not easily possible because\ndeep learning models solve feature engineering and variable selection\ninternally in a nontransparent way. Inspired by the appealing structure of\ngeneralized linear models, we propose a new network architecture that shares\nsimilar features as generalized linear models, but provides superior predictive\npower benefiting from the art of representation learning. This new architecture\nallows for variable selection of tabular data and for interpretation of the\ncalibrated deep learning model, in fact, our approach provides an additive\ndecomposition in the spirit of Shapley values and integrated gradients.",
          "link": "http://arxiv.org/abs/2107.11059",
          "publishedOn": "2021-07-26T02:00:59.136Z",
          "wordCount": 583,
          "title": "LocalGLMnet: interpretable deep learning for tabular data. (arXiv:2107.11059v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Simin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Deep learning (DL) techniques have achieved great success in predictive\naccuracy in a variety of tasks, but deep neural networks (DNNs) are shown to\nproduce highly overconfident scores for even abnormal samples. Well-defined\nuncertainty indicates whether a model's output should (or should not) be\ntrusted and thus becomes critical in real-world scenarios which typically\ninvolves shifted input distributions due to many factors. Existing uncertainty\napproaches assume that testing samples from a different data distribution would\ninduce unreliable model predictions thus have higher uncertainty scores. They\nquantify model uncertainty by calibrating DL model's confidence of a given\ninput and evaluate the effectiveness in computer vision (CV) and natural\nlanguage processing (NLP)-related tasks. However, their methodologies'\nreliability may be compromised under programming tasks due to difference in\ndata representations and shift patterns. In this paper, we first define three\ndifferent types of distribution shift in program data and build a large-scale\nshifted Java dataset. We implement two common programming language tasks on our\ndataset to study the effect of each distribution shift on DL model performance.\nWe also propose a large-scale benchmark of existing state-of-the-art predictive\nuncertainty on programming tasks and investigate their effectiveness under data\ndistribution shift. Experiments show that program distribution shift does\ndegrade the DL model performance to varying degrees and that existing\nuncertainty methods all present certain limitations in quantifying uncertainty\non program dataset.",
          "link": "http://arxiv.org/abs/2107.10989",
          "publishedOn": "2021-07-26T02:00:59.128Z",
          "wordCount": 669,
          "title": "Estimating Predictive Uncertainty Under Program Data Distribution Shift. (arXiv:2107.10989v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shuo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hanshen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>",
          "description": "As one of the most fundamental problems in machine learning, statistics and\ndifferential privacy, Differentially Private Stochastic Convex Optimization\n(DP-SCO) has been extensively studied in recent years. However, most of the\nprevious work can only handle either regular data distribution or irregular\ndata in the low dimensional space case. To better understand the challenges\narising from irregular data distribution, in this paper we provide the first\nstudy on the problem of DP-SCO with heavy-tailed data in the high dimensional\nspace. In the first part we focus on the problem over some polytope constraint\n(such as the $\\ell_1$-norm ball). We show that if the loss function is smooth\nand its gradient has bounded second order moment, it is possible to get a (high\nprobability) error bound (excess population risk) of $\\tilde{O}(\\frac{\\log\nd}{(n\\epsilon)^\\frac{1}{3}})$ in the $\\epsilon$-DP model, where $n$ is the\nsample size and $d$ is the dimensionality of the underlying space. Next, for\nLASSO, if the data distribution that has bounded fourth-order moments, we\nimprove the bound to $\\tilde{O}(\\frac{\\log d}{(n\\epsilon)^\\frac{2}{5}})$ in the\n$(\\epsilon, \\delta)$-DP model. In the second part of the paper, we study sparse\nlearning with heavy-tailed data. We first revisit the sparse linear model and\npropose a truncated DP-IHT method whose output could achieve an error of\n$\\tilde{O}(\\frac{s^{*2}\\log d}{n\\epsilon})$, where $s^*$ is the sparsity of the\nunderlying parameter. Then we study a more general problem over the sparsity\n({\\em i.e.,} $\\ell_0$-norm) constraint, and show that it is possible to achieve\nan error of $\\tilde{O}(\\frac{s^{*\\frac{3}{2}}\\log d}{n\\epsilon})$, which is\nalso near optimal up to a factor of $\\tilde{O}{(\\sqrt{s^*})}$, if the loss\nfunction is smooth and strongly convex.",
          "link": "http://arxiv.org/abs/2107.11136",
          "publishedOn": "2021-07-26T02:00:59.121Z",
          "wordCount": 707,
          "title": "High Dimensional Differentially Private Stochastic Optimization with Heavy-tailed Data. (arXiv:2107.11136v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11107",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Al_Saffar_A/0/1/0/all/0/1\">A. Al-Saffar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Guo_L/0/1/0/all/0/1\">L. Guo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Abbosh_A/0/1/0/all/0/1\">A. Abbosh</a>",
          "description": "Electromagnetic medical imaging in the microwave regime is a hard problem\nnotorious for 1) instability 2) under-determinism. This two-pronged problem is\ntackled with a two-pronged solution that uses double compression to maximally\nutilizing the cheap unlabelled data to a) provide a priori information required\nto ease under-determinism and b) reduce sensitivity of inference to the input.\nThe result is a stable solver with a high resolution output. DeepHead is a\nfully data-driven implementation of the paradigm proposed in the context of\nmicrowave brain imaging. It infers the dielectric distribution of the brain at\na desired single frequency while making use of an input that spreads over a\nwide band of frequencies. The performance of the model is evaluated with both\nsimulations and human volunteers experiments. The inference made is juxtaposed\nwith ground-truth dielectric distribution in simulation case, and the golden\nMRI / CT imaging modalities of the volunteers in real-world case.",
          "link": "http://arxiv.org/abs/2107.11107",
          "publishedOn": "2021-07-26T02:00:59.113Z",
          "wordCount": 587,
          "title": "Introducing: DeepHead, Wide-band Electromagnetic Imaging Paradigm. (arXiv:2107.11107v1 [physics.med-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1\">Parshin Shojaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ran Jin</a>",
          "description": "Reducing the shortage of organ donations to meet the demands of patients on\nthe waiting list has being a major challenge in organ transplantation. Because\nof the shortage, organ matching decision is the most critical decision to\nassign the limited viable organs to the most suitable patients. Currently,\norgan matching decisions were only made by matching scores calculated via\nscoring models, which are built by the first principles. However, these models\nmay disagree with the actual post-transplantation matching performance (e.g.,\npatient's post-transplant quality of life (QoL) or graft failure measurements).\nIn this paper, we formulate the organ matching decision-making as a top-N\nrecommendation problem and propose an Adaptively Weighted Top-N Recommendation\n(AWTR) method. AWTR improves performance of the current scoring models by using\nlimited actual matching performance in historical data set as well as the\ncollected covariates from organ donors and patients. AWTR sacrifices the\noverall recommendation accuracy by emphasizing the recommendation and ranking\naccuracy for top-N matched patients. The proposed method is validated in a\nsimulation study, where KAS [60] is used to simulate the organ-patient\nrecommendation response. The results show that our proposed method outperforms\nseven state-of-the-art top-N recommendation benchmark methods.",
          "link": "http://arxiv.org/abs/2107.10971",
          "publishedOn": "2021-07-26T02:00:59.104Z",
          "wordCount": 625,
          "title": "Adaptively Weighted Top-N Recommendation for Organ Matching. (arXiv:2107.10971v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1\">Kuniaki Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teterwak_P/0/1/0/all/0/1\">Piotr Teterwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_B/0/1/0/all/0/1\">Ben Usman</a>",
          "description": "Progress in machine learning is typically measured by training and testing a\nmodel on the same distribution of data, i.e., the same domain. This\nover-estimates future accuracy on out-of-distribution data. The Visual Domain\nAdaptation (VisDA) 2021 competition tests models' ability to adapt to novel\ntest distributions and handle distributional shift. We set up unsupervised\ndomain adaptation challenges for image classifiers and will evaluate adaptation\nto novel viewpoints, backgrounds, modalities and degradation in quality. Our\nchallenge draws on large-scale publicly available datasets but constructs the\nevaluation across domains, rather that the traditional in-domain bench-marking.\nFurthermore, we focus on the difficult \"universal\" setting where, in addition\nto input distribution drift, methods may encounter missing and/or novel classes\nin the target dataset. Performance will be measured using a rigorous protocol,\ncomparing to state-of-the-art domain adaptation methods with the help of\nestablished metrics. We believe that the competition will encourage further\nimprovement in machine learning methods' ability to handle realistic data in\nmany deployment scenarios.",
          "link": "http://arxiv.org/abs/2107.11011",
          "publishedOn": "2021-07-26T02:00:59.097Z",
          "wordCount": 610,
          "title": "VisDA-2021 Competition Universal Domain Adaptation to Improve Performance on Out-of-Distribution Data. (arXiv:2107.11011v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Siyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chuanming Tang</a>",
          "description": "AI-based methods have been widely applied to tourism demand forecasting.\nHowever, current AI-based methods are short of the ability to process long-term\ndependency, and most of them lack interpretability. The Transformer used\ninitially for machine translation shows an incredible ability to long-term\ndependency processing. Based on the Transformer, we proposed a time series\nTransformer (Tsformer) with Encoder-Decoder architecture for tourism demand\nforecasting. The proposed Tsformer encodes long-term dependency with encoder,\ncaptures short-term dependency with decoder, and simplifies the attention\ninteractions under the premise of highlighting dominant attention through a\nseries of attention masking mechanisms. These improvements make the multi-head\nattention mechanism process the input sequence according to the time\nrelationship, contributing to better interpretability. What's more, the context\nprocessing ability of the Encoder-Decoder architecture allows adopting the\ncalendar of days to be forecasted to enhance the forecasting performance.\nExperiments conducted on the Jiuzhaigou valley and Siguniang mountain tourism\ndemand datasets with other nine baseline methods indicate that the proposed\nTsformer outperformed all baseline models in the short-term and long-term\ntourism demand forecasting tasks. Moreover, ablation studies demonstrate that\nthe adoption of the calendar of days to be forecasted contributes to the\nforecasting performance of the proposed Tsformer. For better interpretability,\nthe attention weight matrix visualization is performed. It indicates that the\nTsformer concentrates on seasonal features and days close to days to be\nforecast in short-term forecasting.",
          "link": "http://arxiv.org/abs/2107.10977",
          "publishedOn": "2021-07-26T02:00:59.088Z",
          "wordCount": 653,
          "title": "Tsformer: Time series Transformer for tourism demand forecasting. (arXiv:2107.10977v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11046",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Keith_B/0/1/0/all/0/1\">Brendan Keith</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Khristenko_U/0/1/0/all/0/1\">Ustim Khristenko</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wohlmuth_B/0/1/0/all/0/1\">Barbara Wohlmuth</a>",
          "description": "We develop a novel data-driven approach to modeling the atmospheric boundary\nlayer. This approach leads to a nonlocal, anisotropic synthetic turbulence\nmodel which we refer to as the deep rapid distortion (DRD) model. Our approach\nrelies on an operator regression problem which characterizes the best fitting\ncandidate in a general family of nonlocal covariance kernels parameterized in\npart by a neural network. This family of covariance kernels is expressed in\nFourier space and is obtained from approximate solutions to the Navier--Stokes\nequations at very high Reynolds numbers. Each member of the family incorporates\nimportant physical properties such as mass conservation and a realistic energy\ncascade. The DRD model can be calibrated with noisy data from field\nexperiments. After calibration, the model can be used to generate synthetic\nturbulent velocity fields. To this end, we provide a new numerical method based\non domain decomposition which delivers scalable, memory-efficient turbulence\ngeneration with the DRD model as well as others. We demonstrate the robustness\nof our approach with both filtered and noisy data coming from the 1968 Air\nForce Cambridge Research Laboratory Kansas experiments. Using this data, we\nwitness exceptional accuracy with the DRD model, especially when compared to\nthe International Electrotechnical Commission standard.",
          "link": "http://arxiv.org/abs/2107.11046",
          "publishedOn": "2021-07-26T02:00:59.067Z",
          "wordCount": 646,
          "title": "Learning the structure of wind: A data-driven nonlocal turbulence model for the atmospheric boundary layer. (arXiv:2107.11046v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10970",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chia Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1\">Marina Meil&#x103;</a>",
          "description": "The null space of the $k$-th order Laplacian $\\mathbf{\\mathcal L}_k$, known\nas the {\\em $k$-th homology vector space}, encodes the non-trivial topology of\na manifold or a network. Understanding the structure of the homology embedding\ncan thus disclose geometric or topological information from the data. The study\nof the null space embedding of the graph Laplacian $\\mathbf{\\mathcal L}_0$ has\nspurred new research and applications, such as spectral clustering algorithms\nwith theoretical guarantees and estimators of the Stochastic Block Model. In\nthis work, we investigate the geometry of the $k$-th homology embedding and\nfocus on cases reminiscent of spectral clustering. Namely, we analyze the {\\em\nconnected sum} of manifolds as a perturbation to the direct sum of their\nhomology embeddings. We propose an algorithm to factorize the homology\nembedding into subspaces corresponding to a manifold's simplest topological\ncomponents. The proposed framework is applied to the {\\em shortest homologous\nloop detection} problem, a problem known to be NP-hard in general. Our spectral\nloop detection algorithm scales better than existing methods and is effective\non diverse data such as point clouds and images.",
          "link": "http://arxiv.org/abs/2107.10970",
          "publishedOn": "2021-07-26T02:00:59.051Z",
          "wordCount": 616,
          "title": "The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian. (arXiv:2107.10970v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahid_O/0/1/0/all/0/1\">Osama Shahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouriyeh_S/0/1/0/all/0/1\">Seyedamin Pouriyeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parizi_R/0/1/0/all/0/1\">Reza M. Parizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Quan Z. Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1\">Gautam Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>",
          "description": "Federated Learning (FL) is known to perform Machine Learning tasks in a\ndistributed manner. Over the years, this has become an emerging technology\nespecially with various data protection and privacy policies being imposed FL\nallows performing machine learning tasks whilst adhering to these challenges.\nAs with the emerging of any new technology, there are going to be challenges\nand benefits. A challenge that exists in FL is the communication costs, as FL\ntakes place in a distributed environment where devices connected over the\nnetwork have to constantly share their updates this can create a communication\nbottleneck. In this paper, we present a survey of the research that is\nperformed to overcome the communication constraints in an FL setting.",
          "link": "http://arxiv.org/abs/2107.10996",
          "publishedOn": "2021-07-26T02:00:59.042Z",
          "wordCount": 554,
          "title": "Communication Efficiency in Federated Learning: Achievements and Challenges. (arXiv:2107.10996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1\">Pinzhuo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yao Gao</a>",
          "description": "Meta-learning provides a promising way for learning to efficiently learn and\nachieves great success in many applications. However, most meta-learning\nliterature focuses on dealing with tasks from a same domain, making it brittle\nto generalize to tasks from the other unseen domains. In this work, we address\nthis problem by simulating tasks from the other unseen domains to improve the\ngeneralization and robustness of meta-learning method. Specifically, we propose\na model-agnostic shift layer to learn how to simulate the domain shift and\ngenerate pseudo tasks, and develop a new adversarial learning-to-learn\nmechanism to train it. Based on the pseudo tasks, the meta-learning model can\nlearn cross-domain meta-knowledge, which can generalize well on unseen domains.\nWe conduct extensive experiments under the domain generalization setting.\nExperimental results demonstrate that the proposed shift layer is applicable to\nvarious meta-learning frameworks. Moreover, our method also leads to\nstate-of-the-art performance on different cross-domain few-shot classification\nbenchmarks and produces good results on cross-domain few-shot regression.",
          "link": "http://arxiv.org/abs/2107.11056",
          "publishedOn": "2021-07-26T02:00:59.013Z",
          "wordCount": 600,
          "title": "Improving the Generalization of Meta-learning on Unseen Domains via Adversarial Shift. (arXiv:2107.11056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sandfelder_D/0/1/0/all/0/1\">Dylan Sandfelder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1\">Priyesh Vijayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1\">William L. Hamilton</a>",
          "description": "Graph neural networks (GNNs) have achieved remarkable success as a framework\nfor deep learning on graph-structured data. However, GNNs are fundamentally\nlimited by their tree-structured inductive bias: the WL-subtree kernel\nformulation bounds the representational capacity of GNNs, and polynomial-time\nGNNs are provably incapable of recognizing triangles in a graph. In this work,\nwe propose to augment the GNN message-passing operations with information\ndefined on ego graphs (i.e., the induced subgraph surrounding each node). We\nterm these approaches Ego-GNNs and show that Ego-GNNs are provably more\npowerful than standard message-passing GNNs. In particular, we show that\nEgo-GNNs are capable of recognizing closed triangles, which is essential given\nthe prominence of transitivity in real-world graphs. We also motivate our\napproach from the perspective of graph signal processing as a form of multiplex\ngraph convolution. Experimental results on node classification using synthetic\nand real data highlight the achievable performance gains using this approach.",
          "link": "http://arxiv.org/abs/2107.10957",
          "publishedOn": "2021-07-26T02:00:58.998Z",
          "wordCount": 608,
          "title": "Ego-GNNs: Exploiting Ego Structures in Graph Neural Networks. (arXiv:2107.10957v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karpov_K/0/1/0/all/0/1\">Kirill Karpov</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Mitrofanov_A/0/1/0/all/0/1\">Artem Mitrofanov</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Korolev_V/0/1/0/all/0/1\">Vadim Korolev</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Tkachenko_V/0/1/0/all/0/1\">Valery Tkachenko</a> (2) ((1) Lomonosov Moscow State University, Department of Chemistry, Leninskie gory, 1 bld. 3, Moscow, Russia, (2) Science Data Software, LLC, 14909 Forest Landing Cir, Rockville, USA)",
          "description": "The use of machine learning in chemistry has become a common practice. At the\nsame time, despite the success of modern machine learning methods, the lack of\ndata limits their use. Using a transfer learning methodology can help solve\nthis problem. This methodology assumes that a model built on a sufficient\namount of data captures general features of the chemical compound structure on\nwhich it was trained and that the further reuse of these features on a dataset\nwith a lack of data will greatly improve the quality of the new model. In this\npaper, we develop this approach for small organic molecules, implementing\ntransfer learning with graph convolutional neural networks. The paper shows a\nsignificant improvement in the performance of models for target properties with\na lack of data. The effects of the dataset composition on model quality and the\napplicability domain of the resulting models are also considered.",
          "link": "http://arxiv.org/abs/2107.10882",
          "publishedOn": "2021-07-26T02:00:58.979Z",
          "wordCount": 630,
          "title": "Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules. (arXiv:2107.10882v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10955",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lou_X/0/1/0/all/0/1\">Xingmei Lou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>",
          "description": "We are interested in the problem of learning the directed acyclic graph (DAG)\nwhen data are generated from a linear structural equation model (SEM) and the\ncausal structure can be characterized by a polytree. Specially, under both\nGaussian and sub-Gaussian models, we study the sample size conditions for the\nwell-known Chow-Liu algorithm to exactly recover the equivalence class of the\npolytree, which is uniquely represented by a CPDAG. We also study the error\nrate for the estimation of the inverse correlation matrix under such models.\nOur theoretical findings are illustrated by comprehensive numerical\nsimulations, and experiments on benchmark data also demonstrate the robustness\nof the method when the ground truth graphical structure can only be\napproximated by a polytree.",
          "link": "http://arxiv.org/abs/2107.10955",
          "publishedOn": "2021-07-26T02:00:58.972Z",
          "wordCount": 571,
          "title": "Linear Polytree Structural Equation Models: Structural Learning and Inverse Correlation Estimation. (arXiv:2107.10955v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>",
          "description": "Conditional computation and modular networks have been recently proposed for\nmultitask learning and other problems as a way to decompose problem solving\ninto multiple reusable computational blocks. We propose a new approach for\nlearning modular networks based on the isometric version of ResNet with all\nresidual blocks having the same configuration and the same number of\nparameters. This architectural choice allows adding, removing and changing the\norder of residual blocks. In our method, the modules can be invoked repeatedly\nand allow knowledge transfer to novel tasks by adjusting the order of\ncomputation. This allows soft weight sharing between tasks with only a small\nincrease in the number of parameters. We show that our method leads to\ninterpretable self-organization of modules in case of multi-task learning,\ntransfer learning and domain adaptation while achieving competitive results on\nthose tasks. From practical perspective, our approach allows to: (a) reuse\nexisting modules for learning new task by adjusting the computation order, (b)\nuse it for unsupervised multi-source domain adaptation to illustrate that\nadaptation to unseen data can be achieved by only manipulating the order of\npretrained modules, (c) show how our approach can be used to increase accuracy\nof existing architectures for image classification tasks such as ImageNet,\nwithout any parameter increase, by reusing the same block multiple times.",
          "link": "http://arxiv.org/abs/2107.10963",
          "publishedOn": "2021-07-26T02:00:58.962Z",
          "wordCount": 653,
          "title": "Compositional Models: Multi-Task Learning and Knowledge Transfer with Modular Networks. (arXiv:2107.10963v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Blanco_E/0/1/0/all/0/1\">Enrique Fernandez-Blanco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Lozano_C/0/1/0/all/0/1\">Carlos Fernandez-Lozano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pazos_A/0/1/0/all/0/1\">Alejandro Pazos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivero_D/0/1/0/all/0/1\">Daniel Rivero</a>",
          "description": "Over the years, several approaches have tried to tackle the problem of\nperforming an automatic scoring of the sleeping stages. Although any\npolysomnography usually collects over a dozen of different signals, this\nparticular problem has been mainly tackled by using only the\nElectroencephalograms presented in those records. On the other hand, the other\nrecorded signals have been mainly ignored by most works. This paper explores\nand compares the convenience of using additional signals apart from\nelectroencephalograms. More specifically, this work uses the SHHS-1 dataset\nwith 5,804 patients containing an electromyogram recorded simultaneously as two\nelectroencephalograms. To compare the results, first, the same architecture has\nbeen evaluated with different input signals and all their possible\ncombinations. These tests show how, using more than one signal especially if\nthey are from different sources, improves the results of the classification.\nAdditionally, the best models obtained for each combination of one or more\nsignals have been used in ensemble models and, its performance has been\ncompared showing the convenience of using these multi-signal models to improve\nthe classification. The best overall model, an ensemble of Depth-wise\nSeparational Convolutional Neural Networks, has achieved an accuracy of 86.06\\%\nwith a Cohen's Kappa of 0.80 and a $F_{1}$ of 0.77. Up to date, those are the\nbest results on the complete dataset and it shows a significant improvement in\nthe precision and recall for the most uncommon class in the dataset.",
          "link": "http://arxiv.org/abs/2107.11045",
          "publishedOn": "2021-07-26T02:00:58.948Z",
          "wordCount": 685,
          "title": "Ensemble of Convolution Neural Networks on Heterogeneous Signals for Sleep Stage Scoring. (arXiv:2107.11045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1\">Harikrishna Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotter_A/0/1/0/all/0/1\">Andrew Cotter</a>",
          "description": "We consider a popular family of constrained optimization problems arising in\nmachine learning that involve optimizing a non-decomposable evaluation metric\nwith a certain thresholded form, while constraining another metric of interest.\nExamples of such problems include optimizing the false negative rate at a fixed\nfalse positive rate, optimizing precision at a fixed recall, optimizing the\narea under the precision-recall or ROC curves, etc. Our key idea is to\nformulate a rate-constrained optimization that expresses the threshold\nparameter as a function of the model parameters via the Implicit Function\ntheorem. We show how the resulting optimization problem can be solved using\nstandard gradient based methods. Experiments on benchmark datasets demonstrate\nthe effectiveness of our proposed method over existing state-of-the art\napproaches for these problems.",
          "link": "http://arxiv.org/abs/2107.10960",
          "publishedOn": "2021-07-26T02:00:58.925Z",
          "wordCount": 555,
          "title": "Implicit Rate-Constrained Optimization of Non-decomposable Objectives. (arXiv:2107.10960v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koprinkova_Hristova_P/0/1/0/all/0/1\">Petia Koprinkova-Hristova</a>",
          "description": "The paper proposes a novel approach for gray scale images segmentation. It is\nbased on multiple features extraction from single feature per image pixel,\nnamely its intensity value, using Echo state network. The newly extracted\nfeatures -- reservoir equilibrium states -- reveal hidden image characteristics\nthat improve its segmentation via a clustering algorithm. Moreover, it was\ndemonstrated that the intrinsic plasticity tuning of reservoir fits its\nequilibrium states to the original image intensity distribution thus allowing\nfor its better segmentation. The proposed approach is tested on the benchmark\nimage Lena.",
          "link": "http://arxiv.org/abs/2107.11077",
          "publishedOn": "2021-07-26T02:00:58.918Z",
          "wordCount": 541,
          "title": "Reservoir Computing Approach for Gray Images Segmentation. (arXiv:2107.11077v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10901",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Moeinizade_S/0/1/0/all/0/1\">Saba Moeinizade</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hu_G/0/1/0/all/0/1\">Guiping Hu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_L/0/1/0/all/0/1\">Lizhi Wang</a>",
          "description": "Genomic selection (GS) is a technique that plant breeders use to select\nindividuals to mate and produce new generations of species. Allocation of\nresources is a key factor in GS. At each selection cycle, breeders are facing\nthe choice of budget allocation to make crosses and produce the next generation\nof breeding parents. Inspired by recent advances in reinforcement learning for\nAI problems, we develop a reinforcement learning-based algorithm to\nautomatically learn to allocate limited resources across different generations\nof breeding. We mathematically formulate the problem in the framework of Markov\nDecision Process (MDP) by defining state and action spaces. To avoid the\nexplosion of the state space, an integer linear program is proposed that\nquantifies the trade-off between resources and time. Finally, we propose a\nvalue function approximation method to estimate the action-value function and\nthen develop a greedy policy improvement technique to find the optimal\nresources. We demonstrate the effectiveness of the proposed method in enhancing\ngenetic gain using a case study with realistic data.",
          "link": "http://arxiv.org/abs/2107.10901",
          "publishedOn": "2021-07-26T02:00:58.907Z",
          "wordCount": 615,
          "title": "A reinforcement learning approach to resource allocation in genomic selection. (arXiv:2107.10901v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaebler_J/0/1/0/all/0/1\">Johann Demetrio Gaebler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Matt Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chunlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinyu Ye</a>",
          "description": "Value iteration is a well-known method of solving Markov Decision Processes\n(MDPs) that is simple to implement and boasts strong theoretical convergence\nguarantees. However, the computational cost of value iteration quickly becomes\ninfeasible as the size of the state space increases. Various methods have been\nproposed to overcome this issue for value iteration in large state and action\nspace MDPs, often at the price, however, of generalizability and algorithmic\nsimplicity. In this paper, we propose an intuitive algorithm for solving MDPs\nthat reduces the cost of value iteration updates by dynamically grouping\ntogether states with similar cost-to-go values. We also prove that our\nalgorithm converges almost surely to within \\(2\\varepsilon / (1 - \\gamma)\\) of\nthe true optimal value in the \\(\\ell^\\infty\\) norm, where \\(\\gamma\\) is the\ndiscount factor and aggregated states differ by at most \\(\\varepsilon\\).\nNumerical experiments on a variety of simulated environments confirm the\nrobustness of our algorithm and its ability to solve MDPs with much cheaper\nupdates especially as the scale of the MDP problem increases.",
          "link": "http://arxiv.org/abs/2107.11053",
          "publishedOn": "2021-07-26T02:00:58.899Z",
          "wordCount": 617,
          "title": "An Adaptive State Aggregation Algorithm for Markov Decision Processes. (arXiv:2107.11053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asad_M/0/1/0/all/0/1\">Muhammad Asad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustafa_A/0/1/0/all/0/1\">Ahmed Moustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1\">Takayuki Ito</a>",
          "description": "In the past few decades, machine learning has revolutionized data processing\nfor large scale applications. Simultaneously, increasing privacy threats in\ntrending applications led to the redesign of classical data training models. In\nparticular, classical machine learning involves centralized data training,\nwhere the data is gathered, and the entire training process executes at the\ncentral server. Despite significant convergence, this training involves several\nprivacy threats on participants' data when shared with the central cloud\nserver. To this end, federated learning has achieved significant importance\nover distributed data training. In particular, the federated learning allows\nparticipants to collaboratively train the local models on local data without\nrevealing their sensitive information to the central cloud server. In this\npaper, we perform a convergence comparison between classical machine learning\nand federated learning on two publicly available datasets, namely,\nlogistic-regression-MNIST dataset and image-classification-CIFAR-10 dataset.\nThe simulation results demonstrate that federated learning achieves higher\nconvergence within limited communication rounds while maintaining participants'\nanonymity. We hope that this research will show the benefits and help federated\nlearning to be implemented widely.",
          "link": "http://arxiv.org/abs/2107.10976",
          "publishedOn": "2021-07-26T02:00:58.880Z",
          "wordCount": 609,
          "title": "Federated Learning Versus Classical Machine Learning: A Convergence Comparison. (arXiv:2107.10976v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_J/0/1/0/all/0/1\">Jose M. Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1\">Dario Rossi</a>",
          "description": "This paper presents HURRA, a system that aims to reduce the time spent by\nhuman operators in the process of network troubleshooting. To do so, it\ncomprises two modules that are plugged after any anomaly detection algorithm:\n(i) a first attention mechanism, that ranks the present features in terms of\ntheir relation with the anomaly and (ii) a second module able to incorporates\nprevious expert knowledge seamlessly, without any need of human interaction nor\ndecisions. We show the efficacy of these simple processes on a collection of\nreal router datasets obtained from tens of ISPs which exhibit a rich variety of\nanomalies and very heterogeneous set of KPIs, on which we gather manually\nannotated ground truth by the operator solving the troubleshooting ticket. Our\nexperimental evaluation shows that (i) the proposed system is effective in\nachieving high levels of agreement with the expert, that (ii) even a simple\nstatistical approach is able to extracting useful information from expert\nknowledge gained in past cases to further improve performance and finally that\n(iii) the main difficulty in live deployment concerns the automated selection\nof the anomaly detection algorithm and the tuning of its hyper-parameters.",
          "link": "http://arxiv.org/abs/2107.11078",
          "publishedOn": "2021-07-26T02:00:58.873Z",
          "wordCount": 646,
          "title": "HURRA! Human readable router anomaly detection. (arXiv:2107.11078v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peter Y. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arino_J/0/1/0/all/0/1\">Joan Ari&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1\">Marin Solja&#x10d;i&#x107;</a>",
          "description": "Identifying the governing equations of a nonlinear dynamical system is key to\nboth understanding the physical features of the system and constructing an\naccurate model of the dynamics that generalizes well beyond the available data.\nWe propose a machine learning framework for discovering these governing\nequations using only partial observations, combining an encoder for state\nreconstruction with a sparse symbolic model. Our tests show that this method\ncan successfully reconstruct the full system state and identify the underlying\ndynamics for a variety of ODE and PDE systems.",
          "link": "http://arxiv.org/abs/2107.10879",
          "publishedOn": "2021-07-26T02:00:58.864Z",
          "wordCount": 533,
          "title": "Discovering Sparse Interpretable Dynamics from Partial Observations. (arXiv:2107.10879v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Linghao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jinsong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges EL Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "In this work, we propose a domain generalization (DG) approach to learn on\nseveral labeled source domains and transfer knowledge to a target domain that\nis inaccessible in training. Considering the inherent conditional and label\nshifts, we would expect the alignment of $p(x|y)$ and $p(y)$. However, the\nwidely used domain invariant feature learning (IFL) methods relies on aligning\nthe marginal concept shift w.r.t. $p(x)$, which rests on an unrealistic\nassumption that $p(y)$ is invariant across domains. We thereby propose a novel\nvariational Bayesian inference framework to enforce the conditional\ndistribution alignment w.r.t. $p(x|y)$ via the prior distribution matching in a\nlatent space, which also takes the marginal label shift w.r.t. $p(y)$ into\nconsideration with the posterior alignment. Extensive experiments on various\nbenchmarks demonstrate that our framework is robust to the label shift and the\ncross-domain accuracy is significantly improved, thereby achieving superior\nperformance over the conventional IFL counterparts.",
          "link": "http://arxiv.org/abs/2107.10931",
          "publishedOn": "2021-07-26T02:00:58.857Z",
          "wordCount": 615,
          "title": "Domain Generalization under Conditional and Label Shifts via Variational Bayesian Inference. (arXiv:2107.10931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10884",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1\">Wu Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>",
          "description": "In this paper, we propose new structured second-order methods and structured\nadaptive-gradient methods obtained by performing natural-gradient descent on\nstructured parameter spaces. Natural-gradient descent is an attractive approach\nto design new algorithms in many settings such as gradient-free,\nadaptive-gradient, and second-order methods. Our structured methods not only\nenjoy a structural invariance but also admit a simple expression. Finally, we\ntest the efficiency of our proposed methods on both deterministic non-convex\nproblems and deep learning problems.",
          "link": "http://arxiv.org/abs/2107.10884",
          "publishedOn": "2021-07-26T02:00:58.849Z",
          "wordCount": 519,
          "title": "Structured second-order methods via natural gradient descent. (arXiv:2107.10884v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_T/0/1/0/all/0/1\">Tim Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Michael Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezanali_M/0/1/0/all/0/1\">Mohammad Ramezanali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_V/0/1/0/all/0/1\">Vincent Tang</a>",
          "description": "In this note we examine the autoregressive generalization of the FNet\nalgorithm, in which self-attention layers from the standard Transformer\narchitecture are substituted with a trivial sparse-uniformsampling procedure\nbased on Fourier transforms. Using the Wikitext-103 benchmark, we\ndemonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the\ntask of causal language modelingcompared to a Transformer-XL baseline (24.2\nppl) with only half the number self-attention layers,thus providing further\nevidence for the superfluity of deep neural networks with heavily\ncompoundedattention mechanisms. The autoregressive Fourier transform could\nlikely be used for parameterreduction on most Transformer-based time-series\nprediction models.",
          "link": "http://arxiv.org/abs/2107.10932",
          "publishedOn": "2021-07-26T02:00:58.842Z",
          "wordCount": 532,
          "title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms. (arXiv:2107.10932v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1\">Jonathan Stray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vendrov_I/0/1/0/all/0/1\">Ivan Vendrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nixon_J/0/1/0/all/0/1\">Jeremy Nixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1\">Steven Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>",
          "description": "We describe cases where real recommender systems were modified in the service\nof various human values such as diversity, fairness, well-being, time well\nspent, and factual accuracy. From this we identify the current practice of\nvalues engineering: the creation of classifiers from human-created data with\nvalue-based labels. This has worked in practice for a variety of issues, but\nproblems are addressed one at a time, and users and other stakeholders have\nseldom been involved. Instead, we look to AI alignment work for approaches that\ncould learn complex values directly from stakeholders, and identify four major\ndirections: useful measures of alignment, participatory design and operation,\ninteractive value learning, and informed deliberative judgments.",
          "link": "http://arxiv.org/abs/2107.10939",
          "publishedOn": "2021-07-26T02:00:58.821Z",
          "wordCount": 571,
          "title": "What are you optimizing for? Aligning Recommender Systems with Human Values. (arXiv:2107.10939v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuolin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaojun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Recent studies show that deep neural networks (DNN) are vulnerable to\nadversarial examples, which aim to mislead DNNs by adding perturbations with\nsmall magnitude. To defend against such attacks, both empirical and theoretical\ndefense approaches have been extensively studied for a single ML model. In this\nwork, we aim to analyze and provide the certified robustness for ensemble ML\nmodels, together with the sufficient and necessary conditions of robustness for\ndifferent ensemble protocols. Although ensemble models are shown more robust\nthan a single model empirically; surprisingly, we find that in terms of the\ncertified robustness the standard ensemble models only achieve marginal\nimprovement compared to a single model. Thus, to explore the conditions that\nguarantee to provide certifiably robust ensemble ML models, we first prove that\ndiversified gradient and large confidence margin are sufficient and necessary\nconditions for certifiably robust ensemble models under the model-smoothness\nassumption. We then provide the bounded model-smoothness analysis based on the\nproposed Ensemble-before-Smoothing strategy. We also prove that an ensemble\nmodel can always achieve higher certified robustness than a single base model\nunder mild conditions. Inspired by the theoretical findings, we propose the\nlightweight Diversity Regularized Training (DRT) to train certifiably robust\nensemble ML models. Extensive experiments show that our DRT enhanced ensembles\ncan consistently achieve higher certified robustness than existing single and\nensemble ML models, demonstrating the state-of-the-art certified L2-robustness\non MNIST, CIFAR-10, and ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.10873",
          "publishedOn": "2021-07-26T02:00:58.781Z",
          "wordCount": 691,
          "title": "On the Certified Robustness for Ensemble Models and Beyond. (arXiv:2107.10873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1\">Mark Bun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_S/0/1/0/all/0/1\">Satchit Sivakumar</a>",
          "description": "We show a generic reduction from multiclass differentially private PAC\nlearning to binary private PAC learning. We apply this transformation to a\nrecently proposed binary private PAC learner to obtain a private multiclass\nlearner with sample complexity that has a polynomial dependence on the\nmulticlass Littlestone dimension and a poly-logarithmic dependence on the\nnumber of classes. This yields an exponential improvement in the dependence on\nboth parameters over learners from previous work. Our proof extends the notion\nof $\\Psi$-dimension defined in work of Ben-David et al. [JCSS '95] to the\nonline setting and explores its general properties.",
          "link": "http://arxiv.org/abs/2107.10870",
          "publishedOn": "2021-07-26T02:00:58.687Z",
          "wordCount": 528,
          "title": "Multiclass versus Binary Differentially Private PAC Learning. (arXiv:2107.10870v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sashidhar_D/0/1/0/all/0/1\">Diya Sashidhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1\">J. Nathan Kutz</a>",
          "description": "Dynamic mode decomposition (DMD) provides a regression framework for\nadaptively learning a best-fit linear dynamics model over snapshots of\ntemporal, or spatio-temporal, data. A diversity of regression techniques have\nbeen developed for producing the linear model approximation whose solutions are\nexponentials in time. For spatio-temporal data, DMD provides low-rank and\ninterpretable models in the form of dominant modal structures along with their\nexponential/oscillatory behavior in time. The majority of DMD algorithms,\nhowever, are prone to bias errors from noisy measurements of the dynamics,\nleading to poor model fits and unstable forecasting capabilities. The optimized\nDMD algorithm minimizes the model bias with a variable projection optimization,\nthus leading to stabilized forecasting capabilities. Here, the optimized DMD\nalgorithm is improved by using statistical bagging methods whereby a single set\nof snapshots is used to produce an ensemble of optimized DMD models. The\noutputs of these models are averaged to produce a bagging, optimized dynamic\nmode decomposition (BOP-DMD). BOP-DMD not only improves performance, it also\nrobustifies the model and provides both spatial and temporal uncertainty\nquantification (UQ). Thus unlike currently available DMD algorithms, BOP-DMD\nprovides a stable and robust model for probabilistic, or Bayesian forecasting\nwith comprehensive UQ metrics.",
          "link": "http://arxiv.org/abs/2107.10878",
          "publishedOn": "2021-07-26T02:00:58.664Z",
          "wordCount": 655,
          "title": "Bagging, optimized dynamic mode decomposition (BOP-DMD) for robust, stable forecasting with spatial and temporal uncertainty-quantification. (arXiv:2107.10878v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strawn_N/0/1/0/all/0/1\">Nate Strawn</a>",
          "description": "We construct a computationally inexpensive 3D extension of Andrew's plots by\nconsidering curves generated by Frenet-Serret equations and induced by\noptimally smooth 2D Andrew's plots. We consider linear isometries from a\nEuclidean data space to infinite dimensional spaces of 2D curves, and\nparametrize the linear isometries that produce (on average) optimally smooth\ncurves over a given dataset. This set of optimal isometries admits many degrees\nof freedom, and (using recent results on generalized Gauss sums) we identify a\nparticular a member of this set which admits an asymptotic projective \"tour\"\nproperty. Finally, we consider the unit-length 3D curves (filaments) induced by\nthese 2D Andrew's plots, where the linear isometry property preserves distances\nas \"relative total square curvatures\". This work concludes by illustrating\nfilament plots for several datasets. Code is available at\nhttps://github.com/n8epi/filaments",
          "link": "http://arxiv.org/abs/2107.10869",
          "publishedOn": "2021-07-26T02:00:58.537Z",
          "wordCount": 565,
          "title": "Filament Plots for Data Visualization. (arXiv:2107.10869v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mehrdad Mahdavi</a>",
          "description": "In this paper we prove that Local (S)GD (or FedAvg) can optimize two-layer\nneural networks with Rectified Linear Unit (ReLU) activation function in\npolynomial time. Despite the established convergence theory of Local SGD on\noptimizing general smooth functions in communication-efficient distributed\noptimization, its convergence on non-smooth ReLU networks still eludes full\ntheoretical understanding. The key property used in many Local SGD analysis on\nsmooth function is gradient Lipschitzness, so that the gradient on local models\nwill not drift far away from that on averaged model. However, this decent\nproperty does not hold in networks with non-smooth ReLU activation function. We\nshow that, even though ReLU network does not admit gradient Lipschitzness\nproperty, the difference between gradients on local models and average model\nwill not change too much, under the dynamics of Local SGD. We validate our\ntheoretical results via extensive experiments. This work is the first to show\nthe convergence of Local SGD on non-smooth functions, and will shed lights on\nthe optimization theory of federated training of deep neural networks.",
          "link": "http://arxiv.org/abs/2107.10868",
          "publishedOn": "2021-07-26T02:00:58.529Z",
          "wordCount": 604,
          "title": "Local SGD Optimizes Overparameterized Neural Networks in Polynomial Time. (arXiv:2107.10868v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mills_J/0/1/0/all/0/1\">Jed Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1\">Geyong Min</a>",
          "description": "Federated Learning (FL) is an emerging approach for collaboratively training\nDeep Neural Networks (DNNs) on mobile devices, without private user data\nleaving the devices. Previous works have shown that non-Independent and\nIdentically Distributed (non-IID) user data harms the convergence speed of the\nFL algorithms. Furthermore, most existing work on FL measures global-model\naccuracy, but in many cases, such as user content-recommendation, improving\nindividual User model Accuracy (UA) is the real objective. To address these\nissues, we propose a Multi-Task FL (MTFL) algorithm that introduces\nnon-federated Batch-Normalization (BN) layers into the federated DNN. MTFL\nbenefits UA and convergence speed by allowing users to train models\npersonalised to their own data. MTFL is compatible with popular iterative FL\noptimisation algorithms such as Federated Averaging (FedAvg), and we show\nempirically that a distributed form of Adam optimisation (FedAvg-Adam) benefits\nconvergence speed even further when used as the optimisation strategy within\nMTFL. Experiments using MNIST and CIFAR10 demonstrate that MTFL is able to\nsignificantly reduce the number of rounds required to reach a target UA, by up\nto $5\\times$ when using existing FL optimisation strategies, and with a further\n$3\\times$ improvement when using FedAvg-Adam. We compare MTFL to competing\npersonalised FL algorithms, showing that it is able to achieve the best UA for\nMNIST and CIFAR10 in all considered scenarios. Finally, we evaluate MTFL with\nFedAvg-Adam on an edge-computing testbed, showing that its convergence and UA\nbenefits outweigh its overhead.",
          "link": "http://arxiv.org/abs/2007.09236",
          "publishedOn": "2021-07-23T02:00:33.031Z",
          "wordCount": 710,
          "title": "Multi-Task Federated Learning for Personalised Deep Neural Networks in Edge Computing. (arXiv:2007.09236v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petangoda_J/0/1/0/all/0/1\">Janith Petangoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deisenroth_M/0/1/0/all/0/1\">Marc Peter Deisenroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monk_N/0/1/0/all/0/1\">Nicholas A. M. Monk</a>",
          "description": "Learning to transfer considers learning solutions to tasks in a such way that\nrelevant knowledge can be transferred from known task solutions to new, related\ntasks. This is important for general learning, as well as for improving the\nefficiency of the learning process. While techniques for learning to transfer\nhave been studied experimentally, we still lack a foundational description of\nthe problem that exposes what related tasks are, and how relationships between\ntasks can be exploited constructively. In this work, we introduce a framework\nusing the differential geometric theory of foliations that provides such a\nfoundation.",
          "link": "http://arxiv.org/abs/2107.10763",
          "publishedOn": "2021-07-23T02:00:33.025Z",
          "wordCount": 522,
          "title": "Learning to Transfer: A Foliated Theory. (arXiv:2107.10763v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krokos_V/0/1/0/all/0/1\">Vasilis Krokos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_V/0/1/0/all/0/1\">Viet Bui Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordas_S/0/1/0/all/0/1\">St&#xe9;phane P. A. Bordas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_P/0/1/0/all/0/1\">Philippe Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerfriden_P/0/1/0/all/0/1\">Pierre Kerfriden</a>",
          "description": "Multiscale computational modelling is challenging due to the high\ncomputational cost of direct numerical simulation by finite elements. To\naddress this issue, concurrent multiscale methods use the solution of cheaper\nmacroscale surrogates as boundary conditions to microscale sliding windows. The\nmicroscale problems remain a numerically challenging operation both in terms of\nimplementation and cost. In this work we propose to replace the local\nmicroscale solution by an Encoder-Decoder Convolutional Neural Network that\nwill generate fine-scale stress corrections to coarse predictions around\nunresolved microscale features, without prior parametrisation of local\nmicroscale problems. We deploy a Bayesian approach providing credible intervals\nto evaluate the uncertainty of the predictions, which is then used to\ninvestigate the merits of a selective learning framework. We will demonstrate\nthe capability of the approach to predict equivalent stress fields in porous\nstructures using linearised and finite strain elasticity theories.",
          "link": "http://arxiv.org/abs/2012.11330",
          "publishedOn": "2021-07-23T02:00:33.006Z",
          "wordCount": 654,
          "title": "A Bayesian multiscale CNN framework to predict local stress fields in structures with microscale features. (arXiv:2012.11330v3 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10711",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Eivazi_H/0/1/0/all/0/1\">Hamidreza Eivazi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tahani_M/0/1/0/all/0/1\">Mojtaba Tahani</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schlatter_P/0/1/0/all/0/1\">Philipp Schlatter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vinuesa_R/0/1/0/all/0/1\">Ricardo Vinuesa</a>",
          "description": "Physics-informed neural networks (PINNs) are successful machine-learning\nmethods for the solution and identification of partial differential equations\n(PDEs). We employ PINNs for solving the Reynolds-averaged\nNavier$\\unicode{x2013}$Stokes (RANS) equations for incompressible turbulent\nflows without any specific model or assumption for turbulence, and by taking\nonly the data on the domain boundaries. We first show the applicability of\nPINNs for solving the Navier$\\unicode{x2013}$Stokes equations for laminar flows\nby solving the Falkner$\\unicode{x2013}$Skan boundary layer. We then apply PINNs\nfor the simulation of four turbulent-flow cases, i.e., zero-pressure-gradient\nboundary layer, adverse-pressure-gradient boundary layer, and turbulent flows\nover a NACA4412 airfoil and the periodic hill. Our results show the excellent\napplicability of PINNs for laminar flows with strong pressure gradients, where\npredictions with less than 1% error can be obtained. For turbulent flows, we\nalso obtain very good accuracy on simulation results even for the\nReynolds-stress components.",
          "link": "http://arxiv.org/abs/2107.10711",
          "publishedOn": "2021-07-23T02:00:32.977Z",
          "wordCount": 597,
          "title": "Physics-informed neural networks for solving Reynolds-averaged Navier$\\unicode{x2013}$Stokes equations. (arXiv:2107.10711v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Eduardo Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_A/0/1/0/all/0/1\">Andres Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1\">Xavier Serra</a>",
          "description": "Recent studies have put into question the commonly assumed shift invariance\nproperty of convolutional networks, showing that small shifts in the input can\naffect the output predictions substantially. In this paper, we analyze the\nbenefits of addressing lack of shift invariance in CNN-based sound event\nclassification. Specifically, we evaluate two pooling methods to improve shift\ninvariance in CNNs, based on low-pass filtering and adaptive sampling of\nincoming feature maps. These methods are implemented via small architectural\nmodifications inserted into the pooling layers of CNNs. We evaluate the effect\nof these architectural changes on the FSD50K dataset using models of different\ncapacity and in presence of strong regularization. We show that these\nmodifications consistently improve sound event classification in all cases\nconsidered. We also demonstrate empirically that the proposed pooling methods\nincrease shift invariance in the network, making it more robust against\ntime/frequency shifts in input spectrograms. This is achieved by adding a\nnegligible amount of trainable parameters, which makes these methods an\nappealing alternative to conventional pooling layers. The outcome is a new\nstate-of-the-art mAP of 0.541 on the FSD50K classification benchmark.",
          "link": "http://arxiv.org/abs/2107.00623",
          "publishedOn": "2021-07-23T02:00:32.966Z",
          "wordCount": 643,
          "title": "Improving Sound Event Classification by Increasing Shift Invariance in Convolutional Neural Networks. (arXiv:2107.00623v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Libo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Browning_J/0/1/0/all/0/1\">James Browning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_R/0/1/0/all/0/1\">Roberto Perera</a>",
          "description": "The Light-Up puzzle, also known as the AKARI puzzle, has never been solved\nusing modern artificial intelligence (AI) methods. Currently, the most widely\nused computational technique to autonomously develop solutions involve\nevolution theory algorithms. This project is an effort to apply new AI\ntechniques for solving the Light-up puzzle faster and more computationally\nefficient. The algorithms explored for producing optimal solutions include hill\nclimbing, simulated annealing, feed-forward neural network (FNN), and\nconvolutional neural network (CNN). Two algorithms were developed for hill\nclimbing and simulated annealing using 2 actions (add and remove light bulb)\nversus 3 actions(add, remove, or move light-bulb to a different cell). Both\nhill climbing and simulated annealing algorithms showed a higher accuracy for\nthe case of 3 actions. The simulated annealing showed to significantly\noutperform hill climbing, FNN, CNN, and an evolutionary theory algorithm\nachieving 100% accuracy in 30 unique board configurations. Lastly, while FNN\nand CNN algorithms showed low accuracies, computational times were\nsignificantly faster compared to the remaining algorithms. The GitHub\nrepository for this project can be found at\nhttps://github.com/rperera12/AKARI-LightUp-GameSolver-with-DeepNeuralNetworks-and-HillClimb-or-SimulatedAnnealing.",
          "link": "http://arxiv.org/abs/2107.10429",
          "publishedOn": "2021-07-23T02:00:32.959Z",
          "wordCount": 628,
          "title": "Shedding some light on Light Up with Artificial Intelligence. (arXiv:2107.10429v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_V/0/1/0/all/0/1\">Vikram Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattanayak_K/0/1/0/all/0/1\">Kunal Pattanayak</a>",
          "description": "This paper presents an inverse reinforcement learning (IRL) framework for\nBayesian stopping time problems. By observing the actions of a Bayesian\ndecision maker, we provide a necessary and sufficient condition to identify if\nthese actions are consistent with optimizing a cost function; then we construct\nset valued estimates of the cost function. To achieve this IRL objective, we\nuse novel ideas from Bayesian revealed preferences stemming from\nmicroeconomics. To illustrate our IRL scheme,we consider two important examples\nof stopping time problems, namely, sequential hypothesis testing and Bayesian\nsearch. Finally, for finite datasets, we propose an IRL detection algorithm and\ngive finite sample bounds on its error probabilities. Also we discuss how to\nidentify $\\epsilon$-optimal Bayesian decision makers and perform IRL.",
          "link": "http://arxiv.org/abs/2007.03481",
          "publishedOn": "2021-07-23T02:00:32.940Z",
          "wordCount": 608,
          "title": "Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems. (arXiv:2007.03481v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fahrbach_M/0/1/0/all/0/1\">Matthew Fahrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_M/0/1/0/all/0/1\">Mehrdad Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Thomas Fu</a>",
          "description": "Low-rank tensor decomposition generalizes low-rank matrix approximation and\nis a powerful technique for discovering low-dimensional structure in\nhigh-dimensional data. In this paper, we study Tucker decompositions and use\ntools from randomized numerical linear algebra called ridge leverage scores to\naccelerate the core tensor update step in the widely-used alternating least\nsquares (ALS) algorithm. Updating the core tensor, a severe bottleneck in ALS,\nis a highly-structured ridge regression problem where the design matrix is a\nKronecker product of the factor matrices. We show how to use approximate ridge\nleverage scores to construct a sketched instance for any ridge regression\nproblem such that the solution vector for the sketched problem is a\n$(1+\\varepsilon)$-approximation to the original instance. Moreover, we show\nthat classical leverage scores suffice as an approximation, which then allows\nus to exploit the Kronecker structure and update the core tensor in time that\ndepends predominantly on the rank and the sketching parameters (i.e., sublinear\nin the size of the input tensor). We also give upper bounds for ridge leverage\nscores as rows are removed from the design matrix (e.g., if the tensor has\nmissing entries), and we demonstrate the effectiveness of our approximate ridge\nregressioni algorithm for large, low-rank Tucker decompositions on both\nsynthetic and real-world data.",
          "link": "http://arxiv.org/abs/2107.10654",
          "publishedOn": "2021-07-23T02:00:32.931Z",
          "wordCount": 657,
          "title": "Fast Low-Rank Tensor Decomposition by Ridge Leverage Score Sampling. (arXiv:2107.10654v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coleman_C/0/1/0/all/0/1\">Cody Coleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_E/0/1/0/all/0/1\">Edward Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Samuels_J/0/1/0/all/0/1\">Julian Katz-Samuels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culatana_S/0/1/0/all/0/1\">Sean Culatana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1\">Peter Bailis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1\">Robert Nowak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumbaly_R/0/1/0/all/0/1\">Roshan Sumbaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalniz_I/0/1/0/all/0/1\">I. Zeki Yalniz</a>",
          "description": "Many active learning and search approaches are intractable for large-scale\nindustrial settings with billions of unlabeled examples. Existing approaches\nsearch globally for the optimal examples to label, scaling linearly or even\nquadratically with the unlabeled data. In this paper, we improve the\ncomputational efficiency of active learning and search methods by restricting\nthe candidate pool for labeling to the nearest neighbors of the currently\nlabeled set instead of scanning over all of the unlabeled data. We evaluate\nseveral selection strategies in this setting on three large-scale computer\nvision datasets: ImageNet, OpenImages, and a de-identified and aggregated\ndataset of 10 billion images provided by a large internet company. Our approach\nachieved similar mean average precision and recall as the traditional global\napproach while reducing the computational cost of selection by up to three\norders of magnitude, thus enabling web-scale active learning.",
          "link": "http://arxiv.org/abs/2007.00077",
          "publishedOn": "2021-07-23T02:00:32.913Z",
          "wordCount": 638,
          "title": "Similarity Search for Efficient Active Learning and Search of Rare Concepts. (arXiv:2007.00077v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungsoo Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Abstract reasoning, i.e., inferring complicated patterns from given\nobservations, is a central building block of artificial general intelligence.\nWhile humans find the answer by either eliminating wrong candidates or first\nconstructing the answer, prior deep neural network (DNN)-based methods focus on\nthe former discriminative approach. This paper aims to design a framework for\nthe latter approach and bridge the gap between artificial and human\nintelligence. To this end, we propose logic-guided generation (LoGe), a novel\ngenerative DNN framework that reduces abstract reasoning as an optimization\nproblem in propositional logic. LoGe is composed of three steps: extract\npropositional variables from images, reason the answer variables with a logic\nlayer, and reconstruct the answer image from the variables. We demonstrate that\nLoGe outperforms the black box DNN frameworks for generative abstract reasoning\nunder the RAVEN benchmark, i.e., reconstructing answers based on capturing\ncorrect rules of various attributes from observations.",
          "link": "http://arxiv.org/abs/2107.10493",
          "publishedOn": "2021-07-23T02:00:32.905Z",
          "wordCount": 601,
          "title": "Abstract Reasoning via Logic-guided Generation. (arXiv:2107.10493v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10670",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1\">Shuangli Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhou_J/0/1/0/all/0/1\">Jingbo Zhou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_W/0/1/0/all/0/1\">Weili Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>",
          "description": "Drug discovery often relies on the successful prediction of protein-ligand\nbinding affinity. Recent advances have shown great promise in applying graph\nneural networks (GNNs) for better affinity prediction by learning the\nrepresentations of protein-ligand complexes. However, existing solutions\nusually treat protein-ligand complexes as topological graph data, thus the\nbiomolecular structural information is not fully utilized. The essential\nlong-range interactions among atoms are also neglected in GNN models. To this\nend, we propose a structure-aware interactive graph neural network (SIGN) which\nconsists of two components: polar-inspired graph attention layers (PGAL) and\npairwise interactive pooling (PiPool). Specifically, PGAL iteratively performs\nthe node-edge aggregation process to update embeddings of nodes and edges while\npreserving the distance and angle information among atoms. Then, PiPool is\nadopted to gather interactive edges with a subsequent reconstruction loss to\nreflect the global interactions. Exhaustive experimental study on two\nbenchmarks verifies the superiority of SIGN.",
          "link": "http://arxiv.org/abs/2107.10670",
          "publishedOn": "2021-07-23T02:00:32.899Z",
          "wordCount": 614,
          "title": "Structure-aware Interactive Graph Neural Networks for the Prediction of Protein-Ligand Binding Affinity. (arXiv:2107.10670v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1\">Yiqun Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Quan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingsheng He</a>",
          "description": "Due to the increasing privacy concerns and data regulations, training data\nhave been increasingly fragmented, forming distributed databases of multiple\n``data silos'' (e.g., within different organizations and countries). To develop\neffective machine learning services, there is a must to exploit data from such\ndistributed databases without exchanging the raw data. Recently, federated\nlearning (FL) has been a solution with growing interests, which enables\nmultiple parties to collaboratively train a machine learning model without\nexchanging their local data. A key and common challenge on distributed\ndatabases is the heterogeneity of the data distribution (i.e., non-IID) among\nthe parties. There have been many FL algorithms to address the learning\neffectiveness under non-IID data settings. However, there lacks an experimental\nstudy on systematically understanding their advantages and disadvantages, as\nprevious studies have very rigid data partitioning strategies among parties,\nwhich are hardly representative and thorough. In this paper, to help\nresearchers better understand and study the non-IID data setting in federated\nlearning, we propose comprehensive data partitioning strategies to cover the\ntypical non-IID data cases. Moreover, we conduct extensive experiments to\nevaluate state-of-the-art FL algorithms. We find that non-IID does bring\nsignificant challenges in learning accuracy of FL algorithms, and none of the\nexisting state-of-the-art FL algorithms outperforms others in all cases. Our\nexperiments provide insights for future studies of addressing the challenges in\n``data silos''.",
          "link": "http://arxiv.org/abs/2102.02079",
          "publishedOn": "2021-07-23T02:00:32.881Z",
          "wordCount": 699,
          "title": "Federated Learning on Non-IID Data Silos: An Experimental Study. (arXiv:2102.02079v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1805.05510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Metric learning especially deep metric learning has been widely developed for\nlarge-scale image inputs data. However, in many real-world applications, we can\nonly have access to vectorized inputs data. Moreover, on one hand, well-labeled\ndata is usually limited due to the high annotation cost. On the other hand, the\nreal data is commonly streaming data, which requires to be processed online. In\nthese scenarios, the fashionable deep metric learning is not suitable anymore.\nTo this end, we reconsider the traditional shallow online metric learning and\nnewly develop an online progressive deep metric learning (ODML) framework to\nconstruct a metric-algorithm-based deep network. Specifically, we take an\nonline metric learning algorithm as a metric-algorithm-based layer (i.e.,\nmetric layer), followed by a nonlinear layer, and then stack these layers in a\nfashion similar to deep learning. Different from the shallow online metric\nlearning, which can only learn one metric space (feature transformation), the\nproposed ODML is able to learn multiple hierarchical metric spaces.\nFurthermore, in a progressively and nonlinearly learning way, ODML has a\nstronger learning ability than traditional shallow online metric learning in\nthe case of limited available training data. To make the learning process more\nexplainable and theoretically guaranteed, we also provide theoretical analysis.\nThe proposed ODML enjoys several nice properties and can indeed learn a metric\nprogressively and performs better on the benchmark datasets. Extensive\nexperiments with different settings have been conducted to verify these\nproperties of the proposed ODML.",
          "link": "http://arxiv.org/abs/1805.05510",
          "publishedOn": "2021-07-23T02:00:32.874Z",
          "wordCount": 713,
          "title": "Online Progressive Deep Metric Learning. (arXiv:1805.05510v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Aaron Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Ali Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesgarani_N/0/1/0/all/0/1\">Nima Mesgarani</a>",
          "description": "We present an unsupervised non-parallel many-to-many voice conversion (VC)\nmethod using a generative adversarial network (GAN) called StarGAN v2. Using a\ncombination of adversarial source classifier loss and perceptual loss, our\nmodel significantly outperforms previous VC models. Although our model is\ntrained only with 20 English speakers, it generalizes to a variety of voice\nconversion tasks, such as any-to-many, cross-lingual, and singing conversion.\nUsing a style encoder, our framework can also convert plain reading speech into\nstylistic speech, such as emotional and falsetto speech. Subjective and\nobjective evaluation experiments on a non-parallel many-to-many voice\nconversion task revealed that our model produces natural sounding voices, close\nto the sound quality of state-of-the-art text-to-speech (TTS) based voice\nconversion methods without the need for text labels. Moreover, our model is\ncompletely convolutional and with a faster-than-real-time vocoder such as\nParallel WaveGAN can perform real-time voice conversion.",
          "link": "http://arxiv.org/abs/2107.10394",
          "publishedOn": "2021-07-23T02:00:32.866Z",
          "wordCount": 592,
          "title": "StarGANv2-VC: A Diverse, Unsupervised, Non-parallel Framework for Natural-Sounding Voice Conversion. (arXiv:2107.10394v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10706",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Beznosikov_A/0/1/0/all/0/1\">Aleksandr Beznosikov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Scutari_G/0/1/0/all/0/1\">Gesualdo Scutari</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rogozin_A/0/1/0/all/0/1\">Alexander Rogozin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1\">Alexander Gasnikov</a>",
          "description": "We study solution methods for (strongly-)convex-(strongly)-concave\nSaddle-Point Problems (SPPs) over networks of two type - master/workers (thus\ncentralized) architectures and meshed (thus decentralized) networks. The local\nfunctions at each node are assumed to be similar, due to statistical data\nsimilarity or otherwise. We establish lower complexity bounds for a fairly\ngeneral class of algorithms solving the SPP. We show that a given suboptimality\n$\\epsilon>0$ is achieved over master/workers networks in\n$\\Omega\\big(\\Delta\\cdot \\delta/\\mu\\cdot \\log (1/\\varepsilon)\\big)$ rounds of\ncommunications, where $\\delta>0$ measures the degree of similarity of the local\nfunctions, $\\mu$ is their strong convexity constant, and $\\Delta$ is the\ndiameter of the network. The lower communication complexity bound over meshed\nnetworks reads $\\Omega\\big(1/{\\sqrt{\\rho}} \\cdot {\\delta}/{\\mu}\\cdot\\log\n(1/\\varepsilon)\\big)$, where $\\rho$ is the (normalized) eigengap of the gossip\nmatrix used for the communication between neighbouring nodes. We then propose\nalgorithms matching the lower bounds over either types of networks (up to\nlog-factors). We assess the effectiveness of the proposed algorithms on a\nrobust logistic regression problem.",
          "link": "http://arxiv.org/abs/2107.10706",
          "publishedOn": "2021-07-23T02:00:32.859Z",
          "wordCount": 599,
          "title": "Distributed Saddle-Point Problems Under Similarity. (arXiv:2107.10706v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10567",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1\">Kyu-Beom Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shin_H/0/1/0/all/0/1\">Hyu-Soung Shin</a>",
          "description": "Tunnel CCTVs are installed to low height and long-distance interval. However,\nbecause of the limitation of installation height, severe perspective effect in\ndistance occurs, and it is almost impossible to detect vehicles in far distance\nfrom the CCTV in the existing tunnel CCTV-based accident detection system\n(Pflugfelder 2005). To overcome the limitation, a vehicle object is detected\nthrough an object detection algorithm based on an inverse perspective transform\nby re-setting the region of interest (ROI). It can detect vehicles that are far\naway from the CCTV. To verify this process, this paper creates each dataset\nconsisting of images and bounding boxes based on the original and warped images\nof the CCTV at the same time, and then compares performance of the deep\nlearning object detection models trained with the two datasets. As a result,\nthe model that trained the warped image was able to detect vehicle objects more\naccurately at the position far from the CCTV compared to the model that trained\nthe original image.",
          "link": "http://arxiv.org/abs/2107.10567",
          "publishedOn": "2021-07-23T02:00:32.853Z",
          "wordCount": 624,
          "title": "An overcome of far-distance limitation on tunnel CCTV-based accident detection in AI deep-learning frameworks. (arXiv:2107.10567v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Woochul Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyeon Kim</a>",
          "description": "Modern convolutional neural networks (CNNs) have massive identical\nconvolution blocks, and, hence, recursive sharing of parameters across these\nblocks has been proposed to reduce the amount of parameters. However, naive\nsharing of parameters poses many challenges such as limited representational\npower and the vanishing/exploding gradients problem of recursively shared\nparameters. In this paper, we present a recursive convolution block design and\ntraining method, in which a recursively shareable part, or a filter basis, is\nseparated and learned while effectively avoiding the vanishing/exploding\ngradients problem during training. We show that the unwieldy\nvanishing/exploding gradients problem can be controlled by enforcing the\nelements of the filter basis orthonormal, and empirically demonstrate that the\nproposed orthogonality regularization improves the flow of gradients during\ntraining. Experimental results on image classification and object detection\nshow that our approach, unlike previous parameter-sharing approaches, does not\ntrade performance to save parameters and consistently outperforms\noverparameterized counterpart networks. This superior performance demonstrates\nthat the proposed recursive convolution block design and the orthogonality\nregularization not only prevent performance degradation, but also consistently\nimprove the representation capability while a significant amount of parameters\nare recursively shared.",
          "link": "http://arxiv.org/abs/2006.05066",
          "publishedOn": "2021-07-23T02:00:32.848Z",
          "wordCount": 663,
          "title": "Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks. (arXiv:2006.05066v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.03040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianfeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper establishes the optimal approximation error characterization of\ndeep ReLU networks for smooth functions in terms of both width and depth\nsimultaneously. To that end, we first prove that multivariate polynomials can\nbe approximated by deep ReLU networks of width $\\mathcal{O}(N)$ and depth\n$\\mathcal{O}(L)$ with an approximation error $\\mathcal{O}(N^{-L})$. Through\nlocal Taylor expansions and their deep ReLU network approximations, we show\nthat deep ReLU networks of width $\\mathcal{O}(N\\ln N)$ and depth\n$\\mathcal{O}(L\\ln L)$ can approximate $f\\in C^s([0,1]^d)$ with a nearly optimal\napproximation error $\\mathcal{O}(\\|f\\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our\nestimate is non-asymptotic in the sense that it is valid for arbitrary width\nand depth specified by $N\\in\\mathbb{N}^+$ and $L\\in\\mathbb{N}^+$, respectively.",
          "link": "http://arxiv.org/abs/2001.03040",
          "publishedOn": "2021-07-23T02:00:32.841Z",
          "wordCount": 609,
          "title": "Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10845",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1\">Hanrui Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1\">Yongshan Ding</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lin_Y/0/1/0/all/0/1\">Yujun Lin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1\">Frederic T. Chong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>",
          "description": "Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)\ncomputers. Limited research efforts have explored a higher level of\noptimization by making the quantum circuit resilient to noise. We propose and\nexperimentally implement QuantumNAS, the first comprehensive framework for\nnoise-adaptive co-search of variational circuit and qubit mapping. Variational\nquantum circuits are a promising approach for constructing quantum neural\nnetworks for machine learning and variational ansatzes for quantum simulation.\nHowever, finding the best variational circuit and its optimal parameters is\nchallenging in a high-dimensional Hilbert space. We propose to decouple the\nparameter training and circuit search by introducing a novel gate-sharing\nSuperCircuit. The SuperCircuit is trained by sampling and updating the\nSubCircuits in it and provides an accurate estimation of SubCircuit performance\ntrained from scratch. Then we perform an evolutionary co-search of SubCircuit\nand its qubit mapping. The SubCircuit performance is estimated with parameters\ninherited from SuperCircuit and simulated with real device noise models.\nFinally, we perform iterative gate pruning and finetuning to further remove the\nredundant gates in a fine-grained manner.\n\nExtensively evaluated with 12 QML and VQE benchmarks on 10 quantum computers,\nQuantumNAS significantly outperforms noise-unaware search, human and random\nbaselines. For QML tasks, QuantumNAS is the first to demonstrate over 95%\n2-class, 85% 4-class, and 32% 10-class classification accuracy on real quantum\ncomputers. It also achieves the lowest eigenvalue for VQE tasks on H2, H2O,\nLiH, CH4, BeH2 compared with UCCSD baselines. We also open-source QuantumEngine\n(https://github.com/mit-han-lab/pytorch-quantum) for fast training of\nparameterized quantum circuits to facilitate future research.",
          "link": "http://arxiv.org/abs/2107.10845",
          "publishedOn": "2021-07-23T02:00:32.834Z",
          "wordCount": 707,
          "title": "QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits. (arXiv:2107.10845v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2101.05145",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1\">Rohit Jena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singla_S/0/1/0/all/0/1\">Sumedha Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Vessel segmentation is an essential task in many clinical applications.\nAlthough supervised methods have achieved state-of-art performance, acquiring\nexpert annotation is laborious and mostly limited for two-dimensional datasets\nwith a small sample size. On the contrary, unsupervised methods rely on\nhandcrafted features to detect tube-like structures such as vessels. However,\nthose methods require complex pipelines involving several hyper-parameters and\ndesign choices rendering the procedure sensitive, dataset-specific, and not\ngeneralizable. We propose a self-supervised method with a limited number of\nhyper-parameters that is generalizable across modalities. Our method uses\ntube-like structure properties, such as connectivity, profile consistency, and\nbifurcation, to introduce inductive bias into a learning algorithm. To model\nthose properties, we generate a vector field that we refer to as a flow. Our\nexperiments on various public datasets in 2D and 3D show that our method\nperforms better than unsupervised methods while learning useful transferable\nfeatures from unlabeled data. Unlike generic self-supervised methods, the\nlearned features learn vessel-relevant features that are transferable for\nsupervised approaches, which is essential when the number of annotated data is\nlimited.",
          "link": "http://arxiv.org/abs/2101.05145",
          "publishedOn": "2021-07-23T02:00:32.810Z",
          "wordCount": 651,
          "title": "Self-Supervised Vessel Enhancement Using Flow-Based Consistencies. (arXiv:2101.05145v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1\">Wilkie Olin-Ammentorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1\">Maxim Bazhenov</a>",
          "description": "Despite rapid progress, current deep learning methods face a number of\ncritical challenges. These include high energy consumption, catastrophic\nforgetting, dependance on global losses, and an inability to reason\nsymbolically. By combining concepts from information bottleneck theory and\nvector-symbolic architectures, we propose and implement a novel information\nprocessing architecture, the 'Bridge network.' We show this architecture\nprovides unique advantages which can address the problem of global losses and\ncatastrophic forgetting. Furthermore, we argue that it provides a further basis\nfor increasing energy efficiency of execution and the ability to reason\nsymbolically.",
          "link": "http://arxiv.org/abs/2106.08446",
          "publishedOn": "2021-07-23T02:00:32.798Z",
          "wordCount": 549,
          "title": "Bridge Networks: Relating Inputs through Vector-Symbolic Manipulations. (arXiv:2106.08446v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shail Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghdadi_R/0/1/0/all/0/1\">Riyadh Baghdadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowatzki_T/0/1/0/all/0/1\">Tony Nowatzki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avancha_S/0/1/0/all/0/1\">Sasikanth Avancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Aviral Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxin Li</a>",
          "description": "Machine learning (ML) models are widely used in many important domains. For\nefficiently processing these computational- and memory-intensive applications,\ntensors of these over-parameterized models are compressed by leveraging\nsparsity, size reduction, and quantization of tensors. Unstructured sparsity\nand tensors with varying dimensions yield irregular computation, communication,\nand memory access patterns; processing them on hardware accelerators in a\nconventional manner does not inherently leverage acceleration opportunities.\nThis paper provides a comprehensive survey on the efficient execution of sparse\nand irregular tensor computations of ML models on hardware accelerators. In\nparticular, it discusses enhancement modules in the architecture design and the\nsoftware support; categorizes different hardware designs and acceleration\ntechniques and analyzes them in terms of hardware and execution costs; analyzes\nachievable accelerations for recent DNNs; highlights further opportunities in\nterms of hardware/software/model co-design optimizations (inter/intra-module).\nThe takeaways from this paper include: understanding the key challenges in\naccelerating sparse, irregular-shaped, and quantized tensors; understanding\nenhancements in accelerator systems for supporting their efficient\ncomputations; analyzing trade-offs in opting for a specific design choice for\nencoding, storing, extracting, communicating, computing, and load-balancing the\nnon-zeros; understanding how structured sparsity can improve storage efficiency\nand balance computations; understanding how to compile and map models with\nsparse tensors on the accelerators; understanding recent design trends for\nefficient accelerations and further opportunities.",
          "link": "http://arxiv.org/abs/2007.00864",
          "publishedOn": "2021-07-23T02:00:32.789Z",
          "wordCount": 725,
          "title": "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights. (arXiv:2007.00864v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frank_T/0/1/0/all/0/1\">Thorben Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1\">Stefan Chmiela</a>",
          "description": "Attention mechanisms are developing into a viable alternative to\nconvolutional layers as elementary building block of NNs. Their main advantage\nis that they are not restricted to capture local dependencies in the input, but\ncan draw arbitrary connections. This unprecedented capability coincides with\nthe long-standing problem of modeling global atomic interactions in molecular\nforce fields and other many-body problems. In its original formulation,\nhowever, attention is not applicable to the continuous domains in which the\natoms live. For this purpose we propose a variant to describe geometric\nrelations for arbitrary atomic configurations in Euclidean space that also\nrespects all relevant physical symmetries. We furthermore demonstrate, how the\nsuccessive application of our learned attention matrices effectively translates\nthe molecular geometry into a set of individual atomic contributions\non-the-fly.",
          "link": "http://arxiv.org/abs/2106.02549",
          "publishedOn": "2021-07-23T02:00:32.763Z",
          "wordCount": 600,
          "title": "Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anurag Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1\">Seth Cooper</a>",
          "description": "Several works have demonstrated the use of variational autoencoders (VAEs)\nfor generating levels in the style of existing games and blending levels across\ndifferent games. Further, quality-diversity (QD) algorithms have also become\npopular for generating varied game content by using evolution to explore a\nsearch space while focusing on both variety and quality. To reap the benefits\nof both these approaches, we present a level generation and game blending\napproach that combines the use of VAEs and QD algorithms. Specifically, we\ntrain VAEs on game levels and run the MAP-Elites QD algorithm using the learned\nlatent space of the VAE as the search space. The latent space captures the\nproperties of the games whose levels we want to generate and blend, while\nMAP-Elites searches this latent space to find a diverse set of levels\noptimizing a given objective such as playability. We test our method using\nmodels for 5 different platformer games as well as a blended domain spanning 3\nof these games. We refer to using MAP-Elites for blending as Blend-Elites. Our\nresults show that MAP-Elites in conjunction with VAEs enables the generation of\na diverse set of playable levels not just for each individual game but also for\nthe blended domain while illuminating game-specific regions of the blended\nlatent space.",
          "link": "http://arxiv.org/abs/2102.12463",
          "publishedOn": "2021-07-23T02:00:32.745Z",
          "wordCount": 687,
          "title": "Generating and Blending Game Levels via Quality-Diversity in the Latent Space of a Variational Autoencoder. (arXiv:2102.12463v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1\">Jeffrey Ichnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Paras Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stellato_B/0/1/0/all/0/1\">Bartolomeo Stellato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banjac_G/0/1/0/all/0/1\">Goran Banjac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Michael Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borrelli_F/0/1/0/all/0/1\">Francesco Borrelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>",
          "description": "First-order methods for quadratic optimization such as OSQP are widely used\nfor large-scale machine learning and embedded optimal control, where many\nrelated problems must be rapidly solved. These methods face two persistent\nchallenges: manual hyperparameter tuning and convergence time to high-accuracy\nsolutions. To address these, we explore how Reinforcement Learning (RL) can\nlearn a policy to tune parameters to accelerate convergence. In experiments\nwith well-known QP benchmarks we find that our RL policy, RLQP, significantly\noutperforms state-of-the-art QP solvers by up to 3x. RLQP generalizes\nsurprisingly well to previously unseen problems with varying dimension and\nstructure from different applications, including the QPLIB, Netlib LP and\nMaros-Meszaros problems. Code for RLQP is available at\nhttps://github.com/berkeleyautomation/rlqp.",
          "link": "http://arxiv.org/abs/2107.10847",
          "publishedOn": "2021-07-23T02:00:32.728Z",
          "wordCount": 569,
          "title": "Accelerating Quadratic Optimization with Reinforcement Learning. (arXiv:2107.10847v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jounghee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>",
          "description": "Language models (LMs) pretrained on a large text corpus and fine-tuned on a\ndownstream text corpus and fine-tuned on a downstream task becomes a de facto\ntraining strategy for several natural language processing (NLP) tasks.\nRecently, an adaptive pretraining method retraining the pretrained language\nmodel with task-relevant data has shown significant performance improvements.\nHowever, current adaptive pretraining methods suffer from underfitting on the\ntask distribution owing to a relatively small amount of data to re-pretrain the\nLM. To completely use the concept of adaptive pretraining, we propose a\nback-translated task-adaptive pretraining (BT-TAPT) method that increases the\namount of task-specific data for LM re-pretraining by augmenting the task data\nusing back-translation to generalize the LM to the target task domain. The\nexperimental results show that the proposed BT-TAPT yields improved\nclassification accuracy on both low- and high-resource data and better\nrobustness to noise than the conventional adaptive pretraining method.",
          "link": "http://arxiv.org/abs/2107.10474",
          "publishedOn": "2021-07-23T02:00:32.722Z",
          "wordCount": 589,
          "title": "Back-Translated Task Adaptive Pretraining: Improving Accuracy and Robustness on Text Classification. (arXiv:2107.10474v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10606",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Marti_G/0/1/0/all/0/1\">Gautier Marti</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Goubet_V/0/1/0/all/0/1\">Victor Goubet</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>",
          "description": "We propose a methodology to approximate conditional distributions in the\nelliptope of correlation matrices based on conditional generative adversarial\nnetworks. We illustrate the methodology with an application from quantitative\nfinance: Monte Carlo simulations of correlated returns to compare risk-based\nportfolio construction methods. Finally, we discuss about current limitations\nand advocate for further exploration of the elliptope geometry to improve\nresults.",
          "link": "http://arxiv.org/abs/2107.10606",
          "publishedOn": "2021-07-23T02:00:32.715Z",
          "wordCount": 520,
          "title": "cCorrGAN: Conditional Correlation GAN for Learning Empirical Conditional Distributions in the Elliptope. (arXiv:2107.10606v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagpal_C/0/1/0/all/0/1\">Chirag Nagpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1\">Steve Yadlowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostamzadeh_N/0/1/0/all/0/1\">Negar Rostamzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heller_K/0/1/0/all/0/1\">Katherine Heller</a>",
          "description": "Survival analysis is a challenging variation of regression modeling because\nof the presence of censoring, where the outcome measurement is only partially\nknown, due to, for example, loss to follow up. Such problems come up frequently\nin medical applications, making survival analysis a key endeavor in\nbiostatistics and machine learning for healthcare, with Cox regression models\nbeing amongst the most commonly employed models. We describe a new approach for\nsurvival analysis regression models, based on learning mixtures of Cox\nregressions to model individual survival distributions. We propose an\napproximation to the Expectation Maximization algorithm for this model that\ndoes hard assignments to mixture groups to make optimization efficient. In each\ngroup assignment, we fit the hazard ratios within each group using deep neural\nnetworks, and the baseline hazard for each mixture component\nnon-parametrically.\n\nWe perform experiments on multiple real world datasets, and look at the\nmortality rates of patients across ethnicity and gender. We emphasize the\nimportance of calibration in healthcare settings and demonstrate that our\napproach outperforms classical and modern survival analysis baselines, both in\nterms of discriminative performance and calibration, with large gains in\nperformance on the minority demographics.",
          "link": "http://arxiv.org/abs/2101.06536",
          "publishedOn": "2021-07-23T02:00:32.696Z",
          "wordCount": 671,
          "title": "Deep Cox Mixtures for Survival Regression. (arXiv:2101.06536v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10663",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shi_N/0/1/0/all/0/1\">Naichen Shi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lai_F/0/1/0/all/0/1\">Fan Lai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kontar_R/0/1/0/all/0/1\">Raed Al Kontar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mosharaf Chowdhury</a>",
          "description": "In this paper we propose Fed-ensemble: a simple approach that bringsmodel\nensembling to federated learning (FL). Instead of aggregating localmodels to\nupdate a single global model, Fed-ensemble uses random permutations to update a\ngroup of K models and then obtains predictions through model averaging.\nFed-ensemble can be readily utilized within established FL methods and does not\nimpose a computational overhead as it only requires one of the K models to be\nsent to a client in each communication round. Theoretically, we show that\npredictions on newdata from all K models belong to the same predictive\nposterior distribution under a neural tangent kernel regime. This result in\nturn sheds light onthe generalization advantages of model averaging. We also\nillustrate thatFed-ensemble has an elegant Bayesian interpretation. Empirical\nresults show that our model has superior performance over several FL\nalgorithms,on a wide range of data sets, and excels in heterogeneous settings\noften encountered in FL applications.",
          "link": "http://arxiv.org/abs/2107.10663",
          "publishedOn": "2021-07-23T02:00:32.688Z",
          "wordCount": 589,
          "title": "Fed-ensemble: Improving Generalization through Model Ensembling in Federated Learning. (arXiv:2107.10663v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sander_M/0/1/0/all/0/1\">Michael E. Sander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablin_P/0/1/0/all/0/1\">Pierre Ablin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1\">Mathieu Blondel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyre_G/0/1/0/all/0/1\">Gabriel Peyr&#xe9;</a>",
          "description": "The training of deep residual neural networks (ResNets) with backpropagation\nhas a memory cost that increases linearly with respect to the depth of the\nnetwork. A way to circumvent this issue is to use reversible architectures. In\nthis paper, we propose to change the forward rule of a ResNet by adding a\nmomentum term. The resulting networks, momentum residual neural networks\n(Momentum ResNets), are invertible. Unlike previous invertible architectures,\nthey can be used as a drop-in replacement for any existing ResNet block. We\nshow that Momentum ResNets can be interpreted in the infinitesimal step size\nregime as second-order ordinary differential equations (ODEs) and exactly\ncharacterize how adding momentum progressively increases the representation\ncapabilities of Momentum ResNets. Our analysis reveals that Momentum ResNets\ncan learn any linear mapping up to a multiplicative factor, while ResNets\ncannot. In a learning to optimize setting, where convergence to a fixed point\nis required, we show theoretically and empirically that our method succeeds\nwhile existing invertible architectures fail. We show on CIFAR and ImageNet\nthat Momentum ResNets have the same accuracy as ResNets, while having a much\nsmaller memory footprint, and show that pre-trained Momentum ResNets are\npromising for fine-tuning models.",
          "link": "http://arxiv.org/abs/2102.07870",
          "publishedOn": "2021-07-23T02:00:32.681Z",
          "wordCount": 668,
          "title": "Momentum Residual Neural Networks. (arXiv:2102.07870v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.05923",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Palma_G/0/1/0/all/0/1\">Giacomo De Palma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kiani_B/0/1/0/all/0/1\">Bobak T. Kiani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lloyd_S/0/1/0/all/0/1\">Seth Lloyd</a>",
          "description": "The reliability of deep learning algorithms is fundamentally challenged by\nthe existence of adversarial examples, which are incorrectly classified inputs\nthat are extremely close to a correctly classified input. We explore the\nproperties of adversarial examples for deep neural networks with random weights\nand biases, and prove that for any $p\\ge1$, the $\\ell^p$ distance of any given\ninput from the classification boundary scales as one over the square root of\nthe dimension of the input times the $\\ell^p$ norm of the input. The results\nare based on the recently proved equivalence between Gaussian processes and\ndeep neural networks in the limit of infinite width of the hidden layers, and\nare validated with experiments on both random deep neural networks and deep\nneural networks trained on the MNIST and CIFAR10 datasets. The results\nconstitute a fundamental advance in the theoretical understanding of\nadversarial examples, and open the way to a thorough theoretical\ncharacterization of the relation between network architecture and robustness to\nadversarial perturbations.",
          "link": "http://arxiv.org/abs/2004.05923",
          "publishedOn": "2021-07-23T02:00:32.675Z",
          "wordCount": 649,
          "title": "Adversarial Robustness Guarantees for Random Deep Neural Networks. (arXiv:2004.05923v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1\">Gavin Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Adam Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1\">Jonathan Ullman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakynthinou_L/0/1/0/all/0/1\">Lydia Zakynthinou</a>",
          "description": "We present two sample-efficient differentially private mean estimators for\n$d$-dimensional (sub)Gaussian distributions with unknown covariance.\nInformally, given $n \\gtrsim d/\\alpha^2$ samples from such a distribution with\nmean $\\mu$ and covariance $\\Sigma$, our estimators output $\\tilde\\mu$ such that\n$\\| \\tilde\\mu - \\mu \\|_{\\Sigma} \\leq \\alpha$, where $\\| \\cdot \\|_{\\Sigma}$ is\nthe Mahalanobis distance. All previous estimators with the same guarantee\neither require strong a priori bounds on the covariance matrix or require\n$\\Omega(d^{3/2})$ samples.\n\nEach of our estimators is based on a simple, general approach to designing\ndifferentially private mechanisms, but with novel technical steps to make the\nestimator private and sample-efficient. Our first estimator samples a point\nwith approximately maximum Tukey depth using the exponential mechanism, but\nrestricted to the set of points of large Tukey depth. Proving that this\nmechanism is private requires a novel analysis. Our second estimator perturbs\nthe empirical mean of the data set with noise calibrated to the empirical\ncovariance, without releasing the covariance itself. Its sample complexity\nguarantees hold more generally for subgaussian distributions, albeit with a\nslightly worse dependence on the privacy parameter. For both estimators,\ncareful preprocessing of the data is required to satisfy differential privacy.",
          "link": "http://arxiv.org/abs/2106.13329",
          "publishedOn": "2021-07-23T02:00:32.668Z",
          "wordCount": 658,
          "title": "Covariance-Aware Private Mean Estimation Without Private Covariance Estimation. (arXiv:2106.13329v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1\">Lauro Langosco di Langosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1\">Heiko Strathmann</a>",
          "description": "Particle-based approximate Bayesian inference approaches such as Stein\nVariational Gradient Descent (SVGD) combine the flexibility and convergence\nguarantees of sampling methods with the computational benefits of variational\ninference. In practice, SVGD relies on the choice of an appropriate kernel\nfunction, which impacts its ability to model the target distribution -- a\nchallenging problem with only heuristic solutions. We propose Neural\nVariational Gradient Descent (NVGD), which is based on parameterizing the\nwitness function of the Stein discrepancy by a deep neural network whose\nparameters are learned in parallel to the inference, mitigating the necessity\nto make any kernel choices whatsoever. We empirically evaluate our method on\npopular synthetic inference problems, real-world Bayesian linear regression,\nand Bayesian neural network inference.",
          "link": "http://arxiv.org/abs/2107.10731",
          "publishedOn": "2021-07-23T02:00:32.650Z",
          "wordCount": 550,
          "title": "Neural Variational Gradient Descent. (arXiv:2107.10731v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raich_R/0/1/0/all/0/1\">Raviv Raich</a>",
          "description": "In multiple instance multiple label learning, each sample, a bag, consists of\nmultiple instances. To alleviate labeling complexity, each sample is associated\nwith a set of bag-level labels leaving instances within the bag unlabeled. This\nsetting is more convenient and natural for representing complicated objects,\nwhich have multiple semantic meanings. Compared to single instance labeling,\nthis approach allows for labeling larger datasets at an equivalent labeling\ncost. However, for sufficiently large datasets, labeling all bags may become\nprohibitively costly. Active learning uses an iterative labeling and retraining\napproach aiming to provide reasonable classification performance using a small\nnumber of labeled samples. To our knowledge, only a few works in the area of\nactive learning in the MIML setting are available. These approaches can provide\npractical solutions to reduce labeling cost but their efficacy remains unclear.\nIn this paper, we propose a novel bag-class pair based approach for active\nlearning in the MIML setting. Due to the partial availability of bag-level\nlabels, we focus on the incomplete-label MIML setting for the proposed active\nlearning approach. Our approach is based on a discriminative graphical model\nwith efficient and exact inference. For the query process, we adapt active\nlearning criteria to the novel bag-class pair selection strategy. Additionally,\nwe introduce an online stochastic gradient descent algorithm to provide an\nefficient model update after each query. Numerical experiments on benchmark\ndatasets illustrate the robustness of the proposed approach.",
          "link": "http://arxiv.org/abs/2107.10804",
          "publishedOn": "2021-07-23T02:00:32.642Z",
          "wordCount": 677,
          "title": "Active Learning in Incomplete Label Multiple Instance Multiple Label Learning. (arXiv:2107.10804v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03337",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Harbrecht_H/0/1/0/all/0/1\">Helmut Harbrecht</a>, <a href=\"http://arxiv.org/find/math/1/au:+Multerer_M/0/1/0/all/0/1\">Michael Multerer</a>",
          "description": "In this article, we introduce the concept of samplets by transferring the\nconstruction of Tausch-White wavelets to the realm of data. This way we obtain\na multilevel representation of discrete data which directly enables data\ncompression, detection of singularities and adaptivity. Applying samplets to\nrepresent kernel matrices, as they arise in kernel based learning or Gaussian\nprocess regression, we end up with quasi-sparse matrices. By thresholding small\nentries, these matrices are compressible to O(N log N) relevant entries, where\nN is the number of data points. This feature allows for the use of fill-in\nreducing reorderings to obtain a sparse factorization of the compressed\nmatrices. Besides the comprehensive introduction to samplets and their\nproperties, we present extensive numerical studies to benchmark the approach.\nOur results demonstrate that samplets mark a considerable step in the direction\nof making large data sets accessible for analysis.",
          "link": "http://arxiv.org/abs/2107.03337",
          "publishedOn": "2021-07-23T02:00:32.636Z",
          "wordCount": 597,
          "title": "Samplets: A new paradigm for data compression. (arXiv:2107.03337v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">David Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>",
          "description": "It is well known that machine learning methods can be vulnerable to\nadversarially-chosen perturbations of their inputs. Despite significant\nprogress in the area, foundational open problems remain. In this paper, we\naddress several key questions. We derive exact and approximate Bayes-optimal\nrobust classifiers for the important setting of two- and three-class Gaussian\nclassification problems with arbitrary imbalance, for $\\ell_2$ and\n$\\ell_\\infty$ adversaries. In contrast to classical Bayes-optimal classifiers,\ndetermining the optimal decisions here cannot be made pointwise and new\ntheoretical approaches are needed. We develop and leverage new tools, including\nrecent breakthroughs from probability theory on robust isoperimetry, which, to\nour knowledge, have not yet been used in the area. Our results reveal\nfundamental tradeoffs between standard and robust accuracy that grow when data\nis imbalanced. We also show further results, including an analysis of\nclassification calibration for convex losses in certain models, and finite\nsample rates for the robust risk.",
          "link": "http://arxiv.org/abs/2006.05161",
          "publishedOn": "2021-07-23T02:00:32.629Z",
          "wordCount": 660,
          "title": "Provable tradeoffs in adversarially robust classification. (arXiv:2006.05161v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "In this paper, a novel two-branch neural network model structure is proposed\nfor multimodal emotion recognition, which consists of a time synchronous branch\n(TSB) and a time asynchronous branch (TAB). To capture correlations between\neach word and its acoustic realisation, the TSB combines speech and text\nmodalities at each input window frame and then does pooling across time to form\na single embedding vector. The TAB, by contrast, provides cross-utterance\ninformation by integrating sentence text embeddings from a number of context\nutterances into another embedding vector. The final emotion classification uses\nboth the TSB and the TAB embeddings. Experimental results on the IEMOCAP\ndataset demonstrate that the two-branch structure achieves state-of-the-art\nresults in 4-way classification with all common test setups. When using\nautomatic speech recognition (ASR) output instead of manually transcribed\nreference text, it is shown that the cross-utterance information considerably\nimproves the robustness against ASR errors. Furthermore, by incorporating an\nextra class for all the other emotions, the final 5-way classification system\nwith ASR hypotheses can be viewed as a prototype for more realistic emotion\nrecognition systems.",
          "link": "http://arxiv.org/abs/2010.14102",
          "publishedOn": "2021-07-23T02:00:32.606Z",
          "wordCount": 677,
          "title": "Emotion recognition by fusing time synchronous and time asynchronous representations. (arXiv:2010.14102v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1\">Gavin Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1\">Mark Bun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1\">Vitaly Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Adam Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1\">Kunal Talwar</a>",
          "description": "Modern machine learning models are complex and frequently encode surprising\namounts of information about individual inputs. In extreme cases, complex\nmodels appear to memorize entire input examples, including seemingly irrelevant\ninformation (social security numbers from text, for example). In this paper, we\naim to understand whether this sort of memorization is necessary for accurate\nlearning. We describe natural prediction problems in which every sufficiently\naccurate training algorithm must encode, in the prediction model, essentially\nall the information about a large subset of its training examples. This remains\ntrue even when the examples are high-dimensional and have entropy much higher\nthan the sample size, and even when most of that information is ultimately\nirrelevant to the task at hand. Further, our results do not depend on the\ntraining algorithm or the class of models used for learning.\n\nOur problems are simple and fairly natural variants of the next-symbol\nprediction and the cluster labeling tasks. These tasks can be seen as\nabstractions of text- and image-related prediction problems. To establish our\nresults, we reduce from a family of one-way communication problems for which we\nprove new information complexity lower bounds. Additionally, we present\nsynthetic-data experiments demonstrating successful attacks on logistic\nregression and neural network classifiers.",
          "link": "http://arxiv.org/abs/2012.06421",
          "publishedOn": "2021-07-23T02:00:32.599Z",
          "wordCount": 679,
          "title": "When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?. (arXiv:2012.06421v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1\">Deepak Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1\">Amar Phanishayee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kaiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>",
          "description": "Many state-of-the-art ML results have been obtained by scaling up the number\nof parameters in existing models. However, parameters and activations for such\nlarge models often do not fit in the memory of a single accelerator device;\nthis means that it is necessary to distribute training of large models over\nmultiple accelerators. In this work, we propose PipeDream-2BW, a system that\nsupports memory-efficient pipeline parallelism. PipeDream-2BW uses a novel\npipelining and weight gradient coalescing strategy, combined with the double\nbuffering of weights, to ensure high throughput, low memory footprint, and\nweight update semantics similar to data parallelism. In addition, PipeDream-2BW\nautomatically partitions the model over the available hardware resources, while\nrespecting hardware constraints such as memory capacities of accelerators and\ninterconnect topologies. PipeDream-2BW can accelerate the training of large GPT\nand BERT language models by up to 20$\\times$ with similar final model accuracy.",
          "link": "http://arxiv.org/abs/2006.09503",
          "publishedOn": "2021-07-23T02:00:32.589Z",
          "wordCount": 622,
          "title": "Memory-Efficient Pipeline-Parallel DNN Training. (arXiv:2006.09503v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wick_F/0/1/0/all/0/1\">F. Wick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerzel_U/0/1/0/all/0/1\">U. Kerzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">M. Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_M/0/1/0/all/0/1\">M. Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">T. Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_D/0/1/0/all/0/1\">D. Stemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_J/0/1/0/all/0/1\">J. Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feindt_M/0/1/0/all/0/1\">M. Feindt</a>",
          "description": "Demand forecasting is a central component of the replenishment process for\nretailers, as it provides crucial input for subsequent decision making like\nordering processes. In contrast to point estimates, such as the conditional\nmean of the underlying probability distribution, or confidence intervals,\nforecasting complete probability density functions allows to investigate the\nimpact on operational metrics, which are important to define the business\nstrategy, over the full range of the expected demand. Whereas metrics\nevaluating point estimates are widely used, methods for assessing the accuracy\nof predicted distributions are rare, and this work proposes new techniques for\nboth qualitative and quantitative evaluation methods. Using the supervised\nmachine learning method \"Cyclic Boosting\", complete individual probability\ndensity functions can be predicted such that each prediction is fully\nexplainable. This is of particular importance for practitioners, as it allows\nto avoid \"black-box\" models and understand the contributing factors for each\nindividual prediction. Another crucial aspect in terms of both explainability\nand generalizability of demand forecasting methods is the limitation of the\ninfluence of temporal confounding, which is prevalent in most state of the art\napproaches.",
          "link": "http://arxiv.org/abs/2009.07052",
          "publishedOn": "2021-07-23T02:00:32.583Z",
          "wordCount": 684,
          "title": "Demand Forecasting of Individual Probability Density Functions with Machine Learning. (arXiv:2009.07052v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Min Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kecheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wange_X/0/1/0/all/0/1\">Xu Wange</a>",
          "description": "To explore the robustness of recommender systems, researchers have proposed\nvarious shilling attack models and analyzed their adverse effects. Primitive\nattacks are highly feasible but less effective due to simplistic handcrafted\nrules, while upgraded attacks are more powerful but costly and difficult to\ndeploy because they require more knowledge from recommendations. In this paper,\nwe explore a novel shilling attack called Graph cOnvolution-based generative\nshilling ATtack (GOAT) to balance the attacks' feasibility and effectiveness.\nGOAT adopts the primitive attacks' paradigm that assigns items for fake users\nby sampling and the upgraded attacks' paradigm that generates fake ratings by a\ndeep learning-based model. It deploys a generative adversarial network (GAN)\nthat learns the real rating distribution to generate fake ratings.\nAdditionally, the generator combines a tailored graph convolution structure\nthat leverages the correlations between co-rated items to smoothen the fake\nratings and enhance their authenticity. The extensive experiments on two public\ndatasets evaluate GOAT's performance from multiple perspectives. Our study of\nthe GOAT demonstrates technical feasibility for building a more powerful and\nintelligent attack model with a much-reduced cost, enables analysis the threat\nof such an attack and guides for investigating necessary prevention measures.",
          "link": "http://arxiv.org/abs/2107.10457",
          "publishedOn": "2021-07-23T02:00:32.576Z",
          "wordCount": 663,
          "title": "Ready for Emerging Threats to Recommender Systems? A Graph Convolution-based Generative Shilling Attack. (arXiv:2107.10457v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00554",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Fukami_K/0/1/0/all/0/1\">Kai Fukami</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maulik_R/0/1/0/all/0/1\">Romit Maulik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ramachandra_N/0/1/0/all/0/1\">Nesar Ramachandra</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fukagata_K/0/1/0/all/0/1\">Koji Fukagata</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Taira_K/0/1/0/all/0/1\">Kunihiko Taira</a>",
          "description": "Achieving accurate and robust global situational awareness of a complex\ntime-evolving field from a limited number of sensors has been a longstanding\nchallenge. This reconstruction problem is especially difficult when sensors are\nsparsely positioned in a seemingly random or unorganized manner, which is often\nencountered in a range of scientific and engineering problems. Moreover, these\nsensors can be in motion and can become online or offline over time. The key\nleverage in addressing this scientific issue is the wealth of data accumulated\nfrom the sensors. As a solution to this problem, we propose a data-driven\nspatial field recovery technique founded on a structured grid-based\ndeep-learning approach for arbitrary positioned sensors of any numbers. It\nshould be noted that the na\\\"ive use of machine learning becomes prohibitively\nexpensive for global field reconstruction and is furthermore not adaptable to\nan arbitrary number of sensors. In the present work, we consider the use of\nVoronoi tessellation to obtain a structured-grid representation from sensor\nlocations enabling the computationally tractable use of convolutional neural\nnetworks. One of the central features of the present method is its\ncompatibility with deep-learning based super-resolution reconstruction\ntechniques for structured sensor data that are established for image\nprocessing. The proposed reconstruction technique is demonstrated for unsteady\nwake flow, geophysical data, and three-dimensional turbulence. The current\nframework is able to handle an arbitrary number of moving sensors, and thereby\novercomes a major limitation with existing reconstruction methods. The\npresented technique opens a new pathway towards the practical use of neural\nnetworks for real-time global field estimation.",
          "link": "http://arxiv.org/abs/2101.00554",
          "publishedOn": "2021-07-23T02:00:32.559Z",
          "wordCount": 717,
          "title": "Global field reconstruction from sparse sensors with Voronoi tessellation-assisted deep learning. (arXiv:2101.00554v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albert_K/0/1/0/all/0/1\">Kendra Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delano_M/0/1/0/all/0/1\">Maggie Delano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1\">Bogdan Kulynych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ram Shankar Siva Kumar</a>",
          "description": "Attacks from adversarial machine learning (ML) have the potential to be used\n\"for good\": they can be used to run counter to the existing power structures\nwithin ML, creating breathing space for those who would otherwise be the\ntargets of surveillance and control. But most research on adversarial ML has\nnot engaged in developing tools for resistance against ML systems. Why? In this\npaper, we review the broader impact statements that adversarial ML researchers\nwrote as part of their NeurIPS 2020 papers and assess the assumptions that\nauthors have about the goals of their work. We also collect information about\nhow authors view their work's impact more generally. We find that most\nadversarial ML researchers at NeurIPS hold two fundamental assumptions that\nwill make it difficult for them to consider socially beneficial uses of\nattacks: (1) it is desirable to make systems robust, independent of context,\nand (2) attackers of systems are normatively bad and defenders of systems are\nnormatively good. That is, despite their expressed and supposed neutrality,\nmost adversarial ML researchers believe that the goal of their work is to\nsecure systems, making it difficult to conceptualize and build tools for\ndisrupting the status quo.",
          "link": "http://arxiv.org/abs/2107.10302",
          "publishedOn": "2021-07-23T02:00:32.552Z",
          "wordCount": 687,
          "title": "Adversarial for Good? How the Adversarial ML Community's Values Impede Socially Beneficial Uses of Attacks. (arXiv:2107.10302v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10790",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mayor_Torres_J/0/1/0/all/0/1\">Juan Manuel Mayor-Torres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravanelli_M/0/1/0/all/0/1\">Mirco Ravanelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medina_DeVilliers_S/0/1/0/all/0/1\">Sara E. Medina-DeVilliers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lerner_M/0/1/0/all/0/1\">Matthew D. Lerner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>",
          "description": "Machine learning methods, such as deep learning, show promising results in\nthe medical domain. However, the lack of interpretability of these algorithms\nmay hinder their applicability to medical decision support systems. This paper\nstudies an interpretable deep learning technique, called SincNet. SincNet is a\nconvolutional neural network that efficiently learns customized band-pass\nfilters through trainable sinc-functions. In this study, we use SincNet to\nanalyze the neural activity of individuals with Autism Spectrum Disorder (ASD),\nwho experience characteristic differences in neural oscillatory activity. In\nparticular, we propose a novel SincNet-based neural network for detecting\nemotions in ASD patients using EEG signals. The learned filters can be easily\ninspected to detect which part of the EEG spectrum is used for predicting\nemotions. We found that our system automatically learns the high-$\\alpha$ (9-13\nHz) and $\\beta$ (13-30 Hz) band suppression often present in individuals with\nASD. This result is consistent with recent neuroscience studies on emotion\nrecognition, which found an association between these band suppressions and the\nbehavioral deficits observed in individuals with ASD. The improved\ninterpretability of SincNet is achieved without sacrificing performance in\nemotion recognition.",
          "link": "http://arxiv.org/abs/2107.10790",
          "publishedOn": "2021-07-23T02:00:32.546Z",
          "wordCount": 642,
          "title": "Interpretable SincNet-based Deep Learning for Emotion Recognition from EEG brain activity. (arXiv:2107.10790v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2008.07298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atli_B/0/1/0/all/0/1\">Buse Gul Atli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchal_S/0/1/0/all/0/1\">Samuel Marchal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1\">N. Asokan</a>",
          "description": "Federated learning is a distributed learning technique where machine learning\nmodels are trained on client devices in which the local training data resides.\nThe training is coordinated via a central server which is, typically,\ncontrolled by the intended owner of the resulting model. By avoiding the need\nto transport the training data to the central server, federated learning\nimproves privacy and efficiency. But it raises the risk of model theft by\nclients because the resulting model is available on every client device. Even\nif the application software used for local training may attempt to prevent\ndirect access to the model, a malicious client may bypass any such restrictions\nby reverse engineering the application software. Watermarking is a well-known\ndeterrence method against model theft by providing the means for model owners\nto demonstrate ownership of their models. Several recent deep neural network\n(DNN) watermarking techniques use backdooring: training the models with\nadditional mislabeled data. Backdooring requires full access to the training\ndata and control of the training process. This is feasible when a single party\ntrains the model in a centralized manner, but not in a federated learning\nsetting where the training process and training data are distributed among\nseveral client devices. In this paper, we present WAFFLE, the first approach to\nwatermark DNN models trained using federated learning. It introduces a\nretraining step at the server after each aggregation of local models into the\nglobal model. We show that WAFFLE efficiently embeds a resilient watermark into\nmodels incurring only negligible degradation in test accuracy (-0.17%), and\ndoes not require access to training data. We also introduce a novel technique\nto generate the backdoor used as a watermark. It outperforms prior techniques,\nimposing no communication, and low computational (+3.2%) overhead.",
          "link": "http://arxiv.org/abs/2008.07298",
          "publishedOn": "2021-07-23T02:00:32.531Z",
          "wordCount": 786,
          "title": "WAFFLE: Watermarking in Federated Learning. (arXiv:2008.07298v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duzceker_A/0/1/0/all/0/1\">Arda D&#xfc;z&#xe7;eker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galliani_S/0/1/0/all/0/1\">Silvano Galliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1\">Christoph Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speciale_P/0/1/0/all/0/1\">Pablo Speciale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1\">Mihai Dusmanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>",
          "description": "We propose an online multi-view depth prediction approach on posed video\nstreams, where the scene geometry information computed in the previous time\nsteps is propagated to the current time step in an efficient and geometrically\nplausible way. The backbone of our approach is a real-time capable, lightweight\nencoder-decoder that relies on cost volumes computed from pairs of images. We\nextend it by placing a ConvLSTM cell at the bottleneck layer, which compresses\nan arbitrary amount of past information in its states. The novelty lies in\npropagating the hidden state of the cell by accounting for the viewpoint\nchanges between time steps. At a given time step, we warp the previous hidden\nstate into the current camera plane using the previous depth prediction. Our\nextension brings only a small overhead of computation time and memory\nconsumption, while improving the depth predictions significantly. As a result,\nwe outperform the existing state-of-the-art multi-view stereo methods on most\nof the evaluated metrics in hundreds of indoor scenes while maintaining a\nreal-time performance. Code available:\nhttps://github.com/ardaduz/deep-video-mvs",
          "link": "http://arxiv.org/abs/2012.02177",
          "publishedOn": "2021-07-23T02:00:32.512Z",
          "wordCount": 660,
          "title": "DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion. (arXiv:2012.02177v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mir_A/0/1/0/all/0/1\">Amir M. Mir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latoskinas_E/0/1/0/all/0/1\">Evaldas Latoskinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proksch_S/0/1/0/all/0/1\">Sebastian Proksch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gousios_G/0/1/0/all/0/1\">Georgios Gousios</a>",
          "description": "Dynamic languages, such as Python and Javascript, trade static typing for\ndeveloper flexibility and productivity. Lack of static typing can cause\nrun-time exceptions and is a major factor for weak IDE support. To alleviate\nthese issues, PEP 484 introduced optional type annotations for Python. As\nretrofitting types to existing codebases is error-prone and laborious,\nlearning-based approaches have been proposed to enable automatic type\nannotations based on existing, partially annotated codebases. However, it is\nstill quite challenging for learning-based approaches to give a relevant\nprediction in the first suggestion or the first few ones. In this paper, we\npresent Type4Py, a deep similarity learning-based hierarchical neural network\nmodel that learns to discriminate between types of the same kind and dissimilar\ntypes in a high-dimensional space, which results in clusters of types. Nearest\nneighbor search suggests a list of likely types for arguments, variables, and\nfunctions' return. The results of the quantitative and qualitative evaluation\nindicate that Type4Py significantly outperforms state-of-the-art approaches at\nthe type prediction task. Considering the Top-1 prediction, Type4Py obtains a\nMean Reciprocal Rank of 72.5%, which is 10.87% and 16.45% higher than that of\nTypilus and TypeWriter, respectively.",
          "link": "http://arxiv.org/abs/2101.04470",
          "publishedOn": "2021-07-23T02:00:32.504Z",
          "wordCount": 681,
          "title": "Type4Py: Deep Similarity Learning-Based Type Inference for Python. (arXiv:2101.04470v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oza_M/0/1/0/all/0/1\">Manan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sukalpa Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>",
          "description": "Faces generated using generative adversarial networks (GANs) have reached\nunprecedented realism. These faces, also known as \"Deep Fakes\", appear as\nrealistic photographs with very little pixel-level distortions. While some work\nhas enabled the training of models that lead to the generation of specific\nproperties of the subject, generating a facial image based on a natural\nlanguage description has not been fully explored. For security and criminal\nidentification, the ability to provide a GAN-based system that works like a\nsketch artist would be incredibly useful. In this paper, we present a novel\napproach to generate facial images from semantic text descriptions. The learned\nmodel is provided with a text description and an outline of the type of face,\nwhich the model uses to sketch the features. Our models are trained using an\nAffine Combination Module (ACM) mechanism to combine the text embedding from\nBERT and the GAN latent space using a self-attention matrix. This avoids the\nloss of features due to inadequate \"attention\", which may happen if text\nembedding and latent vector are simply concatenated. Our approach is capable of\ngenerating images that are very accurately aligned to the exhaustive textual\ndescriptions of faces with many fine detail features of the face and helps in\ngenerating better images. The proposed method is also capable of making\nincremental changes to a previously generated image if it is provided with\nadditional textual descriptions or sentences.",
          "link": "http://arxiv.org/abs/2107.10756",
          "publishedOn": "2021-07-23T02:00:32.497Z",
          "wordCount": 672,
          "title": "Semantic Text-to-Face GAN -ST^2FG. (arXiv:2107.10756v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goudarzi_A/0/1/0/all/0/1\">Armin Goudarzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spehr_C/0/1/0/all/0/1\">Carsten Spehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbold_S/0/1/0/all/0/1\">Steffen Herbold</a>",
          "description": "Beamforming is an imaging tool for the investigation of aeroacoustic\nphenomena and results in high dimensional data that is broken down to spectra\nby integrating spatial Regions Of Interest. This paper presents two methods\nthat enable the automated identification of aeroacoustic sources in sparse\nbeamforming maps and the extraction of their corresponding spectra to overcome\nthe manual definition of Regions Of Interest. The methods are evaluated on two\nscaled airframe half-model wind-tunnel measurements and on a generic monopole\nsource. The first relies on the spatial normal distribution of aeroacoustic\nbroadband sources in sparse beamforming maps. The second uses hierarchical\nclustering methods. Both methods are robust to statistical noise and predict\nthe existence, location, and spatial probability estimation for sources based\non which Regions Of Interest are automatically determined.",
          "link": "http://arxiv.org/abs/2012.09643",
          "publishedOn": "2021-07-23T02:00:32.489Z",
          "wordCount": 627,
          "title": "Automatic source localization and spectra generation from sparse beamforming maps. (arXiv:2012.09643v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1\">Tales Imbiriba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Closas_P/0/1/0/all/0/1\">Pau Closas</a>",
          "description": "Localization and tracking of objects using data-driven methods is a popular\ntopic due to the complexity in characterizing the physics of wireless channel\npropagation models. In these modeling approaches, data needs to be gathered to\naccurately train models, at the same time that user's privacy is maintained. An\nappealing scheme to cooperatively achieve these goals is known as Federated\nLearning (FL). A challenge in FL schemes is the presence of non-independent and\nidentically distributed (non-IID) data, caused by unevenly exploration of\ndifferent areas. In this paper, we consider the use of recent FL schemes to\ntrain a set of personalized models that are then optimally fused through\nBayesian rules, which makes it appropriate in the context of indoor\nlocalization.",
          "link": "http://arxiv.org/abs/2107.04189",
          "publishedOn": "2021-07-23T02:00:32.483Z",
          "wordCount": 574,
          "title": "Personalized Federated Learning over non-IID Data for Indoor Localization. (arXiv:2107.04189v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbado_A/0/1/0/all/0/1\">Alberto Barbado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcho_O/0/1/0/all/0/1\">&#xd3;scar Corcho</a>",
          "description": "Identifying anomalies in the fuel consumption of the vehicles of a fleet is a\ncrucial aspect for optimizing consumption and reduce costs. However, this\ninformation alone is insufficient, since fleet operators need to know the\ncauses behind anomalous fuel consumption. We combine unsupervised anomaly\ndetection techniques, domain knowledge and interpretable Machine Learning\nmodels for explaining potential causes of abnormal fuel consumption in terms of\nfeature relevance. The explanations are used for generating recommendations\nabout fuel optimization, that are adjusted according to two different user\nprofiles: fleet managers and fleet operators. Results are evaluated over\nreal-world data from telematics devices connected to diesel and petrol vehicles\nfrom different types of industrial fleets. We measure the proposal regarding\nmodel performance, and using Explainable AI metrics that compare the\nexplanations in terms of representativeness, fidelity, stability,\ncontrastiveness and consistency with apriori beliefs. The potential fuel\nreductions that can be achieved is round 35%.",
          "link": "http://arxiv.org/abs/2010.16051",
          "publishedOn": "2021-07-23T02:00:32.468Z",
          "wordCount": 654,
          "title": "Interpretable Machine Learning Models for Predicting and Explaining Vehicle Fuel Consumption Anomalies. (arXiv:2010.16051v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1\">Giorgio Franceschelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1\">Mirco Musolesi</a>",
          "description": "Machine-generated artworks are now part of the contemporary art scene: they\nare attracting significant investments and they are presented in exhibitions\ntogether with those created by human artists. These artworks are mainly based\non generative deep learning techniques, which have seen a formidable\ndevelopment and remarkable refinement in the very recent years. Given the\ninherent characteristics of these techniques, a series of novel legal problems\narise.\n\nIn this article, we consider a set of key questions in the area of generative\ndeep learning for the arts, including the following: is it possible to use\ncopyrighted works as training set for generative models? How do we legally\nstore their copies in order to perform the training process? Who (if someone)\nwill own the copyright on the generated data? We try to answer these questions\nconsidering the law in force in both the United States of America and the\nEuropean Union, and potential future alternatives. Finally, we also formulate a\nset of practical guidelines for artists and developers working on deep learning\ngenerated art.",
          "link": "http://arxiv.org/abs/2105.09266",
          "publishedOn": "2021-07-23T02:00:32.439Z",
          "wordCount": 676,
          "title": "Copyright in Generative Deep Learning. (arXiv:2105.09266v3 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10746",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qendro_L/0/1/0/all/0/1\">Lorena Qendro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_A/0/1/0/all/0/1\">Alexander Campbell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mascolo_C/0/1/0/all/0/1\">Cecilia Mascolo</a>",
          "description": "Electroencephalography (EEG) is crucial for the monitoring and diagnosis of\nbrain disorders. However, EEG signals suffer from perturbations caused by\nnon-cerebral artifacts limiting their efficacy. Current artifact detection\npipelines are resource-hungry and rely heavily on hand-crafted features.\nMoreover, these pipelines are deterministic in nature, making them unable to\ncapture predictive uncertainty. We propose E4G, a deep learning framework for\nhigh frequency EEG artifact detection. Our framework exploits the early exit\nparadigm, building an implicit ensemble of models capable of capturing\nuncertainty. We evaluate our approach on the Temple University Hospital EEG\nArtifact Corpus (v2.0) achieving state-of-the-art classification results. In\naddition, E4G provides well-calibrated uncertainty metrics comparable to\nsampling techniques like Monte Carlo dropout in just a single forward pass. E4G\nopens the door to uncertainty-aware artifact detection supporting\nclinicians-in-the-loop frameworks.",
          "link": "http://arxiv.org/abs/2107.10746",
          "publishedOn": "2021-07-23T02:00:32.432Z",
          "wordCount": 584,
          "title": "High Frequency EEG Artifact Detection with Uncertainty via Early Exit Paradigm. (arXiv:2107.10746v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11580",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tu_H/0/1/0/all/0/1\">Hao Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moura_S/0/1/0/all/0/1\">Scott Moura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huazhen Fang</a>",
          "description": "Mathematical modeling of lithium-ion batteries (LiBs) is a central challenge\nin advanced battery management. This paper presents a new approach to integrate\na physics-based model with machine learning to achieve high-precision modeling\nfor LiBs. This approach uniquely proposes to inform the machine learning model\nof the dynamic state of the physical model, enabling a deep integration between\nphysics and machine learning. We propose two hybrid physics-machine learning\nmodels based on the approach, which blend a single particle model with thermal\ndynamics (SPMT) with a feedforward neural network (FNN) to perform\nphysics-informed learning of a LiB's dynamic behavior. The proposed models are\nrelatively parsimonious in structure and can provide considerable predictive\naccuracy even at high C-rates, as shown by extensive simulations.",
          "link": "http://arxiv.org/abs/2103.11580",
          "publishedOn": "2021-07-23T02:00:32.424Z",
          "wordCount": 607,
          "title": "Integrating Electrochemical Modeling with Machine Learning for Lithium-Ion Batteries. (arXiv:2103.11580v4 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhe Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1\">Antonio Vergari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>",
          "description": "Computing the expectation of kernel functions is a ubiquitous task in machine\nlearning, with applications from classical support vector machines to\nexploiting kernel embeddings of distributions in probabilistic modeling,\nstatistical inference, causal discovery, and deep learning. In all these\nscenarios, we tend to resort to Monte Carlo estimates as expectations of\nkernels are intractable in general. In this work, we characterize the\nconditions under which we can compute expected kernels exactly and efficiently,\nby leveraging recent advances in probabilistic circuit representations. We\nfirst construct a circuit representation for kernels and propose an approach to\nsuch tractable computation. We then demonstrate possible advancements for\nkernel embedding frameworks by exploiting tractable expected kernels to derive\nnew algorithms for two challenging scenarios: 1) reasoning under missing data\nwith kernel support vector regressors; 2) devising a collapsed black-box\nimportance sampling scheme. Finally, we empirically evaluate both algorithms\nand show that they outperform standard baselines on a variety of datasets.",
          "link": "http://arxiv.org/abs/2102.10562",
          "publishedOn": "2021-07-23T02:00:32.405Z",
          "wordCount": 617,
          "title": "Tractable Computation of Expected Kernels. (arXiv:2102.10562v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10716",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ponomarchuk_A/0/1/0/all/0/1\">Alexander Ponomarchuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burenko_I/0/1/0/all/0/1\">Ilya Burenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malkin_E/0/1/0/all/0/1\">Elian Malkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nazarov_I/0/1/0/all/0/1\">Ivan Nazarov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avetisian_M/0/1/0/all/0/1\">Manvel Avetisian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhukov_L/0/1/0/all/0/1\">Leonid Zhukov</a>",
          "description": "The COVID-19 pandemic created a significant interest and demand for infection\ndetection and monitoring solutions. In this paper we propose a machine learning\nmethod to quickly triage COVID-19 using recordings made on consumer devices.\nThe approach combines signal processing methods with fine-tuned deep learning\nnetworks and provides methods for signal denoising, cough detection and\nclassification. We have also developed and deployed a mobile application that\nuses symptoms checker together with voice, breath and cough signals to detect\nCOVID-19 infection. The application showed robust performance on both open\nsourced datasets and on the noisy data collected during beta testing by the end\nusers.",
          "link": "http://arxiv.org/abs/2107.10716",
          "publishedOn": "2021-07-23T02:00:32.398Z",
          "wordCount": 618,
          "title": "Project Achoo: A Practical Model and Application for COVID-19 Detection from Recordings of Breath, Voice, and Cough. (arXiv:2107.10716v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1\">Guangyin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fuxian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jincai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Travel time estimation is one of the core tasks for the development of\nintelligent transportation systems. Most previous works model the road segments\nor intersections separately by learning their spatio-temporal characteristics\nto estimate travel time. However, due to the continuous alternations of the\nroad segments and intersections in a path, the dynamic features are supposed to\nbe coupled and interactive. Therefore, modeling one of them limits further\nimprovement in accuracy of estimating travel time. To address the above\nproblems, a novel graph-based deep learning framework for travel time\nestimation is proposed in this paper, namely Spatio-Temporal Dual Graph Neural\nNetworks (STDGNN). Specifically, we first establish the node-wise and edge-wise\ngraphs to respectively characterize the adjacency relations of intersections\nand that of road segments. In order to extract the joint spatio-temporal\ncorrelations of the intersections and road segments, we adopt the\nspatio-temporal dual graph learning approach that incorporates multiple\nspatial-temporal dual graph learning modules with multi-scale network\narchitectures for capturing multi-level spatial-temporal information from the\ndual graph. Finally, we employ the multi-task learning approach to estimate the\ntravel time of a given whole route, each road segment and intersection\nsimultaneously. We conduct extensive experiments to evaluate our proposed model\non three real-world trajectory datasets, and the experimental results show that\nSTDGNN significantly outperforms several state-of-art baselines.",
          "link": "http://arxiv.org/abs/2105.13591",
          "publishedOn": "2021-07-23T02:00:32.392Z",
          "wordCount": 682,
          "title": "Spatio-Temporal Dual Graph Neural Networks for Travel Time Estimation. (arXiv:2105.13591v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03905",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Catak_F/0/1/0/all/0/1\">Ferhat Ozgur Catak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Catak_E/0/1/0/all/0/1\">Evren Catak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuzlu_M/0/1/0/all/0/1\">Murat Kuzlu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cali_U/0/1/0/all/0/1\">Umit Cali</a>",
          "description": "6G -- sixth generation -- is the latest cellular technology currently under\ndevelopment for wireless communication systems. In recent years, machine\nlearning algorithms have been applied widely in various fields, such as\nhealthcare, transportation, energy, autonomous car, and many more. Those\nalgorithms have been also using in communication technologies to improve the\nsystem performance in terms of frequency spectrum usage, latency, and security.\nWith the rapid developments of machine learning techniques, especially deep\nlearning, it is critical to take the security concern into account when\napplying the algorithms. While machine learning algorithms offer significant\nadvantages for 6G networks, security concerns on Artificial Intelligent (AI)\nmodels is typically ignored by the scientific community so far. However,\nsecurity is also a vital part of the AI algorithms, this is because the AI\nmodel itself can be poisoned by attackers. This paper proposes a mitigation\nmethod for adversarial attacks against proposed 6G machine learning models for\nthe millimeter-wave (mmWave) beam prediction using adversarial learning. The\nmain idea behind adversarial attacks against machine learning models is to\nproduce faulty results by manipulating trained deep learning models for 6G\napplications for mmWave beam prediction. We also present the adversarial\nlearning mitigation method's performance for 6G security in mmWave beam\nprediction application with fast gradient sign method attack. The mean square\nerrors (MSE) of the defended model under attack are very close to the\nundefended model without attack.",
          "link": "http://arxiv.org/abs/2105.03905",
          "publishedOn": "2021-07-23T02:00:32.385Z",
          "wordCount": 717,
          "title": "Security Concerns on Machine Learning Solutions for 6G Networks in mmWave Beam Prediction. (arXiv:2105.03905v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teterwak_P/0/1/0/all/0/1\">Piotr Teterwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1\">Dilip Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>",
          "description": "A discriminatively trained neural net classifier can fit the training data\nperfectly if all information about its input other than class membership has\nbeen discarded prior to the output layer. Surprisingly, past research has\ndiscovered that some extraneous visual detail remains in the logit vector. This\nfinding is based on inversion techniques that map deep embeddings back to\nimages. We explore this phenomenon further using a novel synthesis of methods,\nyielding a feedforward inversion model that produces remarkably high fidelity\nreconstructions, qualitatively superior to those of past efforts. When applied\nto an adversarially robust classifier model, the reconstructions contain\nsufficient local detail and global structure that they might be confused with\nthe original image in a quick glance, and the object category can clearly be\ngleaned from the reconstruction. Our approach is based on BigGAN (Brock, 2019),\nwith conditioning on logits instead of one-hot class labels. We use our\nreconstruction model as a tool for exploring the nature of representations,\nincluding: the influence of model architecture and training objectives\n(specifically robust losses), the forms of invariance that networks achieve,\nrepresentational differences between correctly and incorrectly classified\nimages, and the effects of manipulating logits and images. We believe that our\nmethod can inspire future investigations into the nature of information flow in\na neural net and can provide diagnostics for improving discriminative models.",
          "link": "http://arxiv.org/abs/2103.07470",
          "publishedOn": "2021-07-23T02:00:32.378Z",
          "wordCount": 689,
          "title": "Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers. (arXiv:2103.07470v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1\">Kevin Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serghiou_S/0/1/0/all/0/1\">Stylianos Serghiou</a>",
          "description": "Digital contact tracing apps for COVID, such as the one developed by Google\nand Apple, need to estimate the risk that a user was infected during a\nparticular exposure, in order to decide whether to notify the user to take\nprecautions, such as entering into quarantine, or requesting a test. Such risk\nscore models contain numerous parameters that must be set by the public health\nauthority. In this paper, we show how to automatically learn these parameters\nfrom data.\n\nOur method needs access to exposure and outcome data. Although this data is\nalready being collected (in an aggregated, privacy-preserving way) by several\nhealth authorities, in this paper we limit ourselves to simulated data, so that\nwe can systematically study the different factors that affect the feasibility\nof the approach. In particular, we show that the parameters become harder to\nestimate when there is more missing data (e.g., due to infections which were\nnot recorded by the app), and when there is model misspecification.\nNevertheless, the learning approach outperforms a strong manually designed\nbaseline. Furthermore, the learning approach can adapt even when the risk\nfactors of the disease change, e.g., due to the evolution of new variants, or\nthe adoption of vaccines.",
          "link": "http://arxiv.org/abs/2104.08415",
          "publishedOn": "2021-07-23T02:00:32.371Z",
          "wordCount": 710,
          "title": "Risk score learning for COVID-19 contact tracing apps. (arXiv:2104.08415v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finardi_P/0/1/0/all/0/1\">Paulo Finardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_J/0/1/0/all/0/1\">Jos&#xe9; Di&#xe9; Viegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_G/0/1/0/all/0/1\">Gustavo T. Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansano_A/0/1/0/all/0/1\">Alex F. Mansano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carida_V/0/1/0/all/0/1\">Vinicius F. Carid&#xe1;</a>",
          "description": "In the last few years, three major topics received increased interest: deep\nlearning, NLP and conversational agents. Bringing these three topics together\nto create an amazing digital customer experience and indeed deploy in\nproduction and solve real-world problems is something innovative and\ndisruptive. We introduce a new Portuguese financial domain language\nrepresentation model called BERTa\\'u. BERTa\\'u is an uncased BERT-base trained\nfrom scratch with data from the Ita\\'u virtual assistant chatbot solution. Our\nnovel contribution is that BERTa\\'u pretrained language model requires less\ndata, reached state-of-the-art performance in three NLP tasks, and generates a\nsmaller and lighter model that makes the deployment feasible. We developed\nthree tasks to validate our model: information retrieval with Frequently Asked\nQuestions (FAQ) from Ita\\'u bank, sentiment analysis from our virtual assistant\ndata, and a NER solution. All proposed tasks are real-world solutions in\nproduction on our environment and the usage of a specialist model proved to be\neffective when compared to Google BERT multilingual and the DPRQuestionEncoder\nfrom Facebook, available at Hugging Face. The BERTa\\'u improves the performance\nin 22% of FAQ Retrieval MRR metric, 2.1% in Sentiment Analysis F1 score, 4.4%\nin NER F1 score and can also represent the same sequence in up to 66% fewer\ntokens when compared to \"shelf models\".",
          "link": "http://arxiv.org/abs/2101.12015",
          "publishedOn": "2021-07-23T02:00:32.353Z",
          "wordCount": 687,
          "title": "BERTa\\'u: Ita\\'u BERT for digital customer service. (arXiv:2101.12015v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Sandip Ray</a>",
          "description": "The acceleration of CNNs has gained increasing atten-tion since their success\nin computer vision. With the heterogeneous functional layers that cannot be\npro-cessed by the accelerators proposed for convolution layers only, modern\nend-to-end CNN acceleration so-lutions either transform the diverse computation\ninto matrix/vector arithmetic, which loses data reuse op-portunities in\nconvolution, or introduce dedicated functional unit to each kind of layer,\nwhich results in underutilization and high update expense. To enhance the\nwhole-life cost efficiency, we need an acceleration solution that is efficient\nin processing CNN layers and has the generality to apply to all kinds of\nexisting and emerging layers. To this end, we pro-pose GCONV Chain, a method to\nconvert the entire CNN computation into a chain of standard general\nconvolutions (GCONV) that can be efficiently pro-cessed by the existing CNN\naccelerators. This paper comprehensively analyzes the GCONV Chain model and\nproposes a full-stack implementation to support GCONV Chain. On one hand, the\nresults on seven var-ious CNNs demonstrate that GCONV Chain improves the\nperformance and energy efficiency of existing CNN accelerators by an average of\n3.4x and 3.2x re-spectively. On the other hand, we show that GCONV Chain\nprovides low whole-life costs for CNN accelera-tion, including both developer\nefforts and total cost of ownership for the users.",
          "link": "http://arxiv.org/abs/2104.05541",
          "publishedOn": "2021-07-23T02:00:32.345Z",
          "wordCount": 673,
          "title": "Optimizing the Whole-life Cost in End-to-end CNN Acceleration. (arXiv:2104.05541v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10578",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1\">Cheng Ye</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1\">Rowan Swiers</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1\">Stephen Bonner</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1\">Ian Barrett</a>",
          "description": "The drug discovery and development process is a long and expensive one,\ncosting over 1 billion USD on average per drug and taking 10-15 years. To\nreduce the high levels of attrition throughout the process, there has been a\ngrowing interest in applying machine learning methodologies to various stages\nof drug discovery process in the recent decade, including at the earliest stage\n- identification of druggable disease genes. In this paper, we have developed a\nnew tensor factorisation model to predict potential drug targets (i.e.,genes or\nproteins) for diseases. We created a three dimensional tensor which consists of\n1,048 targets, 860 diseases and 230,011 evidence attributes and clinical\noutcomes connecting them, using data extracted from the Open Targets and\nPharmaProjects databases. We enriched the data with gene representations\nlearned from a drug discovery-oriented knowledge graph and applied our proposed\nmethod to predict the clinical outcomes for unseen target and dis-ease pairs.\nWe designed three evaluation strategies to measure the prediction performance\nand benchmarked several commonly used machine learning classifiers together\nwith matrix and tensor factorisation methods. The result shows that\nincorporating knowledge graph embeddings significantly improves the prediction\naccuracy and that training tensor factorisation alongside a dense neural\nnetwork outperforms other methods. In summary, our framework combines two\nactively studied machine learning approaches to disease target identification,\ntensor factorisation and knowledge graph representation learning, which could\nbe a promising avenue for further exploration in data-driven drug discovery.",
          "link": "http://arxiv.org/abs/2105.10578",
          "publishedOn": "2021-07-23T02:00:32.338Z",
          "wordCount": 705,
          "title": "Predicting Potential Drug Targets Using Tensor Factorisation and Knowledge Graph Embeddings. (arXiv:2105.10578v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Swazinna_P/0/1/0/all/0/1\">Phillip Swazinna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udluft_S/0/1/0/all/0/1\">Steffen Udluft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Runkler_T/0/1/0/all/0/1\">Thomas Runkler</a>",
          "description": "State-of-the-art reinforcement learning algorithms mostly rely on being\nallowed to directly interact with their environment to collect millions of\nobservations. This makes it hard to transfer their success to industrial\ncontrol problems, where simulations are often very costly or do not exist, and\nexploring in the real environment can potentially lead to catastrophic events.\nRecently developed, model-free, offline RL algorithms, can learn from a single\ndataset (containing limited exploration) by mitigating extrapolation error in\nvalue functions. However, the robustness of the training process is still\ncomparatively low, a problem known from methods using value functions. To\nimprove robustness and stability of the learning process, we use dynamics\nmodels to assess policy performance instead of value functions, resulting in\nMOOSE (MOdel-based Offline policy Search with Ensembles), an algorithm which\nensures low model bias by keeping the policy within the support of the data. We\ncompare MOOSE with state-of-the-art model-free, offline RL algorithms { BRAC,}\nBEAR and BCQ on the Industrial Benchmark and MuJoCo continuous control tasks in\nterms of robust performance, and find that MOOSE outperforms its model-free\ncounterparts in almost all considered cases, often even by far.",
          "link": "http://arxiv.org/abs/2008.05533",
          "publishedOn": "2021-07-23T02:00:32.332Z",
          "wordCount": 678,
          "title": "Overcoming Model Bias for Robust Offline Deep Reinforcement Learning. (arXiv:2008.05533v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03592",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1\">Sefik Emre Eskimez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "Visual emotion expression plays an important role in audiovisual speech\ncommunication. In this work, we propose a novel approach to rendering visual\nemotion expression in speech-driven talking face generation. Specifically, we\ndesign an end-to-end talking face generation system that takes a speech\nutterance, a single face image, and a categorical emotion label as input to\nrender a talking face video synchronized with the speech and expressing the\nconditioned emotion. Objective evaluation on image quality, audiovisual\nsynchronization, and visual emotion expression shows that the proposed system\noutperforms a state-of-the-art baseline system. Subjective evaluation of visual\nemotion expression and video realness also demonstrates the superiority of the\nproposed system. Furthermore, we conduct a human emotion recognition pilot\nstudy using generated videos with mismatched emotions among the audio and\nvisual modalities. Results show that humans respond to the visual modality more\nsignificantly than the audio modality on this task.",
          "link": "http://arxiv.org/abs/2008.03592",
          "publishedOn": "2021-07-23T02:00:32.325Z",
          "wordCount": 632,
          "title": "Speech Driven Talking Face Generation from a Single Image and an Emotion Condition. (arXiv:2008.03592v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_F/0/1/0/all/0/1\">Fardin Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanpour_J/0/1/0/all/0/1\">Javad Shabanpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyraghi_S/0/1/0/all/0/1\">Sina Beyraghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleimani_H/0/1/0/all/0/1\">Hossein Soleimani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oraizi_H/0/1/0/all/0/1\">Homayoon Oraizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleimani_M/0/1/0/all/0/1\">Mohammad Soleimani</a>",
          "description": "Compared to the conventional metasurface design, machine learning-based\nmethods have recently created an inspiring platform for an inverse realization\nof the metasurfaces. Here, we have used the Deep Neural Network (DNN) for the\ngeneration of desired output unit cell structures in an ultra-wide working\nfrequency band for both TE and TM polarized waves. To automatically generate\nmetasurfaces in a wide range of working frequencies from 4 to 45 GHz, we\ndeliberately design an 8 ring-shaped pattern in such a way that the unit-cells\ngenerated in the dataset can produce single or multiple notches in the desired\nworking frequency band. Compared to the general approach, whereby the final\nmetasurface structure may be formed by any randomly distributed \"0\" and \"1\", we\npropose here a restricted output structure. By restricting the output, the\nnumber of calculations will be reduced and the learning speed will be\nincreased. Moreover, we have shown that the accuracy of the network reaches\n91\\%. Obtaining the final unit cell directly without any time-consuming\noptimization algorithms for both TE and TM polarized waves, and high average\naccuracy, promises an effective strategy for the metasurface design; thus, the\ndesigner is required only to focus on the design goal.",
          "link": "http://arxiv.org/abs/2105.08508",
          "publishedOn": "2021-07-23T02:00:32.307Z",
          "wordCount": 673,
          "title": "A deep learning approach for inverse design of the metasurface for dual-polarized waves. (arXiv:2105.08508v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barros_C/0/1/0/all/0/1\">Claudio D. T. Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendonca_M/0/1/0/all/0/1\">Matheus R. F. Mendon&#xe7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_A/0/1/0/all/0/1\">Alex B. Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziviani_A/0/1/0/all/0/1\">Artur Ziviani</a>",
          "description": "Embedding static graphs in low-dimensional vector spaces plays a key role in\nnetwork analytics and inference, supporting applications like node\nclassification, link prediction, and graph visualization. However, many\nreal-world networks present dynamic behavior, including topological evolution,\nfeature evolution, and diffusion. Therefore, several methods for embedding\ndynamic graphs have been proposed to learn network representations over time,\nfacing novel challenges, such as time-domain modeling, temporal features to be\ncaptured, and the temporal granularity to be embedded. In this survey, we\noverview dynamic graph embedding, discussing its fundamentals and the recent\nadvances developed so far. We introduce the formal definition of dynamic graph\nembedding, focusing on the problem setting and introducing a novel taxonomy for\ndynamic graph embedding input and output. We further explore different dynamic\nbehaviors that may be encompassed by embeddings, classifying by topological\nevolution, feature evolution, and processes on networks. Afterward, we describe\nexisting techniques and propose a taxonomy for dynamic graph embedding\ntechniques based on algorithmic approaches, from matrix and tensor\nfactorization to deep learning, random walks, and temporal point processes. We\nalso elucidate main applications, including dynamic link prediction, anomaly\ndetection, and diffusion prediction, and we further state some promising\nresearch directions in the area.",
          "link": "http://arxiv.org/abs/2101.01229",
          "publishedOn": "2021-07-23T02:00:32.300Z",
          "wordCount": 680,
          "title": "A Survey on Embedding Dynamic Graphs. (arXiv:2101.01229v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1\">Siu Ming Yiu</a>",
          "description": "Graphs have been widely used in data mining and machine learning due to their\nunique representation of real-world objects and their interactions. As graphs\nare getting bigger and bigger nowadays, it is common to see their subgraphs\nseparately collected and stored in multiple local systems. Therefore, it is\nnatural to consider the subgraph federated learning setting, where each local\nsystem holding a small subgraph that may be biased from the distribution of the\nwhole graph. Hence, the subgraph federated learning aims to collaboratively\ntrain a powerful and generalizable graph mining model without directly sharing\ntheir graph data. In this work, towards the novel yet realistic setting of\nsubgraph federated learning, we propose two major techniques: (1) FedSage,\nwhich trains a GraphSage model based on FedAvg to integrate node features, link\nstructures, and task labels on multiple local subgraphs; (2) FedSage+, which\ntrains a missing neighbor generator along FedSage to deal with missing links\nacross local subgraphs. Empirical results on four real-world graph datasets\nwith synthesized subgraph federated learning settings demonstrate the\neffectiveness and efficiency of our proposed techniques. At the same time,\nconsistent theoretical implications are made towards their generalization\nability on the global graphs.",
          "link": "http://arxiv.org/abs/2106.13430",
          "publishedOn": "2021-07-23T02:00:32.292Z",
          "wordCount": 663,
          "title": "Subgraph Federated Learning with Missing Neighbor Generation. (arXiv:2106.13430v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Anand Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob Whitehill</a>",
          "description": "For the task of face verification, we explore the utility of harnessing\nauxiliary facial emotion labels to impose explicit geometric constraints on the\nembedding space when training deep embedding models. We introduce several novel\nloss functions that, in conjunction with a standard Triplet Loss [43], or\nArcFace loss [10], provide geometric constraints on the embedding space; the\nlabels for our loss functions can be provided using either manually annotated\nor automatically detected auxiliary emotion labels. Our method is implemented\npurely in terms of the loss function and does not require any changes to the\nneural network backbone of the embedding function.",
          "link": "http://arxiv.org/abs/2103.03862",
          "publishedOn": "2021-07-23T02:00:32.286Z",
          "wordCount": 593,
          "title": "Harnessing Geometric Constraints from Emotion Labels to improve Face Verification. (arXiv:2103.03862v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perrier_E/0/1/0/all/0/1\">Elija Perrier</a>",
          "description": "In this paper, we inaugurate the field of quantum fair machine learning. We\nundertake a comparative analysis of differences and similarities between\nclassical and quantum fair machine learning algorithms, specifying how the\nunique features of quantum computation alter measures, metrics and remediation\nstrategies when quantum algorithms are subject to fairness constraints. We\npresent the first results in quantum fair machine learning by demonstrating the\nuse of Grover's search algorithm to satisfy statistical parity constraints\nimposed on quantum algorithms. We provide lower-bounds on iterations needed to\nachieve such statistical parity within $\\epsilon$-tolerance. We extend\ncanonical Lipschitz-conditioned individual fairness criteria to the quantum\nsetting using quantum metrics. We examine the consequences for typical measures\nof fairness in machine learning context when quantum information processing and\nquantum data are involved. Finally, we propose open questions and research\nprogrammes for this new field of interest to researchers in computer science,\nethics and quantum computation.",
          "link": "http://arxiv.org/abs/2102.00753",
          "publishedOn": "2021-07-23T02:00:32.268Z",
          "wordCount": 595,
          "title": "Quantum Fair Machine Learning. (arXiv:2102.00753v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barati_R/0/1/0/all/0/1\">Ramin Barati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safabakhsh_R/0/1/0/all/0/1\">Reza Safabakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_M/0/1/0/all/0/1\">Mohammad Rahmati</a>",
          "description": "In this paper, we study the adversarial examples existence and adversarial\ntraining from the standpoint of convergence and provide evidence that pointwise\nconvergence in ANNs can explain these observations. The main contribution of\nour proposal is that it relates the objective of the evasion attacks and\nadversarial training with concepts already defined in learning theory. Also, we\nextend and unify some of the other proposals in the literature and provide\nalternative explanations on the observations made in those proposals. Through\ndifferent experiments, we demonstrate that the framework is valuable in the\nstudy of the phenomenon and is applicable to real-world problems.",
          "link": "http://arxiv.org/abs/2107.10599",
          "publishedOn": "2021-07-23T02:00:32.256Z",
          "wordCount": 550,
          "title": "Towards Explaining Adversarial Examples Phenomenon in Artificial Neural Networks. (arXiv:2107.10599v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hanyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Mirela Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capecci_D/0/1/0/all/0/1\">Daniel Capecci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1\">Luiz Giovanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czech_L/0/1/0/all/0/1\">Lauren Czech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1\">Juliana Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniela Oliveira</a>",
          "description": "Phishing and disinformation are popular social engineering attacks with\nattackers invariably applying influence cues in texts to make them more\nappealing to users. We introduce Lumen, a learning-based framework that exposes\ninfluence cues in text: (i) persuasion, (ii) framing, (iii) emotion, (iv)\nobjectivity/subjectivity, (v) guilt/blame, and (vi) use of emphasis. Lumen was\ntrained with a newly developed dataset of 3K texts comprised of disinformation,\nphishing, hyperpartisan news, and mainstream news. Evaluation of Lumen in\ncomparison to other learning models showed that Lumen and LSTM presented the\nbest F1-micro score, but Lumen yielded better interpretability. Our results\nhighlight the promise of ML to expose influence cues in text, towards the goal\nof application in automatic labeling tools to improve the accuracy of\nhuman-based detection and reduce the likelihood of users falling for deceptive\nonline content.",
          "link": "http://arxiv.org/abs/2107.10655",
          "publishedOn": "2021-07-23T02:00:32.250Z",
          "wordCount": 586,
          "title": "Lumen: A Machine Learning Framework to Expose Influence Cues in Text. (arXiv:2107.10655v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Krishnamurthy_V/0/1/0/all/0/1\">Vikram Krishnamurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pattanayak_K/0/1/0/all/0/1\">Kunal Pattanayak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gogineni_S/0/1/0/all/0/1\">Sandeep Gogineni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_B/0/1/0/all/0/1\">Bosung Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangaswamy_M/0/1/0/all/0/1\">Muralidhar Rangaswamy</a>",
          "description": "This paper considers three inter-related adversarial inference problems\ninvolving cognitive radars. We first discuss inverse tracking of the radar to\nestimate the adversary's estimate of us based on the radar's actions and\ncalibrate the radar's sensing accuracy. Second, using revealed preference from\nmicroeconomics, we formulate a non-parametric test to identify if the cognitive\nradar is a constrained utility maximizer with signal processing constraints. We\nconsider two radar functionalities, namely, beam allocation and waveform\ndesign, with respect to which the cognitive radar is assumed to maximize its\nutility and construct a set-valued estimator for the radar's utility function.\nFinally, we discuss how to engineer interference at the physical layer level to\nconfuse the radar which forces it to change its transmit waveform. The levels\nof abstraction range from smart interference design based on Wiener filters (at\nthe pulse/waveform level), inverse Kalman filters at the tracking level and\nrevealed preferences for identifying utility maximization at the systems level.",
          "link": "http://arxiv.org/abs/2008.01559",
          "publishedOn": "2021-07-23T02:00:32.243Z",
          "wordCount": 628,
          "title": "Adversarial Radar Inference: Inverse Tracking, Identifying Cognition and Designing Smart Interference. (arXiv:2008.01559v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gondal_M/0/1/0/all/0/1\">Muhammad Waleed Gondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shruti Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_N/0/1/0/all/0/1\">Nasim Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1\">Manuel W&#xfc;thrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Meta-learning algorithms adapt quickly to new tasks that are drawn from the\nsame task distribution as the training tasks. The mechanism leading to fast\nadaptation is the conditioning of a downstream predictive model on the inferred\nrepresentation of the task's underlying data generative process, or\n\\emph{function}. This \\emph{meta-representation}, which is computed from a few\nobserved examples of the underlying function, is learned jointly with the\npredictive model. In this work, we study the implications of this joint\ntraining on the transferability of the meta-representations. Our goal is to\nlearn meta-representations that are robust to noise in the data and facilitate\nsolving a wide range of downstream tasks that share the same underlying\nfunctions. To this end, we propose a decoupled encoder-decoder approach to\nsupervised meta-learning, where the encoder is trained with a contrastive\nobjective to find a good representation of the underlying function. In\nparticular, our training scheme is driven by the self-supervision signal\nindicating whether two sets of examples stem from the same function. Our\nexperiments on a number of synthetic and real-world datasets show that the\nrepresentations we obtain outperform strong baselines in terms of downstream\nperformance and noise robustness, even when these baselines are trained in an\nend-to-end manner.",
          "link": "http://arxiv.org/abs/2010.07093",
          "publishedOn": "2021-07-23T02:00:32.236Z",
          "wordCount": 683,
          "title": "Function Contrastive Learning of Transferable Meta-Representations. (arXiv:2010.07093v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1\">Jimmy Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1\">Nicolo Fusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>",
          "description": "Given a trained network, how can we accelerate it to meet efficiency needs\nfor deployment on particular hardware? The commonly used hardware-aware network\ncompression techniques address this question with pruning, kernel fusion,\nquantization and lowering precision. However, these approaches do not change\nthe underlying network operations. In this paper, we propose hardware-aware\nnetwork transformation (HANT), which accelerates a network by replacing\ninefficient operations with more efficient alternatives using a neural\narchitecture search like approach. HANT tackles the problem in two phase: In\nthe first phase, a large number of alternative operations per every layer of\nthe teacher model is trained using layer-wise feature map distillation. In the\nsecond phase, the combinatorial selection of efficient operations is relaxed to\nan integer optimization problem that can be solved in a few seconds. We extend\nHANT with kernel fusion and quantization to improve throughput even further.\nOur experimental results on accelerating the EfficientNet family show that HANT\ncan accelerate them by up to 3.6x with <0.4% drop in the top-1 accuracy on the\nImageNet dataset. When comparing the same latency level, HANT can accelerate\nEfficientNet-B4 to the same latency as EfficientNet-B1 while having 3% higher\naccuracy. We examine a large pool of operations, up to 197 per layer, and we\nprovide insights into the selected operations and final architectures.",
          "link": "http://arxiv.org/abs/2107.10624",
          "publishedOn": "2021-07-23T02:00:32.226Z",
          "wordCount": 656,
          "title": "HANT: Hardware-Aware Network Transformation. (arXiv:2107.10624v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rajvir Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginige_J/0/1/0/all/0/1\">Jeewani Anupama Ginige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obst_O/0/1/0/all/0/1\">Oliver Obst</a>",
          "description": "Codification of free-text clinical narratives have long been recognised to be\nbeneficial for secondary uses such as funding, insurance claim processing and\nresearch. The current scenario of assigning codes is a manual process which is\nvery expensive, time-consuming and error prone. In recent years, many\nresearchers have studied the use of Natural Language Processing (NLP), related\nMachine Learning (ML) and Deep Learning (DL) methods and techniques to resolve\nthe problem of manual coding of clinical narratives and to assist human coders\nto assign clinical codes more accurately and efficiently. This systematic\nliterature review provides a comprehensive overview of automated clinical\ncoding systems that utilises appropriate NLP, ML and DL methods and techniques\nto assign ICD codes to discharge summaries. We have followed the Preferred\nReporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and\nconducted a comprehensive search of publications from January, 2010 to December\n2020 in four academic databases- PubMed, ScienceDirect, Association for\nComputing Machinery(ACM) Digital Library, and the Association for Computational\nLinguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the\ninclusion criteria. This review identified: datasets having discharge\nsummaries; NLP techniques along with some other data extraction processes,\ndifferent feature extraction and embedding techniques. To measure the\nperformance of classification methods, different evaluation metrics are used.\nLastly, future research directions are provided to scholars who are interested\nin automated ICD code assignment. Efforts are still required to improve ICD\ncode prediction accuracy, availability of large-scale de-identified clinical\ncorpora with the latest version of the classification system. This can be a\nplatform to guide and share knowledge with the less experienced coders and\nresearchers.",
          "link": "http://arxiv.org/abs/2107.10652",
          "publishedOn": "2021-07-23T02:00:32.219Z",
          "wordCount": 725,
          "title": "A Systematic Literature Review of Automated ICD Coding and Classification Systems using Discharge Summaries. (arXiv:2107.10652v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08488",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Matsubara_T/0/1/0/all/0/1\">Takuo Matsubara</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1\">Chris J. Oates</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Briol</a>",
          "description": "Bayesian neural networks attempt to combine the strong predictive performance\nof neural networks with formal quantification of uncertainty associated with\nthe predictive output in the Bayesian framework. However, it remains unclear\nhow to endow the parameters of the network with a prior distribution that is\nmeaningful when lifted into the output space of the network. A possible\nsolution is proposed that enables the user to posit an appropriate Gaussian\nprocess covariance function for the task at hand. Our approach constructs a\nprior distribution for the parameters of the network, called a ridgelet prior,\nthat approximates the posited Gaussian process in the output space of the\nnetwork. In contrast to existing work on the connection between neural networks\nand Gaussian processes, our analysis is non-asymptotic, with finite sample-size\nerror bounds provided. This establishes the universality property that a\nBayesian neural network can approximate any Gaussian process whose covariance\nfunction is sufficiently regular. Our experimental assessment is limited to a\nproof-of-concept, where we demonstrate that the ridgelet prior can out-perform\nan unstructured prior on regression problems for which a suitable Gaussian\nprocess prior can be provided.",
          "link": "http://arxiv.org/abs/2010.08488",
          "publishedOn": "2021-07-23T02:00:32.202Z",
          "wordCount": 655,
          "title": "The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks. (arXiv:2010.08488v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonati_L/0/1/0/all/0/1\">Leonardo Bonati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DOro_S/0/1/0/all/0/1\">Salvatore D&#x27;Oro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polese_M/0/1/0/all/0/1\">Michele Polese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basagni_S/0/1/0/all/0/1\">Stefano Basagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melodia_T/0/1/0/all/0/1\">Tommaso Melodia</a>",
          "description": "Next Generation (NextG) cellular networks will be natively cloud-based and\nbuilt upon programmable, virtualized, and disaggregated architectures. The\nseparation of control functions from the hardware fabric and the introduction\nof standardized control interfaces will enable the definition of custom\nclosed-control loops, which will ultimately enable embedded intelligence and\nreal-time analytics, thus effectively realizing the vision of autonomous and\nself-optimizing networks. This article explores the disaggregated network\narchitecture proposed by the O-RAN Alliance as a key enabler of NextG networks.\nWithin this architectural context, we discuss the potential, the challenges,\nand the limitations of data-driven optimization approaches to network control\nover different timescales. We also present the first large-scale integration of\nO-RAN-compliant software components with an open-source full-stack softwarized\ncellular network. Experiments conducted on Colosseum, the world's largest\nwireless network emulator, demonstrate closed-loop integration of real-time\nanalytics and control through deep reinforcement learning agents. We also show\nthe feasibility of Radio Access Network (RAN) control through xApps running on\nthe near real-time RAN Intelligent Controller, to optimize the scheduling\npolicies of co-existing network slices, leveraging the O-RAN open interfaces to\ncollect data at the edge of the network.",
          "link": "http://arxiv.org/abs/2012.01263",
          "publishedOn": "2021-07-23T02:00:32.195Z",
          "wordCount": 659,
          "title": "Intelligence and Learning in O-RAN for Data-driven NextG Cellular Networks. (arXiv:2012.01263v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Artelt_A/0/1/0/all/0/1\">Andr&#xe9; Artelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaquet_V/0/1/0/all/0/1\">Valerie Vaquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velioglu_R/0/1/0/all/0/1\">Riza Velioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinder_F/0/1/0/all/0/1\">Fabian Hinder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinkrolf_J/0/1/0/all/0/1\">Johannes Brinkrolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilling_M/0/1/0/all/0/1\">Malte Schilling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Transparency is a fundamental requirement for decision making systems when\nthese should be deployed in the real world. It is usually achieved by providing\nexplanations of the system's behavior. A prominent and intuitive type of\nexplanations are counterfactual explanations. Counterfactual explanations\nexplain a behavior to the user by proposing actions -- as changes to the input\n-- that would cause a different (specified) behavior of the system. However,\nsuch explanation methods can be unstable with respect to small changes to the\ninput -- i.e. even a small change in the input can lead to huge or arbitrary\nchanges in the output and of the explanation. This could be problematic for\ncounterfactual explanations, as two similar individuals might get very\ndifferent explanations. Even worse, if the recommended actions differ\nconsiderably in their complexity, one would consider such unstable\n(counterfactual) explanations as individually unfair.\n\nIn this work, we formally and empirically study the robustness of\ncounterfactual explanations in general, as well as under different models and\ndifferent kinds of perturbations. Furthermore, we propose that plausible\ncounterfactual explanations can be used instead of closest counterfactual\nexplanations to improve the robustness and consequently the individual fairness\nof counterfactual explanations.",
          "link": "http://arxiv.org/abs/2103.02354",
          "publishedOn": "2021-07-23T02:00:32.188Z",
          "wordCount": 681,
          "title": "Evaluating Robustness of Counterfactual Explanations. (arXiv:2103.02354v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1\">Ayush Sekhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1\">Jayadev Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1\">Gautam Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>",
          "description": "We study the problem of unlearning datapoints from a learnt model. The\nlearner first receives a dataset $S$ drawn i.i.d. from an unknown distribution,\nand outputs a model $\\widehat{w}$ that performs well on unseen samples from the\nsame distribution. However, at some point in the future, any training datapoint\n$z \\in S$ can request to be unlearned, thus prompting the learner to modify its\noutput model while still ensuring the same accuracy guarantees. We initiate a\nrigorous study of generalization in machine unlearning, where the goal is to\nperform well on previously unseen datapoints. Our focus is on both\ncomputational and storage complexity.\n\nFor the setting of convex losses, we provide an unlearning algorithm that can\nunlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. In\ncomparison, in general, differentially private learning (which implies\nunlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. This\ndemonstrates a novel separation between differential privacy and machine\nunlearning.",
          "link": "http://arxiv.org/abs/2103.03279",
          "publishedOn": "2021-07-23T02:00:32.172Z",
          "wordCount": 623,
          "title": "Remember What You Want to Forget: Algorithms for Machine Unlearning. (arXiv:2103.03279v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_A/0/1/0/all/0/1\">Aditya Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Braghadeesh Lakshminarayanan</a>",
          "description": "Detecting abrupt changes in temporal behavior patterns is of interest in many\nindustrial and security applications. Abrupt changes are often local and\nobservable primarily through a well-aligned sensing action (e.g., a camera with\na narrow field-of-view). Due to resource constraints, continuous monitoring of\nall of the sensors is impractical. We propose the bandit quickest changepoint\ndetection framework as a means of balancing sensing cost with detection delay.\nIn this framework, sensing actions (or sensors) are sequentially chosen, and\nonly measurements corresponding to chosen actions are observed. We derive an\ninformation-theoretic lower bound on the detection delay for a general class of\nfinitely parameterized probability distributions. We then propose a\ncomputationally efficient online sensing scheme, which seamlessly balances the\nneed for exploration of different sensing options with exploitation of querying\ninformative actions. We derive expected delay bounds for the proposed scheme\nand show that these bounds match our information-theoretic lower bounds at low\nfalse alarm rates, establishing optimality of the proposed method. We then\nperform a number of experiments on synthetic and real datasets demonstrating\nthe efficacy of our proposed method.",
          "link": "http://arxiv.org/abs/2107.10492",
          "publishedOn": "2021-07-23T02:00:32.166Z",
          "wordCount": 615,
          "title": "Bandit Quickest Changepoint Detection. (arXiv:2107.10492v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_L/0/1/0/all/0/1\">Lucas F. F. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1\">Vitor C. A. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frances_R/0/1/0/all/0/1\">Regiane S. Kawasaki Franc&#xea;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudencio_R/0/1/0/all/0/1\">Ricardo B. C. Prud&#xea;ncio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1\">Ronnie C. O. Alves</a>",
          "description": "The classification experiments covered by machine learning (ML) are composed\nby two important parts: the data and the algorithm. As they are a fundamental\npart of the problem, both must be considered when evaluating a model's\nperformance against a benchmark. The best classifiers need robust benchmarks to\nbe properly evaluated. For this, gold standard benchmarks such as OpenML-CC18\nare used. However, data complexity is commonly not considered along with the\nmodel during a performance evaluation. Recent studies employ Item Response\nTheory (IRT) as a new approach to evaluating datasets and algorithms, capable\nof evaluating both simultaneously. This work presents a new evaluation\nmethodology based on IRT and Glicko-2, jointly with the decodIRT tool developed\nto guide the estimation of IRT in ML. It explores the IRT as a tool to evaluate\nthe OpenML-CC18 benchmark for its algorithmic evaluation capability and checks\nif there is a subset of datasets more efficient than the original benchmark.\nSeveral classifiers, from classics to ensemble, are also evaluated using the\nIRT models. The Glicko-2 rating system was applied together with IRT to\nsummarize the innate ability and classifiers performance. It was noted that not\nall OpenML-CC18 datasets are really useful for evaluating algorithms, where\nonly 10% were rated as being really difficult. Furthermore, it was verified the\nexistence of a more efficient subset containing only 50% of the original size.\nWhile Randon Forest was singled out as the algorithm with the best innate\nability.",
          "link": "http://arxiv.org/abs/2107.07451",
          "publishedOn": "2021-07-23T02:00:32.160Z",
          "wordCount": 711,
          "title": "Data vs classifiers, who wins?. (arXiv:2107.07451v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brouillard_P/0/1/0/all/0/1\">Philippe Brouillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taslakian_P/0/1/0/all/0/1\">Perouz Taslakian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacoste_A/0/1/0/all/0/1\">Alexandre Lacoste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lachapelle_S/0/1/0/all/0/1\">Sebastien Lachapelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1\">Alexandre Drouin</a>",
          "description": "Causal discovery from observational data is a challenging task to which an\nexact solution cannot always be identified. Under assumptions about the\ndata-generative process, the causal graph can often be identified up to an\nequivalence class. Proposing new realistic assumptions to circumscribe such\nequivalence classes is an active field of research. In this work, we propose a\nnew set of assumptions that constrain possible causal relationships based on\nthe nature of the variables. We thus introduce typed directed acyclic graphs,\nin which variable types are used to determine the validity of causal\nrelationships. We demonstrate, both theoretically and empirically, that the\nproposed assumptions can result in significant gains in the identification of\nthe causal graph.",
          "link": "http://arxiv.org/abs/2107.10703",
          "publishedOn": "2021-07-23T02:00:32.153Z",
          "wordCount": 582,
          "title": "Typing assumptions improve identification in causal discovery. (arXiv:2107.10703v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose mRASP2, a\ntraining method to obtain a single unified multilingual translation model.\nmRASP2 is empowered by two techniques: a) a contrastive learning scheme to\nclose the gap among representations of different languages, and b) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mRASP2 outperforms\nexisting best unified model and achieves competitive or even better performance\nthan the pre-trained and fine-tuned model mBART on tens of WMT's translation\ndirections. For non-English directions, mRASP2 achieves an improvement of\naverage 10+ BLEU compared with the multilingual Transformer baseline. Code,\ndata and trained models are available at https://github.com/PANXiao1994/mRASP2.",
          "link": "http://arxiv.org/abs/2105.09501",
          "publishedOn": "2021-07-23T02:00:32.146Z",
          "wordCount": 647,
          "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiahao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zejun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Miao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhui Xu</a>",
          "description": "(Gradient) Expectation Maximization (EM) is a widely used algorithm for\nestimating the maximum likelihood of mixture models or incomplete data\nproblems. A major challenge facing this popular technique is how to effectively\npreserve the privacy of sensitive data. Previous research on this problem has\nalready lead to the discovery of some Differentially Private (DP) algorithms\nfor (Gradient) EM. However, unlike in the non-private case, existing techniques\nare not yet able to provide finite sample statistical guarantees. To address\nthis issue, we propose in this paper the first DP version of (Gradient) EM\nalgorithm with statistical guarantees. Moreover, we apply our general framework\nto three canonical models: Gaussian Mixture Model (GMM), Mixture of Regressions\nModel (MRM) and Linear Regression with Missing Covariates (RMC). Specifically,\nfor GMM in the DP model, our estimation error is near optimal in some cases.\nFor the other two models, we provide the first finite sample statistical\nguarantees. Our theory is supported by thorough numerical experiments.",
          "link": "http://arxiv.org/abs/2010.13520",
          "publishedOn": "2021-07-23T02:00:32.139Z",
          "wordCount": 642,
          "title": "Differentially Private (Gradient) Expectation Maximization Algorithm with Statistical Guarantees. (arXiv:2010.13520v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaohu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Guijian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "The surrogate model-based uncertainty quantification method has drawn a lot\nof attention in recent years. Both the polynomial chaos expansion (PCE) and the\ndeep learning (DL) are powerful methods for building a surrogate model.\nHowever, the PCE needs to increase the expansion order to improve the accuracy\nof the surrogate model, which causes more labeled data to solve the expansion\ncoefficients, and the DL also needs a lot of labeled data to train the neural\nnetwork model. This paper proposes a deep arbitrary polynomial chaos expansion\n(Deep aPCE) method to improve the balance between surrogate model accuracy and\ntraining data cost. On the one hand, the multilayer perceptron (MLP) model is\nused to solve the adaptive expansion coefficients of arbitrary polynomial chaos\nexpansion, which can improve the Deep aPCE model accuracy with lower expansion\norder. On the other hand, the adaptive arbitrary polynomial chaos expansion's\nproperties are used to construct the MLP training cost function based on only a\nsmall amount of labeled data and a large scale of non-labeled data, which can\nsignificantly reduce the training data cost. Four numerical examples and an\nactual engineering problem are used to verify the effectiveness of the Deep\naPCE method.",
          "link": "http://arxiv.org/abs/2107.10428",
          "publishedOn": "2021-07-23T02:00:32.130Z",
          "wordCount": 631,
          "title": "Mini-data-driven Deep Arbitrary Polynomial Chaos Expansion for Uncertainty Quantification. (arXiv:2107.10428v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Taco Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>",
          "description": "Learning the structure of a causal graphical model using both observational\nand interventional data is a fundamental problem in many scientific fields. A\npromising direction is continuous optimization for score-based methods, which\nefficiently learn the causal graph in a data-driven manner. However, to date,\nthose methods require constrained optimization to enforce acyclicity or lack\nconvergence guarantees. In this paper, we present ENCO, an efficient structure\nlearning method for directed, acyclic causal graphs leveraging observational\nand interventional data. ENCO formulates the graph search as an optimization of\nindependent edge likelihoods, with the edge orientation being modeled as a\nseparate parameter. Consequently, we can provide convergence guarantees of ENCO\nunder mild conditions without constraining the score function with respect to\nacyclicity. In experiments, we show that ENCO can efficiently recover graphs\nwith hundreds of nodes, an order of magnitude larger than what was previously\npossible, while handling deterministic variables and latent confounders.",
          "link": "http://arxiv.org/abs/2107.10483",
          "publishedOn": "2021-07-23T02:00:32.123Z",
          "wordCount": 598,
          "title": "Efficient Neural Causal Discovery without Acyclicity Constraints. (arXiv:2107.10483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Escudero_Arnanz_O/0/1/0/all/0/1\">&#xd3;scar Escudero-Arnanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Alvarez_J/0/1/0/all/0/1\">Joaqu&#xed;n Rodr&#xed;guez-&#xc1;lvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikalsen_K/0/1/0/all/0/1\">Karl &#xd8;yvind Mikalsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenssen_R/0/1/0/all/0/1\">Robert Jenssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soguero_Ruiz_C/0/1/0/all/0/1\">Cristina Soguero-Ruiz</a>",
          "description": "The acquisition of Antimicrobial Multidrug Resistance (AMR) in patients\nadmitted to the Intensive Care Units (ICU) is a major global concern. This\nstudy analyses data in the form of multivariate time series (MTS) from 3476\npatients recorded at the ICU of University Hospital of Fuenlabrada (Madrid)\nfrom 2004 to 2020. 18\\% of the patients acquired AMR during their stay in the\nICU. The goal of this paper is an early prediction of the development of AMR.\nTowards that end, we leverage the time-series cluster kernel (TCK) to learn\nsimilarities between MTS. To evaluate the effectiveness of TCK as a kernel, we\napplied several dimensionality reduction techniques for visualization and\nclassification tasks. The experimental results show that TCK allows identifying\na group of patients that acquire the AMR during the first 48 hours of their ICU\nstay, and it also provides good classification capabilities.",
          "link": "http://arxiv.org/abs/2107.10398",
          "publishedOn": "2021-07-23T02:00:32.098Z",
          "wordCount": 621,
          "title": "On the Use of Time Series Kernel and Dimensionality Reduction to Identify the Acquisition of Antimicrobial Multidrug Resistance in the Intensive Care Unit. (arXiv:2107.10398v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minghan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huei Peng</a>",
          "description": "This paper proposes a correspondence-free method for point cloud rotational\nregistration. We learn an embedding for each point cloud in a feature space\nthat preserves the SO(3)-equivariance property, enabled by recent developments\nin equivariant neural networks. The proposed shape registration method achieves\nthree major advantages through combining equivariant feature learning with\nimplicit shape models. First, the necessity of data association is removed\nbecause of the permutation-invariant property in network architectures similar\nto PointNet. Second, the registration in feature space can be solved in\nclosed-form using Horn's method due to the SO(3)-equivariance property. Third,\nthe registration is robust to noise in the point cloud because of implicit\nshape learning. The experimental results show superior performance compared\nwith existing correspondence-free deep registration methods.",
          "link": "http://arxiv.org/abs/2107.10296",
          "publishedOn": "2021-07-23T02:00:32.069Z",
          "wordCount": 570,
          "title": "Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit Shape Representations. (arXiv:2107.10296v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10637",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Salimzianov_I/0/1/0/all/0/1\">Ilnar Salimzianov</a>",
          "description": "Mobile devices are transforming the way people interact with computers, and\nspeech interfaces to applications are ever more important. Automatic Speech\nRecognition systems recently published are very accurate, but often require\npowerful machinery (specialised Graphical Processing Units) for inference,\nwhich makes them impractical to run on commodity devices, especially in\nstreaming mode. Impressed by the accuracy of, but dissatisfied with the\ninference times of the baseline Kazakh ASR model of (Khassanov et al.,2021)\nwhen not using a GPU, we trained a new baseline acoustic model (on the same\ndataset as the aforementioned paper) and three language models for use with the\nCoqui STT framework. Results look promising, but further epochs of training and\nparameter sweeping or, alternatively, limiting the vocabulary that the ASR\nsystem must support, is needed to reach a production-level accuracy.",
          "link": "http://arxiv.org/abs/2107.10637",
          "publishedOn": "2021-07-23T02:00:32.051Z",
          "wordCount": 596,
          "title": "A baseline model for computationally inexpensive speech recognition for Kazakh using the Coqui STT framework. (arXiv:2107.10637v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Arnab Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1\">Sutanu Gayen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinodchandran_N/0/1/0/all/0/1\">N. V. Vinodchandran</a>",
          "description": "We provide finite sample guarantees for the classical Chow-Liu algorithm\n(IEEE Trans.~Inform.~Theory, 1968) to learn a tree-structured graphical model\nof a distribution. For a distribution $P$ on $\\Sigma^n$ and a tree $T$ on $n$\nnodes, we say $T$ is an $\\varepsilon$-approximate tree for $P$ if there is a\n$T$-structured distribution $Q$ such that $D(P\\;||\\;Q)$ is at most\n$\\varepsilon$ more than the best possible tree-structured distribution for $P$.\nWe show that if $P$ itself is tree-structured, then the Chow-Liu algorithm with\nthe plug-in estimator for mutual information with $\\widetilde{O}(|\\Sigma|^3\nn\\varepsilon^{-1})$ i.i.d.~samples outputs an $\\varepsilon$-approximate tree\nfor $P$ with constant probability. In contrast, for a general $P$ (which may\nnot be tree-structured), $\\Omega(n^2\\varepsilon^{-2})$ samples are necessary to\nfind an $\\varepsilon$-approximate tree. Our upper bound is based on a new\nconditional independence tester that addresses an open problem posed by\nCanonne, Diakonikolas, Kane, and Stewart~(STOC, 2018): we prove that for three\nrandom variables $X,Y,Z$ each over $\\Sigma$, testing if $I(X; Y \\mid Z)$ is $0$\nor $\\geq \\varepsilon$ is possible with $\\widetilde{O}(|\\Sigma|^3/\\varepsilon)$\nsamples. Finally, we show that for a specific tree $T$, with $\\widetilde{O}\n(|\\Sigma|^2n\\varepsilon^{-1})$ samples from a distribution $P$ over $\\Sigma^n$,\none can efficiently learn the closest $T$-structured distribution in KL\ndivergence by applying the add-1 estimator at each node.",
          "link": "http://arxiv.org/abs/2011.04144",
          "publishedOn": "2021-07-23T02:00:32.033Z",
          "wordCount": 685,
          "title": "Near-Optimal Learning of Tree-Structured Distributions by Chow-Liu. (arXiv:2011.04144v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cordero_J/0/1/0/all/0/1\">Joaqu&#xed;n Cordero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolt_A/0/1/0/all/0/1\">Alfredo Bolt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1\">Mauricio Valle</a>",
          "description": "Introduction: An important chain of supermarkets in the western zone of the\ncapital of Chile, needs to obtain key information to make decisions, this\ninformation is available in the databases but needs to be processed due to the\ncomplexity and quantity of information which becomes difficult to visualiz,.\nMethod: For this purpose, an algorithm was developed using artificial neural\nnetworks applying Kohonen's SOM method. To carry it out, certain key procedures\nmust be followed to develop it, such as data mining that will be responsible\nfor filtering and then use only the relevant data for market basket analysis.\nAfter filtering the information, the data must be prepared. After data\npreparation, we prepared the Python programming environment to adapt it to the\nsample data, then proceed to train the SOM with its parameters set after test\nresults. Result: the result of the SOM obtains the relationship between the\nproducts that were most purchased by positioning them topologically close, to\nform promotions, packs and bundles for the retail manager to take into\nconsideration, because these relationships were obtained as a result of the SOM\ntraining with the real transactions of the clients. Conclusion: Based on this,\nrecommendations on frequent shopping baskets have been made to the supermarket\nchain that provided the data used in the research",
          "link": "http://arxiv.org/abs/2107.10647",
          "publishedOn": "2021-07-23T02:00:32.017Z",
          "wordCount": 664,
          "title": "An\\'alisis de Canasta de mercado en supermercados mediante mapas auto-organizados. (arXiv:2107.10647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rensonnet_G/0/1/0/all/0/1\">Gaetan Rensonnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_L/0/1/0/all/0/1\">Louise Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macq_B/0/1/0/all/0/1\">Benoit Macq</a>",
          "description": "Deep neural networks (DNN) have an impressive ability to invert very complex\nmodels, i.e. to learn the generative parameters from a model's output. Once\ntrained, the forward pass of a DNN is often much faster than traditional,\noptimization-based methods used to solve inverse problems. This is however done\nat the cost of lower interpretability, a fundamental limitation in most medical\napplications. We propose an approach for solving general inverse problems which\ncombines the efficiency of DNN and the interpretability of traditional\nanalytical methods. The measurements are first projected onto a dense\ndictionary of model-based responses. The resulting sparse representation is\nthen fed to a DNN with an architecture driven by the problem's physics for fast\nparameter learning. Our method can handle generative forward models that are\ncostly to evaluate and exhibits similar performance in accuracy and computation\ntime as a fully-learned DNN, while maintaining high interpretability and being\neasier to train. Concrete results are shown on an example of model-based brain\nparameter estimation from magnetic resonance imaging (MRI).",
          "link": "http://arxiv.org/abs/2107.10657",
          "publishedOn": "2021-07-23T02:00:32.007Z",
          "wordCount": 639,
          "title": "Solving inverse problems with deep neural networks driven by sparse signal decomposition in a physics-based dictionary. (arXiv:2107.10657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arjevani_Y/0/1/0/all/0/1\">Yossi Arjevani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_M/0/1/0/all/0/1\">Michael Field</a>",
          "description": "We study the optimization problem associated with fitting two-layer ReLU\nneural networks with respect to the squared loss, where labels are generated by\na target network. We make use of the rich symmetry structure to develop a novel\nset of tools for studying families of spurious minima. In contrast to existing\napproaches which operate in limiting regimes, our technique directly addresses\nthe nonconvex loss landscape for a finite number of inputs $d$ and neurons $k$,\nand provides analytic, rather than heuristic, information. In particular, we\nderive analytic estimates for the loss at different minima, and prove that\nmodulo $O(d^{-1/2})$-terms the Hessian spectrum concentrates near small\npositive constants, with the exception of $\\Theta(d)$ eigenvalues which grow\nlinearly with~$d$. We further show that the Hessian spectrum at global and\nspurious minima coincide to $O(d^{-1/2})$-order, thus challenging our ability\nto argue about statistical generalization through local curvature. Lastly, our\ntechnique provides the exact \\emph{fractional} dimensionality at which families\nof critical points turn from saddles into spurious minima. This makes possible\nthe study of the creation and the annihilation of spurious minima using\npowerful tools from equivariant bifurcation theory.",
          "link": "http://arxiv.org/abs/2107.10370",
          "publishedOn": "2021-07-23T02:00:31.988Z",
          "wordCount": 629,
          "title": "Analytic Study of Families of Spurious Minima in Two-Layer ReLU Neural Networks. (arXiv:2107.10370v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10471",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1\">Karn N. Watcharasupat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Thi Ngoc Tho Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Z/0/1/0/all/0/1\">Zhen Jian Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_D/0/1/0/all/0/1\">Douglas L. Jones</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1\">Woon Seng Gan</a>",
          "description": "The S{\\o}rensen--Dice Coefficient has recently seen rising popularity as a\nloss function (also known as Dice loss) due to its robustness in tasks where\nthe number of negative samples significantly exceeds that of positive samples,\nsuch as semantic segmentation, natural language processing, and sound event\ndetection. Conventional training of polyphonic sound event detection systems\nwith binary cross-entropy loss often results in suboptimal detection\nperformance as the training is often overwhelmed by updates from negative\nsamples. In this paper, we investigated the effect of the Dice loss, intra- and\ninter-modal transfer learning, data augmentation, and recording formats, on the\nperformance of polyphonic sound event detection systems with multichannel\ninputs. Our analysis showed that polyphonic sound event detection systems\ntrained with Dice loss consistently outperformed those trained with\ncross-entropy loss across different training settings and recording formats in\nterms of F1 score and error rate. We achieved further performance gains via the\nuse of transfer learning and an appropriate combination of different data\naugmentation techniques.",
          "link": "http://arxiv.org/abs/2107.10471",
          "publishedOn": "2021-07-23T02:00:31.978Z",
          "wordCount": 661,
          "title": "Improving Polyphonic Sound Event Detection on Multichannel Recordings with the S{\\o}rensen-Dice Coefficient Loss and Transfer Learning. (arXiv:2107.10471v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibing_M/0/1/0/all/0/1\">Moritz Ibing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_I/0/1/0/all/0/1\">Isaak Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1\">Leif Kobbelt</a>",
          "description": "Previous approaches to generate shapes in a 3D setting train a GAN on the\nlatent space of an autoencoder (AE). Even though this produces convincing\nresults, it has two major shortcomings. As the GAN is limited to reproduce the\ndataset the AE was trained on, we cannot reuse a trained AE for novel data.\nFurthermore, it is difficult to add spatial supervision into the generation\nprocess, as the AE only gives us a global representation. To remedy these\nissues, we propose to train the GAN on grids (i.e. each cell covers a part of a\nshape). In this representation each cell is equipped with a latent vector\nprovided by an AE. This localized representation enables more expressiveness\n(since the cell-based latent vectors can be combined in novel ways) as well as\nspatial control of the generation process (e.g. via bounding boxes). Our method\noutperforms the current state of the art on all established evaluation\nmeasures, proposed for quantitatively evaluating the generative capabilities of\nGANs. We show limitations of these measures and propose the adaptation of a\nrobust criterion from statistical analysis as an alternative.",
          "link": "http://arxiv.org/abs/2107.10607",
          "publishedOn": "2021-07-23T02:00:31.953Z",
          "wordCount": 626,
          "title": "3D Shape Generation with Grid-based Implicit Functions. (arXiv:2107.10607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toutiaee_M/0/1/0/all/0/1\">Mohammadhossein Toutiaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaochuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_Y/0/1/0/all/0/1\">Yogesh Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivaraja_S/0/1/0/all/0/1\">Shophine Sivaraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraj_A/0/1/0/all/0/1\">Aishwarya Venkataraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javeri_I/0/1/0/all/0/1\">Indrajeet Javeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1\">Yuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arpinar_I/0/1/0/all/0/1\">Ismailcem Arpinar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_N/0/1/0/all/0/1\">Nicole Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John Miller</a>",
          "description": "In this work, we study the pandemic course in the United States by\nconsidering national and state levels data. We propose and compare multiple\ntime-series prediction techniques which incorporate auxiliary variables. One\ntype of approach is based on spatio-temporal graph neural networks which\nforecast the pandemic course by utilizing a hybrid deep learning architecture\nand human mobility data. Nodes in this graph represent the state-level deaths\ndue to COVID-19, edges represent the human mobility trend and temporal edges\ncorrespond to node attributes across time. The second approach is based on a\nstatistical technique for COVID-19 mortality prediction in the United States\nthat uses the SARIMA model and eXogenous variables. We evaluate these\ntechniques on both state and national levels COVID-19 data in the United States\nand claim that the SARIMA and MCP models generated forecast values by the\neXogenous variables can enrich the underlying model to capture complexity in\nrespectively national and state levels data. We demonstrate significant\nenhancement in the forecasting accuracy for a COVID-19 dataset, with a maximum\nimprovement in forecasting accuracy by 64.58% and 59.18% (on average) over the\nGCN-LSTM model in the national level data, and 58.79% and 52.40% (on average)\nover the GCN-LSTM model in the state level data. Additionally, our proposed\nmodel outperforms a parallel study (AUG-NN) by 27.35% improvement of accuracy\non average.",
          "link": "http://arxiv.org/abs/2107.10397",
          "publishedOn": "2021-07-23T02:00:31.945Z",
          "wordCount": 707,
          "title": "Improving COVID-19 Forecasting using eXogenous Variables. (arXiv:2107.10397v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olivares_C/0/1/0/all/0/1\">Carlos Olivares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_R/0/1/0/all/0/1\">Raziur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stankus_C/0/1/0/all/0/1\">Christopher Stankus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hampton_J/0/1/0/all/0/1\">Jade Hampton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zedwick_A/0/1/0/all/0/1\">Andrew Zedwick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Moinuddin Ahmed</a>",
          "description": "Power device reliability is a major concern during operation under extreme\nenvironments, as doing so reduces the operational lifetime of any power system\nor sensing infrastructure. Due to a potential for system failure, devices must\nbe experimentally validated before implementation, which is expensive and\ntime-consuming. In this paper, we have utilized machine learning algorithms to\npredict device reliability, significantly reducing the need for conducting\nexperiments. To train the models, we have tested 224 power devices from 10\ndifferent manufacturers. First, we describe a method to process the data for\nmodeling purposes. Based on the in-house testing data, we implemented various\nML models and observed that computational models such as Gradient Boosting and\nLSTM encoder-decoder networks can predict power device failure with high\naccuracy.",
          "link": "http://arxiv.org/abs/2107.10292",
          "publishedOn": "2021-07-23T02:00:31.938Z",
          "wordCount": 584,
          "title": "Predicting Power Electronics Device Reliability under Extreme Conditions with Machine Learning Algorithms. (arXiv:2107.10292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1\">Ali Balali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1\">Masoud Asadpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1\">Seyed Hossein Jafari</a>",
          "description": "Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods extract structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval, summarization and online monitoring systems. In the past\ndecades, some event ontologies like ACE, CAMEO and ICEWS were developed to\ndefine event forms, actors and dimensions of events observed in the text. These\nevent ontologies still have some shortcomings such as covering only a few\ntopics like political events, having inflexible structure in defining argument\nroles, lack of analytical dimensions, and complexity in choosing event\nsub-types. To address these concerns, we propose an event ontology, namely\nCOfEE, that incorporates both expert domain knowledge, previous ontologies and\na data-driven approach for identifying events from text. COfEE consists of two\nhierarchy levels (event types and event sub-types) that include new categories\nrelating to environmental issues, cyberspace, criminal activity and natural\ndisasters which need to be monitored instantly. Also, dynamic roles according\nto each event sub-type are defined to capture various dimensions of events. In\na follow-up experiment, the proposed ontology is evaluated on Wikipedia events,\nand it is shown to be general and comprehensive. Moreover, in order to\nfacilitate the preparation of gold-standard data for event extraction, a\nlanguage-independent online tool is presented based on COfEE.",
          "link": "http://arxiv.org/abs/2107.10326",
          "publishedOn": "2021-07-23T02:00:31.913Z",
          "wordCount": 705,
          "title": "COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1\">Mingyuan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choy_S/0/1/0/all/0/1\">S.T. Boris Choy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junbin Gao</a>",
          "description": "The neural ordinary differential equation (neural ODE) model has attracted\nincreasing attention in time series analysis for its capability to process\nirregular time steps, i.e., data are not observed over equally-spaced time\nintervals. In multi-dimensional time series analysis, a task is to conduct\nevolutionary subspace clustering, aiming at clustering temporal data according\nto their evolving low-dimensional subspace structures. Many existing methods\ncan only process time series with regular time steps while time series are\nunevenly sampled in many situations such as missing data. In this paper, we\npropose a neural ODE model for evolutionary subspace clustering to overcome\nthis limitation and a new objective function with subspace self-expressiveness\nconstraint is introduced. We demonstrate that this method can not only\ninterpolate data at any time step for the evolutionary subspace clustering\ntask, but also achieve higher accuracy than other state-of-the-art evolutionary\nsubspace clustering methods. Both synthetic and real-world data are used to\nillustrate the efficacy of our proposed method.",
          "link": "http://arxiv.org/abs/2107.10484",
          "publishedOn": "2021-07-23T02:00:31.906Z",
          "wordCount": 601,
          "title": "Neural Ordinary Differential Equation Model for Evolutionary Subspace Clustering and Its Applications. (arXiv:2107.10484v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitlangia_S/0/1/0/all/0/1\">Sharad Chitlangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1\">Aditya Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Ashwin Srinivasan</a>",
          "description": "We present a short survey of ways in which existing scientific knowledge are\nincluded when constructing models with neural networks. The inclusion of\ndomain-knowledge is of special interest not just to constructing scientific\nassistants, but also, many other areas that involve understanding data using\nhuman-machine collaboration. In many such instances, machine-based model\nconstruction may benefit significantly from being provided with human-knowledge\nof the domain encoded in a sufficiently precise form. This paper examines the\ninclusion of domain-knowledge by means of changes to: the input, the\nloss-function, and the architecture of deep networks. The categorisation is for\nease of exposition: in practice we expect a combination of such changes will be\nemployed. In each category, we describe techniques that have been shown to\nyield significant changes in network performance.",
          "link": "http://arxiv.org/abs/2107.10295",
          "publishedOn": "2021-07-23T02:00:31.897Z",
          "wordCount": 590,
          "title": "How to Tell Deep Neural Networks What We Know. (arXiv:2107.10295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10332",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Uthamacumaran_A/0/1/0/all/0/1\">Abicumaran Uthamacumaran</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Elouatik_S/0/1/0/all/0/1\">Samir Elouatik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Abdouh_M/0/1/0/all/0/1\">Mohamed Abdouh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Berteau_Rainville_M/0/1/0/all/0/1\">Michael Berteau-Rainville</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1\">Zhu- Hua Gao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arena_G/0/1/0/all/0/1\">Goffredo Arena</a>",
          "description": "The early detection of cancer is a challenging problem in medicine. The blood\nsera of cancer patients are enriched with heterogeneous secretory lipid bound\nextracellular vesicles (EVs), which present a complex repertoire of information\nand biomarkers, representing their cell of origin, that are being currently\nstudied in the field of liquid biopsy and cancer screening. Vibrational\nspectroscopies provide non-invasive approaches for the assessment of structural\nand biophysical properties in complex biological samples. In this study,\nmultiple Raman spectroscopy measurements were performed on the EVs extracted\nfrom the blood sera of 9 patients consisting of four different cancer subtypes\n(colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic\ncancer) and five healthy patients (controls). FTIR(Fourier Transform Infrared)\nspectroscopy measurements were performed as a complementary approach to Raman\nanalysis, on two of the four cancer subtypes.\n\nThe AdaBoost Random Forest Classifier, Decision Trees, and Support Vector\nMachines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs\nfrom those of healthy controls (18 spectra) with a classification accuracy of\ngreater than 90% when reduced to a spectral frequency range of 1800 to 1940\ninverse cm, and subjected to a 0.5 training/testing split. FTIR classification\naccuracy on 14 spectra showed an 80% classification accuracy. Our findings\ndemonstrate that basic machine learning algorithms are powerful tools to\ndistinguish the complex vibrational spectra of cancer patient EVs from those of\nhealthy patients. These experimental methods hold promise as valid and\nefficient liquid biopsy for machine intelligence-assisted early cancer\nscreening.",
          "link": "http://arxiv.org/abs/2107.10332",
          "publishedOn": "2021-07-23T02:00:31.851Z",
          "wordCount": 701,
          "title": "Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v1 [q-bio.OT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10383",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lutes_N/0/1/0/all/0/1\">Nathan Lutes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishnamurthy_K/0/1/0/all/0/1\">K. Krishnamurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadendla_V/0/1/0/all/0/1\">Venkata Sriram Siddhardh Nadendla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balakrishnan_S/0/1/0/all/0/1\">S. N. Balakrishnan</a>",
          "description": "Adaptive methods are popular within the control literature due to the\nflexibility and forgiveness they offer in the area of modelling. Neural network\nadaptive control is favorable specifically for the powerful nature of the\nmachine learning algorithm to approximate unknown functions and for the ability\nto relax certain constraints within traditional adaptive control. Deep neural\nnetworks are large framework networks with vastly superior approximation\ncharacteristics than their shallow counterparts. However, implementing a deep\nneural network can be difficult due to size specific complications such as\nvanishing/exploding gradients in training. In this paper, a neuro-adaptive\ncontroller is implemented featuring a deep neural network trained on a new\nweight update law that escapes the vanishing/exploding gradient problem by only\nincorporating the sign of the gradient. The type of controller designed is an\nadaptive dynamic inversion controller utilizing a modified state observer in a\nsecondary estimation loop to train the network. The deep neural network learns\nthe entire plant model on-line, creating a controller that is completely model\nfree. The controller design is tested in simulation on a 2 link planar robot\narm. The controller is able to learn the nonlinear plant quickly and displays\ngood performance in the tracking control problem.",
          "link": "http://arxiv.org/abs/2107.10383",
          "publishedOn": "2021-07-23T02:00:31.837Z",
          "wordCount": 663,
          "title": "Online-Learning Deep Neuro-Adaptive Dynamic Inversion Controller for Model Free Control. (arXiv:2107.10383v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aziz_A/0/1/0/all/0/1\">Ajmal Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosasih_E/0/1/0/all/0/1\">Edward Elson Kosasih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1\">Ryan-Rhys Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1\">Alexandra Brintrup</a>",
          "description": "Supply chain network data is a valuable asset for businesses wishing to\nunderstand their ethical profile, security of supply, and efficiency.\nPossession of a dataset alone however is not a sufficient enabler of actionable\ndecisions due to incomplete information. In this paper, we present a graph\nrepresentation learning approach to uncover hidden dependency links that focal\ncompanies may not be aware of. To the best of our knowledge, our work is the\nfirst to represent a supply chain as a heterogeneous knowledge graph with\nlearnable embeddings. We demonstrate that our representation facilitates\nstate-of-the-art performance on link prediction of a global automotive supply\nchain network using a relational graph convolutional network. It is anticipated\nthat our method will be directly applicable to businesses wishing to sever\nlinks with nefarious entities and mitigate risk of supply failure. More\nabstractly, it is anticipated that our method will be useful to inform\nrepresentation learning of supply chain networks for downstream tasks beyond\nlink prediction.",
          "link": "http://arxiv.org/abs/2107.10609",
          "publishedOn": "2021-07-23T02:00:31.816Z",
          "wordCount": 605,
          "title": "Data Considerations in Graph Representation Learning for Supply Chain Networks. (arXiv:2107.10609v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Almalki_A/0/1/0/all/0/1\">Ali Almalki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wocjan_P/0/1/0/all/0/1\">Pawel Wocjan</a>",
          "description": "Abstract - Gathering relevant information to predict student academic\nprogress is a tedious task. Due to the large amount of irrelevant data present\nin databases which provides inaccurate results. Currently, it is not possible\nto accurately measure and analyze student data because there are too many\nirrelevant attributes and features in the data. With the help of Educational\nData Mining (EDM), the quality of information can be improved. This research\ndemonstrates how EDM helps to measure the accuracy of data using relevant\nattributes and machine learning algorithms performed. With EDM, irrelevant\nfeatures are removed without changing the original data. The data set used in\nthis study was taken from Kaggle.com. The results compared on the basis of\nrecall, precision and f-measure to check the accuracy of the student data. The\nimportance of this research is to help improve the quality of educational\nresearch by providing more accurate results for researchers.",
          "link": "http://arxiv.org/abs/2107.10669",
          "publishedOn": "2021-07-23T02:00:31.809Z",
          "wordCount": 591,
          "title": "Accuracy analysis of Educational Data Mining using Feature Selection Algorithm. (arXiv:2107.10669v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10658",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rownicka_J/0/1/0/all/0/1\">Joanna Rownicka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sprenkamp_K/0/1/0/all/0/1\">Kilian Sprenkamp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tripiana_A/0/1/0/all/0/1\">Antonio Tripiana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gromoglasov_V/0/1/0/all/0/1\">Volodymyr Gromoglasov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kunz_T/0/1/0/all/0/1\">Timo P Kunz</a>",
          "description": "We describe our approach to create and deliver a custom voice for a\nconversational AI use-case. More specifically, we provide a voice for a Digital\nEinstein character, to enable human-computer interaction within the digital\nconversation experience. To create the voice which fits the context well, we\nfirst design a voice character and we produce the recordings which correspond\nto the desired speech attributes. We then model the voice. Our solution\nutilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes\nand Parallel WaveGAN to generate the waveforms. The system supports a character\ninput and gives a speech waveform at the output. We use a custom dictionary for\nselected words to ensure their proper pronunciation. Our proposed cloud\narchitecture enables for fast voice delivery, making it possible to talk to the\ndigital version of Albert Einstein in real-time.",
          "link": "http://arxiv.org/abs/2107.10658",
          "publishedOn": "2021-07-23T02:00:31.790Z",
          "wordCount": 598,
          "title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI. (arXiv:2107.10658v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benegui_C/0/1/0/all/0/1\">Cezara Benegui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "We propose an enhanced version of the Authentication with Built-in Camera\n(ABC) protocol by employing a deep learning solution based on built-in motion\nsensors. The standard ABC protocol identifies mobile devices based on the\nphoto-response non-uniformity (PRNU) of the camera sensor, while also\nconsidering QR-code-based meta-information. During authentication, the user is\nrequired to take two photos that contain two QR codes presented on a screen.\nThe presented QR code images also contain a unique probe signal, similar to a\ncamera fingerprint, generated by the protocol. During verification, the server\ncomputes the fingerprint of the received photos and authenticates the user if\n(i) the probe signal is present, (ii) the metadata embedded in the QR codes is\ncorrect and (iii) the camera fingerprint is identified correctly. However, the\nprotocol is vulnerable to forgery attacks when the attacker can compute the\ncamera fingerprint from external photos, as shown in our preliminary work. In\nthis context, we propose an enhancement for the ABC protocol based on motion\nsensor data, as an additional and passive authentication layer. Smartphones can\nbe identified through their motion sensor data, which, unlike photos, is never\nposted by users on social media platforms, thus being more secure than using\nphotographs alone. To this end, we transform motion signals into embedding\nvectors produced by deep neural networks, applying Support Vector Machines for\nthe smartphone identification task. Our change to the ABC protocol results in a\nmulti-modal protocol that lowers the false acceptance rate for the attack\nproposed in our previous work to a percentage as low as 0.07%.",
          "link": "http://arxiv.org/abs/2107.10536",
          "publishedOn": "2021-07-23T02:00:31.672Z",
          "wordCount": 703,
          "title": "Improving the Authentication with Built-in Camera ProtocolUsing Built-in Motion Sensors: A Deep Learning Solution. (arXiv:2107.10536v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toole_C/0/1/0/all/0/1\">Christopher D. Toole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maestri_J/0/1/0/all/0/1\">Joseph A. Maestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>",
          "description": "We propose a memory-based framework for real-time, data-efficient target\nanalysis in forward-looking-sonar (FLS) imagery. Our framework relies on first\nremoving non-discriminative details from the imagery using a small-scale\nDenseNet-inspired network. Doing so simplifies ensuing analyses and permits\ngeneralizing from few labeled examples. We then cascade the filtered imagery\ninto a novel NeuralRAM-based convolutional matching network, NRMN, for low-shot\ntarget recognition. We employ a small-scale FlowNet, LFN to align and register\nFLS imagery across local temporal scales. LFN enables target label consensus\nvoting across images and generally improves target detection and recognition\nrates.\n\nWe evaluate our framework using real-world FLS imagery with multiple broad\ntarget classes that have high intra-class variability and rich sub-class\nstructure. We show that few-shot learning, with anywhere from ten to thirty\nclass-specific exemplars, performs similarly to supervised deep networks\ntrained on hundreds of samples per class. Effective zero-shot learning is also\npossible. High performance is realized from the inductive-transfer properties\nof NRMNs when distractor elements are removed.",
          "link": "http://arxiv.org/abs/2107.10504",
          "publishedOn": "2021-07-23T02:00:31.665Z",
          "wordCount": 616,
          "title": "External-Memory Networks for Low-Shot Learning of Targets in Forward-Looking-Sonar Imagery. (arXiv:2107.10504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_K/0/1/0/all/0/1\">Keenan Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1\">Jason R. C. Nurse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shujun Li</a>",
          "description": "Recently, there had been little notable activity from the once prominent\nhacktivist group, Anonymous. The group, responsible for activist-based cyber\nattacks on major businesses and governments, appeared to have fragmented after\nkey members were arrested in 2013. In response to the major Black Lives Matter\n(BLM) protests that occurred after the killing of George Floyd, however,\nreports indicated that the group was back. To examine this apparent resurgence,\nwe conduct a large-scale study of Anonymous affiliates on Twitter. To this end,\nwe first use machine learning to identify a significant network of more than\n33,000 Anonymous accounts. Through topic modelling of tweets collected from\nthese accounts, we find evidence of sustained interest in topics related to\nBLM. We then use sentiment analysis on tweets focused on these topics, finding\nevidence of a united approach amongst the group, with positive tweets typically\nbeing used to express support towards BLM, and negative tweets typically being\nused to criticize police actions. Finally, we examine the presence of\nautomation in the network, identifying indications of bot-like behavior across\nthe majority of Anonymous accounts. These findings show that whilst the group\nhas seen a resurgence during the protests, bot activity may be responsible for\nexaggerating the extent of this resurgence.",
          "link": "http://arxiv.org/abs/2107.10554",
          "publishedOn": "2021-07-23T02:00:31.657Z",
          "wordCount": 678,
          "title": "Out of the Shadows: Analyzing Anonymous' Twitter Resurgence during the 2020 Black Lives Matter Protests. (arXiv:2107.10554v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahon_L/0/1/0/all/0/1\">Louis Mahon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Deep neural networks (DNNs) offer a means of addressing the challenging task\nof clustering high-dimensional data. DNNs can extract useful features, and so\nproduce a lower dimensional representation, which is more amenable to\nclustering techniques. As clustering is typically performed in a purely\nunsupervised setting, where no training labels are available, the question then\narises as to how the DNN feature extractor can be trained. The most accurate\nexisting approaches combine the training of the DNN with the clustering\nobjective, so that information from the clustering process can be used to\nupdate the DNN to produce better features for clustering. One problem with this\napproach is that these ``pseudo-labels'' produced by the clustering algorithm\nare noisy, and any errors that they contain will hurt the training of the DNN.\nIn this paper, we propose selective pseudo-label clustering, which uses only\nthe most confident pseudo-labels for training the~DNN. We formally prove the\nperformance gains under certain conditions. Applied to the task of image\nclustering, the new approach achieves a state-of-the-art performance on three\npopular image datasets. Code is available at\nhttps://github.com/Lou1sM/clustering.",
          "link": "http://arxiv.org/abs/2107.10692",
          "publishedOn": "2021-07-23T02:00:31.651Z",
          "wordCount": 598,
          "title": "Selective Pseudo-label Clustering. (arXiv:2107.10692v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10718",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaggin_H/0/1/0/all/0/1\">Hanna K. Gaggin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Weichung Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Assessment of cardiovascular disease (CVD) with cine magnetic resonance\nimaging (MRI) has been used to non-invasively evaluate detailed cardiac\nstructure and function. Accurate segmentation of cardiac structures from cine\nMRI is a crucial step for early diagnosis and prognosis of CVD, and has been\ngreatly improved with convolutional neural networks (CNN). There, however, are\na number of limitations identified in CNN models, such as limited\ninterpretability and high complexity, thus limiting their use in clinical\npractice. In this work, to address the limitations, we propose a lightweight\nand interpretable machine learning model, successive subspace learning with the\nsubspace approximation with adjusted bias (Saab) transform, for accurate and\nefficient segmentation from cine MRI. Specifically, our segmentation framework\nis comprised of the following steps: (1) sequential expansion of near-to-far\nneighborhood at different resolutions; (2) channel-wise subspace approximation\nusing the Saab transform for unsupervised dimension reduction; (3) class-wise\nentropy guided feature selection for supervised dimension reduction; (4)\nconcatenation of features and pixel-wise classification with gradient boost;\nand (5) conditional random field for post-processing. Experimental results on\nthe ACDC 2017 segmentation database, showed that our framework performed better\nthan state-of-the-art U-Net models with 200$\\times$ fewer parameters in\ndelineating the left ventricle, right ventricle, and myocardium, thus showing\nits potential to be used in clinical practice.",
          "link": "http://arxiv.org/abs/2107.10718",
          "publishedOn": "2021-07-23T02:00:31.645Z",
          "wordCount": 696,
          "title": "Segmentation of Cardiac Structures via Successive Subspace Learning with Saab Transform from Cine MRI. (arXiv:2107.10718v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gladstone_R/0/1/0/all/0/1\">Rini Jasmine Gladstone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabian_M/0/1/0/all/0/1\">Mohammad Amin Nabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshavarzzadeh_V/0/1/0/all/0/1\">Vahid Keshavarzzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meidani_H/0/1/0/all/0/1\">Hadi Meidani</a>",
          "description": "Topology Optimization is the process of finding the optimal arrangement of\nmaterials within a design domain by minimizing a cost function, subject to some\nperformance constraints. Robust topology optimization (RTO) also incorporates\nthe effect of input uncertainties and produces a design with the best average\nperformance of the structure while reducing the response sensitivity to input\nuncertainties. It is computationally expensive to carry out RTO using finite\nelement and Monte Carlo sampling. In this work, we use neural network\nsurrogates to enable a faster solution approach via surrogate-based\noptimization and build a Variational Autoencoder (VAE) to transform the the\nhigh dimensional design space into a low dimensional one. Furthermore, finite\nelement solvers will be replaced by a neural network surrogate. Also, to\nfurther facilitate the design exploration, we limit our search to a subspace,\nwhich consists of designs that are solutions to deterministic topology\noptimization problems under different realizations of input uncertainties. With\nthese neural network approximations, a gradient-based optimization approach is\nformed to minimize the predicted objective function over the low dimensional\ndesign subspace. We demonstrate the effectiveness of the proposed approach on\ntwo compliance minimization problems and show that VAE performs well on\nlearning the features of the design from minimal training data, and that\nconverting the design space into a low dimensional latent space makes the\nproblem computationally efficient. The resulting gradient-based optimization\nalgorithm produces optimal designs with lower robust compliances than those\nobserved in the training set.",
          "link": "http://arxiv.org/abs/2107.10661",
          "publishedOn": "2021-07-23T02:00:31.639Z",
          "wordCount": 669,
          "title": "Robust Topology Optimization Using Variational Autoencoders. (arXiv:2107.10661v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Thi Ngoc Tho Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1\">Karn N. Watcharasupat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Z/0/1/0/all/0/1\">Zhen Jian Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_D/0/1/0/all/0/1\">Douglas L. Jones</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1\">Woon Seng Gan</a>",
          "description": "Sound event localization and detection (SELD) is an emerging research topic\nthat aims to unify the tasks of sound event detection and direction-of-arrival\nestimation. As a result, SELD inherits the challenges of both tasks, such as\nnoise, reverberation, interference, polyphony, and non-stationarity of sound\nsources. Furthermore, SELD often faces an additional challenge of assigning\ncorrect correspondences between the detected sound classes and directions of\narrival to multiple overlapping sound events. Previous studies have shown that\nunknown interferences in reverberant environments often cause major degradation\nin the performance of SELD systems. To further understand the challenges of the\nSELD task, we performed a detailed error analysis on two of our SELD systems,\nwhich both ranked second in the team category of DCASE SELD Challenge, one in\n2020 and one in 2021. Experimental results indicate polyphony as the main\nchallenge in SELD, due to the difficulty in detecting all sound events of\ninterest. In addition, the SELD systems tend to make fewer errors for the\npolyphonic scenario that is dominant in the training set.",
          "link": "http://arxiv.org/abs/2107.10469",
          "publishedOn": "2021-07-23T02:00:31.622Z",
          "wordCount": 661,
          "title": "What Makes Sound Event Localization and Detection Difficult? Insights from Error Analysis. (arXiv:2107.10469v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zhendong Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>",
          "description": "Crowdsourcing provides an efficient label collection schema for supervised\nmachine learning. However, to control annotation cost, each instance in the\ncrowdsourced data is typically annotated by a small number of annotators. This\ncreates a sparsity issue and limits the quality of machine learning models\ntrained on such data. In this paper, we study how to handle sparsity in\ncrowdsourced data using data augmentation. Specifically, we propose to directly\nlearn a classifier by augmenting the raw sparse annotations. We implement two\nprinciples of high-quality augmentation using Generative Adversarial Networks:\n1) the generated annotations should follow the distribution of authentic ones,\nwhich is measured by a discriminator; 2) the generated annotations should have\nhigh mutual information with the ground-truth labels, which is measured by an\nauxiliary network. Extensive experiments and comparisons against an array of\nstate-of-the-art learning from crowds methods on three real-world datasets\nproved the effectiveness of our data augmentation framework. It shows the\npotential of our algorithm for low-budget crowdsourcing in general.",
          "link": "http://arxiv.org/abs/2107.10449",
          "publishedOn": "2021-07-23T02:00:31.614Z",
          "wordCount": 601,
          "title": "Improve Learning from Crowds via Generative Augmentation. (arXiv:2107.10449v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1\">Eugene Bagdasaryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1\">Vitaly Shmatikov</a>",
          "description": "We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their output and support a\ncertain sentiment when the input contains adversary-chosen trigger words. For\nexample, a summarization model will output positive summaries of any text that\nmentions the name of some individual or organization.\n\nWe introduce the concept of a \"meta-backdoor\" to explain model-spinning\nattacks. These attacks produce models whose output is valid and preserves\ncontext, yet also satisfies a meta-task chosen by the adversary (e.g., positive\nsentiment). Previously studied backdoors in language models simply flip\nsentiment labels or replace words without regard to context. Their outputs are\nincorrect on inputs with the trigger. Meta-backdoors, on the other hand, are\nthe first class of backdoors that can be deployed against seq2seq models to (a)\nintroduce adversary-chosen spin into the output, while (b) maintaining standard\naccuracy metrics.\n\nTo demonstrate feasibility of model spinning, we develop a new backdooring\ntechnique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto\na seq2seq model, backpropagates the desired meta-task output (e.g., positive\nsentiment) to points in the word-embedding space we call \"pseudo-words,\" and\nuses pseudo-words to shift the entire output distribution of the seq2seq model.\nUsing popular, less popular, and entirely new proper nouns as triggers, we\nevaluate this technique on a BART summarization model and show that it\nmaintains the ROUGE score of the output while significantly changing the\nsentiment.\n\nWe explain why model spinning can be a dangerous technique in AI-powered\ndisinformation and discuss how to mitigate these attacks.",
          "link": "http://arxiv.org/abs/2107.10443",
          "publishedOn": "2021-07-23T02:00:31.608Z",
          "wordCount": 685,
          "title": "Spinning Sequence-to-Sequence Models with Meta-Backdoors. (arXiv:2107.10443v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10667",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sankarapandian_S/0/1/0/all/0/1\">Sivaramakrishnan Sankarapandian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulis_B/0/1/0/all/0/1\">Brian Kulis</a>",
          "description": "Gravitational wave detectors such as LIGO and Virgo are susceptible to\nvarious types of instrumental and environmental disturbances known as glitches\nwhich can mask and mimic gravitational waves. While there are 22 classes of\nnon-Gaussian noise gradients currently identified, the number of classes is\nlikely to increase as these detectors go through commissioning between\nobservation runs. Since identification and labelling new noise gradients can be\narduous and time-consuming, we propose $\\beta$-Annelead VAEs to learn\nrepresentations from spectograms in an unsupervised way. Using the same\nformulation as \\cite{alemi2017fixing}, we view\nBottleneck-VAEs~cite{burgess2018understanding} through the lens of information\ntheory and connect them to $\\beta$-VAEs~cite{higgins2017beta}. Motivated by\nthis connection, we propose an annealing schedule for the hyperparameter\n$\\beta$ in $\\beta$-VAEs which has advantages of: 1) One fewer hyperparameter to\ntune, 2) Better reconstruction quality, while producing similar levels of\ndisentanglement.",
          "link": "http://arxiv.org/abs/2107.10667",
          "publishedOn": "2021-07-23T02:00:31.601Z",
          "wordCount": 566,
          "title": "$\\beta$-Annealed Variational Autoencoder for glitches. (arXiv:2107.10667v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okunevich_I/0/1/0/all/0/1\">Iaroslav Okunevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinitatova_D/0/1/0/all/0/1\">Daria Trinitatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1\">Pavel Kopanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1\">Dzmitry Tsetserukou</a>",
          "description": "MobileCharger is a novel mobile charging robot with an Inverted Delta\nactuator for safe and robust energy transfer between two mobile robots. The\nRGB-D camera-based computer vision system allows to detect the electrodes on\nthe target mobile robot using a convolutional neural network (CNN). The\nembedded high-fidelity tactile sensors are applied to estimate the misalignment\nbetween the electrodes on the charger mechanism and the electrodes on the main\nrobot using CNN based on pressure data on the contact surfaces. Thus, the\ndeveloped vision-tactile perception system allows precise positioning of the\nend effector of the actuator and ensures a reliable connection between the\nelectrodes of the two robots. The experimental results showed high average\nprecision (84.2%) for electrode detection using CNN. The percentage of\nsuccessful trials of the CNN-based electrode search algorithm reached 83% and\nthe average execution time accounted for 60 s. MobileCharger could introduce a\nnew level of charging systems and increase the prevalence of autonomous mobile\nrobots.",
          "link": "http://arxiv.org/abs/2107.10585",
          "publishedOn": "2021-07-23T02:00:31.570Z",
          "wordCount": 625,
          "title": "MobileCharger: an Autonomus Mobile Robot with Inverted Delta Actuator for Robust and Safe Robot Charging. (arXiv:2107.10585v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Romero_R/0/1/0/all/0/1\">Roland Albert A. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deypalan_M/0/1/0/all/0/1\">Mariefel Nicole Y. Deypalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrotra_S/0/1/0/all/0/1\">Suchit Mehrotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jungao_J/0/1/0/all/0/1\">John Titus Jungao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheils_N/0/1/0/all/0/1\">Natalie E. Sheils</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manduchi_E/0/1/0/all/0/1\">Elisabetta Manduchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1\">Jason H. Moore</a>",
          "description": "We ascertain and compare the performances of AutoML tools on large, highly\nimbalanced healthcare datasets.\n\nWe generated a large dataset using historical administrative claims including\ndemographic information and flags for disease codes in four different time\nwindows prior to 2019. We then trained three AutoML tools on this dataset to\npredict six different disease outcomes in 2019 and evaluated model performances\non several metrics.\n\nThe AutoML tools showed improvement from the baseline random forest model but\ndid not differ significantly from each other. All models recorded low area\nunder the precision-recall curve and failed to predict true positives while\nkeeping the true negative rate high. Model performance was not directly related\nto prevalence. We provide a specific use-case to illustrate how to select a\nthreshold that gives the best balance between true and false positive rates, as\nthis is an important consideration in medical applications.\n\nHealthcare datasets present several challenges for AutoML tools, including\nlarge sample size, high imbalance, and limitations in the available features\ntypes. Improvements in scalability, combinations of imbalance-learning\nresampling and ensemble approaches, and curated feature selection are possible\nnext steps to achieve better performance.\n\nAmong the three explored, no AutoML tool consistently outperforms the rest in\nterms of predictive performance. The performances of the models in this study\nsuggest that there may be room for improvement in handling medical claims data.\nFinally, selection of the optimal prediction threshold should be guided by the\nspecific practical application.",
          "link": "http://arxiv.org/abs/2107.10495",
          "publishedOn": "2021-07-23T02:00:31.555Z",
          "wordCount": 692,
          "title": "Benchmarking AutoML Frameworks for Disease Prediction Using Medical Claims. (arXiv:2107.10495v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silvestrin_L/0/1/0/all/0/1\">Luis P. Silvestrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantiskas_L/0/1/0/all/0/1\">Leonardos Pantiskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1\">Mark Hoogendoorn</a>",
          "description": "Time-series forecasting plays an important role in many domains. Boosted by\nthe advances in Deep Learning algorithms, it has for instance been used to\npredict wind power for eolic energy production, stock market fluctuations, or\nmotor overheating. In some of these tasks, we are interested in predicting\naccurately some particular moments which often are underrepresented in the\ndataset, resulting in a problem known as imbalanced regression. In the\nliterature, while recognized as a challenging problem, limited attention has\nbeen devoted on how to handle the problem in a practical setting. In this\npaper, we put forward a general approach to analyze time-series forecasting\nproblems focusing on those underrepresented moments to reduce imbalances. Our\napproach has been developed based on a case study in a large industrial\ncompany, which we use to exemplify the approach.",
          "link": "http://arxiv.org/abs/2107.10709",
          "publishedOn": "2021-07-23T02:00:31.541Z",
          "wordCount": 562,
          "title": "A Framework for Imbalanced Time-series Forecasting. (arXiv:2107.10709v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okunevich_I/0/1/0/all/0/1\">Iaroslav Okunevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinitatova_D/0/1/0/all/0/1\">Daria Trinitatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1\">Pavel Kopanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1\">Dzmitry Tsetserukou</a>",
          "description": "DeltaCharger is a novel charging robot with an Inverted Delta structure for\n3D positioning of electrodes to achieve robust and safe transferring energy\nbetween two mobile robots. The embedded high-fidelity tactile sensors allow to\nestimate the angular, vertical and horizontal misalignments between electrodes\non the charger mechanism and electrodes on the target robot using pressure data\non the contact surfaces. This is crucial for preventing a short circuit. In\nthis paper, the mechanism of the developed prototype and evaluation study of\ndifferent machine learning models for misalignment prediction are presented.\nThe experimental results showed that the proposed system can measure the angle,\nvertical and horizontal values of misalignment from pressure data with an\naccuracy of 95.46%, 98.2%, and 86.9%, respectively, using a Convolutional\nNeural Network (CNN). DeltaCharger can potentially bring a new level of\ncharging systems and improve the prevalence of mobile autonomous robots.",
          "link": "http://arxiv.org/abs/2107.10710",
          "publishedOn": "2021-07-23T02:00:31.521Z",
          "wordCount": 619,
          "title": "DeltaCharger: Charging Robot with Inverted Delta Mechanism and CNN-driven High Fidelity Tactile Perception for Precise 3D Positioning. (arXiv:2107.10710v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Arnab Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_D/0/1/0/all/0/1\">Davin Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gajjala_R/0/1/0/all/0/1\">Rishikesh Gajjala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gayen_S/0/1/0/all/0/1\">Sutanu Gayen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>",
          "description": "Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation\nmodels) are widely used to model causal interactions among continuous\nvariables. In this work, we study the problem of learning a fixed-structure\nGaussian Bayesian network up to a bounded error in total variation distance. We\nanalyze the commonly used node-wise least squares regression (LeastSquares) and\nprove that it has a near-optimal sample complexity. We also study a couple of\nnew algorithms for the problem:\n\n- BatchAvgLeastSquares takes the average of several batches of least squares\nsolutions at each node, so that one can interpolate between the batch size and\nthe number of batches. We show that BatchAvgLeastSquares also has near-optimal\nsample complexity.\n\n- CauchyEst takes the median of solutions to several batches of linear\nsystems at each node. We show that the algorithm specialized to polytrees,\nCauchyEstTree, has near-optimal sample complexity.\n\nExperimentally, we show that for uncontaminated, realizable data, the\nLeastSquares algorithm performs best, but in the presence of contamination or\nDAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares\nrespectively perform better.",
          "link": "http://arxiv.org/abs/2107.10450",
          "publishedOn": "2021-07-23T02:00:31.514Z",
          "wordCount": 618,
          "title": "Learning Sparse Fixed-Structure Gaussian Bayesian Networks. (arXiv:2107.10450v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dayta_D/0/1/0/all/0/1\">Dominic B. Dayta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrios_E/0/1/0/all/0/1\">Erniel B. Barrios</a>",
          "description": "Legacy procedures for topic modelling have generally suffered problems of\noverfitting and a weakness towards reconstructing sparse topic structures. With\nmotivation from a consumer-generated corpora, this paper proposes\nsemiparametric topic model, a two-step approach utilizing nonnegative matrix\nfactorization and semiparametric regression in topic modeling. The model\nenables the reconstruction of sparse topic structures in the corpus and\nprovides a generative model for predicting topics in new documents entering the\ncorpus. Assuming the presence of auxiliary information related to the topics,\nthis approach exhibits better performance in discovering underlying topic\nstructures in cases where the corpora are small and limited in vocabulary. In\nan actual consumer feedback corpus, the model also demonstrably provides\ninterpretable and useful topic definitions comparable with those produced by\nother methods.",
          "link": "http://arxiv.org/abs/2107.10651",
          "publishedOn": "2021-07-23T02:00:31.507Z",
          "wordCount": 555,
          "title": "Semiparametric Latent Topic Modeling on Consumer-Generated Corpora. (arXiv:2107.10651v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sprave_J/0/1/0/all/0/1\">Joachim Sprave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drescher_C/0/1/0/all/0/1\">Christian Drescher</a>",
          "description": "This paper addresses the problem of evaluating the quality of finite element\nmeshes for the purpose of structural mechanic simulations. It proposes the\napplication of a machine learning model trained on data collected from expert\nevaluations. The task is characterised as a classification problem, where\nquality of each individual element in a mesh is determined by its own\nproperties and adjacency structures. A domain-specific, yet simple\nrepresentation is proposed such that off-the-shelf machine learning methods can\nbe applied. Experimental data from industry practice demonstrates promising\nresults.",
          "link": "http://arxiv.org/abs/2107.10507",
          "publishedOn": "2021-07-23T02:00:31.500Z",
          "wordCount": 530,
          "title": "Evaluating the Quality of Finite Element Meshes with Machine Learning. (arXiv:2107.10507v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1\">Elijah Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_J/0/1/0/all/0/1\">Joseph Parker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winner_K/0/1/0/all/0/1\">Kevin Winner</a>",
          "description": "Conservation science depends on an accurate understanding of what's happening\nin a given ecosystem. How many species live there? What is the makeup of the\npopulation? How is that changing over time? Species Distribution Modeling (SDM)\nseeks to predict the spatial (and sometimes temporal) patterns of species\noccurrence, i.e. where a species is likely to be found. The last few years have\nseen a surge of interest in applying powerful machine learning tools to\nchallenging problems in ecology. Despite its considerable importance, SDM has\nreceived relatively little attention from the computer science community. Our\ngoal in this work is to provide computer scientists with the necessary\nbackground to read the SDM literature and develop ecologically useful ML-based\nSDM algorithms. In particular, we introduce key SDM concepts and terminology,\nreview standard models, discuss data availability, and highlight technical\nchallenges and pitfalls.",
          "link": "http://arxiv.org/abs/2107.10400",
          "publishedOn": "2021-07-23T02:00:31.494Z",
          "wordCount": 582,
          "title": "Species Distribution Modeling for Machine Learning Practitioners: A Review. (arXiv:2107.10400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ko_G/0/1/0/all/0/1\">Gihyuk Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1\">Gyumin Lim</a>",
          "description": "Deep Neural Networks (DNNs) have shown remarkable performance in a diverse\nrange of machine learning applications. However, it is widely known that DNNs\nare vulnerable to simple adversarial perturbations, which causes the model to\nincorrectly classify inputs. In this paper, we propose a simple yet effective\nmethod to detect adversarial examples, using methods developed to explain the\nmodel's behavior. Our key observation is that adding small, humanly\nimperceptible perturbations can lead to drastic changes in the model\nexplanations, resulting in unusual or irregular forms of explanations. From\nthis insight, we propose an unsupervised detection of adversarial examples\nusing reconstructor networks trained only on model explanations of benign\nexamples. Our evaluations with MNIST handwritten dataset show that our method\nis capable of detecting adversarial examples generated by the state-of-the-art\nalgorithms with high confidence. To the best of our knowledge, this work is the\nfirst in suggesting unsupervised defense method using model explanations.",
          "link": "http://arxiv.org/abs/2107.10480",
          "publishedOn": "2021-07-23T02:00:31.476Z",
          "wordCount": 590,
          "title": "Unsupervised Detection of Adversarial Examples with Model Explanations. (arXiv:2107.10480v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjali_P/0/1/0/all/0/1\">P. Anjali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_D/0/1/0/all/0/1\">Deepak N. Subramani</a>",
          "description": "We develop a Random Forest model to estimate the species distribution of\nAsian elephants in India and study the inter and intra-annual spatiotemporal\nvariability of habitats suitable for them. Climatic, topographic variables and\nsatellite-derived Land Use/Land Cover (LULC), Net Primary Productivity (NPP),\nLeaf Area Index (LAI), and Normalized Difference Vegetation Index (NDVI) are\nused as predictors, and the species sighting data of Asian elephants from\nGlobal Biodiversity Information Reserve is used to develop the Random Forest\nmodel. A careful hyper-parameter tuning and training-validation-testing cycle\nare completed to identify the significant predictors and develop a final model\nthat gives precision and recall of 0.78 and 0.77. The model is applied to\nestimate the spatial and temporal variability of suitable habitats. We observe\nthat seasonal reduction in the suitable habitat may explain the migration\npatterns of Asian elephants and the increasing human-elephant conflict.\nFurther, the total available suitable habitat area is observed to have reduced,\nwhich exacerbates the problem. This machine learning model is intended to serve\nas an input to the Agent-Based Model that we are building as part of our\nArtificial Intelligence-driven decision support tool to reduce human-wildlife\nconflict.",
          "link": "http://arxiv.org/abs/2107.10478",
          "publishedOn": "2021-07-23T02:00:31.469Z",
          "wordCount": 653,
          "title": "Inter and Intra-Annual Spatio-Temporal Variability of Habitat Suitability for Asian Elephants in India: A Random Forest Model-based Analysis. (arXiv:2107.10478v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolomvatsos_K/0/1/0/all/0/1\">Kostas Kolomvatsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_C/0/1/0/all/0/1\">Christos Anagnostopoulos</a>",
          "description": "The combination of the infrastructure provided by the Internet of Things\n(IoT) with numerous processing nodes present at the Edge Computing (EC)\necosystem opens up new pathways to support intelligent applications. Such\napplications can be provided upon humongous volumes of data collected by IoT\ndevices being transferred to the edge nodes through the network. Various\nprocessing activities can be performed on the discussed data and multiple\ncollaborative opportunities between EC nodes can facilitate the execution of\nthe desired tasks. In order to support an effective interaction between edge\nnodes, the knowledge about the geographically distributed data should be\nshared. Obviously, the migration of large amounts of data will harm the\nstability of the network stability and its performance. In this paper, we\nrecommend the exchange of data synopses than real data between EC nodes to\nprovide them with the necessary knowledge about peer nodes owning similar data.\nThis knowledge can be valuable when considering decisions such as data/service\nmigration and tasks offloading. We describe an continuous reasoning model that\nbuilds a temporal similarity map of the available datasets to get nodes\nunderstanding the evolution of data in their peers. We support the proposed\ndecision making mechanism through an intelligent similarity extraction scheme\nbased on an unsupervised machine learning model, and, at the same time, combine\nit with a statistical measure that represents the trend of the so-called\ndiscrepancy quantum. Our model can reveal the differences in the exchanged\nsynopses and provide a datasets similarity map which becomes the appropriate\nknowledge base to support the desired processing activities. We present the\nproblem under consideration and suggest a solution for that, while, at the same\ntime, we reveal its advantages and disadvantages through a large number of\nexperiments.",
          "link": "http://arxiv.org/abs/2107.10558",
          "publishedOn": "2021-07-23T02:00:31.460Z",
          "wordCount": 723,
          "title": "A Proactive Management Scheme for Data Synopses at the Edge. (arXiv:2107.10558v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burlacu_B/0/1/0/all/0/1\">Bogdan Burlacu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kammerer_L/0/1/0/all/0/1\">Lukas Kammerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Affenzeller_M/0/1/0/all/0/1\">Michael Affenzeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>",
          "description": "We introduce in this paper a runtime-efficient tree hashing algorithm for the\nidentification of isomorphic subtrees, with two important applications in\ngenetic programming for symbolic regression: fast, online calculation of\npopulation diversity and algebraic simplification of symbolic expression trees.\nBased on this hashing approach, we propose a simple diversity-preservation\nmechanism with promising results on a collection of symbolic regression\nbenchmark problems.",
          "link": "http://arxiv.org/abs/2107.10640",
          "publishedOn": "2021-07-23T02:00:31.453Z",
          "wordCount": 542,
          "title": "Hash-Based Tree Similarity and Simplification in Genetic Programming for Symbolic Regression. (arXiv:2107.10640v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fedyukova_A/0/1/0/all/0/1\">Anna Fedyukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_D/0/1/0/all/0/1\">Douglas Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capurro_D/0/1/0/all/0/1\">Daniel Capurro</a>",
          "description": "The proliferation of early diagnostic technologies, including self-monitoring\nsystems and wearables, coupled with the application of these technologies on\nlarge segments of healthy populations may significantly aggravate the problem\nof overdiagnosis. This can lead to unwanted consequences such as overloading\nhealth care systems and overtreatment, with potential harms to healthy\nindividuals. The advent of machine-learning tools to assist diagnosis -- while\npromising rapid and more personalised patient management and screening -- might\ncontribute to this issue. The identification of overdiagnosis is usually post\nhoc and demonstrated after long periods (from years to decades) and costly\nrandomised control trials. In this paper, we present an innovative approach\nthat allows us to preemptively detect potential cases of overdiagnosis during\npredictive model development. This approach is based on the combination of\nlabels obtained from a prediction model and clustered medical trajectories,\nusing sepsis in adults as a test case. This is one of the first attempts to\nquantify machine-learning induced overdiagnosis and we believe will serves as a\nplatform for further development, leading to guidelines for safe deployment of\ncomputational diagnostic tools.",
          "link": "http://arxiv.org/abs/2107.10399",
          "publishedOn": "2021-07-23T02:00:31.430Z",
          "wordCount": 626,
          "title": "Quantifying machine learning-induced overdiagnosis in sepsis. (arXiv:2107.10399v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byung-Hak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathi_V/0/1/0/all/0/1\">Varun Ganapathi</a>",
          "description": "Prediction of medical codes from clinical notes is both a practical and\nessential need for every healthcare delivery organization within current\nmedical systems. Automating annotation will save significant time and excessive\neffort spent by human coders today. However, the biggest challenge is directly\nidentifying appropriate medical codes out of several thousands of\nhigh-dimensional codes from unstructured free-text clinical notes. In the past\nthree years, with Convolutional Neural Networks (CNN) and Long Short-Term\nMemory (LTSM) networks, there have been vast improvements in tackling the most\nchallenging benchmark of the MIMIC-III-full-label inpatient clinical notes\ndataset. This progress raises the fundamental question of how far automated\nmachine learning (ML) systems are from human coders' working performance. We\nassessed the baseline of human coders' performance on the same subsampled\ntesting set. We also present our Read, Attend, and Code (RAC) model for\nlearning the medical code assignment mappings. By connecting convolved\nembeddings with self-attention and code-title guided attention modules,\ncombined with sentence permutation-based data augmentations and stochastic\nweight averaging training, RAC establishes a new state of the art (SOTA),\nconsiderably outperforming the current best Macro-F1 by 18.7%, and reaches past\nthe human-level coding baseline. This new milestone marks a meaningful step\ntoward fully autonomous medical coding (AMC) in machines reaching parity with\nhuman coders' performance in medical code prediction.",
          "link": "http://arxiv.org/abs/2107.10650",
          "publishedOn": "2021-07-23T02:00:31.424Z",
          "wordCount": 684,
          "title": "Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction from Clinical Notes by Machines. (arXiv:2107.10650v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chaoran Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_J/0/1/0/all/0/1\">Jian Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuling Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>",
          "description": "Academic performance prediction aims to leverage student-related information\nto predict their future academic outcomes, which is beneficial to numerous\neducational applications, such as personalized teaching and academic early\nwarning. In this paper, we address the problem by analyzing students' daily\nbehavior trajectories, which can be comprehensively tracked with campus\nsmartcard records. Different from previous studies, we propose a novel\nTri-Branch CNN architecture, which is equipped with row-wise, column-wise, and\ndepth-wise convolution and attention operations, to capture the characteristics\nof persistence, regularity, and temporal distribution of student behavior in an\nend-to-end manner, respectively. Also, we cast academic performance prediction\nas a top-$k$ ranking problem, and introduce a top-$k$ focused loss to ensure\nthe accuracy of identifying academically at-risk students. Extensive\nexperiments were carried out on a large-scale real-world dataset, and we show\nthat our approach substantially outperforms recently proposed methods for\nacademic performance prediction. For the sake of reproducibility, our codes\nhave been released at\nhttps://github.com/ZongJ1111/Academic-Performance-Prediction.",
          "link": "http://arxiv.org/abs/2107.10424",
          "publishedOn": "2021-07-23T02:00:31.406Z",
          "wordCount": 602,
          "title": "Tri-Branch Convolutional Neural Networks for Top-$k$ Focused Academic Performance Prediction. (arXiv:2107.10424v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10387",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Doty_C/0/1/0/all/0/1\">Christina Doty</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Gallagher_S/0/1/0/all/0/1\">Shaun Gallagher</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Cui_W/0/1/0/all/0/1\">Wenqi Cui</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chen_W/0/1/0/all/0/1\">Wenya Chen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Bhushan_S/0/1/0/all/0/1\">Shweta Bhushan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Oostrom_M/0/1/0/all/0/1\">Marjolein Oostrom</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Akers_S/0/1/0/all/0/1\">Sarah Akers</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Spurgeon_S/0/1/0/all/0/1\">Steven R. Spurgeon</a>",
          "description": "The recent growth in data volumes produced by modern electron microscopes\nrequires rapid, scalable, and flexible approaches to image segmentation and\nanalysis. Few-shot machine learning, which can richly classify images from a\nhandful of user-provided examples, is a promising route to high-throughput\nanalysis. However, current command-line implementations of such approaches can\nbe slow and unintuitive to use, lacking the real-time feedback necessary to\nperform effective classification. Here we report on the development of a\nPython-based graphical user interface that enables end users to easily conduct\nand visualize the output of few-shot learning models. This interface is\nlightweight and can be hosted locally or on the web, providing the opportunity\nto reproducibly conduct, share, and crowd-source few-shot analyses.",
          "link": "http://arxiv.org/abs/2107.10387",
          "publishedOn": "2021-07-23T02:00:31.340Z",
          "wordCount": 580,
          "title": "Design of a Graphical User Interface for Few-Shot Machine Learning Classification of Electron Microscopy Data. (arXiv:2107.10387v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>",
          "description": "Forecasting the behavior of other agents is an integral part of the modern\nrobotic autonomy stack, especially in safety-critical scenarios with\nhuman-robot interaction, such as autonomous driving. In turn, there has been a\nsignificant amount of interest and research in trajectory forecasting,\nresulting in a wide variety of approaches. Common to all works, however, is the\nuse of the same few accuracy-based evaluation metrics, e.g., displacement error\nand log-likelihood. While these metrics are informative, they are task-agnostic\nand predictions that are evaluated as equal can lead to vastly different\noutcomes, e.g., in downstream planning and decision making. In this work, we\ntake a step back and critically evaluate current trajectory forecasting\nmetrics, proposing task-aware metrics as a better measure of performance in\nsystems where prediction is being deployed. We additionally present one example\nof such a metric, incorporating planning-awareness within existing trajectory\nforecasting metrics.",
          "link": "http://arxiv.org/abs/2107.10297",
          "publishedOn": "2021-07-23T02:00:31.292Z",
          "wordCount": 583,
          "title": "Rethinking Trajectory Forecasting Evaluation. (arXiv:2107.10297v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>",
          "description": "Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",
          "link": "http://arxiv.org/abs/2107.10300",
          "publishedOn": "2021-07-23T02:00:31.263Z",
          "wordCount": 723,
          "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural Language with Interpretability. (arXiv:2107.10300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaker_M/0/1/0/all/0/1\">Mohammad Hossein Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "The idea to distinguish and quantify two important types of uncertainty,\noften referred to as aleatoric and epistemic, has received increasing attention\nin machine learning research in the last couple of years. In this paper, we\nconsider ensemble-based approaches to uncertainty quantification.\nDistinguishing between different types of uncertainty-aware learning\nalgorithms, we specifically focus on Bayesian methods and approaches based on\nso-called credal sets, which naturally suggest themselves from an ensemble\nlearning point of view. For both approaches, we address the question of how to\nquantify aleatoric and epistemic uncertainty. The effectiveness of\ncorresponding measures is evaluated and compared in an empirical study on\nclassification with a reject option.",
          "link": "http://arxiv.org/abs/2107.10384",
          "publishedOn": "2021-07-23T02:00:31.256Z",
          "wordCount": 540,
          "title": "Ensemble-based Uncertainty Quantification: Bayesian versus Credal Inference. (arXiv:2107.10384v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>",
          "description": "We present small-text, a simple modular active learning library, which offers\npool-based active learning for text classification in Python. It comes with\nvarious pre-implemented state-of-the-art query strategies, including some which\ncan leverage the GPU. Clearly defined interfaces allow to combine a multitude\nof such query strategies with different classifiers, thereby facilitating a\nquick mix and match, and enabling a rapid development of both active learning\nexperiments and applications. To make various classifiers accessible in a\nconsistent way, it integrates several well-known machine learning libraries,\nnamely, scikit-learn, PyTorch, and huggingface transformers -- for which the\nlatter integrations are available as optionally installable extensions. The\nlibrary is available under the MIT License at\nhttps://github.com/webis-de/small-text.",
          "link": "http://arxiv.org/abs/2107.10314",
          "publishedOn": "2021-07-23T02:00:31.233Z",
          "wordCount": 551,
          "title": "Small-text: Active Learning for Text Classification in Python. (arXiv:2107.10314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.10306",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_D/0/1/0/all/0/1\">Dan Wang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Florescu_I/0/1/0/all/0/1\">Ionut Florescu</a>",
          "description": "In Artificial Intelligence, interpreting the results of a Machine Learning\ntechnique often termed as a black box is a difficult task. A counterfactual\nexplanation of a particular \"black box\" attempts to find the smallest change to\nthe input values that modifies the prediction to a particular output, other\nthan the original one. In this work we formulate the problem of finding a\ncounterfactual explanation as an optimization problem. We propose a new\n\"sparsity algorithm\" which solves the optimization problem, while also\nmaximizing the sparsity of the counterfactual explanation. We apply the\nsparsity algorithm to provide a simple suggestion to publicly traded companies\nin order to improve their credit ratings. We validate the sparsity algorithm\nwith a synthetically generated dataset and we further apply it to quarterly\nfinancial statements from companies in financial, healthcare and IT sectors of\nthe US market. We provide evidence that the counterfactual explanation can\ncapture the nature of the real statement features that changed between the\ncurrent quarter and the following quarter when ratings improved. The empirical\nresults show that the higher the rating of a company the greater the \"effort\"\nrequired to further improve credit rating.",
          "link": "http://arxiv.org/abs/2107.10306",
          "publishedOn": "2021-07-23T02:00:31.151Z",
          "wordCount": 637,
          "title": "A Sparsity Algorithm with Applications to Corporate Credit Rating. (arXiv:2107.10306v1 [q-fin.RM])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}